corpusid,title,domain,url,section,section_title,paragraph,para_listed_answer,answer,indexed_answer,filtered_refids,filtered_refids_qualified,num_reference,segmented_answer
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s1,BERT embeddings,"['p1.0', 'p1.1']","[""Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence."", ""In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).""]","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","[['b22', 'b24'], ['b21', 'b65']]","[['b22', 'b24'], ['b21', 'b65']]",4,"1. Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.
2. In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.
3. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
4. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
5. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
6. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
7. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic)."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s3,Syntactic knowledge,"['p3.0', 'p3.1', 'p3.2']","['As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.', '(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).', 'Regarding syntactic competence of BERT\'s MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT\'s encoding of syntactic structure does not indicate that it actually relies on that knowledge.']","As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","(p3.0) As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(p3.1) (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

(p3.2) Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","[[], ['b18', 'b5'], ['b59', None, 'b64']]","[[], ['b18', 'b5'], ['b59', None, 'b64']]",5,"1. As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it.
2. Htut et al.(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root.
3. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation.
4. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2).
5. Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).
6. Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task.
7. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences.
8. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations.
9. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input.
10. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019).
11. This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s7,Self-attention heads,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4', 'p7.5', 'p7.6', 'p7.7', 'p7.8', 'p7.9']","['Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:', '• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);', '• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).', 'Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.', '[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.', 'Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.', 'Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.', ""(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis."", ""Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data."", ""Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.""]","Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","(p7.0) Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

(p7.1) • attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

(p7.2) • attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

(p7.3) Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

(p7.4) [SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

(p7.5) Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

(p7.6) Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(p7.7) (2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

(p7.8) Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

(p7.9) Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","[[], [], [None, 'b9'], [None], [], [], [], [None, 'b58'], [], [None, 'b9']]","[[], [], [None, 'b9'], [None], [], [], [], [None, 'b58'], [], [None, 'b9']]",7,"1. Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);• attending to previous/next tokens,    (Kovaleva et al., 2019)
2. According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"".
3. However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 .
4. Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019).
5. This apparent redundancy must be related to the overparametrization issue (see section 7).
6. Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation.
7. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them.
8. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.
9. [SEP] gets increased attention starting in layer 5, but its importance for prediction drops.
10. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.
11. Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.
12. Some BERT heads seem to specialize in certain types of syntactic relations.
13. Htut et al.(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions.
14. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline.
15. The evidence for nsubj, advmod, and amod has some variation between these two studies.
16. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context.
17. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).
18. present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora.
19. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.
20. Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,.
21. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks.
22. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998).
23. Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s8,BERT layers,"['p8.0', 'p8.1', 'p8.2', 'p8.3', 'p8.4']","['The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.', 'There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.', 'The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.', ""The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT."", 'The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.']","The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","(p8.0) The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

(p8.1) There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

(p8.2) The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

(p8.3) The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

(p8.4) The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","[[], ['b5'], ['b57', 'b50', 'b5'], [None, 'b9', 'b50'], ['b5']]","[[], ['b5'], ['b57', 'b50', 'b5'], [None, 'b9', 'b50'], ['b5']]",8,"1. The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings.
2. It stands to reason that the lower layers have the most linear word order information.
3. report a decrease in the knowledge of linear word order around layer 4 in BERT-base.
4. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.
5. There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers.
6. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large).
7. Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.
8. The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5).
9. There is conflicting evidence about syntactic chunks.
10. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling.
11. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing.
12. At the same time, the probing experiments by  find the opposite: both POS-3
13. These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.
14. The final layers of BERT are the most taskspecific.
15. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable .
16. In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019).
17. At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance.
18. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers.
19. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182).
20. But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial.
21. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.
22. The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"".
23. However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s10,Pre-training BERT,"['p10.0', 'p10.1', 'p10.2', 'p10.3', 'p10.4', 'p10.5', 'p10.6', 'p10.7', 'p10.8', 'p10.9']","['The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.', '• Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).', '• Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).', ""• Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);"", '• Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;', '• Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).', '• Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).', '• Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.', 'Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .', 'Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).']","The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

• Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

• Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

• Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

• Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

• Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

• Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

• Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","(p10.0) The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

(p10.1) • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

(p10.2) • Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

(p10.3) • Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

(p10.4) • Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

(p10.5) • Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

(p10.6) • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

(p10.7) • Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

(p10.8) Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

(p10.9) Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","[[], ['b62', 'b8', 'b16', None, 'b23'], ['b75', 'b62'], ['b8'], [], ['b47'], [], [], ['b17'], ['b31', 'b9']]","[[], ['b62', 'b8', 'b16', None, 'b23'], ['b75', 'b62'], ['b8'], [], ['b47'], [], [], ['b17'], ['b31', 'b9']]",12,"1. The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM).
2. Multiple studies have come up with alternative training objectives to improve on BERT.
3. • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting .
4. Wang et al. (2019a) Mikolov et al. (2013b).
5. • Permutation language modeling.
6. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order.
7. See also the n-gram word order reconstruction task (Wang et al., 2019a).
8. • Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);• Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;• Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model
9. (Sun et al., 2019b). • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).• Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.
10. Another obvious source of improvement is pretraining data.
11. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training.
12. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .
13. Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE .
14. Alternatively, SemBERT  integrates semantic role information with BERT representations.
15. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides.
16. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6).
17. However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019)."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s11,Model architecture choices,"['p11.0', 'p11.1']","[""To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks."", 'Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.']","To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","(p11.0) To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

(p11.1) Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","[['b20', 'b17', 'b75', 'b58'], []]","[['b20', 'b17', 'b75', 'b58'], []]",4,"1. To date, the most systematic study of BERT architecture was performed by .
2. They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others.
3. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable.
4. Larger hidden representation size was consistently better, but the gains varied by setting.
5. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance.
6. They also publish their recommendations for other model parameters.
7. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.
8. observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.
9. Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers.
10. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s12,Fine-tuning BERT,"['p12.0', 'p12.1', 'p12.2', 'p12.3', 'p12.4', 'p12.5', 'p12.6']","['Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).', '• Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).', 'With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.', '(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.', 'An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).', 'Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.', '7 How big should BERT be?']","Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

• Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

7 How big should BERT be?","(p12.0) Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

(p12.1) • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

(p12.2) With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(p12.3) (2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

(p12.4) An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

(p12.5) Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

(p12.6) 7 How big should BERT be?","[[None, 'b9'], ['b83'], [], [], ['b26'], ['b28'], []]","[[None, 'b9'], ['b83'], [], [], ['b26'], ['b28'], []]",5,"1. Pre-training + fine-tuning workflow is a crucial part of BERT.
2. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand.
3. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns.
4. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).
5. • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).
6. With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules.
7. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost.
8. Artetxe et al.(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.
9. An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).Initialization can have a dramatic effect on the training process (Petrov, 2010).
10. However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018).
11. Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation.
12. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.
13. 7 How big should BERT be?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s13,Overparametrization,"['p13.0', 'p13.1', 'p13.2', 'p13.3', 'p13.4']","['Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.', 'Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).', 'Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .', 'Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.', '(2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.']","Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.","(p13.0) Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

(p13.1) Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

(p13.2) Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

(p13.3) Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(p13.4) (2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.","[['b38', 'b70', 'b43'], ['b20', 'b37', 'b50', 'b46', None, 'b9', 'b58'], [], [], []]","[['b38', 'b70', 'b43'], ['b20', 'b37', 'b50', 'b46', None, 'b9', 'b58'], [], [], []]",10,"1. Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT.
2. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.
3. Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have.
4. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance.
5. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6
6. No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head.
7. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance.
8. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019).
9. Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).
10. Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case.
11. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .
12. Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers.
13. Clark et al.(2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s14,BERT compression,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4']","['Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.', 'Two main approaches include knowledge distillation and quantization.', 'The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).', ""The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware."", ""Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).""]","Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

Two main approaches include knowledge distillation and quantization.

The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","(p14.0) Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

(p14.1) Two main approaches include knowledge distillation and quantization.

(p14.2) The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

(p14.3) The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

(p14.4) Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","[[], [], ['b37', 'b54', None, 'b46'], ['b40', 'b76'], ['b72', 'b11']]","[[], [], ['b37', 'b54', None, 'b46'], ['b40', 'b76'], ['b72', 'b11']]",8,"1. Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss.
2. Such efforts to date are summarized in Table 1.
3. Two main approaches include knowledge distillation and quantization.
4. The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase).
5. This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).
6. The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019).
7. Note that this strategy often requires compatible hardware.
8. Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020)."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s15,Multilingual BERT,"['p15.0', 'p15.1', 'p15.2', 'p15.3', 'p15.4', 'p15.5', 'p15.6', 'p15.7']","['Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).', 'mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.', 'mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.', 'At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.', 'To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.', 'Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).', 'Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.', 'Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.']","Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.","(p15.0) Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

(p15.1) mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

(p15.2) mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

(p15.3) At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

(p15.4) To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

(p15.5) Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

(p15.6) Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

(p15.7) Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.","[['b30', 'b36', 'b64', 'b68', 'b13'], ['b13', 'b30'], ['b13', 'b30', 'b42', 'b68'], ['b13', 'b30', 'b36', 'b68'], ['b50'], ['b64', 'b2'], ['b20', 'b50', None, 'b9', 'b2', 'b58'], ['b56', 'b66', 'b39', 'b4', None]]","[['b30', 'b36', 'b64', 'b68', 'b13'], ['b13', 'b30'], ['b13', 'b30', 'b42', 'b68'], ['b13', 'b30', 'b36', 'b68'], ['b50'], ['b64', 'b2'], ['b20', 'b50', None, 'b9', 'b2', 'b58'], ['b56', 'b66', 'b39', 'b4', None]]",29,"1. Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary).
2. Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing.
3. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019).
4. The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019).
5. Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches.
6. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.
7. mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial .
8. It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019).
9. Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua.
10. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.
11. At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019).
12. Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages.
13. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary.
14. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.
15. To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge.
16. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"".
17. There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis .
18. A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.
19. Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019).
20. Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).
21. Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019).
22. Ablations are also problematic if the same information was duplicated elsewhere in the network.
23. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019)
24. fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.
25. Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019).
26. However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020).
27. Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence."
219177284,Conversational Machine Comprehension: a Literature Review,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/daadce35dc3694ae02dae07da7a3285daf3eab32,s2,What is Conversational Machine Comprehension?,"['p2.0', 'p2.1', 'p2.2', 'p2.3']","['The task of CMC is defined as: Given a passage P , the conversation history in the form of questionanswer pairs {Q 1 , A 1 , Q 2 , A 2 , ..., Q i−1 , A i−1 } and a question Q i , the model needs to predict the answer A i . The answer A i can either be a text span (s i , e i ) (Choi et al., 2018) or a free-form text {a i,1 , a i,2 , ..., a i,j } with evidence R i (Reddy et al., 2019). Single-turn MRC models cannot directly cater to CMC, as the latter is much more challenging to address. The major challenges being:', '• The encoding module needs to encode not only P and A i but also the conversational history.', '• General observation about information-seeking dialog in humans suggests that the starting dialogturns tend to focus on the beginning chunks of the passage and shift focus to the later chunks as the conversation progresses (Choi et al., 2018). The model is thus expected to capture these focal shifts during a conversation and reason pragmatically, instead of only matching lexically or via paraphrasing.', '• Multi-turn conversations are generally incremental and co-referential. These conversational dialogs are either drilling down (the current question is a request for more information about the topic), shifting topic (the current question is not immediately relevant to something previously discussed), returning topic (the current question is asking about a topic again after it had previously been shifted away from), clarification of topic, or definition of an entity (Yatskar, 2019). The model should, therefore, be able to take context from history which may or may not be immediate.']","The task of CMC is defined as: Given a passage P , the conversation history in the form of questionanswer pairs {Q 1 , A 1 , Q 2 , A 2 , ..., Q i−1 , A i−1 } and a question Q i , the model needs to predict the answer A i . The answer A i can either be a text span (s i , e i ) (Choi et al., 2018) or a free-form text {a i,1 , a i,2 , ..., a i,j } with evidence R i (Reddy et al., 2019). Single-turn MRC models cannot directly cater to CMC, as the latter is much more challenging to address. The major challenges being:

• The encoding module needs to encode not only P and A i but also the conversational history.

• General observation about information-seeking dialog in humans suggests that the starting dialogturns tend to focus on the beginning chunks of the passage and shift focus to the later chunks as the conversation progresses (Choi et al., 2018). The model is thus expected to capture these focal shifts during a conversation and reason pragmatically, instead of only matching lexically or via paraphrasing.

• Multi-turn conversations are generally incremental and co-referential. These conversational dialogs are either drilling down (the current question is a request for more information about the topic), shifting topic (the current question is not immediately relevant to something previously discussed), returning topic (the current question is asking about a topic again after it had previously been shifted away from), clarification of topic, or definition of an entity (Yatskar, 2019). The model should, therefore, be able to take context from history which may or may not be immediate.","(p2.0) The task of CMC is defined as: Given a passage P , the conversation history in the form of questionanswer pairs {Q 1 , A 1 , Q 2 , A 2 , ..., Q i−1 , A i−1 } and a question Q i , the model needs to predict the answer A i . The answer A i can either be a text span (s i , e i ) (Choi et al., 2018) or a free-form text {a i,1 , a i,2 , ..., a i,j } with evidence R i (Reddy et al., 2019). Single-turn MRC models cannot directly cater to CMC, as the latter is much more challenging to address. The major challenges being:

(p2.1) • The encoding module needs to encode not only P and A i but also the conversational history.

(p2.2) • General observation about information-seeking dialog in humans suggests that the starting dialogturns tend to focus on the beginning chunks of the passage and shift focus to the later chunks as the conversation progresses (Choi et al., 2018). The model is thus expected to capture these focal shifts during a conversation and reason pragmatically, instead of only matching lexically or via paraphrasing.

(p2.3) • Multi-turn conversations are generally incremental and co-referential. These conversational dialogs are either drilling down (the current question is a request for more information about the topic), shifting topic (the current question is not immediately relevant to something previously discussed), returning topic (the current question is asking about a topic again after it had previously been shifted away from), clarification of topic, or definition of an entity (Yatskar, 2019). The model should, therefore, be able to take context from history which may or may not be immediate.","[['b6', 'b36'], [], ['b6'], ['b42']]","[['b6', 'b36'], [], ['b6'], ['b42']]",4,"1. The task of CMC is defined as: Given a passage P , the conversation history in the form of questionanswer pairs {Q 1 , A 1 , Q 2 , A 2 , ..., Q i−1 , A i−1 } and a question Q i , the model needs to predict the answer A i .
2. The answer A i can either be a text span (s i , e i ) (Choi et al., 2018) or a free-form text {a i,1 , a i,2 , ..., a i,j } with evidence R i (Reddy et al., 2019).
3. Single-turn MRC models cannot directly cater to CMC, as the latter is much more challenging to address.
4. The major challenges being: • The encoding module needs to encode not only P and A i but also the conversational history.
5. • General observation about information-seeking dialog in humans suggests that the starting dialogturns tend to focus on the beginning chunks of the passage and shift focus to the later chunks as the conversation progresses (Choi et al., 2018).
6. The model is thus expected to capture these focal shifts during a conversation and reason pragmatically, instead of only matching lexically or via paraphrasing.
7. • Multi-turn conversations are generally incremental and co-referential.
8. These conversational dialogs are either drilling down (the current question is a request for more information about the topic), shifting topic (the current question is not immediately relevant to something previously discussed), returning topic (the current question is asking about a topic again after it had previously been shifted away from), clarification of topic, or definition of an entity (Yatskar, 2019).
9. The model should, therefore, be able to take context from history which may or may not be immediate."
219177284,Conversational Machine Comprehension: a Literature Review,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/daadce35dc3694ae02dae07da7a3285daf3eab32,s6,Generic Framework of a CMC Model,"['p6.0', 'p6.1', 'p6.2']","['(2) reasoning in the neural space to identify the answer vector and (3) decoding the answer vector into a natural language output. Huang et al. (2018a) adapted these steps in CMC by adding conversational history modeling. Qu et al. (2019c) proposed a ConvQA model with separate modules for history selection and modeling. Based on these prior works, we synthesize a generic framework for a CMC model. A typical CMC model is provided with context C, current question Q i and the conversation history', ', and needs to generate an output set O i . The CMC framework is provided in Fig. 1. There are four major components of the framework, based on their contribution to the overall CMC flow.', '1. History Selection module: With complicated dialog behaviors like topic shift or topic return (Yatskar, 2019), simply selecting immediate turns may not work well. A history selection module, therefore, chooses a subset H i of the history turns H i based on a policy (dynamic or static) that is expected to be more helpful than the others. If the history selection module is based on a dynamic learned policy (e.g. Qu et al. (2019b)), then feedback from the other modules can guide its update.']","(2) reasoning in the neural space to identify the answer vector and (3) decoding the answer vector into a natural language output. Huang et al. (2018a) adapted these steps in CMC by adding conversational history modeling. Qu et al. (2019c) proposed a ConvQA model with separate modules for history selection and modeling. Based on these prior works, we synthesize a generic framework for a CMC model. A typical CMC model is provided with context C, current question Q i and the conversation history

, and needs to generate an output set O i . The CMC framework is provided in Fig. 1. There are four major components of the framework, based on their contribution to the overall CMC flow.

1. History Selection module: With complicated dialog behaviors like topic shift or topic return (Yatskar, 2019), simply selecting immediate turns may not work well. A history selection module, therefore, chooses a subset H i of the history turns H i based on a policy (dynamic or static) that is expected to be more helpful than the others. If the history selection module is based on a dynamic learned policy (e.g. Qu et al. (2019b)), then feedback from the other modules can guide its update.","(p6.0) (2) reasoning in the neural space to identify the answer vector and (3) decoding the answer vector into a natural language output. Huang et al. (2018a) adapted these steps in CMC by adding conversational history modeling. Qu et al. (2019c) proposed a ConvQA model with separate modules for history selection and modeling. Based on these prior works, we synthesize a generic framework for a CMC model. A typical CMC model is provided with context C, current question Q i and the conversation history

(p6.1) , and needs to generate an output set O i . The CMC framework is provided in Fig. 1. There are four major components of the framework, based on their contribution to the overall CMC flow.

(p6.2) 1. History Selection module: With complicated dialog behaviors like topic shift or topic return (Yatskar, 2019), simply selecting immediate turns may not work well. A history selection module, therefore, chooses a subset H i of the history turns H i based on a policy (dynamic or static) that is expected to be more helpful than the others. If the history selection module is based on a dynamic learned policy (e.g. Qu et al. (2019b)), then feedback from the other modules can guide its update.","[['b33', 'b15'], [], ['b42', 'b32']]","[['b33', 'b15'], [], ['b42', 'b32']]",4,"1. (2) reasoning in the neural space to identify the answer vector and (3) decoding the answer vector into a natural language output.
2. Huang et al. (2018a) adapted these steps in CMC by adding conversational history modeling.
3. Qu et al. (2019c) proposed a ConvQA model with separate modules for history selection and modeling.
4. Based on these prior works, we synthesize a generic framework for a CMC model.
5. A typical CMC model is provided with context C, current question Q i and the conversation history, and needs to generate an output set
6. O i . The CMC framework is provided in Fig. 1.
7. There are four major components of the framework, based on their contribution to the overall CMC flow.
8. 1. History Selection module: With complicated dialog behaviors like topic shift or topic return (Yatskar, 2019), simply selecting immediate turns may not work well.
9. A history selection module, therefore, chooses a subset H i of the history turns H
10. i based on a policy (dynamic or static)
11. that is expected to be more helpful than the others.
12. If the history selection module is based on a dynamic learned policy (e.g. Qu et al. (2019b)), then feedback from the other modules can guide its update."
219177284,Conversational Machine Comprehension: a Literature Review,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/daadce35dc3694ae02dae07da7a3285daf3eab32,s7,Encoder:,"['p7.0', 'p7.1', 'p7.2']","['The lexical tokens of the context passage C, selected conversational turns H i , and the current question Q i need to be transformed into input embeddings for the reasoning module. Encoder facilitates this transition. The encoder steps may vary with every approach and reasoning inputs, at a high level, encoding involves transformation and combination of context-independent word embeddings called lexical embeddings such as GloVE (Pennington et al., 2014), intra-sequence contextual  Figure 1: Generic framework of a CMC model. A typical CMC model would consist of (1) History selection module, that selects a subset H i of conversational history H i relevant to the current question Q i ;', '(2) Encoder, that encodes the lexical tokens of context C, Q i and H i into input embeddings for contextual integration layer; (3) Reasoning module, that performs contextual integration of input embeddings into contextualized embeddings; and finally, (4) Output predictor, that predicts the output set O i based on contextualized embeddings. embeddings e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) or RNN, question-aware embeddings, and additional feature embeddings like POS tags , history embedding (Qu et al., 2019c) or conversation count. Conversational history H i is generally integrated with this module into any or all of the contextual input embeddings. This process is called History modeling and is the most significant aspect of a CMC encoder.', '3. Contextual Integration layer: Contextual information accumulated in the passage, query, and/or history embeddings individually must be fused to generate query-aware and/or history-aware contextualized output embeddings. This process may involve a single layer (single-step reasoning) or repetition across multiple layers (multi-step reasoning). Input for this module generally consists of two (or more) sequence sets for every history turn, or aggregated across all turns, which are then fused in each layer and often inter-weaved (Huang et al., 2018b) with attention.']","The lexical tokens of the context passage C, selected conversational turns H i , and the current question Q i need to be transformed into input embeddings for the reasoning module. Encoder facilitates this transition. The encoder steps may vary with every approach and reasoning inputs, at a high level, encoding involves transformation and combination of context-independent word embeddings called lexical embeddings such as GloVE (Pennington et al., 2014), intra-sequence contextual  Figure 1: Generic framework of a CMC model. A typical CMC model would consist of (1) History selection module, that selects a subset H i of conversational history H i relevant to the current question Q i ;

(2) Encoder, that encodes the lexical tokens of context C, Q i and H i into input embeddings for contextual integration layer; (3) Reasoning module, that performs contextual integration of input embeddings into contextualized embeddings; and finally, (4) Output predictor, that predicts the output set O i based on contextualized embeddings. embeddings e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) or RNN, question-aware embeddings, and additional feature embeddings like POS tags , history embedding (Qu et al., 2019c) or conversation count. Conversational history H i is generally integrated with this module into any or all of the contextual input embeddings. This process is called History modeling and is the most significant aspect of a CMC encoder.

3. Contextual Integration layer: Contextual information accumulated in the passage, query, and/or history embeddings individually must be fused to generate query-aware and/or history-aware contextualized output embeddings. This process may involve a single layer (single-step reasoning) or repetition across multiple layers (multi-step reasoning). Input for this module generally consists of two (or more) sequence sets for every history turn, or aggregated across all turns, which are then fused in each layer and often inter-weaved (Huang et al., 2018b) with attention.","(p7.0) The lexical tokens of the context passage C, selected conversational turns H i , and the current question Q i need to be transformed into input embeddings for the reasoning module. Encoder facilitates this transition. The encoder steps may vary with every approach and reasoning inputs, at a high level, encoding involves transformation and combination of context-independent word embeddings called lexical embeddings such as GloVE (Pennington et al., 2014), intra-sequence contextual  Figure 1: Generic framework of a CMC model. A typical CMC model would consist of (1) History selection module, that selects a subset H i of conversational history H i relevant to the current question Q i ;

(p7.1) (2) Encoder, that encodes the lexical tokens of context C, Q i and H i into input embeddings for contextual integration layer; (3) Reasoning module, that performs contextual integration of input embeddings into contextualized embeddings; and finally, (4) Output predictor, that predicts the output set O i based on contextualized embeddings. embeddings e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) or RNN, question-aware embeddings, and additional feature embeddings like POS tags , history embedding (Qu et al., 2019c) or conversation count. Conversational history H i is generally integrated with this module into any or all of the contextual input embeddings. This process is called History modeling and is the most significant aspect of a CMC encoder.

(p7.2) 3. Contextual Integration layer: Contextual information accumulated in the passage, query, and/or history embeddings individually must be fused to generate query-aware and/or history-aware contextualized output embeddings. This process may involve a single layer (single-step reasoning) or repetition across multiple layers (multi-step reasoning). Input for this module generally consists of two (or more) sequence sets for every history turn, or aggregated across all turns, which are then fused in each layer and often inter-weaved (Huang et al., 2018b) with attention.","[['b28'], ['b29', 'b7', 'b33'], ['b16']]","[['b28'], ['b29', 'b7', 'b33'], ['b16']]",5,"1. The lexical tokens of the context passage C, selected conversational turns H i , and the current question Q i need to be transformed into input embeddings for the reasoning module.
2. Encoder facilitates this transition.
3. The encoder steps may vary with every approach and reasoning inputs, at a high level, encoding involves transformation and combination of context-independent word embeddings called lexical embeddings such as GloVE (Pennington et al., 2014), intra-sequence contextual  Figure 1: Generic framework of a CMC model.
4. A typical CMC model would consist of (1) History selection module, that selects a subset H i of conversational history
5. H i relevant to the current question Q i ;(2) Encoder, that encodes the lexical tokens of context C, Q i and H i into input embeddings for contextual integration layer; (3) Reasoning module, that performs contextual integration of input embeddings into contextualized embeddings; and finally, (4) Output predictor, that predicts the output set O i based on contextualized embeddings.
6. embeddings e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) or RNN, question-aware embeddings, and additional feature embeddings like POS tags , history embedding (Qu et al., 2019c) or conversation count.
7. Conversational history H i is generally integrated with this module into any or all of the contextual input embeddings.
8. This process is called History modeling and is the most significant aspect of a CMC encoder.
9. 3. Contextual Integration layer: Contextual information accumulated in the passage, query, and/or history embeddings individually must be fused to generate query-aware and/or history-aware contextualized output embeddings.
10. This process may involve a single layer (single-step reasoning) or repetition across multiple layers (multi-step reasoning).
11. Input for this module generally consists of two (or more) sequence sets for every history turn, or aggregated across all turns, which are then fused in each layer and often inter-weaved (Huang et al., 2018b) with attention."
219177284,Conversational Machine Comprehension: a Literature Review,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/daadce35dc3694ae02dae07da7a3285daf3eab32,s10,Trends in History Selection,['p10.0'],"[""Almost all of the current CMC models select conversational history based on a heuristic of considering k immediate turns, often decided by performance such as BiDAF++ (Choi et al., 2018;Yatskar, 2019), SDNet , BiDAF++ w/ 2-ctx (Ohsugi et al., 2019) use last two turns as including the third turn degrades performance. History Attention Mechanism (HAM) based model Qu et al. (2019b) uses a dynamic history selection policy by attending over contextualized representations of all the previous history turns at word-level or sequence-level and combining with current turn's representation as shown in Fig. 3a.""]","Almost all of the current CMC models select conversational history based on a heuristic of considering k immediate turns, often decided by performance such as BiDAF++ (Choi et al., 2018;Yatskar, 2019), SDNet , BiDAF++ w/ 2-ctx (Ohsugi et al., 2019) use last two turns as including the third turn degrades performance. History Attention Mechanism (HAM) based model Qu et al. (2019b) uses a dynamic history selection policy by attending over contextualized representations of all the previous history turns at word-level or sequence-level and combining with current turn's representation as shown in Fig. 3a.","(p10.0) Almost all of the current CMC models select conversational history based on a heuristic of considering k immediate turns, often decided by performance such as BiDAF++ (Choi et al., 2018;Yatskar, 2019), SDNet , BiDAF++ w/ 2-ctx (Ohsugi et al., 2019) use last two turns as including the third turn degrades performance. History Attention Mechanism (HAM) based model Qu et al. (2019b) uses a dynamic history selection policy by attending over contextualized representations of all the previous history turns at word-level or sequence-level and combining with current turn's representation as shown in Fig. 3a.","[['b6', 'b25', 'b42', 'b32']]","[['b6', 'b25', 'b42', 'b32']]",4,"1. Almost all of the current CMC models select conversational history based on a heuristic of considering k immediate turns, often decided by performance such as BiDAF++ (Choi et al., 2018;Yatskar, 2019), SDNet , BiDAF++ w/ 2-ctx (Ohsugi et al., 2019) use last two turns as including the third turn degrades performance.
2. History Attention Mechanism (HAM) based model Qu et al. (2019b) uses a dynamic history selection policy by attending over contextualized representations of all the previous history turns at word-level or sequence-level and combining with current turn's representation as shown in Fig. 3a."
219177284,Conversational Machine Comprehension: a Literature Review,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/daadce35dc3694ae02dae07da7a3285daf3eab32,s17,Discussion,"['p17.0', 'p17.1', 'p17.2']","['How does the research progress in CMC, a constrained setup, benefit the more into-the-wild domain of Conversational Search? As stated by Qu et al. (2019a), Conversational QA (and CMC) is a simplified setting of Conversational Search (ConvSearch), an information-seeking, ""System Ask, User Respond"" paradigm (Zhang et al., 2018b), that does not focus on asking proactively. CMC, specifically, tries to address the challenges of NLU, via contextual encoding, reasoning, and handling conversational history, via history selection and modeling. In that aspect, CMC is a concrete enough setting for IR researchers to understand the change of information needs and interactivity between conversational cycles.', 'Could Commonsense Reasoning improve CMC? Commonsense Reasoning (CR) is based on the set of background information or world knowledge that an individual is intended to know or assume, and may be missing from context. On the other hand, Pragmatic reasoning, which the current CMC models cater to, is based on the derivation of explicit and implicit meanings within the context. The current MRC systems are nearing human performance on most datasets, however, they still perform poorly on single-turn CR based questions (Zhang et al., 2018a). While there is recently increasing interest in CR in the single-turn MRC setting Ostermann et al., 2018;Lin et al., 2017), CMC remains relatively untouched. This may probably be due to the lack of foreknowledge requiring unanswerable questions (e.g. in SQuAD 2.0 (Rajpurkar et al., 2018)) in current CMC datasets (Yatskar, 2019), suggesting a need for more complex CMC datasets that incorporate CR. However, humans annotators may often apply common-sense reasoning involuntarily while answering questions or comprehending, thus leaving room for incorporating CR in models. There seems to be no recent work that invalidates, experimentally, the role of CR in CMC. QuAC, for example, is drawn from articles on personalities, and current models still lag behind the human benchmark. It may be worth experimenting if adding domain knowledge or attributes about the context, like location and gender, help improve answering these questions.', 'Why did the paper focus on common trends across each component rather than a single overarching classification of CMC models? The study of common trends in modeling, rather than a single overarching classification, helped in providing a multi-faceted view of CMC that can generalize on future models, and identify possible open-ended research questions, such as (a) For history selection, HAM (Qu et al., 2019b) has proved to be both effective and intuitive in selecting relevant history turns. The application of this history selection approach on previous techniques that considered immediate K turns could be experimented with. (b) As mentioned in training methodology (section 6.4), RoBERTa-based CMC model (Ju et al., 2019) that used knowledge distillation and adversarial training achieved state-of-the-art CoQA results (Reddy et al., 2019). This suggests that different training approach along with multi-task learning improves the performance of base models. These procedures could be experimented with more advanced models such as HAM (Qu et al., 2019b) and FlowDelta (Yeh and Chen, 2019).']","How does the research progress in CMC, a constrained setup, benefit the more into-the-wild domain of Conversational Search? As stated by Qu et al. (2019a), Conversational QA (and CMC) is a simplified setting of Conversational Search (ConvSearch), an information-seeking, ""System Ask, User Respond"" paradigm (Zhang et al., 2018b), that does not focus on asking proactively. CMC, specifically, tries to address the challenges of NLU, via contextual encoding, reasoning, and handling conversational history, via history selection and modeling. In that aspect, CMC is a concrete enough setting for IR researchers to understand the change of information needs and interactivity between conversational cycles.

Could Commonsense Reasoning improve CMC? Commonsense Reasoning (CR) is based on the set of background information or world knowledge that an individual is intended to know or assume, and may be missing from context. On the other hand, Pragmatic reasoning, which the current CMC models cater to, is based on the derivation of explicit and implicit meanings within the context. The current MRC systems are nearing human performance on most datasets, however, they still perform poorly on single-turn CR based questions (Zhang et al., 2018a). While there is recently increasing interest in CR in the single-turn MRC setting Ostermann et al., 2018;Lin et al., 2017), CMC remains relatively untouched. This may probably be due to the lack of foreknowledge requiring unanswerable questions (e.g. in SQuAD 2.0 (Rajpurkar et al., 2018)) in current CMC datasets (Yatskar, 2019), suggesting a need for more complex CMC datasets that incorporate CR. However, humans annotators may often apply common-sense reasoning involuntarily while answering questions or comprehending, thus leaving room for incorporating CR in models. There seems to be no recent work that invalidates, experimentally, the role of CR in CMC. QuAC, for example, is drawn from articles on personalities, and current models still lag behind the human benchmark. It may be worth experimenting if adding domain knowledge or attributes about the context, like location and gender, help improve answering these questions.

Why did the paper focus on common trends across each component rather than a single overarching classification of CMC models? The study of common trends in modeling, rather than a single overarching classification, helped in providing a multi-faceted view of CMC that can generalize on future models, and identify possible open-ended research questions, such as (a) For history selection, HAM (Qu et al., 2019b) has proved to be both effective and intuitive in selecting relevant history turns. The application of this history selection approach on previous techniques that considered immediate K turns could be experimented with. (b) As mentioned in training methodology (section 6.4), RoBERTa-based CMC model (Ju et al., 2019) that used knowledge distillation and adversarial training achieved state-of-the-art CoQA results (Reddy et al., 2019). This suggests that different training approach along with multi-task learning improves the performance of base models. These procedures could be experimented with more advanced models such as HAM (Qu et al., 2019b) and FlowDelta (Yeh and Chen, 2019).","(p17.0) How does the research progress in CMC, a constrained setup, benefit the more into-the-wild domain of Conversational Search? As stated by Qu et al. (2019a), Conversational QA (and CMC) is a simplified setting of Conversational Search (ConvSearch), an information-seeking, ""System Ask, User Respond"" paradigm (Zhang et al., 2018b), that does not focus on asking proactively. CMC, specifically, tries to address the challenges of NLU, via contextual encoding, reasoning, and handling conversational history, via history selection and modeling. In that aspect, CMC is a concrete enough setting for IR researchers to understand the change of information needs and interactivity between conversational cycles.

(p17.1) Could Commonsense Reasoning improve CMC? Commonsense Reasoning (CR) is based on the set of background information or world knowledge that an individual is intended to know or assume, and may be missing from context. On the other hand, Pragmatic reasoning, which the current CMC models cater to, is based on the derivation of explicit and implicit meanings within the context. The current MRC systems are nearing human performance on most datasets, however, they still perform poorly on single-turn CR based questions (Zhang et al., 2018a). While there is recently increasing interest in CR in the single-turn MRC setting Ostermann et al., 2018;Lin et al., 2017), CMC remains relatively untouched. This may probably be due to the lack of foreknowledge requiring unanswerable questions (e.g. in SQuAD 2.0 (Rajpurkar et al., 2018)) in current CMC datasets (Yatskar, 2019), suggesting a need for more complex CMC datasets that incorporate CR. However, humans annotators may often apply common-sense reasoning involuntarily while answering questions or comprehending, thus leaving room for incorporating CR in models. There seems to be no recent work that invalidates, experimentally, the role of CR in CMC. QuAC, for example, is drawn from articles on personalities, and current models still lag behind the human benchmark. It may be worth experimenting if adding domain knowledge or attributes about the context, like location and gender, help improve answering these questions.

(p17.2) Why did the paper focus on common trends across each component rather than a single overarching classification of CMC models? The study of common trends in modeling, rather than a single overarching classification, helped in providing a multi-faceted view of CMC that can generalize on future models, and identify possible open-ended research questions, such as (a) For history selection, HAM (Qu et al., 2019b) has proved to be both effective and intuitive in selecting relevant history turns. The application of this history selection approach on previous techniques that considered immediate K turns could be experimented with. (b) As mentioned in training methodology (section 6.4), RoBERTa-based CMC model (Ju et al., 2019) that used knowledge distillation and adversarial training achieved state-of-the-art CoQA results (Reddy et al., 2019). This suggests that different training approach along with multi-task learning improves the performance of base models. These procedures could be experimented with more advanced models such as HAM (Qu et al., 2019b) and FlowDelta (Yeh and Chen, 2019).","[['b31', 'b45'], ['b21', 'b26', 'b35', 'b44', 'b42'], ['b43', 'b18', 'b36', 'b32']]","[['b31', 'b45'], ['b21', 'b26', 'b35', 'b44', 'b42'], ['b43', 'b18', 'b36', 'b32']]",11,"1. How does the research progress in CMC, a constrained setup, benefit the more into-the-wild domain of Conversational Search?
2. As stated by Qu et al. (2019a), Conversational QA (and CMC) is a simplified setting of Conversational Search (ConvSearch), an information-seeking, ""System Ask, User Respond"" paradigm (Zhang et al., 2018b), that does not focus on asking proactively.
3. CMC, specifically, tries to address the challenges of NLU, via contextual encoding, reasoning, and handling conversational history, via history selection and modeling.
4. In that aspect, CMC is a concrete enough setting for IR researchers to understand the change of information needs and interactivity between conversational cycles.
5. Could Commonsense Reasoning improve CMC?
6. Commonsense Reasoning (CR) is based on the set of background information or world knowledge that an individual is intended to know or assume, and may be missing from context.
7. On the other hand, Pragmatic reasoning, which the current CMC models cater to, is based on the derivation of explicit and implicit meanings within the context.
8. The current MRC systems are nearing human performance on most datasets, however, they still perform poorly on single-turn CR based questions (Zhang et al., 2018a).
9. While there is recently increasing interest in CR in the single-turn MRC setting Ostermann et al., 2018;Lin et al., 2017), CMC remains relatively untouched.
10. This may probably be due to the lack of foreknowledge requiring unanswerable questions (e.g. in SQuAD 2.0 (Rajpurkar et al., 2018)) in current CMC datasets (Yatskar, 2019), suggesting a need for more complex CMC datasets that incorporate CR.
11. However, humans annotators may often apply common-sense reasoning involuntarily while answering questions or comprehending, thus leaving room for incorporating CR in models.
12. There seems to be no recent work that invalidates, experimentally, the role of CR in CMC.
13. QuAC, for example, is drawn from articles on personalities, and current models still lag behind the human benchmark.
14. It may be worth experimenting if adding domain knowledge or attributes about the context, like location and gender, help improve answering these questions.
15. Why did the paper focus on common trends across each component rather than a single overarching classification of CMC models?
16. The study of common trends in modeling, rather than a single overarching classification, helped in providing a multi-faceted view of CMC that can generalize on future models, and identify possible open-ended research questions, such as (a) For history selection, HAM (Qu et al., 2019b) has proved to be both effective and intuitive in selecting relevant history turns.
17. The application of this history selection approach on previous techniques that considered immediate K turns could be experimented with.
18. (b) As mentioned in training methodology (section 6.4), RoBERTa-based CMC model (Ju et al., 2019) that used knowledge distillation and adversarial training achieved state-of-the-art CoQA results (Reddy et al., 2019).
19. This suggests that different training approach along with multi-task learning improves the performance of base models.
20. These procedures could be experimented with more advanced models such as HAM (Qu et al., 2019b) and FlowDelta (Yeh and Chen, 2019)."
219177284,Conversational Machine Comprehension: a Literature Review,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/daadce35dc3694ae02dae07da7a3285daf3eab32,s12,Trends in Contextual Reasoning,"['p12.0', 'p12.1']","['While every CMC model has its unique flavor in integrating encoded representations of the query, history, and text contextually, some recurrent themes in reasoning can still be drawn. It is important to note that some of these themes will reflect state-of-the-art techniques around their release, which may now be obsolete. However, having their knowledge would prevent the re-exploration of those ideas. Following are the commonly observed themes:', 'A. Attention-based Reasoning with Sequence Models This was a common theme across MRC models until transformers (Vaswani et al., 2017) were introduced and got rid of sequence modeling. Consequently, initial baseline models were based on this approach. CoQA baseline (Reddy et al., 2019) first involves DrQA (Chen et al., 2017), which performs BiLSTM based contextual integration over encoded tokens for extractive span, and later PGNet, that uses attentionbased neural machine translation (Bahdanau et al., 2015) for abstractive answer reasoning. QuAC baseline (Choi et al., 2018) combines self-attention with BiDAF (Seo et al., 2017) that performs reasoning via multi-layered bidirectional attention followed by multi-layered BiLSTM (BiDAF++). SDNet  applies both inter-attention and self-attention in multiple layers, interleaved with BiLSTM, to comprehend conversation context.']","While every CMC model has its unique flavor in integrating encoded representations of the query, history, and text contextually, some recurrent themes in reasoning can still be drawn. It is important to note that some of these themes will reflect state-of-the-art techniques around their release, which may now be obsolete. However, having their knowledge would prevent the re-exploration of those ideas. Following are the commonly observed themes:

A. Attention-based Reasoning with Sequence Models This was a common theme across MRC models until transformers (Vaswani et al., 2017) were introduced and got rid of sequence modeling. Consequently, initial baseline models were based on this approach. CoQA baseline (Reddy et al., 2019) first involves DrQA (Chen et al., 2017), which performs BiLSTM based contextual integration over encoded tokens for extractive span, and later PGNet, that uses attentionbased neural machine translation (Bahdanau et al., 2015) for abstractive answer reasoning. QuAC baseline (Choi et al., 2018) combines self-attention with BiDAF (Seo et al., 2017) that performs reasoning via multi-layered bidirectional attention followed by multi-layered BiLSTM (BiDAF++). SDNet  applies both inter-attention and self-attention in multiple layers, interleaved with BiLSTM, to comprehend conversation context.","(p12.0) While every CMC model has its unique flavor in integrating encoded representations of the query, history, and text contextually, some recurrent themes in reasoning can still be drawn. It is important to note that some of these themes will reflect state-of-the-art techniques around their release, which may now be obsolete. However, having their knowledge would prevent the re-exploration of those ideas. Following are the commonly observed themes:

(p12.1) A. Attention-based Reasoning with Sequence Models This was a common theme across MRC models until transformers (Vaswani et al., 2017) were introduced and got rid of sequence modeling. Consequently, initial baseline models were based on this approach. CoQA baseline (Reddy et al., 2019) first involves DrQA (Chen et al., 2017), which performs BiLSTM based contextual integration over encoded tokens for extractive span, and later PGNet, that uses attentionbased neural machine translation (Bahdanau et al., 2015) for abstractive answer reasoning. QuAC baseline (Choi et al., 2018) combines self-attention with BiDAF (Seo et al., 2017) that performs reasoning via multi-layered bidirectional attention followed by multi-layered BiLSTM (BiDAF++). SDNet  applies both inter-attention and self-attention in multiple layers, interleaved with BiLSTM, to comprehend conversation context.","[[], ['b3', 'b36', 'b39', 'b0', 'b38', 'b6']]","[[], ['b3', 'b36', 'b39', 'b0', 'b38', 'b6']]",6,"1. While every CMC model has its unique flavor in integrating encoded representations of the query, history, and text contextually, some recurrent themes in reasoning can still be drawn.
2. It is important to note that some of these themes will reflect state-of-the-art techniques around their release, which may now be obsolete.
3. However, having their knowledge would prevent the re-exploration of those ideas.
4. Following are the commonly observed themes:A.
5. Attention-based Reasoning with Sequence Models
6. This was a common theme across MRC models until transformers (Vaswani et al., 2017) were introduced and got rid of sequence modeling.
7. Consequently, initial baseline models were based on this approach.
8. CoQA baseline (Reddy et al., 2019) first involves DrQA (Chen et al., 2017), which performs BiLSTM based contextual integration over encoded tokens for extractive span, and later PGNet, that uses attentionbased neural machine translation (Bahdanau et al., 2015) for abstractive answer reasoning.
9. QuAC baseline (Choi et al., 2018) combines self-attention with BiDAF (Seo et al., 2017) that performs reasoning via multi-layered bidirectional attention followed by multi-layered BiLSTM (BiDAF++).
10. SDNet  applies both inter-attention and self-attention in multiple layers, interleaved with BiLSTM, to comprehend conversation context."
219177284,Conversational Machine Comprehension: a Literature Review,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/daadce35dc3694ae02dae07da7a3285daf3eab32,s15,C. Contextual Integration using Pre-trained Language Models,"['p15.0', 'p15.1', 'p15.2', 'p15.3', 'p15.4', 'p15.5', 'p15.6']","['Large-scale pre-trained LMs such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018) and RoBERTa , have become the current state-of-the-art approaches for contextual reasoning in CMC models, with leaderboards of both datasets stacked with these models or their variants. The approach is based on the fine-tune BERT-based MRC modeling outlined by Devlin et al. (2019), in which question and context are packed together (with marker embeddings to distinguish) in an input sequence to BERT that outputs contextualized question-aware embeddings for each input token.', 'Using pre-trained models for reasoning is advantageous in two aspects: Firstly, it simplifies the architecture by fusing encoding and reasoning modules into a single module. Secondly, it provides a ready-to-tune architecture that abstracts out complex contextual interactions between query and context while providing sufficient flexibility to control interactivity via augmentation of input embeddings i.e. concatenation of special embeddings to input tokens that signal the model to incorporate a desirable characteristic in contextualization.', ""(a) HAM uses a dynamic attention-based history selection policy. Contextualized representations are generated by the model's encoder (BERT+PosHAE) for every history turn at word and sequence levels. Sequence-level embeddings are used to compute attention weights via scaled-dot product, and aggregate representations are generated by a weighted combination of embeddings of each turn in the proportion of their attention weights. Thus, attention weights help in determining the degree of selection (relevance) of each history turn."", ""(b) HAM's BERT based Encoder (Reasoning Architecture) for every conversation turn. The encoder is provided with input sequence consisting of query tokens (yellow) and context tokens (green) separated by [SEP]. It outputs contextualized representations Ti corresponding to aligned question/passage tokens. The Token embeddings are augmented with segment embeddings(to differentiate query and context), positional embeddings (for distinct position in the sequence), and Positional HAE embeddings (for encoding history answer and relative conversational turn). Figure 3: Illustration of (a) history selection module and (b) encoder/reasoning module of History Attention Mechanism (HAM) model (Qu et al., 2019b)."", 'However, incorporating history into these models is a key challenge in this approach as most of the transformer models such as BERT only accept 2 segments ids in the input sequence. Based on recent research in CMC, two main trends in solving the history integration issue are discussed below:', '1. Modify the input embeddings for a single-turn MRC model to incorporate history. This is done by either appending the entire conversation to the question, such as Ju et al. (2019) which uses RoBERTa ) as the base model and truncates query if it exceeds the limit, or add special embeddings to highlight conversational history for the model, such as HAE (Qu et al., 2019a) embeds history answer embeddings with each context token if it is present in any of the history turns (detailed in section 6.2). This approach does not effectively use the model to capture interactions between every dialog-turn and context.', '2. Use separate model for each conversational turn to capture one-to-one interaction between history and context, and merge the per-turn contextualized embeddings into aggregated history-aware embeddings. Two models follow this trend. Ohsugi et al. (2019) uses BERT models to capture contextual interaction for every question (history and current) and answer (2N+1 sequences for N turns) and concatenates all sequences together. Finally, it runs Bi-GRU (Cho et al., 2014) over the aggregated sequence to capture inter-turn interactions before sending for prediction. On the other hand, HAM (Qu et al., 2019b) ignores the history questions and uses the current question as a query with positional History Answer Embeddings (section 6.2), thus generating one output sequence per conversation turn. Fig. 3b illustrates HAM encoder. The final sequence is generated using token-level soft-attention based aggregation across all per-turn contextualized sequences.']","Large-scale pre-trained LMs such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018) and RoBERTa , have become the current state-of-the-art approaches for contextual reasoning in CMC models, with leaderboards of both datasets stacked with these models or their variants. The approach is based on the fine-tune BERT-based MRC modeling outlined by Devlin et al. (2019), in which question and context are packed together (with marker embeddings to distinguish) in an input sequence to BERT that outputs contextualized question-aware embeddings for each input token.

Using pre-trained models for reasoning is advantageous in two aspects: Firstly, it simplifies the architecture by fusing encoding and reasoning modules into a single module. Secondly, it provides a ready-to-tune architecture that abstracts out complex contextual interactions between query and context while providing sufficient flexibility to control interactivity via augmentation of input embeddings i.e. concatenation of special embeddings to input tokens that signal the model to incorporate a desirable characteristic in contextualization.

(a) HAM uses a dynamic attention-based history selection policy. Contextualized representations are generated by the model's encoder (BERT+PosHAE) for every history turn at word and sequence levels. Sequence-level embeddings are used to compute attention weights via scaled-dot product, and aggregate representations are generated by a weighted combination of embeddings of each turn in the proportion of their attention weights. Thus, attention weights help in determining the degree of selection (relevance) of each history turn.

(b) HAM's BERT based Encoder (Reasoning Architecture) for every conversation turn. The encoder is provided with input sequence consisting of query tokens (yellow) and context tokens (green) separated by [SEP]. It outputs contextualized representations Ti corresponding to aligned question/passage tokens. The Token embeddings are augmented with segment embeddings(to differentiate query and context), positional embeddings (for distinct position in the sequence), and Positional HAE embeddings (for encoding history answer and relative conversational turn). Figure 3: Illustration of (a) history selection module and (b) encoder/reasoning module of History Attention Mechanism (HAM) model (Qu et al., 2019b).

However, incorporating history into these models is a key challenge in this approach as most of the transformer models such as BERT only accept 2 segments ids in the input sequence. Based on recent research in CMC, two main trends in solving the history integration issue are discussed below:

1. Modify the input embeddings for a single-turn MRC model to incorporate history. This is done by either appending the entire conversation to the question, such as Ju et al. (2019) which uses RoBERTa ) as the base model and truncates query if it exceeds the limit, or add special embeddings to highlight conversational history for the model, such as HAE (Qu et al., 2019a) embeds history answer embeddings with each context token if it is present in any of the history turns (detailed in section 6.2). This approach does not effectively use the model to capture interactions between every dialog-turn and context.

2. Use separate model for each conversational turn to capture one-to-one interaction between history and context, and merge the per-turn contextualized embeddings into aggregated history-aware embeddings. Two models follow this trend. Ohsugi et al. (2019) uses BERT models to capture contextual interaction for every question (history and current) and answer (2N+1 sequences for N turns) and concatenates all sequences together. Finally, it runs Bi-GRU (Cho et al., 2014) over the aggregated sequence to capture inter-turn interactions before sending for prediction. On the other hand, HAM (Qu et al., 2019b) ignores the history questions and uses the current question as a query with positional History Answer Embeddings (section 6.2), thus generating one output sequence per conversation turn. Fig. 3b illustrates HAM encoder. The final sequence is generated using token-level soft-attention based aggregation across all per-turn contextualized sequences.","(p15.0) Large-scale pre-trained LMs such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018) and RoBERTa , have become the current state-of-the-art approaches for contextual reasoning in CMC models, with leaderboards of both datasets stacked with these models or their variants. The approach is based on the fine-tune BERT-based MRC modeling outlined by Devlin et al. (2019), in which question and context are packed together (with marker embeddings to distinguish) in an input sequence to BERT that outputs contextualized question-aware embeddings for each input token.

(p15.1) Using pre-trained models for reasoning is advantageous in two aspects: Firstly, it simplifies the architecture by fusing encoding and reasoning modules into a single module. Secondly, it provides a ready-to-tune architecture that abstracts out complex contextual interactions between query and context while providing sufficient flexibility to control interactivity via augmentation of input embeddings i.e. concatenation of special embeddings to input tokens that signal the model to incorporate a desirable characteristic in contextualization.

(p15.2) (a) HAM uses a dynamic attention-based history selection policy. Contextualized representations are generated by the model's encoder (BERT+PosHAE) for every history turn at word and sequence levels. Sequence-level embeddings are used to compute attention weights via scaled-dot product, and aggregate representations are generated by a weighted combination of embeddings of each turn in the proportion of their attention weights. Thus, attention weights help in determining the degree of selection (relevance) of each history turn.

(p15.3) (b) HAM's BERT based Encoder (Reasoning Architecture) for every conversation turn. The encoder is provided with input sequence consisting of query tokens (yellow) and context tokens (green) separated by [SEP]. It outputs contextualized representations Ti corresponding to aligned question/passage tokens. The Token embeddings are augmented with segment embeddings(to differentiate query and context), positional embeddings (for distinct position in the sequence), and Positional HAE embeddings (for encoding history answer and relative conversational turn). Figure 3: Illustration of (a) history selection module and (b) encoder/reasoning module of History Attention Mechanism (HAM) model (Qu et al., 2019b).

(p15.4) However, incorporating history into these models is a key challenge in this approach as most of the transformer models such as BERT only accept 2 segments ids in the input sequence. Based on recent research in CMC, two main trends in solving the history integration issue are discussed below:

(p15.5) 1. Modify the input embeddings for a single-turn MRC model to incorporate history. This is done by either appending the entire conversation to the question, such as Ju et al. (2019) which uses RoBERTa ) as the base model and truncates query if it exceeds the limit, or add special embeddings to highlight conversational history for the model, such as HAE (Qu et al., 2019a) embeds history answer embeddings with each context token if it is present in any of the history turns (detailed in section 6.2). This approach does not effectively use the model to capture interactions between every dialog-turn and context.

(p15.6) 2. Use separate model for each conversational turn to capture one-to-one interaction between history and context, and merge the per-turn contextualized embeddings into aggregated history-aware embeddings. Two models follow this trend. Ohsugi et al. (2019) uses BERT models to capture contextual interaction for every question (history and current) and answer (2N+1 sequences for N turns) and concatenates all sequences together. Finally, it runs Bi-GRU (Cho et al., 2014) over the aggregated sequence to capture inter-turn interactions before sending for prediction. On the other hand, HAM (Qu et al., 2019b) ignores the history questions and uses the current question as a query with positional History Answer Embeddings (section 6.2), thus generating one output sequence per conversation turn. Fig. 3b illustrates HAM encoder. The final sequence is generated using token-level soft-attention based aggregation across all per-turn contextualized sequences.","[['b7', 'b34'], [], [], [None, 'b32'], [], ['b31', 'b18'], ['b25', 'b5', 'b32']]","[['b7', 'b34'], [], [], [None, 'b32'], [], ['b31', 'b18'], ['b25', 'b5', 'b32']]",9,"1. Large-scale pre-trained LMs such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018) and RoBERTa , have become the current state-of-the-art approaches for contextual reasoning in CMC models, with leaderboards of both datasets stacked with these models or their variants.
2. The approach is based on the fine-tune BERT-based MRC modeling outlined by Devlin et al. (2019), in which question and context are packed together (with marker embeddings to distinguish) in an input sequence to BERT that outputs contextualized question-aware embeddings for each input token.
3. Using pre-trained models for reasoning is advantageous in two aspects: Firstly, it simplifies the architecture by fusing encoding and reasoning modules into a single module.
4. Secondly, it provides a ready-to-tune architecture that abstracts out complex contextual interactions between query and context while providing sufficient flexibility to control interactivity via augmentation of input embeddings i.e. concatenation of special embeddings to input tokens that signal the model to incorporate a desirable characteristic in contextualization.
5. (a) HAM uses a dynamic attention-based history selection policy.
6. Contextualized representations are generated by the model's encoder (BERT+PosHAE) for every history turn at word and sequence levels.
7. Sequence-level embeddings are used to compute attention weights via scaled-dot product, and aggregate representations are generated by a weighted combination of embeddings of each turn in the proportion of their attention weights.
8. Thus, attention weights help in determining the degree of selection (relevance) of each history turn.
9. (b) HAM's BERT based Encoder (Reasoning Architecture) for every conversation turn.
10. The encoder is provided with input sequence consisting of query tokens (yellow) and context tokens (green) separated by [SEP].
11. It outputs contextualized representations Ti corresponding to aligned question/passage tokens.
12. The Token embeddings are augmented with segment embeddings(to differentiate query and context), positional embeddings (for distinct position in the sequence), and Positional HAE embeddings (for encoding history answer and relative conversational turn).
13. Figure 3: Illustration of (a) history selection module and (b) encoder/reasoning module of History Attention Mechanism (HAM) model (Qu et al., 2019b).
14. However, incorporating history into these models is a key challenge in this approach as most of the transformer models such as BERT only accept 2 segments ids in the input sequence.
15. Based on recent research in CMC, two main trends in solving the history integration issue are discussed below:1.
16. Modify the input embeddings for a single-turn MRC model to incorporate history.
17. This is done by either appending the entire conversation to the question, such as Ju et al. (2019) which uses RoBERTa ) as the base model and truncates query if it exceeds the limit, or add special embeddings to highlight conversational history for the model, such as HAE (Qu et al., 2019a) embeds history answer embeddings with each context token if it is present in any of the history turns (detailed in section 6.2).
18. This approach does not effectively use the model to capture interactions between every dialog-turn and context.
19. 2. Use separate model for each conversational turn to capture one-to-one interaction between history and context, and merge the per-turn contextualized embeddings into aggregated history-aware embeddings.
20. Two models follow this trend. Ohsugi et al. (2019) uses BERT models to capture contextual interaction for every question (history and current) and answer (2N+1 sequences for N turns) and concatenates all sequences together.
21. Finally, it runs Bi-GRU (Cho et al., 2014) over the aggregated sequence to capture inter-turn interactions before sending for prediction.
22. On the other hand, HAM (Qu et al., 2019b) ignores the history questions and uses the current question as a query with positional History Answer Embeddings (section 6.2), thus generating one output sequence per conversation turn.
23. Fig. 3b illustrates HAM encoder.
24. The final sequence is generated using token-level soft-attention based aggregation across all per-turn contextualized sequences."
219177284,Conversational Machine Comprehension: a Literature Review,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/daadce35dc3694ae02dae07da7a3285daf3eab32,s16,Trends in Training Methodology,"['p16.0', 'p16.1']","['Due to the multi-output nature of both CoQA and QuAC, multi-task training is quite common amongst CMC models, e.g. HAM (Qu et al., 2019b) uses multi-task learning over QuAC to also predict dialog prediction and continuation acts, while GraphFlow  uses multi-task learning over', 'CoQA to also predict question type. Besides, recently published (Ju et al., 2019) achieved state-ofthe-art results using RoBERTa, by applying multiple training techniques over CoQA. These consist of rationale tagging multi-task learning (predict if the token exists in CoQA evidence), Adversarial Training (Goodfellow et al., 2015), and Knowledge Distillation (Furlanello et al., 2018).']","Due to the multi-output nature of both CoQA and QuAC, multi-task training is quite common amongst CMC models, e.g. HAM (Qu et al., 2019b) uses multi-task learning over QuAC to also predict dialog prediction and continuation acts, while GraphFlow  uses multi-task learning over

CoQA to also predict question type. Besides, recently published (Ju et al., 2019) achieved state-ofthe-art results using RoBERTa, by applying multiple training techniques over CoQA. These consist of rationale tagging multi-task learning (predict if the token exists in CoQA evidence), Adversarial Training (Goodfellow et al., 2015), and Knowledge Distillation (Furlanello et al., 2018).","(p16.0) Due to the multi-output nature of both CoQA and QuAC, multi-task training is quite common amongst CMC models, e.g. HAM (Qu et al., 2019b) uses multi-task learning over QuAC to also predict dialog prediction and continuation acts, while GraphFlow  uses multi-task learning over

(p16.1) CoQA to also predict question type. Besides, recently published (Ju et al., 2019) achieved state-ofthe-art results using RoBERTa, by applying multiple training techniques over CoQA. These consist of rationale tagging multi-task learning (predict if the token exists in CoQA evidence), Adversarial Training (Goodfellow et al., 2015), and Knowledge Distillation (Furlanello et al., 2018).","[['b32'], ['b11', 'b18', 'b9']]","[['b32'], ['b11', 'b18', 'b9']]",4,"1. Due to the multi-output nature of both CoQA and QuAC, multi-task training is quite common amongst CMC models, e.g. HAM (Qu et al., 2019b) uses multi-task learning over QuAC to also predict dialog prediction and continuation acts, while GraphFlow  uses multi-task learning overCoQA to also predict question type.
2. Besides, recently published (Ju et al., 2019) achieved state-ofthe-art results using RoBERTa, by applying multiple training techniques over CoQA.
3. These consist of rationale tagging multi-task learning (predict if the token exists in CoQA evidence), Adversarial Training (Goodfellow et al., 2015), and Knowledge Distillation (Furlanello et al., 2018)."
219177284,Conversational Machine Comprehension: a Literature Review,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/daadce35dc3694ae02dae07da7a3285daf3eab32,s11,Trends in History Modeling,"['p11.0', 'p11.1', 'p11.2', 'p11.3', 'p11.4', 'p11.5']","['How conversational history is integrated or used in the encoding process of contextual input embeddings can be used to classify CMC models. Different trends observed in this respect are described below. Some models may use a combination of these approaches.', '1. Appending selected history questions and/or answers (in raw form or text span indices) to the current question before encoding. QA tokens across turns should be distinguishable or separated when appending. Models DrQA+PGNet (Reddy et al., 2019), SDNet  and RoBERTa + AT + KD (Ju et al., 2019) append all history QA pairs separated by tokens like symbols', 'On the other hand, QuAC baseline model BiDAF++ w/ 2-ctx (Ohsugi et al., 2019) and GraphFlow  append only the history questions to the current question and encode relative dialogturn number within each question embedding to differentiate. Choi et al. (2018) validate that this dialog-turn encoding strategy performs better in practice.', '2. Encoding context tokens with history answer marker embeddings (HAE) before passing on for reasoning. These embeddings indicate if the context token is present in any conversational history answer or not, such as in BiDAF++ w/ 2-ctx (Choi et al., 2018), GraphFlow , BERT+HAE (Qu et al., 2019a) and HAM (Qu et al., 2019b). HAM encodes a dialog-turn encoded variant of HAE called Positional HAE. It maintains a lookup table of history embeddings for every relative position from the current conversation and embeds the corresponding embedding if the token is found in that history answer, e.g. for the current question q k if a token is found in history answer a k−2 then Positional HAE embedding at index 2 is encoded, otherwise embedding at index 0 is encoded. This setting is illustrated in Fig. 3b.', '3. Integrating intermediate representations generated in the reasoning modules of selected history conversation turns to grasp the deep latent semantics of the history, rather than acting on raw inputs. This approach is also called the FLOW based approach. The models that follow this approach are FlowQA (Huang et al., 2018a), FlowDelta (Yeh and Chen, 2019), and GraphFlow . GraphFlow encodes conversational histories into context graphs which are used by the reasoning module for contextual analysis.', 'For contextual encoding, most of the models utilize one of the two types of encoders: (a.) Bidirectional sequential language models such as BiDAF (Seo et al., 2017) or ELMo (Peters et al., 2018) (b.) Deep bidirectional transformer-based models such as BERT (Devlin et al., 2019) or RoBERTa .']","How conversational history is integrated or used in the encoding process of contextual input embeddings can be used to classify CMC models. Different trends observed in this respect are described below. Some models may use a combination of these approaches.

1. Appending selected history questions and/or answers (in raw form or text span indices) to the current question before encoding. QA tokens across turns should be distinguishable or separated when appending. Models DrQA+PGNet (Reddy et al., 2019), SDNet  and RoBERTa + AT + KD (Ju et al., 2019) append all history QA pairs separated by tokens like symbols

On the other hand, QuAC baseline model BiDAF++ w/ 2-ctx (Ohsugi et al., 2019) and GraphFlow  append only the history questions to the current question and encode relative dialogturn number within each question embedding to differentiate. Choi et al. (2018) validate that this dialog-turn encoding strategy performs better in practice.

2. Encoding context tokens with history answer marker embeddings (HAE) before passing on for reasoning. These embeddings indicate if the context token is present in any conversational history answer or not, such as in BiDAF++ w/ 2-ctx (Choi et al., 2018), GraphFlow , BERT+HAE (Qu et al., 2019a) and HAM (Qu et al., 2019b). HAM encodes a dialog-turn encoded variant of HAE called Positional HAE. It maintains a lookup table of history embeddings for every relative position from the current conversation and embeds the corresponding embedding if the token is found in that history answer, e.g. for the current question q k if a token is found in history answer a k−2 then Positional HAE embedding at index 2 is encoded, otherwise embedding at index 0 is encoded. This setting is illustrated in Fig. 3b.

3. Integrating intermediate representations generated in the reasoning modules of selected history conversation turns to grasp the deep latent semantics of the history, rather than acting on raw inputs. This approach is also called the FLOW based approach. The models that follow this approach are FlowQA (Huang et al., 2018a), FlowDelta (Yeh and Chen, 2019), and GraphFlow . GraphFlow encodes conversational histories into context graphs which are used by the reasoning module for contextual analysis.

For contextual encoding, most of the models utilize one of the two types of encoders: (a.) Bidirectional sequential language models such as BiDAF (Seo et al., 2017) or ELMo (Peters et al., 2018) (b.) Deep bidirectional transformer-based models such as BERT (Devlin et al., 2019) or RoBERTa .","(p11.0) How conversational history is integrated or used in the encoding process of contextual input embeddings can be used to classify CMC models. Different trends observed in this respect are described below. Some models may use a combination of these approaches.

(p11.1) 1. Appending selected history questions and/or answers (in raw form or text span indices) to the current question before encoding. QA tokens across turns should be distinguishable or separated when appending. Models DrQA+PGNet (Reddy et al., 2019), SDNet  and RoBERTa + AT + KD (Ju et al., 2019) append all history QA pairs separated by tokens like symbols

(p11.2) On the other hand, QuAC baseline model BiDAF++ w/ 2-ctx (Ohsugi et al., 2019) and GraphFlow  append only the history questions to the current question and encode relative dialogturn number within each question embedding to differentiate. Choi et al. (2018) validate that this dialog-turn encoding strategy performs better in practice.

(p11.3) 2. Encoding context tokens with history answer marker embeddings (HAE) before passing on for reasoning. These embeddings indicate if the context token is present in any conversational history answer or not, such as in BiDAF++ w/ 2-ctx (Choi et al., 2018), GraphFlow , BERT+HAE (Qu et al., 2019a) and HAM (Qu et al., 2019b). HAM encodes a dialog-turn encoded variant of HAE called Positional HAE. It maintains a lookup table of history embeddings for every relative position from the current conversation and embeds the corresponding embedding if the token is found in that history answer, e.g. for the current question q k if a token is found in history answer a k−2 then Positional HAE embedding at index 2 is encoded, otherwise embedding at index 0 is encoded. This setting is illustrated in Fig. 3b.

(p11.4) 3. Integrating intermediate representations generated in the reasoning modules of selected history conversation turns to grasp the deep latent semantics of the history, rather than acting on raw inputs. This approach is also called the FLOW based approach. The models that follow this approach are FlowQA (Huang et al., 2018a), FlowDelta (Yeh and Chen, 2019), and GraphFlow . GraphFlow encodes conversational histories into context graphs which are used by the reasoning module for contextual analysis.

(p11.5) For contextual encoding, most of the models utilize one of the two types of encoders: (a.) Bidirectional sequential language models such as BiDAF (Seo et al., 2017) or ELMo (Peters et al., 2018) (b.) Deep bidirectional transformer-based models such as BERT (Devlin et al., 2019) or RoBERTa .","[[], ['b18', 'b36'], ['b6', 'b25'], ['b31', 'b6', 'b32'], ['b43', 'b15'], ['b29', 'b7', 'b38']]","[[], ['b18', 'b36'], ['b6', 'b25'], ['b31', 'b6', 'b32'], ['b43', 'b15'], ['b29', 'b7', 'b38']]",12,"1. How conversational history is integrated or used in the encoding process of contextual input embeddings can be used to classify CMC models.
2. Different trends observed in this respect are described below.
3. Some models may use a combination of these approaches.
4. 1. Appending selected history questions and/or answers (in raw form or text span indices) to the current question before encoding.
5. QA tokens across turns should be distinguishable or separated when appending.
6. Models DrQA+PGNet (Reddy et al., 2019), SDNet  and RoBERTa + AT + KD (Ju et al., 2019) append all history QA pairs separated by tokens like symbolsOn the other hand, QuAC baseline model BiDAF++ w/ 2-ctx (Ohsugi et al., 2019) and GraphFlow  append only the history questions to the current question and encode relative dialogturn number within each question embedding to differentiate.
7. Choi et al. (2018) validate that this dialog-turn encoding strategy performs better in practice.
8. 2. Encoding context tokens with history answer marker embeddings (HAE) before passing on for reasoning.
9. These embeddings indicate if the context token is present in any conversational history answer or not, such as in BiDAF++ w/ 2-ctx (Choi et al., 2018), GraphFlow , BERT+HAE (Qu et al., 2019a) and HAM (Qu et al., 2019b).
10. HAM encodes a dialog-turn encoded variant of HAE called Positional HAE.
11. It maintains a lookup table of history embeddings for every relative position from the current conversation and embeds the corresponding embedding if the token is found in that history answer, e.g. for the current question q k if a token is found in history answer a k−2 then Positional HAE embedding at index 2 is encoded, otherwise embedding at index 0 is encoded.
12. This setting is illustrated in Fig. 3b.
13. 3. Integrating intermediate representations generated in the reasoning modules of selected history conversation turns to grasp the deep latent semantics of the history, rather than acting on raw inputs.
14. This approach is also called the FLOW based approach.
15. The models that follow this approach are FlowQA (Huang et al., 2018a), FlowDelta (Yeh and Chen, 2019), and GraphFlow .
16. GraphFlow encodes conversational histories into context graphs which are used by the reasoning module for contextual analysis.
17. For contextual encoding, most of the models utilize one of the two types of encoders: (a.) Bidirectional sequential language models such as BiDAF (Seo et al., 2017) or ELMo (Peters et al., 2018) (b.) Deep bidirectional transformer-based models such as BERT (Devlin et al., 2019) or RoBERTa ."
222133962,A Survey of Unsupervised Dependency Parsing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/16160fbefc830117a8395a4b665c39feae834e48,s3,Related Areas,"['p3.0', 'p3.1', 'p3.2', 'p3.3', 'p3.4']","['Supervised Dependency Parsing Supervised dependency parsing aims to train a dependency parser from training sentences that are manually annotated with their dependency parse trees. Generally, supervised dependency parsing approaches can be divided into graph-based approaches and transition-based approaches. A graph-based dependency parser searches for the best spanning tree of the graph that is formed by connecting all pairs of words in the input sentence. In the simplest form, a graph-based parser makes the first-order assumption that the score of a dependency tree is the summation of scores of its edges (McDonald et al., 2005). A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets.', 'Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015;McDonald et al., 2011;Ma and Xia, 2014;Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either.', 'Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce not only edges but also nodes of a tree. Consequently, there have been far more papers in unsupervised dependency parsing than in unsupervised constituency parsing over the past decade. More recently, however, there is a surge in interest in unsupervised constituency parsing and several novel approaches were proposed in the past two years . While we focus on unsupervised dependency parsing in this paper, most of our discussions on the classification of approaches and recent trends apply to unsupervised constituency parsing as well.', 'Latent Tree Models with Downstream Tasks Latent tree models treat the parse tree as a latent variable that is used in downstream tasks such as sentiment classification. While no treebank is used in training, these models rely on the performance of the downstream tasks to guide the learning of the latent parse trees. To enable end-to-end learning, the REINFORCE algorithm and the Gumbel-softmax trick (Jang et al., 2017) can be utilized (Yogatama et al., 2016;Choi et al., 2018). There also exists previous work on latent dependency tree models that utilizes structured attention mechanisms (Kim et al., 2017) for applications. Latent tree models differ from unsupervised parsing in that they utilize training signals from downstream tasks and that they aim to improve performance of downstream tasks instead of syntactic parsing.', '3 General Approaches 3.1 Generative Approaches']","Supervised Dependency Parsing Supervised dependency parsing aims to train a dependency parser from training sentences that are manually annotated with their dependency parse trees. Generally, supervised dependency parsing approaches can be divided into graph-based approaches and transition-based approaches. A graph-based dependency parser searches for the best spanning tree of the graph that is formed by connecting all pairs of words in the input sentence. In the simplest form, a graph-based parser makes the first-order assumption that the score of a dependency tree is the summation of scores of its edges (McDonald et al., 2005). A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets.

Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015;McDonald et al., 2011;Ma and Xia, 2014;Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either.

Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce not only edges but also nodes of a tree. Consequently, there have been far more papers in unsupervised dependency parsing than in unsupervised constituency parsing over the past decade. More recently, however, there is a surge in interest in unsupervised constituency parsing and several novel approaches were proposed in the past two years . While we focus on unsupervised dependency parsing in this paper, most of our discussions on the classification of approaches and recent trends apply to unsupervised constituency parsing as well.

Latent Tree Models with Downstream Tasks Latent tree models treat the parse tree as a latent variable that is used in downstream tasks such as sentiment classification. While no treebank is used in training, these models rely on the performance of the downstream tasks to guide the learning of the latent parse trees. To enable end-to-end learning, the REINFORCE algorithm and the Gumbel-softmax trick (Jang et al., 2017) can be utilized (Yogatama et al., 2016;Choi et al., 2018). There also exists previous work on latent dependency tree models that utilizes structured attention mechanisms (Kim et al., 2017) for applications. Latent tree models differ from unsupervised parsing in that they utilize training signals from downstream tasks and that they aim to improve performance of downstream tasks instead of syntactic parsing.

3 General Approaches 3.1 Generative Approaches","(p3.0) Supervised Dependency Parsing Supervised dependency parsing aims to train a dependency parser from training sentences that are manually annotated with their dependency parse trees. Generally, supervised dependency parsing approaches can be divided into graph-based approaches and transition-based approaches. A graph-based dependency parser searches for the best spanning tree of the graph that is formed by connecting all pairs of words in the input sentence. In the simplest form, a graph-based parser makes the first-order assumption that the score of a dependency tree is the summation of scores of its edges (McDonald et al., 2005). A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets.

(p3.1) Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015;McDonald et al., 2011;Ma and Xia, 2014;Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either.

(p3.2) Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce not only edges but also nodes of a tree. Consequently, there have been far more papers in unsupervised dependency parsing than in unsupervised constituency parsing over the past decade. More recently, however, there is a surge in interest in unsupervised constituency parsing and several novel approaches were proposed in the past two years . While we focus on unsupervised dependency parsing in this paper, most of our discussions on the classification of approaches and recent trends apply to unsupervised constituency parsing as well.

(p3.3) Latent Tree Models with Downstream Tasks Latent tree models treat the parse tree as a latent variable that is used in downstream tasks such as sentiment classification. While no treebank is used in training, these models rely on the performance of the downstream tasks to guide the learning of the latent parse trees. To enable end-to-end learning, the REINFORCE algorithm and the Gumbel-softmax trick (Jang et al., 2017) can be utilized (Yogatama et al., 2016;Choi et al., 2018). There also exists previous work on latent dependency tree models that utilizes structured attention mechanisms (Kim et al., 2017) for applications. Latent tree models differ from unsupervised parsing in that they utilize training signals from downstream tasks and that they aim to improve performance of downstream tasks instead of syntactic parsing.

(p3.4) 3 General Approaches 3.1 Generative Approaches","[['b39'], ['b13', 'b40', 'b36', 'b62'], [], ['b61', 'b7', 'b25', 'b29'], []]","[['b39'], ['b13', 'b40', 'b36', 'b62'], [], ['b61', 'b7', 'b25', 'b29'], []]",9,"1. Supervised Dependency Parsing Supervised dependency parsing aims to train a dependency parser from training sentences that are manually annotated with their dependency parse trees.
2. Generally, supervised dependency parsing approaches can be divided into graph-based approaches and transition-based approaches.
3. A graph-based dependency parser searches for the best spanning tree of the graph that is formed by connecting all pairs of words in the input sentence.
4. In the simplest form, a graph-based parser makes the first-order assumption that the score of a dependency tree is the summation of scores of its edges (McDonald et al., 2005).
5. A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right.
6. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets.
7. Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015;McDonald et al., 2011;Ma and Xia, 2014;Duong et al., 2015).
8. This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language.
9. However, unsupervised parsing is more challenging because it does not have access to any source treebank either.
10. Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases.
11. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce not only edges but also nodes of a tree.
12. Consequently, there have been far more papers in unsupervised dependency parsing than in unsupervised constituency parsing over the past decade.
13. More recently, however, there is a surge in interest in unsupervised constituency parsing and several novel approaches were proposed in the past two years .
14. While we focus on unsupervised dependency parsing in this paper, most of our discussions on the classification of approaches and recent trends apply to unsupervised constituency parsing as well.
15. Latent Tree Models with Downstream Tasks Latent tree models treat the parse tree as a latent variable that is used in downstream tasks such as sentiment classification.
16. While no treebank is used in training, these models rely on the performance of the downstream tasks to guide the learning of the latent parse trees.
17. To enable end-to-end learning, the REINFORCE algorithm and the Gumbel-softmax trick (Jang et al., 2017) can be utilized (Yogatama et al., 2016;Choi et al., 2018).
18. There also exists previous work on latent dependency tree models that utilizes structured attention mechanisms (Kim et al., 2017) for applications.
19. Latent tree models differ from unsupervised parsing in that they utilize training signals from downstream tasks and that they aim to improve performance of downstream tasks instead of syntactic parsing.
20. 3 General Approaches 3.1 Generative Approaches"
222133962,A Survey of Unsupervised Dependency Parsing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/16160fbefc830117a8395a4b665c39feae834e48,s4,Models,"['p4.0', 'p4.1', 'p4.2']","['A generative approach models the joint probability of the sentence and the corresponding parse tree. Traditional generative models are mostly based on probabilistic grammars. To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions. The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else. Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference. However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.', 'Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree. Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner. The generation of a child token is conditioned on the head token and the dependency direction. In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden  propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).', 'Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.  use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV.']","A generative approach models the joint probability of the sentence and the corresponding parse tree. Traditional generative models are mostly based on probabilistic grammars. To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions. The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else. Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference. However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.

Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree. Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner. The generation of a child token is conditioned on the head token and the dependency direction. In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden  propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).

Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.  use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV.","(p4.0) A generative approach models the joint probability of the sentence and the corresponding parse tree. Traditional generative models are mostly based on probabilistic grammars. To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions. The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else. Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference. However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.

(p4.1) Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree. Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner. The generation of a child token is conditioned on the head token and the dependency direction. In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden  propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).

(p4.2) Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.  use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV.","[[], ['b3', 'b30', 'b60', 'b4', 'b6', 'b44', 'b2'], ['b26']]","[[], ['b3', 'b30', 'b60', 'b4', 'b6', 'b44', 'b2'], ['b26']]",8,"1. A generative approach models the joint probability of the sentence and the corresponding parse tree.
2. Traditional generative models are mostly based on probabilistic grammars.
3. To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions.
4. The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else.
5. Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference.
6. However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.
7. Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree.
8. Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner.
9. The generation of a child token is conditioned on the head token and the dependency direction.
10. In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously.
11. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token.
12. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token.
13. Headden  propose to also introduce the valence into the condition of decision sampling.
14. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.
15. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information.
16. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).
17. Similar tokens may have similar syntactic behaviors in a grammar.
18. For example, all the verbs are very likely to generate a noun to the left as the subject.
19. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.
20. use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features.
21. Both approaches are based on DMV."
222133962,A Survey of Unsupervised Dependency Parsing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/16160fbefc830117a8395a4b665c39feae834e48,s8,Intermediate Representation Encoder Decoder,"['p8.0', 'p8.1', 'p8.2', 'p8.3']","['Autoencoder CRFAE (Cai et al., 2017) Z P (z|x) P (x|z) D-NDMV (Han et al., 2019a) Deterministic Variant S P (s|x) P (z,x|s)', 'Variational Autoencoder (Li et al., 2019) Z P (z|x) P (z, x) D-NDMV (Han et al., 2019a) Variational Variant S P (s|x) P (z, x|s) (Corro and Titov, 2018) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing. Z: dependency tree. S: continuous sentence representation.x is a copy of x representing the reconstructed sentence. z is the dependency tree. s is the continuous representation of sentence x.', 'In addition to the EM algorithm, the learning objective can also be optimized with gradient descent. Yang et al. (2020) recently observe that gradient descent can sometimes significantly outperform EM when learning neural DMV.', 'Better learning results can also be achieved by manipulating the training data. Spitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the shortest sentences and then progresses to increasingly longer sentences. Tu and Honavar (2011) provide a theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing. Spitkovsky et al. (2013) propose to treat different learning algorithms and configurations as modules and connect them to form a network. Some approaches discussed above, such as Lateen EM and curriculum learning, can be seen as special cases of this approach.']","Autoencoder CRFAE (Cai et al., 2017) Z P (z|x) P (x|z) D-NDMV (Han et al., 2019a) Deterministic Variant S P (s|x) P (z,x|s)

Variational Autoencoder (Li et al., 2019) Z P (z|x) P (z, x) D-NDMV (Han et al., 2019a) Variational Variant S P (s|x) P (z, x|s) (Corro and Titov, 2018) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing. Z: dependency tree. S: continuous sentence representation.x is a copy of x representing the reconstructed sentence. z is the dependency tree. s is the continuous representation of sentence x.

In addition to the EM algorithm, the learning objective can also be optimized with gradient descent. Yang et al. (2020) recently observe that gradient descent can sometimes significantly outperform EM when learning neural DMV.

Better learning results can also be achieved by manipulating the training data. Spitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the shortest sentences and then progresses to increasingly longer sentences. Tu and Honavar (2011) provide a theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing. Spitkovsky et al. (2013) propose to treat different learning algorithms and configurations as modules and connect them to form a network. Some approaches discussed above, such as Lateen EM and curriculum learning, can be seen as special cases of this approach.","(p8.0) Autoencoder CRFAE (Cai et al., 2017) Z P (z|x) P (x|z) D-NDMV (Han et al., 2019a) Deterministic Variant S P (s|x) P (z,x|s)

(p8.1) Variational Autoencoder (Li et al., 2019) Z P (z|x) P (z, x) D-NDMV (Han et al., 2019a) Variational Variant S P (s|x) P (z, x|s) (Corro and Titov, 2018) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing. Z: dependency tree. S: continuous sentence representation.x is a copy of x representing the reconstructed sentence. z is the dependency tree. s is the continuous representation of sentence x.

(p8.2) In addition to the EM algorithm, the learning objective can also be optimized with gradient descent. Yang et al. (2020) recently observe that gradient descent can sometimes significantly outperform EM when learning neural DMV.

(p8.3) Better learning results can also be achieved by manipulating the training data. Spitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the shortest sentences and then progresses to increasingly longer sentences. Tu and Honavar (2011) provide a theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing. Spitkovsky et al. (2013) propose to treat different learning algorithms and configurations as modules and connect them to form a network. Some approaches discussed above, such as Lateen EM and curriculum learning, can be seen as special cases of this approach.","[['b20', 'b5'], ['b20', 'b10', 'b33'], ['b60'], ['b57', 'b58']]","[['b20', 'b5'], ['b20', 'b10', 'b33'], ['b60'], ['b57', 'b58']]",8,"1. Autoencoder CRFAE (Cai et al., 2017) Z P (z|x) P (x|z) D-NDMV (Han et al., 2019a)
2. Deterministic Variant S P (s|x) P (z,x|s)Variational Autoencoder (Li et al., 2019) Z P (z|x) P (z, x)
3. D-NDMV (Han et al., 2019a) Variational Variant S P (s|x) P (z, x|s) (Corro and Titov, 2018) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing.
4. Z: dependency tree. S: continuous sentence representation.x is a copy of x representing the reconstructed sentence.
5. z is the dependency tree. s is the continuous representation of sentence x.
6. In addition to the EM algorithm, the learning objective can also be optimized with gradient descent.
7. Yang et al. (2020) recently observe that gradient descent can sometimes significantly outperform EM when learning neural DMV.
8. Better learning results can also be achieved by manipulating the training data.
9. Spitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the shortest sentences and then progresses to increasingly longer sentences.
10. Tu and Honavar (2011) provide a theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing.
11. Spitkovsky et al. (2013) propose to treat different learning algorithms and configurations as modules and connect them to form a network.
12. Some approaches discussed above, such as Lateen EM and curriculum learning, can be seen as special cases of this approach."
222133962,A Survey of Unsupervised Dependency Parsing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/16160fbefc830117a8395a4b665c39feae834e48,s12,Variational Autoencoder-Based Approaches,"['p12.0', 'p12.1', 'p12.2', 'p12.3', 'p12.4']","['As mentioned in Section 3.1, the training objective of a generative model is typically the probability of the training sentence and the dependency tree is marginalized as a hidden variable. However, the marginalized probability cannot usually be calculated accurately for more complex models that do not make strict independence assumption. Instead, a variational autoencoder maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability. Since the intermediate representation follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema.', 'Three unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1). There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.', 'Recurrent Neural Network Grammars (RNNG) ) is a transition-based constituent parser, with a discriminative and a generative variant. Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing. Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence. The probability of each operation is calculated by a neural network. Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNNG as the encoder and decoder of a variational autoencoder respectively. However, because RNNG has a strong expressive power, it is prone to overfitting in the unsupervised setting. Li et al. (2019) propose to use posterior regularization to introduce linguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent.', 'The model proposed by Corro and Titov (2018) is also based on a variational autoencoder. It is designed for semi-supervised dependency parsing, but in principle it can also be applied for unsupervised dependency parsing. The encoder of this model is a conditional random field model while the decoder generates a sentence based on a graph convolutional neural network whose structure is specified by the dependency tree. Since the variational autoencoder needs Monte Carlo sampling to approximate the gradient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel random perturbation. Jang et al. (2017) use differentiable dynamic programming to design an efficient approximate sampling algorithm.', 'The variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also specifies a Gaussian prior over the intermediate continuous vector.']","As mentioned in Section 3.1, the training objective of a generative model is typically the probability of the training sentence and the dependency tree is marginalized as a hidden variable. However, the marginalized probability cannot usually be calculated accurately for more complex models that do not make strict independence assumption. Instead, a variational autoencoder maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability. Since the intermediate representation follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema.

Three unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1). There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.

Recurrent Neural Network Grammars (RNNG) ) is a transition-based constituent parser, with a discriminative and a generative variant. Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing. Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence. The probability of each operation is calculated by a neural network. Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNNG as the encoder and decoder of a variational autoencoder respectively. However, because RNNG has a strong expressive power, it is prone to overfitting in the unsupervised setting. Li et al. (2019) propose to use posterior regularization to introduce linguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent.

The model proposed by Corro and Titov (2018) is also based on a variational autoencoder. It is designed for semi-supervised dependency parsing, but in principle it can also be applied for unsupervised dependency parsing. The encoder of this model is a conditional random field model while the decoder generates a sentence based on a graph convolutional neural network whose structure is specified by the dependency tree. Since the variational autoencoder needs Monte Carlo sampling to approximate the gradient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel random perturbation. Jang et al. (2017) use differentiable dynamic programming to design an efficient approximate sampling algorithm.

The variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also specifies a Gaussian prior over the intermediate continuous vector.","(p12.0) As mentioned in Section 3.1, the training objective of a generative model is typically the probability of the training sentence and the dependency tree is marginalized as a hidden variable. However, the marginalized probability cannot usually be calculated accurately for more complex models that do not make strict independence assumption. Instead, a variational autoencoder maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability. Since the intermediate representation follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema.

(p12.1) Three unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1). There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.

(p12.2) Recurrent Neural Network Grammars (RNNG) ) is a transition-based constituent parser, with a discriminative and a generative variant. Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing. Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence. The probability of each operation is calculated by a neural network. Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNNG as the encoder and decoder of a variational autoencoder respectively. However, because RNNG has a strong expressive power, it is prone to overfitting in the unsupervised setting. Li et al. (2019) propose to use posterior regularization to introduce linguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent.

(p12.3) The model proposed by Corro and Titov (2018) is also based on a variational autoencoder. It is designed for semi-supervised dependency parsing, but in principle it can also be applied for unsupervised dependency parsing. The encoder of this model is a conditional random field model while the decoder generates a sentence based on a graph convolutional neural network whose structure is specified by the dependency tree. Since the variational autoencoder needs Monte Carlo sampling to approximate the gradient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel random perturbation. Jang et al. (2017) use differentiable dynamic programming to design an efficient approximate sampling algorithm.

(p12.4) The variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also specifies a Gaussian prior over the intermediate continuous vector.","[[], [], ['b33'], ['b10', 'b25'], ['b20']]","[[], [], ['b33'], ['b10', 'b25'], ['b20']]",4,"1. As mentioned in Section 3.1, the training objective of a generative model is typically the probability of the training sentence and the dependency tree is marginalized as a hidden variable.
2. However, the marginalized probability cannot usually be calculated accurately for more complex models that do not make strict independence assumption.
3. Instead, a variational autoencoder maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability.
4. Since the intermediate representation follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema.
5. Three unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1).
6. There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.
7. Recurrent Neural Network Grammars (RNNG) ) is a transition-based constituent parser, with a discriminative and a generative variant.
8. Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing.
9. Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence.
10. The probability of each operation is calculated by a neural network.
11. Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNNG as the encoder and decoder of a variational autoencoder respectively.
12. However, because RNNG has a strong expressive power, it is prone to overfitting in the unsupervised setting.
13. Li et al. (2019) propose to use posterior regularization to introduce linguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent.
14. The model proposed by Corro and Titov (2018) is also based on a variational autoencoder.
15. It is designed for semi-supervised dependency parsing, but in principle it can also be applied for unsupervised dependency parsing.
16. The encoder of this model is a conditional random field model while the decoder generates a sentence based on a graph convolutional neural network whose structure is specified by the dependency tree.
17. Since the variational autoencoder needs Monte Carlo sampling to approximate the gradient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel random perturbation.
18. Jang et al. (2017) use differentiable dynamic programming to design an efficient approximate sampling algorithm.
19. The variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution.
20. It also specifies a Gaussian prior over the intermediate continuous vector."
222133962,A Survey of Unsupervised Dependency Parsing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/16160fbefc830117a8395a4b665c39feae834e48,s17,Neural Parameterization,['p17.0'],"['Traditional generative approaches either directly learn or use manually-designed features to compute dependency rule probabilities. Following the recent rise of deep learning in the field of NLP, Jiang et al. (2016) propose to predict dependency rule probabilities using a neural network that takes as input the vector representations of the rule components such as the head and child tokens. The neural network can automatically learn features that capture correlations between tokens and rules. Han et al. (2019a) extend this generative approach to a discriminative approach by further introducing sentence information into the neural network in order to compute sentence-specific rule probabilities. Compared with generative approaches, it is more natural for discriminative approaches to use neural networks to score dependencies or parsing actions, so recent discriminative approaches all make use of neural networks (Li et al., 2019;Corro and Titov, 2018).']","Traditional generative approaches either directly learn or use manually-designed features to compute dependency rule probabilities. Following the recent rise of deep learning in the field of NLP, Jiang et al. (2016) propose to predict dependency rule probabilities using a neural network that takes as input the vector representations of the rule components such as the head and child tokens. The neural network can automatically learn features that capture correlations between tokens and rules. Han et al. (2019a) extend this generative approach to a discriminative approach by further introducing sentence information into the neural network in order to compute sentence-specific rule probabilities. Compared with generative approaches, it is more natural for discriminative approaches to use neural networks to score dependencies or parsing actions, so recent discriminative approaches all make use of neural networks (Li et al., 2019;Corro and Titov, 2018).","(p17.0) Traditional generative approaches either directly learn or use manually-designed features to compute dependency rule probabilities. Following the recent rise of deep learning in the field of NLP, Jiang et al. (2016) propose to predict dependency rule probabilities using a neural network that takes as input the vector representations of the rule components such as the head and child tokens. The neural network can automatically learn features that capture correlations between tokens and rules. Han et al. (2019a) extend this generative approach to a discriminative approach by further introducing sentence information into the neural network in order to compute sentence-specific rule probabilities. Compared with generative approaches, it is more natural for discriminative approaches to use neural networks to score dependencies or parsing actions, so recent discriminative approaches all make use of neural networks (Li et al., 2019;Corro and Titov, 2018).","[['b10', 'b20', 'b26', 'b33']]","[['b10', 'b20', 'b26', 'b33']]",4,"1. Traditional generative approaches either directly learn or use manually-designed features to compute dependency rule probabilities.
2. Following the recent rise of deep learning in the field of NLP, Jiang et al. (2016) propose to predict dependency rule probabilities using a neural network that takes as input the vector representations of the rule components such as the head and child tokens.
3. The neural network can automatically learn features that capture correlations between tokens and rules.
4. Han et al. (2019a) extend this generative approach to a discriminative approach by further introducing sentence information into the neural network in order to compute sentence-specific rule probabilities.
5. Compared with generative approaches, it is more natural for discriminative approaches to use neural networks to score dependencies or parsing actions, so recent discriminative approaches all make use of neural networks (Li et al., 2019;Corro and Titov, 2018)."
222133962,A Survey of Unsupervised Dependency Parsing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/16160fbefc830117a8395a4b665c39feae834e48,s18,Lexicalization,"['p18.0', 'p18.1']","['In the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a;He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010 use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007, Pate andSpitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. For instance,  use neural networks to predict dependency probabilities that are automatically smoothed.', 'In principle, lexicalized approaches could also benefit from pretrained word embeddings, which capture syntactic and semantic similarities between words. Recently proposed contextual word embeddings  Table 2: Reported directed dependency accuracies on section 23 of the WSJ corpus, evaluated on sentences of length ≤ 10 and all lengths. *: without gold POS tags. †: with more training data in addition to WSJ. (Devlin et al., 2019) are even more informative, capturing contextual information. However, word embeddings have not been widely used in unsupervised dependency parsing. One concern is that word embeddings are too informative and may make unsupervised models more prone to overfitting. One exception is He et al. (2018), who propose to use invertible neural projections to map word embeddings into a latent space that is more amenable to unsupervised parsing.']","In the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a;He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010 use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007, Pate andSpitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. For instance,  use neural networks to predict dependency probabilities that are automatically smoothed.

In principle, lexicalized approaches could also benefit from pretrained word embeddings, which capture syntactic and semantic similarities between words. Recently proposed contextual word embeddings  Table 2: Reported directed dependency accuracies on section 23 of the WSJ corpus, evaluated on sentences of length ≤ 10 and all lengths. *: without gold POS tags. †: with more training data in addition to WSJ. (Devlin et al., 2019) are even more informative, capturing contextual information. However, word embeddings have not been widely used in unsupervised dependency parsing. One concern is that word embeddings are too informative and may make unsupervised models more prone to overfitting. One exception is He et al. (2018), who propose to use invertible neural projections to map word embeddings into a latent space that is more amenable to unsupervised parsing.","(p18.0) In the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a;He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010 use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007, Pate andSpitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. For instance,  use neural networks to predict dependency probabilities that are automatically smoothed.

(p18.1) In principle, lexicalized approaches could also benefit from pretrained word embeddings, which capture syntactic and semantic similarities between words. Recently proposed contextual word embeddings  Table 2: Reported directed dependency accuracies on section 23 of the WSJ corpus, evaluated on sentences of length ≤ 10 and all lengths. *: without gold POS tags. †: with more training data in addition to WSJ. (Devlin et al., 2019) are even more informative, capturing contextual information. However, word embeddings have not been widely used in unsupervised dependency parsing. One concern is that word embeddings are too informative and may make unsupervised models more prone to overfitting. One exception is He et al. (2018), who propose to use invertible neural projections to map word embeddings into a latent space that is more amenable to unsupervised parsing.","[['b53', 'b57', 'b48', 'b4', 'b63', 'b45', 'b22', 'b24'], ['b22', 'b12']]","[['b53', 'b57', 'b48', 'b4', 'b63', 'b45', 'b22', 'b24'], ['b22', 'b12']]",10,"1. In the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences.
2. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a;He et al., 2018).
3. However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers.
4. Headden III et al. (2009), Blunsom and Cohn (2010 use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007, Pate andSpitkovsky et al. (2013) experiment with full lexicalization.
5. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data.
6. To mitigate the negative impact of data scarcity, smoothing techniques can be used.
7. For instance,  use neural networks to predict dependency probabilities that are automatically smoothed.
8. In principle, lexicalized approaches could also benefit from pretrained word embeddings, which capture syntactic and semantic similarities between words.
9. Recently proposed contextual word embeddings  Table 2: Reported directed dependency accuracies on section 23 of the WSJ corpus, evaluated on sentences of length ≤ 10 and all lengths.
10. *: without gold POS tags. †: with more training data in addition to WSJ.
11. (Devlin et al., 2019) are even more informative, capturing contextual information.
12. However, word embeddings have not been widely used in unsupervised dependency parsing.
13. One concern is that word embeddings are too informative and may make unsupervised models more prone to overfitting.
14. One exception is He et al. (2018), who propose to use invertible neural projections to map word embeddings into a latent space that is more amenable to unsupervised parsing."
222133962,A Survey of Unsupervised Dependency Parsing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/16160fbefc830117a8395a4b665c39feae834e48,s22,Utilization of Syntactic Information in Pretrained Language Modeling,['p22.0'],"['Pretrained language modeling (Peters et al., 2018;Devlin et al., 2019;Radford et al., 2019), as a new NLP paradigm, has been utilized in various areas including question answering, machine translation, grammatical error correction, and so on. Pretrained language models leverage a large-scale corpus for pretraining and then small data sets of specific tasks for finetuning, reducing the difficulty of downstream tasks and boosting their performance. Current state-of-the-art approaches on supervised dependency parsing, such as Zhou and Zhao (2019), adopt the new paradigm and benefit from pretrained language modeling. However, pretrained language models have not been widely used in unsupervised dependency parsing. One major concern is that pretrained language models are too informative and may make unsupervised models more prone to overfitting. Besides, massive syntactic and semantic information is encoded in pretrained language models and how to extract the syntactic part from them is a challenging task.']","Pretrained language modeling (Peters et al., 2018;Devlin et al., 2019;Radford et al., 2019), as a new NLP paradigm, has been utilized in various areas including question answering, machine translation, grammatical error correction, and so on. Pretrained language models leverage a large-scale corpus for pretraining and then small data sets of specific tasks for finetuning, reducing the difficulty of downstream tasks and boosting their performance. Current state-of-the-art approaches on supervised dependency parsing, such as Zhou and Zhao (2019), adopt the new paradigm and benefit from pretrained language modeling. However, pretrained language models have not been widely used in unsupervised dependency parsing. One major concern is that pretrained language models are too informative and may make unsupervised models more prone to overfitting. Besides, massive syntactic and semantic information is encoded in pretrained language models and how to extract the syntactic part from them is a challenging task.","(p22.0) Pretrained language modeling (Peters et al., 2018;Devlin et al., 2019;Radford et al., 2019), as a new NLP paradigm, has been utilized in various areas including question answering, machine translation, grammatical error correction, and so on. Pretrained language models leverage a large-scale corpus for pretraining and then small data sets of specific tasks for finetuning, reducing the difficulty of downstream tasks and boosting their performance. Current state-of-the-art approaches on supervised dependency parsing, such as Zhou and Zhao (2019), adopt the new paradigm and benefit from pretrained language modeling. However, pretrained language models have not been widely used in unsupervised dependency parsing. One major concern is that pretrained language models are too informative and may make unsupervised models more prone to overfitting. Besides, massive syntactic and semantic information is encoded in pretrained language models and how to extract the syntactic part from them is a challenging task.","[['b64', 'b12', 'b47', 'b46']]","[['b64', 'b12', 'b47', 'b46']]",4,"1. Pretrained language modeling (Peters et al., 2018;Devlin et al., 2019;Radford et al., 2019), as a new NLP paradigm, has been utilized in various areas including question answering, machine translation, grammatical error correction, and so on.
2. Pretrained language models leverage a large-scale corpus for pretraining and then small data sets of specific tasks for finetuning, reducing the difficulty of downstream tasks and boosting their performance.
3. Current state-of-the-art approaches on supervised dependency parsing, such as Zhou and Zhao (2019), adopt the new paradigm and benefit from pretrained language modeling.
4. However, pretrained language models have not been widely used in unsupervised dependency parsing.
5. One major concern is that pretrained language models are too informative and may make unsupervised models more prone to overfitting.
6. Besides, massive syntactic and semantic information is encoded in pretrained language models and how to extract the syntactic part from them is a challenging task."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s4,How Low is Low-Resource?,"['p4.0', 'p4.1', 'p4.2']","['On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.', '(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.', 'Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.']","On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","(p4.0) On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(p4.1) (2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

(p4.2) Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","[[], ['b0', 'b13'], ['b13', None]]","[[], ['b0', 'b13'], ['b13', None]]",4,"1. On the dimension of task-specific labels, different thresholds are used to define low-resource.
2. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens.
3. Kann et al.(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens.
4. The threshold is also task-dependent and more complex tasks might also increase the resource requirements.
5. For text generation,  frame their work as low-resource with 350k labeled training instances.
6. Similar to the task, the resource requirements can also depend on the language.
7. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.
8. Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability.
9. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches.
10. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s6,Data Augmentation,"['p6.0', 'p6.1', 'p6.2', 'p6.3']","[""New instances can be obtained based on existing ones by modifying the features with transformations that do not change the label. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms (Wei and Zou, 2019), entities of the same type (Raiman and Miller, 2017;Dai and Adel, 2020) or words that share the same morphology (Gulordava et al., 2018;Vania et al., 2019). Such replacements can also be guided by a language model that takes context into consideration (Fadaee et al., 2017;Kobayashi, 2018)."", 'To go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree (Şahin and Steedman, 2018;Vania et al., 2019;Dehouck and Gómez-Rodríguez, 2020), simplification of sentences by removal of sentence parts (Şahin and Steedman, 2018) and inversion of the subject-object relation (Min et al., 2020). For whole sentences, paraphrasing through backtranslation can be used. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011;Hoang et al., 2018). An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Ma et al., 2019). Back-translation has also been leveraged for text classification (Xie et al., 2020;Hegde and Patil, 2020). This setting assumes, however, the availability of a translation system. Instead, a language model can also be used for augmenting text classification datasets (Kumar et al., 2020;Anaby-Tavor et al., 2020). It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label. It then generates additional sentences that fit this label. Ding et al. (2020) extend this idea for token level tasks.', 'Adversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020;Garg and Ramakrishnan, 2020). They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018;Morris et al., 2020). Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score). This approach is often applied on the level of vector representations. For instance, Grundkiewicz et al. (2019) reverse the augmentation setting by applying transformations that flip the (binary) label. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task.', 'Open Issues: While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are taskindependent, it has not found such widespread use in natural language processing. A reason might be that several of the approaches require an indepth understanding of the language. There is not yet a unified framework that allows applying data augmentation across tasks and languages. Recently, Longpre et al. (2020) hypothesised that data augmentation provides the same benefits as pretraining in transformer models. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited.']","New instances can be obtained based on existing ones by modifying the features with transformations that do not change the label. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms (Wei and Zou, 2019), entities of the same type (Raiman and Miller, 2017;Dai and Adel, 2020) or words that share the same morphology (Gulordava et al., 2018;Vania et al., 2019). Such replacements can also be guided by a language model that takes context into consideration (Fadaee et al., 2017;Kobayashi, 2018).

To go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree (Şahin and Steedman, 2018;Vania et al., 2019;Dehouck and Gómez-Rodríguez, 2020), simplification of sentences by removal of sentence parts (Şahin and Steedman, 2018) and inversion of the subject-object relation (Min et al., 2020). For whole sentences, paraphrasing through backtranslation can be used. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011;Hoang et al., 2018). An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Ma et al., 2019). Back-translation has also been leveraged for text classification (Xie et al., 2020;Hegde and Patil, 2020). This setting assumes, however, the availability of a translation system. Instead, a language model can also be used for augmenting text classification datasets (Kumar et al., 2020;Anaby-Tavor et al., 2020). It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label. It then generates additional sentences that fit this label. Ding et al. (2020) extend this idea for token level tasks.

Adversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020;Garg and Ramakrishnan, 2020). They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018;Morris et al., 2020). Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score). This approach is often applied on the level of vector representations. For instance, Grundkiewicz et al. (2019) reverse the augmentation setting by applying transformations that flip the (binary) label. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task.

Open Issues: While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are taskindependent, it has not found such widespread use in natural language processing. A reason might be that several of the approaches require an indepth understanding of the language. There is not yet a unified framework that allows applying data augmentation across tasks and languages. Recently, Longpre et al. (2020) hypothesised that data augmentation provides the same benefits as pretraining in transformer models. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited.","(p6.0) New instances can be obtained based on existing ones by modifying the features with transformations that do not change the label. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms (Wei and Zou, 2019), entities of the same type (Raiman and Miller, 2017;Dai and Adel, 2020) or words that share the same morphology (Gulordava et al., 2018;Vania et al., 2019). Such replacements can also be guided by a language model that takes context into consideration (Fadaee et al., 2017;Kobayashi, 2018).

(p6.1) To go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree (Şahin and Steedman, 2018;Vania et al., 2019;Dehouck and Gómez-Rodríguez, 2020), simplification of sentences by removal of sentence parts (Şahin and Steedman, 2018) and inversion of the subject-object relation (Min et al., 2020). For whole sentences, paraphrasing through backtranslation can be used. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011;Hoang et al., 2018). An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Ma et al., 2019). Back-translation has also been leveraged for text classification (Xie et al., 2020;Hegde and Patil, 2020). This setting assumes, however, the availability of a translation system. Instead, a language model can also be used for augmenting text classification datasets (Kumar et al., 2020;Anaby-Tavor et al., 2020). It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label. It then generates additional sentences that fit this label. Ding et al. (2020) extend this idea for token level tasks.

(p6.2) Adversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020;Garg and Ramakrishnan, 2020). They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018;Morris et al., 2020). Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score). This approach is often applied on the level of vector representations. For instance, Grundkiewicz et al. (2019) reverse the augmentation setting by applying transformations that flip the (binary) label. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task.

(p6.3) Open Issues: While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are taskindependent, it has not found such widespread use in natural language processing. A reason might be that several of the approaches require an indepth understanding of the language. There is not yet a unified framework that allows applying data augmentation across tasks and languages. Recently, Longpre et al. (2020) hypothesised that data augmentation provides the same benefits as pretraining in transformer models. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited.","[['b49', 'b18', None, 'b46'], ['b55', None, 'b9', 'b46'], [None, 'b60'], [None]]","[['b49', 'b18', None, 'b46'], ['b55', None, 'b9', 'b46'], [None, 'b60'], [None]]",11,"1. New instances can be obtained based on existing ones by modifying the features with transformations that do not change the label.
2. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content.
3. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms (Wei and Zou, 2019), entities of the same type (Raiman and Miller, 2017;Dai and Adel, 2020) or words that share the same morphology (Gulordava et al., 2018;Vania et al., 2019).
4. Such replacements can also be guided by a language model that takes context into consideration (Fadaee et al., 2017;Kobayashi, 2018).
5. To go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts.
6. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree (Şahin and Steedman, 2018;Vania et al., 2019;Dehouck and Gómez-Rodríguez, 2020), simplification of sentences by removal of sentence parts (Şahin and Steedman, 2018) and inversion of the subject-object relation (Min et al., 2020).
7. For whole sentences, paraphrasing through backtranslation can be used.
8. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011;Hoang et al., 2018).
9. An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict.
10. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Ma et al., 2019).
11. Back-translation has also been leveraged for text classification (Xie et al., 2020;Hegde and Patil, 2020).
12. This setting assumes, however, the availability of a translation system.
13. Instead, a language model can also be used for augmenting text classification datasets (Kumar et al., 2020;Anaby-Tavor et al., 2020).
14. It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label.
15. It then generates additional sentences that fit this label.
16. Ding et al. (2020) extend this idea for token level tasks.
17. Adversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020;Garg and Ramakrishnan, 2020).
18. They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018;Morris et al., 2020).
19. Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score).
20. This approach is often applied on the level of vector representations.
21. For instance, Grundkiewicz et al. (2019) reverse the augmentation setting by applying transformations that flip the (binary) label.
22. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task.
23. Open Issues: While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are taskindependent, it has not found such widespread use in natural language processing.
24. A reason might be that several of the approaches require an indepth understanding of the language.
25. There is not yet a unified framework that allows applying data augmentation across tasks and languages.
26. Recently, Longpre et al. (2020) hypothesised that data augmentation provides the same benefits as pretraining in transformer models.
27. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s7,Distant & Weak Supervision,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4']","['In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).', 'While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.', 'Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.', 'Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.', 'While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).']","In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","(p7.0) In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

(p7.1) While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

(p7.2) Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

(p7.3) Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

(p7.4) While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","[['b20', 'b22', 'b67', 'b2', None, 'b38', 'b61', 'b23', 'b1'], [None, 'b5'], [None, 'b54'], [None], [None, 'b14']]","[['b20', 'b22', 'b67', 'b2', None, 'b38', 'b61', 'b23', 'b1'], [None, 'b5'], [None, 'b54'], [None], [None, 'b14']]",16,"1. In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified.
2. The corresponding labels are obtained through a (semi-)automatic process from an external source of information.
3. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations.
4. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).
5. It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a).
6. The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019).
7. This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.
8. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).
9. While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP.
10. Nevertheless, distant supervision has also been successfully em-
11. (2020) build a discourse-structure dataset using guidance from sentiment annotations.
12. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020).
13. For some classification tasks, the labels can be rephrased with simple rules into sentences.
14. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;).
15. An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.
16. Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited.
17. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules.
18. It is an open question whether a task needs to have specific properties to be suitable for this approach.
19. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.
20. Distant supervision methods heavily rely on auxiliary data.
21. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.
22. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.
23. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.
24. While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.
25. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.
26. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches.
27. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020)."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s8,Cross-Lingual Annotation Projections,"['p8.0', 'p8.1']","['For cross-lingual projections, a task-specific classifier is trained in a high-resource language. Using parallel corpora, the unlabeled low-resource data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001). This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013;Wisniewski et al., 2014;Plank and Agić, 2018;Eskander et al., 2020). Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014;Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agić and Vulić, 2019). Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019;Zhang et al., 2019a;Fei et al., 2020;Amjad et al., 2020). Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015;Marasović et al., 2016;Friedrich and Gateva, 2017).', 'Open issues: Cross-lingual projections set high requirements on the auxiliary data needing both labels in a high-resource language and means to project them into a low-resource language. Especially the latter can be an issue as machine translation by itself might be problematic for a specific low-resource language. A limitation of the parallel corpora is their domains like political proceedings or religious texts. Mayhew et al. (2017), Fang and Cohn (2017) and Karamanolakis et al. (2020) propose systems with fewer requirements based on word translations, bilingual dictionaries and taskspecific seed words, respectively.']","For cross-lingual projections, a task-specific classifier is trained in a high-resource language. Using parallel corpora, the unlabeled low-resource data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001). This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013;Wisniewski et al., 2014;Plank and Agić, 2018;Eskander et al., 2020). Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014;Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agić and Vulić, 2019). Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019;Zhang et al., 2019a;Fei et al., 2020;Amjad et al., 2020). Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015;Marasović et al., 2016;Friedrich and Gateva, 2017).

Open issues: Cross-lingual projections set high requirements on the auxiliary data needing both labels in a high-resource language and means to project them into a low-resource language. Especially the latter can be an issue as machine translation by itself might be problematic for a specific low-resource language. A limitation of the parallel corpora is their domains like political proceedings or religious texts. Mayhew et al. (2017), Fang and Cohn (2017) and Karamanolakis et al. (2020) propose systems with fewer requirements based on word translations, bilingual dictionaries and taskspecific seed words, respectively.","(p8.0) For cross-lingual projections, a task-specific classifier is trained in a high-resource language. Using parallel corpora, the unlabeled low-resource data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001). This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013;Wisniewski et al., 2014;Plank and Agić, 2018;Eskander et al., 2020). Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014;Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agić and Vulić, 2019). Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019;Zhang et al., 2019a;Fei et al., 2020;Amjad et al., 2020). Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015;Marasović et al., 2016;Friedrich and Gateva, 2017).

(p8.1) Open issues: Cross-lingual projections set high requirements on the auxiliary data needing both labels in a high-resource language and means to project them into a low-resource language. Especially the latter can be an issue as machine translation by itself might be problematic for a specific low-resource language. A limitation of the parallel corpora is their domains like political proceedings or religious texts. Mayhew et al. (2017), Fang and Cohn (2017) and Karamanolakis et al. (2020) propose systems with fewer requirements based on word translations, bilingual dictionaries and taskspecific seed words, respectively.","[['b65', 'b69', 'b39', 'b41', None, 'b12', 'b52', 'b59'], [None]]","[['b65', 'b69', 'b39', 'b41', None, 'b12', 'b52', 'b59'], [None]]",9,"1. For cross-lingual projections, a task-specific classifier is trained in a high-resource language.
2. Using parallel corpora, the unlabeled low-resource data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier.
3. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001).
4. This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages.
5. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013;Wisniewski et al., 2014;Plank and Agić, 2018;Eskander et al., 2020).
6. Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014;Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agić and Vulić, 2019).
7. Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019;Zhang et al., 2019a;Fei et al., 2020;Amjad et al., 2020).
8. Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015;Marasović et al., 2016;Friedrich and Gateva, 2017).
9. Open issues: Cross-lingual projections set high requirements on the auxiliary data needing both labels in a high-resource language and means to project them into a low-resource language.
10. Especially the latter can be an issue as machine translation by itself might be problematic for a specific low-resource language.
11. A limitation of the parallel corpora is their domains like political proceedings or religious texts.
12. Mayhew et al. (2017), Fang and Cohn (2017) and Karamanolakis et al. (2020) propose systems with fewer requirements based on word translations, bilingual dictionaries and taskspecific seed words, respectively."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s9,Learning with Noisy Labels,"['p9.0', 'p9.1', 'p9.2', 'p9.3']","['The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain more errors. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision. We categorize these into two ideas: noise filtering and noise modeling.', 'Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).', 'The noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances. Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.', 'In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly. Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).']","The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain more errors. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision. We categorize these into two ideas: noise filtering and noise modeling.

Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).

The noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances. Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.

In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly. Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).","(p9.0) The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain more errors. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision. We categorize these into two ideas: noise filtering and noise modeling.

(p9.1) Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).

(p9.2) The noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances. Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.

(p9.3) In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly. Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).","[[], [None, 'b1'], ['b21', 'b22', 'b48', 'b61', None, 'b10', 'b67'], [None, 'b1', 'b11']]","[[], [None, 'b1'], ['b21', 'b22', 'b48', 'b61', None, 'b10', 'b67'], [None, 'b1', 'b11']]",12,"1. The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations.
2. These labels tend, however, to contain more errors.
3. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance.
4. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision.
5. We categorize these into two ideas: noise filtering and noise modeling.
6. Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled.
7. This often includes training a classifier to make the filtering decision.
8. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019).
9. Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).
10. The noise in the labels can also be modeled.
11. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b).
12. The classifier is no longer trained directly on the noisily-labeled data.
13. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution.
14. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels.
15. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing.
16. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances.
17. Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.
18. In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method.
19. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly.
20. Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019)."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s12,Pre-Trained Language Representations,"['p12.0', 'p12.1']","['Feature vectors are the core input component of many neural network-based models for NLP tasks. They are numerical representations of words or sentences, as neural architectures do not allow the processing of strings and characters as such. Collobert et al. (2011) showed that training these models for the task of language-modeling on a large-scale corpus results in high-quality word representations, which can be reused for other downstream tasks as well. Subword-based embeddings such as fastText n-gram embeddings (Bojanowski et al., 2017) and byte-pair-encoding embeddings (Heinzerling and Strube, 2018) addressed out-of-vocabulary issues by splitting words into multiple subwords, which in combination represent the original word.  showed that these embeddings leveraging subword information are beneficial for lowresource sequence labeling tasks, such as named entity recognition and typing, and outperform wordlevel embeddings. Jungmaier et al. (2020) added smoothing to word2vec models to correct its bias towards rare words and achieved improvements in particular for low-resource settings. In addition, pre-trained embeddings were published for more than 270 languages for both embedding methods. This enabled the processing of texts in many languages, including multiple low-resource languages found in Wikipedia. More recently, a trend emerged of pre-training large embedding models using a language model objective to create contextaware word representations by predicting the next word or sentence. This includes pre-trained transformer models (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b). These methods are particularly helpful for low-resource languages for which large amounts of unlabeled data are available, but task-specific labeled data is scarce (Cruz and Cheng, 2019).', 'Open Issues: While pre-trained language models achieve significant performance increases compared to standard word embeddings, it is still questionable if these methods are suited for real-world low-resource scenarios. For example, all of these models require large hardware requirements, in particular, considering that the transformer model size keeps increasing to boost performance (Raffel et al., 2020). Therefore, these large-scale methods might not be suited for low-resource scenarios where hardware is also low-resource. Biljon et al. (2020) showed that low-to mediumdepth transformer sizes perform better than larger models for low-resource languages and Schick and Schütze (2020) managed to train models with three orders of magnitude fewer parameters that perform on-par with large-scale models like GPT-3 on few-shot task by reformulating the training task and using ensembling. Melamud et al. (2019) showed that simple bag-of-words approaches are better when there are only a few dozen training instances or less for text classification, while more complex transformer models require more training data. Bhattacharjee et al. (2020) found that crossview training (Clark et al., 2018) leverages large amounts of unlabeled data better for task-specific applications in contrast to the general representations learned by BERT. Moreover, data quality for low-resource, even for unlabeled data, might not be comparable to data from high-resource languages. Alabi et al. (2020) found that word embeddings trained on larger amounts of unlabeled data from low-resource languages are not competitive to embeddings trained on smaller, but curated data sources.']","Feature vectors are the core input component of many neural network-based models for NLP tasks. They are numerical representations of words or sentences, as neural architectures do not allow the processing of strings and characters as such. Collobert et al. (2011) showed that training these models for the task of language-modeling on a large-scale corpus results in high-quality word representations, which can be reused for other downstream tasks as well. Subword-based embeddings such as fastText n-gram embeddings (Bojanowski et al., 2017) and byte-pair-encoding embeddings (Heinzerling and Strube, 2018) addressed out-of-vocabulary issues by splitting words into multiple subwords, which in combination represent the original word.  showed that these embeddings leveraging subword information are beneficial for lowresource sequence labeling tasks, such as named entity recognition and typing, and outperform wordlevel embeddings. Jungmaier et al. (2020) added smoothing to word2vec models to correct its bias towards rare words and achieved improvements in particular for low-resource settings. In addition, pre-trained embeddings were published for more than 270 languages for both embedding methods. This enabled the processing of texts in many languages, including multiple low-resource languages found in Wikipedia. More recently, a trend emerged of pre-training large embedding models using a language model objective to create contextaware word representations by predicting the next word or sentence. This includes pre-trained transformer models (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b). These methods are particularly helpful for low-resource languages for which large amounts of unlabeled data are available, but task-specific labeled data is scarce (Cruz and Cheng, 2019).

Open Issues: While pre-trained language models achieve significant performance increases compared to standard word embeddings, it is still questionable if these methods are suited for real-world low-resource scenarios. For example, all of these models require large hardware requirements, in particular, considering that the transformer model size keeps increasing to boost performance (Raffel et al., 2020). Therefore, these large-scale methods might not be suited for low-resource scenarios where hardware is also low-resource. Biljon et al. (2020) showed that low-to mediumdepth transformer sizes perform better than larger models for low-resource languages and Schick and Schütze (2020) managed to train models with three orders of magnitude fewer parameters that perform on-par with large-scale models like GPT-3 on few-shot task by reformulating the training task and using ensembling. Melamud et al. (2019) showed that simple bag-of-words approaches are better when there are only a few dozen training instances or less for text classification, while more complex transformer models require more training data. Bhattacharjee et al. (2020) found that crossview training (Clark et al., 2018) leverages large amounts of unlabeled data better for task-specific applications in contrast to the general representations learned by BERT. Moreover, data quality for low-resource, even for unlabeled data, might not be comparable to data from high-resource languages. Alabi et al. (2020) found that word embeddings trained on larger amounts of unlabeled data from low-resource languages are not competitive to embeddings trained on smaller, but curated data sources.","(p12.0) Feature vectors are the core input component of many neural network-based models for NLP tasks. They are numerical representations of words or sentences, as neural architectures do not allow the processing of strings and characters as such. Collobert et al. (2011) showed that training these models for the task of language-modeling on a large-scale corpus results in high-quality word representations, which can be reused for other downstream tasks as well. Subword-based embeddings such as fastText n-gram embeddings (Bojanowski et al., 2017) and byte-pair-encoding embeddings (Heinzerling and Strube, 2018) addressed out-of-vocabulary issues by splitting words into multiple subwords, which in combination represent the original word.  showed that these embeddings leveraging subword information are beneficial for lowresource sequence labeling tasks, such as named entity recognition and typing, and outperform wordlevel embeddings. Jungmaier et al. (2020) added smoothing to word2vec models to correct its bias towards rare words and achieved improvements in particular for low-resource settings. In addition, pre-trained embeddings were published for more than 270 languages for both embedding methods. This enabled the processing of texts in many languages, including multiple low-resource languages found in Wikipedia. More recently, a trend emerged of pre-training large embedding models using a language model objective to create contextaware word representations by predicting the next word or sentence. This includes pre-trained transformer models (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b). These methods are particularly helpful for low-resource languages for which large amounts of unlabeled data are available, but task-specific labeled data is scarce (Cruz and Cheng, 2019).

(p12.1) Open Issues: While pre-trained language models achieve significant performance increases compared to standard word embeddings, it is still questionable if these methods are suited for real-world low-resource scenarios. For example, all of these models require large hardware requirements, in particular, considering that the transformer model size keeps increasing to boost performance (Raffel et al., 2020). Therefore, these large-scale methods might not be suited for low-resource scenarios where hardware is also low-resource. Biljon et al. (2020) showed that low-to mediumdepth transformer sizes perform better than larger models for low-resource languages and Schick and Schütze (2020) managed to train models with three orders of magnitude fewer parameters that perform on-par with large-scale models like GPT-3 on few-shot task by reformulating the training task and using ensembling. Melamud et al. (2019) showed that simple bag-of-words approaches are better when there are only a few dozen training instances or less for text classification, while more complex transformer models require more training data. Bhattacharjee et al. (2020) found that crossview training (Clark et al., 2018) leverages large amounts of unlabeled data better for task-specific applications in contrast to the general representations learned by BERT. Moreover, data quality for low-resource, even for unlabeled data, might not be comparable to data from high-resource languages. Alabi et al. (2020) found that word embeddings trained on larger amounts of unlabeled data from low-resource languages are not competitive to embeddings trained on smaller, but curated data sources.","[[None, 'b47'], [None, 'b16']]","[[None, 'b47'], [None, 'b16']]",4,"1. Feature vectors are the core input component of many neural network-based models for NLP tasks.
2. They are numerical representations of words or sentences, as neural architectures do not allow the processing of strings and characters as such.
3. Collobert et al. (2011) showed that training these models for the task of language-modeling on a large-scale corpus results in high-quality word representations, which can be reused for other downstream tasks as well.
4. Subword-based embeddings such as fastText n-gram embeddings (Bojanowski et al., 2017) and byte-pair-encoding embeddings (Heinzerling and Strube, 2018) addressed out-of-vocabulary issues by splitting words into multiple subwords, which in combination represent the original word.
5. showed that these embeddings leveraging subword information are beneficial for lowresource sequence labeling tasks, such as named entity recognition and typing, and outperform wordlevel embeddings.
6. Jungmaier et al. (2020) added smoothing to word2vec models to correct its bias towards rare words and achieved improvements in particular for low-resource settings.
7. In addition, pre-trained embeddings were published for more than 270 languages for both embedding methods.
8. This enabled the processing of texts in many languages, including multiple low-resource languages found in Wikipedia.
9. More recently, a trend emerged of pre-training large embedding models using a language model objective to create contextaware word representations by predicting the next word or sentence.
10. This includes pre-trained transformer models (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b).
11. These methods are particularly helpful for low-resource languages for which large amounts of unlabeled data are available, but task-specific labeled data is scarce (Cruz and Cheng, 2019).
12. Open Issues: While pre-trained language models achieve significant performance increases compared to standard word embeddings, it is still questionable if these methods are suited for real-world low-resource scenarios.
13. For example, all of these models require large hardware requirements, in particular, considering that the transformer model size keeps increasing to boost performance (Raffel et al., 2020).
14. Therefore, these large-scale methods might not be suited for low-resource scenarios where hardware is also low-resource.
15. Biljon et al. (2020) showed that low-to mediumdepth transformer sizes perform better than larger models for low-resource languages and Schick and Schütze (2020) managed to train models with three orders of magnitude fewer parameters that perform on-par with large-scale models like GPT-3 on few-shot task by reformulating the training task and using ensembling.
16. Melamud et al. (2019) showed that simple bag-of-words approaches are better when there are only a few dozen training instances or less for text classification, while more complex transformer models require more training data.
17. Bhattacharjee et al. (2020) found that crossview training (Clark et al., 2018) leverages large amounts of unlabeled data better for task-specific applications in contrast to the general representations learned by BERT.
18. Moreover, data quality for low-resource, even for unlabeled data, might not be comparable to data from high-resource languages.
19. Alabi et al. (2020) found that word embeddings trained on larger amounts of unlabeled data from low-resource languages are not competitive to embeddings trained on smaller, but curated data sources."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s14,Multilingual Language Models,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4']","['Analogously to low-resource domains, lowresource languages can also benefit from labeled resources available in other high-resource languages. This usually requires the training of multilingual language representations by combining monolingual representations (Lange et al., 2020a) or training a single model for many languages, such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020) . These models are trained using unlabeled, monolingual corpora from different languages and can be used in crossand multilingual settings, due to many languages seen during pre-training.', 'In cross-lingual zero-shot learning, no taskspecific labeled data is available in the low-resource target language. Instead, labeled data from a high-resource language is leveraged. A multilingual model can be trained on the target task in a high-resource language and afterwards, applied to the unseen target languages, such as for named entity recognition ( 2020) proposed adding a minimal amount of target-task and -language data (in the range of 10 to 100 labeled sentences) which resulted in a significant boost in performance for classification in low-resource languages.', 'The transfer between two languages can be improved by creating a common multilingual embedding space of multiple languages. This is useful for standard word embeddings  as well as pre-trained language models. For example, by aligning the languages inside a single multilin- This alignment is typically done by computing a mapping between two different embedding spaces, such that the words in both embeddings share similar feature vectors after the mapping (Mikolov et al., 2013;Joulin et al., 2018). This allows to use different embeddings inside the same model and helps when two languages do not share the same space inside a single model (Cao et al., 2020). For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the highresource language English and three low-resource African languages, Swahili, Tagalog, and Somali, to improve document retrieval performance for the African languages.', 'Open Issues: While these multilingual models are a tremendous step towards enabling NLP in many languages, possible claims that these are universal language models do not hold. For example, mBERT covers 104 and XLM-R 100 languages, which is a third of all languages in Wikipedia as outlined earlier. Further, Wu and Dredze (2020) showed that, in particular, low-resource languages are not well-represented in mBERT. Figure 2 shows which language families with at least 1 million speakers are covered by mBERT and XLM-RoBERTa 2 . In particular, African and American languages are not well-represented within the transformer models, even though millions of people speak these languages. This can be problematic, as languages from more distant language families are less suited for transfer learning, as Lauscher et al.', '(2020) showed.']","Analogously to low-resource domains, lowresource languages can also benefit from labeled resources available in other high-resource languages. This usually requires the training of multilingual language representations by combining monolingual representations (Lange et al., 2020a) or training a single model for many languages, such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020) . These models are trained using unlabeled, monolingual corpora from different languages and can be used in crossand multilingual settings, due to many languages seen during pre-training.

In cross-lingual zero-shot learning, no taskspecific labeled data is available in the low-resource target language. Instead, labeled data from a high-resource language is leveraged. A multilingual model can be trained on the target task in a high-resource language and afterwards, applied to the unseen target languages, such as for named entity recognition ( 2020) proposed adding a minimal amount of target-task and -language data (in the range of 10 to 100 labeled sentences) which resulted in a significant boost in performance for classification in low-resource languages.

The transfer between two languages can be improved by creating a common multilingual embedding space of multiple languages. This is useful for standard word embeddings  as well as pre-trained language models. For example, by aligning the languages inside a single multilin- This alignment is typically done by computing a mapping between two different embedding spaces, such that the words in both embeddings share similar feature vectors after the mapping (Mikolov et al., 2013;Joulin et al., 2018). This allows to use different embeddings inside the same model and helps when two languages do not share the same space inside a single model (Cao et al., 2020). For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the highresource language English and three low-resource African languages, Swahili, Tagalog, and Somali, to improve document retrieval performance for the African languages.

Open Issues: While these multilingual models are a tremendous step towards enabling NLP in many languages, possible claims that these are universal language models do not hold. For example, mBERT covers 104 and XLM-R 100 languages, which is a third of all languages in Wikipedia as outlined earlier. Further, Wu and Dredze (2020) showed that, in particular, low-resource languages are not well-represented in mBERT. Figure 2 shows which language families with at least 1 million speakers are covered by mBERT and XLM-RoBERTa 2 . In particular, African and American languages are not well-represented within the transformer models, even though millions of people speak these languages. This can be problematic, as languages from more distant language families are less suited for transfer learning, as Lauscher et al.

(2020) showed.","(p14.0) Analogously to low-resource domains, lowresource languages can also benefit from labeled resources available in other high-resource languages. This usually requires the training of multilingual language representations by combining monolingual representations (Lange et al., 2020a) or training a single model for many languages, such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020) . These models are trained using unlabeled, monolingual corpora from different languages and can be used in crossand multilingual settings, due to many languages seen during pre-training.

(p14.1) In cross-lingual zero-shot learning, no taskspecific labeled data is available in the low-resource target language. Instead, labeled data from a high-resource language is leveraged. A multilingual model can be trained on the target task in a high-resource language and afterwards, applied to the unseen target languages, such as for named entity recognition ( 2020) proposed adding a minimal amount of target-task and -language data (in the range of 10 to 100 labeled sentences) which resulted in a significant boost in performance for classification in low-resource languages.

(p14.2) The transfer between two languages can be improved by creating a common multilingual embedding space of multiple languages. This is useful for standard word embeddings  as well as pre-trained language models. For example, by aligning the languages inside a single multilin- This alignment is typically done by computing a mapping between two different embedding spaces, such that the words in both embeddings share similar feature vectors after the mapping (Mikolov et al., 2013;Joulin et al., 2018). This allows to use different embeddings inside the same model and helps when two languages do not share the same space inside a single model (Cao et al., 2020). For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the highresource language English and three low-resource African languages, Swahili, Tagalog, and Somali, to improve document retrieval performance for the African languages.

(p14.3) Open Issues: While these multilingual models are a tremendous step towards enabling NLP in many languages, possible claims that these are universal language models do not hold. For example, mBERT covers 104 and XLM-R 100 languages, which is a third of all languages in Wikipedia as outlined earlier. Further, Wu and Dredze (2020) showed that, in particular, low-resource languages are not well-represented in mBERT. Figure 2 shows which language families with at least 1 million speakers are covered by mBERT and XLM-RoBERTa 2 . In particular, African and American languages are not well-represented within the transformer models, even though millions of people speak these languages. This can be problematic, as languages from more distant language families are less suited for transfer learning, as Lauscher et al.

(p14.4) (2020) showed.","[[None], [], [None, 'b66'], ['b53'], []]","[[None], [], [None, 'b66'], ['b53'], []]",4,"1. Analogously to low-resource domains, lowresource languages can also benefit from labeled resources available in other high-resource languages.
2. This usually requires the training of multilingual language representations by combining monolingual representations (Lange et al., 2020a) or training a single model for many languages, such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020) .
3. These models are trained using unlabeled, monolingual corpora from different languages and can be used in crossand multilingual settings, due to many languages seen during pre-training.
4. In cross-lingual zero-shot learning, no taskspecific labeled data is available in the low-resource target language.
5. Instead, labeled data from a high-resource language is leveraged.
6. A multilingual model can be trained on the target task in a high-resource language and afterwards, applied to the unseen target languages, such as for named entity recognition ( 2020) proposed adding a minimal amount of target-task and -language data (in the range of 10 to 100 labeled sentences) which resulted in a significant boost in performance for classification in low-resource languages.
7. The transfer between two languages can be improved by creating a common multilingual embedding space of multiple languages.
8. This is useful for standard word embeddings  as well as pre-trained language models.
9. For example, by aligning the languages inside a single multilin-
10. This alignment is typically done by computing a mapping between two different embedding spaces, such that the words in both embeddings share similar feature vectors after the mapping (Mikolov et al., 2013;Joulin et al., 2018).
11. This allows to use different embeddings inside the same model and helps when two languages do not share the same space inside a single model (Cao et al., 2020).
12. For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the highresource language English and three low-resource African languages, Swahili, Tagalog, and Somali, to improve document retrieval performance for the African languages.
13. Open Issues: While these multilingual models are a tremendous step towards enabling NLP in many languages, possible claims that these are universal language models do not hold.
14. For example, mBERT covers 104 and XLM-R 100 languages, which is a third of all languages in Wikipedia as outlined earlier.
15. Further, Wu and Dredze (2020) showed that, in particular, low-resource languages are not well-represented in mBERT.
16. Figure 2 shows which language families with at least 1 million speakers are covered by mBERT and XLM-RoBERTa 2 .
17. In particular, African and American languages are not well-represented within the transformer models, even though millions of people speak these languages.
18. This can be problematic, as languages from more distant language families are less suited for transfer learning, as Lauscher et al.(2020) showed."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s2,Source(s) Target,"['p2.0', 'p2.1', 'p2.2', 'p2.3']","[""Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread."", '2 What is Stance?', 'In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker\'s standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).', 'Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.']","Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","(p2.0) Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

(p2.1) 2 What is Stance?

(p2.2) In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

(p2.3) Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","[['b57', 'b12', None, 'b17'], [], [None], []]","[['b57', 'b12', None, 'b17'], [], [None], []]",5,"1. Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011)
2. Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b)
3. Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article *
4. 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '
5. 17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection.
6. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds.
7. * the article's body is summarised.
8. Sources: Twitter, ǌ News, ɀikipedia, Reddit.
9. Evidence: Single, Multiple, Thread.
10. 2 What is Stance? In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process.
11. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition.
12. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc.
13. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither.
14. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016).
15. Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).
16. Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s6,Multiple languages,"['p6.0', 'p6.1', 'p6.2', 'p6.3', 'p6.4', 'p6.5', 'p6.6', 'p6.7', 'p6.8', 'p6.9', 'p6.10', 'p6.11', 'p6.12', 'p6.13']","['In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.', 'Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.', ""Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim."", 'More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.', 'Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).', 'Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.', 'Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).', 'Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.', 'Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.', ""These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019)."", 'A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).', 'Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.', 'Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.', 'Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.']","In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","(p6.0) In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

(p6.1) Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

(p6.2) Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

(p6.3) More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

(p6.4) Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

(p6.5) Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

(p6.6) Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

(p6.7) Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

(p6.8) Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

(p6.9) These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

(p6.10) A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

(p6.11) Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

(p6.12) Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

(p6.13) Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","[[], [None, 'b19'], [], ['b31'], [None], ['b30', 'b50', 'b8', None, 'b47', 'b52', 'b51', 'b43'], [None], [], ['b57', None], ['b55', None, 'b49', 'b36'], ['b57', 'b49', None], [None, 'b38'], [], ['b24', None, 'b25']]","[[], [None, 'b19'], [], ['b31'], [None], ['b30', 'b50', 'b8', None, 'b47', 'b52', 'b51', 'b43'], [None], [], ['b57', None], ['b55', None, 'b49', 'b36'], ['b57', 'b49', None], [None, 'b38'], [], ['b24', None, 'b25']]",27,"1. In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1.
2. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance.
3. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead.
4. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a).
5. Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help.
6. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful.
7. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance.
8. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1).
9. Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general.
10. This can be attributed to the use of n-grams, topic models, and lexica.
11. Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents.
12. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t.
13. the target claim. More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.
14. Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help.
15. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree.
16. The success of these models is also seen in cross-lingual settings.
17. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT.
18. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets.
19. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).
20. Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER .
21. To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted.
22. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021).
23. Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score.
24. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points.
25. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70.
26. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes.
27. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information.
28. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.
29. Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021).
30. Such models do not require a retrieval step, as they use the knowledge stored in language models.
31. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"".
32. Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type.
33. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).
34. Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer.""
35. vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club.
36. "", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers.""
37. (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm.""
38. is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.
39. Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.
40. These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours).
41. A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017).
42. Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets.
43. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020).
44. Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1).
45. Li et al. (2020) deviated from this structure and modelled the conversations as a graph.
46. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection.
47. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task.
48. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification .
49. Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).
50. A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0.
51. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1.
52. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class.
53. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).
54. Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.
55. Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness.
56. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.
57. Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance.
58. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models.
59. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets.
60. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.
61. They showed that MDL helps for low-resource and substantively for full-resource scenarios.
62. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance.
63. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1.
64. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc.
65. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s9,Shades of Truth,"['p9.0', 'p9.1']","['The notion of shades of truth is important in mis-and disinformation detection. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false. We believe that such shades could be applied to stance and used in a larger pipeline. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).', 'Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.']","The notion of shades of truth is important in mis-and disinformation detection. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false. We believe that such shades could be applied to stance and used in a larger pipeline. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).

Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.","(p9.0) The notion of shades of truth is important in mis-and disinformation detection. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false. We believe that such shades could be applied to stance and used in a larger pipeline. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).

(p9.1) Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.","[['b20', 'b3', 'b18', 'b9'], [None, 'b25']]","[['b20', 'b3', 'b18', 'b9'], [None, 'b25']]",6,"1. The notion of shades of truth is important in mis-and disinformation detection.
2. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false.
3. We believe that such shades could be applied to stance and used in a larger pipeline.
4. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).
5. Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance.
6. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance.
7. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s10,Explainability,"['p10.0', 'p10.1', 'p10.2']","['The ability for a model to be able to explain its decisions is getting increasingly important, especially for mis-and disinformation detection, as one could argue that it is a crucial step towards adopting fully automated fact-checking. The FEVER 2.0 task formulation (Thorne et al., 2019) can be viewed as a step towards obtaining such explanations, e.g., there have been efforts to identify adversarial triggers that offer explanations for the vulnerabilities at the model level (Atanasova et al., 2020b). However, FEVER is artificially created and is limited to Wikipedia, which may not reflect real-world settings. To mitigate this, explanation by professional journalists can be found on fact-checking websites, and can be further combined with stance detection in an automated system. In a step in this direction, Atanasova et al. (2020a) generated natural language explanations for claims from PolitiFact 4 given gold evidence document summaries by journalists.', ""Moreover, partial explanations can be obtained automatically from the underlying models, e.g., from memory networks (Mohtarami et al., 2018), attention weights (Zhou et al., 2019;Liu et al., 2020b), or topic relations (Si et al., 2021). However, such approaches are limited as they can require gold snippets justifying the document's stance, attention weights can be misleading (Jain and Wallace, 2019), and topics might be noisy due to their unsupervised nature. Other existing systems (Popat et al., 2017(Popat et al., , 2018Nadeem et al., 2019) offer explanations to a more limited extent, highlighting span overlaps between the target text and the evidence documents. Overall, there is a need for holistic and realistic explanations of how a factchecking model arrived at its prediction."", 'Integration People question false information more and tend to confirm true information (Mendoza et al., 2010). Thus, stance can play a vital role in verifying dubious content. In Appendix C, we discuss existing systems and real-world applications of stance for mis-and disinformation identification in more detail. However, we argue that a tighter integration between stance and factchecking is needed. Stance can be expressed in different forms, e.g., tweets, news articles, user posts, sentences in Wikipedia, and Wiki tables, among others and can have different formulations as part of the fact-checking pipeline (see Section 3). All these can guide human fact-checkers through the process of fact-checking, and can point them to relevant evidence. Moreover, the wisdom of the crowd can be a powerful instrument in the fight against mis-and disinformation (Pennycook and Rand, 2019), but we should note that vocal minorities can derail public discourse (Scannell et al., 2021). Nevertheless, these risks can be mitigated by taking into account the credibility of the user or of the information source, which can be done automatically or with the help of human fact-checkers.']","The ability for a model to be able to explain its decisions is getting increasingly important, especially for mis-and disinformation detection, as one could argue that it is a crucial step towards adopting fully automated fact-checking. The FEVER 2.0 task formulation (Thorne et al., 2019) can be viewed as a step towards obtaining such explanations, e.g., there have been efforts to identify adversarial triggers that offer explanations for the vulnerabilities at the model level (Atanasova et al., 2020b). However, FEVER is artificially created and is limited to Wikipedia, which may not reflect real-world settings. To mitigate this, explanation by professional journalists can be found on fact-checking websites, and can be further combined with stance detection in an automated system. In a step in this direction, Atanasova et al. (2020a) generated natural language explanations for claims from PolitiFact 4 given gold evidence document summaries by journalists.

Moreover, partial explanations can be obtained automatically from the underlying models, e.g., from memory networks (Mohtarami et al., 2018), attention weights (Zhou et al., 2019;Liu et al., 2020b), or topic relations (Si et al., 2021). However, such approaches are limited as they can require gold snippets justifying the document's stance, attention weights can be misleading (Jain and Wallace, 2019), and topics might be noisy due to their unsupervised nature. Other existing systems (Popat et al., 2017(Popat et al., , 2018Nadeem et al., 2019) offer explanations to a more limited extent, highlighting span overlaps between the target text and the evidence documents. Overall, there is a need for holistic and realistic explanations of how a factchecking model arrived at its prediction.

Integration People question false information more and tend to confirm true information (Mendoza et al., 2010). Thus, stance can play a vital role in verifying dubious content. In Appendix C, we discuss existing systems and real-world applications of stance for mis-and disinformation identification in more detail. However, we argue that a tighter integration between stance and factchecking is needed. Stance can be expressed in different forms, e.g., tweets, news articles, user posts, sentences in Wikipedia, and Wiki tables, among others and can have different formulations as part of the fact-checking pipeline (see Section 3). All these can guide human fact-checkers through the process of fact-checking, and can point them to relevant evidence. Moreover, the wisdom of the crowd can be a powerful instrument in the fight against mis-and disinformation (Pennycook and Rand, 2019), but we should note that vocal minorities can derail public discourse (Scannell et al., 2021). Nevertheless, these risks can be mitigated by taking into account the credibility of the user or of the information source, which can be done automatically or with the help of human fact-checkers.","(p10.0) The ability for a model to be able to explain its decisions is getting increasingly important, especially for mis-and disinformation detection, as one could argue that it is a crucial step towards adopting fully automated fact-checking. The FEVER 2.0 task formulation (Thorne et al., 2019) can be viewed as a step towards obtaining such explanations, e.g., there have been efforts to identify adversarial triggers that offer explanations for the vulnerabilities at the model level (Atanasova et al., 2020b). However, FEVER is artificially created and is limited to Wikipedia, which may not reflect real-world settings. To mitigate this, explanation by professional journalists can be found on fact-checking websites, and can be further combined with stance detection in an automated system. In a step in this direction, Atanasova et al. (2020a) generated natural language explanations for claims from PolitiFact 4 given gold evidence document summaries by journalists.

(p10.1) Moreover, partial explanations can be obtained automatically from the underlying models, e.g., from memory networks (Mohtarami et al., 2018), attention weights (Zhou et al., 2019;Liu et al., 2020b), or topic relations (Si et al., 2021). However, such approaches are limited as they can require gold snippets justifying the document's stance, attention weights can be misleading (Jain and Wallace, 2019), and topics might be noisy due to their unsupervised nature. Other existing systems (Popat et al., 2017(Popat et al., , 2018Nadeem et al., 2019) offer explanations to a more limited extent, highlighting span overlaps between the target text and the evidence documents. Overall, there is a need for holistic and realistic explanations of how a factchecking model arrived at its prediction.

(p10.2) Integration People question false information more and tend to confirm true information (Mendoza et al., 2010). Thus, stance can play a vital role in verifying dubious content. In Appendix C, we discuss existing systems and real-world applications of stance for mis-and disinformation identification in more detail. However, we argue that a tighter integration between stance and factchecking is needed. Stance can be expressed in different forms, e.g., tweets, news articles, user posts, sentences in Wikipedia, and Wiki tables, among others and can have different formulations as part of the fact-checking pipeline (see Section 3). All these can guide human fact-checkers through the process of fact-checking, and can point them to relevant evidence. Moreover, the wisdom of the crowd can be a powerful instrument in the fight against mis-and disinformation (Pennycook and Rand, 2019), but we should note that vocal minorities can derail public discourse (Scannell et al., 2021). Nevertheless, these risks can be mitigated by taking into account the credibility of the user or of the information source, which can be done automatically or with the help of human fact-checkers.","[[None], ['b30', 'b14', None, 'b52', 'b13', 'b1'], [None, 'b23', 'b10']]","[[None], ['b30', 'b14', None, 'b52', 'b13', 'b1'], [None, 'b23', 'b10']]",10,"1. The ability for a model to be able to explain its decisions is getting increasingly important, especially for mis-and disinformation detection, as one could argue that it is a crucial step towards adopting fully automated fact-checking.
2. The FEVER 2.0 task formulation (Thorne et al., 2019) can be viewed as a step towards obtaining such explanations, e.g., there have been efforts to identify adversarial triggers that offer explanations for the vulnerabilities at the model level (Atanasova et al., 2020b).
3. However, FEVER is artificially created and is limited to Wikipedia, which may not reflect real-world settings.
4. To mitigate this, explanation by professional journalists can be found on fact-checking websites, and can be further combined with stance detection in an automated system.
5. In a step in this direction, Atanasova et al. (2020a) generated natural language explanations for claims from PolitiFact 4 given gold evidence document summaries by journalists.
6. Moreover, partial explanations can be obtained automatically from the underlying models, e.g., from memory networks (Mohtarami et al., 2018), attention weights (Zhou et al., 2019;Liu et al., 2020b), or topic relations (Si et al., 2021).
7. However, such approaches are limited as they can require gold snippets justifying the document's stance, attention weights can be misleading (Jain and Wallace, 2019), and topics might be noisy due to their unsupervised nature.
8. Other existing systems (Popat et al., 2017(Popat et al., , 2018Nadeem et al., 2019) offer explanations to a more limited extent, highlighting span overlaps between the target text and the evidence documents.
9. Overall, there is a need for holistic and realistic explanations of how a factchecking model arrived at its prediction.
10. Integration People question false information more and tend to confirm true information (Mendoza et al., 2010).
11. Thus, stance can play a vital role in verifying dubious content.
12. In Appendix C, we discuss existing systems and real-world applications of stance for mis-and disinformation identification in more detail.
13. However, we argue that a tighter integration between stance and factchecking is needed.
14. Stance can be expressed in different forms, e.g., tweets, news articles, user posts, sentences in Wikipedia, and Wiki tables, among others and can have different formulations as part of the fact-checking pipeline (see Section 3).
15. All these can guide human fact-checkers through the process of fact-checking, and can point them to relevant evidence.
16. Moreover, the wisdom of the crowd can be a powerful instrument in the fight against mis-and disinformation (Pennycook and Rand, 2019), but we should note that vocal minorities can derail public discourse (Scannell et al., 2021).
17. Nevertheless, these risks can be mitigated by taking into account the credibility of the user or of the information source, which can be done automatically or with the help of human fact-checkers."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s14,B Additional Formulations of Stance as a Component for Fact-Checking,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4', 'p14.5', 'p14.6', 'p14.7', 'p14.8']","['Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline. Below, we describe some work that follows these formulations.', 'Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia. They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions. A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020). This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.', 'Media Profiling Stance detection has also been used for media profiling. Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics. They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020). This is an important step towards understanding media biases. Tweet: Wow, that is fascinating! I hope you never mock our proud Scandi heritage again.', '(b) Examples from Qazvinian et al. (2011) andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992. ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.', '(c) Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how do you know its an ISIS flag? Can you actually confirm that? ɳ u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table 3: Illustrative examples for different stance detection scenarios included in our survey. We annotate the expressed stance with (support, for), (deny, against), ɳ (query), and (comment).', 'The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.', ""More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information. In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling."", 'There is a well-known connection between factuality and bias. 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.', 'User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user. In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017).']","Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline. Below, we describe some work that follows these formulations.

Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia. They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions. A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020). This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.

Media Profiling Stance detection has also been used for media profiling. Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics. They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020). This is an important step towards understanding media biases. Tweet: Wow, that is fascinating! I hope you never mock our proud Scandi heritage again.

(b) Examples from Qazvinian et al. (2011) andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992. ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.

(c) Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how do you know its an ISIS flag? Can you actually confirm that? ɳ u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table 3: Illustrative examples for different stance detection scenarios included in our survey. We annotate the expressed stance with (support, for), (deny, against), ɳ (query), and (comment).

The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.

More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information. In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling.

There is a well-known connection between factuality and bias. 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.

User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user. In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017).","(p14.0) Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline. Below, we describe some work that follows these formulations.

(p14.1) Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia. They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions. A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020). This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.

(p14.2) Media Profiling Stance detection has also been used for media profiling. Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics. They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020). This is an important step towards understanding media biases. Tweet: Wow, that is fascinating! I hope you never mock our proud Scandi heritage again.

(p14.3) (b) Examples from Qazvinian et al. (2011) andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992. ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.

(p14.4) (c) Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how do you know its an ISIS flag? Can you actually confirm that? ɳ u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table 3: Illustrative examples for different stance detection scenarios included in our survey. We annotate the expressed stance with (support, for), (deny, against), ɳ (query), and (comment).

(p14.5) The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.

(p14.6) More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information. In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling.

(p14.7) There is a well-known connection between factuality and bias. 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.

(p14.8) User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user. In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017).","[[], [None], ['b32'], [None, 'b17'], [], ['b0', None, 'b14', 'b13'], ['b16'], ['b15'], [None]]","[[], [None], ['b32'], [None, 'b17'], [], ['b0', None, 'b14', 'b13'], ['b16'], ['b15'], [None]]",11,"1. Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline.
2. Below, we describe some work that follows these formulations.
3. Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia.
4. They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions.
5. A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020).
6. This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.
7. Media Profiling Stance detection has also been used for media profiling.
8. Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics.
9. They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020).
10. This is an important step towards understanding media biases.
11. Tweet: Wow, that is fascinating!
12. I hope you never mock our proud Scandi heritage again.
13. (b) Examples from Qazvinian et al. (2011)
14. andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992.
15. ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.(c)
16. Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ
17. Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ
18. Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how
19. do you know its an ISIS flag? Can you actually confirm that?
20. ɳ u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table
21. 3: Illustrative examples for different stance detection scenarios included in our survey.
22. We annotate the expressed stance with (support, for), (deny, against), ɳ (query), and (comment).
23. The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.
24. More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information.
25. In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling.
26. There is a well-known connection between factuality and bias.
27. 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.
28. User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user.
29. In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017)."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s8,Lessons Learned and Future Trends,"['p8.0', 'p8.1', 'p8.2', 'p8.3', 'p8.4', 'p8.5']","['Dataset Size A major limitation holding back the performance of machine learning for stance detection is the size of the existing stance datasets, the vast majority of which contain at most a few thousand examples. Contrasted with the related task of Natural Language Inference, where datasets such as SNLI (Bowman et al., 2015) of more than half a million samples have been collected, this is far from optimal. Moreover, the small dataset sizes are often accompanied with skewed class distribution with very few examples from the minority classes, including many of the datasets in this study (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021).', 'This can lead to a significant disparity for label performance (see Section 4). Several techniques have been proposed to mitigate this, such as sampling strategies (Nie et al., 2019), weighting classes (Veyseh et al., 2017), 3 crafting artificial examples from auxiliary tasks Hardalov et al., 2022), or training on multiple datasets (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.', 'Data Mixing A potential way of overcoming limitations in terms of dataset size and focus is to combine multiple datasets. Yet, as we previously discussed (see Section 3), task definitions and label inventories vary across stance datasets. Further, large-scale studies of approaches that leverage the relationships between label inventories, or the similarity between datasets are still largely lacking. One promising direction is the use of label embeddings (Augenstein et al., 2018), as they offer a convenient way to learn interactions between disjoint label sets that carry semantic relations. One such first study was recently presented by Hardalov et al. (2021), which explored different strategies for leveraging inter-dataset signals and label interactions in both in-(seen targets) and out-of-domain (unseen targets) settings. This could help to overcome challenges faced by models trained on smallsize datasets, and even for smaller minority classes.', 'Multilinguality Multi-linguality is important for several reasons: (i) the content may originate in various languages, (ii) the evidence or the stance may not be expressed in the same language, thus (iii) posing a challenge for fact-checkers, who might not be speakers of the language the claim was originally made in, and (iv) it adds more data that can be leveraged for modelling stance. Currently, only a handful of datasets for factuality and stance cover languages other than English (see Table 1), and they are small in size and do not offer a cross-lingual setup. Recently, Vamvas and Sennrich (2020) proposed such a setup for three languages for stance in debates, Schick andSchütze (2021) explored few-shot learning, andHardalov et al. (2022) extended that paradigm with sentiment and stance pre-training and evaluated on twelve languages from various domains. Since cultural norms and expressed linguistic phenomena play a crucial role in understanding the context of a claim (Sap et al., 2019), we do not argue for a completely language-agnostic framework. Yet, empirically, training in cross-lingual setups improves performance by leveraging better representations learned on a similar language or by acting as a regulariser.', 'Modelling the Context Modelling the context is a particularly important, yet challenging task. In many cases, there is a need to consider the background of the stance-taker as well as the characteristics of the targeted object. In particular, in the context of social media, one can provide information about the users such as their previous activity, other users they interact most with, the threads they participate in, or even their interests (Zubiaga et al., 2016b;Gorrell et al., 2019;Li et al., 2019b). The context of the stance expressed in news articles is related to the features of the media outlets, such as source of funding, previously known biases, or credibility (Baly et al., 2019;Darwish et al., 2020;Stefanov et al., 2020;Baly et al., 2020). When using contextual information about the object, factual information about the real world, and the time of posting are all important. Incorporating these into a stance detection pipeline, while challenging, paves the way towards a robust detection process.', ""Multimodal Content Spreading mis-and disinformation through multiple modalities is becoming increasingly popular. One such example are deepfakes, i.e., synthetically created images or videos, in which (usually) the face of one person is replaced with another person's face. Another example are information propagation techniques such as memetic warfare. Hence, it is increasingly important to combine different modalities to understand the full context stance is being expressed in. Some work in this area is on fake news detection for images (Nakamura et al., 2020), claim verification for images (Zlatkova et al., 2019), or searching for fact-checked information to alleviate the spread of fake news (Vo and Lee, 2020). There has been work on meme analysis for related tasks: detecting hateful (Kiela et al., 2020), harmful (Pramanick et al., 2021;Sharma et al., 2022a), and propagandistic memes (Dimitrov et al., 2021a,b); see also a recent survey of harmful memes (Sharma et al., 2022b). This line of research is especially relevant for mis-and disinformation tasks that depend on the wisdom of the crowd in social media as it adds additional information sources (Qazvinian et al., 2011;Zubiaga et al., 2016b;Derczynski et al., 2017;Hossain et al., 2020); see Section 5.""]","Dataset Size A major limitation holding back the performance of machine learning for stance detection is the size of the existing stance datasets, the vast majority of which contain at most a few thousand examples. Contrasted with the related task of Natural Language Inference, where datasets such as SNLI (Bowman et al., 2015) of more than half a million samples have been collected, this is far from optimal. Moreover, the small dataset sizes are often accompanied with skewed class distribution with very few examples from the minority classes, including many of the datasets in this study (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021).

This can lead to a significant disparity for label performance (see Section 4). Several techniques have been proposed to mitigate this, such as sampling strategies (Nie et al., 2019), weighting classes (Veyseh et al., 2017), 3 crafting artificial examples from auxiliary tasks Hardalov et al., 2022), or training on multiple datasets (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.

Data Mixing A potential way of overcoming limitations in terms of dataset size and focus is to combine multiple datasets. Yet, as we previously discussed (see Section 3), task definitions and label inventories vary across stance datasets. Further, large-scale studies of approaches that leverage the relationships between label inventories, or the similarity between datasets are still largely lacking. One promising direction is the use of label embeddings (Augenstein et al., 2018), as they offer a convenient way to learn interactions between disjoint label sets that carry semantic relations. One such first study was recently presented by Hardalov et al. (2021), which explored different strategies for leveraging inter-dataset signals and label interactions in both in-(seen targets) and out-of-domain (unseen targets) settings. This could help to overcome challenges faced by models trained on smallsize datasets, and even for smaller minority classes.

Multilinguality Multi-linguality is important for several reasons: (i) the content may originate in various languages, (ii) the evidence or the stance may not be expressed in the same language, thus (iii) posing a challenge for fact-checkers, who might not be speakers of the language the claim was originally made in, and (iv) it adds more data that can be leveraged for modelling stance. Currently, only a handful of datasets for factuality and stance cover languages other than English (see Table 1), and they are small in size and do not offer a cross-lingual setup. Recently, Vamvas and Sennrich (2020) proposed such a setup for three languages for stance in debates, Schick andSchütze (2021) explored few-shot learning, andHardalov et al. (2022) extended that paradigm with sentiment and stance pre-training and evaluated on twelve languages from various domains. Since cultural norms and expressed linguistic phenomena play a crucial role in understanding the context of a claim (Sap et al., 2019), we do not argue for a completely language-agnostic framework. Yet, empirically, training in cross-lingual setups improves performance by leveraging better representations learned on a similar language or by acting as a regulariser.

Modelling the Context Modelling the context is a particularly important, yet challenging task. In many cases, there is a need to consider the background of the stance-taker as well as the characteristics of the targeted object. In particular, in the context of social media, one can provide information about the users such as their previous activity, other users they interact most with, the threads they participate in, or even their interests (Zubiaga et al., 2016b;Gorrell et al., 2019;Li et al., 2019b). The context of the stance expressed in news articles is related to the features of the media outlets, such as source of funding, previously known biases, or credibility (Baly et al., 2019;Darwish et al., 2020;Stefanov et al., 2020;Baly et al., 2020). When using contextual information about the object, factual information about the real world, and the time of posting are all important. Incorporating these into a stance detection pipeline, while challenging, paves the way towards a robust detection process.

Multimodal Content Spreading mis-and disinformation through multiple modalities is becoming increasingly popular. One such example are deepfakes, i.e., synthetically created images or videos, in which (usually) the face of one person is replaced with another person's face. Another example are information propagation techniques such as memetic warfare. Hence, it is increasingly important to combine different modalities to understand the full context stance is being expressed in. Some work in this area is on fake news detection for images (Nakamura et al., 2020), claim verification for images (Zlatkova et al., 2019), or searching for fact-checked information to alleviate the spread of fake news (Vo and Lee, 2020). There has been work on meme analysis for related tasks: detecting hateful (Kiela et al., 2020), harmful (Pramanick et al., 2021;Sharma et al., 2022a), and propagandistic memes (Dimitrov et al., 2021a,b); see also a recent survey of harmful memes (Sharma et al., 2022b). This line of research is especially relevant for mis-and disinformation tasks that depend on the wisdom of the crowd in social media as it adds additional information sources (Qazvinian et al., 2011;Zubiaga et al., 2016b;Derczynski et al., 2017;Hossain et al., 2020); see Section 5.","(p8.0) Dataset Size A major limitation holding back the performance of machine learning for stance detection is the size of the existing stance datasets, the vast majority of which contain at most a few thousand examples. Contrasted with the related task of Natural Language Inference, where datasets such as SNLI (Bowman et al., 2015) of more than half a million samples have been collected, this is far from optimal. Moreover, the small dataset sizes are often accompanied with skewed class distribution with very few examples from the minority classes, including many of the datasets in this study (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021).

(p8.1) This can lead to a significant disparity for label performance (see Section 4). Several techniques have been proposed to mitigate this, such as sampling strategies (Nie et al., 2019), weighting classes (Veyseh et al., 2017), 3 crafting artificial examples from auxiliary tasks Hardalov et al., 2022), or training on multiple datasets (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.

(p8.2) Data Mixing A potential way of overcoming limitations in terms of dataset size and focus is to combine multiple datasets. Yet, as we previously discussed (see Section 3), task definitions and label inventories vary across stance datasets. Further, large-scale studies of approaches that leverage the relationships between label inventories, or the similarity between datasets are still largely lacking. One promising direction is the use of label embeddings (Augenstein et al., 2018), as they offer a convenient way to learn interactions between disjoint label sets that carry semantic relations. One such first study was recently presented by Hardalov et al. (2021), which explored different strategies for leveraging inter-dataset signals and label interactions in both in-(seen targets) and out-of-domain (unseen targets) settings. This could help to overcome challenges faced by models trained on smallsize datasets, and even for smaller minority classes.

(p8.3) Multilinguality Multi-linguality is important for several reasons: (i) the content may originate in various languages, (ii) the evidence or the stance may not be expressed in the same language, thus (iii) posing a challenge for fact-checkers, who might not be speakers of the language the claim was originally made in, and (iv) it adds more data that can be leveraged for modelling stance. Currently, only a handful of datasets for factuality and stance cover languages other than English (see Table 1), and they are small in size and do not offer a cross-lingual setup. Recently, Vamvas and Sennrich (2020) proposed such a setup for three languages for stance in debates, Schick andSchütze (2021) explored few-shot learning, andHardalov et al. (2022) extended that paradigm with sentiment and stance pre-training and evaluated on twelve languages from various domains. Since cultural norms and expressed linguistic phenomena play a crucial role in understanding the context of a claim (Sap et al., 2019), we do not argue for a completely language-agnostic framework. Yet, empirically, training in cross-lingual setups improves performance by leveraging better representations learned on a similar language or by acting as a regulariser.

(p8.4) Modelling the Context Modelling the context is a particularly important, yet challenging task. In many cases, there is a need to consider the background of the stance-taker as well as the characteristics of the targeted object. In particular, in the context of social media, one can provide information about the users such as their previous activity, other users they interact most with, the threads they participate in, or even their interests (Zubiaga et al., 2016b;Gorrell et al., 2019;Li et al., 2019b). The context of the stance expressed in news articles is related to the features of the media outlets, such as source of funding, previously known biases, or credibility (Baly et al., 2019;Darwish et al., 2020;Stefanov et al., 2020;Baly et al., 2020). When using contextual information about the object, factual information about the real world, and the time of posting are all important. Incorporating these into a stance detection pipeline, while challenging, paves the way towards a robust detection process.

(p8.5) Multimodal Content Spreading mis-and disinformation through multiple modalities is becoming increasingly popular. One such example are deepfakes, i.e., synthetically created images or videos, in which (usually) the face of one person is replaced with another person's face. Another example are information propagation techniques such as memetic warfare. Hence, it is increasingly important to combine different modalities to understand the full context stance is being expressed in. Some work in this area is on fake news detection for images (Nakamura et al., 2020), claim verification for images (Zlatkova et al., 2019), or searching for fact-checked information to alleviate the spread of fake news (Vo and Lee, 2020). There has been work on meme analysis for related tasks: detecting hateful (Kiela et al., 2020), harmful (Pramanick et al., 2021;Sharma et al., 2022a), and propagandistic memes (Dimitrov et al., 2021a,b); see also a recent survey of harmful memes (Sharma et al., 2022b). This line of research is especially relevant for mis-and disinformation tasks that depend on the wisdom of the crowd in social media as it adds additional information sources (Qazvinian et al., 2011;Zubiaga et al., 2016b;Derczynski et al., 2017;Hossain et al., 2020); see Section 5.","[[None, 'b57', 'b12'], ['b7', 'b25', None], [None], ['b22', 'b24', None], ['b57', None, 'b32'], ['b53', 'b28', 'b57', 'b39', 'b16', 'b29', None, 'b17', 'b2']]","[[None, 'b57', 'b12'], ['b7', 'b25', None], [None], ['b22', 'b24', None], ['b57', None, 'b32'], ['b53', 'b28', 'b57', 'b39', 'b16', 'b29', None, 'b17', 'b2']]",22,"1. Dataset Size A major limitation holding back the performance of machine learning for stance detection is the size of the existing stance datasets, the vast majority of which contain at most a few thousand examples.
2. Contrasted with the related task of Natural Language Inference, where datasets such as SNLI (Bowman et al., 2015) of more than half a million samples have been collected, this is far from optimal.
3. Moreover, the small dataset sizes are often accompanied with skewed class distribution with very few examples from the minority classes, including many of the datasets in this study (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021).
4. This can lead to a significant disparity for label performance (see Section 4).
5. Several techniques have been proposed to mitigate this, such as sampling strategies (Nie et al., 2019), weighting classes (Veyseh et al., 2017), 3 crafting artificial examples from auxiliary tasks Hardalov et al., 2022), or training on multiple datasets (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.
6. Data Mixing A potential way of overcoming limitations in terms of dataset size and focus is to combine multiple datasets.
7. Yet, as we previously discussed (see Section 3), task definitions and label inventories vary across stance datasets.
8. Further, large-scale studies of approaches that leverage the relationships between label inventories, or the similarity between datasets are still largely lacking.
9. One promising direction is the use of label embeddings (Augenstein et al., 2018), as they offer a convenient way to learn interactions between disjoint label sets that carry semantic relations.
10. One such first study was recently presented by Hardalov et al. (2021), which explored different strategies for leveraging inter-dataset signals and label interactions in both in-(seen targets) and out-of-domain (unseen targets) settings.
11. This could help to overcome challenges faced by models trained on smallsize datasets, and even for smaller minority classes.
12. Multilinguality Multi-linguality is important for several reasons: (i) the content may originate in various languages, (ii) the evidence or the stance may not be expressed in the same language, thus (iii) posing a challenge for fact-checkers, who might not be speakers of the language the claim was originally made in, and (iv) it adds more data that can be leveraged for modelling stance.
13. Currently, only a handful of datasets for factuality and stance cover languages other than English (see Table 1), and they are small in size and do not offer a cross-lingual setup.
14. Recently, Vamvas and Sennrich (2020) proposed such a setup for three languages for stance in debates, Schick andSchütze (2021) explored few-shot learning, andHardalov et al. (2022) extended that paradigm with sentiment and stance pre-training and evaluated on twelve languages from various domains.
15. Since cultural norms and expressed linguistic phenomena play a crucial role in understanding the context of a claim (Sap et al., 2019), we do not argue for a completely language-agnostic framework.
16. Yet, empirically, training in cross-lingual setups improves performance by leveraging better representations learned on a similar language or by acting as a regulariser.
17. Modelling the Context Modelling the context is a particularly important, yet challenging task.
18. In many cases, there is a need to consider the background of the stance-taker as well as the characteristics of the targeted object.
19. In particular, in the context of social media, one can provide information about the users such as their previous activity, other users they interact most with, the threads they participate in, or even their interests (Zubiaga et al., 2016b;Gorrell et al., 2019;Li et al., 2019b).
20. The context of the stance expressed in news articles is related to the features of the media outlets, such as source of funding, previously known biases, or credibility (Baly et al., 2019;Darwish et al., 2020;Stefanov et al., 2020;Baly et al., 2020).
21. When using contextual information about the object, factual information about the real world, and the time of posting are all important.
22. Incorporating these into a stance detection pipeline, while challenging, paves the way towards a robust detection process.
23. Multimodal Content Spreading mis-and disinformation through multiple modalities is becoming increasingly popular.
24. One such example are deepfakes, i.e., synthetically created images or videos, in which (usually) the face of one person is replaced with another person's face.
25. Another example are information propagation techniques such as memetic warfare.
26. Hence, it is increasingly important to combine different modalities to understand the full context stance is being expressed in.
27. Some work in this area is on fake news detection for images (Nakamura et al., 2020), claim verification for images (Zlatkova et al., 2019), or searching for fact-checked information to alleviate the spread of fake news (Vo and Lee, 2020).
28. There has been work on meme analysis for related tasks: detecting hateful (Kiela et al., 2020), harmful (Pramanick et al., 2021;Sharma et al., 2022a), and propagandistic memes (Dimitrov et al., 2021a,b); see also a recent survey of harmful memes (Sharma et al., 2022b).
29. This line of research is especially relevant for mis-and disinformation tasks that depend on the wisdom of the crowd in social media as it adds additional information sources (Qazvinian et al., 2011;Zubiaga et al., 2016b;Derczynski et al., 2017;Hossain et al., 2020); see Section 5."
232320384,A Survey on Multimodal Disinformation Detection,Computer Science,https://www.semanticscholar.org/paper/71d2dc1fc38e0c48c865de5f5c023ccf7c5ad018,s6,Network and Temporal Information,"['p6.0', 'p6.1', 'p6.2']","['The rationale for leveraging network information stems from early work (Shao et al., 2018;Vosoughi et al., 2018a) that showed that propagation and interaction networks of fake news are deeper and wider than those of real news. Vosoughi et al. (2018a) further found that fake information spreads faster than factual one, thus advocating for the use of temporal information.', 'Propagation networks can be homogeneous or heterogeneous (e.g., encompassing news articles, publishers, users, and posts) and they can be analyzed at different scales (e.g., node-level, ego-level, triad-level, community-level and the overall network, as shown in Figure 3, in Appendix) (Zhou and Zafarani, 2019). Shu et al. (2020) tackled the fake news classification task by proposing an approach based on hierarchical propagation networks. At both micro-and macro-scale, they extracted and jointly considered network features, temporal features, and linguistic features. Experiments on PolitiFact and GossipCop datasets revealed that temporal features have maximum contribution, followed by network and linguistic features. Shu et al. (2019) provided one of the most thorough multimodal frameworks for fake news classification. Their experimental results suggest that social context (i.e., network-derived) features are more informative than news content ones. Vosoughi et al. (2017) proposed Rumor Gauge, a system that jointly exploits temporal and propagation features, in conjunction with linguistic and user credibility features, for checking the veracity of rumors. In particular, Rumor Gauge leverages text, and network propagation. The temporal modality does not directly provide features, but is instead considered by recomputing all other features at regular time steps, thus yielding multiple time series. Results by Vosoughi et al. (2017) and Kwon et al. (2017) also demonstrated that the contribution of the different data modalities change over time.', 'To mitigate the ""cold start"" problem of propagation-based early detection of fake news, Liu and Wu (2018) proposed an approach that is primarily based on user and temporal information. First, they built a propagation path of each news as a time series of user representations. The time series for a given news only contains the ordered representations of those users that shared such news. Then, they learned two vector representations of each propagation path via GRUs and CNNs, respectively. Zannettou et al. (2018) analyzed different aspects of memes, such as how they evolve and propagate in different mainstream and fringe web communities, and variants of memes that propa-gate. Finally, Nguyen et al. (2020) proposed Factual News Graph (FANG) to exploit the social structure and the engagement patterns of users for fake news detection.']","The rationale for leveraging network information stems from early work (Shao et al., 2018;Vosoughi et al., 2018a) that showed that propagation and interaction networks of fake news are deeper and wider than those of real news. Vosoughi et al. (2018a) further found that fake information spreads faster than factual one, thus advocating for the use of temporal information.

Propagation networks can be homogeneous or heterogeneous (e.g., encompassing news articles, publishers, users, and posts) and they can be analyzed at different scales (e.g., node-level, ego-level, triad-level, community-level and the overall network, as shown in Figure 3, in Appendix) (Zhou and Zafarani, 2019). Shu et al. (2020) tackled the fake news classification task by proposing an approach based on hierarchical propagation networks. At both micro-and macro-scale, they extracted and jointly considered network features, temporal features, and linguistic features. Experiments on PolitiFact and GossipCop datasets revealed that temporal features have maximum contribution, followed by network and linguistic features. Shu et al. (2019) provided one of the most thorough multimodal frameworks for fake news classification. Their experimental results suggest that social context (i.e., network-derived) features are more informative than news content ones. Vosoughi et al. (2017) proposed Rumor Gauge, a system that jointly exploits temporal and propagation features, in conjunction with linguistic and user credibility features, for checking the veracity of rumors. In particular, Rumor Gauge leverages text, and network propagation. The temporal modality does not directly provide features, but is instead considered by recomputing all other features at regular time steps, thus yielding multiple time series. Results by Vosoughi et al. (2017) and Kwon et al. (2017) also demonstrated that the contribution of the different data modalities change over time.

To mitigate the ""cold start"" problem of propagation-based early detection of fake news, Liu and Wu (2018) proposed an approach that is primarily based on user and temporal information. First, they built a propagation path of each news as a time series of user representations. The time series for a given news only contains the ordered representations of those users that shared such news. Then, they learned two vector representations of each propagation path via GRUs and CNNs, respectively. Zannettou et al. (2018) analyzed different aspects of memes, such as how they evolve and propagate in different mainstream and fringe web communities, and variants of memes that propa-gate. Finally, Nguyen et al. (2020) proposed Factual News Graph (FANG) to exploit the social structure and the engagement patterns of users for fake news detection.","(p6.0) The rationale for leveraging network information stems from early work (Shao et al., 2018;Vosoughi et al., 2018a) that showed that propagation and interaction networks of fake news are deeper and wider than those of real news. Vosoughi et al. (2018a) further found that fake information spreads faster than factual one, thus advocating for the use of temporal information.

(p6.1) Propagation networks can be homogeneous or heterogeneous (e.g., encompassing news articles, publishers, users, and posts) and they can be analyzed at different scales (e.g., node-level, ego-level, triad-level, community-level and the overall network, as shown in Figure 3, in Appendix) (Zhou and Zafarani, 2019). Shu et al. (2020) tackled the fake news classification task by proposing an approach based on hierarchical propagation networks. At both micro-and macro-scale, they extracted and jointly considered network features, temporal features, and linguistic features. Experiments on PolitiFact and GossipCop datasets revealed that temporal features have maximum contribution, followed by network and linguistic features. Shu et al. (2019) provided one of the most thorough multimodal frameworks for fake news classification. Their experimental results suggest that social context (i.e., network-derived) features are more informative than news content ones. Vosoughi et al. (2017) proposed Rumor Gauge, a system that jointly exploits temporal and propagation features, in conjunction with linguistic and user credibility features, for checking the veracity of rumors. In particular, Rumor Gauge leverages text, and network propagation. The temporal modality does not directly provide features, but is instead considered by recomputing all other features at regular time steps, thus yielding multiple time series. Results by Vosoughi et al. (2017) and Kwon et al. (2017) also demonstrated that the contribution of the different data modalities change over time.

(p6.2) To mitigate the ""cold start"" problem of propagation-based early detection of fake news, Liu and Wu (2018) proposed an approach that is primarily based on user and temporal information. First, they built a propagation path of each news as a time series of user representations. The time series for a given news only contains the ordered representations of those users that shared such news. Then, they learned two vector representations of each propagation path via GRUs and CNNs, respectively. Zannettou et al. (2018) analyzed different aspects of memes, such as how they evolve and propagate in different mainstream and fringe web communities, and variants of memes that propa-gate. Finally, Nguyen et al. (2020) proposed Factual News Graph (FANG) to exploit the social structure and the engagement patterns of users for fake news detection.","[['b53', 'b72'], ['b55', 'b57', 'b8', 'b90', 'b71'], ['b29', 'b84', 'b16']]","[['b53', 'b72'], ['b55', 'b57', 'b8', 'b90', 'b71'], ['b29', 'b84', 'b16']]",10,"1. The rationale for leveraging network information stems from early work (Shao et al., 2018;Vosoughi et al., 2018a) that showed that propagation and interaction networks of fake news are deeper and wider than those of real news.
2. Vosoughi et al. (2018a) further found that fake information spreads faster than factual one, thus advocating for the use of temporal information.
3. Propagation networks can be homogeneous or heterogeneous (e.g., encompassing news articles, publishers, users, and posts)
4. and they can be analyzed at different scales
5. (e.g., node-level, ego-level, triad-level, community-level and the overall network, as shown in Figure 3, in Appendix) (Zhou and Zafarani, 2019).
6. Shu et al. (2020) tackled the fake news classification task by proposing an approach based on hierarchical propagation networks.
7. At both micro-and macro-scale, they extracted and jointly considered network features, temporal features, and linguistic features.
8. Experiments on PolitiFact and GossipCop datasets revealed that temporal features have maximum contribution, followed by network and linguistic features.
9. Shu et al. (2019) provided one of the most thorough multimodal frameworks for fake news classification.
10. Their experimental results suggest that social context (i.e., network-derived) features are more informative than news content ones.
11. Vosoughi et al. (2017) proposed Rumor Gauge, a system that jointly exploits temporal and propagation features, in conjunction with linguistic and user credibility features, for checking the veracity of rumors.
12. In particular, Rumor Gauge leverages text, and network propagation.
13. The temporal modality does not directly provide features, but is instead considered by recomputing all other features at regular time steps, thus yielding multiple time series.
14. Results by Vosoughi et al. (2017) and Kwon et al. (2017) also demonstrated that the contribution of the different data modalities change over time.
15. To mitigate the ""cold start"" problem of propagation-based early detection of fake news, Liu and Wu (2018) proposed an approach that is primarily based on user and temporal information.
16. First, they built a propagation path of each news as a time series of user representations.
17. The time series for a given news only contains the ordered representations of those users that shared such news.
18. Then, they learned two vector representations of each propagation path via GRUs and CNNs, respectively.
19. Zannettou et al. (2018) analyzed different aspects of memes, such as how they evolve and propagate in different mainstream and fringe web communities, and variants of memes that propa-gate.
20. Finally, Nguyen et al. (2020) proposed Factual News Graph (FANG) to exploit the social structure and the engagement patterns of users for fake news detection."
232320384,A Survey on Multimodal Disinformation Detection,Computer Science,https://www.semanticscholar.org/paper/71d2dc1fc38e0c48c865de5f5c023ccf7c5ad018,s2,Text,['p2.0'],"['Due to the availability of large amounts of textual content, research on the text modality is comparatively richer than for other modalities. Notable work in this direction covers fake news spread on social media (Vosoughi et al., 2018b), fake news and fact-checking on news media (Rashkin et al., 2017), fact-checking such as fact-checked URL recommendation model (Vo and Lee, 2018) (Sathe et al., 2020). There have also been recent efforts for factchecking from political debates (Shaar et al., 2020(Shaar et al., , 2022(Shaar et al., , 2021Nakov et al., 2022b,a), fact-checking with evidence reasoning (Si et al., 2021;Jiang et al., 2021;Wan et al., 2021) and fact-checking by claim matching (Kazemi et al., 2021). Given that there have been surveys on the text modality for fake news/disinformation detection and fact-checking, here we will not go into more detail about the individual studies.']","Due to the availability of large amounts of textual content, research on the text modality is comparatively richer than for other modalities. Notable work in this direction covers fake news spread on social media (Vosoughi et al., 2018b), fake news and fact-checking on news media (Rashkin et al., 2017), fact-checking such as fact-checked URL recommendation model (Vo and Lee, 2018) (Sathe et al., 2020). There have also been recent efforts for factchecking from political debates (Shaar et al., 2020(Shaar et al., , 2022(Shaar et al., , 2021Nakov et al., 2022b,a), fact-checking with evidence reasoning (Si et al., 2021;Jiang et al., 2021;Wan et al., 2021) and fact-checking by claim matching (Kazemi et al., 2021). Given that there have been surveys on the text modality for fake news/disinformation detection and fact-checking, here we will not go into more detail about the individual studies.","(p2.0) Due to the availability of large amounts of textual content, research on the text modality is comparatively richer than for other modalities. Notable work in this direction covers fake news spread on social media (Vosoughi et al., 2018b), fake news and fact-checking on news media (Rashkin et al., 2017), fact-checking such as fact-checked URL recommendation model (Vo and Lee, 2018) (Sathe et al., 2020). There have also been recent efforts for factchecking from political debates (Shaar et al., 2020(Shaar et al., , 2022(Shaar et al., , 2021Nakov et al., 2022b,a), fact-checking with evidence reasoning (Si et al., 2021;Jiang et al., 2021;Wan et al., 2021) and fact-checking by claim matching (Kazemi et al., 2021). Given that there have been surveys on the text modality for fake news/disinformation detection and fact-checking, here we will not go into more detail about the individual studies.","[['b73', 'b69', 'b50', 'b45', None, 'b74', 'b52', 'b51', 'b42', 'b58']]","[['b73', 'b69', 'b50', 'b45', None, 'b74', 'b52', 'b51', 'b42', 'b58']]",10,"1. Due to the availability of large amounts of textual content, research on the text modality is comparatively richer than for other modalities.
2. Notable work in this direction covers fake news spread on social media (Vosoughi et al., 2018b), fake news and fact-checking on news media (Rashkin et al., 2017), fact-checking such as fact-checked URL recommendation model (Vo and Lee, 2018) (Sathe et al., 2020).
3. There have also been recent efforts for factchecking from political debates (Shaar et al., 2020(Shaar et al., , 2022(Shaar et al., , 2021Nakov et al., 2022b,a), fact-checking with evidence reasoning (Si et al., 2021;Jiang et al., 2021;Wan et al., 2021) and fact-checking by claim matching (Kazemi et al., 2021).
4. Given that there have been surveys on the text modality for fake news/disinformation detection and fact-checking, here we will not go into more detail about the individual studies."
232320384,A Survey on Multimodal Disinformation Detection,Computer Science,https://www.semanticscholar.org/paper/71d2dc1fc38e0c48c865de5f5c023ccf7c5ad018,s3,Image,"['p3.0', 'p3.1']","['Text with visual content (e.g., images) in social media is more prominent as it is more intuitive; thus, it is easier to consume, it spreads faster, it gets 18% more clicks, 89% more likes, and 150% more retweets . Due to the growing number of claims disseminated with images, in the current literature, there have been various studies that address the visual content with text for predicting misleading information (Volkova et al., 2019), fake images (Gupta et al., 2013), images shared with misinformation in political groups (Garimella and Eckles, 2020), and fauxtography Wang et al., 2021). Some of these studies attempt to understand how two different modalities are used. Their analyses show that the extension of text with images increases the effectiveness of misleading content. Gupta et al. (2013) highlighted the role of Twitter to spread fake images. This study reports that 86% tweets spreading fake images are retweets. Garimella and Eckles (2020) manually annotated a sample of 2,500 images collected from public WhatsApp groups, and labeled them as misinformation, not misinformation, misinformation already fact-checked, and unclear; however, experiments were conducted with binary labels: misinformation vs. not-misinformation. The authors found that violent and graphic images spread faster. Nakamura et al. (2020) developed a multimodal dataset containing 1M posts including text, images, metadata, and comments collected from Reddit. The dataset was labeled with 2, 3, and 6-ways labels. Volkova et al. (2019) proposed models for detecting misleading information using images and text.', 'Fauxtography is defined as ""visual images, especially news photographs, which convey a questionable (or outright false) sense of the events they seem to depict"" (Cooper, 2007). It is also commonly used in social media in different forms such as a fake image with false claims, a true image with false claims.  defined that ""a post is a fauxtography if the image of the post (i) directly supports a false claim, or (ii) conveys misinformation of a true claim."" An example is shown in Figure 2 (in Appendix A).  developed FauxBuster to detect fauxtographic social media content, which uses social media comments in addition to the content in the images and the texts. Zlatkova et al. (2019) investigated the factuality of claims with respect to images and compared the performance of different feature groups between text and images. Wang et al. (2021) analyzed fauxtography images in social media posts and found that posts with doctored images increase user engagement in the form of re-shares, likes, and comments, specifically in Twitter and Reddit. They pointed out that doctored images are often used as memes to mislead or as a means of satire, and that they have a \'clickbait\' power to drive engagement.']","Text with visual content (e.g., images) in social media is more prominent as it is more intuitive; thus, it is easier to consume, it spreads faster, it gets 18% more clicks, 89% more likes, and 150% more retweets . Due to the growing number of claims disseminated with images, in the current literature, there have been various studies that address the visual content with text for predicting misleading information (Volkova et al., 2019), fake images (Gupta et al., 2013), images shared with misinformation in political groups (Garimella and Eckles, 2020), and fauxtography Wang et al., 2021). Some of these studies attempt to understand how two different modalities are used. Their analyses show that the extension of text with images increases the effectiveness of misleading content. Gupta et al. (2013) highlighted the role of Twitter to spread fake images. This study reports that 86% tweets spreading fake images are retweets. Garimella and Eckles (2020) manually annotated a sample of 2,500 images collected from public WhatsApp groups, and labeled them as misinformation, not misinformation, misinformation already fact-checked, and unclear; however, experiments were conducted with binary labels: misinformation vs. not-misinformation. The authors found that violent and graphic images spread faster. Nakamura et al. (2020) developed a multimodal dataset containing 1M posts including text, images, metadata, and comments collected from Reddit. The dataset was labeled with 2, 3, and 6-ways labels. Volkova et al. (2019) proposed models for detecting misleading information using images and text.

Fauxtography is defined as ""visual images, especially news photographs, which convey a questionable (or outright false) sense of the events they seem to depict"" (Cooper, 2007). It is also commonly used in social media in different forms such as a fake image with false claims, a true image with false claims.  defined that ""a post is a fauxtography if the image of the post (i) directly supports a false claim, or (ii) conveys misinformation of a true claim."" An example is shown in Figure 2 (in Appendix A).  developed FauxBuster to detect fauxtographic social media content, which uses social media comments in addition to the content in the images and the texts. Zlatkova et al. (2019) investigated the factuality of claims with respect to images and compared the performance of different feature groups between text and images. Wang et al. (2021) analyzed fauxtography images in social media posts and found that posts with doctored images increase user engagement in the form of re-shares, likes, and comments, specifically in Twitter and Reddit. They pointed out that doctored images are often used as memes to mislead or as a means of satire, and that they have a 'clickbait' power to drive engagement.","(p3.0) Text with visual content (e.g., images) in social media is more prominent as it is more intuitive; thus, it is easier to consume, it spreads faster, it gets 18% more clicks, 89% more likes, and 150% more retweets . Due to the growing number of claims disseminated with images, in the current literature, there have been various studies that address the visual content with text for predicting misleading information (Volkova et al., 2019), fake images (Gupta et al., 2013), images shared with misinformation in political groups (Garimella and Eckles, 2020), and fauxtography Wang et al., 2021). Some of these studies attempt to understand how two different modalities are used. Their analyses show that the extension of text with images increases the effectiveness of misleading content. Gupta et al. (2013) highlighted the role of Twitter to spread fake images. This study reports that 86% tweets spreading fake images are retweets. Garimella and Eckles (2020) manually annotated a sample of 2,500 images collected from public WhatsApp groups, and labeled them as misinformation, not misinformation, misinformation already fact-checked, and unclear; however, experiments were conducted with binary labels: misinformation vs. not-misinformation. The authors found that violent and graphic images spread faster. Nakamura et al. (2020) developed a multimodal dataset containing 1M posts including text, images, metadata, and comments collected from Reddit. The dataset was labeled with 2, 3, and 6-ways labels. Volkova et al. (2019) proposed models for detecting misleading information using images and text.

(p3.1) Fauxtography is defined as ""visual images, especially news photographs, which convey a questionable (or outright false) sense of the events they seem to depict"" (Cooper, 2007). It is also commonly used in social media in different forms such as a fake image with false claims, a true image with false claims.  defined that ""a post is a fauxtography if the image of the post (i) directly supports a false claim, or (ii) conveys misinformation of a true claim."" An example is shown in Figure 2 (in Appendix A).  developed FauxBuster to detect fauxtographic social media content, which uses social media comments in addition to the content in the images and the texts. Zlatkova et al. (2019) investigated the factuality of claims with respect to images and compared the performance of different feature groups between text and images. Wang et al. (2021) analyzed fauxtography images in social media posts and found that posts with doctored images increase user engagement in the form of re-shares, likes, and comments, specifically in Twitter and Reddit. They pointed out that doctored images are often used as memes to mislead or as a means of satire, and that they have a 'clickbait' power to drive engagement.","[[None, 'b70', 'b25'], ['b93', None]]","[[None, 'b70', 'b25'], ['b93', None]]",5,"1. Text with visual content (e.g., images) in social media is more prominent as it is more intuitive; thus, it is easier to consume, it spreads faster, it gets 18% more clicks, 89% more likes, and 150% more retweets .
2. Due to the growing number of claims disseminated with images, in the current literature, there have been various studies that address the visual content with text for predicting misleading information (Volkova et al., 2019), fake images (Gupta et al., 2013), images shared with misinformation in political groups (Garimella and Eckles, 2020), and fauxtography Wang et al., 2021).
3. Some of these studies attempt to understand how two different modalities are used.
4. Their analyses show that the extension of text with images increases the effectiveness of misleading content.
5. Gupta et al. (2013) highlighted the role of Twitter to spread fake images.
6. This study reports that 86% tweets spreading fake images are retweets.
7. Garimella and Eckles (2020) manually annotated a sample of 2,500 images collected from public WhatsApp groups, and labeled them as misinformation, not misinformation, misinformation already fact-checked, and unclear; however, experiments were conducted with binary labels: misinformation vs. not-misinformation.
8. The authors found that violent and graphic images spread faster.
9. Nakamura et al. (2020) developed a multimodal dataset containing 1M posts including text, images, metadata, and comments collected from Reddit.
10. The dataset was labeled with 2, 3, and 6-ways labels.
11. Volkova et al. (2019) proposed models for detecting misleading information using images and text.
12. Fauxtography is defined as ""visual images, especially news photographs, which convey a questionable (or outright false) sense of the events they seem to depict"" (Cooper, 2007).
13. It is also commonly used in social media in different forms such as a fake image with false claims, a true image with false claims.
14. defined that ""a post is a fauxtography if the image of the post (i) directly supports a false claim, or (ii) conveys misinformation of a true claim.""
15. An example is shown in Figure 2 (in Appendix A).
16. developed FauxBuster to detect fauxtographic social media content, which uses social media comments in addition to the content in the images and the texts.
17. Zlatkova et al. (2019) investigated the factuality of claims with respect to images and compared the performance of different feature groups between text and images.
18. Wang et al. (2021) analyzed fauxtography images in social media posts and found that posts with doctored images increase user engagement in the form of re-shares, likes, and comments, specifically in Twitter and Reddit.
19. They pointed out that doctored images are often used as memes to mislead or as a means of satire, and that they have a 'clickbait' power to drive engagement."
232320384,A Survey on Multimodal Disinformation Detection,Computer Science,https://www.semanticscholar.org/paper/71d2dc1fc38e0c48c865de5f5c023ccf7c5ad018,s4,Speech/Audio,['p4.0'],"['There have been attempts to use acoustic signals to predict the factuality of claims in political de-bates (Kopev et al., 2019;Shaar et al., 2020), leftcenter-right bias in YouTube channels (Dinkov et al., 2019), and deception in speech (Hirschberg et al., 2005). Kopev et al. (2019) found that the acoustic signal helps in improving the performance compared to using only textual and metadata features. Similarly, Dinkov et al. (2019) reported that the use of speech signal improves the performance of the system for detecting the political bias (i.e., left, center, right) of Youtube channels. Moreover, a large body of work was done on deception detection using the acoustic signal. Hirschberg et al. (2005) created the Columbia-SRI-Colorado (CSC) corpus by eliciting within-speaker deceptive and non-deceptive speech. Their experiments consist of the use of acoustic, prosodic, and a variety of lexical features including 68 LIWC categories, filled pauses, and paralinguistic information (e.g., speaker information, gender, field-pause). Using the same corpus, an evaluation campaign was organized, where different multimodal approaches were proposed, such as fusion of different acoustic, prosodic, lexical, and phonotactics representations (Levitan et al., 2016;Kaya and Karpov, 2016).']","There have been attempts to use acoustic signals to predict the factuality of claims in political de-bates (Kopev et al., 2019;Shaar et al., 2020), leftcenter-right bias in YouTube channels (Dinkov et al., 2019), and deception in speech (Hirschberg et al., 2005). Kopev et al. (2019) found that the acoustic signal helps in improving the performance compared to using only textual and metadata features. Similarly, Dinkov et al. (2019) reported that the use of speech signal improves the performance of the system for detecting the political bias (i.e., left, center, right) of Youtube channels. Moreover, a large body of work was done on deception detection using the acoustic signal. Hirschberg et al. (2005) created the Columbia-SRI-Colorado (CSC) corpus by eliciting within-speaker deceptive and non-deceptive speech. Their experiments consist of the use of acoustic, prosodic, and a variety of lexical features including 68 LIWC categories, filled pauses, and paralinguistic information (e.g., speaker information, gender, field-pause). Using the same corpus, an evaluation campaign was organized, where different multimodal approaches were proposed, such as fusion of different acoustic, prosodic, lexical, and phonotactics representations (Levitan et al., 2016;Kaya and Karpov, 2016).","(p4.0) There have been attempts to use acoustic signals to predict the factuality of claims in political de-bates (Kopev et al., 2019;Shaar et al., 2020), leftcenter-right bias in YouTube channels (Dinkov et al., 2019), and deception in speech (Hirschberg et al., 2005). Kopev et al. (2019) found that the acoustic signal helps in improving the performance compared to using only textual and metadata features. Similarly, Dinkov et al. (2019) reported that the use of speech signal improves the performance of the system for detecting the political bias (i.e., left, center, right) of Youtube channels. Moreover, a large body of work was done on deception detection using the acoustic signal. Hirschberg et al. (2005) created the Columbia-SRI-Colorado (CSC) corpus by eliciting within-speaker deceptive and non-deceptive speech. Their experiments consist of the use of acoustic, prosodic, and a variety of lexical features including 68 LIWC categories, filled pauses, and paralinguistic information (e.g., speaker information, gender, field-pause). Using the same corpus, an evaluation campaign was organized, where different multimodal approaches were proposed, such as fusion of different acoustic, prosodic, lexical, and phonotactics representations (Levitan et al., 2016;Kaya and Karpov, 2016).","[['b3', 'b9', None, 'b52']]","[['b3', 'b9', None, 'b52']]",4,"1. There have been attempts to use acoustic signals to predict the factuality of claims in political de-bates (Kopev et al., 2019;Shaar et al., 2020), leftcenter-right bias in YouTube channels (Dinkov et al., 2019), and deception in speech (Hirschberg et al., 2005).
2. Kopev et al. (2019) found that the acoustic signal helps in improving the performance compared to using only textual and metadata features.
3. Similarly, Dinkov et al. (2019) reported that the use of speech signal improves the performance of the system for detecting the political bias (i.e., left, center, right) of Youtube channels.
4. Moreover, a large body of work was done on deception detection using the acoustic signal.
5. Hirschberg et al. (2005) created the Columbia-SRI-Colorado (CSC) corpus by eliciting within-speaker deceptive and non-deceptive speech.
6. Their experiments consist of the use of acoustic, prosodic, and a variety of lexical features including 68 LIWC categories, filled pauses, and paralinguistic information (e.g., speaker information, gender, field-pause).
7. Using the same corpus, an evaluation campaign was organized, where different multimodal approaches were proposed, such as fusion of different acoustic, prosodic, lexical, and phonotactics representations (Levitan et al., 2016;Kaya and Karpov, 2016)."
232320384,A Survey on Multimodal Disinformation Detection,Computer Science,https://www.semanticscholar.org/paper/71d2dc1fc38e0c48c865de5f5c023ccf7c5ad018,s7,Multimodal Harmful Content Detection,['p7.0'],"['In this section, we focus on the second aspect of disinformation: harmfulness. It is essential to filter or to flag online harmful content. The harmful content includes child abuse material, violent and extreme content, hate speech, graphic content, sexual content, and spam content (Banko et al., 2020). 3 In recent years, the ability to recognize harmful content within online communities has received a lot of attention by researchers (Pramanick et al., 2021a,b) and policymakers that aim to keep users safe in the digital world. Studies in this direction include detecting harmful contents in network science (Ribeiro et al., 2018), natural language processing (Waseem et al., 2017;Schmidt and Wiegand, 2017b;Fortuna and Nunes, 2018) and computer vision (Yang et al., 2019a;Vijayaraghavan et al., 2021;Gomez et al., 2020;Dimitrov et al., 2021b). In Table 2, we provide a list of relevant work addressing different types of harmful content, modalities, source of data, annotation approach, language of the content and the methods.']","In this section, we focus on the second aspect of disinformation: harmfulness. It is essential to filter or to flag online harmful content. The harmful content includes child abuse material, violent and extreme content, hate speech, graphic content, sexual content, and spam content (Banko et al., 2020). 3 In recent years, the ability to recognize harmful content within online communities has received a lot of attention by researchers (Pramanick et al., 2021a,b) and policymakers that aim to keep users safe in the digital world. Studies in this direction include detecting harmful contents in network science (Ribeiro et al., 2018), natural language processing (Waseem et al., 2017;Schmidt and Wiegand, 2017b;Fortuna and Nunes, 2018) and computer vision (Yang et al., 2019a;Vijayaraghavan et al., 2021;Gomez et al., 2020;Dimitrov et al., 2021b). In Table 2, we provide a list of relevant work addressing different types of harmful content, modalities, source of data, annotation approach, language of the content and the methods.","(p7.0) In this section, we focus on the second aspect of disinformation: harmfulness. It is essential to filter or to flag online harmful content. The harmful content includes child abuse material, violent and extreme content, hate speech, graphic content, sexual content, and spam content (Banko et al., 2020). 3 In recent years, the ability to recognize harmful content within online communities has received a lot of attention by researchers (Pramanick et al., 2021a,b) and policymakers that aim to keep users safe in the digital world. Studies in this direction include detecting harmful contents in network science (Ribeiro et al., 2018), natural language processing (Waseem et al., 2017;Schmidt and Wiegand, 2017b;Fortuna and Nunes, 2018) and computer vision (Yang et al., 2019a;Vijayaraghavan et al., 2021;Gomez et al., 2020;Dimitrov et al., 2021b). In Table 2, we provide a list of relevant work addressing different types of harmful content, modalities, source of data, annotation approach, language of the content and the methods.","[['b81', None, 'b47', 'b38', 'b68', 'b78', 'b44']]","[['b81', None, 'b47', 'b38', 'b68', 'b78', 'b44']]",7,"1. In this section, we focus on the second aspect of disinformation: harmfulness.
2. It is essential to filter or to flag online harmful content.
3. The harmful content includes child abuse material, violent and extreme content, hate speech, graphic content, sexual content, and spam content (Banko et al., 2020). 3
4. In recent years, the ability to recognize harmful content within online communities has received a lot of attention by researchers (Pramanick et al., 2021a,b) and policymakers that aim to keep users safe in the digital world.
5. Studies in this direction include detecting harmful contents in network science (Ribeiro et al., 2018), natural language processing (Waseem et al., 2017;Schmidt and Wiegand, 2017b;Fortuna and Nunes, 2018) and computer vision (Yang et al., 2019a;Vijayaraghavan et al., 2021;Gomez et al., 2020;Dimitrov et al., 2021b).
6. In Table 2, we provide a list of relevant work addressing different types of harmful content, modalities, source of data, annotation approach, language of the content and the methods."
232320384,A Survey on Multimodal Disinformation Detection,Computer Science,https://www.semanticscholar.org/paper/71d2dc1fc38e0c48c865de5f5c023ccf7c5ad018,s9,Image,"['p9.0', 'p9.1', 'p9.2']","['Among different types of harmful content, cyberbullying is one of the major growing problems, significantly affecting teens. Hosseinmardi et al. (2015) investigated Instagram images and their associated comments for detecting cyberbullying and online harassment. They developed a manually labeled dataset using CrowdFlower (which is now Appen), where they followed standard procedures for the annotation: using annotation guidelines, qualification tests, gold standard evaluation and quality control criteria such as minimum annotation time. The annotated dataset consists of 998 media sessions (images and their associated comments). A key finding of this study is that a large fraction of the annotated posts (48%) with a high percentage of negative words have not been labeled as cyberbullying. To train and to evaluate the model, the authors used n-grams from text, meta-data (e.g., the number of followers, followees, likes, and shared media), and image categories as features and experimented with Naïve Bayes and SVM classifiers. Their study suggests that combining multiple modalities helps to improve the performance of the SVM classifier.', 'Hate speech is another important problem that spreads over social media. The ""Hateful Memes Challenge"" is an important milestone to advance the research on this topic and the tasks is to detect hateful memes (Kiela et al., 2020). Das et al. (2020) proposed different approaches for hatefulness detection in memes such as (i) extract the caption and include this information with the multimodal model, (ii) use sentiment as an additional feature with multimodal representations. For hate speech detection, Yang et al. (2019a) explored different fusion techniques such as concatenation, bilinear, gated summation, and attention, and reported that combining the text with image embedding boosted the performance in all cases. Vijayaraghavan et al. (2021) proposed methods for interpreting multimodal hate speech detection models, where the modalities consist of text and socio-cultural information rather than images. Concurrently, Gomez et al. (2020) introduced a larger dataset of 150K tweets for multimodal hate speech detection, consisting of six labels.', 'Propaganda is another topic that has been explored in multimodal settings. Seo (2014) showed how Twitter was used as a propaganda tool during the 2012 Gaza conflict to build international support for each side of the conflict. Dimitrov et al. (2021b) addressed the detection of persuasion techniques in memes. Their analysis of the dataset showed that while propaganda is not always factually false or harmful, most memes are used to damage the reputation of a person or a group of people. Dimitrov et al. (2021a) highlighted the importance of both modalities for detecting finegrained propaganda techniques, with VisualBERT yielding 19% improvement compared to using the image modality only (with ResNet-152), and 11% improvement compared to using the text modality only (with BERT). Similar observations were made by (Kiela et al., 2020) for hateful meme detection. Glenski et al. (2019) explored multilingual multimodal content and categorizes disinformation, propaganda, conspiracy, hoax, and clickbait.']","Among different types of harmful content, cyberbullying is one of the major growing problems, significantly affecting teens. Hosseinmardi et al. (2015) investigated Instagram images and their associated comments for detecting cyberbullying and online harassment. They developed a manually labeled dataset using CrowdFlower (which is now Appen), where they followed standard procedures for the annotation: using annotation guidelines, qualification tests, gold standard evaluation and quality control criteria such as minimum annotation time. The annotated dataset consists of 998 media sessions (images and their associated comments). A key finding of this study is that a large fraction of the annotated posts (48%) with a high percentage of negative words have not been labeled as cyberbullying. To train and to evaluate the model, the authors used n-grams from text, meta-data (e.g., the number of followers, followees, likes, and shared media), and image categories as features and experimented with Naïve Bayes and SVM classifiers. Their study suggests that combining multiple modalities helps to improve the performance of the SVM classifier.

Hate speech is another important problem that spreads over social media. The ""Hateful Memes Challenge"" is an important milestone to advance the research on this topic and the tasks is to detect hateful memes (Kiela et al., 2020). Das et al. (2020) proposed different approaches for hatefulness detection in memes such as (i) extract the caption and include this information with the multimodal model, (ii) use sentiment as an additional feature with multimodal representations. For hate speech detection, Yang et al. (2019a) explored different fusion techniques such as concatenation, bilinear, gated summation, and attention, and reported that combining the text with image embedding boosted the performance in all cases. Vijayaraghavan et al. (2021) proposed methods for interpreting multimodal hate speech detection models, where the modalities consist of text and socio-cultural information rather than images. Concurrently, Gomez et al. (2020) introduced a larger dataset of 150K tweets for multimodal hate speech detection, consisting of six labels.

Propaganda is another topic that has been explored in multimodal settings. Seo (2014) showed how Twitter was used as a propaganda tool during the 2012 Gaza conflict to build international support for each side of the conflict. Dimitrov et al. (2021b) addressed the detection of persuasion techniques in memes. Their analysis of the dataset showed that while propaganda is not always factually false or harmful, most memes are used to damage the reputation of a person or a group of people. Dimitrov et al. (2021a) highlighted the importance of both modalities for detecting finegrained propaganda techniques, with VisualBERT yielding 19% improvement compared to using the image modality only (with ResNet-152), and 11% improvement compared to using the text modality only (with BERT). Similar observations were made by (Kiela et al., 2020) for hateful meme detection. Glenski et al. (2019) explored multilingual multimodal content and categorizes disinformation, propaganda, conspiracy, hoax, and clickbait.","(p9.0) Among different types of harmful content, cyberbullying is one of the major growing problems, significantly affecting teens. Hosseinmardi et al. (2015) investigated Instagram images and their associated comments for detecting cyberbullying and online harassment. They developed a manually labeled dataset using CrowdFlower (which is now Appen), where they followed standard procedures for the annotation: using annotation guidelines, qualification tests, gold standard evaluation and quality control criteria such as minimum annotation time. The annotated dataset consists of 998 media sessions (images and their associated comments). A key finding of this study is that a large fraction of the annotated posts (48%) with a high percentage of negative words have not been labeled as cyberbullying. To train and to evaluate the model, the authors used n-grams from text, meta-data (e.g., the number of followers, followees, likes, and shared media), and image categories as features and experimented with Naïve Bayes and SVM classifiers. Their study suggests that combining multiple modalities helps to improve the performance of the SVM classifier.

(p9.1) Hate speech is another important problem that spreads over social media. The ""Hateful Memes Challenge"" is an important milestone to advance the research on this topic and the tasks is to detect hateful memes (Kiela et al., 2020). Das et al. (2020) proposed different approaches for hatefulness detection in memes such as (i) extract the caption and include this information with the multimodal model, (ii) use sentiment as an additional feature with multimodal representations. For hate speech detection, Yang et al. (2019a) explored different fusion techniques such as concatenation, bilinear, gated summation, and attention, and reported that combining the text with image embedding boosted the performance in all cases. Vijayaraghavan et al. (2021) proposed methods for interpreting multimodal hate speech detection models, where the modalities consist of text and socio-cultural information rather than images. Concurrently, Gomez et al. (2020) introduced a larger dataset of 150K tweets for multimodal hate speech detection, consisting of six labels.

(p9.2) Propaganda is another topic that has been explored in multimodal settings. Seo (2014) showed how Twitter was used as a propaganda tool during the 2012 Gaza conflict to build international support for each side of the conflict. Dimitrov et al. (2021b) addressed the detection of persuasion techniques in memes. Their analysis of the dataset showed that while propaganda is not always factually false or harmful, most memes are used to damage the reputation of a person or a group of people. Dimitrov et al. (2021a) highlighted the importance of both modalities for detecting finegrained propaganda techniques, with VisualBERT yielding 19% improvement compared to using the image modality only (with ResNet-152), and 11% improvement compared to using the text modality only (with BERT). Similar observations were made by (Kiela et al., 2020) for hateful meme detection. Glenski et al. (2019) explored multilingual multimodal content and categorizes disinformation, propaganda, conspiracy, hoax, and clickbait.","[['b41'], [None, 'b2', 'b81', 'b68'], ['b49', 'b38', 'b2']]","[['b41'], [None, 'b2', 'b81', 'b68'], ['b49', 'b38', 'b2']]",8,"1. Among different types of harmful content, cyberbullying is one of the major growing problems, significantly affecting teens.
2. Hosseinmardi et al. (2015) investigated Instagram images and their associated comments for detecting cyberbullying and online harassment.
3. They developed a manually labeled dataset using CrowdFlower (which is now Appen), where they followed standard procedures for the annotation: using annotation guidelines, qualification tests, gold standard evaluation and quality control criteria such as minimum annotation time.
4. The annotated dataset consists of 998 media sessions (images and their associated comments).
5. A key finding of this study is that a large fraction of the annotated posts (48%) with a high percentage of negative words have not been labeled as cyberbullying.
6. To train and to evaluate the model, the authors used n-grams from text, meta-data (e.g., the number of followers, followees, likes, and shared media), and image categories as features and experimented with Naïve Bayes and SVM classifiers.
7. Their study suggests that combining multiple modalities helps to improve the performance of the SVM classifier.
8. Hate speech is another important problem that spreads over social media.
9. The ""Hateful Memes Challenge"" is an important milestone to advance the research on this topic and the tasks is to detect hateful memes (Kiela et al., 2020).
10. Das et al. (2020) proposed different approaches for hatefulness detection in memes such as (i) extract the caption and include this information with the multimodal model, (ii) use sentiment as an additional feature with multimodal representations.
11. For hate speech detection, Yang et al. (2019a) explored different fusion techniques such as concatenation, bilinear, gated summation, and attention, and reported that combining the text with image embedding boosted the performance in all cases.
12. Vijayaraghavan et al. (2021) proposed methods for interpreting multimodal hate speech detection models, where the modalities consist of text and socio-cultural information rather than images.
13. Concurrently, Gomez et al. (2020) introduced a larger dataset of 150K tweets for multimodal hate speech detection, consisting of six labels.
14. Propaganda is another topic that has been explored in multimodal settings.
15. Seo (2014) showed how Twitter was used as a propaganda tool during the 2012 Gaza conflict to build international support for each side of the conflict.
16. Dimitrov et al. (2021b) addressed the detection of persuasion techniques in memes.
17. Their analysis of the dataset showed that while propaganda is not always factually false or harmful, most memes are used to damage the reputation of a person or a group of people.
18. Dimitrov et al. (2021a) highlighted the importance of both modalities for detecting finegrained propaganda techniques, with VisualBERT yielding 19% improvement compared to using the image modality only (with ResNet-152), and 11% improvement compared to using the text modality only (with BERT).
19. Similar observations were made by (Kiela et al., 2020) for hateful meme detection.
20. Glenski et al. (2019) explored multilingual multimodal content and categorizes disinformation, propaganda, conspiracy, hoax, and clickbait."
232320384,A Survey on Multimodal Disinformation Detection,Computer Science,https://www.semanticscholar.org/paper/71d2dc1fc38e0c48c865de5f5c023ccf7c5ad018,s18,A Examples of Factuality and Harmful Content,"['p18.0', 'p18.1', 'p18.2', 'p18.3', 'p18.4', 'p18.5']","['In Figure 2, we provide examples textual and visual content that are harmful and false, true image with false claim, and harmful meme.', 'B Modeling Techniques Figure 4 shows various multimodal approaches that have been proposed in the literature.', 'C Lessons Learned 1. A lot of progress has been made on the problem, but the two components in the definition of disinformation (falseness and harmfulness) have been considered mostly in isolation. We argue that there is a need for tight integration of the factuality and the intentional harmfulness into the same detection model. These two aspects have been addressed together in (Alam et al., 2021), which shows that 56% of Arabic false content is also harmful. From Tables 1 and 2, we observe that most multimodal datasets cover just 2-3 modalities, which combine some approaches depicted in Figure 4. Moreover, no multimodal dataset looks at both aspects of disinformation: factuality and harmfulness. While Alam et al.', '(2021) did address both aspects, they only covered the text modality.', '2. In the early phase of (dis)information spreading, user and content features are those that provide the highest contribution for detecting factuality. Indeed, at that time, a few interactions with content are available and the propagation network is small and sparse. As information spreads, the contribution of content-derived features remains constant, while propagation-derived features become richer and more informative. In summary, early prediction of factuality and veracity must necessarily rely heavily on users and content -be it text, image, audio or video. Instead, analyses carried out at later times benefit more from network and temporal data. In the past decade, research on multimodality has shown its potential in several fields, which include audio-visual fusion (Mroueh et al., Figure 3: Example of social network with users. Node: A node can be a users or a spreader. Ego: ""Ego"" is an individual ""focal"" node (central user) and the nodes that are directly connected to it are called ""alters/spreaders."" Triad: It (a set of three connected users) is the most basic subgraph of the network. Community: A community structure refers to the occurrence of groups of nodes in a network that are more densely connected internally than with the rest of the network.  (Liu et al., 2021), multimedia retrieval and visual question answering (Summaira et al., 2021). For factuality, Baly et al. (2020) showed that combining different modalities such as text, speech, and metadata yields improved performance compared to using individual modalities. Similar phenomena have been observed for other tasks such as hateful memes (Kiela et al., 2020), and propaganda detection (Dimitrov et al., 2021b).', 'D More Challenges 1. Contextualization. Existing methods of disinformation detection are mostly noncontextualized, i.e., the broader context of a news article in terms of the responses of the readers and how the users perceive them are not captured. We argue that the response']","In Figure 2, we provide examples textual and visual content that are harmful and false, true image with false claim, and harmful meme.

B Modeling Techniques Figure 4 shows various multimodal approaches that have been proposed in the literature.

C Lessons Learned 1. A lot of progress has been made on the problem, but the two components in the definition of disinformation (falseness and harmfulness) have been considered mostly in isolation. We argue that there is a need for tight integration of the factuality and the intentional harmfulness into the same detection model. These two aspects have been addressed together in (Alam et al., 2021), which shows that 56% of Arabic false content is also harmful. From Tables 1 and 2, we observe that most multimodal datasets cover just 2-3 modalities, which combine some approaches depicted in Figure 4. Moreover, no multimodal dataset looks at both aspects of disinformation: factuality and harmfulness. While Alam et al.

(2021) did address both aspects, they only covered the text modality.

2. In the early phase of (dis)information spreading, user and content features are those that provide the highest contribution for detecting factuality. Indeed, at that time, a few interactions with content are available and the propagation network is small and sparse. As information spreads, the contribution of content-derived features remains constant, while propagation-derived features become richer and more informative. In summary, early prediction of factuality and veracity must necessarily rely heavily on users and content -be it text, image, audio or video. Instead, analyses carried out at later times benefit more from network and temporal data. In the past decade, research on multimodality has shown its potential in several fields, which include audio-visual fusion (Mroueh et al., Figure 3: Example of social network with users. Node: A node can be a users or a spreader. Ego: ""Ego"" is an individual ""focal"" node (central user) and the nodes that are directly connected to it are called ""alters/spreaders."" Triad: It (a set of three connected users) is the most basic subgraph of the network. Community: A community structure refers to the occurrence of groups of nodes in a network that are more densely connected internally than with the rest of the network.  (Liu et al., 2021), multimedia retrieval and visual question answering (Summaira et al., 2021). For factuality, Baly et al. (2020) showed that combining different modalities such as text, speech, and metadata yields improved performance compared to using individual modalities. Similar phenomena have been observed for other tasks such as hateful memes (Kiela et al., 2020), and propaganda detection (Dimitrov et al., 2021b).

D More Challenges 1. Contextualization. Existing methods of disinformation detection are mostly noncontextualized, i.e., the broader context of a news article in terms of the responses of the readers and how the users perceive them are not captured. We argue that the response","(p18.0) In Figure 2, we provide examples textual and visual content that are harmful and false, true image with false claim, and harmful meme.

(p18.1) B Modeling Techniques Figure 4 shows various multimodal approaches that have been proposed in the literature.

(p18.2) C Lessons Learned 1. A lot of progress has been made on the problem, but the two components in the definition of disinformation (falseness and harmfulness) have been considered mostly in isolation. We argue that there is a need for tight integration of the factuality and the intentional harmfulness into the same detection model. These two aspects have been addressed together in (Alam et al., 2021), which shows that 56% of Arabic false content is also harmful. From Tables 1 and 2, we observe that most multimodal datasets cover just 2-3 modalities, which combine some approaches depicted in Figure 4. Moreover, no multimodal dataset looks at both aspects of disinformation: factuality and harmfulness. While Alam et al.

(p18.3) (2021) did address both aspects, they only covered the text modality.

(p18.4) 2. In the early phase of (dis)information spreading, user and content features are those that provide the highest contribution for detecting factuality. Indeed, at that time, a few interactions with content are available and the propagation network is small and sparse. As information spreads, the contribution of content-derived features remains constant, while propagation-derived features become richer and more informative. In summary, early prediction of factuality and veracity must necessarily rely heavily on users and content -be it text, image, audio or video. Instead, analyses carried out at later times benefit more from network and temporal data. In the past decade, research on multimodality has shown its potential in several fields, which include audio-visual fusion (Mroueh et al., Figure 3: Example of social network with users. Node: A node can be a users or a spreader. Ego: ""Ego"" is an individual ""focal"" node (central user) and the nodes that are directly connected to it are called ""alters/spreaders."" Triad: It (a set of three connected users) is the most basic subgraph of the network. Community: A community structure refers to the occurrence of groups of nodes in a network that are more densely connected internally than with the rest of the network.  (Liu et al., 2021), multimedia retrieval and visual question answering (Summaira et al., 2021). For factuality, Baly et al. (2020) showed that combining different modalities such as text, speech, and metadata yields improved performance compared to using individual modalities. Similar phenomena have been observed for other tasks such as hateful memes (Kiela et al., 2020), and propaganda detection (Dimitrov et al., 2021b).

(p18.5) D More Challenges 1. Contextualization. Existing methods of disinformation detection are mostly noncontextualized, i.e., the broader context of a news article in terms of the responses of the readers and how the users perceive them are not captured. We argue that the response","[[], [], ['b51'], [], [None, 'b64', 'b2', 'b38'], []]","[[], [], ['b51'], [], [None, 'b64', 'b2', 'b38'], []]",5,"1. In Figure 2, we provide examples textual and visual content that are harmful and false, true image with false claim, and harmful meme.
2. B Modeling Techniques Figure 4 shows various multimodal approaches that have been proposed in the literature.
3. C Lessons Learned 1. A lot of progress has been made on the problem, but the two components in the definition of disinformation (falseness and harmfulness) have been considered mostly in isolation.
4. We argue that there is a need for tight integration of the factuality and the intentional harmfulness into the same detection model.
5. These two aspects have been addressed together in (Alam et al., 2021), which shows that 56% of Arabic false content is also harmful.
6. From Tables 1 and 2, we observe that most multimodal datasets cover just 2-3 modalities, which combine some approaches depicted in Figure 4.
7. Moreover, no multimodal dataset looks at both aspects of disinformation: factuality and harmfulness.
8. While Alam et al.(2021) did address both aspects, they only covered the text modality.
9. 2. In the early phase of (dis)information spreading, user and content features are those that provide the highest contribution for detecting factuality.
10. Indeed, at that time, a few interactions with content are available and the propagation network is small and sparse.
11. As information spreads, the contribution of content-derived features remains constant, while propagation-derived features become richer and more informative.
12. In summary, early prediction of factuality and veracity must necessarily rely heavily on users and content
13. -be it text, image, audio or video.
14. Instead, analyses carried out at later times benefit more from network and temporal data.
15. In the past decade, research on multimodality has shown its potential in several fields, which include audio-visual fusion (Mroueh et al., Figure 3: Example of social network with users. Node: A node can be a users or a spreader. Ego: ""Ego"" is an individual ""focal"" node (central user) and the nodes that are directly connected to it are called ""alters/spreaders.""
16. Triad: It (a set of three connected users) is the most basic subgraph of the network.
17. Community: A community structure refers to the occurrence of groups of nodes in a network that are more densely connected internally than with the rest of the network.
18. (Liu et al., 2021), multimedia retrieval and visual question answering (Summaira et al., 2021).
19. For factuality, Baly et al. (2020) showed that combining different modalities such as text, speech, and metadata yields improved performance compared to using individual modalities.
20. Similar phenomena have been observed for other tasks such as hateful memes (Kiela et al., 2020), and propaganda detection (Dimitrov et al., 2021b).
21. D More Challenges 1. Contextualization.
22. Existing methods of disinformation detection are mostly noncontextualized, i.e., the broader context of a news article in terms of the responses of the readers and how the users perceive them are not captured.
23. We argue that the response"
232320384,A Survey on Multimodal Disinformation Detection,Computer Science,https://www.semanticscholar.org/paper/71d2dc1fc38e0c48c865de5f5c023ccf7c5ad018,s8,Text,['p8.0'],"['In the past few years there has been significant research effort on detecting harmful content (e.g., hate speech) from social media posts (Van Hee et al., 2015;Waseem and Hovy, 2016;Waseem et al., 2017;Schmidt and Wiegand, 2017b). Waseem and Hovy (2016) developed a dataset of hate speech consisting of 16K tweets, and reported a baseline results using char-and word-ngrams and a logistic regression classifier. (Davidson et al., 2017) distinguished between hate speech, and offensive language. They developed a dataset of ∼24K labeled tweets with categories such as hate speech, offensive language and neither. Qian et al. (2018) took a different approach to classic hate speech classification. Instead of binary classes, they proposed 13 fine-grained hate categories such as nationalist, anti-immigrant, racist skinhead, among others, providing a dataset of tweets collected from 40 hate groups. Ribeiro et al. (2018) proposed an approach to find hateful users on Twitter. Mathew   users and 21M posts collected from Gab to understand the diffusion dynamics of hateful content. Their findings suggest that the posts from hateful user diffuse faster, wider, and have a greater outreach compared to the posts from non-hateful ones.']","In the past few years there has been significant research effort on detecting harmful content (e.g., hate speech) from social media posts (Van Hee et al., 2015;Waseem and Hovy, 2016;Waseem et al., 2017;Schmidt and Wiegand, 2017b). Waseem and Hovy (2016) developed a dataset of hate speech consisting of 16K tweets, and reported a baseline results using char-and word-ngrams and a logistic regression classifier. (Davidson et al., 2017) distinguished between hate speech, and offensive language. They developed a dataset of ∼24K labeled tweets with categories such as hate speech, offensive language and neither. Qian et al. (2018) took a different approach to classic hate speech classification. Instead of binary classes, they proposed 13 fine-grained hate categories such as nationalist, anti-immigrant, racist skinhead, among others, providing a dataset of tweets collected from 40 hate groups. Ribeiro et al. (2018) proposed an approach to find hateful users on Twitter. Mathew   users and 21M posts collected from Gab to understand the diffusion dynamics of hateful content. Their findings suggest that the posts from hateful user diffuse faster, wider, and have a greater outreach compared to the posts from non-hateful ones.","(p8.0) In the past few years there has been significant research effort on detecting harmful content (e.g., hate speech) from social media posts (Van Hee et al., 2015;Waseem and Hovy, 2016;Waseem et al., 2017;Schmidt and Wiegand, 2017b). Waseem and Hovy (2016) developed a dataset of hate speech consisting of 16K tweets, and reported a baseline results using char-and word-ngrams and a logistic regression classifier. (Davidson et al., 2017) distinguished between hate speech, and offensive language. They developed a dataset of ∼24K labeled tweets with categories such as hate speech, offensive language and neither. Qian et al. (2018) took a different approach to classic hate speech classification. Instead of binary classes, they proposed 13 fine-grained hate categories such as nationalist, anti-immigrant, racist skinhead, among others, providing a dataset of tweets collected from 40 hate groups. Ribeiro et al. (2018) proposed an approach to find hateful users on Twitter. Mathew   users and 21M posts collected from Gab to understand the diffusion dynamics of hateful content. Their findings suggest that the posts from hateful user diffuse faster, wider, and have a greater outreach compared to the posts from non-hateful ones.","[['b40', 'b79', 'b78', None, 'b47', 'b67', 'b44']]","[['b40', 'b79', 'b78', None, 'b47', 'b67', 'b44']]",7,"1. In the past few years there has been significant research effort on detecting harmful content (e.g., hate speech) from social media posts (Van Hee et al., 2015;Waseem and Hovy, 2016;Waseem et al., 2017;Schmidt and Wiegand, 2017b).
2. Waseem and Hovy (2016) developed a dataset of hate speech consisting of 16K tweets, and reported a baseline results using char-and word-ngrams and a logistic regression classifier.
3. (Davidson et al., 2017) distinguished between hate speech, and offensive language.
4. They developed a dataset of ∼24K labeled tweets with categories such as hate speech, offensive language and neither.
5. Qian et al. (2018) took a different approach to classic hate speech classification.
6. Instead of binary classes, they proposed 13 fine-grained hate categories such as nationalist, anti-immigrant, racist skinhead, among others, providing a dataset of tweets collected from 40 hate groups.
7. Ribeiro et al. (2018) proposed an approach to find hateful users on Twitter.
8. Mathew   users and 21M posts collected from Gab to understand the diffusion dynamics of hateful content.
9. Their findings suggest that the posts from hateful user diffuse faster, wider, and have a greater outreach compared to the posts from non-hateful ones."
232320384,A Survey on Multimodal Disinformation Detection,Computer Science,https://www.semanticscholar.org/paper/71d2dc1fc38e0c48c865de5f5c023ccf7c5ad018,s10,Speech/Audio,"['p10.0', 'p10.1', 'p10.2']","['Cues in spoken content can represent harmful behaviors and those cues can be used to automatically detect such content. Due to the lack of data, studies using the speech-only modality are comparatively lower than other modalities even though it plays a major role in many contexts. For example, for detecting violent content such as screaming and gunshots, the speech modality can play an important role, which other modalities might not be able to offer. This is important as most often user-generated contents are posted on newspapers or their social media accounts without verifying the content of the post, which can have serious consequences (Harkin et al., 2012;Rauchfleisch et al., 2017).', 'Giannakopoulos (2009) studied the audio segmentation approaches for segmenting violent (e.g., gunshots, screams) and non-violent (e.g., music, speech) content in movies. The studies related to violent content detection using acoustic features also include (Acar et al., 2013), where the focus was on finding violent content in movies. Liang et al. (2017) proposed Localized Self-Paced Reranking (LSPaR) for detecting gunshots and explosion in videos using acoustic features. Soni and Singh (2018) investigated audio, visual and textual features for cyberbullying detection.', 'Their findings suggest that audio and visual features are associated with the occurrence of cyberbullying, and both these features complement textual features.']","Cues in spoken content can represent harmful behaviors and those cues can be used to automatically detect such content. Due to the lack of data, studies using the speech-only modality are comparatively lower than other modalities even though it plays a major role in many contexts. For example, for detecting violent content such as screaming and gunshots, the speech modality can play an important role, which other modalities might not be able to offer. This is important as most often user-generated contents are posted on newspapers or their social media accounts without verifying the content of the post, which can have serious consequences (Harkin et al., 2012;Rauchfleisch et al., 2017).

Giannakopoulos (2009) studied the audio segmentation approaches for segmenting violent (e.g., gunshots, screams) and non-violent (e.g., music, speech) content in movies. The studies related to violent content detection using acoustic features also include (Acar et al., 2013), where the focus was on finding violent content in movies. Liang et al. (2017) proposed Localized Self-Paced Reranking (LSPaR) for detecting gunshots and explosion in videos using acoustic features. Soni and Singh (2018) investigated audio, visual and textual features for cyberbullying detection.

Their findings suggest that audio and visual features are associated with the occurrence of cyberbullying, and both these features complement textual features.","(p10.0) Cues in spoken content can represent harmful behaviors and those cues can be used to automatically detect such content. Due to the lack of data, studies using the speech-only modality are comparatively lower than other modalities even though it plays a major role in many contexts. For example, for detecting violent content such as screaming and gunshots, the speech modality can play an important role, which other modalities might not be able to offer. This is important as most often user-generated contents are posted on newspapers or their social media accounts without verifying the content of the post, which can have serious consequences (Harkin et al., 2012;Rauchfleisch et al., 2017).

(p10.1) Giannakopoulos (2009) studied the audio segmentation approaches for segmenting violent (e.g., gunshots, screams) and non-violent (e.g., music, speech) content in movies. The studies related to violent content detection using acoustic features also include (Acar et al., 2013), where the focus was on finding violent content in movies. Liang et al. (2017) proposed Localized Self-Paced Reranking (LSPaR) for detecting gunshots and explosion in videos using acoustic features. Soni and Singh (2018) investigated audio, visual and textual features for cyberbullying detection.

(p10.2) Their findings suggest that audio and visual features are associated with the occurrence of cyberbullying, and both these features complement textual features.","[[None, 'b43'], ['b63', 'b13', None], []]","[[None, 'b43'], ['b63', 'b13', None], []]",5,"1. Cues in spoken content can represent harmful behaviors and those cues can be used to automatically detect such content.
2. Due to the lack of data, studies using the speech-only modality are comparatively lower than other modalities even though it plays a major role in many contexts.
3. For example, for detecting violent content such as screaming and gunshots, the speech modality can play an important role, which other modalities might not be able to offer.
4. This is important as most often user-generated contents are posted on newspapers or their social media accounts without verifying the content of the post, which can have serious consequences (Harkin et al., 2012;Rauchfleisch et al., 2017).Giannakopoulos (2009) studied the audio segmentation approaches for segmenting violent (e.g., gunshots, screams) and non-violent (e.g., music, speech) content in movies.
5. The studies related to violent content detection using acoustic features also include (Acar et al., 2013), where the focus was on finding violent content in movies.
6. Liang et al. (2017) proposed Localized Self-Paced Reranking (LSPaR) for detecting gunshots and explosion in videos using acoustic features.
7. Soni and Singh (2018) investigated audio, visual and textual features for cyberbullying detection.
8. Their findings suggest that audio and visual features are associated with the occurrence of cyberbullying, and both these features complement textual features."
232320384,A Survey on Multimodal Disinformation Detection,Computer Science,https://www.semanticscholar.org/paper/71d2dc1fc38e0c48c865de5f5c023ccf7c5ad018,s14,Major Challenges,"['p14.0', 'p14.1']","['Recently, several initiatives were undertaken by major companies and government entities to combat disinformation in social media (DIGI, 2021). 5 However, automatic detection of misleading and harmful content poses a number of challenges as discussed below and in Appendix (Section D).', 'Models Combining Multiple Modalities. The major challenge is to devise a mechanism to combine multiple modalities in a systematic way so that one modality complements the others. Current stateof-the-art primarily adopts early and late fusion, which are limited and do not always yield strong results (Dimitrov et al., 2021a). Very recently, jointly trained multimodal transformer-based models (e.g., ViLBERT (Lu et al., 2019), Visual BERT (Lin et al., 2014) and Multimodal Bitransformers (MMBT) (Kiela et al., 2019)) have shown strong potential (Dimitrov et al., 2021b,a;Kiela et al., 2020). However, such models are trained considering only two modalities (textual and visual), while fact-checking or disinformation-related content consists of more than two modalities e.g., text, speech, video, network, etc. (Baly et al., 2020). Hence, there is a room for improvement in developing multimodal models that involve additional, and potentially more than two modalities. Another important problem is cross-modal inconsistency in social media content, as shown in Figure 2(c), which poses a challenge in a multimodal setting (Tan et al., 2020). Datasets. One of the major challenges when working with such diverse modalities, i.e., text, image, speech, video, and network, is to get access to an appropriate dataset, and moreover to one that considers both factuality and harmfulness. Furthermore, there is a need to integrate data from multiple platforms (e.g., news, posts from Twitter, Reddit and Instagram) as different data sources present different styles and focus on different topics.']","Recently, several initiatives were undertaken by major companies and government entities to combat disinformation in social media (DIGI, 2021). 5 However, automatic detection of misleading and harmful content poses a number of challenges as discussed below and in Appendix (Section D).

Models Combining Multiple Modalities. The major challenge is to devise a mechanism to combine multiple modalities in a systematic way so that one modality complements the others. Current stateof-the-art primarily adopts early and late fusion, which are limited and do not always yield strong results (Dimitrov et al., 2021a). Very recently, jointly trained multimodal transformer-based models (e.g., ViLBERT (Lu et al., 2019), Visual BERT (Lin et al., 2014) and Multimodal Bitransformers (MMBT) (Kiela et al., 2019)) have shown strong potential (Dimitrov et al., 2021b,a;Kiela et al., 2020). However, such models are trained considering only two modalities (textual and visual), while fact-checking or disinformation-related content consists of more than two modalities e.g., text, speech, video, network, etc. (Baly et al., 2020). Hence, there is a room for improvement in developing multimodal models that involve additional, and potentially more than two modalities. Another important problem is cross-modal inconsistency in social media content, as shown in Figure 2(c), which poses a challenge in a multimodal setting (Tan et al., 2020). Datasets. One of the major challenges when working with such diverse modalities, i.e., text, image, speech, video, and network, is to get access to an appropriate dataset, and moreover to one that considers both factuality and harmfulness. Furthermore, there is a need to integrate data from multiple platforms (e.g., news, posts from Twitter, Reddit and Instagram) as different data sources present different styles and focus on different topics.","(p14.0) Recently, several initiatives were undertaken by major companies and government entities to combat disinformation in social media (DIGI, 2021). 5 However, automatic detection of misleading and harmful content poses a number of challenges as discussed below and in Appendix (Section D).

(p14.1) Models Combining Multiple Modalities. The major challenge is to devise a mechanism to combine multiple modalities in a systematic way so that one modality complements the others. Current stateof-the-art primarily adopts early and late fusion, which are limited and do not always yield strong results (Dimitrov et al., 2021a). Very recently, jointly trained multimodal transformer-based models (e.g., ViLBERT (Lu et al., 2019), Visual BERT (Lin et al., 2014) and Multimodal Bitransformers (MMBT) (Kiela et al., 2019)) have shown strong potential (Dimitrov et al., 2021b,a;Kiela et al., 2020). However, such models are trained considering only two modalities (textual and visual), while fact-checking or disinformation-related content consists of more than two modalities e.g., text, speech, video, network, etc. (Baly et al., 2020). Hence, there is a room for improvement in developing multimodal models that involve additional, and potentially more than two modalities. Another important problem is cross-modal inconsistency in social media content, as shown in Figure 2(c), which poses a challenge in a multimodal setting (Tan et al., 2020). Datasets. One of the major challenges when working with such diverse modalities, i.e., text, image, speech, video, and network, is to get access to an appropriate dataset, and moreover to one that considers both factuality and harmfulness. Furthermore, there is a need to integrate data from multiple platforms (e.g., news, posts from Twitter, Reddit and Instagram) as different data sources present different styles and focus on different topics.","[[None], ['b37', 'b65', 'b14', 'b2', None, 'b17', 'b1']]","[[None], ['b37', 'b65', 'b14', 'b2', None, 'b17', 'b1']]",8,"1. Recently, several initiatives were undertaken by major companies and government entities to combat disinformation in social media (DIGI, 2021). 5
2. However, automatic detection of misleading and harmful content poses a number of challenges as discussed below and in Appendix (Section D).Models Combining Multiple Modalities.
3. The major challenge is to devise a mechanism to combine multiple modalities in a systematic way so that one modality complements the others.
4. Current stateof-the-art primarily adopts early and late fusion, which are limited and do not always yield strong results (Dimitrov et al., 2021a).
5. Very recently, jointly trained multimodal transformer-based models (e.g., ViLBERT (Lu et al., 2019), Visual BERT (Lin et al., 2014) and Multimodal Bitransformers (MMBT) (Kiela et al., 2019)) have shown strong potential (Dimitrov et al., 2021b,a;Kiela et al., 2020).
6. However, such models are trained considering only two modalities (textual and visual), while fact-checking or disinformation-related content consists of more than two modalities e.g., text, speech, video, network, etc.
7. (Baly et al., 2020). Hence, there is a room for improvement in developing multimodal models that involve additional, and potentially more than two modalities.
8. Another important problem is cross-modal inconsistency in social media content, as shown in Figure 2(c), which poses a challenge in a multimodal setting (Tan et al., 2020).
9. Datasets. One of the major challenges when working with such diverse modalities, i.e., text, image, speech, video, and network, is to get access to an appropriate dataset, and moreover to one that considers both factuality and harmfulness.
10. Furthermore, there is a need to integrate data from multiple platforms (e.g., news, posts from Twitter, Reddit and Instagram) as different data sources present different styles and focus on different topics."
233476148,Explanation-Based Human Debugging of NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/d84ed05ab860b75f9e6b28e717abf4bc12da03d7,s3,Tasks,['p3.0'],"['Most papers in Table 1 focus on text classification with single input (TC) for a variety of specific problems such as e-mail categorization , topic classification (Kulesza et al., 2015;Teso and Kersting, 2019), spam classification (Koh and Liang, 2017), sentiment analysis (Ribeiro et al., 2018b) and auto-coding of transcripts (Kulesza et al., 2010). By contrast, Zylberajch et al. (2021) targeted natural language inference (NLI) which is a type of text-pair classification, predicting whether a given premise entails a given hypothesis. . Ghai et al. (2021) suggested that most researchers work on TC because, for this task, it is much easier for lay participants to understand explanations and give feedback (e.g., which keywords should be added or removed from the list of top features) 4 . Meanwhile, some other NLP tasks require the feedback providers to have linguistic knowledge such as part-of-speech tagging, parsing, and machine translation. The need for linguists or experts renders experiments for these tasks more difficult and costly. However, we suggest that there are several tasks where the trained models are prone to be buggy but the tasks are underexplored in the EBHD setting, though they are not too difficult to experiment on with lay people. NLI, the focus of (Zylberajch et al., 2021), is one of them. Indeed, McCoy et al. (2019) and Gururangan et al. (2018) showed that NLI models can exploit annotation artifacts and fallible syntactic heuristics to make predictions rather than learning the logic of the actual task. Other tasks and their bugs include: QA, where Ribeiro et al. (2019) found that the answers from models are sometimes inconsistent (i.e., contradicting previous answers); and reading comprehension, where Jia and Liang (2017) showed that models, which answer a question by reading a given paragraph, can be fooled by an irrelevant sentence being appended to the paragraph. These non-TC NLP tasks would be worth exploring further in the EBHD setting.']","Most papers in Table 1 focus on text classification with single input (TC) for a variety of specific problems such as e-mail categorization , topic classification (Kulesza et al., 2015;Teso and Kersting, 2019), spam classification (Koh and Liang, 2017), sentiment analysis (Ribeiro et al., 2018b) and auto-coding of transcripts (Kulesza et al., 2010). By contrast, Zylberajch et al. (2021) targeted natural language inference (NLI) which is a type of text-pair classification, predicting whether a given premise entails a given hypothesis. . Ghai et al. (2021) suggested that most researchers work on TC because, for this task, it is much easier for lay participants to understand explanations and give feedback (e.g., which keywords should be added or removed from the list of top features) 4 . Meanwhile, some other NLP tasks require the feedback providers to have linguistic knowledge such as part-of-speech tagging, parsing, and machine translation. The need for linguists or experts renders experiments for these tasks more difficult and costly. However, we suggest that there are several tasks where the trained models are prone to be buggy but the tasks are underexplored in the EBHD setting, though they are not too difficult to experiment on with lay people. NLI, the focus of (Zylberajch et al., 2021), is one of them. Indeed, McCoy et al. (2019) and Gururangan et al. (2018) showed that NLI models can exploit annotation artifacts and fallible syntactic heuristics to make predictions rather than learning the logic of the actual task. Other tasks and their bugs include: QA, where Ribeiro et al. (2019) found that the answers from models are sometimes inconsistent (i.e., contradicting previous answers); and reading comprehension, where Jia and Liang (2017) showed that models, which answer a question by reading a given paragraph, can be fooled by an irrelevant sentence being appended to the paragraph. These non-TC NLP tasks would be worth exploring further in the EBHD setting.","(p3.0) Most papers in Table 1 focus on text classification with single input (TC) for a variety of specific problems such as e-mail categorization , topic classification (Kulesza et al., 2015;Teso and Kersting, 2019), spam classification (Koh and Liang, 2017), sentiment analysis (Ribeiro et al., 2018b) and auto-coding of transcripts (Kulesza et al., 2010). By contrast, Zylberajch et al. (2021) targeted natural language inference (NLI) which is a type of text-pair classification, predicting whether a given premise entails a given hypothesis. . Ghai et al. (2021) suggested that most researchers work on TC because, for this task, it is much easier for lay participants to understand explanations and give feedback (e.g., which keywords should be added or removed from the list of top features) 4 . Meanwhile, some other NLP tasks require the feedback providers to have linguistic knowledge such as part-of-speech tagging, parsing, and machine translation. The need for linguists or experts renders experiments for these tasks more difficult and costly. However, we suggest that there are several tasks where the trained models are prone to be buggy but the tasks are underexplored in the EBHD setting, though they are not too difficult to experiment on with lay people. NLI, the focus of (Zylberajch et al., 2021), is one of them. Indeed, McCoy et al. (2019) and Gururangan et al. (2018) showed that NLI models can exploit annotation artifacts and fallible syntactic heuristics to make predictions rather than learning the logic of the actual task. Other tasks and their bugs include: QA, where Ribeiro et al. (2019) found that the answers from models are sometimes inconsistent (i.e., contradicting previous answers); and reading comprehension, where Jia and Liang (2017) showed that models, which answer a question by reading a given paragraph, can be fooled by an irrelevant sentence being appended to the paragraph. These non-TC NLP tasks would be worth exploring further in the EBHD setting.","[['b77', 'b37', 'b67', 'b48', None, 'b22', 'b23']]","[['b77', 'b37', 'b67', 'b48', None, 'b22', 'b23']]",7,"1. Most papers in Table 1 focus on text classification with single input (TC) for a variety of specific problems such as e-mail categorization , topic classification (Kulesza et al., 2015;Teso and Kersting, 2019), spam classification (Koh and Liang, 2017), sentiment analysis (Ribeiro et al., 2018b) and auto-coding of transcripts (Kulesza et al., 2010).
2. By contrast, Zylberajch et al. (2021) targeted natural language inference (NLI) which is a type of text-pair classification, predicting whether a given premise entails a given hypothesis. .
3. Ghai et al. (2021) suggested that most researchers work on TC because, for this task, it is much easier for lay participants to understand explanations and give feedback (e.g., which keywords should be added or removed from the list of top features) 4 .
4. Meanwhile, some other NLP tasks require the feedback providers to have linguistic knowledge such as part-of-speech tagging, parsing, and machine translation.
5. The need for linguists or experts renders experiments for these tasks more difficult and costly.
6. However, we suggest that there are several tasks where the trained models are prone to be buggy but the tasks are underexplored in the EBHD setting, though they are not too difficult to experiment on with lay people.
7. NLI, the focus of (Zylberajch et al., 2021), is one of them.
8. Indeed, McCoy et al. (2019) and Gururangan et al. (2018) showed that NLI models can exploit annotation artifacts and fallible syntactic heuristics to make predictions rather than learning the logic of the actual task.
9. Other tasks and their bugs include: QA, where Ribeiro et al. (2019) found that the answers from models are sometimes inconsistent (i.e., contradicting previous answers); and reading comprehension, where Jia and Liang (2017) showed that models, which answer a question by reading a given paragraph, can be fooled by an irrelevant sentence being appended to the paragraph.
10. These non-TC NLP tasks would be worth exploring further in the EBHD setting."
233476148,Explanation-Based Human Debugging of NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/d84ed05ab860b75f9e6b28e717abf4bc12da03d7,s4,Models,['p4.0'],"['Early work used Naive Bayes models with bag-ofwords (NB) as text classifiers (Kulesza et al., 2009(Kulesza et al., , 2010, which are relatively easy to generate explanations for and to incorporate human feedback into (discussed in section 2.2). Other traditional models used include logistic regression (LR) (Teso and Kersting, 2019;Han and Ghosh, 2020)  (2019), respectively. While the NLP community nowadays is mainly driven by pre-trained language models (Qiu et al., 2020) with many papers studying their behaviors (Rogers et al., 2021;Hoover et al., 2020), only Zylberajch et al. (2021) and Yao et al. (2021) have used pre-trained language models, including BERT (Devlin et al., 2019) and RoBERTa , as test beds for EBHD.']","Early work used Naive Bayes models with bag-ofwords (NB) as text classifiers (Kulesza et al., 2009(Kulesza et al., , 2010, which are relatively easy to generate explanations for and to incorporate human feedback into (discussed in section 2.2). Other traditional models used include logistic regression (LR) (Teso and Kersting, 2019;Han and Ghosh, 2020)  (2019), respectively. While the NLP community nowadays is mainly driven by pre-trained language models (Qiu et al., 2020) with many papers studying their behaviors (Rogers et al., 2021;Hoover et al., 2020), only Zylberajch et al. (2021) and Yao et al. (2021) have used pre-trained language models, including BERT (Devlin et al., 2019) and RoBERTa , as test beds for EBHD.","(p4.0) Early work used Naive Bayes models with bag-ofwords (NB) as text classifiers (Kulesza et al., 2009(Kulesza et al., , 2010, which are relatively easy to generate explanations for and to incorporate human feedback into (discussed in section 2.2). Other traditional models used include logistic regression (LR) (Teso and Kersting, 2019;Han and Ghosh, 2020)  (2019), respectively. While the NLP community nowadays is mainly driven by pre-trained language models (Qiu et al., 2020) with many papers studying their behaviors (Rogers et al., 2021;Hoover et al., 2020), only Zylberajch et al. (2021) and Yao et al. (2021) have used pre-trained language models, including BERT (Devlin et al., 2019) and RoBERTa , as test beds for EBHD.","[['b77', 'b72', 'b54', None, 'b47', 'b67', 'b24', 'b23']]","[['b77', 'b72', 'b54', None, 'b47', 'b67', 'b24', 'b23']]",8,"1. Early work used Naive Bayes models with bag-ofwords (NB) as text classifiers (Kulesza et al., 2009(Kulesza et al., , 2010, which are relatively easy to generate explanations for and to incorporate human feedback into (discussed in section 2.2).
2. Other traditional models used include logistic regression (LR) (Teso and Kersting, 2019;Han and Ghosh, 2020)  (2019), respectively.
3. While the NLP community nowadays is mainly driven by pre-trained language models (Qiu et al., 2020) with many papers studying their behaviors (Rogers et al., 2021;Hoover et al., 2020), only Zylberajch et al. (2021) and Yao et al. (2021) have used pre-trained language models, including BERT (Devlin et al., 2019) and RoBERTa , as test beds for EBHD."
233476148,Explanation-Based Human Debugging of NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/d84ed05ab860b75f9e6b28e717abf4bc12da03d7,s5,Bug Sources,"['p5.0', 'p5.1']","[""Most of the papers in Table 1 experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs (i.e., the input texts having signals which are correlated to but not the reasons for specific outputs) and undermine models' generalizability. Out of the 15 papers we surveyed, 5 used the 20Newsgroups dataset (Lang, 1995) as a case study, since it has lots of natural artifacts. For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions. However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when we apply the model to texts in the wild. Apart from classification performance drops, natural artifacts can also cause model biases, as shown in (De-Arteaga et al., 2019;Park et al., 2018) and debugged in (Lertvittayakumjorn et al., 2020;Yao et al., 2021)."", 'In the absence of strong natural artifacts, bugs can still be simulated using several techniques. First, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlation leading to poor performance (Kulesza et al., 2010). Second, injecting wrong labels (WL) into the training data can obviously blunt the model quality (Koh and Liang, 2017). Third, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on (Lertvittayakumjorn et al., 2020;Yao et al., 2021). All of these techniques give rise to undesirable model behaviors, requiring debugging. Another technique, not found in Table 1 but suggested in related work (Idahl et al., 2021), is contaminating input texts in the training data with decoys (i.e., injected artifacts) which could deceive the model into predicting for the wrong reasons. This has been experimented with in the computer vision domain (Rieger et al., 2020), and its use in the EBHD setting in NLP could be an interesting direction to explore.']","Most of the papers in Table 1 experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs (i.e., the input texts having signals which are correlated to but not the reasons for specific outputs) and undermine models' generalizability. Out of the 15 papers we surveyed, 5 used the 20Newsgroups dataset (Lang, 1995) as a case study, since it has lots of natural artifacts. For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions. However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when we apply the model to texts in the wild. Apart from classification performance drops, natural artifacts can also cause model biases, as shown in (De-Arteaga et al., 2019;Park et al., 2018) and debugged in (Lertvittayakumjorn et al., 2020;Yao et al., 2021).

In the absence of strong natural artifacts, bugs can still be simulated using several techniques. First, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlation leading to poor performance (Kulesza et al., 2010). Second, injecting wrong labels (WL) into the training data can obviously blunt the model quality (Koh and Liang, 2017). Third, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on (Lertvittayakumjorn et al., 2020;Yao et al., 2021). All of these techniques give rise to undesirable model behaviors, requiring debugging. Another technique, not found in Table 1 but suggested in related work (Idahl et al., 2021), is contaminating input texts in the training data with decoys (i.e., injected artifacts) which could deceive the model into predicting for the wrong reasons. This has been experimented with in the computer vision domain (Rieger et al., 2020), and its use in the EBHD setting in NLP could be an interesting direction to explore.","(p5.0) Most of the papers in Table 1 experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs (i.e., the input texts having signals which are correlated to but not the reasons for specific outputs) and undermine models' generalizability. Out of the 15 papers we surveyed, 5 used the 20Newsgroups dataset (Lang, 1995) as a case study, since it has lots of natural artifacts. For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions. However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when we apply the model to texts in the wild. Apart from classification performance drops, natural artifacts can also cause model biases, as shown in (De-Arteaga et al., 2019;Park et al., 2018) and debugged in (Lertvittayakumjorn et al., 2020;Yao et al., 2021).

(p5.1) In the absence of strong natural artifacts, bugs can still be simulated using several techniques. First, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlation leading to poor performance (Kulesza et al., 2010). Second, injecting wrong labels (WL) into the training data can obviously blunt the model quality (Koh and Liang, 2017). Third, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on (Lertvittayakumjorn et al., 2020;Yao et al., 2021). All of these techniques give rise to undesirable model behaviors, requiring debugging. Another technique, not found in Table 1 but suggested in related work (Idahl et al., 2021), is contaminating input texts in the training data with decoys (i.e., injected artifacts) which could deceive the model into predicting for the wrong reasons. This has been experimented with in the computer vision domain (Rieger et al., 2020), and its use in the EBHD setting in NLP could be an interesting direction to explore.","[['b28', 'b30', 'b72', None, 'b43'], ['b53', 'b30', 'b72', None, 'b19', 'b23']]","[['b28', 'b30', 'b72', None, 'b43'], ['b53', 'b30', 'b72', None, 'b19', 'b23']]",11,"1. Most of the papers in Table 1 experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs (i.e., the input texts having signals which are correlated to but not the reasons for specific outputs) and undermine models' generalizability.
2. Out of the 15 papers we surveyed, 5 used the 20Newsgroups dataset (Lang, 1995) as a case study, since it has lots of natural artifacts.
3. For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions.
4. However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when we apply the model to texts in the wild.
5. Apart from classification performance drops, natural artifacts can also cause model biases, as shown in (De-Arteaga et al., 2019;Park et al., 2018) and debugged in (Lertvittayakumjorn et al., 2020;Yao et al., 2021).
6. In the absence of strong natural artifacts, bugs can still be simulated using several techniques.
7. First, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlation leading to poor performance (Kulesza et al., 2010).
8. Second, injecting wrong labels (WL) into the training data can obviously blunt the model quality (Koh and Liang, 2017).
9. Third, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on (Lertvittayakumjorn et al., 2020;Yao et al., 2021).
10. All of these techniques give rise to undesirable model behaviors, requiring debugging.
11. Another technique, not found in Table 1 but suggested in related work (Idahl et al., 2021), is contaminating input texts in the training data with decoys (i.e., injected artifacts) which could deceive the model into predicting for the wrong reasons.
12. This has been experimented with in the computer vision domain (Rieger et al., 2020), and its use in the EBHD setting in NLP could be an interesting direction to explore."
233476148,Explanation-Based Human Debugging of NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/d84ed05ab860b75f9e6b28e717abf4bc12da03d7,s7,Providing Explanations,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4', 'p7.5', 'p7.6']","['The main role of explanations here is to provide interpretable insights into the model and uncover its potential misbehavior or irrationality, which sometimes cannot be noticed by looking at the model outputs or the evaluation metrics.', ""Explanation scopes. Basically, there are two main types of explanations that could be provided to feedback providers. Local explanations (L) explain the predictions by the model for individual inputs. In contrast, global explanations (G) explain the model overall, independently of any specific inputs. It can be seen from Table 1 that most existing work use local explanations. One reason for this may be that, for complex models, global explanations can hardly reveal details of the models' inner workings in a comprehensible way to users. So, some bugs are imperceptible in such highlevel global explanations and then not corrected by the users. For example, the debugging framework FIND, proposed by Lertvittayakumjorn et al. (2020), uses only global explanations, and it was shown to work more effectively on significant bugs (such as gender bias in abusive language detection) than on less-obvious bugs (such as dataset shift between product types of sentiment analysis on product reviews). Otherwise, Ribeiro et al. (2018b) presented adversarial replacement rules as global explanations to reveal the model weaknesses only, without explaining how the whole model worked."", 'On the other hand, using local explanations has limitations in that it demands a large amount of effort from feedback providers to inspect the explanation of every single example in the training/validation set. With limited human resources, efficient ways to rank or select examples to explain would be required (Idahl et al., 2021). Recently, some work in explainable AI considers generating explanations for a group of predictions (Johnson et al., 2020; Chan et al., 2020) (e.g., for all the false positives of a certain class), thus staying in the middle of the two extreme explanation types (i.e., local and global). This kind of explanation is not too fine-grained, yet it can capture some suspicious model behaviors if we target the right group of examples. So, it would be worth studying in the context of EBHD (to the best of our knowledge, no existing study experiments with it).', 'Generating explanations. To generate explanations in general, there are two important questions we need to answer. First, which format should the explanations have? Second, how do we generate the explanations?', 'For the first question, we see many possible answers in the literature of explainable NLP (e.g., see the survey by Danilevsky et al. (2020)). For instance, input-based explanations (so called feature importance explanations) identify parts of the input that are important for the prediction. The explanation could be a list of importance scores of words in the input, so called attribution scores or relevance scores (Lundberg and Lee, 2017; Arras et al., 2016). Example-based explanations select influential, important, or similar examples from the training set to explain why the model makes a specific prediction (Han et al., 2020;Guo et al., 2020). Rule-based explanations provide interpretable decision rules that approximate the prediction process (Ribeiro et al., 2018a). Adversarial-based explanations return the smallest changes in the inputs that could change the predictions, revealing the model misbehavior (Zhang et al., 2020a). In most NLP tasks, inputbased explanations are the most popular approach for explaining predictions (Bhatt et al., 2020). This is also the case for EBHD as most selected studies use input-based explanations (Kulesza et al., 2009(Kulesza et al., , 2010Teso and Kersting, 2019;Cho et al. For the second question, there are two ways to generate the explanations: self-explaining methods and post-hoc explanation methods. Some models, e.g., Naive Bayes, logistic regression, and decision trees, are self-explaining (SE) (Danilevsky et al., 2020), also referred to as transparent (Adadi and Berrada, 2018) or inherently interpretable (Rudin, 2019). Local explanations of self-explaining models can be obtained at the same time as predictions, usually from the process of making those predictions, while the models themselves can often serve directly as global explanations. For example, feature importance explanations for a Naive Bayes model can be directly derived from the likelihood terms in the Naive Bayes equation, as done by several papers in Table 1 (Kulesza et al., 2009;Smith-Renner et al., 2020). Also, using attention scores on input as explanations, as done in (Cho et al., 2019), is a self-explaining method because the scores were obtained during the prediction process.', 'In contrast, post-hoc explanation methods (PH) perform additional steps to extract explanations after the model is trained (for a global explanation) or after the prediction is made (for a local explanation). If the method is allowed to access model parameters, it may calculate word relevance scores by propagating the output scores back to the input words (Arras et al., 2016) or analyzing the derivative of the output with respect to the input words (Smilkov et al., 2017;Sundararajan et al., 2017). If the method cannot access the model parameters, it may perturb the input and see how the output changes to estimate the importance of the altered parts of the input (Ribeiro et al., 2016;Jin et al., 2020). The important words and/or the relevance scores can be presented to the feedback providers in the EBHD workflow in many forms such as a list of words and their scores (Teso and Kersting, 2019;Ribeiro et al., 2016), word clouds (Lertvittayakumjorn et al., 2020), and a parse tree (Yao et al., 2021). Meanwhile, the influence functions method, used in (Koh and Liang, 2017;Zylberajch et al., 2021), identifies training examples which influence the prediction by analyzing how the prediction would change if we did not have each training point. This is another post-hoc explanation method as it takes place after prediction. It is similar to the other two example-based explanation methods used in (Khanna et al., 2019;Han and Ghosh, 2020).', 'Presenting explanations. It is important to carefully design the presentation of explanations, taking into consideration the background knowledge, desires, and limits of the feedback providers. In the debugging application by Kulesza et al. (2009), lay users were asked to provide feedback to email categorizations predicted by the system. The users were allowed to ask several Why questions (inspired by Myers et al. (2006)) through either the menu bar, or by right-clicking on the object of interest (such as a particular word). Examples include ""Why will this message be filed to folder A?"", ""Why does word x matter to folder B?"". The system then responded by textual explanations (generated using templates), together with visual explanations such as bar plots for some types of questions. All of these made the interface become more user-friendly. In 2015, Kulesza et al. proposed, as desirable principles, that the presented explanations should be sound (i.e., truthful in describing the underlying model), complete (i.e., not omitting important information about the model), but not overwhelming (i.e., remaining comprehensible). However, these principles are challenging especially when working on non-interpretable complex models.']","The main role of explanations here is to provide interpretable insights into the model and uncover its potential misbehavior or irrationality, which sometimes cannot be noticed by looking at the model outputs or the evaluation metrics.

Explanation scopes. Basically, there are two main types of explanations that could be provided to feedback providers. Local explanations (L) explain the predictions by the model for individual inputs. In contrast, global explanations (G) explain the model overall, independently of any specific inputs. It can be seen from Table 1 that most existing work use local explanations. One reason for this may be that, for complex models, global explanations can hardly reveal details of the models' inner workings in a comprehensible way to users. So, some bugs are imperceptible in such highlevel global explanations and then not corrected by the users. For example, the debugging framework FIND, proposed by Lertvittayakumjorn et al. (2020), uses only global explanations, and it was shown to work more effectively on significant bugs (such as gender bias in abusive language detection) than on less-obvious bugs (such as dataset shift between product types of sentiment analysis on product reviews). Otherwise, Ribeiro et al. (2018b) presented adversarial replacement rules as global explanations to reveal the model weaknesses only, without explaining how the whole model worked.

On the other hand, using local explanations has limitations in that it demands a large amount of effort from feedback providers to inspect the explanation of every single example in the training/validation set. With limited human resources, efficient ways to rank or select examples to explain would be required (Idahl et al., 2021). Recently, some work in explainable AI considers generating explanations for a group of predictions (Johnson et al., 2020; Chan et al., 2020) (e.g., for all the false positives of a certain class), thus staying in the middle of the two extreme explanation types (i.e., local and global). This kind of explanation is not too fine-grained, yet it can capture some suspicious model behaviors if we target the right group of examples. So, it would be worth studying in the context of EBHD (to the best of our knowledge, no existing study experiments with it).

Generating explanations. To generate explanations in general, there are two important questions we need to answer. First, which format should the explanations have? Second, how do we generate the explanations?

For the first question, we see many possible answers in the literature of explainable NLP (e.g., see the survey by Danilevsky et al. (2020)). For instance, input-based explanations (so called feature importance explanations) identify parts of the input that are important for the prediction. The explanation could be a list of importance scores of words in the input, so called attribution scores or relevance scores (Lundberg and Lee, 2017; Arras et al., 2016). Example-based explanations select influential, important, or similar examples from the training set to explain why the model makes a specific prediction (Han et al., 2020;Guo et al., 2020). Rule-based explanations provide interpretable decision rules that approximate the prediction process (Ribeiro et al., 2018a). Adversarial-based explanations return the smallest changes in the inputs that could change the predictions, revealing the model misbehavior (Zhang et al., 2020a). In most NLP tasks, inputbased explanations are the most popular approach for explaining predictions (Bhatt et al., 2020). This is also the case for EBHD as most selected studies use input-based explanations (Kulesza et al., 2009(Kulesza et al., , 2010Teso and Kersting, 2019;Cho et al. For the second question, there are two ways to generate the explanations: self-explaining methods and post-hoc explanation methods. Some models, e.g., Naive Bayes, logistic regression, and decision trees, are self-explaining (SE) (Danilevsky et al., 2020), also referred to as transparent (Adadi and Berrada, 2018) or inherently interpretable (Rudin, 2019). Local explanations of self-explaining models can be obtained at the same time as predictions, usually from the process of making those predictions, while the models themselves can often serve directly as global explanations. For example, feature importance explanations for a Naive Bayes model can be directly derived from the likelihood terms in the Naive Bayes equation, as done by several papers in Table 1 (Kulesza et al., 2009;Smith-Renner et al., 2020). Also, using attention scores on input as explanations, as done in (Cho et al., 2019), is a self-explaining method because the scores were obtained during the prediction process.

In contrast, post-hoc explanation methods (PH) perform additional steps to extract explanations after the model is trained (for a global explanation) or after the prediction is made (for a local explanation). If the method is allowed to access model parameters, it may calculate word relevance scores by propagating the output scores back to the input words (Arras et al., 2016) or analyzing the derivative of the output with respect to the input words (Smilkov et al., 2017;Sundararajan et al., 2017). If the method cannot access the model parameters, it may perturb the input and see how the output changes to estimate the importance of the altered parts of the input (Ribeiro et al., 2016;Jin et al., 2020). The important words and/or the relevance scores can be presented to the feedback providers in the EBHD workflow in many forms such as a list of words and their scores (Teso and Kersting, 2019;Ribeiro et al., 2016), word clouds (Lertvittayakumjorn et al., 2020), and a parse tree (Yao et al., 2021). Meanwhile, the influence functions method, used in (Koh and Liang, 2017;Zylberajch et al., 2021), identifies training examples which influence the prediction by analyzing how the prediction would change if we did not have each training point. This is another post-hoc explanation method as it takes place after prediction. It is similar to the other two example-based explanation methods used in (Khanna et al., 2019;Han and Ghosh, 2020).

Presenting explanations. It is important to carefully design the presentation of explanations, taking into consideration the background knowledge, desires, and limits of the feedback providers. In the debugging application by Kulesza et al. (2009), lay users were asked to provide feedback to email categorizations predicted by the system. The users were allowed to ask several Why questions (inspired by Myers et al. (2006)) through either the menu bar, or by right-clicking on the object of interest (such as a particular word). Examples include ""Why will this message be filed to folder A?"", ""Why does word x matter to folder B?"". The system then responded by textual explanations (generated using templates), together with visual explanations such as bar plots for some types of questions. All of these made the interface become more user-friendly. In 2015, Kulesza et al. proposed, as desirable principles, that the presented explanations should be sound (i.e., truthful in describing the underlying model), complete (i.e., not omitting important information about the model), but not overwhelming (i.e., remaining comprehensible). However, these principles are challenging especially when working on non-interpretable complex models.","(p7.0) The main role of explanations here is to provide interpretable insights into the model and uncover its potential misbehavior or irrationality, which sometimes cannot be noticed by looking at the model outputs or the evaluation metrics.

(p7.1) Explanation scopes. Basically, there are two main types of explanations that could be provided to feedback providers. Local explanations (L) explain the predictions by the model for individual inputs. In contrast, global explanations (G) explain the model overall, independently of any specific inputs. It can be seen from Table 1 that most existing work use local explanations. One reason for this may be that, for complex models, global explanations can hardly reveal details of the models' inner workings in a comprehensible way to users. So, some bugs are imperceptible in such highlevel global explanations and then not corrected by the users. For example, the debugging framework FIND, proposed by Lertvittayakumjorn et al. (2020), uses only global explanations, and it was shown to work more effectively on significant bugs (such as gender bias in abusive language detection) than on less-obvious bugs (such as dataset shift between product types of sentiment analysis on product reviews). Otherwise, Ribeiro et al. (2018b) presented adversarial replacement rules as global explanations to reveal the model weaknesses only, without explaining how the whole model worked.

(p7.2) On the other hand, using local explanations has limitations in that it demands a large amount of effort from feedback providers to inspect the explanation of every single example in the training/validation set. With limited human resources, efficient ways to rank or select examples to explain would be required (Idahl et al., 2021). Recently, some work in explainable AI considers generating explanations for a group of predictions (Johnson et al., 2020; Chan et al., 2020) (e.g., for all the false positives of a certain class), thus staying in the middle of the two extreme explanation types (i.e., local and global). This kind of explanation is not too fine-grained, yet it can capture some suspicious model behaviors if we target the right group of examples. So, it would be worth studying in the context of EBHD (to the best of our knowledge, no existing study experiments with it).

(p7.3) Generating explanations. To generate explanations in general, there are two important questions we need to answer. First, which format should the explanations have? Second, how do we generate the explanations?

(p7.4) For the first question, we see many possible answers in the literature of explainable NLP (e.g., see the survey by Danilevsky et al. (2020)). For instance, input-based explanations (so called feature importance explanations) identify parts of the input that are important for the prediction. The explanation could be a list of importance scores of words in the input, so called attribution scores or relevance scores (Lundberg and Lee, 2017; Arras et al., 2016). Example-based explanations select influential, important, or similar examples from the training set to explain why the model makes a specific prediction (Han et al., 2020;Guo et al., 2020). Rule-based explanations provide interpretable decision rules that approximate the prediction process (Ribeiro et al., 2018a). Adversarial-based explanations return the smallest changes in the inputs that could change the predictions, revealing the model misbehavior (Zhang et al., 2020a). In most NLP tasks, inputbased explanations are the most popular approach for explaining predictions (Bhatt et al., 2020). This is also the case for EBHD as most selected studies use input-based explanations (Kulesza et al., 2009(Kulesza et al., , 2010Teso and Kersting, 2019;Cho et al. For the second question, there are two ways to generate the explanations: self-explaining methods and post-hoc explanation methods. Some models, e.g., Naive Bayes, logistic regression, and decision trees, are self-explaining (SE) (Danilevsky et al., 2020), also referred to as transparent (Adadi and Berrada, 2018) or inherently interpretable (Rudin, 2019). Local explanations of self-explaining models can be obtained at the same time as predictions, usually from the process of making those predictions, while the models themselves can often serve directly as global explanations. For example, feature importance explanations for a Naive Bayes model can be directly derived from the likelihood terms in the Naive Bayes equation, as done by several papers in Table 1 (Kulesza et al., 2009;Smith-Renner et al., 2020). Also, using attention scores on input as explanations, as done in (Cho et al., 2019), is a self-explaining method because the scores were obtained during the prediction process.

(p7.5) In contrast, post-hoc explanation methods (PH) perform additional steps to extract explanations after the model is trained (for a global explanation) or after the prediction is made (for a local explanation). If the method is allowed to access model parameters, it may calculate word relevance scores by propagating the output scores back to the input words (Arras et al., 2016) or analyzing the derivative of the output with respect to the input words (Smilkov et al., 2017;Sundararajan et al., 2017). If the method cannot access the model parameters, it may perturb the input and see how the output changes to estimate the importance of the altered parts of the input (Ribeiro et al., 2016;Jin et al., 2020). The important words and/or the relevance scores can be presented to the feedback providers in the EBHD workflow in many forms such as a list of words and their scores (Teso and Kersting, 2019;Ribeiro et al., 2016), word clouds (Lertvittayakumjorn et al., 2020), and a parse tree (Yao et al., 2021). Meanwhile, the influence functions method, used in (Koh and Liang, 2017;Zylberajch et al., 2021), identifies training examples which influence the prediction by analyzing how the prediction would change if we did not have each training point. This is another post-hoc explanation method as it takes place after prediction. It is similar to the other two example-based explanation methods used in (Khanna et al., 2019;Han and Ghosh, 2020).

(p7.6) Presenting explanations. It is important to carefully design the presentation of explanations, taking into consideration the background knowledge, desires, and limits of the feedback providers. In the debugging application by Kulesza et al. (2009), lay users were asked to provide feedback to email categorizations predicted by the system. The users were allowed to ask several Why questions (inspired by Myers et al. (2006)) through either the menu bar, or by right-clicking on the object of interest (such as a particular word). Examples include ""Why will this message be filed to folder A?"", ""Why does word x matter to folder B?"". The system then responded by textual explanations (generated using templates), together with visual explanations such as bar plots for some types of questions. All of these made the interface become more user-friendly. In 2015, Kulesza et al. proposed, as desirable principles, that the presented explanations should be sound (i.e., truthful in describing the underlying model), complete (i.e., not omitting important information about the model), but not overwhelming (i.e., remaining comprehensible). However, these principles are challenging especially when working on non-interpretable complex models.","[[], ['b51', 'b30'], [None], [], ['b55', 'b50', 'b8', 'b62', None, 'b25', 'b67', 'b24', 'b23', 'b74', 'b1', 'b5'], ['b77', 'b49', 'b65', 'b30', 'b72', 'b16', None, 'b19', 'b67', 'b5'], ['b24', None, 'b40']]","[[], ['b51', 'b30'], [None], [], ['b55', 'b50', 'b8', 'b62', None, 'b25', 'b67', 'b24', 'b23', 'b74', 'b1', 'b5'], ['b77', 'b49', 'b65', 'b30', 'b72', 'b16', None, 'b19', 'b67', 'b5'], ['b24', None, 'b40']]",28,"1. The main role of explanations here is to provide interpretable insights into the model and uncover its potential misbehavior or irrationality, which sometimes cannot be noticed by looking at the model outputs or the evaluation metrics.
2. Explanation scopes. Basically, there are two main types of explanations that could be provided to feedback providers.
3. Local explanations (L) explain the predictions by the model for individual inputs.
4. In contrast, global explanations (G) explain the model overall, independently of any specific inputs.
5. It can be seen from Table 1 that most existing work use local explanations.
6. One reason for this may be that, for complex models, global explanations can hardly reveal details of the models' inner workings in a comprehensible way to users.
7. So, some bugs are imperceptible in such highlevel global explanations and then not corrected by the users.
8. For example, the debugging framework FIND, proposed by Lertvittayakumjorn et al. (2020), uses only global explanations, and it was shown to work more effectively on significant bugs (such as gender bias in abusive language detection) than on less-obvious bugs (such as dataset shift between product types of sentiment analysis on product reviews).
9. Otherwise, Ribeiro et al. (2018b) presented adversarial replacement rules as global explanations to reveal the model weaknesses only, without explaining how the whole model worked.
10. On the other hand, using local explanations has limitations in that it demands a large amount of effort from feedback providers to inspect the explanation of every single example in the training/validation set.
11. With limited human resources, efficient ways to rank or select examples to explain would be required (Idahl et al., 2021).
12. Recently, some work in explainable AI considers generating explanations for a group of predictions (Johnson et al., 2020; Chan et al., 2020)
13. (e.g., for all the false positives of a certain class), thus staying in the middle of the two extreme explanation types (i.e., local and global).
14. This kind of explanation is not too fine-grained, yet it can capture some suspicious model behaviors if we target the right group of examples.
15. So, it would be worth studying in the context of EBHD (to the best of our knowledge, no existing study experiments with it).
16. Generating explanations. To generate explanations in general, there are two important questions we need to answer.
17. First, which format should the explanations have?
18. Second, how do we generate the explanations?
19. For the first question, we see many possible answers in the literature of explainable NLP (e.g., see the survey by Danilevsky et al. (2020)).
20. For instance, input-based explanations (so called feature importance explanations) identify parts of the input that are important for the prediction.
21. The explanation could be a list of importance scores of words in the input, so called attribution scores or relevance scores (Lundberg and Lee, 2017; Arras et al., 2016).
22. Example-based explanations select influential, important, or similar examples from the training set to explain why the model makes a specific prediction (Han et al., 2020;Guo et al., 2020).
23. Rule-based explanations provide interpretable decision rules that approximate the prediction process (Ribeiro et al., 2018a).
24. Adversarial-based explanations return the smallest changes in the inputs that could change the predictions, revealing the model misbehavior (Zhang et al., 2020a).
25. In most NLP tasks, inputbased explanations are the most popular approach for explaining predictions (Bhatt et al., 2020).
26. This is also the case for EBHD as most selected studies use input-based explanations (Kulesza et al., 2009(Kulesza et al., , 2010Teso and Kersting, 2019;Cho et al. For the second question, there are two ways to generate the explanations: self-explaining methods and post-hoc explanation methods. Some models, e.g., Naive Bayes, logistic regression, and decision trees, are self-explaining (SE) (Danilevsky et al., 2020), also referred to as transparent (Adadi and Berrada, 2018) or inherently interpretable (Rudin, 2019).
27. Local explanations of self-explaining models can be obtained at the same time as predictions, usually from the process of making those predictions, while the models themselves can often serve directly as global explanations.
28. For example, feature importance explanations for a Naive Bayes model can be directly derived from the likelihood terms in the Naive Bayes equation, as done by several papers in Table 1 (Kulesza et al., 2009;Smith-Renner et al., 2020).
29. Also, using attention scores on input as explanations, as done in (Cho et al., 2019), is a self-explaining method because the scores were obtained during the prediction process.
30. In contrast, post-hoc explanation methods (PH) perform additional steps to extract explanations after the model is trained (for a global explanation) or after the prediction is made (for a local explanation).
31. If the method is allowed to access model parameters, it may calculate word relevance scores by propagating the output scores back to the input words (Arras et al., 2016) or analyzing the derivative of the output with respect to the input words (Smilkov et al., 2017;Sundararajan et al., 2017).
32. If the method cannot access the model parameters, it may perturb the input and see how the output changes to estimate the importance of the altered parts of the input (Ribeiro et al., 2016;Jin et al., 2020).
33. The important words and/or the relevance scores can be presented to the feedback providers in the EBHD workflow in many forms such as a list of words and their scores (Teso and Kersting, 2019;Ribeiro et al., 2016), word clouds (Lertvittayakumjorn et al., 2020), and a parse tree (Yao et al., 2021).
34. Meanwhile, the influence functions method, used in (Koh and Liang, 2017;Zylberajch et al., 2021), identifies training examples which influence the prediction by analyzing how the prediction would change if we did not have each training point.
35. This is another post-hoc explanation method as it takes place after prediction.
36. It is similar to the other two example-based explanation methods used in (Khanna et al., 2019;Han and Ghosh, 2020).
37. Presenting explanations. It is important to carefully design the presentation of explanations, taking into consideration the background knowledge, desires, and limits of the feedback providers.
38. In the debugging application by Kulesza et al. (2009), lay users were asked to provide feedback to email categorizations predicted by the system.
39. The users were allowed to ask several Why questions (inspired by Myers et al. (2006)) through either the menu bar, or by right-clicking on the object of interest (such as a particular word).
40. Examples include ""Why will this message be filed to folder A?"", ""Why does word x matter to folder B?"".
41. The system then responded by textual explanations (generated using templates), together with visual explanations such as bar plots for some types of questions.
42. All of these made the interface become more user-friendly.
43. In 2015, Kulesza et al. proposed, as desirable principles, that the presented explanations should be sound (i.e., truthful in describing the underlying model), complete (i.e., not omitting important information about the model), but not overwhelming (i.e., remaining comprehensible).
44. However, these principles are challenging especially when working on non-interpretable complex models."
233476148,Explanation-Based Human Debugging of NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/d84ed05ab860b75f9e6b28e717abf4bc12da03d7,s15,Trust,"['p15.0', 'p15.1', 'p15.2']","['Trust (as well as frustration and expectation, discussed next) is an important issue when the system end users are feedback providers in the EBHD framework. It has been discussed widely that explanations engender human trust in AI systems (Pu and Chen, 2006;Lipton, 2018;Toreini et al., 2020). This trust may be misplaced at times. Showing more detailed explanations can cause users to over rely on the system, leading to misuse where users agree with incorrect system predictions (Stumpf et al., 2016). Moreover, some users may over trust the explanations (without fully understanding them) only because the tools generating them are publicly available, widely used, and showing appealing visualizations (Kaur et al., 2020).', 'However, recent research reported that explanations do not necessarily increase trust and reliance. Cheng et al. (2019) found that, even though explanations help users comprehend systems, they cannot increase human trust in using the systems in high-stakes applications involving lots of qualitative factors, such as graduate school admissions. Smith-Renner et al. (2020) reported that explanations of low-quality models decrease trust and system acceptance as they reveal model weaknesses to the users. According to Schramowski et al. (2020), despite correct predictions, the trust still drops if the users see from the explanations that the model relies on the wrong reasons. These studies go along with a perspective by Zhang et al. (2020b) that explanations should help calibrate user perceptions to the model quality, signaling whether the users should trust or distrust the AI. Although, in some cases, explanations successfully warned users of faulty models (Ribeiro et al., 2016), this is not easy when the model flaws are not obvious (Zhang et al., 2020b;Lertvittayakumjorn and Toni, 2019).', 'Besides explanations, the effect of feedback on human trust is quite inconclusive according to some (but fewer) studies. On one hand, Smith-Renner et al. (2020) found that, after lay humans see explanations of low-quality models and lose their trust, the ability to provide feedback makes human trust and acceptance rally, remedying the situation. In contrast, Honeycutt et al. (2020) reported that providing feedback decreases human trust in the system as well as their perception of system accuracy no matter whether the system truly improves after being updated or not.']","Trust (as well as frustration and expectation, discussed next) is an important issue when the system end users are feedback providers in the EBHD framework. It has been discussed widely that explanations engender human trust in AI systems (Pu and Chen, 2006;Lipton, 2018;Toreini et al., 2020). This trust may be misplaced at times. Showing more detailed explanations can cause users to over rely on the system, leading to misuse where users agree with incorrect system predictions (Stumpf et al., 2016). Moreover, some users may over trust the explanations (without fully understanding them) only because the tools generating them are publicly available, widely used, and showing appealing visualizations (Kaur et al., 2020).

However, recent research reported that explanations do not necessarily increase trust and reliance. Cheng et al. (2019) found that, even though explanations help users comprehend systems, they cannot increase human trust in using the systems in high-stakes applications involving lots of qualitative factors, such as graduate school admissions. Smith-Renner et al. (2020) reported that explanations of low-quality models decrease trust and system acceptance as they reveal model weaknesses to the users. According to Schramowski et al. (2020), despite correct predictions, the trust still drops if the users see from the explanations that the model relies on the wrong reasons. These studies go along with a perspective by Zhang et al. (2020b) that explanations should help calibrate user perceptions to the model quality, signaling whether the users should trust or distrust the AI. Although, in some cases, explanations successfully warned users of faulty models (Ribeiro et al., 2016), this is not easy when the model flaws are not obvious (Zhang et al., 2020b;Lertvittayakumjorn and Toni, 2019).

Besides explanations, the effect of feedback on human trust is quite inconclusive according to some (but fewer) studies. On one hand, Smith-Renner et al. (2020) found that, after lay humans see explanations of low-quality models and lose their trust, the ability to provide feedback makes human trust and acceptance rally, remedying the situation. In contrast, Honeycutt et al. (2020) reported that providing feedback decreases human trust in the system as well as their perception of system accuracy no matter whether the system truly improves after being updated or not.","(p15.0) Trust (as well as frustration and expectation, discussed next) is an important issue when the system end users are feedback providers in the EBHD framework. It has been discussed widely that explanations engender human trust in AI systems (Pu and Chen, 2006;Lipton, 2018;Toreini et al., 2020). This trust may be misplaced at times. Showing more detailed explanations can cause users to over rely on the system, leading to misuse where users agree with incorrect system predictions (Stumpf et al., 2016). Moreover, some users may over trust the explanations (without fully understanding them) only because the tools generating them are publicly available, widely used, and showing appealing visualizations (Kaur et al., 2020).

(p15.1) However, recent research reported that explanations do not necessarily increase trust and reliance. Cheng et al. (2019) found that, even though explanations help users comprehend systems, they cannot increase human trust in using the systems in high-stakes applications involving lots of qualitative factors, such as graduate school admissions. Smith-Renner et al. (2020) reported that explanations of low-quality models decrease trust and system acceptance as they reveal model weaknesses to the users. According to Schramowski et al. (2020), despite correct predictions, the trust still drops if the users see from the explanations that the model relies on the wrong reasons. These studies go along with a perspective by Zhang et al. (2020b) that explanations should help calibrate user perceptions to the model quality, signaling whether the users should trust or distrust the AI. Although, in some cases, explanations successfully warned users of faulty models (Ribeiro et al., 2016), this is not easy when the model flaws are not obvious (Zhang et al., 2020b;Lertvittayakumjorn and Toni, 2019).

(p15.2) Besides explanations, the effect of feedback on human trust is quite inconclusive according to some (but fewer) studies. On one hand, Smith-Renner et al. (2020) found that, after lay humans see explanations of low-quality models and lose their trust, the ability to provide feedback makes human trust and acceptance rally, remedying the situation. In contrast, Honeycutt et al. (2020) reported that providing feedback decreases human trust in the system as well as their perception of system accuracy no matter whether the system truly improves after being updated or not.","[['b63', 'b33', None, 'b46'], ['b49', 'b62', 'b15', 'b31', 'b75', 'b58'], [None, 'b62']]","[['b63', 'b33', None, 'b46'], ['b49', 'b62', 'b15', 'b31', 'b75', 'b58'], [None, 'b62']]",12,"1. Trust (as well as frustration and expectation, discussed next) is an important issue when the system end users are feedback providers in the EBHD framework.
2. It has been discussed widely that explanations engender human trust in AI systems (Pu and Chen, 2006;Lipton, 2018;Toreini et al., 2020).
3. This trust may be misplaced at times.
4. Showing more detailed explanations can cause users to over rely on the system, leading to misuse where users agree with incorrect system predictions (Stumpf et al., 2016).
5. Moreover, some users may over trust the explanations (without fully understanding them) only because the tools generating them are publicly available, widely used, and showing appealing visualizations (Kaur et al., 2020).
6. However, recent research reported that explanations do not necessarily increase trust and reliance.
7. Cheng et al. (2019) found that, even though explanations help users comprehend systems, they cannot increase human trust in using the systems in high-stakes applications involving lots of qualitative factors, such as graduate school admissions.
8. Smith-Renner et al. (2020) reported that explanations of low-quality models decrease trust and system acceptance as they reveal model weaknesses to the users.
9. According to Schramowski et al. (2020), despite correct predictions, the trust still drops if the users see from the explanations that the model relies on the wrong reasons.
10. These studies go along with a perspective by Zhang et al. (2020b) that explanations should help calibrate user perceptions to the model quality, signaling whether the users should trust or distrust the AI.
11. Although, in some cases, explanations successfully warned users of faulty models (Ribeiro et al., 2016), this is not easy when the model flaws are not obvious (Zhang et al., 2020b;Lertvittayakumjorn and Toni, 2019).
12. Besides explanations, the effect of feedback on human trust is quite inconclusive according to some (but fewer) studies.
13. On one hand, Smith-Renner et al. (2020) found that, after lay humans see explanations of low-quality models and lose their trust, the ability to provide feedback makes human trust and acceptance rally, remedying the situation.
14. In contrast, Honeycutt et al. (2020) reported that providing feedback decreases human trust in the system as well as their perception of system accuracy no matter whether the system truly improves after being updated or not."
233476148,Explanation-Based Human Debugging of NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/d84ed05ab860b75f9e6b28e717abf4bc12da03d7,s9,Updating the Model,"['p9.0', 'p9.1', 'p9.2', 'p9.3', 'p9.4']","['Techniques to incorporate human feedback into the model can be categorized into three approaches.', '(1) Directly adjust the model parameters (M). When the model is transparent and the explanation displays the model parameters in an intelligible way, humans can directly adjust the parameters based on their judgements. This idea was adopted by Kulesza et al. (2009Kulesza et al. ( , 2015 where humans can adjust a bar chart showing word importance scores, corresponding to the parameters of the underlying Naive Bayes model. In this special case, steps 2 and 3 in Figure 1 are combined into a single step. Besides, human feedback can be used to modify the model parameters indirectly. For example, Smith-Renner et al. (2020) increased a word weight in the Naive Bayes model by 20% for the class that the word supported, according to human feedback, and reduced the weight by 20% for the opposite class (binary classification). This choice gives good results, however, it is not clear why and whether 20% is the best choice here.', 'Overall, this approach is fast because it does not require model retraining. However, it is important to ensure that the adjustments made by humans generalize well to all examples. Therefore, the system should update the overall results (e.g., performance metrics, predictions, and explanations) in real time after applying any adjustment, so the humans can investigate the effects and further adjust the model parameters (or undo the adjustments) if necessary. This agrees with the correctability principles proposed by Kulesza et al. (2015) that the system should be actionable and reversible, honor user feedback, and show incremental changes.', '(2) Improve the training data (D). We can use human feedback to improve the training data and retrain the model to fix bugs. This approach includes  Zylberajch et al., 2021). As this approach modifies the training data only, it is applicable to any model regardless of the model complexity.', '(3) Influence the training process (T). Another approach is to influence the (re-)training process in a way that the resulting model will behave as the feedback suggests. This approach could be either model-specific (such as attention supervision) or model-agnostic (such as user co-training). Cho et al. (2019) used human feedback to supervise attention weights of the model. Similarly, Yao et al. (2021) added a loss term to regularize explanations guided by human feedback.  proposed (i) constraint optimization, translating human feedback into constraints governing the training process and (ii) user co-training, using feedback as another classifier working together with the main ML model in a semi-supervised learning setting. Lertvittayakumjorn et al. (2020) disabled some learned features deemed irrelevant, based on the feedback, and re-trained the model, forcing it to use only the remaining features. With many techniques available, however, there has not been a study testing which technique is more appropriate for which task, domain, or model architecture. The comparison issue is one of the open problems for EBHD research (to be discussed in section 4).']","Techniques to incorporate human feedback into the model can be categorized into three approaches.

(1) Directly adjust the model parameters (M). When the model is transparent and the explanation displays the model parameters in an intelligible way, humans can directly adjust the parameters based on their judgements. This idea was adopted by Kulesza et al. (2009Kulesza et al. ( , 2015 where humans can adjust a bar chart showing word importance scores, corresponding to the parameters of the underlying Naive Bayes model. In this special case, steps 2 and 3 in Figure 1 are combined into a single step. Besides, human feedback can be used to modify the model parameters indirectly. For example, Smith-Renner et al. (2020) increased a word weight in the Naive Bayes model by 20% for the class that the word supported, according to human feedback, and reduced the weight by 20% for the opposite class (binary classification). This choice gives good results, however, it is not clear why and whether 20% is the best choice here.

Overall, this approach is fast because it does not require model retraining. However, it is important to ensure that the adjustments made by humans generalize well to all examples. Therefore, the system should update the overall results (e.g., performance metrics, predictions, and explanations) in real time after applying any adjustment, so the humans can investigate the effects and further adjust the model parameters (or undo the adjustments) if necessary. This agrees with the correctability principles proposed by Kulesza et al. (2015) that the system should be actionable and reversible, honor user feedback, and show incremental changes.

(2) Improve the training data (D). We can use human feedback to improve the training data and retrain the model to fix bugs. This approach includes  Zylberajch et al., 2021). As this approach modifies the training data only, it is applicable to any model regardless of the model complexity.

(3) Influence the training process (T). Another approach is to influence the (re-)training process in a way that the resulting model will behave as the feedback suggests. This approach could be either model-specific (such as attention supervision) or model-agnostic (such as user co-training). Cho et al. (2019) used human feedback to supervise attention weights of the model. Similarly, Yao et al. (2021) added a loss term to regularize explanations guided by human feedback.  proposed (i) constraint optimization, translating human feedback into constraints governing the training process and (ii) user co-training, using feedback as another classifier working together with the main ML model in a semi-supervised learning setting. Lertvittayakumjorn et al. (2020) disabled some learned features deemed irrelevant, based on the feedback, and re-trained the model, forcing it to use only the remaining features. With many techniques available, however, there has not been a study testing which technique is more appropriate for which task, domain, or model architecture. The comparison issue is one of the open problems for EBHD research (to be discussed in section 4).","(p9.0) Techniques to incorporate human feedback into the model can be categorized into three approaches.

(p9.1) (1) Directly adjust the model parameters (M). When the model is transparent and the explanation displays the model parameters in an intelligible way, humans can directly adjust the parameters based on their judgements. This idea was adopted by Kulesza et al. (2009Kulesza et al. ( , 2015 where humans can adjust a bar chart showing word importance scores, corresponding to the parameters of the underlying Naive Bayes model. In this special case, steps 2 and 3 in Figure 1 are combined into a single step. Besides, human feedback can be used to modify the model parameters indirectly. For example, Smith-Renner et al. (2020) increased a word weight in the Naive Bayes model by 20% for the class that the word supported, according to human feedback, and reduced the weight by 20% for the opposite class (binary classification). This choice gives good results, however, it is not clear why and whether 20% is the best choice here.

(p9.2) Overall, this approach is fast because it does not require model retraining. However, it is important to ensure that the adjustments made by humans generalize well to all examples. Therefore, the system should update the overall results (e.g., performance metrics, predictions, and explanations) in real time after applying any adjustment, so the humans can investigate the effects and further adjust the model parameters (or undo the adjustments) if necessary. This agrees with the correctability principles proposed by Kulesza et al. (2015) that the system should be actionable and reversible, honor user feedback, and show incremental changes.

(p9.3) (2) Improve the training data (D). We can use human feedback to improve the training data and retrain the model to fix bugs. This approach includes  Zylberajch et al., 2021). As this approach modifies the training data only, it is applicable to any model regardless of the model complexity.

(p9.4) (3) Influence the training process (T). Another approach is to influence the (re-)training process in a way that the resulting model will behave as the feedback suggests. This approach could be either model-specific (such as attention supervision) or model-agnostic (such as user co-training). Cho et al. (2019) used human feedback to supervise attention weights of the model. Similarly, Yao et al. (2021) added a loss term to regularize explanations guided by human feedback.  proposed (i) constraint optimization, translating human feedback into constraints governing the training process and (ii) user co-training, using feedback as another classifier working together with the main ML model in a semi-supervised learning setting. Lertvittayakumjorn et al. (2020) disabled some learned features deemed irrelevant, based on the feedback, and re-trained the model, forcing it to use only the remaining features. With many techniques available, however, there has not been a study testing which technique is more appropriate for which task, domain, or model architecture. The comparison issue is one of the open problems for EBHD research (to be discussed in section 4).","[[], ['b22', 'b24'], ['b22'], ['b77'], ['b72', 'b30']]","[[], ['b22', 'b24'], ['b22'], ['b77'], ['b72', 'b30']]",6,"1. Techniques to incorporate human feedback into the model can be categorized into three approaches.(1)
2. Directly adjust the model parameters (M).
3. When the model is transparent and the explanation displays the model parameters in an intelligible way, humans can directly adjust the parameters based on their judgements.
4. This idea was adopted by Kulesza et al. (2009Kulesza et al. ( , 2015 where humans can adjust a bar chart showing word importance scores, corresponding to the parameters of the underlying Naive Bayes model. In this special case, steps 2 and 3 in Figure 1 are combined into a single step. Besides, human feedback can be used to modify the model parameters indirectly. For example, Smith-Renner et al. (2020) increased a word weight in the Naive Bayes model by 20% for the class that the word supported, according to human feedback, and reduced the weight by 20% for the opposite class (binary classification).
5. This choice gives good results, however, it is not clear why and whether 20% is the best choice here.
6. Overall, this approach is fast because it does not require model retraining.
7. However, it is important to ensure that the adjustments made by humans generalize well to all examples.
8. Therefore, the system should update the overall results (e.g., performance metrics, predictions, and explanations) in real time after applying any adjustment, so the humans can investigate the effects and further adjust the model parameters (or undo the adjustments) if necessary.
9. This agrees with the correctability principles proposed by Kulesza et al. (2015) that the system should be actionable and reversible, honor user feedback, and show incremental changes.
10. (2) Improve the training data (D).
11. We can use human feedback to improve the training data and retrain the model to fix bugs.
12. This approach includes  Zylberajch et al., 2021).
13. As this approach modifies the training data only, it is applicable to any model regardless of the model complexity.
14. (3) Influence the training process (T).
15. Another approach is to influence the (re-)training process in a way that the resulting model will behave as the feedback suggests.
16. This approach could be either model-specific (such as attention supervision) or model-agnostic (such as user co-training).
17. Cho et al. (2019) used human feedback to supervise attention weights of the model.
18. Similarly, Yao et al. (2021) added a loss term to regularize explanations guided by human feedback.
19. proposed (i) constraint optimization, translating human feedback into constraints governing the training process and (ii) user co-training, using feedback as another classifier working together with the main ML model in a semi-supervised learning setting.
20. Lertvittayakumjorn et al. (2020) disabled some learned features deemed irrelevant, based on the feedback, and re-trained the model, forcing it to use only the remaining features.
21. With many techniques available, however, there has not been a study testing which technique is more appropriate for which task, domain, or model architecture.
22. The comparison issue is one of the open problems for EBHD research (to be discussed in section 4)."
233476148,Explanation-Based Human Debugging of NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/d84ed05ab860b75f9e6b28e717abf4bc12da03d7,s11,Experimental Setting,"['p11.0', 'p11.1', 'p11.2']","['To conduct experiments, some studies in Table 1 selected human participants (SP) to be their feedback providers. The selected participants could be people without ML/NLP knowledge (Kulesza et al., 2010(Kulesza et al., , 2015 or with ML/NLP knowledge (Ribeiro et al., 2018b;Zylberajch et al., 2021) depending on the study objectives and the complexity of the feedback process. Early work even conducted experiments with the participants in-person Kulesza et al., 2009Kulesza et al., , 2015. Although this limited the number of participants (to less than 100), the researchers could closely observe their behaviors and gain some insights concerning humancomputer interaction.', 'By contrast, some used a crowdsourcing platform, Amazon Mechanical Turk 5 in particular, to collect human feedback for debugging the models. Crowdsourcing (CS) enables researchers to conduct experiments at a large scale; however, the quality of human responses could be varying. So, it is important to ensure some quality control such as specifying required qualifications (Smith-Renner et al., 2020), using multiple annotations per question (Lertvittayakumjorn et al., 2020), having a training phase for participants, and setting up some obvious questions to check if the participants are paying attention to the tasks (Egelman et al., 2014).', 'Finally, simulation (SM), without real humans involved but using oracles as human feedback instead, has also been considered (for the purpose of testing the EBHD framework only). For example, Teso and Kersting (2019) set 20% of input words as relevant using feature selection. These were used to respond to post-hoc explanations, i.e., top k words selected by LIME. Koh and Liang (2017) simulated mislabeled examples by flipping the labels of a random 10% of the training data. So, when the explanation showed suspicious training examples, the true labels could be used to provide feedback. Compared to the other settings, simulation is faster and cheaper, yet its results may not reflect the effectiveness of the framework when deployed with real humans. Naturally, human feedback is sometimes inaccurate and noisy, and humans could also be interrupted or frustrated while providing feedback (Amershi et al., 2014). These factors, discussed in detail in the next section, cannot be thoroughly studied in only simulated experiments.']","To conduct experiments, some studies in Table 1 selected human participants (SP) to be their feedback providers. The selected participants could be people without ML/NLP knowledge (Kulesza et al., 2010(Kulesza et al., , 2015 or with ML/NLP knowledge (Ribeiro et al., 2018b;Zylberajch et al., 2021) depending on the study objectives and the complexity of the feedback process. Early work even conducted experiments with the participants in-person Kulesza et al., 2009Kulesza et al., , 2015. Although this limited the number of participants (to less than 100), the researchers could closely observe their behaviors and gain some insights concerning humancomputer interaction.

By contrast, some used a crowdsourcing platform, Amazon Mechanical Turk 5 in particular, to collect human feedback for debugging the models. Crowdsourcing (CS) enables researchers to conduct experiments at a large scale; however, the quality of human responses could be varying. So, it is important to ensure some quality control such as specifying required qualifications (Smith-Renner et al., 2020), using multiple annotations per question (Lertvittayakumjorn et al., 2020), having a training phase for participants, and setting up some obvious questions to check if the participants are paying attention to the tasks (Egelman et al., 2014).

Finally, simulation (SM), without real humans involved but using oracles as human feedback instead, has also been considered (for the purpose of testing the EBHD framework only). For example, Teso and Kersting (2019) set 20% of input words as relevant using feature selection. These were used to respond to post-hoc explanations, i.e., top k words selected by LIME. Koh and Liang (2017) simulated mislabeled examples by flipping the labels of a random 10% of the training data. So, when the explanation showed suspicious training examples, the true labels could be used to provide feedback. Compared to the other settings, simulation is faster and cheaper, yet its results may not reflect the effectiveness of the framework when deployed with real humans. Naturally, human feedback is sometimes inaccurate and noisy, and humans could also be interrupted or frustrated while providing feedback (Amershi et al., 2014). These factors, discussed in detail in the next section, cannot be thoroughly studied in only simulated experiments.","(p11.0) To conduct experiments, some studies in Table 1 selected human participants (SP) to be their feedback providers. The selected participants could be people without ML/NLP knowledge (Kulesza et al., 2010(Kulesza et al., , 2015 or with ML/NLP knowledge (Ribeiro et al., 2018b;Zylberajch et al., 2021) depending on the study objectives and the complexity of the feedback process. Early work even conducted experiments with the participants in-person Kulesza et al., 2009Kulesza et al., , 2015. Although this limited the number of participants (to less than 100), the researchers could closely observe their behaviors and gain some insights concerning humancomputer interaction.

(p11.1) By contrast, some used a crowdsourcing platform, Amazon Mechanical Turk 5 in particular, to collect human feedback for debugging the models. Crowdsourcing (CS) enables researchers to conduct experiments at a large scale; however, the quality of human responses could be varying. So, it is important to ensure some quality control such as specifying required qualifications (Smith-Renner et al., 2020), using multiple annotations per question (Lertvittayakumjorn et al., 2020), having a training phase for participants, and setting up some obvious questions to check if the participants are paying attention to the tasks (Egelman et al., 2014).

(p11.2) Finally, simulation (SM), without real humans involved but using oracles as human feedback instead, has also been considered (for the purpose of testing the EBHD framework only). For example, Teso and Kersting (2019) set 20% of input words as relevant using feature selection. These were used to respond to post-hoc explanations, i.e., top k words selected by LIME. Koh and Liang (2017) simulated mislabeled examples by flipping the labels of a random 10% of the training data. So, when the explanation showed suspicious training examples, the true labels could be used to provide feedback. Compared to the other settings, simulation is faster and cheaper, yet its results may not reflect the effectiveness of the framework when deployed with real humans. Naturally, human feedback is sometimes inaccurate and noisy, and humans could also be interrupted or frustrated while providing feedback (Amershi et al., 2014). These factors, discussed in detail in the next section, cannot be thoroughly studied in only simulated experiments.","[['b77', 'b22', 'b24', 'b23', 'b51'], [None, 'b30'], ['b67', 'b3']]","[['b77', 'b22', 'b24', 'b23', 'b51'], [None, 'b30'], ['b67', 'b3']]",9,"1. To conduct experiments, some studies in Table 1 selected human participants (SP) to be their feedback providers.
2. The selected participants could be people without ML/NLP knowledge (Kulesza et al., 2010(Kulesza et al., , 2015 or with ML/NLP knowledge (Ribeiro et al., 2018b;Zylberajch et al., 2021) depending on the study objectives and the complexity of the feedback process.
3. Early work even conducted experiments with the participants in-person Kulesza et al., 2009Kulesza et al., , 2015.
4. Although this limited the number of participants (to less than 100), the researchers could closely observe their behaviors and gain some insights concerning humancomputer interaction.
5. By contrast, some used a crowdsourcing platform, Amazon Mechanical Turk 5 in particular, to collect human feedback for debugging the models.
6. Crowdsourcing (CS) enables researchers to conduct experiments at a large scale; however, the quality of human responses could be varying.
7. So, it is important to ensure some quality control such as specifying required qualifications (Smith-Renner et al., 2020), using multiple annotations per question (Lertvittayakumjorn et al., 2020), having a training phase for participants, and setting up some obvious questions to check if the participants are paying attention to the tasks (Egelman et al., 2014).
8. Finally, simulation (SM), without real humans involved but using oracles as human feedback instead, has also been considered (for the purpose of testing the EBHD framework only).
9. For example, Teso and Kersting (2019) set 20% of input words as relevant using feature selection.
10. These were used to respond to post-hoc explanations, i.e., top k words selected by LIME.
11. Koh and Liang (2017) simulated mislabeled examples by flipping the labels of a random 10% of the training data.
12. So, when the explanation showed suspicious training examples, the true labels could be used to provide feedback.
13. Compared to the other settings, simulation is faster and cheaper, yet its results may not reflect the effectiveness of the framework when deployed with real humans.
14. Naturally, human feedback is sometimes inaccurate and noisy, and humans could also be interrupted or frustrated while providing feedback (Amershi et al., 2014).
15. These factors, discussed in detail in the next section, cannot be thoroughly studied in only simulated experiments."
233476148,Explanation-Based Human Debugging of NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/d84ed05ab860b75f9e6b28e717abf4bc12da03d7,s14,Willingness,"['p14.0', 'p14.1']","['We would like humans to provide feedback for improving models, but do humans naturally want to? Prior to the emerging of EBHD, studies found that humans are not willing to be constantly asked about labels of examples as if they were just simple oracles (Cakmak et al., 2010;Guillory and Bilmes, 2011). Rather, they want to provide more than just data labels after being given explanations (Amershi et al., 2014;Smith-Renner et al., 2020). By collecting free-form feedback from users, Ghai et al. (2021) discovered various feedback types. The most prominent ones include removing-adding features (words), tuning weights, and leveraging feature combinations.  further analyzed categories of background knowledge underlying the feedback and found, in their experiment, that it was mainly based on commonsense knowledge and English language knowledge. Such knowledge may not be efficiently injected into the model if we exploit human feedback which contains only labels. This agrees with some participants, in (Smith-Renner et al., 2020), who described their feedback as inadequate when they could only confirm or correct predicted labels.', 'Although human feedback beyond labels contains helpful information, it is naturally neither complete nor precise. Ghai et al. (2021) observed that human feedback usually focuses on a few features that are most different from human expectation, ignoring the others. Also, they found that humans, especially lay people, are not good at correcting model explanations quantitatively (e.g., adjusting weights). This is consistent with the findings of Miller (2019) that human explanations are selective (in a biased way) and rarely refer to probabilities but express causal relationships instead.']","We would like humans to provide feedback for improving models, but do humans naturally want to? Prior to the emerging of EBHD, studies found that humans are not willing to be constantly asked about labels of examples as if they were just simple oracles (Cakmak et al., 2010;Guillory and Bilmes, 2011). Rather, they want to provide more than just data labels after being given explanations (Amershi et al., 2014;Smith-Renner et al., 2020). By collecting free-form feedback from users, Ghai et al. (2021) discovered various feedback types. The most prominent ones include removing-adding features (words), tuning weights, and leveraging feature combinations.  further analyzed categories of background knowledge underlying the feedback and found, in their experiment, that it was mainly based on commonsense knowledge and English language knowledge. Such knowledge may not be efficiently injected into the model if we exploit human feedback which contains only labels. This agrees with some participants, in (Smith-Renner et al., 2020), who described their feedback as inadequate when they could only confirm or correct predicted labels.

Although human feedback beyond labels contains helpful information, it is naturally neither complete nor precise. Ghai et al. (2021) observed that human feedback usually focuses on a few features that are most different from human expectation, ignoring the others. Also, they found that humans, especially lay people, are not good at correcting model explanations quantitatively (e.g., adjusting weights). This is consistent with the findings of Miller (2019) that human explanations are selective (in a biased way) and rarely refer to probabilities but express causal relationships instead.","(p14.0) We would like humans to provide feedback for improving models, but do humans naturally want to? Prior to the emerging of EBHD, studies found that humans are not willing to be constantly asked about labels of examples as if they were just simple oracles (Cakmak et al., 2010;Guillory and Bilmes, 2011). Rather, they want to provide more than just data labels after being given explanations (Amershi et al., 2014;Smith-Renner et al., 2020). By collecting free-form feedback from users, Ghai et al. (2021) discovered various feedback types. The most prominent ones include removing-adding features (words), tuning weights, and leveraging feature combinations.  further analyzed categories of background knowledge underlying the feedback and found, in their experiment, that it was mainly based on commonsense knowledge and English language knowledge. Such knowledge may not be efficiently injected into the model if we exploit human feedback which contains only labels. This agrees with some participants, in (Smith-Renner et al., 2020), who described their feedback as inadequate when they could only confirm or correct predicted labels.

(p14.1) Although human feedback beyond labels contains helpful information, it is naturally neither complete nor precise. Ghai et al. (2021) observed that human feedback usually focuses on a few features that are most different from human expectation, ignoring the others. Also, they found that humans, especially lay people, are not good at correcting model explanations quantitatively (e.g., adjusting weights). This is consistent with the findings of Miller (2019) that human explanations are selective (in a biased way) and rarely refer to probabilities but express causal relationships instead.","[[None, 'b10', 'b62', 'b3'], [None]]","[[None, 'b10', 'b62', 'b3'], [None]]",5,"1. We would like humans to provide feedback for improving models, but do humans naturally want to?
2. Prior to the emerging of EBHD, studies found that humans are not willing to be constantly asked about labels of examples as if they were just simple oracles (Cakmak et al., 2010;Guillory and Bilmes, 2011).
3. Rather, they want to provide more than just data labels after being given explanations (Amershi et al., 2014;Smith-Renner et al., 2020).
4. By collecting free-form feedback from users, Ghai et al. (2021) discovered various feedback types.
5. The most prominent ones include removing-adding features (words), tuning weights, and leveraging feature combinations.  further analyzed categories of background knowledge underlying the feedback and found, in their experiment, that it was mainly based on commonsense knowledge and English language knowledge.
6. Such knowledge may not be efficiently injected into the model if we exploit human feedback which contains only labels.
7. This agrees with some participants, in (Smith-Renner et al., 2020), who described their feedback as inadequate when they could only confirm or correct predicted labels.
8. Although human feedback beyond labels contains helpful information, it is naturally neither complete nor precise.
9. Ghai et al. (2021) observed that human feedback usually focuses on a few features that are most different from human expectation, ignoring the others.
10. Also, they found that humans, especially lay people, are not good at correcting model explanations quantitatively (e.g., adjusting weights).
11. This is consistent with the findings of Miller (2019) that human explanations are selective (in a biased way) and rarely refer to probabilities but express causal relationships instead."
233476148,Explanation-Based Human Debugging of NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/d84ed05ab860b75f9e6b28e717abf4bc12da03d7,s22,Analyzing and Enhancing Efficiency,"['p22.0', 'p22.1', 'p22.2', 'p22.3']","['Most selected studies focus on improving correctness of the model (e.g., by expecting a higher F1 or a lower bias after debugging). However, only some of them discuss efficiency of the proposed frameworks. In general, we can analyze the efficiency of an EBHD framework by looking at the efficiency of each main step in Figure 1. Step 1 generates the explanations, so its efficiency depends on the explanation method used and, in the case of local explanation methods, the number of local explanations needed.', 'Step 2 lets humans give feedback, so its efficiency concerns the amount of time they spend to understand the explanations and to produce the feedback.', 'Step 3 updates the model using the feedback, so its efficiency relates to the time used for processing the feedback and retraining the model (if needed). Existing work mainly reported efficiency of step 1 or step 2. For instance, approaches using example-based explanations measured the improved performance with respect to the number of explanations computed (step 1) (Koh and Liang, 2017;Khanna et al., 2019;Han and Ghosh, 2020). Kulesza et al. (2015) compared the improved F1 of EBHD with the F1 of instance labelling given the same amount of time for humans to perform the task (step 2). Conversely, Yao et al. (2021) compared the time humans need to do EBHD versus instance labelling in order to achieve the equivalent degree of correctness improvement (step 2).', 'None of the selected studies considered the efficiency of the three steps altogether. In fact, the efficiency of step 1 and 3 is important especially for black box models where the cost of post-hoc explanation generation and model retraining is not negligible. It is even more crucial for iterative or responsive EBHD. Thus, analyzing and enhancing efficiency of EBHD frameworks (for both machine and human sides) require further research.']","Most selected studies focus on improving correctness of the model (e.g., by expecting a higher F1 or a lower bias after debugging). However, only some of them discuss efficiency of the proposed frameworks. In general, we can analyze the efficiency of an EBHD framework by looking at the efficiency of each main step in Figure 1. Step 1 generates the explanations, so its efficiency depends on the explanation method used and, in the case of local explanation methods, the number of local explanations needed.

Step 2 lets humans give feedback, so its efficiency concerns the amount of time they spend to understand the explanations and to produce the feedback.

Step 3 updates the model using the feedback, so its efficiency relates to the time used for processing the feedback and retraining the model (if needed). Existing work mainly reported efficiency of step 1 or step 2. For instance, approaches using example-based explanations measured the improved performance with respect to the number of explanations computed (step 1) (Koh and Liang, 2017;Khanna et al., 2019;Han and Ghosh, 2020). Kulesza et al. (2015) compared the improved F1 of EBHD with the F1 of instance labelling given the same amount of time for humans to perform the task (step 2). Conversely, Yao et al. (2021) compared the time humans need to do EBHD versus instance labelling in order to achieve the equivalent degree of correctness improvement (step 2).

None of the selected studies considered the efficiency of the three steps altogether. In fact, the efficiency of step 1 and 3 is important especially for black box models where the cost of post-hoc explanation generation and model retraining is not negligible. It is even more crucial for iterative or responsive EBHD. Thus, analyzing and enhancing efficiency of EBHD frameworks (for both machine and human sides) require further research.","(p22.0) Most selected studies focus on improving correctness of the model (e.g., by expecting a higher F1 or a lower bias after debugging). However, only some of them discuss efficiency of the proposed frameworks. In general, we can analyze the efficiency of an EBHD framework by looking at the efficiency of each main step in Figure 1. Step 1 generates the explanations, so its efficiency depends on the explanation method used and, in the case of local explanation methods, the number of local explanations needed.

(p22.1) Step 2 lets humans give feedback, so its efficiency concerns the amount of time they spend to understand the explanations and to produce the feedback.

(p22.2) Step 3 updates the model using the feedback, so its efficiency relates to the time used for processing the feedback and retraining the model (if needed). Existing work mainly reported efficiency of step 1 or step 2. For instance, approaches using example-based explanations measured the improved performance with respect to the number of explanations computed (step 1) (Koh and Liang, 2017;Khanna et al., 2019;Han and Ghosh, 2020). Kulesza et al. (2015) compared the improved F1 of EBHD with the F1 of instance labelling given the same amount of time for humans to perform the task (step 2). Conversely, Yao et al. (2021) compared the time humans need to do EBHD versus instance labelling in order to achieve the equivalent degree of correctness improvement (step 2).

(p22.3) None of the selected studies considered the efficiency of the three steps altogether. In fact, the efficiency of step 1 and 3 is important especially for black box models where the cost of post-hoc explanation generation and model retraining is not negligible. It is even more crucial for iterative or responsive EBHD. Thus, analyzing and enhancing efficiency of EBHD frameworks (for both machine and human sides) require further research.","[[], [], ['b72', 'b16', None, 'b19', 'b22'], []]","[[], [], ['b72', 'b16', None, 'b19', 'b22'], []]",5,"1. Most selected studies focus on improving correctness of the model (e.g., by expecting a higher F1 or a lower bias after debugging).
2. However, only some of them discuss efficiency of the proposed frameworks.
3. In general, we can analyze the efficiency of an EBHD framework by looking at the efficiency of each main step in Figure 1.
4. Step 1 generates the explanations, so its efficiency depends on the explanation method used and, in the case of local explanation methods, the number of local explanations needed.
5. Step 2 lets humans give feedback, so its efficiency concerns the amount of time they spend to understand the explanations and to produce the feedback.
6. Step 3 updates the model using the feedback, so its efficiency relates to the time used for processing the feedback and retraining the model (if needed).
7. Existing work mainly reported efficiency of step 1 or step 2.
8. For instance, approaches using example-based explanations measured the improved performance with respect to the number of explanations computed (step 1) (Koh and Liang, 2017;Khanna et al., 2019;Han and Ghosh, 2020).
9. Kulesza et al. (2015) compared the improved F1 of EBHD with the F1 of instance labelling given the same amount of time for humans to perform the task (step 2).
10. Conversely, Yao et al. (2021) compared the time humans need to do EBHD versus instance labelling in order to achieve the equivalent degree of correctness improvement (step 2).None of the selected studies considered the efficiency of the three steps altogether.
11. In fact, the efficiency of step 1 and 3 is important especially for black box models where the cost of post-hoc explanation generation and model retraining is not negligible.
12. It is even more crucial for iterative or responsive EBHD.
13. Thus, analyzing and enhancing efficiency of EBHD frameworks (for both machine and human sides) require further research."
233476148,Explanation-Based Human Debugging of NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/d84ed05ab860b75f9e6b28e717abf4bc12da03d7,s20,Beyond English Text Classification,"['p20.0', 'p20.1']","['All papers in Table 1 conducted experiments only on English datasets. We acknowledge that qualitatively analyzing explanations and feedback in languages at which one is not fluent is not easy, not to mention recruiting human subjects who know the languages. However, we hope that, with more multilingual data publicly available (Wolf et al., 2020) and growing awareness in the NLP community (Bender, 2019), there will be more EBHD studies targeting other languages in the near future.', 'Also, most existing EBHD works target text classifiers. It would be interesting to conduct more EBHD work for other NLP tasks such as reading comprehension, question answering, and natural language inference (NLI), to see whether existing techniques still work effectively. Shifting to other tasks requires an understanding of specific bug characteristics in those tasks. For instance, unlike bugs in text classification which are usually due to word artifacts, bugs in NLI concern syntactic heuristics between premises and hypotheses (Mc-Coy et al., 2019). Thus, giving human feedback at word level may not be helpful, and more advanced methods may be needed. Lakkaraju et al. (2020) remarked that the evaluation setup of existing EBHD work is often too easy or unrealistic. For example, bugs are obvious artifacts which could be removed using simple text pre-processing (e.g., removing punctuation and redacting named entities). Hence, it is not clear how powerful such EBHD frameworks are when dealing with real-world bugs. If bugs are not dominant and happen less often, global explanations may be too coarse-grained to capture them while many local explanations may be needed to spot a few appearances of the bugs, leading to inefficiency. As reported by Smith-Renner et al. (2020), feedback results in minor improvements when the model is already reasonably good.']","All papers in Table 1 conducted experiments only on English datasets. We acknowledge that qualitatively analyzing explanations and feedback in languages at which one is not fluent is not easy, not to mention recruiting human subjects who know the languages. However, we hope that, with more multilingual data publicly available (Wolf et al., 2020) and growing awareness in the NLP community (Bender, 2019), there will be more EBHD studies targeting other languages in the near future.

Also, most existing EBHD works target text classifiers. It would be interesting to conduct more EBHD work for other NLP tasks such as reading comprehension, question answering, and natural language inference (NLI), to see whether existing techniques still work effectively. Shifting to other tasks requires an understanding of specific bug characteristics in those tasks. For instance, unlike bugs in text classification which are usually due to word artifacts, bugs in NLI concern syntactic heuristics between premises and hypotheses (Mc-Coy et al., 2019). Thus, giving human feedback at word level may not be helpful, and more advanced methods may be needed. Lakkaraju et al. (2020) remarked that the evaluation setup of existing EBHD work is often too easy or unrealistic. For example, bugs are obvious artifacts which could be removed using simple text pre-processing (e.g., removing punctuation and redacting named entities). Hence, it is not clear how powerful such EBHD frameworks are when dealing with real-world bugs. If bugs are not dominant and happen less often, global explanations may be too coarse-grained to capture them while many local explanations may be needed to spot a few appearances of the bugs, leading to inefficiency. As reported by Smith-Renner et al. (2020), feedback results in minor improvements when the model is already reasonably good.","(p20.0) All papers in Table 1 conducted experiments only on English datasets. We acknowledge that qualitatively analyzing explanations and feedback in languages at which one is not fluent is not easy, not to mention recruiting human subjects who know the languages. However, we hope that, with more multilingual data publicly available (Wolf et al., 2020) and growing awareness in the NLP community (Bender, 2019), there will be more EBHD studies targeting other languages in the near future.

(p20.1) Also, most existing EBHD works target text classifiers. It would be interesting to conduct more EBHD work for other NLP tasks such as reading comprehension, question answering, and natural language inference (NLI), to see whether existing techniques still work effectively. Shifting to other tasks requires an understanding of specific bug characteristics in those tasks. For instance, unlike bugs in text classification which are usually due to word artifacts, bugs in NLI concern syntactic heuristics between premises and hypotheses (Mc-Coy et al., 2019). Thus, giving human feedback at word level may not be helpful, and more advanced methods may be needed. Lakkaraju et al. (2020) remarked that the evaluation setup of existing EBHD work is often too easy or unrealistic. For example, bugs are obvious artifacts which could be removed using simple text pre-processing (e.g., removing punctuation and redacting named entities). Hence, it is not clear how powerful such EBHD frameworks are when dealing with real-world bugs. If bugs are not dominant and happen less often, global explanations may be too coarse-grained to capture them while many local explanations may be needed to spot a few appearances of the bugs, leading to inefficiency. As reported by Smith-Renner et al. (2020), feedback results in minor improvements when the model is already reasonably good.","[['b7', 'b70'], [None, 'b27', 'b62']]","[['b7', 'b70'], [None, 'b27', 'b62']]",5,"1. All papers in Table 1 conducted experiments only on English datasets.
2. We acknowledge that qualitatively analyzing explanations and feedback in languages at which one is not fluent is not easy, not to mention recruiting human subjects who know the languages.
3. However, we hope that, with more multilingual data publicly available (Wolf et al., 2020) and growing awareness in the NLP community (Bender, 2019), there will be more EBHD studies targeting other languages in the near future.
4. Also, most existing EBHD works target text classifiers.
5. It would be interesting to conduct more EBHD work for other NLP tasks such as reading comprehension, question answering, and natural language inference (NLI), to see whether existing techniques still work effectively.
6. Shifting to other tasks requires an understanding of specific bug characteristics in those tasks.
7. For instance, unlike bugs in text classification which are usually due to word artifacts, bugs in NLI concern syntactic heuristics between premises and hypotheses (Mc-Coy et al., 2019).
8. Thus, giving human feedback at word level may not be helpful, and more advanced methods may be needed.
9. Lakkaraju et al. (2020) remarked that the evaluation setup of existing EBHD work is often too easy or unrealistic.
10. For example, bugs are obvious artifacts which could be removed using simple text pre-processing (e.g., removing punctuation and redacting named entities).
11. Hence, it is not clear how powerful such EBHD frameworks are when dealing with real-world bugs.
12. If bugs are not dominant and happen less often, global explanations may be too coarse-grained to capture them while many local explanations may be needed to spot a few appearances of the bugs, leading to inefficiency.
13. As reported by Smith-Renner et al. (2020), feedback results in minor improvements when the model is already reasonably good."
233476148,Explanation-Based Human Debugging of NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/d84ed05ab860b75f9e6b28e717abf4bc12da03d7,s16,Frustration,"['p16.0', 'p16.1']","['Working with explanations can cause frustration sometimes. Following the discussion on trust, explanations of poor models increase user frustration (as they reveal model flaws), whereas the ability to provide feedback reduces frustration. Hence, in general situations, the most frustrating condition is showing explanations to the users without allowing them to give feedback (Smith-Renner et al., 2020).', ""Another cause of frustration is the risk of detailed explanations overloading users (Narayanan et al., 2018). This is especially a crucial issue for inherently interpretable models where all the internal workings can be exposed to the users. Though presenting all the details is comprehensive and faithful, it could create barriers for lay users (Gershon, 1998). In fact, even ML experts may feel frustrated if they need to understand a decision tree with a depth of ten or more. Poursabzi-Sangdeh et al. (2018) found that showing all the model internals undermined users' ability to detect flaws in the model, likely due to information overload. So, they suggested that model internals should be revealed only when the users request to see them.""]","Working with explanations can cause frustration sometimes. Following the discussion on trust, explanations of poor models increase user frustration (as they reveal model flaws), whereas the ability to provide feedback reduces frustration. Hence, in general situations, the most frustrating condition is showing explanations to the users without allowing them to give feedback (Smith-Renner et al., 2020).

Another cause of frustration is the risk of detailed explanations overloading users (Narayanan et al., 2018). This is especially a crucial issue for inherently interpretable models where all the internal workings can be exposed to the users. Though presenting all the details is comprehensive and faithful, it could create barriers for lay users (Gershon, 1998). In fact, even ML experts may feel frustrated if they need to understand a decision tree with a depth of ten or more. Poursabzi-Sangdeh et al. (2018) found that showing all the model internals undermined users' ability to detect flaws in the model, likely due to information overload. So, they suggested that model internals should be revealed only when the users request to see them.","(p16.0) Working with explanations can cause frustration sometimes. Following the discussion on trust, explanations of poor models increase user frustration (as they reveal model flaws), whereas the ability to provide feedback reduces frustration. Hence, in general situations, the most frustrating condition is showing explanations to the users without allowing them to give feedback (Smith-Renner et al., 2020).

(p16.1) Another cause of frustration is the risk of detailed explanations overloading users (Narayanan et al., 2018). This is especially a crucial issue for inherently interpretable models where all the internal workings can be exposed to the users. Though presenting all the details is comprehensive and faithful, it could create barriers for lay users (Gershon, 1998). In fact, even ML experts may feel frustrated if they need to understand a decision tree with a depth of ten or more. Poursabzi-Sangdeh et al. (2018) found that showing all the model internals undermined users' ability to detect flaws in the model, likely due to information overload. So, they suggested that model internals should be revealed only when the users request to see them.","[['b62'], ['b45', None, 'b41']]","[['b62'], ['b45', None, 'b41']]",4,"1. Working with explanations can cause frustration sometimes.
2. Following the discussion on trust, explanations of poor models increase user frustration (as they reveal model flaws), whereas the ability to provide feedback reduces frustration.
3. Hence, in general situations, the most frustrating condition is showing explanations to the users without allowing them to give feedback (Smith-Renner et al., 2020).
4. Another cause of frustration is the risk of detailed explanations overloading users (Narayanan et al., 2018).
5. This is especially a crucial issue for inherently interpretable models where all the internal workings can be exposed to the users.
6. Though presenting all the details is comprehensive and faithful, it could create barriers for lay users (Gershon, 1998).
7. In fact, even ML experts may feel frustrated if they need to understand a decision tree with a depth of ten or more.
8. Poursabzi-Sangdeh et al. (2018) found that showing all the model internals undermined users' ability to detect flaws in the model, likely due to information overload.
9. So, they suggested that model internals should be revealed only when the users request to see them."
233476148,Explanation-Based Human Debugging of NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/d84ed05ab860b75f9e6b28e717abf4bc12da03d7,s8,Collecting Feedback,['p8.0'],"['After seeing explanations, humans generally desire to improve the model by giving feedback (Smith-Renner et al., 2020). Some existing work asked humans to confirm or correct machine-computed explanations. Hence, the form of feedback fairly depends on the form of the explanations, and in turn this shapes how to update the model too (discussed in section 2.2.3). For text classification, most EBHD papers asked humans to decide which words (WO) in the explanation (considered important by the model) are in fact relevant or irrelevant (Kulesza et al., 2010;Ribeiro et al., 2016;Teso and Kersting, 2019). Some papers even allowed humans to adjust the word importance scores (WS) (Kulesza et al., 2009(Kulesza et al., , 2015. This is analogous to specifying relevancy scores for example-based explanations (ES) in (Zylberajch et al., 2021). Meanwhile, feedback at the level of learned features (FE) (i.e., the internal neurons in the model) and learned rules (RU) rather than individual words, was asked in (Lertvittayakumjorn et al., 2020) and (Ribeiro et al., 2018b), respectively. Additionally, humans may be asked to check the predicted labels (Kulesza et al., 2009;Smith-Renner et al., 2020) or even the ground truth labels (collectively noted as LB in Ta It is likely that identifying important parts in the input is sufficient to make the model accomplish simple text classification tasks. However, this might not be enough for complex tasks which require reasoning. Recently, Yao et al. (2021) asked humans to provide, as feedback, compositional explanations to show how the humans would reason (RE) about the models\' failure cases. An example of the feedback for a hate speech detection is ""Because X is the word dumb, Y is a hateful word, and X is directly before Y , the attribution scores of both X and Y as well as the interaction score between X and Y should be increased"". To acquire richer information like this as feedback, their framework requires more expertise from the feedback providers. In the future, it would be interesting to explore how we can collect and utilize other forms of feedback, e.g., natural language feedback (Camburu et al., 2018), new training examples (Fiebrink et al., 2009), and other forms of decision rules used by humans (Carstens and Toni, 2017).']","After seeing explanations, humans generally desire to improve the model by giving feedback (Smith-Renner et al., 2020). Some existing work asked humans to confirm or correct machine-computed explanations. Hence, the form of feedback fairly depends on the form of the explanations, and in turn this shapes how to update the model too (discussed in section 2.2.3). For text classification, most EBHD papers asked humans to decide which words (WO) in the explanation (considered important by the model) are in fact relevant or irrelevant (Kulesza et al., 2010;Ribeiro et al., 2016;Teso and Kersting, 2019). Some papers even allowed humans to adjust the word importance scores (WS) (Kulesza et al., 2009(Kulesza et al., , 2015. This is analogous to specifying relevancy scores for example-based explanations (ES) in (Zylberajch et al., 2021). Meanwhile, feedback at the level of learned features (FE) (i.e., the internal neurons in the model) and learned rules (RU) rather than individual words, was asked in (Lertvittayakumjorn et al., 2020) and (Ribeiro et al., 2018b), respectively. Additionally, humans may be asked to check the predicted labels (Kulesza et al., 2009;Smith-Renner et al., 2020) or even the ground truth labels (collectively noted as LB in Ta It is likely that identifying important parts in the input is sufficient to make the model accomplish simple text classification tasks. However, this might not be enough for complex tasks which require reasoning. Recently, Yao et al. (2021) asked humans to provide, as feedback, compositional explanations to show how the humans would reason (RE) about the models' failure cases. An example of the feedback for a hate speech detection is ""Because X is the word dumb, Y is a hateful word, and X is directly before Y , the attribution scores of both X and Y as well as the interaction score between X and Y should be increased"". To acquire richer information like this as feedback, their framework requires more expertise from the feedback providers. In the future, it would be interesting to explore how we can collect and utilize other forms of feedback, e.g., natural language feedback (Camburu et al., 2018), new training examples (Fiebrink et al., 2009), and other forms of decision rules used by humans (Carstens and Toni, 2017).","(p8.0) After seeing explanations, humans generally desire to improve the model by giving feedback (Smith-Renner et al., 2020). Some existing work asked humans to confirm or correct machine-computed explanations. Hence, the form of feedback fairly depends on the form of the explanations, and in turn this shapes how to update the model too (discussed in section 2.2.3). For text classification, most EBHD papers asked humans to decide which words (WO) in the explanation (considered important by the model) are in fact relevant or irrelevant (Kulesza et al., 2010;Ribeiro et al., 2016;Teso and Kersting, 2019). Some papers even allowed humans to adjust the word importance scores (WS) (Kulesza et al., 2009(Kulesza et al., , 2015. This is analogous to specifying relevancy scores for example-based explanations (ES) in (Zylberajch et al., 2021). Meanwhile, feedback at the level of learned features (FE) (i.e., the internal neurons in the model) and learned rules (RU) rather than individual words, was asked in (Lertvittayakumjorn et al., 2020) and (Ribeiro et al., 2018b), respectively. Additionally, humans may be asked to check the predicted labels (Kulesza et al., 2009;Smith-Renner et al., 2020) or even the ground truth labels (collectively noted as LB in Ta It is likely that identifying important parts in the input is sufficient to make the model accomplish simple text classification tasks. However, this might not be enough for complex tasks which require reasoning. Recently, Yao et al. (2021) asked humans to provide, as feedback, compositional explanations to show how the humans would reason (RE) about the models' failure cases. An example of the feedback for a hate speech detection is ""Because X is the word dumb, Y is a hateful word, and X is directly before Y , the attribution scores of both X and Y as well as the interaction score between X and Y should be increased"". To acquire richer information like this as feedback, their framework requires more expertise from the feedback providers. In the future, it would be interesting to explore how we can collect and utilize other forms of feedback, e.g., natural language feedback (Camburu et al., 2018), new training examples (Fiebrink et al., 2009), and other forms of decision rules used by humans (Carstens and Toni, 2017).","[['b77', 'b49', 'b22', 'b30', 'b62', 'b72', 'b11', None, 'b12', 'b67', 'b24', 'b23', 'b51']]","[['b77', 'b49', 'b22', 'b30', 'b62', 'b72', 'b11', None, 'b12', 'b67', 'b24', 'b23', 'b51']]",13,"1. After seeing explanations, humans generally desire to improve the model by giving feedback (Smith-Renner et al., 2020).
2. Some existing work asked humans to confirm or correct machine-computed explanations.
3. Hence, the form of feedback fairly depends on the form of the explanations, and in turn this shapes how to update the model too (discussed in section 2.2.3).
4. For text classification, most EBHD papers asked humans to decide which words (WO) in the explanation (considered important by the model) are in fact relevant or irrelevant (Kulesza et al., 2010;Ribeiro et al., 2016;Teso and Kersting, 2019).
5. Some papers even allowed humans to adjust the word importance scores (WS) (Kulesza et al., 2009(Kulesza et al., , 2015. This is analogous to specifying relevancy scores for example-based explanations (ES) in (Zylberajch et al., 2021).
6. Meanwhile, feedback at the level of learned features (FE) (i.e., the internal neurons in the model) and learned rules (RU) rather than individual words, was asked in (Lertvittayakumjorn et al., 2020) and (Ribeiro et al., 2018b), respectively.
7. Additionally, humans may be asked to check the predicted labels (Kulesza et al., 2009;Smith-Renner et al., 2020) or even the ground truth labels (collectively noted as LB in Ta It is likely that identifying important parts in the input is sufficient to make the model accomplish simple text classification tasks. However, this might not be enough for complex tasks which require reasoning. Recently, Yao et al. (2021) asked humans to provide, as feedback, compositional explanations to show how the humans would reason (RE) about the models' failure cases.
8. An example of the feedback for a hate speech detection is ""Because X is the word dumb, Y is a hateful word, and X is directly before Y , the attribution scores of both X and Y as well as the interaction score between X and Y should be increased"".
9. To acquire richer information like this as feedback, their framework requires more expertise from the feedback providers.
10. In the future, it would be interesting to explore how we can collect and utilize other forms of feedback, e.g., natural language feedback (Camburu et al., 2018), new training examples (Fiebrink et al., 2009), and other forms of decision rules used by humans (Carstens and Toni, 2017)."
234093015,A Survey of Data Augmentation Approaches for NLP,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/63d8426ba1f51a8525dd19fd8ec92934ec71aea5,s1,Background,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5']","['What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.', 'What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.', 'Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.', 'Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.', 'Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.', 'Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.']","What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","(p1.0) What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

(p1.1) What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

(p1.2) Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

(p1.3) Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

(p1.4) Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

(p1.5) Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","[[None], [None, 'b23'], [None, 'b1'], [], [], [None]]","[[None], [None, 'b23'], [None, 'b1'], [], [], [None]]",6,"1. What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.
2. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020).
3. DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training.
4. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.
5. What are the goals and trade-offs?
6. Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).
7. As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance.
8. Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).
9. Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.
10. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.
11. Further, the distribution of augmented data should neither be too similar nor too different from the original.
12. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.
13. Effective DA approaches should aim for a balance.
14. Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.
15. Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.
16. Overall, there indeed appears to be a lack of research on why exactly DA works.
17. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles.
18. We discuss this challenge more in §6, and highlight some of the existing work below.
19. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2).
20. Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.
21. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization.
22. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant."
234093015,A Survey of Data Augmentation Approaches for NLP,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/63d8426ba1f51a8525dd19fd8ec92934ec71aea5,s4,Example Interpolation Techniques,"['p4.0', 'p4.1', 'p4.2']","['Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA). Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).', 'Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020). For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).', 'A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs. This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c). Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).']","Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA). Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).

Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020). For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).

A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs. This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c). Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).","(p4.0) Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA). Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).

(p4.1) Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020). For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).

(p4.2) A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs. This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c). Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).","[['b20', None], [None, 'b19'], [None]]","[['b20', None], [None, 'b19'], [None]]",5,"1. Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples.
2. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA).
3. Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).
4. Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements.
5. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism.
6. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020).
7. For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes.
8. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).
9. A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs.
10. This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c).
11. Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others.
12. SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α).
13. The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a)."
234093015,A Survey of Data Augmentation Approaches for NLP,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/63d8426ba1f51a8525dd19fd8ec92934ec71aea5,s3,Rule-Based Techniques,"['p3.0', 'p3.1']","['Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components. Feature space DA approaches generate augmented examples in the model\'s feature space rather than input data. Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4). Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold. Wei and Zou (2019) propose EASY DATA AUG-MENTATION (EDA), a set of token-level random perturbation operations including random insertion, deletion, and swap. They show improved performance on many text classification tasks. UDA (Xie et al., 2020) show how supervised DA methods can be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.', 'For paraphrase identification, Chen et al. (2020b) construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges. They use balance theory and transitivity to infer augmented sentence pairs from this graph. Motivated by image cropping and rotation, Şahin and Steedman (2018) propose dependency tree morphing. For dependency-annotated sentences, children of the same parent are swapped (à la rotation) or some deleted (à la cropping), as seen in Figure 2. This is most beneficial for language families with rich case marking systems (e.g. Baltic and Slavic).']","Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components. Feature space DA approaches generate augmented examples in the model's feature space rather than input data. Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4). Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold. Wei and Zou (2019) propose EASY DATA AUG-MENTATION (EDA), a set of token-level random perturbation operations including random insertion, deletion, and swap. They show improved performance on many text classification tasks. UDA (Xie et al., 2020) show how supervised DA methods can be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.

For paraphrase identification, Chen et al. (2020b) construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges. They use balance theory and transitivity to infer augmented sentence pairs from this graph. Motivated by image cropping and rotation, Şahin and Steedman (2018) propose dependency tree morphing. For dependency-annotated sentences, children of the same parent are swapped (à la rotation) or some deleted (à la cropping), as seen in Figure 2. This is most beneficial for language families with rich case marking systems (e.g. Baltic and Slavic).","(p3.0) Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components. Feature space DA approaches generate augmented examples in the model's feature space rather than input data. Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4). Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold. Wei and Zou (2019) propose EASY DATA AUG-MENTATION (EDA), a set of token-level random perturbation operations including random insertion, deletion, and swap. They show improved performance on many text classification tasks. UDA (Xie et al., 2020) show how supervised DA methods can be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.

(p3.1) For paraphrase identification, Chen et al. (2020b) construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges. They use balance theory and transitivity to infer augmented sentence pairs from this graph. Motivated by image cropping and rotation, Şahin and Steedman (2018) propose dependency tree morphing. For dependency-annotated sentences, children of the same parent are swapped (à la rotation) or some deleted (à la cropping), as seen in Figure 2. This is most beneficial for language families with rich case marking systems (e.g. Baltic and Slavic).","[['b7', None, 'b1'], [None]]","[['b7', None, 'b1'], [None]]",4,"1. Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components.
2. Feature space DA approaches generate augmented examples in the model's feature space rather than input data.
3. Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4).
4. Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold.
5. Wei and Zou (2019) propose EASY DATA AUG-MENTATION (EDA), a set of token-level random perturbation operations including random insertion, deletion, and swap.
6. They show improved performance on many text classification tasks.
7. UDA (Xie et al., 2020) show how supervised DA methods can be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.
8. For paraphrase identification, Chen et al. (2020b) construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges.
9. They use balance theory and transitivity to infer augmented sentence pairs from this graph.
10. Motivated by image cropping and rotation, Şahin and
11. Steedman (2018) propose dependency tree morphing.
12. For dependency-annotated sentences, children of the same parent are swapped (à la rotation) or some deleted (à la cropping), as seen in Figure 2.
13. This is most beneficial for language families with rich case marking systems (e.g. Baltic and Slavic)."
236460206,Towards Argument Mining for Social Good: A Survey,"Computer Science, Linguistics, Political Science",https://www.semanticscholar.org/paper/dcb0b23685c9c116d8d53fe47e5157753659d3bd,s6,NLP-Supported Moderation: desiderata and challenges,"['p6.0', 'p6.1', 'p6.2', 'p6.3', 'p6.4', 'p6.5']","['NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy. Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions. For example, in Social Sciences, one of the most discussed issues in crowd-scale deliberation is ""flaming"", i.e., aggressive and disrespectful communicative behavior (Lampe et al., 2014). Here, moderators could benefit from hate-speech and trolling detection methods in NLP. NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017). Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).', 'Beyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019). In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion. Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b) which has a counterpart in the framing detection task (Card et al., 2015;Akyürek et al., 2020), as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.', 'Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012;Waseem and Hovy, 2016;Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014;Kotonya and Toni, 2020), Facts recognition and source identification (Dusmanu et al., 2017).', 'How to represent discourse? Thus far, we have discussed the main ingredients of a rich NLP-informed approach to deliberative discourse. These components, together with the deliberationaugmented definition of AQ sketched in section 3 are the features that the NLP moderator takes as an input. One question remains open: How to represent the argumentative discourse within a contribution (e.g. a forum post) and across contributions (e.g. an entire online deliberation campaign)? We can approach also this question from an interdisciplinary perspective. Reference work in political science aims at modeling the mechanisms of political discourse in forms of discourse networks, as defined in Leifeld (2017). A discourse network is a bipartite graph, containing two classes of nodes: actors (e.g. Angela Merkel; the left-wing party; etc.) and claims (e.g. housing opportunities should be established for refugees); Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim. Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012;Haunss et al., 2013). Recent NLP research has targeted integration machine learning in the discourse network analysis workflow (Padó et al., 2019;Haunss et al., 2020). Crucially for AM, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim. This type of representation is perfectly compatible with a graph-based approach on argument representation which has already been established as to be preferred to a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).', 'Moderation can thus be modeled as optimization of specific quantitative properties of the discourse network: participant inclusion, can be enforced by ensuring that the contributions of peripheric actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution (the grounded notion of AQ we have been advocating thus far) can be quantified as the perturbation introduced in the network, with its long term effects on convergence or polarization.', 'Who moderates the (NLP) moderators? The problem of biased moderation obviously relates to the issue of bias in NLP (Blodgett et al., 2020;Caliskan et al., 2017;Bolukbasi et al., 2016;Spliethöver and Wachsmuth, 2020) and it has a clear implication in the application of NLP methods to moderation. For example, we would not want our NLP models to infer a negative impact on AQ from cues which just reveal that the user belongs to certain groups. This is a real risk when quality is equated to ""success"", in turn quantified in terms of likes, replies, retweets. The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them. Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be a very good move in the ""cooperation challenge"" -one that moderators (NLP or humans, online or in-person) have to ensure will not be left unexploited.']","NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy. Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions. For example, in Social Sciences, one of the most discussed issues in crowd-scale deliberation is ""flaming"", i.e., aggressive and disrespectful communicative behavior (Lampe et al., 2014). Here, moderators could benefit from hate-speech and trolling detection methods in NLP. NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017). Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).

Beyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019). In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion. Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b) which has a counterpart in the framing detection task (Card et al., 2015;Akyürek et al., 2020), as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.

Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012;Waseem and Hovy, 2016;Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014;Kotonya and Toni, 2020), Facts recognition and source identification (Dusmanu et al., 2017).

How to represent discourse? Thus far, we have discussed the main ingredients of a rich NLP-informed approach to deliberative discourse. These components, together with the deliberationaugmented definition of AQ sketched in section 3 are the features that the NLP moderator takes as an input. One question remains open: How to represent the argumentative discourse within a contribution (e.g. a forum post) and across contributions (e.g. an entire online deliberation campaign)? We can approach also this question from an interdisciplinary perspective. Reference work in political science aims at modeling the mechanisms of political discourse in forms of discourse networks, as defined in Leifeld (2017). A discourse network is a bipartite graph, containing two classes of nodes: actors (e.g. Angela Merkel; the left-wing party; etc.) and claims (e.g. housing opportunities should be established for refugees); Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim. Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012;Haunss et al., 2013). Recent NLP research has targeted integration machine learning in the discourse network analysis workflow (Padó et al., 2019;Haunss et al., 2020). Crucially for AM, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim. This type of representation is perfectly compatible with a graph-based approach on argument representation which has already been established as to be preferred to a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).

Moderation can thus be modeled as optimization of specific quantitative properties of the discourse network: participant inclusion, can be enforced by ensuring that the contributions of peripheric actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution (the grounded notion of AQ we have been advocating thus far) can be quantified as the perturbation introduced in the network, with its long term effects on convergence or polarization.

Who moderates the (NLP) moderators? The problem of biased moderation obviously relates to the issue of bias in NLP (Blodgett et al., 2020;Caliskan et al., 2017;Bolukbasi et al., 2016;Spliethöver and Wachsmuth, 2020) and it has a clear implication in the application of NLP methods to moderation. For example, we would not want our NLP models to infer a negative impact on AQ from cues which just reveal that the user belongs to certain groups. This is a real risk when quality is equated to ""success"", in turn quantified in terms of likes, replies, retweets. The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them. Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be a very good move in the ""cooperation challenge"" -one that moderators (NLP or humans, online or in-person) have to ensure will not be left unexploited.","(p6.0) NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy. Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions. For example, in Social Sciences, one of the most discussed issues in crowd-scale deliberation is ""flaming"", i.e., aggressive and disrespectful communicative behavior (Lampe et al., 2014). Here, moderators could benefit from hate-speech and trolling detection methods in NLP. NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017). Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).

(p6.1) Beyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019). In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion. Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b) which has a counterpart in the framing detection task (Card et al., 2015;Akyürek et al., 2020), as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.

(p6.2) Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012;Waseem and Hovy, 2016;Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014;Kotonya and Toni, 2020), Facts recognition and source identification (Dusmanu et al., 2017).

(p6.3) How to represent discourse? Thus far, we have discussed the main ingredients of a rich NLP-informed approach to deliberative discourse. These components, together with the deliberationaugmented definition of AQ sketched in section 3 are the features that the NLP moderator takes as an input. One question remains open: How to represent the argumentative discourse within a contribution (e.g. a forum post) and across contributions (e.g. an entire online deliberation campaign)? We can approach also this question from an interdisciplinary perspective. Reference work in political science aims at modeling the mechanisms of political discourse in forms of discourse networks, as defined in Leifeld (2017). A discourse network is a bipartite graph, containing two classes of nodes: actors (e.g. Angela Merkel; the left-wing party; etc.) and claims (e.g. housing opportunities should be established for refugees); Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim. Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012;Haunss et al., 2013). Recent NLP research has targeted integration machine learning in the discourse network analysis workflow (Padó et al., 2019;Haunss et al., 2020). Crucially for AM, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim. This type of representation is perfectly compatible with a graph-based approach on argument representation which has already been established as to be preferred to a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).

(p6.4) Moderation can thus be modeled as optimization of specific quantitative properties of the discourse network: participant inclusion, can be enforced by ensuring that the contributions of peripheric actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution (the grounded notion of AQ we have been advocating thus far) can be quantified as the perturbation introduced in the network, with its long term effects on convergence or polarization.

(p6.5) Who moderates the (NLP) moderators? The problem of biased moderation obviously relates to the issue of bias in NLP (Blodgett et al., 2020;Caliskan et al., 2017;Bolukbasi et al., 2016;Spliethöver and Wachsmuth, 2020) and it has a clear implication in the application of NLP methods to moderation. For example, we would not want our NLP models to infer a negative impact on AQ from cues which just reveal that the user belongs to certain groups. This is a real risk when quality is equated to ""success"", in turn quantified in terms of likes, replies, retweets. The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them. Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be a very good move in the ""cooperation challenge"" -one that moderators (NLP or humans, online or in-person) have to ensure will not be left unexploited.","[['b24', None, 'b19', 'b17'], ['b37', None, 'b2', 'b11'], ['b53', 'b39', 'b15', None, 'b59', 'b58'], ['b21', 'b3', 'b32', 'b31', 'b4', None, 'b22', 'b1'], [], [None, 'b43']]","[['b24', None, 'b19', 'b17'], ['b37', None, 'b2', 'b11'], ['b53', 'b39', 'b15', None, 'b59', 'b58'], ['b21', 'b3', 'b32', 'b31', 'b4', None, 'b22', 'b1'], [], [None, 'b43']]",24,"1. NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy.
2. Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions.
3. For example, in Social Sciences, one of the most discussed issues in crowd-scale deliberation is ""flaming"", i.e., aggressive and disrespectful communicative behavior (Lampe et al., 2014).
4. Here, moderators could benefit from hate-speech and trolling detection methods in NLP.
5. NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017).
6. Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).
7. Beyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019).
8. In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion.
9. Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b) which has a counterpart in the framing detection task (Card et al., 2015;Akyürek et al., 2020), as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.
10. Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012;Waseem and Hovy, 2016;Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014;Kotonya and Toni, 2020), Facts recognition and source identification (Dusmanu et al., 2017).How to represent discourse?
11. Thus far, we have discussed the main ingredients of a rich NLP-informed approach to deliberative discourse.
12. These components, together with the deliberationaugmented definition of AQ sketched in section 3 are the features that the NLP moderator takes as an input.
13. One question remains open: How to represent the argumentative discourse within a contribution (e.g. a forum post) and across contributions (e.g. an entire online deliberation campaign)?
14. We can approach also this question from an interdisciplinary perspective.
15. Reference work in political science aims at modeling the mechanisms of political discourse in forms of discourse networks, as defined in Leifeld (2017).
16. A discourse network is a bipartite graph, containing two classes of nodes: actors (e.g. Angela Merkel; the left-wing party; etc.) and claims (e.g. housing opportunities should be established for refugees); Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim.
17. Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012;Haunss et al., 2013).
18. Recent NLP research has targeted integration machine learning in the discourse network analysis workflow (Padó et al., 2019;Haunss et al., 2020).
19. Crucially for AM, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim.
20. This type of representation is perfectly compatible with a graph-based approach on argument representation which has already been established as to be preferred to a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).
21. Moderation can thus be modeled as optimization of specific quantitative properties of the discourse network: participant inclusion, can be enforced by ensuring that the contributions of peripheric actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution (the grounded notion of AQ we have been advocating thus far) can be quantified as the perturbation introduced in the network, with its long term effects on convergence or polarization.
22. Who moderates the (NLP) moderators?
23. The problem of biased moderation obviously relates to the issue of bias in NLP (Blodgett et al., 2020;Caliskan et al., 2017;Bolukbasi et al., 2016;Spliethöver and Wachsmuth, 2020) and it has a clear implication in the application of NLP methods to moderation.
24. For example, we would not want our NLP models to infer a negative impact on AQ from cues which just reveal that the user belongs to certain groups.
25. This is a real risk when quality is equated to ""success"", in turn quantified in terms of likes, replies, retweets.
26. The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them.
27. Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be a very good move in the ""cooperation challenge"" -one that moderators (NLP or humans, online or in-person) have to ensure will not be left unexploited."
236460206,Towards Argument Mining for Social Good: A Survey,"Computer Science, Linguistics, Political Science",https://www.semanticscholar.org/paper/dcb0b23685c9c116d8d53fe47e5157753659d3bd,s5,Grounding AQ in deliberation: moderation as a real-world application,"['p5.0', 'p5.1', 'p5.2', 'p5.3', 'p5.4', 'p5.5', 'p5.6']","['Grounding AQ in a discourse perspective which quantifies ""team-playing"" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies. We propose to make a first step in tackling this challenge by mapping it into a concrete application: (semi-)automatic moderation implemented as a form of discourse optimization, or, as it is commonly referred to in the Social Sciences, facilitation (Kaner et al., 2007;Trénel, 2009). To illustrate the dynamics of moderation, let us start from concrete examples from a deliberation platform, RegulationRoom. This discussion forum has been employed by public institutions to gather citizens contributions on discussions targeting very heterogeneous issues (more details can be found in Appendix). Let us consider the following example from a discussion on the distracted driving by commercial vehicle operators (e.g., truckers and bus drivers). The posts we selected (arrows indicate comment nesting) are from the discussion sub-thread: Texting -what are the risks? 5 The example involves two users who clearly differ in their argumentation style and position. User 1 has a clear position on the topic (claim in bold: not just texting, but all cellphone interactions should be banned), which she/he supports with personal reports (underlined text) an emotional tone, and a style which is typical of social media text. User 2 replies, opening the post on a sarcastic note, which serves as the first premise to her/his (implicit) claim which is encoded in three rethorical questions (in bold): there should be no restrictions at all, because imposing them would be unfair. This is the case because (premises underlined): any distraction can cause an accident, some people are capable of using their phone while driving, people who spend lot of time in the car for professional reasons still need to communicate with loved ones. A moderator then joins the discussion to (a) provide a clarification as to why the focus is on texting and a link to further information on the matter, and (b) ask User 2 to elaborate on the personal communication issue, and to propose alternatives. In the Appendix we report another example from the same topic and thread, where the user acts as a problematizer, challenging the scope and definition of the rule under discussion and the moderator acts as a ""discourse traffic director"", pointing out that the user should read and contribute to different threads in the discussion.', ""The guidelines for human moderators in Reg-ulationRoom have been defined in advance in a 'moderator protocol' (eRulemaking Initiative et al., 2017) which reflect the moderator actions mentioned in the examples. In the protocol the moderator roles were divided into two main classes. Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (policing), or helping with technical difficulties. Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse. As the examples above clearly show, this can both mean that the moderator encourages exchanges between discourse participants and participation in other posts (broadening the scope of the discussion), or helping users to improve the content of their posts (requests for clarification, focusing on one topic, substantive reasoning, sharing personal experiences)."", 'RegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants. However, to the best of our knowledge, there is little to no NLP work targeting moderation modeling. Park et al. (2012) used data from Regula-tionRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol. Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment. However this work is limited as it only focuses on two types of moderator interventions (broadening the scope of the discussion, improving argument quality) and as it does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as ""in need for moderation"".', 'Besides the concrete example of Regulation-Room, moderation and discourse facilitation have been, and still are, a crucial topic in digital democracy. 6 The know-how of digital democracy experts is an invaluable starting point for the application of AM to moderation, as current research targets both the integration of digital solutions to facilitate online campaigns, and a critical reflection of the effects of such innovations on the deliberation outcomes.', 'Digital innovation supporting deliberation Argument maps (Walton, 2005) are widely employed to support online discussions, as an emerging optimization of the deliberation. Given a specific topic, for example possible reactions to climate change, users who wish to contribute to the discussion are requested to structure their contribution by producing an item in a conceptual map and optionally writing an accompanying post. Their contribution to the argument maps is often reviewed by a moderator. So in a sense, the argument map for a given deliberation process is the outcome of a process that comes both from below (the user) and above (the moderator).', ""Thanks to argument maps, the overall discourse picture can be overviewed and it is easier for the group of contributors to express support for one (or many) of the available options, without having to read a large number of long posts. An example of this approach is represented in Deliberatorium 7 , an e-deliberation platform which has been extensively employed in many reference studies on the effect of digital innovation on deliberation (Klein, 2011). Another example of a digital deliberation platform which integrates argument maps and offers an option for moderation is COLAGREE (Yang et al., 2021;Ito, 2018). Among the studies testing the impact of such digital platforms on online deliberation, Spada et al. (2015) tests the effect of Deliberatorium's argument maps on an online discussion among the supporters of the Italian Democratic party concerning the desired features of electoral law to be proposed by the party to the Parliament. This study compared the discussion of users employing Deliberatorium and a control group using a traditional forum format which was then encoded into argument maps. The comparison showed that the argument map modality did not discourage participation, and while it appeared to make users less creative (fewer new ideas as compared to the traditional forum), it also reduced the rate of claims without further discussion."", ""Yet, the need for trained moderators tends to be a significant bottleneck (both in terms of time and of costs) in digital deliberation. Moreover, empirical research on the effect of moderation on deliberation has uncovered the risks of biased moderation. For example, the experiment in Spada and Vreeland (2013) tests the extent to which moderators can influence participants' behavior by expressing their views during the moderation process.""]","Grounding AQ in a discourse perspective which quantifies ""team-playing"" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies. We propose to make a first step in tackling this challenge by mapping it into a concrete application: (semi-)automatic moderation implemented as a form of discourse optimization, or, as it is commonly referred to in the Social Sciences, facilitation (Kaner et al., 2007;Trénel, 2009). To illustrate the dynamics of moderation, let us start from concrete examples from a deliberation platform, RegulationRoom. This discussion forum has been employed by public institutions to gather citizens contributions on discussions targeting very heterogeneous issues (more details can be found in Appendix). Let us consider the following example from a discussion on the distracted driving by commercial vehicle operators (e.g., truckers and bus drivers). The posts we selected (arrows indicate comment nesting) are from the discussion sub-thread: Texting -what are the risks? 5 The example involves two users who clearly differ in their argumentation style and position. User 1 has a clear position on the topic (claim in bold: not just texting, but all cellphone interactions should be banned), which she/he supports with personal reports (underlined text) an emotional tone, and a style which is typical of social media text. User 2 replies, opening the post on a sarcastic note, which serves as the first premise to her/his (implicit) claim which is encoded in three rethorical questions (in bold): there should be no restrictions at all, because imposing them would be unfair. This is the case because (premises underlined): any distraction can cause an accident, some people are capable of using their phone while driving, people who spend lot of time in the car for professional reasons still need to communicate with loved ones. A moderator then joins the discussion to (a) provide a clarification as to why the focus is on texting and a link to further information on the matter, and (b) ask User 2 to elaborate on the personal communication issue, and to propose alternatives. In the Appendix we report another example from the same topic and thread, where the user acts as a problematizer, challenging the scope and definition of the rule under discussion and the moderator acts as a ""discourse traffic director"", pointing out that the user should read and contribute to different threads in the discussion.

The guidelines for human moderators in Reg-ulationRoom have been defined in advance in a 'moderator protocol' (eRulemaking Initiative et al., 2017) which reflect the moderator actions mentioned in the examples. In the protocol the moderator roles were divided into two main classes. Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (policing), or helping with technical difficulties. Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse. As the examples above clearly show, this can both mean that the moderator encourages exchanges between discourse participants and participation in other posts (broadening the scope of the discussion), or helping users to improve the content of their posts (requests for clarification, focusing on one topic, substantive reasoning, sharing personal experiences).

RegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants. However, to the best of our knowledge, there is little to no NLP work targeting moderation modeling. Park et al. (2012) used data from Regula-tionRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol. Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment. However this work is limited as it only focuses on two types of moderator interventions (broadening the scope of the discussion, improving argument quality) and as it does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as ""in need for moderation"".

Besides the concrete example of Regulation-Room, moderation and discourse facilitation have been, and still are, a crucial topic in digital democracy. 6 The know-how of digital democracy experts is an invaluable starting point for the application of AM to moderation, as current research targets both the integration of digital solutions to facilitate online campaigns, and a critical reflection of the effects of such innovations on the deliberation outcomes.

Digital innovation supporting deliberation Argument maps (Walton, 2005) are widely employed to support online discussions, as an emerging optimization of the deliberation. Given a specific topic, for example possible reactions to climate change, users who wish to contribute to the discussion are requested to structure their contribution by producing an item in a conceptual map and optionally writing an accompanying post. Their contribution to the argument maps is often reviewed by a moderator. So in a sense, the argument map for a given deliberation process is the outcome of a process that comes both from below (the user) and above (the moderator).

Thanks to argument maps, the overall discourse picture can be overviewed and it is easier for the group of contributors to express support for one (or many) of the available options, without having to read a large number of long posts. An example of this approach is represented in Deliberatorium 7 , an e-deliberation platform which has been extensively employed in many reference studies on the effect of digital innovation on deliberation (Klein, 2011). Another example of a digital deliberation platform which integrates argument maps and offers an option for moderation is COLAGREE (Yang et al., 2021;Ito, 2018). Among the studies testing the impact of such digital platforms on online deliberation, Spada et al. (2015) tests the effect of Deliberatorium's argument maps on an online discussion among the supporters of the Italian Democratic party concerning the desired features of electoral law to be proposed by the party to the Parliament. This study compared the discussion of users employing Deliberatorium and a control group using a traditional forum format which was then encoded into argument maps. The comparison showed that the argument map modality did not discourage participation, and while it appeared to make users less creative (fewer new ideas as compared to the traditional forum), it also reduced the rate of claims without further discussion.

Yet, the need for trained moderators tends to be a significant bottleneck (both in terms of time and of costs) in digital deliberation. Moreover, empirical research on the effect of moderation on deliberation has uncovered the risks of biased moderation. For example, the experiment in Spada and Vreeland (2013) tests the extent to which moderators can influence participants' behavior by expressing their views during the moderation process.","(p5.0) Grounding AQ in a discourse perspective which quantifies ""team-playing"" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies. We propose to make a first step in tackling this challenge by mapping it into a concrete application: (semi-)automatic moderation implemented as a form of discourse optimization, or, as it is commonly referred to in the Social Sciences, facilitation (Kaner et al., 2007;Trénel, 2009). To illustrate the dynamics of moderation, let us start from concrete examples from a deliberation platform, RegulationRoom. This discussion forum has been employed by public institutions to gather citizens contributions on discussions targeting very heterogeneous issues (more details can be found in Appendix). Let us consider the following example from a discussion on the distracted driving by commercial vehicle operators (e.g., truckers and bus drivers). The posts we selected (arrows indicate comment nesting) are from the discussion sub-thread: Texting -what are the risks? 5 The example involves two users who clearly differ in their argumentation style and position. User 1 has a clear position on the topic (claim in bold: not just texting, but all cellphone interactions should be banned), which she/he supports with personal reports (underlined text) an emotional tone, and a style which is typical of social media text. User 2 replies, opening the post on a sarcastic note, which serves as the first premise to her/his (implicit) claim which is encoded in three rethorical questions (in bold): there should be no restrictions at all, because imposing them would be unfair. This is the case because (premises underlined): any distraction can cause an accident, some people are capable of using their phone while driving, people who spend lot of time in the car for professional reasons still need to communicate with loved ones. A moderator then joins the discussion to (a) provide a clarification as to why the focus is on texting and a link to further information on the matter, and (b) ask User 2 to elaborate on the personal communication issue, and to propose alternatives. In the Appendix we report another example from the same topic and thread, where the user acts as a problematizer, challenging the scope and definition of the rule under discussion and the moderator acts as a ""discourse traffic director"", pointing out that the user should read and contribute to different threads in the discussion.

(p5.1) The guidelines for human moderators in Reg-ulationRoom have been defined in advance in a 'moderator protocol' (eRulemaking Initiative et al., 2017) which reflect the moderator actions mentioned in the examples. In the protocol the moderator roles were divided into two main classes. Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (policing), or helping with technical difficulties. Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse. As the examples above clearly show, this can both mean that the moderator encourages exchanges between discourse participants and participation in other posts (broadening the scope of the discussion), or helping users to improve the content of their posts (requests for clarification, focusing on one topic, substantive reasoning, sharing personal experiences).

(p5.2) RegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants. However, to the best of our knowledge, there is little to no NLP work targeting moderation modeling. Park et al. (2012) used data from Regula-tionRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol. Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment. However this work is limited as it only focuses on two types of moderator interventions (broadening the scope of the discussion, improving argument quality) and as it does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as ""in need for moderation"".

(p5.3) Besides the concrete example of Regulation-Room, moderation and discourse facilitation have been, and still are, a crucial topic in digital democracy. 6 The know-how of digital democracy experts is an invaluable starting point for the application of AM to moderation, as current research targets both the integration of digital solutions to facilitate online campaigns, and a critical reflection of the effects of such innovations on the deliberation outcomes.

(p5.4) Digital innovation supporting deliberation Argument maps (Walton, 2005) are widely employed to support online discussions, as an emerging optimization of the deliberation. Given a specific topic, for example possible reactions to climate change, users who wish to contribute to the discussion are requested to structure their contribution by producing an item in a conceptual map and optionally writing an accompanying post. Their contribution to the argument maps is often reviewed by a moderator. So in a sense, the argument map for a given deliberation process is the outcome of a process that comes both from below (the user) and above (the moderator).

(p5.5) Thanks to argument maps, the overall discourse picture can be overviewed and it is easier for the group of contributors to express support for one (or many) of the available options, without having to read a large number of long posts. An example of this approach is represented in Deliberatorium 7 , an e-deliberation platform which has been extensively employed in many reference studies on the effect of digital innovation on deliberation (Klein, 2011). Another example of a digital deliberation platform which integrates argument maps and offers an option for moderation is COLAGREE (Yang et al., 2021;Ito, 2018). Among the studies testing the impact of such digital platforms on online deliberation, Spada et al. (2015) tests the effect of Deliberatorium's argument maps on an online discussion among the supporters of the Italian Democratic party concerning the desired features of electoral law to be proposed by the party to the Parliament. This study compared the discussion of users employing Deliberatorium and a control group using a traditional forum format which was then encoded into argument maps. The comparison showed that the argument map modality did not discourage participation, and while it appeared to make users less creative (fewer new ideas as compared to the traditional forum), it also reduced the rate of claims without further discussion.

(p5.6) Yet, the need for trained moderators tends to be a significant bottleneck (both in terms of time and of costs) in digital deliberation. Moreover, empirical research on the effect of moderation on deliberation has uncovered the risks of biased moderation. For example, the experiment in Spada and Vreeland (2013) tests the extent to which moderators can influence participants' behavior by expressing their views during the moderation process.","[['b12', 'b51'], [], ['b34'], [None], ['b57'], ['b13', 'b41', 'b60', 'b8'], ['b42']]","[['b12', 'b51'], [], ['b34'], [None], ['b57'], ['b13', 'b41', 'b60', 'b8'], ['b42']]",10,"1. Grounding AQ in a discourse perspective which quantifies ""team-playing"" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies.
2. We propose to make a first step in tackling this challenge by mapping it into a concrete application: (semi-)automatic moderation implemented as a form of discourse optimization, or, as it is commonly referred to in the Social Sciences, facilitation (Kaner et al., 2007;Trénel, 2009).
3. To illustrate the dynamics of moderation, let us start from concrete examples from a deliberation platform, RegulationRoom.
4. This discussion forum has been employed by public institutions to gather citizens contributions on discussions targeting very heterogeneous issues (more details can be found in Appendix).
5. Let us consider the following example from a discussion on the distracted driving by commercial vehicle operators (e.g., truckers and bus drivers).
6. The posts we selected (arrows indicate comment nesting) are from the discussion sub-thread: Texting -what are the risks?
7. 5 The example involves two users who clearly differ in their argumentation style and position.
8. User 1 has a clear position on the topic (claim in bold: not just texting, but all cellphone interactions should be banned), which she/he supports with personal reports (underlined text) an emotional tone, and a style which is typical of social media text.
9. User 2 replies, opening the post on a sarcastic note, which serves as the first premise to her/his (implicit) claim which is encoded in three rethorical questions (in bold): there should be no restrictions at all, because imposing them would be unfair.
10. This is the case because (premises underlined): any distraction can cause an accident, some people are capable of using their phone while driving, people who spend lot of time in the car for professional reasons still need to communicate with loved ones.
11. A moderator then joins the discussion to (a) provide a clarification as to why the focus is on texting and a link to further information on the matter, and (b) ask User 2 to elaborate on the personal communication issue, and to propose alternatives.
12. In the Appendix we report another example from the same topic and thread, where the user acts as a problematizer, challenging the scope and definition of the rule under discussion and the moderator acts as a ""discourse traffic director"", pointing out that the user should read and contribute to different threads in the discussion.
13. The guidelines for human moderators in Reg-ulationRoom have been defined in advance in a 'moderator protocol' (eRulemaking Initiative et al., 2017) which reflect the moderator actions mentioned in the examples.
14. In the protocol the moderator roles were divided into two main classes.
15. Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (policing), or helping with technical difficulties.
16. Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse.
17. As the examples above clearly show, this can both mean that the moderator encourages exchanges between discourse participants and participation in other posts (broadening the scope of the discussion), or helping users to improve the content of their posts (requests for clarification, focusing on one topic, substantive reasoning, sharing personal experiences).RegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants.
18. However, to the best of our knowledge, there is little to no NLP work targeting moderation modeling.
19. Park et al. (2012) used data from Regula-tionRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol.
20. Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment.
21. However this work is limited as it only focuses on two types of moderator interventions (broadening the scope of the discussion, improving argument quality) and as it does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as ""in need for moderation"".
22. Besides the concrete example of Regulation-Room, moderation and discourse facilitation have been, and still are, a crucial topic in digital democracy.
23. 6 The know-how of digital democracy experts is an invaluable starting point for the application of AM to moderation, as current research targets both the integration of digital solutions to facilitate online campaigns, and a critical reflection of the effects of such innovations on the deliberation outcomes.
24. Digital innovation supporting deliberation Argument maps (Walton, 2005) are widely employed to support online discussions, as an emerging optimization of the deliberation.
25. Given a specific topic, for example possible reactions to climate change, users who wish to contribute to the discussion are requested to structure their contribution by producing an item in a conceptual map and optionally writing an accompanying post.
26. Their contribution to the argument maps is often reviewed by a moderator.
27. So in a sense, the argument map for a given deliberation process is the outcome of a process that comes both from below (the user) and above (the moderator).
28. Thanks to argument maps, the overall discourse picture can be overviewed and it is easier for the group of contributors to express support for one (or many) of the available options, without having to read a large number of long posts.
29. An example of this approach is represented in Deliberatorium 7 , an e-deliberation platform which has been extensively employed in many reference studies on the effect of digital innovation on deliberation (Klein, 2011).
30. Another example of a digital deliberation platform which integrates argument maps and offers an option for moderation is COLAGREE (Yang et al., 2021;Ito, 2018).
31. Among the studies testing the impact of such digital platforms on online deliberation, Spada et al. (2015) tests the effect of Deliberatorium's argument maps on an online discussion among the supporters of the Italian Democratic party concerning the desired features of electoral law to be proposed by the party to the Parliament.
32. This study compared the discussion of users employing Deliberatorium and a control group using a traditional forum format which was then encoded into argument maps.
33. The comparison showed that the argument map modality did not discourage participation, and while it appeared to make users less creative (fewer new ideas as compared to the traditional forum), it also reduced the rate of claims without further discussion.
34. Yet, the need for trained moderators tends to be a significant bottleneck (both in terms of time and of costs) in digital deliberation.
35. Moreover, empirical research on the effect of moderation on deliberation has uncovered the risks of biased moderation.
36. For example, the experiment in Spada and Vreeland (2013) tests the extent to which moderators can influence participants' behavior by expressing their views during the moderation process."
236460206,Towards Argument Mining for Social Good: A Survey,"Computer Science, Linguistics, Political Science",https://www.semanticscholar.org/paper/dcb0b23685c9c116d8d53fe47e5157753659d3bd,s3,Scaling Up Argument Mining,"['p3.0', 'p3.1', 'p3.2', 'p3.3', 'p3.4', 'p3.5', 'p3.6', 'p3.7']","['In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.', 'Social media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020). Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.', 'Recent work has aimed to tackle such challenges in social media. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.', 'Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020). On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.', 'The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.', 'Multilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.', 'Various recent studies have investigated multilinguality for AM. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.', 'Other work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019). This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.']","In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.

Social media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020). Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.

Recent work has aimed to tackle such challenges in social media. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.

Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020). On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.

The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.

Multilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.

Various recent studies have investigated multilinguality for AM. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.

Other work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019). This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.","(p3.0) In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.

(p3.1) Social media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020). Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.

(p3.2) Recent work has aimed to tackle such challenges in social media. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.

(p3.3) Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020). On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.

(p3.4) The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.

(p3.5) Multilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.

(p3.6) Various recent studies have investigated multilinguality for AM. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.

(p3.7) Other work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019). This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.","[[], ['b15'], [None], ['b38'], [], [None], [None, 'b16', 'b52'], [None, 'b38', 'b25', 'b62']]","[[], ['b15'], [None], ['b38'], [], [None], [None, 'b16', 'b52'], [None, 'b38', 'b25', 'b62']]",11,"1. In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.
2. Social media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons.
3. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users.
4. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020).
5. Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates.
6. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.
7. Recent work has aimed to tackle such challenges in social media.
8. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification.
9. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics.
10. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.
11. Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020).
12. On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited.
13. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets.
14. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis.
15. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.
16. The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges.
17. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.
18. Multilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale.
19. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.
20. Various recent studies have investigated multilinguality for AM.
21. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese.
22. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling.
23. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores.
24. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.
25. Other work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages.
26. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019).
27. This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact."
236460206,Towards Argument Mining for Social Good: A Survey,"Computer Science, Linguistics, Political Science",https://www.semanticscholar.org/paper/dcb0b23685c9c116d8d53fe47e5157753659d3bd,s2,Framework,"['p2.0', 'p2.1', 'p2.2', 'p2.3', 'p2.4', 'p2.5', 'p2.6', 'p2.7', 'p2.8', 'p2.9', 'p2.10', 'p2.11', 'p2.12']","['Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).', 'Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.', 'To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.', 'Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.', 'A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.', ""Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation."", 'A1: Marvel Universe is better than DC Universe.', ""A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses."", 'A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.', 'A4: This is especially true due to his unfortunate passing.', ""A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics."", ""The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise)."", ""Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).""]","Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

A1: Marvel Universe is better than DC Universe.

A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

A4: This is especially true due to his unfortunate passing.

A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","(p2.0) Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

(p2.1) Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

(p2.2) To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

(p2.3) Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

(p2.4) A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

(p2.5) Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

(p2.6) A1: Marvel Universe is better than DC Universe.

(p2.7) A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

(p2.8) A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

(p2.9) A4: This is especially true due to his unfortunate passing.

(p2.10) A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

(p2.11) The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

(p2.12) Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","[['b29', None, 'b26', 'b28'], ['b49', 'b14'], ['b31', 'b44', None, 'b35'], [], [], [], [], [], [], [], [], [], ['b44']]","[['b29', None, 'b26', 'b28'], ['b49', 'b14'], ['b31', 'b44', None, 'b35'], [], [], [], [], [], [], [], [], [], ['b44']]",11,"1. Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM.
2. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.
3. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).
4. A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).Relation assignment
5. The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.
6. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).
7. Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019).
8. Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate).
9. As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues.
10. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.
11. To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017).
12. Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020).
13. Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly.
14. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?""
15. (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects.
16. Vaccinated people become immune to a certain pathogen and do not develop a disease.
17. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.
18. A2: Many vaccines have serious and sometimes deadly side effects.
19. With many vaccines the immunity is not lifelong.
20. Sometimes the vaccines itself can cause a serious disease to develop as a side effect.
21. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.
22. Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted.
23. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it.
24. However, consider another example, extracted from an online debate platform Kialo 4 .
25. Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.
26. A1: Marvel Universe is better than DC Universe.
27. A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.
28. A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.
29. A4: This is especially true due to his unfortunate passing.
30. A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.
31. The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components.
32. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level.
33. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).
34. Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict.
35. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized.
36. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.)."
236460206,Towards Argument Mining for Social Good: A Survey,"Computer Science, Linguistics, Political Science",https://www.semanticscholar.org/paper/dcb0b23685c9c116d8d53fe47e5157753659d3bd,s4,Argument Quality: An Integrated Definition,"['p4.0', 'p4.1', 'p4.2', 'p4.3', 'p4.4', 'p4.5', 'p4.6', 'p4.7', 'p4.8', 'p4.9', 'p4.10', 'p4.11']","['The second stage in the framework of AM is defined as relation assignment (c.f. Section 2.1); a complex task that aims to predict the relations holding between the arguments defined in the first stage. Being able to model the relations between arguments and components within the structure, for example in argument graphs (Besnard and Hunter, 2014; Craven and Toni, 2016), allows us to actually work with the argumentative text in an applicationbased setting, understand the stance and context of arguments, and develop a story for the consequential impact of arguments on the discourse, among other things. Generally speaking, we can use this task as an approach to analyze argument quality (AQ). However, within the AM community, an open question concerns the adequate definition and operationalization of the notion of AQ. Despite this, to move forward with the task of AQ analysis and to create large corpora with crowd-sourced annotations, some approaches rely on the relative assessment of quality: Given two arguments, which is more convincing? (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020) Thus the natural way of quantifying the success of an argument is in terms of its persuasiveness. Indeed, plenty of previous work has explored the many factors which contribute to the persuasiveness of a message: the linguistic features employed by the authors (Persing and Ng, 2017), the semantic type of claims and premises (Hidey et al., 2017), the different sources of evidence produced to support an argument (Addawood and Bashir, 2016), the effects of the personality traits and prior beliefs on persuasiveness (Lukin et al., 2017;Durmus and Cardie, 2018;Al Khatib et al., 2020), the interaction with other participants (Ji et al., 2018;Egawa et al., 2020), the use of argument invention when debating about unknown topics (Bilu et al., 2019), the structure of the arguments (Li et al., 2020), and the effect of the style of the text in achieving persuasion (El Baff et al., 2020).', 'Persuasiveness is, however, not the only way to define whether an argument is good -at least not from a deliberation point of view. A good contribution to a debate is one which uncovers a previously unnoticed aspect of a problem, thus generating a perturbation in the discourse (controversies can be productive!). Or else, a good contribution is one that settles an issue, by stating the differences between opposing views and allowing the discourse to stabilize in a series of clusters (convergence on just one position is not necessarily a good outcome).', 'Most recent research projects (Wachsmuth et al., 2017b) aim to address the challenge of redefining the notion of AQ, away from persuasiveness and towards a more ""situated"" definition which has to do with the needs of argumentation in a real-world scenario. This new definition has been the basis for the creation of new corpora from different domains , where feature-based (Wachsmuth and Werner, 2020) and neural models were tested for automatic prediction . Other aspects of AQ have become the subject of AM research such as the relevance and impact of arguments (Durmus et al., 2019), the verifiability (Park and Cardie, 2018), local acceptability (Yang et al., 2019) and the best ""deliberative move"" (Al-Khatib et al., 2018).', 'We argue that this shift is necessary for two reasons: (1) Working with real-world applications of AM naturally forces us into the more heterogeneous realm of data structures, such as social media, in which language, structure, and content are less uniform and confined to the classic notion of logical debate; and (2) In order to encourage deliberation from an open audience of citizens, we need to redefine our concept of AQ and productive discourse such that there is equal worth and participation granted to each contributor of the argument.', 'Deliberative Quality We therefore propose adapting the definition of quality to integrate the abundant research on the topic from the field of Social Sciences. Here, the quality of a discourse has been investigated in the context of deliberation with the focus on inclusivity: how can the interplay of the different participants in the discourse lead to an optimal outcome for the collective? The focus here is not on the quality of the individual contributions. Instead, an overall quality of the discourse is determined by the fact that the individual quality dimensions are distributed among different contributions (e.g some participants do more rational reasoning, others share personal experiences). We would like to integrate those aspects that focus on inclusivity and cooperation.', 'Similar to Wachsmuth et al. (2017b), social scientists have developed a taxonomy, the discourse quality index (DQI), that describes the different desirable aspects of a discourse (Steenbergen et al., 2003). This taxonomy has been used to analyze the quality of deliberation in different contexts, ranging from more formal contexts, such as parliamentary debates (Steiner et al., 2005), to informal discussions in online forums (Trénel, 2004). Both implementations integrate logical coherence as one dimension, cogency in Wachsmuth et al. (2017b), justification in the DQI. Some aspects of inclusivity are also being touched upon in the rhetorical and dialectical dimension of Wachsmuth et al. (2017b), such as using appropriate language (Appropriateness) or whether an argument supports conflict resolution (global relevance). We concentrate on the following dimensions from the DQI, which particularly focus on the collaborative aspect of discourse.', '• Respect: this dimension includes respectful tone, respect for other social groups/backgrounds, and openness towards other opinions.', '• Equality / Participation: it is not desirable that some dominant participants make the bulk of contributions while many others remain passive. All participants should have equal opportunities to contribute and all topics, including those that DQI (Steenbergen et al., 2003)  • Interactivity: beyond simply sharing opinions, acknowledging other viewpoints and interacting with other participants through listening and responding lead to new perspectives arising -compromises can emerge.', '• Testimoniality / Report of personal accounts:', 'sharing stories and personal narratives as an alternative form of communication can involve more people in the discourse, especially those who cannot identify themselves with rational argumentation. It can also make other participants aware of other perspectives as it generally increases empathy. Especially when traditional or universal norms need to be questioned, narratives are particularly well suited, as their ambiguity and vagueness creates room for interpretation. This is particularly important when new ideas or perspectives are introduced, since they cannot yet be rationally articulated. Table 1 establishes a direct comparison between discourse quality dimensions of the DQI (Steenbergen et al., 2003;Steiner et al., 2005) and argument quality dimensions as defined in Wachsmuth et al. (2017b). Apart from the potential theoretical insights, the existing guidelines can be applied to annotate new or enrich existing corpora for AM. Despite the small size, the data already annotated based on the DQI can be made usable and extended for NLP. In addition, some of the quality dimensions can be further quantified or approximated using statistical methods. For example, interactivity or equality can be assessed with frequency-based methods, such as frequency of posts by distinct participants and response rate.', 'Summing up The overview of the definitions of AQ along with the discussion of the potential of the integration of Deliberative Quality features into an AM framework has one strong take-home message:', 'The need for the scope of the investigation to go beyond (a) the persuasiveness of a an argumentative text (speeches, forum posts, tweets), and (b) their relation to the immediate preceding discourse. Instead, we pointed out the need to also assess the potential of the impact of that argumentative text on the upcoming discourse: this dimension of quality, inherently related to the interpretation of argumentation as a cooperation challenge, is currently lacking in current approaches to AQ.']","The second stage in the framework of AM is defined as relation assignment (c.f. Section 2.1); a complex task that aims to predict the relations holding between the arguments defined in the first stage. Being able to model the relations between arguments and components within the structure, for example in argument graphs (Besnard and Hunter, 2014; Craven and Toni, 2016), allows us to actually work with the argumentative text in an applicationbased setting, understand the stance and context of arguments, and develop a story for the consequential impact of arguments on the discourse, among other things. Generally speaking, we can use this task as an approach to analyze argument quality (AQ). However, within the AM community, an open question concerns the adequate definition and operationalization of the notion of AQ. Despite this, to move forward with the task of AQ analysis and to create large corpora with crowd-sourced annotations, some approaches rely on the relative assessment of quality: Given two arguments, which is more convincing? (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020) Thus the natural way of quantifying the success of an argument is in terms of its persuasiveness. Indeed, plenty of previous work has explored the many factors which contribute to the persuasiveness of a message: the linguistic features employed by the authors (Persing and Ng, 2017), the semantic type of claims and premises (Hidey et al., 2017), the different sources of evidence produced to support an argument (Addawood and Bashir, 2016), the effects of the personality traits and prior beliefs on persuasiveness (Lukin et al., 2017;Durmus and Cardie, 2018;Al Khatib et al., 2020), the interaction with other participants (Ji et al., 2018;Egawa et al., 2020), the use of argument invention when debating about unknown topics (Bilu et al., 2019), the structure of the arguments (Li et al., 2020), and the effect of the style of the text in achieving persuasion (El Baff et al., 2020).

Persuasiveness is, however, not the only way to define whether an argument is good -at least not from a deliberation point of view. A good contribution to a debate is one which uncovers a previously unnoticed aspect of a problem, thus generating a perturbation in the discourse (controversies can be productive!). Or else, a good contribution is one that settles an issue, by stating the differences between opposing views and allowing the discourse to stabilize in a series of clusters (convergence on just one position is not necessarily a good outcome).

Most recent research projects (Wachsmuth et al., 2017b) aim to address the challenge of redefining the notion of AQ, away from persuasiveness and towards a more ""situated"" definition which has to do with the needs of argumentation in a real-world scenario. This new definition has been the basis for the creation of new corpora from different domains , where feature-based (Wachsmuth and Werner, 2020) and neural models were tested for automatic prediction . Other aspects of AQ have become the subject of AM research such as the relevance and impact of arguments (Durmus et al., 2019), the verifiability (Park and Cardie, 2018), local acceptability (Yang et al., 2019) and the best ""deliberative move"" (Al-Khatib et al., 2018).

We argue that this shift is necessary for two reasons: (1) Working with real-world applications of AM naturally forces us into the more heterogeneous realm of data structures, such as social media, in which language, structure, and content are less uniform and confined to the classic notion of logical debate; and (2) In order to encourage deliberation from an open audience of citizens, we need to redefine our concept of AQ and productive discourse such that there is equal worth and participation granted to each contributor of the argument.

Deliberative Quality We therefore propose adapting the definition of quality to integrate the abundant research on the topic from the field of Social Sciences. Here, the quality of a discourse has been investigated in the context of deliberation with the focus on inclusivity: how can the interplay of the different participants in the discourse lead to an optimal outcome for the collective? The focus here is not on the quality of the individual contributions. Instead, an overall quality of the discourse is determined by the fact that the individual quality dimensions are distributed among different contributions (e.g some participants do more rational reasoning, others share personal experiences). We would like to integrate those aspects that focus on inclusivity and cooperation.

Similar to Wachsmuth et al. (2017b), social scientists have developed a taxonomy, the discourse quality index (DQI), that describes the different desirable aspects of a discourse (Steenbergen et al., 2003). This taxonomy has been used to analyze the quality of deliberation in different contexts, ranging from more formal contexts, such as parliamentary debates (Steiner et al., 2005), to informal discussions in online forums (Trénel, 2004). Both implementations integrate logical coherence as one dimension, cogency in Wachsmuth et al. (2017b), justification in the DQI. Some aspects of inclusivity are also being touched upon in the rhetorical and dialectical dimension of Wachsmuth et al. (2017b), such as using appropriate language (Appropriateness) or whether an argument supports conflict resolution (global relevance). We concentrate on the following dimensions from the DQI, which particularly focus on the collaborative aspect of discourse.

• Respect: this dimension includes respectful tone, respect for other social groups/backgrounds, and openness towards other opinions.

• Equality / Participation: it is not desirable that some dominant participants make the bulk of contributions while many others remain passive. All participants should have equal opportunities to contribute and all topics, including those that DQI (Steenbergen et al., 2003)  • Interactivity: beyond simply sharing opinions, acknowledging other viewpoints and interacting with other participants through listening and responding lead to new perspectives arising -compromises can emerge.

• Testimoniality / Report of personal accounts:

sharing stories and personal narratives as an alternative form of communication can involve more people in the discourse, especially those who cannot identify themselves with rational argumentation. It can also make other participants aware of other perspectives as it generally increases empathy. Especially when traditional or universal norms need to be questioned, narratives are particularly well suited, as their ambiguity and vagueness creates room for interpretation. This is particularly important when new ideas or perspectives are introduced, since they cannot yet be rationally articulated. Table 1 establishes a direct comparison between discourse quality dimensions of the DQI (Steenbergen et al., 2003;Steiner et al., 2005) and argument quality dimensions as defined in Wachsmuth et al. (2017b). Apart from the potential theoretical insights, the existing guidelines can be applied to annotate new or enrich existing corpora for AM. Despite the small size, the data already annotated based on the DQI can be made usable and extended for NLP. In addition, some of the quality dimensions can be further quantified or approximated using statistical methods. For example, interactivity or equality can be assessed with frequency-based methods, such as frequency of posts by distinct participants and response rate.

Summing up The overview of the definitions of AQ along with the discussion of the potential of the integration of Deliberative Quality features into an AM framework has one strong take-home message:

The need for the scope of the investigation to go beyond (a) the persuasiveness of a an argumentative text (speeches, forum posts, tweets), and (b) their relation to the immediate preceding discourse. Instead, we pointed out the need to also assess the potential of the impact of that argumentative text on the upcoming discourse: this dimension of quality, inherently related to the interpretation of argumentation as a cooperation challenge, is currently lacking in current approaches to AQ.","(p4.0) The second stage in the framework of AM is defined as relation assignment (c.f. Section 2.1); a complex task that aims to predict the relations holding between the arguments defined in the first stage. Being able to model the relations between arguments and components within the structure, for example in argument graphs (Besnard and Hunter, 2014; Craven and Toni, 2016), allows us to actually work with the argumentative text in an applicationbased setting, understand the stance and context of arguments, and develop a story for the consequential impact of arguments on the discourse, among other things. Generally speaking, we can use this task as an approach to analyze argument quality (AQ). However, within the AM community, an open question concerns the adequate definition and operationalization of the notion of AQ. Despite this, to move forward with the task of AQ analysis and to create large corpora with crowd-sourced annotations, some approaches rely on the relative assessment of quality: Given two arguments, which is more convincing? (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020) Thus the natural way of quantifying the success of an argument is in terms of its persuasiveness. Indeed, plenty of previous work has explored the many factors which contribute to the persuasiveness of a message: the linguistic features employed by the authors (Persing and Ng, 2017), the semantic type of claims and premises (Hidey et al., 2017), the different sources of evidence produced to support an argument (Addawood and Bashir, 2016), the effects of the personality traits and prior beliefs on persuasiveness (Lukin et al., 2017;Durmus and Cardie, 2018;Al Khatib et al., 2020), the interaction with other participants (Ji et al., 2018;Egawa et al., 2020), the use of argument invention when debating about unknown topics (Bilu et al., 2019), the structure of the arguments (Li et al., 2020), and the effect of the style of the text in achieving persuasion (El Baff et al., 2020).

(p4.1) Persuasiveness is, however, not the only way to define whether an argument is good -at least not from a deliberation point of view. A good contribution to a debate is one which uncovers a previously unnoticed aspect of a problem, thus generating a perturbation in the discourse (controversies can be productive!). Or else, a good contribution is one that settles an issue, by stating the differences between opposing views and allowing the discourse to stabilize in a series of clusters (convergence on just one position is not necessarily a good outcome).

(p4.2) Most recent research projects (Wachsmuth et al., 2017b) aim to address the challenge of redefining the notion of AQ, away from persuasiveness and towards a more ""situated"" definition which has to do with the needs of argumentation in a real-world scenario. This new definition has been the basis for the creation of new corpora from different domains , where feature-based (Wachsmuth and Werner, 2020) and neural models were tested for automatic prediction . Other aspects of AQ have become the subject of AM research such as the relevance and impact of arguments (Durmus et al., 2019), the verifiability (Park and Cardie, 2018), local acceptability (Yang et al., 2019) and the best ""deliberative move"" (Al-Khatib et al., 2018).

(p4.3) We argue that this shift is necessary for two reasons: (1) Working with real-world applications of AM naturally forces us into the more heterogeneous realm of data structures, such as social media, in which language, structure, and content are less uniform and confined to the classic notion of logical debate; and (2) In order to encourage deliberation from an open audience of citizens, we need to redefine our concept of AQ and productive discourse such that there is equal worth and participation granted to each contributor of the argument.

(p4.4) Deliberative Quality We therefore propose adapting the definition of quality to integrate the abundant research on the topic from the field of Social Sciences. Here, the quality of a discourse has been investigated in the context of deliberation with the focus on inclusivity: how can the interplay of the different participants in the discourse lead to an optimal outcome for the collective? The focus here is not on the quality of the individual contributions. Instead, an overall quality of the discourse is determined by the fact that the individual quality dimensions are distributed among different contributions (e.g some participants do more rational reasoning, others share personal experiences). We would like to integrate those aspects that focus on inclusivity and cooperation.

(p4.5) Similar to Wachsmuth et al. (2017b), social scientists have developed a taxonomy, the discourse quality index (DQI), that describes the different desirable aspects of a discourse (Steenbergen et al., 2003). This taxonomy has been used to analyze the quality of deliberation in different contexts, ranging from more formal contexts, such as parliamentary debates (Steiner et al., 2005), to informal discussions in online forums (Trénel, 2004). Both implementations integrate logical coherence as one dimension, cogency in Wachsmuth et al. (2017b), justification in the DQI. Some aspects of inclusivity are also being touched upon in the rhetorical and dialectical dimension of Wachsmuth et al. (2017b), such as using appropriate language (Appropriateness) or whether an argument supports conflict resolution (global relevance). We concentrate on the following dimensions from the DQI, which particularly focus on the collaborative aspect of discourse.

(p4.6) • Respect: this dimension includes respectful tone, respect for other social groups/backgrounds, and openness towards other opinions.

(p4.7) • Equality / Participation: it is not desirable that some dominant participants make the bulk of contributions while many others remain passive. All participants should have equal opportunities to contribute and all topics, including those that DQI (Steenbergen et al., 2003)  • Interactivity: beyond simply sharing opinions, acknowledging other viewpoints and interacting with other participants through listening and responding lead to new perspectives arising -compromises can emerge.

(p4.8) • Testimoniality / Report of personal accounts:

(p4.9) sharing stories and personal narratives as an alternative form of communication can involve more people in the discourse, especially those who cannot identify themselves with rational argumentation. It can also make other participants aware of other perspectives as it generally increases empathy. Especially when traditional or universal norms need to be questioned, narratives are particularly well suited, as their ambiguity and vagueness creates room for interpretation. This is particularly important when new ideas or perspectives are introduced, since they cannot yet be rationally articulated. Table 1 establishes a direct comparison between discourse quality dimensions of the DQI (Steenbergen et al., 2003;Steiner et al., 2005) and argument quality dimensions as defined in Wachsmuth et al. (2017b). Apart from the potential theoretical insights, the existing guidelines can be applied to annotate new or enrich existing corpora for AM. Despite the small size, the data already annotated based on the DQI can be made usable and extended for NLP. In addition, some of the quality dimensions can be further quantified or approximated using statistical methods. For example, interactivity or equality can be assessed with frequency-based methods, such as frequency of posts by distinct participants and response rate.

(p4.10) Summing up The overview of the definitions of AQ along with the discussion of the potential of the integration of Deliberative Quality features into an AM framework has one strong take-home message:

(p4.11) The need for the scope of the investigation to go beyond (a) the persuasiveness of a an argumentative text (speeches, forum posts, tweets), and (b) their relation to the immediate preceding discourse. Instead, we pointed out the need to also assess the potential of the impact of that argumentative text on the upcoming discourse: this dimension of quality, inherently related to the interpretation of argumentation as a cooperation challenge, is currently lacking in current approaches to AQ.","[['b36', None, 'b47', 'b6', 'b27', 'b23', 'b9'], [], ['b55', 'b56', 'b33', None, 'b61'], [], [], ['b45', 'b55', 'b50', 'b46'], [], ['b45'], [], ['b45', 'b55', 'b46'], [], []]","[['b36', None, 'b47', 'b6', 'b27', 'b23', 'b9'], [], ['b55', 'b56', 'b33', None, 'b61'], [], [], ['b45', 'b55', 'b50', 'b46'], [], ['b45'], [], ['b45', 'b55', 'b46'], [], []]",20,"1. The second stage in the framework of AM is defined as relation assignment (c.f. Section 2.1); a complex task that aims to predict the relations holding between the arguments defined in the first stage.
2. Being able to model the relations between arguments and components within the structure, for example in argument graphs (Besnard and Hunter, 2014; Craven and Toni, 2016), allows us to actually work with the argumentative text in an applicationbased setting, understand the stance and context of arguments, and develop a story for the consequential impact of arguments on the discourse, among other things.
3. Generally speaking, we can use this task as an approach to analyze argument quality (AQ).
4. However, within the AM community, an open question concerns the adequate definition and operationalization of the notion of AQ.
5. Despite this, to move forward with the task of AQ analysis and to create large corpora with crowd-sourced annotations, some approaches rely on the relative assessment of quality: Given two arguments, which is more convincing?
6. (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020)
7. Thus the natural way of quantifying the success of an argument is in terms of its persuasiveness.
8. Indeed, plenty of previous work has explored the many factors which contribute to the persuasiveness of a message: the linguistic features employed by the authors (Persing and Ng, 2017), the semantic type of claims and premises (Hidey et al., 2017), the different sources of evidence produced to support an argument (Addawood and Bashir, 2016), the effects of the personality traits and prior beliefs on persuasiveness (Lukin et al., 2017;Durmus and Cardie, 2018;Al Khatib et al., 2020), the interaction with other participants (Ji et al., 2018;Egawa et al., 2020), the use of argument invention when debating about unknown topics (Bilu et al., 2019), the structure of the arguments (Li et al., 2020), and the effect of the style of the text in achieving persuasion (El Baff et al., 2020).
9. Persuasiveness is, however, not the only way to define whether an argument is good -at least not from a deliberation point of view.
10. A good contribution to a debate is one which uncovers a previously unnoticed aspect of a problem, thus generating a perturbation in the discourse (controversies can be productive!).
11. Or else, a good contribution is one that settles an issue, by stating the differences between opposing views and allowing the discourse to stabilize in a series of clusters (convergence on just one position is not necessarily a good outcome).
12. Most recent research projects (Wachsmuth et al., 2017b) aim to address the challenge of redefining the notion of AQ, away from persuasiveness and towards a more ""situated"" definition which has to do with the needs of argumentation in a real-world scenario.
13. This new definition has been the basis for the creation of new corpora from different domains , where feature-based (Wachsmuth and Werner, 2020) and neural models were tested for automatic prediction .
14. Other aspects of AQ have become the subject of AM research such as the relevance and impact of arguments (Durmus et al., 2019), the verifiability (Park and Cardie, 2018), local acceptability (Yang et al., 2019) and the best ""deliberative move"" (Al-Khatib et al., 2018).
15. We argue that this shift is necessary for two reasons: (1) Working with real-world applications of AM naturally forces us into the more heterogeneous realm of data structures, such as social media, in which language, structure, and content are less uniform and confined to the classic notion of logical debate; and
16. (2) In order to encourage deliberation from an open audience of citizens, we need to redefine our concept of AQ and productive discourse such that there is equal worth and participation granted to each contributor of the argument.
17. Deliberative Quality We therefore propose adapting the definition of quality to integrate the abundant research on the topic from the field of Social Sciences.
18. Here, the quality of a discourse has been investigated in the context of deliberation with the focus on inclusivity: how can the interplay of the different participants in the discourse lead to an optimal outcome for the collective?
19. The focus here is not on the quality of the individual contributions.
20. Instead, an overall quality of the discourse is determined by the fact that the individual quality dimensions are distributed among different contributions (e.g some participants do more rational reasoning, others share personal experiences).
21. We would like to integrate those aspects that focus on inclusivity and cooperation.
22. Similar to Wachsmuth et al. (2017b), social scientists have developed a taxonomy, the discourse quality index (DQI), that describes the different desirable aspects of a discourse (Steenbergen et al., 2003).
23. This taxonomy has been used to analyze the quality of deliberation in different contexts, ranging from more formal contexts, such as parliamentary debates (Steiner et al., 2005), to informal discussions in online forums (Trénel, 2004).
24. Both implementations integrate logical coherence as one dimension, cogency in Wachsmuth et al. (2017b), justification in the DQI.
25. Some aspects of inclusivity are also being touched upon in the rhetorical and dialectical dimension of Wachsmuth et al. (2017b), such as using appropriate language (Appropriateness) or whether an argument supports conflict resolution (global relevance).
26. We concentrate on the following dimensions from the DQI, which particularly focus on the collaborative aspect of discourse.
27. • Respect: this dimension includes respectful tone, respect for other social groups/backgrounds, and openness towards other opinions.
28. • Equality / Participation: it is not desirable that some dominant participants make the bulk of contributions while many others remain passive.
29. All participants should have equal opportunities to contribute and all topics, including those that DQI (Steenbergen et al., 2003)  • Interactivity: beyond simply sharing opinions, acknowledging other viewpoints and interacting with other participants through listening and responding lead to new perspectives arising -compromises can emerge.
30. • Testimoniality / Report of personal accounts:sharing stories and personal narratives as an alternative form of communication can involve more people in the discourse, especially those who cannot identify themselves with rational argumentation.
31. It can also make other participants aware of other perspectives as it generally increases empathy.
32. Especially when traditional or universal norms need to be questioned, narratives are particularly well suited, as their ambiguity and vagueness creates room for interpretation.
33. This is particularly important when new ideas or perspectives are introduced, since they cannot yet be rationally articulated.
34. Table 1 establishes a direct comparison between discourse quality dimensions of the DQI (Steenbergen et al., 2003;Steiner et al., 2005) and argument quality dimensions as defined in Wachsmuth et al. (2017b).
35. Apart from the potential theoretical insights, the existing guidelines can be applied to annotate new or enrich existing corpora for AM.
36. Despite the small size, the data already annotated based on the DQI can be made usable and extended for NLP.
37. In addition, some of the quality dimensions can be further quantified or approximated using statistical methods.
38. For example, interactivity or equality can be assessed with frequency-based methods, such as frequency of posts by distinct participants and response rate.
39. Summing up The overview of the definitions of AQ along with the discussion of the potential of the integration of Deliberative Quality features into an AM framework has one strong take-home message:
40. The need for the scope of the investigation to go beyond (a) the persuasiveness of a an argumentative text (speeches, forum posts, tweets), and (b) their relation to the immediate preceding discourse.
41. Instead, we pointed out the need to also assess the potential of the impact of that argumentative text on the upcoming discourse: this dimension of quality, inherently related to the interpretation of argumentation as a cooperation challenge, is currently lacking in current approaches to AQ."
236460241,A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/ee6d66efc86746d42ace14db30fcbaf9d3380e25,s3,C-S across Languages: European Context,"['p3.0', 'p3.1', 'p3.2', 'p3.3', 'p3.4', 'p3.5', 'p3.6']","[""The contexts in which people acquire and use multiple languages in Europe are diverse. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively)."", 'Within an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway. Similarly, Jørgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.', 'C-S does not only take place between standard languages but between minority languages and dialects as well. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.', 'C-S is commonly observable across immigrant contexts in Europe. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England. In addition, Ożańska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and Ç etinoglu (2017), Russian-German C-S by Khakimov (2016)). In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990). In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.', 'In addition to daily communication, some linguists are also interested in the C-S observed in historical documents. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.', 'Within the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.', 'Similar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions.']","The contexts in which people acquire and use multiple languages in Europe are diverse. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively).

Within an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway. Similarly, Jørgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.

C-S does not only take place between standard languages but between minority languages and dialects as well. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.

C-S is commonly observable across immigrant contexts in Europe. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England. In addition, Ożańska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and Ç etinoglu (2017), Russian-German C-S by Khakimov (2016)). In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990). In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.

In addition to daily communication, some linguists are also interested in the C-S observed in historical documents. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.

Within the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.

Similar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions.","(p3.0) The contexts in which people acquire and use multiple languages in Europe are diverse. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively).

(p3.1) Within an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway. Similarly, Jørgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.

(p3.2) C-S does not only take place between standard languages but between minority languages and dialects as well. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.

(p3.3) C-S is commonly observable across immigrant contexts in Europe. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England. In addition, Ożańska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and Ç etinoglu (2017), Russian-German C-S by Khakimov (2016)). In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990). In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.

(p3.4) In addition to daily communication, some linguists are also interested in the C-S observed in historical documents. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.

(p3.5) Within the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.

(p3.6) Similar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions.","[['b40', 'b56', 'b39', 'b54', 'b29', None, 'b11', 'b17'], [], ['b28', 'b69', 'b75'], ['b46', None, 'b12', 'b38', 'b1', 'b52', 'b24', 'b23', 'b51', 'b80'], [], ['b83', 'b35'], ['b81']]","[['b40', 'b56', 'b39', 'b54', 'b29', None, 'b11', 'b17'], [], ['b28', 'b69', 'b75'], ['b46', None, 'b12', 'b38', 'b1', 'b52', 'b24', 'b23', 'b51', 'b80'], [], ['b83', 'b35'], ['b81']]",24,"1. The contexts in which people acquire and use multiple languages in Europe are diverse.
2. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction.
3. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors.
4. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe.
5. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children.
6. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany.
7. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children.
8. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively).
9. Within an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway.
10. Similarly, Jørgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.
11. C-S does not only take place between standard languages but between minority languages and dialects as well.
12. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK.
13. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy.
14. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria.
15. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.C-S is commonly observable across immigrant contexts in Europe.
16. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London.
17. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England.
18. In addition, Ożańska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well.
19. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and Ç etinoglu (2017), Russian-German C-S by Khakimov (2016)).
20. In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990).
21. In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels.
22. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants.
23. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well.
24. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands.
25. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.
26. In addition to daily communication, some linguists are also interested in the C-S observed in historical documents.
27. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire.
28. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations.
29. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.
30. Within the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses.
31. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.
32. Similar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience.
33. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions."
236460241,A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/ee6d66efc86746d42ace14db30fcbaf9d3380e25,s4,C-S across Languages: Indian Context,"['p4.0', 'p4.1', 'p4.2', 'p4.3', 'p4.4', 'p4.5']","['According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages). Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families. The census excludes languages with a population lower than 10,000 speakers. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.', 'In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.', 'Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India. Both languages have been in contact with each other for over four hundred years. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).', 'C-S in India has been investigated through written media, advertising and film industry as well. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004. Her results indicate a change of direction C-S over the years. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions. A similar trend has been observed for Bengali movie scripts as well. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted). In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S). Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity). Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.', 'From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S. Their results indicate more intrasentential C-S than intersentential ones on the billboards. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India. In addition, she finds it unrealistic to ask the students to separate the two languages harshly. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006). In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.', 'In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.']","According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages). Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families. The census excludes languages with a population lower than 10,000 speakers. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.

In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.

Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India. Both languages have been in contact with each other for over four hundred years. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).

C-S in India has been investigated through written media, advertising and film industry as well. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004. Her results indicate a change of direction C-S over the years. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions. A similar trend has been observed for Bengali movie scripts as well. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted). In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S). Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity). Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.

From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S. Their results indicate more intrasentential C-S than intersentential ones on the billboards. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India. In addition, she finds it unrealistic to ask the students to separate the two languages harshly. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006). In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.

In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.","(p4.0) According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages). Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families. The census excludes languages with a population lower than 10,000 speakers. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.

(p4.1) In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.

(p4.2) Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India. Both languages have been in contact with each other for over four hundred years. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).

(p4.3) C-S in India has been investigated through written media, advertising and film industry as well. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004. Her results indicate a change of direction C-S over the years. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions. A similar trend has been observed for Bengali movie scripts as well. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted). In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S). Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity). Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.

(p4.4) From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S. Their results indicate more intrasentential C-S than intersentential ones on the billboards. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India. In addition, she finds it unrealistic to ask the students to separate the two languages harshly. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006). In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.

(p4.5) In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.","[[None, 'b19'], ['b73', 'b27'], [None], ['b67', 'b84'], ['b50', 'b36', 'b41', 'b72', None], []]","[[None, 'b19'], ['b73', 'b27'], [None], ['b67', 'b84'], ['b50', 'b36', 'b41', 'b72', None], []]",12,"1. According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual.
2. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages).
3. Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families.
4. The census excludes languages with a population lower than 10,000 speakers.
5. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism.
6. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.
7. In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages.
8. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences.
9. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view.
10. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele.
11. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings.
12. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India.
13. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.
14. Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means.
15. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research.
16. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost.
17. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India.
18. Both languages have been in contact with each other for over four hundred years.
19. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada.
20. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context.
21. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S.
22. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).C-S in India has been investigated through written media, advertising and film industry as well.
23. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004.
24. Her results indicate a change of direction C-S over the years.
25. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions.
26. A similar trend has been observed for Bengali movie scripts as well.
27. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society.
28. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK.
29. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted).
30. In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S).
31. Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity).
32. Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.
33. From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India.
34. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S.
35. Their results indicate more intrasentential C-S than intersentential ones on the billboards.
36. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group.
37. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum.
38. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families.
39. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages.
40. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India.
41. In addition, she finds it unrealistic to ask the students to separate the two languages harshly.
42. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK.
43. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006).
44. In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali.
45. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well.
46. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.
47. In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group.
48. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English.
49. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well."
236460241,A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/ee6d66efc86746d42ace14db30fcbaf9d3380e25,s8,User-facing applications,"['p8.0', 'p8.1', 'p8.2']","['Although speech and NLP models for C-S have been built for various applications, a major limitation of the work done so far in computational processing of C-S is the lack of end-to-end userfacing applications that interact directly with users in multilingual communities. For example, there is no widely-used spoken dialogue system that can understand as well as produce code-switched speech, although some voice assistants may recognize and produce C-S in limited scenarios in some locales. Although computational implementations of grammatical models of C-S exist (Bhat et al., 2016), they do not necessarily generate natural C-S utterances that a bilingual speaker would produce (Pratapa et al., 2018). Most crucially, current computational approaches to C-S language technologies do not usually take into account the linguistic and social factors that influence why and when speakers/users choose to code-switch.', ""Bawa et al. (2020) conducted a Wizard-of-Oz study using a Hindi-English chatbot and found that not only did bilingual users prefer chatbots that could code-switch, they also showed a preference towards bots that mimicked their own C-S patterns. Rudra et al. (2016) report a study on 430k tweets from Hindi-English bilingual users and find that Hindi is preferred for the expression of negative sentiment. In a follow-up study, Agarwal et al. (2017) find that Hindi is the preferred language for swearing in Hindi-English C-S tweets, and swearing may be a motivating factor for users to switch to Hindi. The study also finds a gender difference, with women preferring to swear in English more often than Hindi. Such studies indicate that multilingual chatbots and intelligent agents need to be able to adapt to users' linguistic styles, while also being capable of determining when and how to code-switch."", 'Due to the paucity of user-facing systems and standard benchmarks covering only a handful of simpler NLP tasks, it is likely that we overestimate how well computational models are able to handle C-S. In sum, language technologies for C-S seem to be constrained by the lack of availability of diverse C-S training data, evaluation benchmarks and the absence of user-facing applications. They need to go beyond pattern recognition and grammatical constraints of C-S in order to process and produce C-S the way humans do. Hence, it is important for the CL community to be aware of the vast litera-ture around C-S in linguistics, particularly as we proceed to solving more challenging tasks.']","Although speech and NLP models for C-S have been built for various applications, a major limitation of the work done so far in computational processing of C-S is the lack of end-to-end userfacing applications that interact directly with users in multilingual communities. For example, there is no widely-used spoken dialogue system that can understand as well as produce code-switched speech, although some voice assistants may recognize and produce C-S in limited scenarios in some locales. Although computational implementations of grammatical models of C-S exist (Bhat et al., 2016), they do not necessarily generate natural C-S utterances that a bilingual speaker would produce (Pratapa et al., 2018). Most crucially, current computational approaches to C-S language technologies do not usually take into account the linguistic and social factors that influence why and when speakers/users choose to code-switch.

Bawa et al. (2020) conducted a Wizard-of-Oz study using a Hindi-English chatbot and found that not only did bilingual users prefer chatbots that could code-switch, they also showed a preference towards bots that mimicked their own C-S patterns. Rudra et al. (2016) report a study on 430k tweets from Hindi-English bilingual users and find that Hindi is preferred for the expression of negative sentiment. In a follow-up study, Agarwal et al. (2017) find that Hindi is the preferred language for swearing in Hindi-English C-S tweets, and swearing may be a motivating factor for users to switch to Hindi. The study also finds a gender difference, with women preferring to swear in English more often than Hindi. Such studies indicate that multilingual chatbots and intelligent agents need to be able to adapt to users' linguistic styles, while also being capable of determining when and how to code-switch.

Due to the paucity of user-facing systems and standard benchmarks covering only a handful of simpler NLP tasks, it is likely that we overestimate how well computational models are able to handle C-S. In sum, language technologies for C-S seem to be constrained by the lack of availability of diverse C-S training data, evaluation benchmarks and the absence of user-facing applications. They need to go beyond pattern recognition and grammatical constraints of C-S in order to process and produce C-S the way humans do. Hence, it is important for the CL community to be aware of the vast litera-ture around C-S in linguistics, particularly as we proceed to solving more challenging tasks.","(p8.0) Although speech and NLP models for C-S have been built for various applications, a major limitation of the work done so far in computational processing of C-S is the lack of end-to-end userfacing applications that interact directly with users in multilingual communities. For example, there is no widely-used spoken dialogue system that can understand as well as produce code-switched speech, although some voice assistants may recognize and produce C-S in limited scenarios in some locales. Although computational implementations of grammatical models of C-S exist (Bhat et al., 2016), they do not necessarily generate natural C-S utterances that a bilingual speaker would produce (Pratapa et al., 2018). Most crucially, current computational approaches to C-S language technologies do not usually take into account the linguistic and social factors that influence why and when speakers/users choose to code-switch.

(p8.1) Bawa et al. (2020) conducted a Wizard-of-Oz study using a Hindi-English chatbot and found that not only did bilingual users prefer chatbots that could code-switch, they also showed a preference towards bots that mimicked their own C-S patterns. Rudra et al. (2016) report a study on 430k tweets from Hindi-English bilingual users and find that Hindi is preferred for the expression of negative sentiment. In a follow-up study, Agarwal et al. (2017) find that Hindi is the preferred language for swearing in Hindi-English C-S tweets, and swearing may be a motivating factor for users to switch to Hindi. The study also finds a gender difference, with women preferring to swear in English more often than Hindi. Such studies indicate that multilingual chatbots and intelligent agents need to be able to adapt to users' linguistic styles, while also being capable of determining when and how to code-switch.

(p8.2) Due to the paucity of user-facing systems and standard benchmarks covering only a handful of simpler NLP tasks, it is likely that we overestimate how well computational models are able to handle C-S. In sum, language technologies for C-S seem to be constrained by the lack of availability of diverse C-S training data, evaluation benchmarks and the absence of user-facing applications. They need to go beyond pattern recognition and grammatical constraints of C-S in order to process and produce C-S the way humans do. Hence, it is important for the CL community to be aware of the vast litera-ture around C-S in linguistics, particularly as we proceed to solving more challenging tasks.","[['b59', None], ['b63', None], []]","[['b59', None], ['b63', None], []]",4,"1. Although speech and NLP models for C-S have been built for various applications, a major limitation of the work done so far in computational processing of C-S is the lack of end-to-end userfacing applications that interact directly with users in multilingual communities.
2. For example, there is no widely-used spoken dialogue system that can understand as well as produce code-switched speech, although some voice assistants may recognize and produce C-S in limited scenarios in some locales.
3. Although computational implementations of grammatical models of C-S exist (Bhat et al., 2016), they do not necessarily generate natural C-S utterances that a bilingual speaker would produce (Pratapa et al., 2018).
4. Most crucially, current computational approaches to C-S language technologies do not usually take into account the linguistic and social factors that influence why and when speakers/users choose to code-switch.
5. Bawa et al. (2020) conducted a Wizard-of-Oz study using a Hindi-English chatbot and found that not only did bilingual users prefer chatbots that could code-switch, they also showed a preference towards bots that mimicked their own C-S patterns.
6. Rudra et al. (2016) report a study on 430k tweets from Hindi-English bilingual users and find that Hindi is preferred for the expression of negative sentiment.
7. In a follow-up study, Agarwal et al. (2017) find that Hindi is the preferred language for swearing in Hindi-English C-S tweets, and swearing may be a motivating factor for users to switch to Hindi.
8. The study also finds a gender difference, with women preferring to swear in English more often than Hindi.
9. Such studies indicate that multilingual chatbots and intelligent agents need to be able to adapt to users' linguistic styles, while also being capable of determining when and how to code-switch.
10. Due to the paucity of user-facing systems and standard benchmarks covering only a handful of simpler NLP tasks, it is likely that we overestimate how well computational models are able to handle C-S. In sum, language technologies for C-S seem to be constrained by the lack of availability of diverse C-S training data, evaluation benchmarks and the absence of user-facing applications.
11. They need to go beyond pattern recognition and grammatical constraints of C-S in order to process and produce C-S the way humans do.
12. Hence, it is important for the CL community to be aware of the vast litera-ture around C-S in linguistics, particularly as we proceed to solving more challenging tasks."
236460241,A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/ee6d66efc86746d42ace14db30fcbaf9d3380e25,s1,Competing models of C-S,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5', 'p1.6', 'p1.7']","['For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.', ""2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007)."", ""3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '"", 'Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).', 'For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.', '3 Why do speakers code-switch?', ""In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997)."", 'According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.']","For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.

2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).

3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '

Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).

For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.

3 Why do speakers code-switch?

In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).

According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.","(p1.0) For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.

(p1.1) 2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).

(p1.2) 3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '

(p1.3) Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).

(p1.4) For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.

(p1.5) 3 Why do speakers code-switch?

(p1.6) In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).

(p1.7) According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.","[[], ['b3', 'b16', None, 'b71', 'b43'], [], ['b37', None, 'b32'], ['b53', 'b57', 'b32', 'b31', 'b33', None], [], ['b57', 'b82', 'b44', 'b9', 'b42'], ['b65', 'b30', 'b79', None, 'b7', 'b10', 'b52', 'b5']]","[[], ['b3', 'b16', None, 'b71', 'b43'], [], ['b37', None, 'b32'], ['b53', 'b57', 'b32', 'b31', 'b33', None], [], ['b57', 'b82', 'b44', 'b9', 'b42'], ['b65', 'b30', 'b79', None, 'b7', 'b10', 'b52', 'b5']]",27,"1. For linguists, the specific ways in which languages are switched matters.
2. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1.
3. In fact, it may not signal multilingualism at all, simply borrowing.
4. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.
5. 2. This is a good baile! 'This is a good dance party!'
6. (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word.
7. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from.
8. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997)
9. as the Matrix Language Frame (MLF) model.
10. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause.
11. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).3.
12. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '
13. Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds.
14. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005).
15. Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).
16. For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order.
17. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994).
18. Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016).
19. Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016).
20. This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages.
21. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018).
22. In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.
23. 3 Why do speakers code-switch? In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages.
24. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome.
25. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns.
26. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech.
27. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980).
28. These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so).
29. Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).
30. According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices.
31. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations.
32. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups.
33. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns.
34. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently.
35. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013).
36. Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014).
37. From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically."
236460241,A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/ee6d66efc86746d42ace14db30fcbaf9d3380e25,s2,"Code-switching, Borrowing, Transfer, Loan Translation","['p2.0', 'p2.1', 'p2.2']","[""While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruöz and Backus, 2009). 4.İlkokul-uİstanbul-da yap-tı-m."", ""primary.school-ACCİstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'"", ""In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher après 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.""]","While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruöz and Backus, 2009). 4.İlkokul-uİstanbul-da yap-tı-m.

primary.school-ACCİstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher après 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","(p2.0) While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruöz and Backus, 2009). 4.İlkokul-uİstanbul-da yap-tı-m.

(p2.1) primary.school-ACCİstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'

(p2.2) In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher après 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","[['b37', None, 'b64', 'b2', 'b58'], [], []]","[['b37', None, 'b64', 'b2', 'b58'], [], []]",5,"1. While C-S implies active alternation between grammatical systems, borrowing does not.
2. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).
3. When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).
4. Similarly, what looks like complex C-S may not be perceived as switching at all.
5. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.
6. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two.
7. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate.
8. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.
9. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruöz and Backus, 2009).
10. 4.İlkokul-uİstanbul-da yap-tı-m.primary.school-ACCİstanbul-LOC do-past-1sg.
11. 'I finished primary school in Istanbul.'
12. In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.
13. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French.
14. In Brussels French (Belgium), the construction chercher après 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'.
15. In reference French (France), there is normally no particle following the verb.
16. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s18,Open Questions,"['p18.0', 'p18.1', 'p18.2', 'p18.3']","['In addition to the challenges mentioned above, we list below a few open questions that call for additional research going forward.', ""Identifying Unknown Robustness Failures Existing identification around robustness failures rely heavily on human priors and error analyses, which usually pre-define a small or limited set of patterns that the model could be vulnerable to. This requires extensive amount of expertise and efforts, and might still suffer from human or subjective biases in the end. How to proactively discover and identify models' unrobust regions automatically and comprehensively remains challenging."", ""Interpreting and Mitigating Spurious Correlations Interpretability matters for large NLP models, especially key to the robustness and spurious patterns. How can we develop ways to attribute or interpret these vulnerable portions of NLP models and communicate these robustness failures with designers, practitioners, and users? In addition, recent work (Wallace et al., 2019c;Wang et al., 2021d;Zhang et al., 2021) show interpretability methods can be utilized to better understand how a model makes its decision, which in turn can be used to uncover models' bias, diagnose errors, and discover spurious correlations."", 'Furthermore, the mitigation of spurious correlations often suffers from the trade-off between removing shortcuts and sacrificing model performance Zhang et al., 2019a). Additionally, most existing mitigation strategies work in a pipeline fashion where defining and detecting spurious correlations are prerequisites, which might lead to error cascades in this process. How to design end-to-end frameworks for automatic mitigation deserves much attention.']","In addition to the challenges mentioned above, we list below a few open questions that call for additional research going forward.

Identifying Unknown Robustness Failures Existing identification around robustness failures rely heavily on human priors and error analyses, which usually pre-define a small or limited set of patterns that the model could be vulnerable to. This requires extensive amount of expertise and efforts, and might still suffer from human or subjective biases in the end. How to proactively discover and identify models' unrobust regions automatically and comprehensively remains challenging.

Interpreting and Mitigating Spurious Correlations Interpretability matters for large NLP models, especially key to the robustness and spurious patterns. How can we develop ways to attribute or interpret these vulnerable portions of NLP models and communicate these robustness failures with designers, practitioners, and users? In addition, recent work (Wallace et al., 2019c;Wang et al., 2021d;Zhang et al., 2021) show interpretability methods can be utilized to better understand how a model makes its decision, which in turn can be used to uncover models' bias, diagnose errors, and discover spurious correlations.

Furthermore, the mitigation of spurious correlations often suffers from the trade-off between removing shortcuts and sacrificing model performance Zhang et al., 2019a). Additionally, most existing mitigation strategies work in a pipeline fashion where defining and detecting spurious correlations are prerequisites, which might lead to error cascades in this process. How to design end-to-end frameworks for automatic mitigation deserves much attention.","(p18.0) In addition to the challenges mentioned above, we list below a few open questions that call for additional research going forward.

(p18.1) Identifying Unknown Robustness Failures Existing identification around robustness failures rely heavily on human priors and error analyses, which usually pre-define a small or limited set of patterns that the model could be vulnerable to. This requires extensive amount of expertise and efforts, and might still suffer from human or subjective biases in the end. How to proactively discover and identify models' unrobust regions automatically and comprehensively remains challenging.

(p18.2) Interpreting and Mitigating Spurious Correlations Interpretability matters for large NLP models, especially key to the robustness and spurious patterns. How can we develop ways to attribute or interpret these vulnerable portions of NLP models and communicate these robustness failures with designers, practitioners, and users? In addition, recent work (Wallace et al., 2019c;Wang et al., 2021d;Zhang et al., 2021) show interpretability methods can be utilized to better understand how a model makes its decision, which in turn can be used to uncover models' bias, diagnose errors, and discover spurious correlations.

(p18.3) Furthermore, the mitigation of spurious correlations often suffers from the trade-off between removing shortcuts and sacrificing model performance Zhang et al., 2019a). Additionally, most existing mitigation strategies work in a pipeline fashion where defining and detecting spurious correlations are prerequisites, which might lead to error cascades in this process. How to design end-to-end frameworks for automatic mitigation deserves much attention.","[[], [], ['b49', None, 'b62'], ['b77']]","[[], [], ['b49', None, 'b62'], ['b77']]",4,"1. In addition to the challenges mentioned above, we list below a few open questions that call for additional research going forward.
2. Identifying Unknown Robustness Failures Existing identification around robustness failures rely heavily on human priors and error analyses, which usually pre-define a small or limited set of patterns that the model could be vulnerable to.
3. This requires extensive amount of expertise and efforts, and might still suffer from human or subjective biases in the end.
4. How to proactively discover and identify models' unrobust regions automatically and comprehensively remains challenging.
5. Interpreting and Mitigating Spurious Correlations Interpretability matters for large NLP models, especially key to the robustness and spurious patterns.
6. How can we develop ways to attribute or interpret these vulnerable portions of NLP models and communicate these robustness failures with designers, practitioners, and users?
7. In addition, recent work (Wallace et al., 2019c;Wang et al., 2021d;Zhang et al., 2021) show interpretability methods can be utilized to better understand how a model makes its decision, which in turn can be used to uncover models' bias, diagnose errors, and discover spurious correlations.
8. Furthermore, the mitigation of spurious correlations often suffers from the trade-off between removing shortcuts and sacrificing model performance Zhang et al., 2019a).
9. Additionally, most existing mitigation strategies work in a pipeline fashion where defining and detecting spurious correlations are prerequisites, which might lead to error cascades in this process.
10. How to design end-to-end frameworks for automatic mitigation deserves much attention."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s15,Inductive-prior-based Approaches,"['p15.0', 'p15.1', 'p15.2']","['Another thread is to introduce inductive bias (i.e., to regularize the hypothesis space) to force the model to discard some spurious features. This is closely connected to the human-prior-based identification approaches in Section 4.1 as those human-priors can often be used to re-formulate the training objective with additional regularizers. To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component. The construction of this side component usually relies on prior knowledge of what the misaligned features are. Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020). Similarly, Clark et al. (2019Utama et al. (2020a,b) propose to ensemble with a model explicitly capturing bias, where the main model is trained together with this ""bias-only"" model such that the main model is discouraged from using biases. More recent work (Xiong et al., 2021) shows the ensemble-based approaches can be further improved via better calibrating the bias-only model. Furthermore, additional regularizers have been introduced for robust fine-tuning over pre-trained models, e.g., mutual-information-based regularizers (Wang et al., 2021a) and smoothness-inducing adversarial regularization (Jiang et al., 2020).', ""In a broader scope, given that one of the main challenges of domain adaptation is to counter the model's tendency in learning domain-specific spurious features (Ganin et al., 2016), some methods contributing to domain adaption may have also progressed along the line of our interest, e.g., domain adversarial neural network (Ganin et al., 2016). This line of work also inspires a family of methods forcing the model to learn auxiliary-annotationinvariant representations with a side component (Ghifary et al., 2016;Wang et al., 2017;Rozantsev et al., 2018;Motiian et al., 2017;Li et al., 2018;Vernikos et al., 2020)."", 'Despite the diverse concrete ideas introduced, the above is mainly training for small empirical loss across different domains or distributions in addition to forcing the model to be invariant to domain-specific spurious features. As an extension along this direction, invariant risk minimization (IRM) (Arjovsky et al., 2019) introduces the idea of invariant predictors across multiple environments, which was later followed and discussed by a variety of extensions (Choe et al., 2020;Ahmed et al., 2020;Rosenfeld et al., 2021). More recently, Dranker et al. (2021) applied IRM in natural language inference and found that a more naturalistic characterization of the problem setup is needed.']","Another thread is to introduce inductive bias (i.e., to regularize the hypothesis space) to force the model to discard some spurious features. This is closely connected to the human-prior-based identification approaches in Section 4.1 as those human-priors can often be used to re-formulate the training objective with additional regularizers. To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component. The construction of this side component usually relies on prior knowledge of what the misaligned features are. Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020). Similarly, Clark et al. (2019Utama et al. (2020a,b) propose to ensemble with a model explicitly capturing bias, where the main model is trained together with this ""bias-only"" model such that the main model is discouraged from using biases. More recent work (Xiong et al., 2021) shows the ensemble-based approaches can be further improved via better calibrating the bias-only model. Furthermore, additional regularizers have been introduced for robust fine-tuning over pre-trained models, e.g., mutual-information-based regularizers (Wang et al., 2021a) and smoothness-inducing adversarial regularization (Jiang et al., 2020).

In a broader scope, given that one of the main challenges of domain adaptation is to counter the model's tendency in learning domain-specific spurious features (Ganin et al., 2016), some methods contributing to domain adaption may have also progressed along the line of our interest, e.g., domain adversarial neural network (Ganin et al., 2016). This line of work also inspires a family of methods forcing the model to learn auxiliary-annotationinvariant representations with a side component (Ghifary et al., 2016;Wang et al., 2017;Rozantsev et al., 2018;Motiian et al., 2017;Li et al., 2018;Vernikos et al., 2020).

Despite the diverse concrete ideas introduced, the above is mainly training for small empirical loss across different domains or distributions in addition to forcing the model to be invariant to domain-specific spurious features. As an extension along this direction, invariant risk minimization (IRM) (Arjovsky et al., 2019) introduces the idea of invariant predictors across multiple environments, which was later followed and discussed by a variety of extensions (Choe et al., 2020;Ahmed et al., 2020;Rosenfeld et al., 2021). More recently, Dranker et al. (2021) applied IRM in natural language inference and found that a more naturalistic characterization of the problem setup is needed.","(p15.0) Another thread is to introduce inductive bias (i.e., to regularize the hypothesis space) to force the model to discard some spurious features. This is closely connected to the human-prior-based identification approaches in Section 4.1 as those human-priors can often be used to re-formulate the training objective with additional regularizers. To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component. The construction of this side component usually relies on prior knowledge of what the misaligned features are. Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020). Similarly, Clark et al. (2019Utama et al. (2020a,b) propose to ensemble with a model explicitly capturing bias, where the main model is trained together with this ""bias-only"" model such that the main model is discouraged from using biases. More recent work (Xiong et al., 2021) shows the ensemble-based approaches can be further improved via better calibrating the bias-only model. Furthermore, additional regularizers have been introduced for robust fine-tuning over pre-trained models, e.g., mutual-information-based regularizers (Wang et al., 2021a) and smoothness-inducing adversarial regularization (Jiang et al., 2020).

(p15.1) In a broader scope, given that one of the main challenges of domain adaptation is to counter the model's tendency in learning domain-specific spurious features (Ganin et al., 2016), some methods contributing to domain adaption may have also progressed along the line of our interest, e.g., domain adversarial neural network (Ganin et al., 2016). This line of work also inspires a family of methods forcing the model to learn auxiliary-annotationinvariant representations with a side component (Ghifary et al., 2016;Wang et al., 2017;Rozantsev et al., 2018;Motiian et al., 2017;Li et al., 2018;Vernikos et al., 2020).

(p15.2) Despite the diverse concrete ideas introduced, the above is mainly training for small empirical loss across different domains or distributions in addition to forcing the model to be invariant to domain-specific spurious features. As an extension along this direction, invariant risk minimization (IRM) (Arjovsky et al., 2019) introduces the idea of invariant predictors across multiple environments, which was later followed and discussed by a variety of extensions (Choe et al., 2020;Ahmed et al., 2020;Rosenfeld et al., 2021). More recently, Dranker et al. (2021) applied IRM in natural language inference and found that a more naturalistic characterization of the problem setup is needed.","[['b28', 'b69', 'b16', None, 'b51'], [None, 'b56', 'b25'], [None]]","[['b28', 'b69', 'b16', None, 'b51'], [None, 'b56', 'b25'], [None]]",9,"1. Another thread is to introduce inductive bias (i.e., to regularize the hypothesis space) to force the model to discard some spurious features.
2. This is closely connected to the human-prior-based identification approaches in Section 4.1 as those human-priors can often be used to re-formulate the training objective with additional regularizers.
3. To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component.
4. The construction of this side component usually relies on prior knowledge of what the misaligned features are.
5. Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020).
6. Similarly, Clark et al. (2019Utama et al. (2020a,b) propose to ensemble with a model explicitly capturing bias, where the main model is trained together with this ""bias-only"" model such that the main model is discouraged from using biases.
7. More recent work (Xiong et al., 2021) shows the ensemble-based approaches can be further improved via better calibrating the bias-only model.
8. Furthermore, additional regularizers have been introduced for robust fine-tuning over pre-trained models, e.g., mutual-information-based regularizers (Wang et al., 2021a) and smoothness-inducing adversarial regularization (Jiang et al., 2020).
9. In a broader scope, given that one of the main challenges of domain adaptation is to counter the model's tendency in learning domain-specific spurious features (Ganin et al., 2016), some methods contributing to domain adaption may have also progressed along the line of our interest, e.g., domain adversarial neural network (Ganin et al., 2016).
10. This line of work also inspires a family of methods forcing the model to learn auxiliary-annotationinvariant representations with a side component (Ghifary et al., 2016;Wang et al., 2017;Rozantsev et al., 2018;Motiian et al., 2017;Li et al., 2018;Vernikos et al., 2020).
11. Despite the diverse concrete ideas introduced, the above is mainly training for small empirical loss across different domains or distributions in addition to forcing the model to be invariant to domain-specific spurious features.
12. As an extension along this direction, invariant risk minimization (IRM) (Arjovsky et al., 2019) introduces the idea of invariant predictors across multiple environments, which was later followed and discussed by a variety of extensions (Choe et al., 2020;Ahmed et al., 2020;Rosenfeld et al., 2021).
13. More recently, Dranker et al. (2021) applied IRM in natural language inference and found that a more naturalistic characterization of the problem setup is needed."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s14,Model and Training-based Approaches,"['p14.0', 'p14.1']","['Pre-training Recent work has demonstrated pretraining as an effective way to improve NLP models\' out-of-distribution robustness (Hendrycks et al., 2020a;Tu et al., 2020), potentially due to its self-supervised objective and the use of large amounts of diverse pre-training data that encourages generalization from a small number of examples that counter the spurious correlations. Tu et al. (2020) showed a few other factors can also contribute to robust accuracy, including larger model size, more fine-tuning data, and longer fine-tuning. A similar observation is made by Taori et al. (2020) in the vision domain, where the authors found training with larger and more diverse datasets offer better robustness consistently in multiple cases, compared to various robustness interventions proposed in the existing literature. In general, the training strategy with an emphasis on a subset of samples that are particularly hard for the model to learn is sometimes also referred to as group DRO (Sagawa et al., 2020a), as an extension of vanilla distributional robust optimization (DRO) (Ben-Tal et al., 2013;Duchi et al., 2021). Extensions of DRO are mostly discussing the strategies on how to identify the samples considered as minority: Nam et al. (2020) trained two models in parallel, where the ""debiased"" model focuses on examples not learned by the ""biased"" model; Lahoti et al. (2020) used an adversary model to identify samples that are challenging to the main model; Liu et al. (2021) proposed to train the model a second time via up-weighting examples that have high training losses during the first time.', ""When to Use Data-driven or Model-based Approaches? In many cases both the data and the model can contribute to a model's lack of robustness, hence data-driven and model-based approaches could be combined to further improve a model's robustness. One interesting phenomenon observed by (Liu et al., 2019) is to attribute models' robustness failures to blind spots in the training data, or the intrinsic learning ability of the model. The authors found that both patterns are possible: in some cases models can be inoculated via being exposed to a small amount of challenging data, similar to the data augmentation approaches mentioned in Section 5.1; on the other hand, some challenging patterns remain difficult which connects to the larger question around generalizability to unseen adversarial and counterfactual patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), which is relatively under-explored but deserves much attention.""]","Pre-training Recent work has demonstrated pretraining as an effective way to improve NLP models' out-of-distribution robustness (Hendrycks et al., 2020a;Tu et al., 2020), potentially due to its self-supervised objective and the use of large amounts of diverse pre-training data that encourages generalization from a small number of examples that counter the spurious correlations. Tu et al. (2020) showed a few other factors can also contribute to robust accuracy, including larger model size, more fine-tuning data, and longer fine-tuning. A similar observation is made by Taori et al. (2020) in the vision domain, where the authors found training with larger and more diverse datasets offer better robustness consistently in multiple cases, compared to various robustness interventions proposed in the existing literature. In general, the training strategy with an emphasis on a subset of samples that are particularly hard for the model to learn is sometimes also referred to as group DRO (Sagawa et al., 2020a), as an extension of vanilla distributional robust optimization (DRO) (Ben-Tal et al., 2013;Duchi et al., 2021). Extensions of DRO are mostly discussing the strategies on how to identify the samples considered as minority: Nam et al. (2020) trained two models in parallel, where the ""debiased"" model focuses on examples not learned by the ""biased"" model; Lahoti et al. (2020) used an adversary model to identify samples that are challenging to the main model; Liu et al. (2021) proposed to train the model a second time via up-weighting examples that have high training losses during the first time.

When to Use Data-driven or Model-based Approaches? In many cases both the data and the model can contribute to a model's lack of robustness, hence data-driven and model-based approaches could be combined to further improve a model's robustness. One interesting phenomenon observed by (Liu et al., 2019) is to attribute models' robustness failures to blind spots in the training data, or the intrinsic learning ability of the model. The authors found that both patterns are possible: in some cases models can be inoculated via being exposed to a small amount of challenging data, similar to the data augmentation approaches mentioned in Section 5.1; on the other hand, some challenging patterns remain difficult which connects to the larger question around generalizability to unseen adversarial and counterfactual patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), which is relatively under-explored but deserves much attention.","(p14.0) Pre-training Recent work has demonstrated pretraining as an effective way to improve NLP models' out-of-distribution robustness (Hendrycks et al., 2020a;Tu et al., 2020), potentially due to its self-supervised objective and the use of large amounts of diverse pre-training data that encourages generalization from a small number of examples that counter the spurious correlations. Tu et al. (2020) showed a few other factors can also contribute to robust accuracy, including larger model size, more fine-tuning data, and longer fine-tuning. A similar observation is made by Taori et al. (2020) in the vision domain, where the authors found training with larger and more diverse datasets offer better robustness consistently in multiple cases, compared to various robustness interventions proposed in the existing literature. In general, the training strategy with an emphasis on a subset of samples that are particularly hard for the model to learn is sometimes also referred to as group DRO (Sagawa et al., 2020a), as an extension of vanilla distributional robust optimization (DRO) (Ben-Tal et al., 2013;Duchi et al., 2021). Extensions of DRO are mostly discussing the strategies on how to identify the samples considered as minority: Nam et al. (2020) trained two models in parallel, where the ""debiased"" model focuses on examples not learned by the ""biased"" model; Lahoti et al. (2020) used an adversary model to identify samples that are challenging to the main model; Liu et al. (2021) proposed to train the model a second time via up-weighting examples that have high training losses during the first time.

(p14.1) When to Use Data-driven or Model-based Approaches? In many cases both the data and the model can contribute to a model's lack of robustness, hence data-driven and model-based approaches could be combined to further improve a model's robustness. One interesting phenomenon observed by (Liu et al., 2019) is to attribute models' robustness failures to blind spots in the training data, or the intrinsic learning ability of the model. The authors found that both patterns are possible: in some cases models can be inoculated via being exposed to a small amount of challenging data, similar to the data augmentation approaches mentioned in Section 5.1; on the other hand, some challenging patterns remain difficult which connects to the larger question around generalizability to unseen adversarial and counterfactual patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), which is relatively under-explored but deserves much attention.","[['b13', None, 'b28', 'b39'], [None, 'b14']]","[['b13', None, 'b28', 'b39'], [None, 'b14']]",6,"1. Pre-training Recent work has demonstrated pretraining as an effective way to improve NLP models' out-of-distribution robustness (Hendrycks et al., 2020a;Tu et al., 2020), potentially due to its self-supervised objective and the use of large amounts of diverse pre-training data that encourages generalization from a small number of examples that counter the spurious correlations.
2. Tu et al. (2020) showed a few other factors can also contribute to robust accuracy, including larger model size, more fine-tuning data, and longer fine-tuning.
3. A similar observation is made by Taori et al. (2020) in the vision domain, where the authors found training with larger and more diverse datasets offer better robustness consistently in multiple cases, compared to various robustness interventions proposed in the existing literature.
4. In general, the training strategy with an emphasis on a subset of samples that are particularly hard for the model to learn is sometimes also referred to as group DRO (Sagawa et al., 2020a), as an extension of vanilla distributional robust optimization (DRO) (Ben-Tal et al., 2013;Duchi et al., 2021).
5. Extensions of DRO are mostly discussing the strategies on how to identify the samples considered as minority: Nam et al. (2020) trained two models in parallel, where the ""debiased"" model focuses on examples not learned by the ""biased"" model; Lahoti et al. (2020) used an adversary model to identify samples that are challenging to the main model; Liu et al. (2021) proposed to train the model a second time via up-weighting examples that have high training losses during the first time.
6. When to Use Data-driven or Model-based Approaches?
7. In many cases both the data and the model can contribute to a model's lack of robustness, hence data-driven and model-based approaches could be combined to further improve a model's robustness.
8. One interesting phenomenon observed by (Liu et al., 2019) is to attribute models' robustness failures to blind spots in the training data, or the intrinsic learning ability of the model.
9. The authors found that both patterns are possible: in some cases models can be inoculated via being exposed to a small amount of challenging data, similar to the data augmentation approaches mentioned in Section 5.1; on the other hand, some challenging patterns remain difficult which connects to the larger question around generalizability to unseen adversarial and counterfactual patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), which is relatively under-explored but deserves much attention."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s13,Data-driven Approaches,"['p13.0', 'p13.1']","['Data augmentation recently gained a lot of interest, in improving performance in low-resourced language settings, few-shot learning, mitigating biases, and improving robustness in NLP models (Feng et al., 2021;Dhole et al., 2021). Techniques like Mixup (Zhang et al., 2018), MixText (Chen et al., 2020), CutOut (DeVries and Taylor, 2017), Aug-Mix (Hendrycks et al., 2020b), HiddenCut (Chen et al., 2021a), have been shown to substantially improve the robustness and the generalization of models. Such mitigation strategies are operated at the data level, and often hard to be interpreted in terms of how and why mitigation works.', 'Other lines of work deal with spans or regions associated within data points to prevent models from heavily relying on spurious patterns. To make NLP models more robust on sentiment analysis and NLI tasks, Kaushik et al. (2019) proposed curating counterfactually augmented data via a human-inthe-loop process, and showed that models trained on the combination of this augmented data and original data are less sensitive to spurious patterns. Differently, Wang et al. (2021d) performed strategic data augmentation to perturb the set of ""shortcuts"" that are automatically identified, and found that mitigating these leads to more robust models in multiple NLP tasks. This line of mitigation strategies closely relates to how spurious correlations can be measured and identified, as many of the challenging or adversarial examples (Table 1) can sometimes be used to augment the original model to improve its robustness, either in the discrete input space as additional training examples (Liu et al., 2019;Kaushik et al., 2019;Anastasopoulos et al., 2019;Vaibhav et al., 2019;Khashabi et al., 2020), or in the embedding space (Zhu et al., 2020;Zhao et al., 2018b;Miyato et al., 2017;.']","Data augmentation recently gained a lot of interest, in improving performance in low-resourced language settings, few-shot learning, mitigating biases, and improving robustness in NLP models (Feng et al., 2021;Dhole et al., 2021). Techniques like Mixup (Zhang et al., 2018), MixText (Chen et al., 2020), CutOut (DeVries and Taylor, 2017), Aug-Mix (Hendrycks et al., 2020b), HiddenCut (Chen et al., 2021a), have been shown to substantially improve the robustness and the generalization of models. Such mitigation strategies are operated at the data level, and often hard to be interpreted in terms of how and why mitigation works.

Other lines of work deal with spans or regions associated within data points to prevent models from heavily relying on spurious patterns. To make NLP models more robust on sentiment analysis and NLI tasks, Kaushik et al. (2019) proposed curating counterfactually augmented data via a human-inthe-loop process, and showed that models trained on the combination of this augmented data and original data are less sensitive to spurious patterns. Differently, Wang et al. (2021d) performed strategic data augmentation to perturb the set of ""shortcuts"" that are automatically identified, and found that mitigating these leads to more robust models in multiple NLP tasks. This line of mitigation strategies closely relates to how spurious correlations can be measured and identified, as many of the challenging or adversarial examples (Table 1) can sometimes be used to augment the original model to improve its robustness, either in the discrete input space as additional training examples (Liu et al., 2019;Kaushik et al., 2019;Anastasopoulos et al., 2019;Vaibhav et al., 2019;Khashabi et al., 2020), or in the embedding space (Zhu et al., 2020;Zhao et al., 2018b;Miyato et al., 2017;.","(p13.0) Data augmentation recently gained a lot of interest, in improving performance in low-resourced language settings, few-shot learning, mitigating biases, and improving robustness in NLP models (Feng et al., 2021;Dhole et al., 2021). Techniques like Mixup (Zhang et al., 2018), MixText (Chen et al., 2020), CutOut (DeVries and Taylor, 2017), Aug-Mix (Hendrycks et al., 2020b), HiddenCut (Chen et al., 2021a), have been shown to substantially improve the robustness and the generalization of models. Such mitigation strategies are operated at the data level, and often hard to be interpreted in terms of how and why mitigation works.

(p13.1) Other lines of work deal with spans or regions associated within data points to prevent models from heavily relying on spurious patterns. To make NLP models more robust on sentiment analysis and NLI tasks, Kaushik et al. (2019) proposed curating counterfactually augmented data via a human-inthe-loop process, and showed that models trained on the combination of this augmented data and original data are less sensitive to spurious patterns. Differently, Wang et al. (2021d) performed strategic data augmentation to perturb the set of ""shortcuts"" that are automatically identified, and found that mitigating these leads to more robust models in multiple NLP tasks. This line of mitigation strategies closely relates to how spurious correlations can be measured and identified, as many of the challenging or adversarial examples (Table 1) can sometimes be used to augment the original model to improve its robustness, either in the discrete input space as additional training examples (Liu et al., 2019;Kaushik et al., 2019;Anastasopoulos et al., 2019;Vaibhav et al., 2019;Khashabi et al., 2020), or in the embedding space (Zhu et al., 2020;Zhao et al., 2018b;Miyato et al., 2017;.","[['b78', None, 'b50'], ['b62', 'b86', 'b14', 'b84', 'b2', None, 'b22', 'b44', 'b1']]","[['b78', None, 'b50'], ['b62', 'b86', 'b14', 'b84', 'b2', None, 'b22', 'b44', 'b1']]",12,"1. Data augmentation recently gained a lot of interest, in improving performance in low-resourced language settings, few-shot learning, mitigating biases, and improving robustness in NLP models (Feng et al., 2021;Dhole et al., 2021).
2. Techniques like Mixup (Zhang et al., 2018), MixText (Chen et al., 2020), CutOut (DeVries and Taylor, 2017), Aug-Mix (Hendrycks et al., 2020b), HiddenCut (Chen et al., 2021a), have been shown to substantially improve the robustness and the generalization of models.
3. Such mitigation strategies are operated at the data level, and often hard to be interpreted in terms of how and why mitigation works.
4. Other lines of work deal with spans or regions associated within data points to prevent models from heavily relying on spurious patterns.
5. To make NLP models more robust on sentiment analysis and NLI tasks, Kaushik et al. (2019) proposed curating counterfactually augmented data via a human-inthe-loop process, and showed that models trained on the combination of this augmented data and original data are less sensitive to spurious patterns.
6. Differently, Wang et al. (2021d) performed strategic data augmentation to perturb the set of ""shortcuts"" that are automatically identified, and found that mitigating these leads to more robust models in multiple NLP tasks.
7. This line of mitigation strategies closely relates to how spurious correlations can be measured and identified, as many of the challenging or adversarial examples (Table 1) can sometimes be used to augment the original model to improve its robustness, either in the discrete input space as additional training examples (Liu et al., 2019;Kaushik et al., 2019;Anastasopoulos et al., 2019;Vaibhav et al., 2019;Khashabi et al., 2020), or in the embedding space (Zhu et al., 2020;Zhao et al., 2018b;Miyato et al., 2017;."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s11,Model-in-the-loop vs.,['p11.0'],"['Human-in-the-loop Some work adopts human-in-the-loop to generate challenging examples, e.g., Counterfacutal-NLI (Kaushik et al., 2019) and Natural-Perturbed-QA (Khashabi et al., 2020). Other work applies modelin-the-loop to increase the likelihood that the perturbed examples are challenging for state-of-the-art models, but it might also introduce biases towards the particular model used. For example, SWAG (Zellers et al., 2018) was introduced that fooled most models at the time of publishing but was soon ""solved"" after BERT (Devlin et al., 2019) was introduced. As a result, Yuan et al. (2021) present a study over the transferability of adversarial examples, and Contrast Sets (Gardner et al., 2020) intentionally avoid using model-in-the-loop. Further, more recent work adopts adversarial humanand-model-in-the-loop to create more difficult examples for benchmarking, e.g., Adv-QA (Bartolo et al., 2020), Adv-Quizbowl (Wallace et al., 2019b), ANLI (Nie et al., 2020), and Dynabench (Kiela et al., 2021).']","Human-in-the-loop Some work adopts human-in-the-loop to generate challenging examples, e.g., Counterfacutal-NLI (Kaushik et al., 2019) and Natural-Perturbed-QA (Khashabi et al., 2020). Other work applies modelin-the-loop to increase the likelihood that the perturbed examples are challenging for state-of-the-art models, but it might also introduce biases towards the particular model used. For example, SWAG (Zellers et al., 2018) was introduced that fooled most models at the time of publishing but was soon ""solved"" after BERT (Devlin et al., 2019) was introduced. As a result, Yuan et al. (2021) present a study over the transferability of adversarial examples, and Contrast Sets (Gardner et al., 2020) intentionally avoid using model-in-the-loop. Further, more recent work adopts adversarial humanand-model-in-the-loop to create more difficult examples for benchmarking, e.g., Adv-QA (Bartolo et al., 2020), Adv-Quizbowl (Wallace et al., 2019b), ANLI (Nie et al., 2020), and Dynabench (Kiela et al., 2021).","(p11.0) Human-in-the-loop Some work adopts human-in-the-loop to generate challenging examples, e.g., Counterfacutal-NLI (Kaushik et al., 2019) and Natural-Perturbed-QA (Khashabi et al., 2020). Other work applies modelin-the-loop to increase the likelihood that the perturbed examples are challenging for state-of-the-art models, but it might also introduce biases towards the particular model used. For example, SWAG (Zellers et al., 2018) was introduced that fooled most models at the time of publishing but was soon ""solved"" after BERT (Devlin et al., 2019) was introduced. As a result, Yuan et al. (2021) present a study over the transferability of adversarial examples, and Contrast Sets (Gardner et al., 2020) intentionally avoid using model-in-the-loop. Further, more recent work adopts adversarial humanand-model-in-the-loop to create more difficult examples for benchmarking, e.g., Adv-QA (Bartolo et al., 2020), Adv-Quizbowl (Wallace et al., 2019b), ANLI (Nie et al., 2020), and Dynabench (Kiela et al., 2021).","[['b73', 'b3', 'b48', 'b29', 'b75', 'b1', 'b2']]","[['b73', 'b3', 'b48', 'b29', 'b75', 'b1', 'b2']]",7,"1. Human-in-the-loop Some work adopts human-in-the-loop to generate challenging examples, e.g., Counterfacutal-NLI (Kaushik et al., 2019) and Natural-Perturbed-QA (Khashabi et al., 2020).
2. Other work applies modelin-the-loop to increase the likelihood that the perturbed examples are challenging for state-of-the-art models, but it might also introduce biases towards the particular model used.
3. For example, SWAG (Zellers et al., 2018) was introduced that fooled most models at the time of publishing but was soon ""solved"" after BERT (Devlin et al., 2019) was introduced.
4. As a result, Yuan et al. (2021) present a study over the transferability of adversarial examples, and Contrast Sets (Gardner et al., 2020) intentionally avoid using model-in-the-loop.
5. Further, more recent work adopts adversarial humanand-model-in-the-loop to create more difficult examples for benchmarking, e.g., Adv-QA (Bartolo et al., 2020), Adv-Quizbowl (Wallace et al., 2019b), ANLI (Nie et al., 2020), and Dynabench (Kiela et al., 2021)."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s8,Human Prior and Error Analyses Driven,"['p8.0', 'p8.1', 'p8.2', 'p8.3']","['An increasing body of work has been conducted on understanding and measuring robustness in NLP models (Tu et al., 2020;Sagawa et al., 2020b;Geirhos et al., 2020) across various NLP tasks, largely relying on human priors and error analyses.', 'Natural Language Inference Naik et al. (2018) sampled misclassified examples and analyzed their potential sources of errors, which are then grouped into a typology of common reasons for error. Such error types then served as the bases to construct the stress test set, to further evaluate whether NLI models have the ability to make real inferential decisions, or simply rely on sophisticated pattern matching. Gururangan et al. (2018) found that current NLI models are likely to identify the label by relying only on the hypothesis, and Poliak et al. (2018)     showed another approach by limiting the input space of the characters so that the models will be likely to perceive data typos and misspellings.', 'Syntactic and Semantic Parsing Robust parsing has been studied in several existing works (Lee et al., 1995;Aït-Mokhtar et al., 2002). More recent work showed that neural semantic parsers are still not robust against lexical and stylistic variations, or meaning-preserving perturbations (Marzinotto et al., 2019;Huang et al., 2021), and proposed ways to improve their robustness through data augmentation (Huang et al., 2021) and adversarial learning (Marzinotto et al., 2019).', ""Text Generation Existing work found that text generation models also suffer from robustness issues, e.g., text summarization models suffer from positional bias (Jung et al., 2019), layout bias (Kryscinski et al., 2019), and a lack of faithfulness and factuality (Kryscinski et al., 2019;Maynez et al., 2020;Chen et al., 2021b); data-to-text models sometimes hallucinate texts that are not supported by the data (Parikh et al., 2020;Wang et al., 2020d). In addition, Sellam et al. (2020);  pointed out the deficiency of existing automatic evaluation metrics and proposed new metrics to better align the generation quality with human judgements.  (2019) show that commonly used crowdsourced datasets for training NLI models might make certain syntactic heuristics more easily adopted by statistical learners. Further, Bras et al. (2020) propose to use a lightweight adversarial filtering approach to filter dataset biases, which is approximated using each instance's predictability score.""]","An increasing body of work has been conducted on understanding and measuring robustness in NLP models (Tu et al., 2020;Sagawa et al., 2020b;Geirhos et al., 2020) across various NLP tasks, largely relying on human priors and error analyses.

Natural Language Inference Naik et al. (2018) sampled misclassified examples and analyzed their potential sources of errors, which are then grouped into a typology of common reasons for error. Such error types then served as the bases to construct the stress test set, to further evaluate whether NLI models have the ability to make real inferential decisions, or simply rely on sophisticated pattern matching. Gururangan et al. (2018) found that current NLI models are likely to identify the label by relying only on the hypothesis, and Poliak et al. (2018)     showed another approach by limiting the input space of the characters so that the models will be likely to perceive data typos and misspellings.

Syntactic and Semantic Parsing Robust parsing has been studied in several existing works (Lee et al., 1995;Aït-Mokhtar et al., 2002). More recent work showed that neural semantic parsers are still not robust against lexical and stylistic variations, or meaning-preserving perturbations (Marzinotto et al., 2019;Huang et al., 2021), and proposed ways to improve their robustness through data augmentation (Huang et al., 2021) and adversarial learning (Marzinotto et al., 2019).

Text Generation Existing work found that text generation models also suffer from robustness issues, e.g., text summarization models suffer from positional bias (Jung et al., 2019), layout bias (Kryscinski et al., 2019), and a lack of faithfulness and factuality (Kryscinski et al., 2019;Maynez et al., 2020;Chen et al., 2021b); data-to-text models sometimes hallucinate texts that are not supported by the data (Parikh et al., 2020;Wang et al., 2020d). In addition, Sellam et al. (2020);  pointed out the deficiency of existing automatic evaluation metrics and proposed new metrics to better align the generation quality with human judgements.  (2019) show that commonly used crowdsourced datasets for training NLI models might make certain syntactic heuristics more easily adopted by statistical learners. Further, Bras et al. (2020) propose to use a lightweight adversarial filtering approach to filter dataset biases, which is approximated using each instance's predictability score.","(p8.0) An increasing body of work has been conducted on understanding and measuring robustness in NLP models (Tu et al., 2020;Sagawa et al., 2020b;Geirhos et al., 2020) across various NLP tasks, largely relying on human priors and error analyses.

(p8.1) Natural Language Inference Naik et al. (2018) sampled misclassified examples and analyzed their potential sources of errors, which are then grouped into a typology of common reasons for error. Such error types then served as the bases to construct the stress test set, to further evaluate whether NLI models have the ability to make real inferential decisions, or simply rely on sophisticated pattern matching. Gururangan et al. (2018) found that current NLI models are likely to identify the label by relying only on the hypothesis, and Poliak et al. (2018)     showed another approach by limiting the input space of the characters so that the models will be likely to perceive data typos and misspellings.

(p8.2) Syntactic and Semantic Parsing Robust parsing has been studied in several existing works (Lee et al., 1995;Aït-Mokhtar et al., 2002). More recent work showed that neural semantic parsers are still not robust against lexical and stylistic variations, or meaning-preserving perturbations (Marzinotto et al., 2019;Huang et al., 2021), and proposed ways to improve their robustness through data augmentation (Huang et al., 2021) and adversarial learning (Marzinotto et al., 2019).

(p8.3) Text Generation Existing work found that text generation models also suffer from robustness issues, e.g., text summarization models suffer from positional bias (Jung et al., 2019), layout bias (Kryscinski et al., 2019), and a lack of faithfulness and factuality (Kryscinski et al., 2019;Maynez et al., 2020;Chen et al., 2021b); data-to-text models sometimes hallucinate texts that are not supported by the data (Parikh et al., 2020;Wang et al., 2020d). In addition, Sellam et al. (2020);  pointed out the deficiency of existing automatic evaluation metrics and proposed new metrics to better align the generation quality with human judgements.  (2019) show that commonly used crowdsourced datasets for training NLI models might make certain syntactic heuristics more easily adopted by statistical learners. Further, Bras et al. (2020) propose to use a lightweight adversarial filtering approach to filter dataset biases, which is approximated using each instance's predictability score.","[[None], [None, 'b27'], [None, 'b10', 'b17'], [None, 'b18', 'b66', 'b5']]","[[None], [None, 'b27'], [None, 'b10', 'b17'], [None, 'b18', 'b66', 'b5']]",10,"1. An increasing body of work has been conducted on understanding and measuring robustness in NLP models (Tu et al., 2020;Sagawa et al., 2020b;Geirhos et al., 2020) across various NLP tasks, largely relying on human priors and error analyses.
2. Natural Language Inference Naik et al. (2018) sampled misclassified examples and analyzed their potential sources of errors, which are then grouped into a typology of common reasons for error.
3. Such error types then served as the bases to construct the stress test set, to further evaluate whether NLI models have the ability to make real inferential decisions, or simply rely on sophisticated pattern matching.
4. Gururangan et al. (2018) found that current NLI models are likely to identify the label by relying only on the hypothesis, and Poliak et al. (2018)     showed another approach by limiting the input space of the characters so that the models will be likely to perceive data typos and misspellings.
5. Syntactic and Semantic Parsing Robust parsing has been studied in several existing works (Lee et al., 1995;Aït-Mokhtar et al., 2002).
6. More recent work showed that neural semantic parsers are still not robust against lexical and stylistic variations, or meaning-preserving perturbations (Marzinotto et al., 2019;Huang et al., 2021), and proposed ways to improve their robustness through data augmentation (Huang et al., 2021) and adversarial learning (Marzinotto et al., 2019).Text Generation Existing work found that text generation models also suffer from robustness issues, e.g., text summarization models suffer from positional bias (Jung et al., 2019), layout bias (Kryscinski et al., 2019), and a lack of faithfulness and factuality (Kryscinski et al., 2019;Maynez et al., 2020;Chen et al., 2021b); data-to-text models sometimes hallucinate texts that are not supported by the data (Parikh et al., 2020;Wang et al., 2020d).
7. In addition, Sellam et al. (2020);  pointed out the deficiency of existing automatic evaluation metrics and proposed new metrics to better align the generation quality with human judgements.
8. (2019) show that commonly used crowdsourced datasets for training NLI models might make certain syntactic heuristics more easily adopted by statistical learners.
9. Further, Bras et al. (2020) propose to use a lightweight adversarial filtering approach to filter dataset biases, which is approximated using each instance's predictability score."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s6,Continuous vs. Discrete in Search Space,"['p6.0', 'p6.1', 'p6.2']","['The most obvious characteristic is probably the discrete nature of the space of text. This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP (Lei et al., 2019;Zhang et al., 2020c), in the sense that simple gradient-based adversarial attacks will not directly translate to meaningful attacks in the discrete text space, and multiple novel attack methods are proposed to fill the gap, as we will discuss in later sections.', 'Perceptible to Human vs. Not On a related topic, one of the most impressive property of ad-versarial attack in vision is that small perturbation of the image data imperceptible to human are sufficient to deceive the model (Szegedy et al., 2013), while this can hardly be true for NLP attacks. Instead of being imperceptible, the adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences are not altered (despite being perceptible). On the other hand, there are ways to generate samples where the changes, although being perceptible, are often ignored by human brain due to some psychological prior on how a human processes the text (Anastasopoulos et al., 2019;. Support vs. Density Difference of the Data Distributions Another difference is more likely seen in the discussion of the domain adaptation of vision and NLP study. In vision study, although the images from training distribution and test distribution can be sufficiently different, the train and test distributions mostly share the same support (the pixels are always sampled from a 0-255 integer space), although the density of these distributions can be very different (e.g., photos vs. sketches). On the other hand, domain adaptation of NLP sometimes studies the regime where the supports of the data differ, e.g., the vocabularies can be significantly different in cross-lingual studies (Abad et al., 2020;Zhang et al., 2020a).', ""A Common Theme Despite the disparities between vision and NLP, the common theme of pushing the model to generalize from D to D preserves. The practical difference between D and D is more than often defined by the human's understanding of the data, and can differ in vision and NLP as humans perceive and process images and texts in subtly different ways, which creates both opportunities for learning and barriers for direct transfer. Certain lines of research try to bridge the learning in the vision domain to the embedding space in the NLP domain, while other lines of research create more interpretable attacks in the discrete text space (see Table 1 for these two lines of work). How those two lines of research transfer to each other, or complement each other, is not fully explored and calls for additional research.""]","The most obvious characteristic is probably the discrete nature of the space of text. This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP (Lei et al., 2019;Zhang et al., 2020c), in the sense that simple gradient-based adversarial attacks will not directly translate to meaningful attacks in the discrete text space, and multiple novel attack methods are proposed to fill the gap, as we will discuss in later sections.

Perceptible to Human vs. Not On a related topic, one of the most impressive property of ad-versarial attack in vision is that small perturbation of the image data imperceptible to human are sufficient to deceive the model (Szegedy et al., 2013), while this can hardly be true for NLP attacks. Instead of being imperceptible, the adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences are not altered (despite being perceptible). On the other hand, there are ways to generate samples where the changes, although being perceptible, are often ignored by human brain due to some psychological prior on how a human processes the text (Anastasopoulos et al., 2019;. Support vs. Density Difference of the Data Distributions Another difference is more likely seen in the discussion of the domain adaptation of vision and NLP study. In vision study, although the images from training distribution and test distribution can be sufficiently different, the train and test distributions mostly share the same support (the pixels are always sampled from a 0-255 integer space), although the density of these distributions can be very different (e.g., photos vs. sketches). On the other hand, domain adaptation of NLP sometimes studies the regime where the supports of the data differ, e.g., the vocabularies can be significantly different in cross-lingual studies (Abad et al., 2020;Zhang et al., 2020a).

A Common Theme Despite the disparities between vision and NLP, the common theme of pushing the model to generalize from D to D preserves. The practical difference between D and D is more than often defined by the human's understanding of the data, and can differ in vision and NLP as humans perceive and process images and texts in subtly different ways, which creates both opportunities for learning and barriers for direct transfer. Certain lines of research try to bridge the learning in the vision domain to the embedding space in the NLP domain, while other lines of research create more interpretable attacks in the discrete text space (see Table 1 for these two lines of work). How those two lines of research transfer to each other, or complement each other, is not fully explored and calls for additional research.","(p6.0) The most obvious characteristic is probably the discrete nature of the space of text. This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP (Lei et al., 2019;Zhang et al., 2020c), in the sense that simple gradient-based adversarial attacks will not directly translate to meaningful attacks in the discrete text space, and multiple novel attack methods are proposed to fill the gap, as we will discuss in later sections.

(p6.1) Perceptible to Human vs. Not On a related topic, one of the most impressive property of ad-versarial attack in vision is that small perturbation of the image data imperceptible to human are sufficient to deceive the model (Szegedy et al., 2013), while this can hardly be true for NLP attacks. Instead of being imperceptible, the adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences are not altered (despite being perceptible). On the other hand, there are ways to generate samples where the changes, although being perceptible, are often ignored by human brain due to some psychological prior on how a human processes the text (Anastasopoulos et al., 2019;. Support vs. Density Difference of the Data Distributions Another difference is more likely seen in the discussion of the domain adaptation of vision and NLP study. In vision study, although the images from training distribution and test distribution can be sufficiently different, the train and test distributions mostly share the same support (the pixels are always sampled from a 0-255 integer space), although the density of these distributions can be very different (e.g., photos vs. sketches). On the other hand, domain adaptation of NLP sometimes studies the regime where the supports of the data differ, e.g., the vocabularies can be significantly different in cross-lingual studies (Abad et al., 2020;Zhang et al., 2020a).

(p6.2) A Common Theme Despite the disparities between vision and NLP, the common theme of pushing the model to generalize from D to D preserves. The practical difference between D and D is more than often defined by the human's understanding of the data, and can differ in vision and NLP as humans perceive and process images and texts in subtly different ways, which creates both opportunities for learning and barriers for direct transfer. Certain lines of research try to bridge the learning in the vision domain to the embedding space in the NLP domain, while other lines of research create more interpretable attacks in the discrete text space (see Table 1 for these two lines of work). How those two lines of research transfer to each other, or complement each other, is not fully explored and calls for additional research.","[['b11', 'b80'], ['b0', None, 'b76'], []]","[['b11', 'b80'], ['b0', None, 'b76'], []]",5,"1. The most obvious characteristic is probably the discrete nature of the space of text.
2. This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP (Lei et al., 2019;Zhang et al., 2020c), in the sense that simple gradient-based adversarial attacks will not directly translate to meaningful attacks in the discrete text space, and multiple novel attack methods are proposed to fill the gap, as we will discuss in later sections.
3. Perceptible to Human vs. Not On a related topic, one of the most impressive property of ad-versarial attack in vision is that small perturbation of the image data imperceptible to human are sufficient to deceive the model (Szegedy et al., 2013), while this can hardly be true for NLP attacks.
4. Instead of being imperceptible, the adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences are not altered (despite being perceptible).
5. On the other hand, there are ways to generate samples where the changes, although being perceptible, are often ignored by human brain due to some psychological prior on how a human processes the text (Anastasopoulos et al., 2019;. Support vs. Density Difference of the Data Distributions Another difference is more likely seen in the discussion of the domain adaptation of vision and NLP study. In vision study, although the images from training distribution and test distribution can be sufficiently different, the train and test distributions mostly share the same support (the pixels are always sampled from a 0-255 integer space), although the density of these distributions can be very different (e.g., photos vs. sketches).
6. On the other hand, domain adaptation of NLP sometimes studies the regime where the supports of the data differ, e.g., the vocabularies can be significantly different in cross-lingual studies (Abad et al., 2020;Zhang et al., 2020a).
7. A Common Theme Despite the disparities between vision and NLP, the common theme of pushing the model to generalize from D to D preserves.
8. The practical difference between D and D is more than often defined by the human's understanding of the data, and can differ in vision and NLP as humans perceive and process images and texts in subtly different ways, which creates both opportunities for learning and barriers for direct transfer.
9. Certain lines of research try to bridge the learning in the vision domain to the embedding space in the NLP domain, while other lines of research create more interpretable attacks in the discrete text space (see Table 1 for these two lines of work).
10. How those two lines of research transfer to each other, or complement each other, is not fully explored and calls for additional research."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s4,Connections and A Common Theme,"['p4.0', 'p4.1', 'p4.2']","[""The above two categories of robustness can be unified under the same framework, i.e., whether D represents a synthetic distribution shift (via adversarial attacks) or a natural distribution shift. Existing work has shown a model's performance might degrade substantially in both cases, but the transferability of the two categories is relatively underexplored. In the vision domain, Taori et al. (2020) investigate models' robustness to natural distribution shift, and show that robustness to synthetic distribution shift might offer little to no robustness improvement under natural distribution shift. Some studies show NLP models might not generalize to unseen adversarial patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), but more work is needed to systematically bridge the gap between NLP models' robustness under natural and synthetic distribution shifts."", 'To better understand why models exhibit a lack of robustness, some existing work attributed this to the fact that models sometimes utilize spurious correlations between input features and labels, rather than the genuine ones, where spurious features are commonly defined as features that do not causally affect a task\'s label (Srivastava et al., 2020;Wang and Culotta, 2020b): they correlate with task labels but fail to transfer to more challenging test conditions or out-of-distribution data (Geirhos et al., 2020). Some other work defined it as ""prediction rules that work for the majority examples but do not hold in general"" (Tu et al., 2020). Such spurious correlations are sometimes referred as dataset bias (Clark et al., 2019;He et al., 2019), annotation artifacts (Gururangan et al., 2018), or group shift (Oren et al., 2019) in the literature. Further, evidence showed that controlling model\'s learning in spurious features will improve model\'s performances in distribution shifts (Wang et al., 2019a,b); also, discussions on the connections between adversarial robustness and learning of spurious features have been raised (Ilyas et al., 2019;. Theoretical discussions connecting these fields have also been offered by crediting a reason of model\'s lack of robustness in either distribution shift or adversarial attack to model\'s learning of spurious features (Wang et al., 2021c).', 'Further, in certain applications, model ""robustness"" can also be connected with models\' instability (Milani Fard et al., 2016), or models having poorly-calibrated uncertainty estimation (Guo et al., 2017), where Bayesian methods (Graves, 2011;Blundell et al., 2015), dropout-based (Gal and Ghahramani, 2016;Kingma et al., 2015) and ensemble-based approaches (Lakshminarayanan et al., 2017) have been proposed to improve models\' uncertainty estimation. Recently, Ovadia et al. (2019) have shown models\' uncertainty estimation can degrade significantly under distributional shift, and call for more work to ensure a model ""knows when it doesn\'t know"" by giving lower uncertainty estimates over out-of-distribution data. This is another example where models can be less robust under distributional shifts, and again emphasizes the need of building more unified benchmarks to measure a model\'s performance (e.g., robust accuracy, calibration, stability) under distribution shifts, in addition to in-distribution accuracy.']","The above two categories of robustness can be unified under the same framework, i.e., whether D represents a synthetic distribution shift (via adversarial attacks) or a natural distribution shift. Existing work has shown a model's performance might degrade substantially in both cases, but the transferability of the two categories is relatively underexplored. In the vision domain, Taori et al. (2020) investigate models' robustness to natural distribution shift, and show that robustness to synthetic distribution shift might offer little to no robustness improvement under natural distribution shift. Some studies show NLP models might not generalize to unseen adversarial patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), but more work is needed to systematically bridge the gap between NLP models' robustness under natural and synthetic distribution shifts.

To better understand why models exhibit a lack of robustness, some existing work attributed this to the fact that models sometimes utilize spurious correlations between input features and labels, rather than the genuine ones, where spurious features are commonly defined as features that do not causally affect a task's label (Srivastava et al., 2020;Wang and Culotta, 2020b): they correlate with task labels but fail to transfer to more challenging test conditions or out-of-distribution data (Geirhos et al., 2020). Some other work defined it as ""prediction rules that work for the majority examples but do not hold in general"" (Tu et al., 2020). Such spurious correlations are sometimes referred as dataset bias (Clark et al., 2019;He et al., 2019), annotation artifacts (Gururangan et al., 2018), or group shift (Oren et al., 2019) in the literature. Further, evidence showed that controlling model's learning in spurious features will improve model's performances in distribution shifts (Wang et al., 2019a,b); also, discussions on the connections between adversarial robustness and learning of spurious features have been raised (Ilyas et al., 2019;. Theoretical discussions connecting these fields have also been offered by crediting a reason of model's lack of robustness in either distribution shift or adversarial attack to model's learning of spurious features (Wang et al., 2021c).

Further, in certain applications, model ""robustness"" can also be connected with models' instability (Milani Fard et al., 2016), or models having poorly-calibrated uncertainty estimation (Guo et al., 2017), where Bayesian methods (Graves, 2011;Blundell et al., 2015), dropout-based (Gal and Ghahramani, 2016;Kingma et al., 2015) and ensemble-based approaches (Lakshminarayanan et al., 2017) have been proposed to improve models' uncertainty estimation. Recently, Ovadia et al. (2019) have shown models' uncertainty estimation can degrade significantly under distributional shift, and call for more work to ensure a model ""knows when it doesn't know"" by giving lower uncertainty estimates over out-of-distribution data. This is another example where models can be less robust under distributional shifts, and again emphasizes the need of building more unified benchmarks to measure a model's performance (e.g., robust accuracy, calibration, stability) under distribution shifts, in addition to in-distribution accuracy.","(p4.0) The above two categories of robustness can be unified under the same framework, i.e., whether D represents a synthetic distribution shift (via adversarial attacks) or a natural distribution shift. Existing work has shown a model's performance might degrade substantially in both cases, but the transferability of the two categories is relatively underexplored. In the vision domain, Taori et al. (2020) investigate models' robustness to natural distribution shift, and show that robustness to synthetic distribution shift might offer little to no robustness improvement under natural distribution shift. Some studies show NLP models might not generalize to unseen adversarial patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), but more work is needed to systematically bridge the gap between NLP models' robustness under natural and synthetic distribution shifts.

(p4.1) To better understand why models exhibit a lack of robustness, some existing work attributed this to the fact that models sometimes utilize spurious correlations between input features and labels, rather than the genuine ones, where spurious features are commonly defined as features that do not causally affect a task's label (Srivastava et al., 2020;Wang and Culotta, 2020b): they correlate with task labels but fail to transfer to more challenging test conditions or out-of-distribution data (Geirhos et al., 2020). Some other work defined it as ""prediction rules that work for the majority examples but do not hold in general"" (Tu et al., 2020). Such spurious correlations are sometimes referred as dataset bias (Clark et al., 2019;He et al., 2019), annotation artifacts (Gururangan et al., 2018), or group shift (Oren et al., 2019) in the literature. Further, evidence showed that controlling model's learning in spurious features will improve model's performances in distribution shifts (Wang et al., 2019a,b); also, discussions on the connections between adversarial robustness and learning of spurious features have been raised (Ilyas et al., 2019;. Theoretical discussions connecting these fields have also been offered by crediting a reason of model's lack of robustness in either distribution shift or adversarial attack to model's learning of spurious features (Wang et al., 2021c).

(p4.2) Further, in certain applications, model ""robustness"" can also be connected with models' instability (Milani Fard et al., 2016), or models having poorly-calibrated uncertainty estimation (Guo et al., 2017), where Bayesian methods (Graves, 2011;Blundell et al., 2015), dropout-based (Gal and Ghahramani, 2016;Kingma et al., 2015) and ensemble-based approaches (Lakshminarayanan et al., 2017) have been proposed to improve models' uncertainty estimation. Recently, Ovadia et al. (2019) have shown models' uncertainty estimation can degrade significantly under distributional shift, and call for more work to ensure a model ""knows when it doesn't know"" by giving lower uncertainty estimates over out-of-distribution data. This is another example where models can be less robust under distributional shifts, and again emphasizes the need of building more unified benchmarks to measure a model's performance (e.g., robust accuracy, calibration, stability) under distribution shifts, in addition to in-distribution accuracy.","[[None, 'b39'], ['b55', 'b65', 'b30', 'b32', None], ['b20', None, 'b4', 'b9']]","[[None, 'b39'], ['b55', 'b65', 'b30', 'b32', None], ['b20', None, 'b4', 'b9']]",11,"1. The above two categories of robustness can be unified under the same framework, i.e., whether D represents a synthetic distribution shift (via adversarial attacks) or a natural distribution shift.
2. Existing work has shown a model's performance might degrade substantially in both cases, but the transferability of the two categories is relatively underexplored.
3. In the vision domain, Taori et al. (2020) investigate models' robustness to natural distribution shift, and show that robustness to synthetic distribution shift might offer little to no robustness improvement under natural distribution shift.
4. Some studies show NLP models might not generalize to unseen adversarial patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), but more work is needed to systematically bridge the gap between NLP models' robustness under natural and synthetic distribution shifts.
5. To better understand why models exhibit a lack of robustness, some existing work attributed this to the fact that models sometimes utilize spurious correlations between input features and labels, rather than the genuine ones, where spurious features are commonly defined as features that do not causally affect a task's label (Srivastava et al., 2020;Wang and Culotta, 2020b): they correlate with task labels but fail to transfer to more challenging test conditions or out-of-distribution data (Geirhos et al., 2020).
6. Some other work defined it as ""prediction rules that work for the majority examples but do not hold in general"" (Tu et al., 2020).
7. Such spurious correlations are sometimes referred as dataset bias (Clark et al., 2019;He et al., 2019), annotation artifacts (Gururangan et al., 2018), or group shift (Oren et al., 2019) in the literature.
8. Further, evidence showed that controlling model's learning in spurious features will improve model's performances in distribution shifts (Wang et al., 2019a,b); also, discussions on the connections between adversarial robustness and learning of spurious features have been raised (Ilyas et al., 2019;. Theoretical discussions connecting these fields have also been offered by crediting a reason of model's lack of robustness in either distribution shift or adversarial attack to model's learning of spurious features (Wang et al., 2021c).
9. Further, in certain applications, model ""robustness"" can also be connected with models' instability (Milani Fard et al., 2016), or models having poorly-calibrated uncertainty estimation (Guo et al., 2017), where Bayesian methods (Graves, 2011;Blundell et al., 2015), dropout-based (Gal and Ghahramani, 2016;Kingma et al., 2015) and ensemble-based approaches (Lakshminarayanan et al., 2017) have been proposed to improve models' uncertainty estimation.
10. Recently, Ovadia et al. (2019) have shown models' uncertainty estimation can degrade significantly under distributional shift, and call for more work to ensure a model ""knows when it doesn't know"" by giving lower uncertainty estimates over out-of-distribution data.
11. This is another example where models can be less robust under distributional shifts, and again emphasizes the need of building more unified benchmarks to measure a model's performance (e.g., robust accuracy, calibration, stability) under distribution shifts, in addition to in-distribution accuracy."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s3,Robustness under Distribution Shift,['p3.0'],"[""Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-jussà, 2019).""]","Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-jussà, 2019).","(p3.0) Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-jussà, 2019).","[['b21', 'b33', 'b26', None, 'b83', 'b67']]","[['b21', 'b33', 'b26', None, 'b83', 'b67']]",6,"1. Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.
2. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.
3. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).
4. Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area.
5. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-jussà, 2019)."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s2,Robustness against Adversarial Attacks,"['p2.0', 'p2.1', 'p2.2']","[""In one line of research, D is constructed by perturbations around input x to form x (x typically being defined within some proximity of x). This topic has been widely explored in computer vision under the concept of adversarial robustness, which measures models' performances against carefully crafted noises generated deliberately to deceive the model to predict wrongly, pioneered by (Szegedy et al., 2013;Goodfellow et al., 2015), and later extended to NLP, such as (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022) and multilingual adversaries . The generation of adversarial examples primarily builds upon the observation that we can generate samples that are meaningful to humans (e.g., by perturbing the samples with changes that are imperceptible to humans) while altering the prediction of the models for this sample. In this regard, human's remarkable ability in understanding a large set of synonyms (Li et al., 2020) or interesting characteristics in ignoring the exact order of letters  are often opportunities to create adversarial examples. A related line of work such as data-poisoning (Wallace et al., 2021) and weight-poisoning (Kurita et al., 2020) exposes NLP models' vulnerability against attacks during the training process. One can refer to more comprehensive reviews and broader discussions on this topic in Zhang et al. (2020c) and Morris et al. (2020b). Assumptions around Label-preserving and Semantic-preserving Most existing work in vision makes a relatively simplified assumption that the gold label of x remains unchanged under a bounded perturbation over x, i.e., y = y, and a model's robust behaviour should be f (x ) = y (Szegedy et al., 2013;Goodfellow et al., 2015). A similar line of work in NLP follows the same label-preserving assumption with small text perturbations like token and character swapping (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019;Ebrahimi et al., 2018), paraphrasing (Iyyer et al., 2018;Gan and Ng, 2019), semantically equivalent adversarial rules (Ribeiro et al., 2018), and adding distractors (Jia and Liang, 2017). However, this label-preserving assumption might not always hold, e.g., Wang et al. (2021b) studied several existing text perturbation techniques and found that a significant portion of perturbed examples are not label-preserving (despite their label-preserving assumptions), or the resulting labels have a high disagreement among human raters (i.e., can even fool humans). Morris et al. (2020a) also call for more attention to the validity of perturbed examples for a more accurate robustness evaluation."", ""Another line of work aims to perturb the input x to x in small but meaningful ways that explicitly change the gold label, i.e., y = y, under which case the robust behaviour of a model should be f (x ) = y and f (x ) = y (Gardner et al., 2020;Kaushik et al., 2019;Schlegel et al., 2021). We believe these two lines of work are complementary to each other, and both should be explored in future research to measure models' robustness more comprehensively."", 'One alternative notion is whether the perturbation from x to x is ""semantic-preseving"" (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019) or ""semantic-modifying"" (Shi and Huang, 2020;Jia and Liang, 2017). Note this is slightly different from the above label-preserving assumptions, as it is defined over the perturbations on (x, x ) rather than making an assumption on (y, y ), e.g., semantic-modifying perturbations can be either label-preserving (Jia and Liang, 2017;Huang, 2020) or label-changing (Gardner et al., 2020;Kaushik et al., 2019).']","In one line of research, D is constructed by perturbations around input x to form x (x typically being defined within some proximity of x). This topic has been widely explored in computer vision under the concept of adversarial robustness, which measures models' performances against carefully crafted noises generated deliberately to deceive the model to predict wrongly, pioneered by (Szegedy et al., 2013;Goodfellow et al., 2015), and later extended to NLP, such as (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022) and multilingual adversaries . The generation of adversarial examples primarily builds upon the observation that we can generate samples that are meaningful to humans (e.g., by perturbing the samples with changes that are imperceptible to humans) while altering the prediction of the models for this sample. In this regard, human's remarkable ability in understanding a large set of synonyms (Li et al., 2020) or interesting characteristics in ignoring the exact order of letters  are often opportunities to create adversarial examples. A related line of work such as data-poisoning (Wallace et al., 2021) and weight-poisoning (Kurita et al., 2020) exposes NLP models' vulnerability against attacks during the training process. One can refer to more comprehensive reviews and broader discussions on this topic in Zhang et al. (2020c) and Morris et al. (2020b). Assumptions around Label-preserving and Semantic-preserving Most existing work in vision makes a relatively simplified assumption that the gold label of x remains unchanged under a bounded perturbation over x, i.e., y = y, and a model's robust behaviour should be f (x ) = y (Szegedy et al., 2013;Goodfellow et al., 2015). A similar line of work in NLP follows the same label-preserving assumption with small text perturbations like token and character swapping (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019;Ebrahimi et al., 2018), paraphrasing (Iyyer et al., 2018;Gan and Ng, 2019), semantically equivalent adversarial rules (Ribeiro et al., 2018), and adding distractors (Jia and Liang, 2017). However, this label-preserving assumption might not always hold, e.g., Wang et al. (2021b) studied several existing text perturbation techniques and found that a significant portion of perturbed examples are not label-preserving (despite their label-preserving assumptions), or the resulting labels have a high disagreement among human raters (i.e., can even fool humans). Morris et al. (2020a) also call for more attention to the validity of perturbed examples for a more accurate robustness evaluation.

Another line of work aims to perturb the input x to x in small but meaningful ways that explicitly change the gold label, i.e., y = y, under which case the robust behaviour of a model should be f (x ) = y and f (x ) = y (Gardner et al., 2020;Kaushik et al., 2019;Schlegel et al., 2021). We believe these two lines of work are complementary to each other, and both should be explored in future research to measure models' robustness more comprehensively.

One alternative notion is whether the perturbation from x to x is ""semantic-preseving"" (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019) or ""semantic-modifying"" (Shi and Huang, 2020;Jia and Liang, 2017). Note this is slightly different from the above label-preserving assumptions, as it is defined over the perturbations on (x, x ) rather than making an assumption on (y, y ), e.g., semantic-modifying perturbations can be either label-preserving (Jia and Liang, 2017;Huang, 2020) or label-changing (Gardner et al., 2020;Kaushik et al., 2019).","(p2.0) In one line of research, D is constructed by perturbations around input x to form x (x typically being defined within some proximity of x). This topic has been widely explored in computer vision under the concept of adversarial robustness, which measures models' performances against carefully crafted noises generated deliberately to deceive the model to predict wrongly, pioneered by (Szegedy et al., 2013;Goodfellow et al., 2015), and later extended to NLP, such as (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022) and multilingual adversaries . The generation of adversarial examples primarily builds upon the observation that we can generate samples that are meaningful to humans (e.g., by perturbing the samples with changes that are imperceptible to humans) while altering the prediction of the models for this sample. In this regard, human's remarkable ability in understanding a large set of synonyms (Li et al., 2020) or interesting characteristics in ignoring the exact order of letters  are often opportunities to create adversarial examples. A related line of work such as data-poisoning (Wallace et al., 2021) and weight-poisoning (Kurita et al., 2020) exposes NLP models' vulnerability against attacks during the training process. One can refer to more comprehensive reviews and broader discussions on this topic in Zhang et al. (2020c) and Morris et al. (2020b). Assumptions around Label-preserving and Semantic-preserving Most existing work in vision makes a relatively simplified assumption that the gold label of x remains unchanged under a bounded perturbation over x, i.e., y = y, and a model's robust behaviour should be f (x ) = y (Szegedy et al., 2013;Goodfellow et al., 2015). A similar line of work in NLP follows the same label-preserving assumption with small text perturbations like token and character swapping (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019;Ebrahimi et al., 2018), paraphrasing (Iyyer et al., 2018;Gan and Ng, 2019), semantically equivalent adversarial rules (Ribeiro et al., 2018), and adding distractors (Jia and Liang, 2017). However, this label-preserving assumption might not always hold, e.g., Wang et al. (2021b) studied several existing text perturbation techniques and found that a significant portion of perturbed examples are not label-preserving (despite their label-preserving assumptions), or the resulting labels have a high disagreement among human raters (i.e., can even fool humans). Morris et al. (2020a) also call for more attention to the validity of perturbed examples for a more accurate robustness evaluation.

(p2.1) Another line of work aims to perturb the input x to x in small but meaningful ways that explicitly change the gold label, i.e., y = y, under which case the robust behaviour of a model should be f (x ) = y and f (x ) = y (Gardner et al., 2020;Kaushik et al., 2019;Schlegel et al., 2021). We believe these two lines of work are complementary to each other, and both should be explored in future research to measure models' robustness more comprehensively.

(p2.2) One alternative notion is whether the perturbation from x to x is ""semantic-preseving"" (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019) or ""semantic-modifying"" (Shi and Huang, 2020;Jia and Liang, 2017). Note this is slightly different from the above label-preserving assumptions, as it is defined over the perturbations on (x, x ) rather than making an assumption on (y, y ), e.g., semantic-modifying perturbations can be either label-preserving (Jia and Liang, 2017;Huang, 2020) or label-changing (Gardner et al., 2020;Kaushik et al., 2019).","[['b50', 'b63', None, 'b7', 'b6', 'b52', 'b24', 'b23', 'b74', 'b80'], [None, 'b1'], [None, 'b1']]","[['b50', 'b63', None, 'b7', 'b6', 'b52', 'b24', 'b23', 'b74', 'b80'], [None, 'b1'], [None, 'b1']]",14,"1. In one line of research, D is constructed by perturbations around input x to form x (x typically being defined within some proximity of x).
2. This topic has been widely explored in computer vision under the concept of adversarial robustness, which measures models' performances against carefully crafted noises generated deliberately to deceive the model to predict wrongly, pioneered by (Szegedy et al., 2013;Goodfellow et al., 2015), and later extended to NLP, such as (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022) and multilingual adversaries .
3. The generation of adversarial examples primarily builds upon the observation that we can generate samples that are meaningful to humans (e.g., by perturbing the samples with changes that are imperceptible to humans) while altering the prediction of the models for this sample.
4. In this regard, human's remarkable ability in understanding a large set of synonyms (Li et al., 2020) or interesting characteristics in ignoring the exact order of letters  are often opportunities to create adversarial examples.
5. A related line of work such as data-poisoning (Wallace et al., 2021) and weight-poisoning (Kurita et al., 2020) exposes NLP models' vulnerability against attacks during the training process.
6. One can refer to more comprehensive reviews and broader discussions on this topic in Zhang et al. (2020c) and Morris et al. (2020b).
7. Assumptions around Label-preserving and Semantic-preserving Most existing work in vision makes a relatively simplified assumption that the gold label of x remains unchanged under a bounded perturbation over x, i.e., y = y, and a model's robust behaviour should be f (x ) = y (Szegedy et al., 2013;Goodfellow et al., 2015).
8. A similar line of work in NLP follows the same label-preserving assumption with small text perturbations like token and character swapping (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019;Ebrahimi et al., 2018), paraphrasing (Iyyer et al., 2018;Gan and Ng, 2019), semantically equivalent adversarial rules (Ribeiro et al., 2018), and adding distractors (Jia and Liang, 2017).
9. However, this label-preserving assumption might not always hold, e.g., Wang et al. (2021b) studied several existing text perturbation techniques and found that a significant portion of perturbed examples are not label-preserving (despite their label-preserving assumptions), or the resulting labels have a high disagreement among human raters (i.e., can even fool humans).
10. Morris et al. (2020a) also call for more attention to the validity of perturbed examples for a more accurate robustness evaluation.
11. Another line of work aims to perturb the input x to x in small but meaningful ways that explicitly change the gold label, i.e., y = y, under which case the robust behaviour of a model should be f (x ) = y and f (x ) = y (Gardner et al., 2020;Kaushik et al., 2019;Schlegel et al., 2021).
12. We believe these two lines of work are complementary to each other, and both should be explored in future research to measure models' robustness more comprehensively.
13. One alternative notion is whether the perturbation from x to x is ""semantic-preseving"" (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019) or ""semantic-modifying"" (Shi and Huang, 2020;Jia and Liang, 2017).
14. Note this is slightly different from the above label-preserving assumptions, as it is defined over the perturbations on (x, x ) rather than making an assumption on (y, y ), e.g., semantic-modifying perturbations can be either label-preserving (Jia and Liang, 2017;Huang, 2020) or label-changing (Gardner et al., 2020;Kaushik et al., 2019)."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s1,Definitions of Robustness in NLP,"['p1.0', 'p1.1']","[""Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2)."", 'The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.']","Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).

The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.","(p1.0) Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).

(p1.1) The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.","[['b40'], [None, 'b18', 'b5']]","[['b40'], [None, 'b18', 'b5']]",4,"1. Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ].
2. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).
3. The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels.
4. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x )
5. and y when both are open-ended texts."
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,s2,Method Decision based on,"['p2.0', 'p2.1', 'p2.2', 'p2.3', 'p2.4', 'p2.5', 'p2.6']","['Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc. Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)', 'PoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; ""flush"" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information. LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed. Structural information that naturally exists in sentences can also play a role in skimming. Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).', 'In the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers. The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores. The number of tokens to be dropped, referred to as the ""schedule,"" is optimized by combining the sparsity of a soft mask layer with the original loss function. This results in an improved balance between accuracy and processing time. TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained. In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them. The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning. The schedule for LAT is found through an evolutionary search algorithm. LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule. It simply drops tokens with attention scores lower than the learned threshold. Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer. This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.', 'Computation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely. VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step. Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.', 'Instead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN. At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token. If the model decides to skim, the small RNN will update only a fraction of the hid-den states. Otherwise, a regular full update will be conducted by the big RNN.', 'Dynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed. HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture. In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.', 'In question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed. Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering. It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.']","Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc. Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)

PoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; ""flush"" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information. LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed. Structural information that naturally exists in sentences can also play a role in skimming. Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).

In the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers. The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores. The number of tokens to be dropped, referred to as the ""schedule,"" is optimized by combining the sparsity of a soft mask layer with the original loss function. This results in an improved balance between accuracy and processing time. TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained. In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them. The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning. The schedule for LAT is found through an evolutionary search algorithm. LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule. It simply drops tokens with attention scores lower than the learned threshold. Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer. This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.

Computation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely. VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step. Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.

Instead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN. At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token. If the model decides to skim, the small RNN will update only a fraction of the hid-den states. Otherwise, a regular full update will be conducted by the big RNN.

Dynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed. HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture. In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.

In question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed. Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering. It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.","(p2.0) Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc. Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)

(p2.1) PoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; ""flush"" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information. LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed. Structural information that naturally exists in sentences can also play a role in skimming. Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).

(p2.2) In the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers. The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores. The number of tokens to be dropped, referred to as the ""schedule,"" is optimized by combining the sparsity of a soft mask layer with the original loss function. This results in an improved balance between accuracy and processing time. TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained. In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them. The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning. The schedule for LAT is found through an evolutionary search algorithm. LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule. It simply drops tokens with attention scores lower than the learned threshold. Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer. This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.

(p2.3) Computation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely. VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step. Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.

(p2.4) Instead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN. At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token. If the model decides to skim, the small RNN will update only a fraction of the hid-den states. Otherwise, a regular full update will be conducted by the big RNN.

(p2.5) Dynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed. HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture. In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.

(p2.6) In question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed. Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering. It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.","[['b3', 'b65', 'b30', 'b50', 'b15', 'b64', 'b10'], ['b15', 'b4', 'b14', 'b63', 'b47', 'b10', 'b19', 'b24', 'b13', 'b23'], ['b63', 'b13', 'b14', 'b24'], ['b19'], ['b47'], ['b3', 'b4'], ['b23']]","[['b3', 'b65', 'b30', 'b50', 'b15', 'b64', 'b10'], ['b15', 'b4', 'b14', 'b63', 'b47', 'b10', 'b19', 'b24', 'b13', 'b23'], ['b63', 'b13', 'b14', 'b24'], ['b19'], ['b47'], ['b3', 'b4'], ['b23']]",26,"1. Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc.
2. Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)PoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens
3. Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; ""flush"" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information.
4. LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed.
5. Structural information that naturally exists in sentences can also play a role in skimming.
6. Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).
7. In the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers.
8. The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores.
9. The number of tokens to be dropped, referred to as the ""schedule,"" is optimized by combining the sparsity of a soft mask layer with the original loss function.
10. This results in an improved balance between accuracy and processing time.
11. TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained.
12. In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them.
13. The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning.
14. The schedule for LAT is found through an evolutionary search algorithm.
15. LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule.
16. It simply drops tokens with attention scores lower than the learned threshold.
17. Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer.
18. This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.
19. Computation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely.
20. VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step.
21. Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.
22. Instead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN.
23. At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token.
24. If the model decides to skim, the small RNN will update only a fraction of the hid-den states.
25. Otherwise, a regular full update will be conducted by the big RNN.
26. Dynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed.
27. HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture.
28. In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.
29. In question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed.
30. Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering.
31. It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding."
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,s1,Skimming,"['p1.0', 'p1.1', 'p1.2', 'p1.3']","['Skimming techniques, as summarized in Table 1, skip some time steps or allocate different computation on them. Intuitively, skimming matches how human beings efficiently read text and extract information from it (Li et al., 2019). By em-phasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies. The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN, corresponding with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only.', 'Skipping and Early Stopping Skipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early. LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU). At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading. LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions. The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label. Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up. Skip RNN (Campos et al., 2018) introduces a binary gate to learn whether to skip a state update. If the gate decides to skip a time step, the hidden states will be directly copied without any update.', 'To stop reading early as needed, Rea-soNet (Shen et al., 2017) introduces a terminal state which decides whether to terminate early for machine reading comprehension on each time step at the token level. Jumper  first splits a paragraph to several sub-sentences and encodes them into sentence embeddings. They then apply early stopping at a sentence when the policy network decides to stop reading. Li et al. (2019) use eye-tracking devices and confirm that skipping and early stopping are common when humans read text. They propose Reading Inspired Model to mimic the behaviors of humans, which allows the model to decide whether to skip a single time step or stop reading early. Yu et al. (2018) add a rereading operation to LSTM-Jump (Yu et al., 2017) which allows the model to stay on the current time step, allocating more computation to important information.', 'The aforementioned techniques can only go forward, which makes it impossible to regret if hav-']","Skimming techniques, as summarized in Table 1, skip some time steps or allocate different computation on them. Intuitively, skimming matches how human beings efficiently read text and extract information from it (Li et al., 2019). By em-phasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies. The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN, corresponding with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only.

Skipping and Early Stopping Skipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early. LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU). At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading. LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions. The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label. Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up. Skip RNN (Campos et al., 2018) introduces a binary gate to learn whether to skip a state update. If the gate decides to skip a time step, the hidden states will be directly copied without any update.

To stop reading early as needed, Rea-soNet (Shen et al., 2017) introduces a terminal state which decides whether to terminate early for machine reading comprehension on each time step at the token level. Jumper  first splits a paragraph to several sub-sentences and encodes them into sentence embeddings. They then apply early stopping at a sentence when the policy network decides to stop reading. Li et al. (2019) use eye-tracking devices and confirm that skipping and early stopping are common when humans read text. They propose Reading Inspired Model to mimic the behaviors of humans, which allows the model to decide whether to skip a single time step or stop reading early. Yu et al. (2018) add a rereading operation to LSTM-Jump (Yu et al., 2017) which allows the model to stay on the current time step, allocating more computation to important information.

The aforementioned techniques can only go forward, which makes it impossible to regret if hav-","(p1.0) Skimming techniques, as summarized in Table 1, skip some time steps or allocate different computation on them. Intuitively, skimming matches how human beings efficiently read text and extract information from it (Li et al., 2019). By em-phasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies. The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN, corresponding with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only.

(p1.1) Skipping and Early Stopping Skipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early. LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU). At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading. LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions. The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label. Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up. Skip RNN (Campos et al., 2018) introduces a binary gate to learn whether to skip a state update. If the gate decides to skip a time step, the hidden states will be directly copied without any update.

(p1.2) To stop reading early as needed, Rea-soNet (Shen et al., 2017) introduces a terminal state which decides whether to terminate early for machine reading comprehension on each time step at the token level. Jumper  first splits a paragraph to several sub-sentences and encodes them into sentence embeddings. They then apply early stopping at a sentence when the policy network decides to stop reading. Li et al. (2019) use eye-tracking devices and confirm that skipping and early stopping are common when humans read text. They propose Reading Inspired Model to mimic the behaviors of humans, which allows the model to decide whether to skip a single time step or stop reading early. Yu et al. (2018) add a rereading operation to LSTM-Jump (Yu et al., 2017) which allows the model to stay on the current time step, allocating more computation to important information.

(p1.3) The aforementioned techniques can only go forward, which makes it impossible to regret if hav-","[['b30'], ['b3', 'b64'], ['b64', 'b65', 'b30', 'b50'], []]","[['b30'], ['b3', 'b64'], ['b64', 'b65', 'b30', 'b50'], []]",7,"1. Skimming techniques, as summarized in Table 1, skip some time steps or allocate different computation on them.
2. Intuitively, skimming matches how human beings efficiently read text and extract information from it (Li et al., 2019).
3. By em-phasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies.
4. The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN, corresponding with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only.
5. Skipping and Early Stopping Skipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early.
6. LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU).
7. At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading.
8. LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions.
9. The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label.
10. Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up.
11. Skip RNN (Campos et al., 2018) introduces a binary gate to learn whether to skip a state update.
12. If the gate decides to skip a time step, the hidden states will be directly copied without any update.
13. To stop reading early as needed, Rea-soNet (Shen et al., 2017) introduces a terminal state which decides whether to terminate early for machine reading comprehension on each time step at the token level.
14. Jumper  first splits a paragraph to several sub-sentences and encodes them into sentence embeddings.
15. They then apply early stopping at a sentence when the policy network decides to stop reading.
16. Li et al. (2019) use eye-tracking devices and confirm that skipping and early stopping are common when humans read text.
17. They propose Reading Inspired Model to mimic the behaviors of humans, which allows the model to decide whether to skip a single time step or stop reading early.
18. Yu et al. (2018) add a rereading operation to LSTM-Jump (Yu et al., 2017) which allows the model to stay on the current time step, allocating more computation to important information.
19. The aforementioned techniques can only go forward, which makes it impossible to regret if hav-"
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,s9,Internal classifier training Exit criterion,"['p9.0', 'p9.1', 'p9.2', 'p9.3', 'p9.4', 'p9.5', 'p9.6', 'p9.7', 'p9.8']","['DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy < θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ ) Voting  joint; sum of CE + diversity loss accumulated votes > θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ ) Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)', 'BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θ', 'CascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.', 'Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.  further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair. They optimize these weights by a cross-level optimization algorithm.', 'They adopt PABEE\'s patience-based strategy for exiting. Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners. Entropy is used as the exit criterion. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods. The inference is terminated when multiple layers are confident.', 'Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.', 'BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit. CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.', 'Cascading Cascading can be seen as a special form of early exit, performed at the model level. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading. CascadeBERT executes models one by one, from the smallest to the largest. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.', 'Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling. They use the maximum class probability as confidence on a per-token basis. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens. The model completely exits when every token exits. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.']","DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy < θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ ) Voting  joint; sum of CE + diversity loss accumulated votes > θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ ) Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)

BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θ

CascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.

Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.  further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair. They optimize these weights by a cross-level optimization algorithm.

They adopt PABEE's patience-based strategy for exiting. Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners. Entropy is used as the exit criterion. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods. The inference is terminated when multiple layers are confident.

Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit. CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.

Cascading Cascading can be seen as a special form of early exit, performed at the model level. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading. CascadeBERT executes models one by one, from the smallest to the largest. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.

Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling. They use the maximum class probability as confidence on a per-token basis. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens. The model completely exits when every token exits. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","(p9.0) DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy < θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ ) Voting  joint; sum of CE + diversity loss accumulated votes > θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ ) Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)

(p9.1) BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θ

(p9.2) CascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.

(p9.3) Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.  further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair. They optimize these weights by a cross-level optimization algorithm.

(p9.4) They adopt PABEE's patience-based strategy for exiting. Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners. Entropy is used as the exit criterion. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods. The inference is terminated when multiple layers are confident.

(p9.5) Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.

(p9.6) BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit. CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.

(p9.7) Cascading Cascading can be seen as a special form of early exit, performed at the model level. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading. CascadeBERT executes models one by one, from the smallest to the largest. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.

(p9.8) Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling. They use the maximum class probability as confidence on a per-token basis. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens. The model completely exits when every token exits. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","[['b56', 'b72', None, 'b11', 'b70'], ['b45', 'b57'], ['b29'], ['b72', 'b70'], ['b67', None], [], ['b45', 'b57'], ['b29', 'b56'], ['b55', 'b37', 'b56', 'b8', None]]","[['b56', 'b72', None, 'b11', 'b70'], ['b45', 'b57'], ['b29'], ['b72', 'b70'], ['b67', None], [], ['b45', 'b57'], ['b29', 'b56'], ['b55', 'b37', 'b56', 'b8', None]]",21,"1. DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy <
2. θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ )
3. Voting  joint; sum of CE + diversity loss accumulated votes >
4. θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ )
5. Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θCascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.
6. Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation.
7. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded.
8. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions.
9. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training.
10. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers.
11. For inference, the model exits when k consecutive internal classifiers make the same prediction.
12. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.
13. further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution.
14. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold.
15. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other.
16. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair.
17. They optimize these weights by a cross-level optimization algorithm.
18. They adopt PABEE's patience-based strategy for exiting.
19. Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states.
20. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners.
21. Entropy is used as the exit criterion.
22. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods.
23. The inference is terminated when multiple layers are confident.
24. Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.
25. BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations.
26. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction.
27. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit.
28. CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.
29. Cascading Cascading can be seen as a special form of early exit, performed at the model level.
30. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers.
31. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading.
32. CascadeBERT executes models one by one, from the smallest to the largest.
33. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.
34. Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings.
35. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling.
36. They use the maximum class probability as confidence on a per-token basis.
37. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers.
38. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens.
39. The model completely exits when every token exits.
40. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification.
41. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking.
42. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane.
43. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning."
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,s4,MoE Layers with Learned Routing,"['p4.0', 'p4.1', 'p4.2', 'p4.3', 'p4.4', 'p4.5', 'p4.6', 'p4.7', 'p4.8']","['A straightforward idea to implement MoE is to learn a router to allocate inputs to experts. Sparsely-Gated MoE layer  contains up to thousands of feed-forward sub-networks with a trainable gating network which determines a sparse combination of these experts to use for each example. There are two major challenges to address: (1) Sparsity. The gating network predicts a softmax weight for the experts based on the input. The gating network is trained by simple back-propagation, together with other parts of the model. Then, only the top-k experts in the layer will be activated based on the softmax prediction of the gating network. They insert one MoE layer between stacked LSTM layers and achieve improvement on language modeling and machine translation tasks.', '(2) Load balancing.  observe a self-reinforcing phenomenon that the gating network tends to converge to a state where it always produces large weights for the same few experts. They resolve the problem by defining the importance of an expert relative to a batch of training examples to be batch-wise sum of the gate values for that expert. Then, they introduce an additional loss, the square of the coefficient of variation of the set of importance values, to encourage a more balanced update during training. Besides encouraging a balanced update, the authors also introduce a loss function with a smooth estimator that estimate the number of examples assigned to each expert for a batch of inputs, to encourage experts to receive roughly equal numbers of training examples.', 'GShard  enables scaling up multilingual neural machine translation Transformer beyond 600 billion parameters. It adapts Sparsely-Gated MoE  to Transformer (Vaswani et al., 2017) by replacing every other feed forward layer with an MoE layer, which routes to top-2 experts. When scaling to multiple devices, the MoE layer is sharded across devices, i.e., each device has different allocated experts, while all other layers are replicated. To achieve workload balance, GShard employs a threshold, namely expert capacity, to limit the maximum number of tokens processed by one single expert. They also introduce a local group dispatching mechanism, which partitions all tokens in a training batch evenly into groups to be processed independently in parallel, to balance the overall workload. Following , they use an additional loss to enforce even top-2 expert capacity; local group dispatching; auxiliary loss; random routing Switch (Fedus et al., 2021) Transformer (T5) top-1 expert capacity; auxiliary loss BASE (Lewis et al., 2021) Transformer (GPT) top-1 linear assignment M6-T  Transformer (M6) k top-1 expert capacity DTS (Nie et al., 2021) Transformer (GPT) dynamic sparsity scheduler', 'Hash (Roller et al., 2021) Transformer hash deterministic hash THOR (Zuo et al., 2022) Transformer (NMT) random random selection allocation for experts. Additionally, they propose a random routing mechanism, which only routes to the second-best expert with probability proportional to its weight, to simplify sparse training. Switch Transformer (Fedus et al., 2021) aims to simplify the Sparsely-Gated MoE  for efficiency and performance. They propose a Switch Layer which only routes to one expert at a time, to reduce gating computation, batch size and communication costs. Switch Transformer inherits expert capacity and an auxiliary load balancing loss from GShard . Combined with low-precision training, compared to T5-Base and T5-Large (Raffel et al., 2020), Switch Transformer obtains up to 7× increases in pretraining speed with the same computational resources. They further scale Switch Transformer to more than 1.5 trillion parameters and achieve 4× speed-up over T5-XXL.', 'The Balanced Assignment of Sparse Experts (BASE) layer (Lewis et al., 2021) formulates token-to-expert allocation as a linear assignment problem and solves it with the auction algorithm (Bertsekas, 1992). This allows an optimal assignment in which each expert receives an equal number of tokens, improving efficiency and getting rid of the expert capacity and auxiliary loss in previous works. The experiments show that BASE layers are more efficient for training compared to Sparsely-Gated MoE layers  and Switch Layers (Fedus et al., 2021), and can successfully learn a good balanced routing without any auxiliary balancing loss.', 'M6 ) is a multi-modal multitask Transformer, trained in the same way as Switch Transformer (Fedus et al., 2021), scaling up to 100B parameters. Following this, M6-T  splits experts into k prototypes (i.e., groups of experts). In each forward pass, each token is sent to the k prototypes, within which the top-1 routing is done lo-cally. The experiments demonstrate this ""k top-1"" strategy outperforms the top-1 routing in Switch Transformer (Fedus et al., 2021) while being more computation-efficient than ""top-k"" routing. They also claim that the load balancing loss may be ineffective for improving the performance of an MoE model, although it can indeed help balance the workload. They subsequently train a 1 trillion parameter model with the finding.', 'Dense-to-Sparse gate (Nie et al., 2021) begins as a dense gate that routes tokens to all experts then gradually learns to become sparser and route tokens to fewer experts, demonstrating higher training efficiency in experiments. Their experiments confirm the finding in  that an auxiliary load balancing loss does not improve the model performance.', 'MoE Layer with Unlearnable Routing Although learning-based routing has shown effectiveness only with the help of complicated load balancing mechanisms, recent studies have attempted to get rid of those. Hash Layer (Roller et al., 2021) simplifies routing by using a parameter-free hashing function to route tokens to specific experts. This design eliminates the need for a load balancing loss and sophisticated assignment algorithms. They also study the performance of different hashing techniques, hash sizes and input features, and conclude that balanced and random hashes focused on the most local features work best. The experiments show that a Hash Layer achieves comparable performance with a Switch Layer (Fedus et al., 2021) and BASE Layer (Lewis et al., 2021).', 'THOR (Zuo et al., 2022) is a special form of MoE layer, which completely discards the conditional routing mechanism and instead optimizes the consistency between a randomly selected pair of experts. During inference, one expert will be randomly selected to be activated.']","A straightforward idea to implement MoE is to learn a router to allocate inputs to experts. Sparsely-Gated MoE layer  contains up to thousands of feed-forward sub-networks with a trainable gating network which determines a sparse combination of these experts to use for each example. There are two major challenges to address: (1) Sparsity. The gating network predicts a softmax weight for the experts based on the input. The gating network is trained by simple back-propagation, together with other parts of the model. Then, only the top-k experts in the layer will be activated based on the softmax prediction of the gating network. They insert one MoE layer between stacked LSTM layers and achieve improvement on language modeling and machine translation tasks.

(2) Load balancing.  observe a self-reinforcing phenomenon that the gating network tends to converge to a state where it always produces large weights for the same few experts. They resolve the problem by defining the importance of an expert relative to a batch of training examples to be batch-wise sum of the gate values for that expert. Then, they introduce an additional loss, the square of the coefficient of variation of the set of importance values, to encourage a more balanced update during training. Besides encouraging a balanced update, the authors also introduce a loss function with a smooth estimator that estimate the number of examples assigned to each expert for a batch of inputs, to encourage experts to receive roughly equal numbers of training examples.

GShard  enables scaling up multilingual neural machine translation Transformer beyond 600 billion parameters. It adapts Sparsely-Gated MoE  to Transformer (Vaswani et al., 2017) by replacing every other feed forward layer with an MoE layer, which routes to top-2 experts. When scaling to multiple devices, the MoE layer is sharded across devices, i.e., each device has different allocated experts, while all other layers are replicated. To achieve workload balance, GShard employs a threshold, namely expert capacity, to limit the maximum number of tokens processed by one single expert. They also introduce a local group dispatching mechanism, which partitions all tokens in a training batch evenly into groups to be processed independently in parallel, to balance the overall workload. Following , they use an additional loss to enforce even top-2 expert capacity; local group dispatching; auxiliary loss; random routing Switch (Fedus et al., 2021) Transformer (T5) top-1 expert capacity; auxiliary loss BASE (Lewis et al., 2021) Transformer (GPT) top-1 linear assignment M6-T  Transformer (M6) k top-1 expert capacity DTS (Nie et al., 2021) Transformer (GPT) dynamic sparsity scheduler

Hash (Roller et al., 2021) Transformer hash deterministic hash THOR (Zuo et al., 2022) Transformer (NMT) random random selection allocation for experts. Additionally, they propose a random routing mechanism, which only routes to the second-best expert with probability proportional to its weight, to simplify sparse training. Switch Transformer (Fedus et al., 2021) aims to simplify the Sparsely-Gated MoE  for efficiency and performance. They propose a Switch Layer which only routes to one expert at a time, to reduce gating computation, batch size and communication costs. Switch Transformer inherits expert capacity and an auxiliary load balancing loss from GShard . Combined with low-precision training, compared to T5-Base and T5-Large (Raffel et al., 2020), Switch Transformer obtains up to 7× increases in pretraining speed with the same computational resources. They further scale Switch Transformer to more than 1.5 trillion parameters and achieve 4× speed-up over T5-XXL.

The Balanced Assignment of Sparse Experts (BASE) layer (Lewis et al., 2021) formulates token-to-expert allocation as a linear assignment problem and solves it with the auction algorithm (Bertsekas, 1992). This allows an optimal assignment in which each expert receives an equal number of tokens, improving efficiency and getting rid of the expert capacity and auxiliary loss in previous works. The experiments show that BASE layers are more efficient for training compared to Sparsely-Gated MoE layers  and Switch Layers (Fedus et al., 2021), and can successfully learn a good balanced routing without any auxiliary balancing loss.

M6 ) is a multi-modal multitask Transformer, trained in the same way as Switch Transformer (Fedus et al., 2021), scaling up to 100B parameters. Following this, M6-T  splits experts into k prototypes (i.e., groups of experts). In each forward pass, each token is sent to the k prototypes, within which the top-1 routing is done lo-cally. The experiments demonstrate this ""k top-1"" strategy outperforms the top-1 routing in Switch Transformer (Fedus et al., 2021) while being more computation-efficient than ""top-k"" routing. They also claim that the load balancing loss may be ineffective for improving the performance of an MoE model, although it can indeed help balance the workload. They subsequently train a 1 trillion parameter model with the finding.

Dense-to-Sparse gate (Nie et al., 2021) begins as a dense gate that routes tokens to all experts then gradually learns to become sparser and route tokens to fewer experts, demonstrating higher training efficiency in experiments. Their experiments confirm the finding in  that an auxiliary load balancing loss does not improve the model performance.

MoE Layer with Unlearnable Routing Although learning-based routing has shown effectiveness only with the help of complicated load balancing mechanisms, recent studies have attempted to get rid of those. Hash Layer (Roller et al., 2021) simplifies routing by using a parameter-free hashing function to route tokens to specific experts. This design eliminates the need for a load balancing loss and sophisticated assignment algorithms. They also study the performance of different hashing techniques, hash sizes and input features, and conclude that balanced and random hashes focused on the most local features work best. The experiments show that a Hash Layer achieves comparable performance with a Switch Layer (Fedus et al., 2021) and BASE Layer (Lewis et al., 2021).

THOR (Zuo et al., 2022) is a special form of MoE layer, which completely discards the conditional routing mechanism and instead optimizes the consistency between a randomly selected pair of experts. During inference, one expert will be randomly selected to be activated.","(p4.0) A straightforward idea to implement MoE is to learn a router to allocate inputs to experts. Sparsely-Gated MoE layer  contains up to thousands of feed-forward sub-networks with a trainable gating network which determines a sparse combination of these experts to use for each example. There are two major challenges to address: (1) Sparsity. The gating network predicts a softmax weight for the experts based on the input. The gating network is trained by simple back-propagation, together with other parts of the model. Then, only the top-k experts in the layer will be activated based on the softmax prediction of the gating network. They insert one MoE layer between stacked LSTM layers and achieve improvement on language modeling and machine translation tasks.

(p4.1) (2) Load balancing.  observe a self-reinforcing phenomenon that the gating network tends to converge to a state where it always produces large weights for the same few experts. They resolve the problem by defining the importance of an expert relative to a batch of training examples to be batch-wise sum of the gate values for that expert. Then, they introduce an additional loss, the square of the coefficient of variation of the set of importance values, to encourage a more balanced update during training. Besides encouraging a balanced update, the authors also introduce a loss function with a smooth estimator that estimate the number of examples assigned to each expert for a batch of inputs, to encourage experts to receive roughly equal numbers of training examples.

(p4.2) GShard  enables scaling up multilingual neural machine translation Transformer beyond 600 billion parameters. It adapts Sparsely-Gated MoE  to Transformer (Vaswani et al., 2017) by replacing every other feed forward layer with an MoE layer, which routes to top-2 experts. When scaling to multiple devices, the MoE layer is sharded across devices, i.e., each device has different allocated experts, while all other layers are replicated. To achieve workload balance, GShard employs a threshold, namely expert capacity, to limit the maximum number of tokens processed by one single expert. They also introduce a local group dispatching mechanism, which partitions all tokens in a training batch evenly into groups to be processed independently in parallel, to balance the overall workload. Following , they use an additional loss to enforce even top-2 expert capacity; local group dispatching; auxiliary loss; random routing Switch (Fedus et al., 2021) Transformer (T5) top-1 expert capacity; auxiliary loss BASE (Lewis et al., 2021) Transformer (GPT) top-1 linear assignment M6-T  Transformer (M6) k top-1 expert capacity DTS (Nie et al., 2021) Transformer (GPT) dynamic sparsity scheduler

(p4.3) Hash (Roller et al., 2021) Transformer hash deterministic hash THOR (Zuo et al., 2022) Transformer (NMT) random random selection allocation for experts. Additionally, they propose a random routing mechanism, which only routes to the second-best expert with probability proportional to its weight, to simplify sparse training. Switch Transformer (Fedus et al., 2021) aims to simplify the Sparsely-Gated MoE  for efficiency and performance. They propose a Switch Layer which only routes to one expert at a time, to reduce gating computation, batch size and communication costs. Switch Transformer inherits expert capacity and an auxiliary load balancing loss from GShard . Combined with low-precision training, compared to T5-Base and T5-Large (Raffel et al., 2020), Switch Transformer obtains up to 7× increases in pretraining speed with the same computational resources. They further scale Switch Transformer to more than 1.5 trillion parameters and achieve 4× speed-up over T5-XXL.

(p4.4) The Balanced Assignment of Sparse Experts (BASE) layer (Lewis et al., 2021) formulates token-to-expert allocation as a linear assignment problem and solves it with the auction algorithm (Bertsekas, 1992). This allows an optimal assignment in which each expert receives an equal number of tokens, improving efficiency and getting rid of the expert capacity and auxiliary loss in previous works. The experiments show that BASE layers are more efficient for training compared to Sparsely-Gated MoE layers  and Switch Layers (Fedus et al., 2021), and can successfully learn a good balanced routing without any auxiliary balancing loss.

(p4.5) M6 ) is a multi-modal multitask Transformer, trained in the same way as Switch Transformer (Fedus et al., 2021), scaling up to 100B parameters. Following this, M6-T  splits experts into k prototypes (i.e., groups of experts). In each forward pass, each token is sent to the k prototypes, within which the top-1 routing is done lo-cally. The experiments demonstrate this ""k top-1"" strategy outperforms the top-1 routing in Switch Transformer (Fedus et al., 2021) while being more computation-efficient than ""top-k"" routing. They also claim that the load balancing loss may be ineffective for improving the performance of an MoE model, although it can indeed help balance the workload. They subsequently train a 1 trillion parameter model with the finding.

(p4.6) Dense-to-Sparse gate (Nie et al., 2021) begins as a dense gate that routes tokens to all experts then gradually learns to become sparser and route tokens to fewer experts, demonstrating higher training efficiency in experiments. Their experiments confirm the finding in  that an auxiliary load balancing loss does not improve the model performance.

(p4.7) MoE Layer with Unlearnable Routing Although learning-based routing has shown effectiveness only with the help of complicated load balancing mechanisms, recent studies have attempted to get rid of those. Hash Layer (Roller et al., 2021) simplifies routing by using a parameter-free hashing function to route tokens to specific experts. This design eliminates the need for a load balancing loss and sophisticated assignment algorithms. They also study the performance of different hashing techniques, hash sizes and input features, and conclude that balanced and random hashes focused on the most local features work best. The experiments show that a Hash Layer achieves comparable performance with a Switch Layer (Fedus et al., 2021) and BASE Layer (Lewis et al., 2021).

(p4.8) THOR (Zuo et al., 2022) is a special form of MoE layer, which completely discards the conditional routing mechanism and instead optimizes the consistency between a randomly selected pair of experts. During inference, one expert will be randomly selected to be activated.","[[], [], ['b53', 'b28', 'b9', None], ['b73', 'b41', 'b9', 'b43'], ['b28', 'b9', 'b1'], ['b9'], [None], ['b28', 'b9', 'b43'], ['b73']]","[[], [], ['b53', 'b28', 'b9', None], ['b73', 'b41', 'b9', 'b43'], ['b28', 'b9', 'b1'], ['b9'], [None], ['b28', 'b9', 'b43'], ['b73']]",17,"1. A straightforward idea to implement MoE is to learn a router to allocate inputs to experts.
2. Sparsely-Gated MoE layer  contains up to thousands of feed-forward sub-networks with a trainable gating network which determines a sparse combination of these experts to use for each example.
3. There are two major challenges to address: (1) Sparsity.
4. The gating network predicts a softmax weight for the experts based on the input.
5. The gating network is trained by simple back-propagation, together with other parts of the model.
6. Then, only the top-k experts in the layer will be activated based on the softmax prediction of the gating network.
7. They insert one MoE layer between stacked LSTM layers and achieve improvement on language modeling and machine translation tasks.
8. (2) Load balancing. observe a self-reinforcing phenomenon that the gating network tends to converge to a state where it always produces large weights for the same few experts.
9. They resolve the problem by defining the importance of an expert relative to a batch of training examples to be batch-wise sum of the gate values for that expert.
10. Then, they introduce an additional loss, the square of the coefficient of variation of the set of importance values, to encourage a more balanced update during training.
11. Besides encouraging a balanced update, the authors also introduce a loss function with a smooth estimator that estimate the number of examples assigned to each expert for a batch of inputs, to encourage experts to receive roughly equal numbers of training examples.
12. GShard  enables scaling up multilingual neural machine translation Transformer beyond 600 billion parameters.
13. It adapts Sparsely-Gated MoE  to Transformer (Vaswani et al., 2017) by replacing every other feed forward layer with an MoE layer, which routes to top-2 experts.
14. When scaling to multiple devices, the MoE layer is sharded across devices, i.e., each device has different allocated experts, while all other layers are replicated.
15. To achieve workload balance, GShard employs a threshold, namely expert capacity, to limit the maximum number of tokens processed by one single expert.
16. They also introduce a local group dispatching mechanism, which partitions all tokens in a training batch evenly into groups to be processed independently in parallel, to balance the overall workload.
17. Following , they use an additional loss to enforce even top-2 expert capacity; local group dispatching; auxiliary loss; random routing Switch (Fedus et al., 2021) Transformer (T5) top-1 expert capacity; auxiliary loss BASE (Lewis et al., 2021) Transformer (GPT) top-1 linear assignment M6-T  Transformer (M6) k top-1 expert capacity DTS (Nie et al., 2021) Transformer (GPT) dynamic sparsity schedulerHash (Roller et al., 2021) Transformer hash deterministic hash THOR (Zuo et al., 2022) Transformer (NMT) random random selection allocation for experts.
18. Additionally, they propose a random routing mechanism, which only routes to the second-best expert with probability proportional to its weight, to simplify sparse training.
19. Switch Transformer (Fedus et al., 2021) aims to simplify the Sparsely-Gated MoE  for efficiency and performance.
20. They propose a Switch Layer which only routes to one expert at a time, to reduce gating computation, batch size and communication costs.
21. Switch Transformer inherits expert capacity and an auxiliary load balancing loss from GShard .
22. Combined with low-precision training, compared to T5-Base and T5-Large (Raffel et al., 2020), Switch Transformer obtains up to 7× increases in pretraining speed with the same computational resources.
23. They further scale Switch Transformer to more than 1.5 trillion parameters and achieve 4× speed-up over T5-XXL.
24. The Balanced Assignment of Sparse Experts (BASE) layer (Lewis et al., 2021) formulates token-to-expert allocation as a linear assignment problem and solves it with the auction algorithm (Bertsekas, 1992).
25. This allows an optimal assignment in which each expert receives an equal number of tokens, improving efficiency and getting rid of the expert capacity and auxiliary loss in previous works.
26. The experiments show that BASE layers are more efficient for training compared to Sparsely-Gated MoE layers  and Switch Layers (Fedus et al., 2021), and can successfully learn a good balanced routing without any auxiliary balancing loss.M6 ) is a multi-modal multitask Transformer, trained in the same way as Switch Transformer (Fedus et al., 2021), scaling up to 100B parameters.
27. Following this, M6-T  splits experts into k prototypes (i.e., groups of experts).
28. In each forward pass, each token is sent to the k prototypes, within which the top-1 routing is done lo-cally.
29. The experiments demonstrate this ""k top-1"" strategy outperforms the top-1 routing in Switch Transformer (Fedus et al., 2021) while being more computation-efficient than ""top-k"" routing.
30. They also claim that the load balancing loss may be ineffective for improving the performance of an MoE model, although it can indeed help balance the workload.
31. They subsequently train a 1 trillion parameter model with the finding.
32. Dense-to-Sparse gate (Nie et al., 2021) begins as a dense gate that routes tokens to all experts then gradually learns to become sparser and route tokens to fewer experts, demonstrating higher training efficiency in experiments.
33. Their experiments confirm the finding in  that an auxiliary load balancing loss does not improve the model performance.
34. MoE Layer with Unlearnable Routing
35. Although learning-based routing has shown effectiveness only with the help of complicated load balancing mechanisms, recent studies have attempted to get rid of those.
36. Hash Layer (Roller et al., 2021) simplifies routing by using a parameter-free hashing function to route tokens to specific experts.
37. This design eliminates the need for a load balancing loss and sophisticated assignment algorithms.
38. They also study the performance of different hashing techniques, hash sizes and input features, and conclude that balanced and random hashes focused on the most local features work best.
39. The experiments show that a Hash Layer achieves comparable performance with a Switch Layer (Fedus et al., 2021) and BASE Layer (Lewis et al., 2021).
40. THOR (Zuo et al., 2022) is a special form of MoE layer, which completely discards the conditional routing mechanism and instead optimizes the consistency between a randomly selected pair of experts.
41. During inference, one expert will be randomly selected to be activated."
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,s5,Applications and Analysis,['p5.0'],"['GLaM  trains a family of GPT-style language models with up to 1.2 trillion parameters using GShard . CPM-2 (Zhang et al., 2022b) trains a large Chinese language model with 198 billion parameters with BASE layers (Lewis et al., 2021). Artetxe et al. (2021) conduct a detailed empirical study of how autoregressive MoE language models scale compared to dense models. They find MoEs to be substantially more efficient with the exception of fine-tuning. MoE models can match the performance of dense models with 25% of computation in a low-resource setting. Although the advantage fades at scale, their largest MoE model with 1.1 trillion parameters can consistently outperform its dense counterpart with the same amount of computation. Clark et al. (2022) examine the scaling law of BASE Layer (Lewis et al., 2021), Hash Layer (Roller et al., 2021) and earlier Reinforcement Learning-based routing algorithms providing suggestions for best-practices in training MoE models.  propose MoEfication to split feedforward neural networks (FFNN) in a trained large model to experts. They find that a T5-Large (Raffel et al., 2020) model with 700 million parameters only activates 5% neurons for 80% inputs on a downstream task, indicating high redundancy within large pretrained language models. To transform a pretrained language model to an MoE model, they first construct a co-activation graph for each FFNN and then divide the graph into subgraphs with strong internal connections with graph partitioning algorithm. Each subgraph forms an expert. They train a router with oracle best routing for training data. Then, they further fine-tune the resulted model for better performance.']","GLaM  trains a family of GPT-style language models with up to 1.2 trillion parameters using GShard . CPM-2 (Zhang et al., 2022b) trains a large Chinese language model with 198 billion parameters with BASE layers (Lewis et al., 2021). Artetxe et al. (2021) conduct a detailed empirical study of how autoregressive MoE language models scale compared to dense models. They find MoEs to be substantially more efficient with the exception of fine-tuning. MoE models can match the performance of dense models with 25% of computation in a low-resource setting. Although the advantage fades at scale, their largest MoE model with 1.1 trillion parameters can consistently outperform its dense counterpart with the same amount of computation. Clark et al. (2022) examine the scaling law of BASE Layer (Lewis et al., 2021), Hash Layer (Roller et al., 2021) and earlier Reinforcement Learning-based routing algorithms providing suggestions for best-practices in training MoE models.  propose MoEfication to split feedforward neural networks (FFNN) in a trained large model to experts. They find that a T5-Large (Raffel et al., 2020) model with 700 million parameters only activates 5% neurons for 80% inputs on a downstream task, indicating high redundancy within large pretrained language models. To transform a pretrained language model to an MoE model, they first construct a co-activation graph for each FFNN and then divide the graph into subgraphs with strong internal connections with graph partitioning algorithm. Each subgraph forms an expert. They train a router with oracle best routing for training data. Then, they further fine-tune the resulted model for better performance.","(p5.0) GLaM  trains a family of GPT-style language models with up to 1.2 trillion parameters using GShard . CPM-2 (Zhang et al., 2022b) trains a large Chinese language model with 198 billion parameters with BASE layers (Lewis et al., 2021). Artetxe et al. (2021) conduct a detailed empirical study of how autoregressive MoE language models scale compared to dense models. They find MoEs to be substantially more efficient with the exception of fine-tuning. MoE models can match the performance of dense models with 25% of computation in a low-resource setting. Although the advantage fades at scale, their largest MoE model with 1.1 trillion parameters can consistently outperform its dense counterpart with the same amount of computation. Clark et al. (2022) examine the scaling law of BASE Layer (Lewis et al., 2021), Hash Layer (Roller et al., 2021) and earlier Reinforcement Learning-based routing algorithms providing suggestions for best-practices in training MoE models.  propose MoEfication to split feedforward neural networks (FFNN) in a trained large model to experts. They find that a T5-Large (Raffel et al., 2020) model with 700 million parameters only activates 5% neurons for 80% inputs on a downstream task, indicating high redundancy within large pretrained language models. To transform a pretrained language model to an MoE model, they first construct a co-activation graph for each FFNN and then divide the graph into subgraphs with strong internal connections with graph partitioning algorithm. Each subgraph forms an expert. They train a router with oracle best routing for training data. Then, they further fine-tune the resulted model for better performance.","[['b28', 'b41', 'b0', 'b68', 'b5', 'b43']]","[['b28', 'b41', 'b0', 'b68', 'b5', 'b43']]",6,"1. GLaM  trains a family of GPT-style language models with up to 1.2 trillion parameters using GShard .
2. CPM-2 (Zhang et al., 2022b) trains a large Chinese language model with 198 billion parameters with BASE layers (Lewis et al., 2021).
3. Artetxe et al. (2021) conduct a detailed empirical study of how autoregressive MoE language models scale compared to dense models.
4. They find MoEs to be substantially more efficient with the exception of fine-tuning.
5. MoE models can match the performance of dense models with 25% of computation in a low-resource setting.
6. Although the advantage fades at scale, their largest MoE model with 1.1 trillion parameters can consistently outperform its dense counterpart with the same amount of computation.
7. Clark et al. (2022) examine the scaling law of BASE Layer (Lewis et al., 2021), Hash Layer (Roller et al., 2021) and earlier Reinforcement Learning-based routing algorithms providing suggestions for best-practices in training MoE models.
8. propose MoEfication to split feedforward neural networks (FFNN) in a trained large model to experts.
9. They find that a T5-Large (Raffel et al., 2020) model with 700 million parameters only activates 5% neurons for 80% inputs on a downstream task, indicating high redundancy within large pretrained language models.
10. To transform a pretrained language model to an MoE model, they first construct a co-activation graph for each FFNN and then divide the graph into subgraphs with strong internal connections with graph partitioning algorithm.
11. Each subgraph forms an expert. They train a router with oracle best routing for training data.
12. Then, they further fine-tune the resulted model for better performance."
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,s7,Confidence-based Early Exit,['p7.0'],"['Early works for early exit in computer vision (Park et al., 2015;Teerapittayanon et al., 2016;Kaya et al., 2019) often fall into this category. They define a metric as the proxy for confidence of a model prediction. The model exits early when the confidence hits a predefined threshold. DeeBERT (Xin et al., 2020b) applies BranchyNet (Teerapittayanon et al., 2016) to BERT inference. The training for DeeBERT is two-stage: they first train BERT on downstream tasks following standard fine-tuning. Then, they freeze the parameters of the Transformer and insert a linear classifier (i.e., internal classifier) after each Transformer layer. They train the classifiers by minimizing the sum of their cross-entropy loss. For inference, the model exits early when an internal classifier outputs a prediction probability distribution that has an entropy lower than a predefined threshold. RightTool (Schwartz et al., 2020) jointly finetunes BERT with internal classifiers. They use the temperature-calibrated maximum class probability as confidence. FastBERT  first trains the BERT backbone and the final classifier. Then, they distill the final classifier layer to the internal classifiers (Hinton et al., 2015). For inference, the model exits when the entropy of a prediction is below the threshold. Rome-BERT (Geng et al., 2021) provides a simple fix for learning internal classifiers efficiently. Besides self-distillation as in FastBERT, they propose gradient regularization (GR) to facilitate distillation. SkipBERT (Wang et al., 2022) caches pre-computed representation of text chunks to re-']","Early works for early exit in computer vision (Park et al., 2015;Teerapittayanon et al., 2016;Kaya et al., 2019) often fall into this category. They define a metric as the proxy for confidence of a model prediction. The model exits early when the confidence hits a predefined threshold. DeeBERT (Xin et al., 2020b) applies BranchyNet (Teerapittayanon et al., 2016) to BERT inference. The training for DeeBERT is two-stage: they first train BERT on downstream tasks following standard fine-tuning. Then, they freeze the parameters of the Transformer and insert a linear classifier (i.e., internal classifier) after each Transformer layer. They train the classifiers by minimizing the sum of their cross-entropy loss. For inference, the model exits early when an internal classifier outputs a prediction probability distribution that has an entropy lower than a predefined threshold. RightTool (Schwartz et al., 2020) jointly finetunes BERT with internal classifiers. They use the temperature-calibrated maximum class probability as confidence. FastBERT  first trains the BERT backbone and the final classifier. Then, they distill the final classifier layer to the internal classifiers (Hinton et al., 2015). For inference, the model exits when the entropy of a prediction is below the threshold. Rome-BERT (Geng et al., 2021) provides a simple fix for learning internal classifiers efficiently. Besides self-distillation as in FastBERT, they propose gradient regularization (GR) to facilitate distillation. SkipBERT (Wang et al., 2022) caches pre-computed representation of text chunks to re-","(p7.0) Early works for early exit in computer vision (Park et al., 2015;Teerapittayanon et al., 2016;Kaya et al., 2019) often fall into this category. They define a metric as the proxy for confidence of a model prediction. The model exits early when the confidence hits a predefined threshold. DeeBERT (Xin et al., 2020b) applies BranchyNet (Teerapittayanon et al., 2016) to BERT inference. The training for DeeBERT is two-stage: they first train BERT on downstream tasks following standard fine-tuning. Then, they freeze the parameters of the Transformer and insert a linear classifier (i.e., internal classifier) after each Transformer layer. They train the classifiers by minimizing the sum of their cross-entropy loss. For inference, the model exits early when an internal classifier outputs a prediction probability distribution that has an entropy lower than a predefined threshold. RightTool (Schwartz et al., 2020) jointly finetunes BERT with internal classifiers. They use the temperature-calibrated maximum class probability as confidence. FastBERT  first trains the BERT backbone and the final classifier. Then, they distill the final classifier layer to the internal classifiers (Hinton et al., 2015). For inference, the model exits when the entropy of a prediction is below the threshold. Rome-BERT (Geng et al., 2021) provides a simple fix for learning internal classifiers efficiently. Besides self-distillation as in FastBERT, they propose gradient regularization (GR) to facilitate distillation. SkipBERT (Wang et al., 2022) caches pre-computed representation of text chunks to re-","[['b56', 'b39', 'b54', None, 'b17', 'b11', 'b52', 'b22']]","[['b56', 'b39', 'b54', None, 'b17', 'b11', 'b52', 'b22']]",8,"1. Early works for early exit in computer vision (Park et al., 2015;Teerapittayanon et al., 2016;Kaya et al., 2019) often fall into this category.
2. They define a metric as the proxy for confidence of a model prediction.
3. The model exits early when the confidence hits a predefined threshold.
4. DeeBERT (Xin et al., 2020b) applies BranchyNet (Teerapittayanon et al., 2016) to BERT inference.
5. The training for DeeBERT is two-stage: they first train BERT on downstream tasks following standard fine-tuning.
6. Then, they freeze the parameters of the Transformer and insert a linear classifier (i.e., internal classifier) after each Transformer layer.
7. They train the classifiers by minimizing the sum of their cross-entropy loss.
8. For inference, the model exits early when an internal classifier outputs a prediction probability distribution that has an entropy lower than a predefined threshold.
9. RightTool (Schwartz et al., 2020) jointly finetunes BERT with internal classifiers.
10. They use the temperature-calibrated maximum class probability as confidence.
11. FastBERT  first trains the BERT backbone and the final classifier.
12. Then, they distill the final classifier layer to the internal classifiers (Hinton et al., 2015).
13. For inference, the model exits when the entropy of a prediction is below the threshold.
14. Rome-BERT (Geng et al., 2021) provides a simple fix for learning internal classifiers efficiently.
15. Besides self-distillation as in FastBERT, they propose gradient regularization (GR) to facilitate distillation.
16. SkipBERT (Wang et al., 2022) caches pre-computed representation of text chunks to re-"
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Computer Science, Linguistics",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s28,B Simulator,"['p28.0', 'p28.1', 'p28.2', 'p28.3', 'p28.4', 'p28.5', 'p28.6']","['The virtual features of the dataset are deeply connected with the simulator in which datasets are built. Here we summarize simulators frequently used during the VLN dataset creation process. House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset. An agent in the environment has access to first-person view RGB images, together with semantic/instance masks and depth information.', 'Matterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017). Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches. An agent can navigate between viewpoints along a pre-defined graph. Most indoors VLN datasets such as R2R and its variants are based on the Matterport3D simulator.', 'Habitat ( where agents could navigate and interact with objects. Based on the object interaction function, it helps to build a dataset that requires object interaction, such as ALFRED (Shridhar et al., 2020).', 'Gibson (Xia et al., 2018) is a real-world perception interactive environment with complex semantics. Each viewpoint has a set of RGB panoramas with global camera poses and reconstructed 3D meshes. Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator.', ""House3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints (e.g. it cannot pass through walls or objects)."", 'LANI (Misra et al., 2018) is a 3D simulator built in Unity3D platform. The environment in LANI is a fenced, square, grass field containing randomly placed landmarks. An agent needs to navigate between landmarks following the natural language instruction. Drone navigation tasks (Blukis et al., 2018(Blukis et al., , 2019 are also built based on LANI.', 'Currently, most datasets and simulators focus on indoors navigable scenes partly because of the difficulty of building an outdoor photo-realistic 3D simulator out of the increased complexity. Google Street View 4 , an online API that is integrated with Google Maps, is composed of billions of realistic street-level panoramas. It has been frequently used to create outdoor VLN tasks since the development of TOUCHDOWN (Chen et al., 2019).']","The virtual features of the dataset are deeply connected with the simulator in which datasets are built. Here we summarize simulators frequently used during the VLN dataset creation process. House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset. An agent in the environment has access to first-person view RGB images, together with semantic/instance masks and depth information.

Matterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017). Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches. An agent can navigate between viewpoints along a pre-defined graph. Most indoors VLN datasets such as R2R and its variants are based on the Matterport3D simulator.

Habitat ( where agents could navigate and interact with objects. Based on the object interaction function, it helps to build a dataset that requires object interaction, such as ALFRED (Shridhar et al., 2020).

Gibson (Xia et al., 2018) is a real-world perception interactive environment with complex semantics. Each viewpoint has a set of RGB panoramas with global camera poses and reconstructed 3D meshes. Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator.

House3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints (e.g. it cannot pass through walls or objects).

LANI (Misra et al., 2018) is a 3D simulator built in Unity3D platform. The environment in LANI is a fenced, square, grass field containing randomly placed landmarks. An agent needs to navigate between landmarks following the natural language instruction. Drone navigation tasks (Blukis et al., 2018(Blukis et al., , 2019 are also built based on LANI.

Currently, most datasets and simulators focus on indoors navigable scenes partly because of the difficulty of building an outdoor photo-realistic 3D simulator out of the increased complexity. Google Street View 4 , an online API that is integrated with Google Maps, is composed of billions of realistic street-level panoramas. It has been frequently used to create outdoor VLN tasks since the development of TOUCHDOWN (Chen et al., 2019).","(p28.0) The virtual features of the dataset are deeply connected with the simulator in which datasets are built. Here we summarize simulators frequently used during the VLN dataset creation process. House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset. An agent in the environment has access to first-person view RGB images, together with semantic/instance masks and depth information.

(p28.1) Matterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017). Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches. An agent can navigate between viewpoints along a pre-defined graph. Most indoors VLN datasets such as R2R and its variants are based on the Matterport3D simulator.

(p28.2) Habitat ( where agents could navigate and interact with objects. Based on the object interaction function, it helps to build a dataset that requires object interaction, such as ALFRED (Shridhar et al., 2020).

(p28.3) Gibson (Xia et al., 2018) is a real-world perception interactive environment with complex semantics. Each viewpoint has a set of RGB panoramas with global camera poses and reconstructed 3D meshes. Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator.

(p28.4) House3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints (e.g. it cannot pass through walls or objects).

(p28.5) LANI (Misra et al., 2018) is a 3D simulator built in Unity3D platform. The environment in LANI is a fenced, square, grass field containing randomly placed landmarks. An agent needs to navigate between landmarks following the natural language instruction. Drone navigation tasks (Blukis et al., 2018(Blukis et al., , 2019 are also built based on LANI.

(p28.6) Currently, most datasets and simulators focus on indoors navigable scenes partly because of the difficulty of building an outdoor photo-realistic 3D simulator out of the increased complexity. Google Street View 4 , an online API that is integrated with Google Maps, is composed of billions of realistic street-level panoramas. It has been frequently used to create outdoor VLN tasks since the development of TOUCHDOWN (Chen et al., 2019).","[[None, 'b16'], [None], [None], [None, 'b17'], ['b16'], [None], []]","[[None, 'b16'], [None], [None], [None, 'b17'], ['b16'], [None], []]",8,"1. The virtual features of the dataset are deeply connected with the simulator in which datasets are built.
2. Here we summarize simulators frequently used during the VLN dataset creation process.
3. House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset.
4. An agent in the environment has access to first-person view RGB images, together with semantic/instance masks and depth information.
5. Matterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017).
6. Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches.
7. An agent can navigate between viewpoints along a pre-defined graph.
8. Most indoors VLN datasets such as R2R and its variants are based on the Matterport3D simulator.
9. Habitat ( where agents could navigate and interact with objects. Based on the object interaction function, it helps to build a dataset that requires object interaction, such as ALFRED (Shridhar et al., 2020).
10. Gibson (Xia et al., 2018) is a real-world perception interactive environment with complex semantics.
11. Each viewpoint has a set of RGB panoramas with global camera poses and reconstructed 3D meshes.
12. Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator.
13. House3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints (e.g. it cannot pass through walls or objects).
14. LANI (Misra et al., 2018) is a 3D simulator built in Unity3D platform.
15. The environment in LANI is a fenced, square, grass field containing randomly placed landmarks.
16. An agent needs to navigate between landmarks following the natural language instruction.
17. Drone navigation tasks (Blukis et al., 2018(Blukis et al., , 2019 are also built based on LANI.
18. Currently, most datasets and simulators focus on indoors navigable scenes partly because of the difficulty of building an outdoor photo-realistic 3D simulator out of the increased complexity.
19. Google Street View 4 , an online API that is integrated with Google Maps, is composed of billions of realistic street-level panoramas.
20. It has been frequently used to create outdoor VLN tasks since the development of TOUCHDOWN (Chen et al., 2019)."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Computer Science, Linguistics",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s2,Initial Instruction,"['p2.0', 'p2.1']","['In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.', 'Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.']","In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","(p2.0) In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

(p2.1) Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","[[None, 'b16', 'b25', 'b19'], [None]]","[[None, 'b16', 'b25', 'b19'], [None]]",5,"1. In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room.
2. Turn left and go through the door in the middle.""
3. Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal.
4. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017).
5. An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views.
6. R2R is extended to create other VLN benchmarks.
7. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019).
8. Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments.
9. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object.
10. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.
11. Some work uses natural language to guide drones.
12. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions.
13. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Computer Science, Linguistics",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s3,Coarse-grained Navigation,"['p3.0', 'p3.1', 'p3.2', 'p3.3']","['In real life, detailed information about the route may not be available since it may be unknown to the human instructor (oracle). Usually, instructions are more concise and contain merely information of the target goal.', 'RoomNav (Wu et al., 2018) requires agent navigate according to instruction ""go to X"", where X is a predefined room or object.', 'In Embodied QA (Das et al., 2018), the agent navigates through the environment to find answer for a given question. The instructions in REVERIE (Qi et al., 2020b) are annotated by humans, and thus more complicated and diverse. The agent navigates through the rooms and differentiates the object against multiple competing candidates. In SOON (Zhu et al., 2021a), an agent receives a long, complex coarse-to-fine instruction which gradually narrows down the search scope.', 'Navigation+Object Interaction For some tasks, the target object might be hidden (e.g., the spoon in a drawer), or need to change status (e.g., a sliced apple is requested but only a whole apple is available). In these scenarios, it is necessary to interact with the objects to accomplish the task (e.g., opening the drawer or cutting the apple). Interactive Question Answering (IQA) requires the agent to navigate and sometimes to interact with objects to answer a given question. Based on indoor scenes in AI2-THOR (Kolve et al., 2017), Shridhar et al. (2020) propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions complete household tasks in an interactive visual environment. CHAI (Misra et al., 2018) requires the agent to navigate and simply interact with the environments.']","In real life, detailed information about the route may not be available since it may be unknown to the human instructor (oracle). Usually, instructions are more concise and contain merely information of the target goal.

RoomNav (Wu et al., 2018) requires agent navigate according to instruction ""go to X"", where X is a predefined room or object.

In Embodied QA (Das et al., 2018), the agent navigates through the environment to find answer for a given question. The instructions in REVERIE (Qi et al., 2020b) are annotated by humans, and thus more complicated and diverse. The agent navigates through the rooms and differentiates the object against multiple competing candidates. In SOON (Zhu et al., 2021a), an agent receives a long, complex coarse-to-fine instruction which gradually narrows down the search scope.

Navigation+Object Interaction For some tasks, the target object might be hidden (e.g., the spoon in a drawer), or need to change status (e.g., a sliced apple is requested but only a whole apple is available). In these scenarios, it is necessary to interact with the objects to accomplish the task (e.g., opening the drawer or cutting the apple). Interactive Question Answering (IQA) requires the agent to navigate and sometimes to interact with objects to answer a given question. Based on indoor scenes in AI2-THOR (Kolve et al., 2017), Shridhar et al. (2020) propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions complete household tasks in an interactive visual environment. CHAI (Misra et al., 2018) requires the agent to navigate and simply interact with the environments.","(p3.0) In real life, detailed information about the route may not be available since it may be unknown to the human instructor (oracle). Usually, instructions are more concise and contain merely information of the target goal.

(p3.1) RoomNav (Wu et al., 2018) requires agent navigate according to instruction ""go to X"", where X is a predefined room or object.

(p3.2) In Embodied QA (Das et al., 2018), the agent navigates through the environment to find answer for a given question. The instructions in REVERIE (Qi et al., 2020b) are annotated by humans, and thus more complicated and diverse. The agent navigates through the rooms and differentiates the object against multiple competing candidates. In SOON (Zhu et al., 2021a), an agent receives a long, complex coarse-to-fine instruction which gradually narrows down the search scope.

(p3.3) Navigation+Object Interaction For some tasks, the target object might be hidden (e.g., the spoon in a drawer), or need to change status (e.g., a sliced apple is requested but only a whole apple is available). In these scenarios, it is necessary to interact with the objects to accomplish the task (e.g., opening the drawer or cutting the apple). Interactive Question Answering (IQA) requires the agent to navigate and sometimes to interact with objects to answer a given question. Based on indoor scenes in AI2-THOR (Kolve et al., 2017), Shridhar et al. (2020) propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions complete household tasks in an interactive visual environment. CHAI (Misra et al., 2018) requires the agent to navigate and simply interact with the environments.","[[], ['b16'], [None, 'b25'], ['b31', None]]","[[], ['b16'], [None, 'b25'], ['b31', None]]",5,"1. In real life, detailed information about the route may not be available since it may be unknown to the human instructor (oracle).
2. Usually, instructions are more concise and contain merely information of the target goal.
3. RoomNav (Wu et al., 2018) requires agent navigate according to instruction ""go to X"", where X is a predefined room or object.
4. In Embodied QA (Das et al., 2018), the agent navigates through the environment to find answer for a given question.
5. The instructions in REVERIE (Qi et al., 2020b) are annotated by humans, and thus more complicated and diverse.
6. The agent navigates through the rooms and differentiates the object against multiple competing candidates.
7. In SOON (Zhu et al., 2021a), an agent receives a long, complex coarse-to-fine instruction which gradually narrows down the search scope.
8. Navigation+Object Interaction For some tasks, the target object might be hidden (e.g., the spoon in a drawer), or need to change status (e.g., a sliced apple is requested but only a whole apple is available).
9. In these scenarios, it is necessary to interact with the objects to accomplish the task (e.g., opening the drawer or cutting the apple).
10. Interactive Question Answering (IQA) requires the agent to navigate and sometimes to interact with objects to answer a given question.
11. Based on indoor scenes in AI2-THOR (Kolve et al., 2017), Shridhar et al. (2020) propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions complete household tasks in an interactive visual environment.
12. CHAI (Misra et al., 2018) requires the agent to navigate and simply interact with the environments."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Computer Science, Linguistics",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s9,Semantic Understanding,"['p9.0', 'p9.1', 'p9.2']","['Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.', 'Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).', 'Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).']","Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","(p9.0) Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

(p9.1) Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

(p9.2) Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","[[], ['b21', 'b28', 'b4', None, 'b22'], ['b21', None]]","[[], ['b21', 'b28', 'b4', None, 'b22'], ['b21', None]]",7,"1. Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.
2. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.
3. Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN.
4. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b).
5. Therefore, it is important to find the feature(s) that best improve performance.
6. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).
7. Different types of tokens within the instruction also function differently (Zhu et al., 2021b).
8. Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).
9. Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).
10. The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a)."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Computer Science, Linguistics",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s14,Reinforcement Learning,"['p14.0', 'p14.1']","['VLN is a sequential decision-making problem and can naturally be modeled as a Markov decision process. So Reinforcement Learning (RL) methods are proposed to learn better policy for VLN tasks. A critical challenge for RL methods is that VLN agents only receive the success signal at the end of the episode, so it is difficult to know which actions to attribute success to, and which to penalize. To address the ill-posed feedback issue, Wang et al. (2019) propose RCM model to enforces cross-modal grounding both locally and globally, with goal-oriented extrinsic reward and instructionfidelity intrinsic reward. He et al. (2021) propose to utilize the local alignment between the instruction and critical landmarks as the reward. Evaluation metrics such as CLS (Jain et al., 2019) or nDTW (Ilharco et al., 2019) can also provide informative reward signal (Landi et al., 2020), and natural language may also provide suggestions for reward (Fu et al., 2019).', 'To model the dynamics in the environment, Wang et al. (2018) leverage model-based reinforcement learning to predict the next state and improve the generalization in unseen environment. Zhang et al. (2020a) find recursively alternating the learning schemes of imitation and reinforcement learning improve the performance.']","VLN is a sequential decision-making problem and can naturally be modeled as a Markov decision process. So Reinforcement Learning (RL) methods are proposed to learn better policy for VLN tasks. A critical challenge for RL methods is that VLN agents only receive the success signal at the end of the episode, so it is difficult to know which actions to attribute success to, and which to penalize. To address the ill-posed feedback issue, Wang et al. (2019) propose RCM model to enforces cross-modal grounding both locally and globally, with goal-oriented extrinsic reward and instructionfidelity intrinsic reward. He et al. (2021) propose to utilize the local alignment between the instruction and critical landmarks as the reward. Evaluation metrics such as CLS (Jain et al., 2019) or nDTW (Ilharco et al., 2019) can also provide informative reward signal (Landi et al., 2020), and natural language may also provide suggestions for reward (Fu et al., 2019).

To model the dynamics in the environment, Wang et al. (2018) leverage model-based reinforcement learning to predict the next state and improve the generalization in unseen environment. Zhang et al. (2020a) find recursively alternating the learning schemes of imitation and reinforcement learning improve the performance.","(p14.0) VLN is a sequential decision-making problem and can naturally be modeled as a Markov decision process. So Reinforcement Learning (RL) methods are proposed to learn better policy for VLN tasks. A critical challenge for RL methods is that VLN agents only receive the success signal at the end of the episode, so it is difficult to know which actions to attribute success to, and which to penalize. To address the ill-posed feedback issue, Wang et al. (2019) propose RCM model to enforces cross-modal grounding both locally and globally, with goal-oriented extrinsic reward and instructionfidelity intrinsic reward. He et al. (2021) propose to utilize the local alignment between the instruction and critical landmarks as the reward. Evaluation metrics such as CLS (Jain et al., 2019) or nDTW (Ilharco et al., 2019) can also provide informative reward signal (Landi et al., 2020), and natural language may also provide suggestions for reward (Fu et al., 2019).

(p14.1) To model the dynamics in the environment, Wang et al. (2018) leverage model-based reinforcement learning to predict the next state and improve the generalization in unseen environment. Zhang et al. (2020a) find recursively alternating the learning schemes of imitation and reinforcement learning improve the performance.","[['b11', None], ['b21', 'b12']]","[['b11', None], ['b21', 'b12']]",4,"1. VLN is a sequential decision-making problem and can naturally be modeled as a Markov decision process.
2. So Reinforcement Learning (RL) methods are proposed to learn better policy for VLN tasks.
3. A critical challenge for RL methods is that VLN agents only receive the success signal at the end of the episode, so it is difficult to know which actions to attribute success to, and which to penalize.
4. To address the ill-posed feedback issue, Wang et al. (2019) propose RCM model to enforces cross-modal grounding both locally and globally, with goal-oriented extrinsic reward and instructionfidelity intrinsic reward.
5. He et al. (2021) propose to utilize the local alignment between the instruction and critical landmarks as the reward.
6. Evaluation metrics such as CLS (Jain et al., 2019) or nDTW (Ilharco et al., 2019) can also provide informative reward signal (Landi et al., 2020), and natural language may also provide suggestions for reward (Fu et al., 2019).
7. To model the dynamics in the environment, Wang et al. (2018) leverage model-based reinforcement learning to predict the next state and improve the generalization in unseen environment.
8. Zhang et al. (2020a) find recursively alternating the learning schemes of imitation and reinforcement learning improve the performance."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Computer Science, Linguistics",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s19,Data Augmentation,['p19.0'],"['Trajectory-Instruction Augmentation Augmented path-instruction pairs could be used in VLN directly. Currently the common practice is to train a speaker module to generate instructions given a navigation path (Fried et al., 2018). This generated data have varying quality (Zhao et al., 2021). Therefore an alignment scorer (Huang et al., 2019) or adversarial discriminator (Fu et al., 2020) can select high-quality pairs for augmentation. Environment Augmentation Generating more environment data not only helps generate more trajectories, but also alleviates the problem of overfitting in seen environments. Randomly masking the same visual feature across different viewpoints (Tan et al., 2019) or simply splitting the house scenes and remixing them (Liu et al., 2021) could create new environments, which could further be used to generate more trajectory-instruction pairs (Fried et al., 2018). Training data may also be augmented by replacing some visual features with counterfactual ones (Parvaneh et al., 2020).']","Trajectory-Instruction Augmentation Augmented path-instruction pairs could be used in VLN directly. Currently the common practice is to train a speaker module to generate instructions given a navigation path (Fried et al., 2018). This generated data have varying quality (Zhao et al., 2021). Therefore an alignment scorer (Huang et al., 2019) or adversarial discriminator (Fu et al., 2020) can select high-quality pairs for augmentation. Environment Augmentation Generating more environment data not only helps generate more trajectories, but also alleviates the problem of overfitting in seen environments. Randomly masking the same visual feature across different viewpoints (Tan et al., 2019) or simply splitting the house scenes and remixing them (Liu et al., 2021) could create new environments, which could further be used to generate more trajectory-instruction pairs (Fried et al., 2018). Training data may also be augmented by replacing some visual features with counterfactual ones (Parvaneh et al., 2020).","(p19.0) Trajectory-Instruction Augmentation Augmented path-instruction pairs could be used in VLN directly. Currently the common practice is to train a speaker module to generate instructions given a navigation path (Fried et al., 2018). This generated data have varying quality (Zhao et al., 2021). Therefore an alignment scorer (Huang et al., 2019) or adversarial discriminator (Fu et al., 2020) can select high-quality pairs for augmentation. Environment Augmentation Generating more environment data not only helps generate more trajectories, but also alleviates the problem of overfitting in seen environments. Randomly masking the same visual feature across different viewpoints (Tan et al., 2019) or simply splitting the house scenes and remixing them (Liu et al., 2021) could create new environments, which could further be used to generate more trajectory-instruction pairs (Fried et al., 2018). Training data may also be augmented by replacing some visual features with counterfactual ones (Parvaneh et al., 2020).","[[None, 'b23', 'b1', 'b11']]","[[None, 'b23', 'b1', 'b11']]",4,"1. Trajectory-Instruction Augmentation Augmented path-instruction pairs could be used in VLN directly.
2. Currently the common practice is to train a speaker module to generate instructions given a navigation path (Fried et al., 2018).
3. This generated data have varying quality (Zhao et al., 2021).
4. Therefore an alignment scorer (Huang et al., 2019) or adversarial discriminator (Fu et al., 2020) can select high-quality pairs for augmentation.
5. Environment Augmentation Generating more environment data not only helps generate more trajectories, but also alleviates the problem of overfitting in seen environments.
6. Randomly masking the same visual feature across different viewpoints (Tan et al., 2019) or simply splitting the house scenes and remixing them (Liu et al., 2021) could create new environments, which could further be used to generate more trajectory-instruction pairs (Fried et al., 2018).
7. Training data may also be augmented by replacing some visual features with counterfactual ones (Parvaneh et al., 2020)."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Computer Science, Linguistics",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s21,Multitask Learning,['p21.0'],"['Different VLN tasks can benefit from each other by cross-task knowledge transfer. Wang et al. (2020c) propose an environment-agnostic multitask navigation model for both VLN and Navigation from Dialog History tasks (Thomason et al., 2019b). Chaplot et al. (2020) propose an attention module to train a multitask navigation agent to follow instructions and answer questions (Wijmans et al., 2019a).']","Different VLN tasks can benefit from each other by cross-task knowledge transfer. Wang et al. (2020c) propose an environment-agnostic multitask navigation model for both VLN and Navigation from Dialog History tasks (Thomason et al., 2019b). Chaplot et al. (2020) propose an attention module to train a multitask navigation agent to follow instructions and answer questions (Wijmans et al., 2019a).","(p21.0) Different VLN tasks can benefit from each other by cross-task knowledge transfer. Wang et al. (2020c) propose an environment-agnostic multitask navigation model for both VLN and Navigation from Dialog History tasks (Thomason et al., 2019b). Chaplot et al. (2020) propose an attention module to train a multitask navigation agent to follow instructions and answer questions (Wijmans et al., 2019a).","[['b13', None, 'b14', 'b5']]","[['b13', None, 'b14', 'b5']]",4,"1. Different VLN tasks can benefit from each other by cross-task knowledge transfer.
2. Wang et al. (2020c) propose an environment-agnostic multitask navigation model for both VLN and Navigation from Dialog History tasks (Thomason et al., 2019b).
3. Chaplot et al. (2020) propose an attention module to train a multitask navigation agent to follow instructions and answer questions (Wijmans et al., 2019a)."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Computer Science, Linguistics",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s23,Prior Exploration,['p23.0'],"[""Good performance in seen environments often cannot generalize to unseen environments (Hu et al., 2019;Parvaneh et al., 2020;Tan et al., 2019). Prior exploration methods allow the agent to observe and adapt to unseen environments, 3 bridging the performance gap between seen and unseen environments. Wang et al. (2019) introduce a self-supervised imitation learning to learn from the agent's own past, good behaviors. The best navigation path determined to align the instruction the best by a matching critic will be used to update the agent. Tan et al. (2019) leverage the testing environments to sample and augment paths for adaptation. Fu et al. (2020) propose environment-based prior exploration, where the agent can only explore a particular environment where it is deployed. When utilizing graph, prior exploration may construct a map or overview about the unseen environment to provide explicit guidance for navigation (Chen et al., 2021a;Zhou et al., 2021).""]","Good performance in seen environments often cannot generalize to unseen environments (Hu et al., 2019;Parvaneh et al., 2020;Tan et al., 2019). Prior exploration methods allow the agent to observe and adapt to unseen environments, 3 bridging the performance gap between seen and unseen environments. Wang et al. (2019) introduce a self-supervised imitation learning to learn from the agent's own past, good behaviors. The best navigation path determined to align the instruction the best by a matching critic will be used to update the agent. Tan et al. (2019) leverage the testing environments to sample and augment paths for adaptation. Fu et al. (2020) propose environment-based prior exploration, where the agent can only explore a particular environment where it is deployed. When utilizing graph, prior exploration may construct a map or overview about the unseen environment to provide explicit guidance for navigation (Chen et al., 2021a;Zhou et al., 2021).","(p23.0) Good performance in seen environments often cannot generalize to unseen environments (Hu et al., 2019;Parvaneh et al., 2020;Tan et al., 2019). Prior exploration methods allow the agent to observe and adapt to unseen environments, 3 bridging the performance gap between seen and unseen environments. Wang et al. (2019) introduce a self-supervised imitation learning to learn from the agent's own past, good behaviors. The best navigation path determined to align the instruction the best by a matching critic will be used to update the agent. Tan et al. (2019) leverage the testing environments to sample and augment paths for adaptation. Fu et al. (2020) propose environment-based prior exploration, where the agent can only explore a particular environment where it is deployed. When utilizing graph, prior exploration may construct a map or overview about the unseen environment to provide explicit guidance for navigation (Chen et al., 2021a;Zhou et al., 2021).","[['b24', None, 'b1', 'b11']]","[['b24', None, 'b1', 'b11']]",4,"1. Good performance in seen environments often cannot generalize to unseen environments (Hu et al., 2019;Parvaneh et al., 2020;Tan et al., 2019).
2. Prior exploration methods allow the agent to observe and adapt to unseen environments, 3 bridging the performance gap between seen and unseen environments.
3. Wang et al. (2019) introduce a self-supervised imitation learning to learn from the agent's own past, good behaviors.
4. The best navigation path determined to align the instruction the best by a matching critic will be used to update the agent.
5. Tan et al. (2019) leverage the testing environments to sample and augment paths for adaptation.
6. Fu et al. (2020) propose environment-based prior exploration, where the agent can only explore a particular environment where it is deployed.
7. When utilizing graph, prior exploration may construct a map or overview about the unseen environment to provide explicit guidance for navigation (Chen et al., 2021a;Zhou et al., 2021)."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Computer Science, Linguistics",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s24,Related Visual-and-Language Tasks,"['p24.0', 'p24.1']","['This paper focuses on Vision-and-Language Navigation tasks with an emphasis on photo-realistic environments. 2D map may also be a uesful virtual environment for navigation tasks (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Paz-Argaman and Tsarfaty, 2019). Synthetic environments may also be a substitute for realistic environment (MacMahon et al., 2006;Blukis et al., 2020). Tellex et al. (2011) propose to instantiate a probabilistic graphical model for natural language commands in robotic navigation and mobile manipulation process.', 'In VLN, an agent needs to follow the given instruction and even ask for assistants in human language. An agent in Visual Navigation tasks is usually not required to understand information from textual modality. Visual Navigation is a problem of navigating an agent from the current location to find the goal target. Researchers have achieved success in both simulated environments (Zhu et al., 2017;Mirowski, 2019) and real environments (Mirowski et al., 2018).']","This paper focuses on Vision-and-Language Navigation tasks with an emphasis on photo-realistic environments. 2D map may also be a uesful virtual environment for navigation tasks (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Paz-Argaman and Tsarfaty, 2019). Synthetic environments may also be a substitute for realistic environment (MacMahon et al., 2006;Blukis et al., 2020). Tellex et al. (2011) propose to instantiate a probabilistic graphical model for natural language commands in robotic navigation and mobile manipulation process.

In VLN, an agent needs to follow the given instruction and even ask for assistants in human language. An agent in Visual Navigation tasks is usually not required to understand information from textual modality. Visual Navigation is a problem of navigating an agent from the current location to find the goal target. Researchers have achieved success in both simulated environments (Zhu et al., 2017;Mirowski, 2019) and real environments (Mirowski et al., 2018).","(p24.0) This paper focuses on Vision-and-Language Navigation tasks with an emphasis on photo-realistic environments. 2D map may also be a uesful virtual environment for navigation tasks (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Paz-Argaman and Tsarfaty, 2019). Synthetic environments may also be a substitute for realistic environment (MacMahon et al., 2006;Blukis et al., 2020). Tellex et al. (2011) propose to instantiate a probabilistic graphical model for natural language commands in robotic navigation and mobile manipulation process.

(p24.1) In VLN, an agent needs to follow the given instruction and even ask for assistants in human language. An agent in Visual Navigation tasks is usually not required to understand information from textual modality. Visual Navigation is a problem of navigating an agent from the current location to find the goal target. Researchers have achieved success in both simulated environments (Zhu et al., 2017;Mirowski, 2019) and real environments (Mirowski et al., 2018).","[['b7', None, 'b3'], ['b31', None]]","[['b7', None, 'b3'], ['b31', None]]",5,"1. This paper focuses on Vision-and-Language Navigation tasks with an emphasis on photo-realistic environments.
2. 2D map may also be a uesful virtual environment for navigation tasks (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Paz-Argaman and Tsarfaty, 2019).
3. Synthetic environments may also be a substitute for realistic environment (MacMahon et al., 2006;Blukis et al., 2020).
4. Tellex et al. (2011) propose to instantiate a probabilistic graphical model for natural language commands in robotic navigation and mobile manipulation process.
5. In VLN, an agent needs to follow the given instruction and even ask for assistants in human language.
6. An agent in Visual Navigation tasks is usually not required to understand information from textual modality.
7. Visual Navigation is a problem of navigating an agent from the current location to find the goal target.
8. Researchers have achieved success in both simulated environments (Zhu et al., 2017;Mirowski, 2019) and real environments (Mirowski et al., 2018)."
248426721,What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,Computer Science,https://www.semanticscholar.org/paper/29a369d83a7d6a49f6a3259bee23bd4d95db0b16,s1,Relation Extraction Datasets Survey,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5', 'p1.6', 'p1.7', 'p1.8', 'p1.9', 'p1.10', 'p1.11']","['RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.', 'Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.', '3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).', 'Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.', 'The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.', 'Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).', 'All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).', 'Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.', 'Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.', 'Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:', '1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).', 'In the case study of this paper, given the scientific datasets available, we focus on the first setup.']","RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.","(p1.0) RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

(p1.1) Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

(p1.2) 3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

(p1.3) Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

(p1.4) The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

(p1.5) Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

(p1.6) All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

(p1.7) Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

(p1.8) Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

(p1.9) Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

(p1.10) 1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

(p1.11) In the case study of this paper, given the scientific datasets available, we focus on the first setup.","[[None, 'b30'], ['b29', 'b59', None, 'b10'], ['b7', None], [None, 'b18', 'b1'], ['b11', 'b32'], ['b55', 'b31', 'b7', None, 'b6'], ['b48', 'b52'], [None], [], [], ['b25'], []]","[[None, 'b30'], ['b29', 'b59', None, 'b10'], ['b7', None], [None, 'b18', 'b1'], ['b11', 'b32'], ['b55', 'b31', 'b7', None, 'b6'], ['b48', 'b52'], [None], [], [], ['b25'], []]",22,"1. RE has been broadly studied in the last decades and many datasets were published.
2. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia.
3. An overview of the datasets is given in Table 1.
4. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work.
5. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works.
6. It contains annotations for named entities and relations in news articles.
7. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004).
8. It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic.
9. The corpus is divided into six domains.
10. Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010).
11. It contains over 1.8 million articles by the NYT between 1987 and 2007.
12. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.
13. 3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far.
14. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021).
15. For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals.
16. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks.
17. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF).
18. The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields.
19. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7.
20. The data will be described in further detail in Section 4.1.
21. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution.
22. It contains abstracts from scientific AI-related conferences.
23. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC).
24. We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.
25. The Wikipedia domain has been first introduced in 2013.
26. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia.
27. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models.
28. The multi-lingual dimension is gaining more interest for RE.
29. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.
30. Previous datasets were restricted to the same label collection in the training set and in the test set.
31. To address this gap and make RE experimental scenarios more realistic,
32. Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs.
33. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models.
34. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).
35. All datasets so far present a sentence level annotation.
36. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata.
37. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level.
38. In addition to RE, DocRED annotates coreference chains.
39. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).
40. Lastly, there are works focusing on creating datasets for specific RE aspects.
41. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.
42. Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia.
43. Similarly, we observe the emerging trend for FSL.
44. Different datasets lend themselves to study different aspects of the task.
45. Concerning crossdomain RE, we propose to distinguish three setups:1.
46. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).
47. In the case study of this paper, given the scientific datasets available, we focus on the first setup."
248426721,What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,Computer Science,https://www.semanticscholar.org/paper/29a369d83a7d6a49f6a3259bee23bd4d95db0b16,s2,The Relation Extraction Task,"['p2.0', 'p2.1', 'p2.2']","['Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.', 'One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.', 'Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.']","Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","(p2.0) Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

(p2.1) One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

(p2.2) Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","[[], ['b3', 'b36', 'b32', 'b7', None], ['b56', 'b36', 'b54', None, 'b44', 'b43']]","[[], ['b3', 'b36', 'b32', 'b7', None], ['b56', 'b36', 'b54', None, 'b44', 'b43']]",11,"1. Conceptually, RE involves a pipeline of steps (see Figure 2).
2. Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.
3. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).
4. 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.
5. One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.
6. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
7. As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered).
8. The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.
9. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.
10. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).
11. Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.
12. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).
13. Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set.
14. 6 What Do You Mean by Relation Extraction?
15. RE studies rarely address the whole pipeline.
16. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.
17. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC).
18. Table 2 shows such investigation.
19. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE.
20. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.
21. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020).
22. Our analysis further shows that it is difficult to determine the RI setup.
23. While RC is always performed, the situation is different for RI (or no-rel).
24. Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances).
25. As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled.
26. These details are utterly important as they impact both model estimation and evaluation.
27. Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.
28. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021).
29. However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models.
30. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a).
31. They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
32. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
33. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.
34. Since the output label space is different, separate encoders could better capture distinct contextual information.
35. At the moment it is not clear if one approach is more suitable than the other for RE.
36. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task."
251402499,Abstractive Meeting Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/6dd2ffed96ab44004b2e4cb1bf36a44daae14d42,s8,Evaluation methods,"['p8.0', 'p8.1', 'p8.2', 'p8.3', 'p8.4', 'p8.5']","['As noted in Section 2, the subjectivity of meeting summaries, stemming in part from the preference for abstractive summarization, complicates the evaluation task. Evaluation requires both verifying that the content in a system-based summary follows from the original transcript and measuring the overlap with a gold-standard summary. However, given the expressive freedom encouraged by abstractive approaches, checking for semantic entailment and overlap of summary-worthy content requires deep semantic understanding, not just recognition that the same words are used.', 'Unfortunately, the ROUGE metric (Lin, 2004), which remains the standard for both meeting and general text summarization, scores systemproduced summaries based purely on surface lexicographic matches with a (usually single) gold summary, making it unideal for assessing abstractive summaries. If we take the abstractive summary from Figure 1 (""Some part of the casing will be made of a spongy material"") as a gold example, ROUGE would assign a higher score to a system that produces ""Some part of the casing will be made of broccoli"" than one that output ""A portion of the outer layer will be constructed from a sponge-like material,"" even though the latter is a perfect reformulation of the gold summary, while the former says something very different (and false).', 'A reasonable way to try to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap. Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text. Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022;Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks.', 'Despite their rapid development, recent studies on meta-evaluation of these metrics show mixed results. Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, e.g., there is no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate. Bhandari et al. (2020b) argue that there is no one-size-fitsall metric that correlates better with human judgement than the others, and that can outperform others on all datasets.', 'Clearly, evaluation is itself a very challenging task. And we note that none of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency. When we summarize a meeting or detail decisions and actions items in our own words, it is important to get the facts straight. Otherwise, the resulting summaries are not reliable for an end user. While we do not know of current work that focuses on evaluation of factuality explicitly for the meeting domain, the study of factual consistency in summarization more generally is a budding research area that we discuss in Section 6.', 'In the absence of a clear winner for summary evaluation metrics, none of the alternatives has yet to be widely adopted. In our comparison of different summarization systems in Section 5, we therefore stick with ROUGE, which offers the additional advantage of conceptual simplicity and lower computational cost compared to metrics based on contextual embeddings and pretrained language models.']","As noted in Section 2, the subjectivity of meeting summaries, stemming in part from the preference for abstractive summarization, complicates the evaluation task. Evaluation requires both verifying that the content in a system-based summary follows from the original transcript and measuring the overlap with a gold-standard summary. However, given the expressive freedom encouraged by abstractive approaches, checking for semantic entailment and overlap of summary-worthy content requires deep semantic understanding, not just recognition that the same words are used.

Unfortunately, the ROUGE metric (Lin, 2004), which remains the standard for both meeting and general text summarization, scores systemproduced summaries based purely on surface lexicographic matches with a (usually single) gold summary, making it unideal for assessing abstractive summaries. If we take the abstractive summary from Figure 1 (""Some part of the casing will be made of a spongy material"") as a gold example, ROUGE would assign a higher score to a system that produces ""Some part of the casing will be made of broccoli"" than one that output ""A portion of the outer layer will be constructed from a sponge-like material,"" even though the latter is a perfect reformulation of the gold summary, while the former says something very different (and false).

A reasonable way to try to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap. Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text. Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022;Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks.

Despite their rapid development, recent studies on meta-evaluation of these metrics show mixed results. Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, e.g., there is no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate. Bhandari et al. (2020b) argue that there is no one-size-fitsall metric that correlates better with human judgement than the others, and that can outperform others on all datasets.

Clearly, evaluation is itself a very challenging task. And we note that none of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency. When we summarize a meeting or detail decisions and actions items in our own words, it is important to get the facts straight. Otherwise, the resulting summaries are not reliable for an end user. While we do not know of current work that focuses on evaluation of factuality explicitly for the meeting domain, the study of factual consistency in summarization more generally is a budding research area that we discuss in Section 6.

In the absence of a clear winner for summary evaluation metrics, none of the alternatives has yet to be widely adopted. In our comparison of different summarization systems in Section 5, we therefore stick with ROUGE, which offers the additional advantage of conceptual simplicity and lower computational cost compared to metrics based on contextual embeddings and pretrained language models.","(p8.0) As noted in Section 2, the subjectivity of meeting summaries, stemming in part from the preference for abstractive summarization, complicates the evaluation task. Evaluation requires both verifying that the content in a system-based summary follows from the original transcript and measuring the overlap with a gold-standard summary. However, given the expressive freedom encouraged by abstractive approaches, checking for semantic entailment and overlap of summary-worthy content requires deep semantic understanding, not just recognition that the same words are used.

(p8.1) Unfortunately, the ROUGE metric (Lin, 2004), which remains the standard for both meeting and general text summarization, scores systemproduced summaries based purely on surface lexicographic matches with a (usually single) gold summary, making it unideal for assessing abstractive summaries. If we take the abstractive summary from Figure 1 (""Some part of the casing will be made of a spongy material"") as a gold example, ROUGE would assign a higher score to a system that produces ""Some part of the casing will be made of broccoli"" than one that output ""A portion of the outer layer will be constructed from a sponge-like material,"" even though the latter is a perfect reformulation of the gold summary, while the former says something very different (and false).

(p8.2) A reasonable way to try to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap. Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text. Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022;Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks.

(p8.3) Despite their rapid development, recent studies on meta-evaluation of these metrics show mixed results. Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, e.g., there is no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate. Bhandari et al. (2020b) argue that there is no one-size-fitsall metric that correlates better with human judgement than the others, and that can outperform others on all datasets.

(p8.4) Clearly, evaluation is itself a very challenging task. And we note that none of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency. When we summarize a meeting or detail decisions and actions items in our own words, it is important to get the facts straight. Otherwise, the resulting summaries are not reliable for an end user. While we do not know of current work that focuses on evaluation of factuality explicitly for the meeting domain, the study of factual consistency in summarization more generally is a budding research area that we discuss in Section 6.

(p8.5) In the absence of a clear winner for summary evaluation metrics, none of the alternatives has yet to be widely adopted. In our comparison of different summarization systems in Section 5, we therefore stick with ROUGE, which offers the additional advantage of conceptual simplicity and lower computational cost compared to metrics based on contextual embeddings and pretrained language models.","[[], ['b17'], ['b55', 'b49', None, 'b27', 'b52'], ['b7', 'b6'], [], []]","[[], ['b17'], ['b55', 'b49', None, 'b27', 'b52'], ['b7', 'b6'], [], []]",8,"1. As noted in Section 2, the subjectivity of meeting summaries, stemming in part from the preference for abstractive summarization, complicates the evaluation task.
2. Evaluation requires both verifying that the content in a system-based summary follows from the original transcript and measuring the overlap with a gold-standard summary.
3. However, given the expressive freedom encouraged by abstractive approaches, checking for semantic entailment and overlap of summary-worthy content requires deep semantic understanding, not just recognition that the same words are used.
4. Unfortunately, the ROUGE metric (Lin, 2004), which remains the standard for both meeting and general text summarization, scores systemproduced summaries based purely on surface lexicographic matches with a (usually single) gold summary, making it unideal for assessing abstractive summaries.
5. If we take the abstractive summary from Figure 1 (""Some part of the casing will be made of a spongy material"") as a gold example, ROUGE would assign a higher score to a system that produces ""Some part of the casing will be made of broccoli"" than one that output ""A portion of the outer layer will be constructed from a sponge-like material,"" even though the latter is a perfect reformulation of the gold summary, while the former says something very different (and false).
6. A reasonable way to try to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap.
7. Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text.
8. Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022;Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks.
9. Despite their rapid development, recent studies on meta-evaluation of these metrics show mixed results.
10. Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, e.g., there is no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate.
11. Bhandari et al. (2020b) argue that there is no one-size-fitsall metric that correlates better with human judgement than the others, and that can outperform others on all datasets.
12. Clearly, evaluation is itself a very challenging task.
13. And we note that none of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency.
14. When we summarize a meeting or detail decisions and actions items in our own words, it is important to get the facts straight.
15. Otherwise, the resulting summaries are not reliable for an end user.
16. While we do not know of current work that focuses on evaluation of factuality explicitly for the meeting domain, the study of factual consistency in summarization more generally is a budding research area that we discuss in Section 6.
17. In the absence of a clear winner for summary evaluation metrics, none of the alternatives has yet to be widely adopted.
18. In our comparison of different summarization systems in Section 5, we therefore stick with ROUGE, which offers the additional advantage of conceptual simplicity and lower computational cost compared to metrics based on contextual embeddings and pretrained language models."
251402499,Abstractive Meeting Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/6dd2ffed96ab44004b2e4cb1bf36a44daae14d42,s11,Discursive information,"['p11.0', 'p11.1', 'p11.2', 'p11.3', 'p11.4', 'p11.5']","['While most work on abstractive meeting summarization implicitly assumes that conversation is merely a linear sequence of utterances without semantic relations between them, example (1) underscores the importance of semantic relations for conversational understanding. Drawing on similar insights, certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993;Lascarides and Asher, 2008), to improve summarization. Accounts like RST and SDRT maintain that in a coherent conversation, each (roughly) clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, Contrast, etc. to reflect its contribution to the larger discourse. Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations. Figure 2 il-  , 2021a). The * indicates summary-level (with sentence split) ROUGE-L score (Lin, 2004).', 'lustrates a possible SDRT graph for example (1).', 'To the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries. They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora. Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices. Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary. A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.', 'An alternative approach to discourse interpretation that developed largely independently of RST and SDRT is dialogue act classification. Detailing the differences between the approaches is out of the scope of this paper, but in a nutshell, dialogue acts provide a shallower notion of discourse structure (Jurafsky et al., 1997) in that they do not entail a full graph structure over a conversation. On the other hand, systems for dialogue act labeling, such as DAMSL (Allen and Core, 1997; Core and Allen, 1997) or DiAML (Bunt et al., 2010(Bunt et al., , 2012, place more emphasis on interactive acts such as stalling to hold the floor, assessing other discourse moves, suggesting or informing.', 'Both the AMI and ICSI corpora provide gold dialogue act labels, and Goo and Chen (2018) use the labels from the AMI corpus to develop a sentence-gated mechanism that jointly models the relationships between dialogue acts and topic summaries to improve abstractive meeting summarization. In particular, they show that dialogue acts of the type inform are more closely linked to summary-worthy material than acts such as stall or assess. Using LSTM, their model consists of three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder.', 'Dialogue acts have also been used to good effect for summarizing decisions. Fernández et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution. Then, key fragments of the decision related utterances are retained to form an extractive decision summary. Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006(Purver et al., , 2007.']","While most work on abstractive meeting summarization implicitly assumes that conversation is merely a linear sequence of utterances without semantic relations between them, example (1) underscores the importance of semantic relations for conversational understanding. Drawing on similar insights, certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993;Lascarides and Asher, 2008), to improve summarization. Accounts like RST and SDRT maintain that in a coherent conversation, each (roughly) clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, Contrast, etc. to reflect its contribution to the larger discourse. Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations. Figure 2 il-  , 2021a). The * indicates summary-level (with sentence split) ROUGE-L score (Lin, 2004).

lustrates a possible SDRT graph for example (1).

To the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries. They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora. Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices. Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary. A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.

An alternative approach to discourse interpretation that developed largely independently of RST and SDRT is dialogue act classification. Detailing the differences between the approaches is out of the scope of this paper, but in a nutshell, dialogue acts provide a shallower notion of discourse structure (Jurafsky et al., 1997) in that they do not entail a full graph structure over a conversation. On the other hand, systems for dialogue act labeling, such as DAMSL (Allen and Core, 1997; Core and Allen, 1997) or DiAML (Bunt et al., 2010(Bunt et al., , 2012, place more emphasis on interactive acts such as stalling to hold the floor, assessing other discourse moves, suggesting or informing.

Both the AMI and ICSI corpora provide gold dialogue act labels, and Goo and Chen (2018) use the labels from the AMI corpus to develop a sentence-gated mechanism that jointly models the relationships between dialogue acts and topic summaries to improve abstractive meeting summarization. In particular, they show that dialogue acts of the type inform are more closely linked to summary-worthy material than acts such as stall or assess. Using LSTM, their model consists of three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder.

Dialogue acts have also been used to good effect for summarizing decisions. Fernández et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution. Then, key fragments of the decision related utterances are retained to form an extractive decision summary. Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006(Purver et al., , 2007.","(p11.0) While most work on abstractive meeting summarization implicitly assumes that conversation is merely a linear sequence of utterances without semantic relations between them, example (1) underscores the importance of semantic relations for conversational understanding. Drawing on similar insights, certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993;Lascarides and Asher, 2008), to improve summarization. Accounts like RST and SDRT maintain that in a coherent conversation, each (roughly) clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, Contrast, etc. to reflect its contribution to the larger discourse. Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations. Figure 2 il-  , 2021a). The * indicates summary-level (with sentence split) ROUGE-L score (Lin, 2004).

(p11.1) lustrates a possible SDRT graph for example (1).

(p11.2) To the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries. They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora. Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices. Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary. A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.

(p11.3) An alternative approach to discourse interpretation that developed largely independently of RST and SDRT is dialogue act classification. Detailing the differences between the approaches is out of the scope of this paper, but in a nutshell, dialogue acts provide a shallower notion of discourse structure (Jurafsky et al., 1997) in that they do not entail a full graph structure over a conversation. On the other hand, systems for dialogue act labeling, such as DAMSL (Allen and Core, 1997; Core and Allen, 1997) or DiAML (Bunt et al., 2010(Bunt et al., , 2012, place more emphasis on interactive acts such as stalling to hold the floor, assessing other discourse moves, suggesting or informing.

(p11.4) Both the AMI and ICSI corpora provide gold dialogue act labels, and Goo and Chen (2018) use the labels from the AMI corpus to develop a sentence-gated mechanism that jointly models the relationships between dialogue acts and topic summaries to improve abstractive meeting summarization. In particular, they show that dialogue acts of the type inform are more closely linked to summary-worthy material than acts such as stall or assess. Using LSTM, their model consists of three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder.

(p11.5) Dialogue acts have also been used to good effect for summarizing decisions. Fernández et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution. Then, key fragments of the decision related utterances are retained to form an extractive decision summary. Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006(Purver et al., , 2007.","[['b21', None, 'b1', 'b17'], [], ['b31', None, 'b2'], ['b0', None, 'b10', 'b11'], [], [None, 'b9']]","[['b21', None, 'b1', 'b17'], [], ['b31', None, 'b2'], ['b0', None, 'b10', 'b11'], [], [None, 'b9']]",13,"1. While most work on abstractive meeting summarization implicitly assumes that conversation is merely a linear sequence of utterances without semantic relations between them, example (1) underscores the importance of semantic relations for conversational understanding.
2. Drawing on similar insights, certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993;Lascarides and Asher, 2008), to improve summarization.
3. Accounts like RST and SDRT maintain that in a coherent conversation, each (roughly) clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, Contrast, etc. to reflect its contribution to the larger discourse.
4. Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations.
5. Figure 2 il-  , 2021a). The * indicates summary-level (with sentence split) ROUGE-L score (Lin, 2004).lustrates a possible SDRT graph for example (1).
6. To the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries.
7. They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora.
8. Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices.
9. Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary.
10. A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.
11. An alternative approach to discourse interpretation that developed largely independently of RST and SDRT is dialogue act classification.
12. Detailing the differences between the approaches is out of the scope of this paper, but in a nutshell, dialogue acts provide a shallower notion of discourse structure (Jurafsky et al., 1997) in that they do not entail a full graph structure over a conversation.
13. On the other hand, systems for dialogue act labeling, such as DAMSL (Allen and Core, 1997; Core and Allen, 1997) or DiAML (Bunt et al., 2010(Bunt et al., , 2012, place more emphasis on interactive acts such as stalling to hold the floor, assessing other discourse moves, suggesting or informing.
14. Both the AMI and ICSI corpora provide gold dialogue act labels, and Goo and Chen (2018) use the labels from the AMI corpus to develop a sentence-gated mechanism that jointly models the relationships between dialogue acts and topic summaries to improve abstractive meeting summarization.
15. In particular, they show that dialogue acts of the type inform are more closely linked to summary-worthy material than acts such as stall or assess.
16. Using LSTM, their model consists of three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder.
17. Dialogue acts have also been used to good effect for summarizing decisions.
18. Fernández et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution.
19. Then, key fragments of the decision related utterances are retained to form an extractive decision summary.
20. Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006(Purver et al., , 2007."
251402499,Abstractive Meeting Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/6dd2ffed96ab44004b2e4cb1bf36a44daae14d42,s12,Multimodality,"['p12.0', 'p12.1', 'p12.2', 'p12.3']","[""Meetings tend to take place in shared visual contexts, allowing important information to be conveyed through gestures, facial expressions, head poses, eye gaze, and so on. It is thus reasonable to think that harnessing this information could lead to improved performance over a summarization system that draws on speech transcripts alone. Indeed, Li et al. (2019) has shown that the Visual Focus Of Attention (VFOA), which is estimated based on a participant's head orientation and eye gaze, can help a multi-modal summarization system determine salient utterances, thereby improving the quality of abstractive meeting summaries. In this work, VFOA is integrated with attention mechanisms in a PGN text decoder and topic segmenter trained in a multi-task learning setting."", 'In the context of extractive meeting summarization, Erol et al. (2003) exploits multimodality to select segments of meeting video recordings in order to facilitate browsing of the videos for important information. Along with TF-IDF text analysis of the speech transcript, their system attends to certain visual motion activities (e.g., someone entering the meeting room or standing up to make a presentation) and to acoustic features (e.g., speech volume and degree of interaction) to enhance detection of summary-worthy events. Similar ideas can be found in Xie and Liu (2010);Nihei et al. (2016Nihei et al. ( , 2018; Nihei and Nakano (2019).', 'Of course, enhancing meeting transcripts with information to improve interpretation comes at a cost: most annotations are performed manually and require linguistic expertise to do well, and annotations produced automatically, as in Shi and Huang (2019) or Shang et al. (2020b), can add uncertainties to the data. It is perhaps thus unsurprising that the last couple of years have seen a shift away from Interpretation-focused methods presented in this section towards Generation methods, as shown in Table 1. Still, certain approaches showed promising results, especially Feng et al.', '(2020) and Li et al. (2019), and arguably, Interpretation approaches offer other advantages that should encourage us to explore them further. Discourse graphs, for example, can provide input to graph-based networks, helping to bypass limits on input document length imposed by some Generation approaches (see Section 5.3). And a rich source representation can arguably make a single transcript more valuable for training, partially offsetting data scarcity described in Section 3.']","Meetings tend to take place in shared visual contexts, allowing important information to be conveyed through gestures, facial expressions, head poses, eye gaze, and so on. It is thus reasonable to think that harnessing this information could lead to improved performance over a summarization system that draws on speech transcripts alone. Indeed, Li et al. (2019) has shown that the Visual Focus Of Attention (VFOA), which is estimated based on a participant's head orientation and eye gaze, can help a multi-modal summarization system determine salient utterances, thereby improving the quality of abstractive meeting summaries. In this work, VFOA is integrated with attention mechanisms in a PGN text decoder and topic segmenter trained in a multi-task learning setting.

In the context of extractive meeting summarization, Erol et al. (2003) exploits multimodality to select segments of meeting video recordings in order to facilitate browsing of the videos for important information. Along with TF-IDF text analysis of the speech transcript, their system attends to certain visual motion activities (e.g., someone entering the meeting room or standing up to make a presentation) and to acoustic features (e.g., speech volume and degree of interaction) to enhance detection of summary-worthy events. Similar ideas can be found in Xie and Liu (2010);Nihei et al. (2016Nihei et al. ( , 2018; Nihei and Nakano (2019).

Of course, enhancing meeting transcripts with information to improve interpretation comes at a cost: most annotations are performed manually and require linguistic expertise to do well, and annotations produced automatically, as in Shi and Huang (2019) or Shang et al. (2020b), can add uncertainties to the data. It is perhaps thus unsurprising that the last couple of years have seen a shift away from Interpretation-focused methods presented in this section towards Generation methods, as shown in Table 1. Still, certain approaches showed promising results, especially Feng et al.

(2020) and Li et al. (2019), and arguably, Interpretation approaches offer other advantages that should encourage us to explore them further. Discourse graphs, for example, can provide input to graph-based networks, helping to bypass limits on input document length imposed by some Generation approaches (see Section 5.3). And a rich source representation can arguably make a single transcript more valuable for training, partially offsetting data scarcity described in Section 3.","(p12.0) Meetings tend to take place in shared visual contexts, allowing important information to be conveyed through gestures, facial expressions, head poses, eye gaze, and so on. It is thus reasonable to think that harnessing this information could lead to improved performance over a summarization system that draws on speech transcripts alone. Indeed, Li et al. (2019) has shown that the Visual Focus Of Attention (VFOA), which is estimated based on a participant's head orientation and eye gaze, can help a multi-modal summarization system determine salient utterances, thereby improving the quality of abstractive meeting summaries. In this work, VFOA is integrated with attention mechanisms in a PGN text decoder and topic segmenter trained in a multi-task learning setting.

(p12.1) In the context of extractive meeting summarization, Erol et al. (2003) exploits multimodality to select segments of meeting video recordings in order to facilitate browsing of the videos for important information. Along with TF-IDF text analysis of the speech transcript, their system attends to certain visual motion activities (e.g., someone entering the meeting room or standing up to make a presentation) and to acoustic features (e.g., speech volume and degree of interaction) to enhance detection of summary-worthy events. Similar ideas can be found in Xie and Liu (2010);Nihei et al. (2016Nihei et al. ( , 2018; Nihei and Nakano (2019).

(p12.2) Of course, enhancing meeting transcripts with information to improve interpretation comes at a cost: most annotations are performed manually and require linguistic expertise to do well, and annotations produced automatically, as in Shi and Huang (2019) or Shang et al. (2020b), can add uncertainties to the data. It is perhaps thus unsurprising that the last couple of years have seen a shift away from Interpretation-focused methods presented in this section towards Generation methods, as shown in Table 1. Still, certain approaches showed promising results, especially Feng et al.

(p12.3) (2020) and Li et al. (2019), and arguably, Interpretation approaches offer other advantages that should encourage us to explore them further. Discourse graphs, for example, can provide input to graph-based networks, helping to bypass limits on input document length imposed by some Generation approaches (see Section 5.3). And a rich source representation can arguably make a single transcript more valuable for training, partially offsetting data scarcity described in Section 3.","[['b16'], [None, 'b46'], ['b31', 'b30'], ['b16']]","[['b16'], [None, 'b46'], ['b31', 'b30'], ['b16']]",6,"1. Meetings tend to take place in shared visual contexts, allowing important information to be conveyed through gestures, facial expressions, head poses, eye gaze, and so on.
2. It is thus reasonable to think that harnessing this information could lead to improved performance over a summarization system that draws on speech transcripts alone.
3. Indeed, Li et al. (2019) has shown that the Visual Focus Of Attention (VFOA), which is estimated based on a participant's head orientation and eye gaze, can help a multi-modal summarization system determine salient utterances, thereby improving the quality of abstractive meeting summaries.
4. In this work, VFOA is integrated with attention mechanisms in a PGN text decoder and topic segmenter trained in a multi-task learning setting.
5. In the context of extractive meeting summarization, Erol et al. (2003) exploits multimodality to select segments of meeting video recordings in order to facilitate browsing of the videos for important information.
6. Along with TF-IDF text analysis of the speech transcript, their system attends to certain visual motion activities (e.g., someone entering the meeting room or standing up to make a presentation) and to acoustic features (e.g., speech volume and degree of interaction) to enhance detection of summary-worthy events.
7. Similar ideas can be found in Xie and Liu (2010);Nihei et al. (2016Nihei et al. ( , 2018; Nihei and Nakano (2019).
8. Of course, enhancing meeting transcripts with information to improve interpretation comes at a cost: most annotations are performed manually and require linguistic expertise to do well, and annotations produced automatically, as in Shi and Huang (2019) or Shang et al. (2020b), can add uncertainties to the data.
9. It is perhaps thus unsurprising that the last couple of years have seen a shift away from Interpretation-focused methods presented in this section towards Generation methods, as shown in Table 1.
10. Still, certain approaches showed promising results, especially Feng et al.(2020) and Li et al. (2019), and arguably, Interpretation approaches offer other advantages that should encourage us to explore them further.
11. Discourse graphs, for example, can provide input to graph-based networks, helping to bypass limits on input document length imposed by some Generation approaches (see Section 5.3).
12. And a rich source representation can arguably make a single transcript more valuable for training, partially offsetting data scarcity described in Section 3."
251402499,Abstractive Meeting Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/6dd2ffed96ab44004b2e4cb1bf36a44daae14d42,s17,Query-related clusters,"['p17.0', 'p17.1']","['To deal with the problem of heterogeneous meeting formats as well as the subjectivity inherent in summary-production, Mehdad et al. (2014) proposed a query-based meeting summarization system that allows for the creation of different summaries for different user interests expressed via queries. Given a phrasal query, the system first extracts the utterances that are most relevant to the keywords of the query and of the main meeting content. Then, these utterances are compressed into summary sentences with an MSCG. Since there were no human-written query-based summaries for AMI at the time, this system was evaluated manually. The QMSum dataset (Zhong et al., 2021) provides multiple query-summary pairs for each meeting in the AMI and ICSI corpora, covering both general and specific points people might be interest in. They also propose a system with two distinct modules: a locator based on pointer network (Vinyals et al., 2015) and a neural seq2seq summarizer. The former is used to locate the query-related utterances in the meeting transcripts, and the latter is meant to summarize selected utterances into summaries.', 'While versatile in the tasks they could potentially be used to solve, Transformation-focused systems tend to perform worse than Interpretation and Generation-focused systems (cf. Table 1). This can be attributed to the relative age of those methods, as well as their conceptual simplicity that focus on extractive and unsupervised summarization (Shang et al., 2018;Krishna et al., 2021). The spontaneous nature of meetings can lead to meandering structures and information dispersed over a variety of (not necessarily sequential) turns: participants have side conversations, they forget things and come back, they get interrupted, and so on. All of this makes meeting transcripts harder to accurately segment than traditional documents (Xing and Carenini, 2021). Finding ways to preprocess a text with an eye to creating high-quality topic-based segmentations to feed to Interpretation or Generation-focused systems could be a way to improve the state of the art.']","To deal with the problem of heterogeneous meeting formats as well as the subjectivity inherent in summary-production, Mehdad et al. (2014) proposed a query-based meeting summarization system that allows for the creation of different summaries for different user interests expressed via queries. Given a phrasal query, the system first extracts the utterances that are most relevant to the keywords of the query and of the main meeting content. Then, these utterances are compressed into summary sentences with an MSCG. Since there were no human-written query-based summaries for AMI at the time, this system was evaluated manually. The QMSum dataset (Zhong et al., 2021) provides multiple query-summary pairs for each meeting in the AMI and ICSI corpora, covering both general and specific points people might be interest in. They also propose a system with two distinct modules: a locator based on pointer network (Vinyals et al., 2015) and a neural seq2seq summarizer. The former is used to locate the query-related utterances in the meeting transcripts, and the latter is meant to summarize selected utterances into summaries.

While versatile in the tasks they could potentially be used to solve, Transformation-focused systems tend to perform worse than Interpretation and Generation-focused systems (cf. Table 1). This can be attributed to the relative age of those methods, as well as their conceptual simplicity that focus on extractive and unsupervised summarization (Shang et al., 2018;Krishna et al., 2021). The spontaneous nature of meetings can lead to meandering structures and information dispersed over a variety of (not necessarily sequential) turns: participants have side conversations, they forget things and come back, they get interrupted, and so on. All of this makes meeting transcripts harder to accurately segment than traditional documents (Xing and Carenini, 2021). Finding ways to preprocess a text with an eye to creating high-quality topic-based segmentations to feed to Interpretation or Generation-focused systems could be a way to improve the state of the art.","(p17.0) To deal with the problem of heterogeneous meeting formats as well as the subjectivity inherent in summary-production, Mehdad et al. (2014) proposed a query-based meeting summarization system that allows for the creation of different summaries for different user interests expressed via queries. Given a phrasal query, the system first extracts the utterances that are most relevant to the keywords of the query and of the main meeting content. Then, these utterances are compressed into summary sentences with an MSCG. Since there were no human-written query-based summaries for AMI at the time, this system was evaluated manually. The QMSum dataset (Zhong et al., 2021) provides multiple query-summary pairs for each meeting in the AMI and ICSI corpora, covering both general and specific points people might be interest in. They also propose a system with two distinct modules: a locator based on pointer network (Vinyals et al., 2015) and a neural seq2seq summarizer. The former is used to locate the query-related utterances in the meeting transcripts, and the latter is meant to summarize selected utterances into summaries.

(p17.1) While versatile in the tasks they could potentially be used to solve, Transformation-focused systems tend to perform worse than Interpretation and Generation-focused systems (cf. Table 1). This can be attributed to the relative age of those methods, as well as their conceptual simplicity that focus on extractive and unsupervised summarization (Shang et al., 2018;Krishna et al., 2021). The spontaneous nature of meetings can lead to meandering structures and information dispersed over a variety of (not necessarily sequential) turns: participants have side conversations, they forget things and come back, they get interrupted, and so on. All of this makes meeting transcripts harder to accurately segment than traditional documents (Xing and Carenini, 2021). Finding ways to preprocess a text with an eye to creating high-quality topic-based segmentations to feed to Interpretation or Generation-focused systems could be a way to improve the state of the art.","[['b40', 'b58'], ['b28', 'b47', None]]","[['b40', 'b58'], ['b28', 'b47', None]]",5,"1. To deal with the problem of heterogeneous meeting formats as well as the subjectivity inherent in summary-production, Mehdad et al. (2014) proposed a query-based meeting summarization system that allows for the creation of different summaries for different user interests expressed via queries.
2. Given a phrasal query, the system first extracts the utterances that are most relevant to the keywords of the query and of the main meeting content.
3. Then, these utterances are compressed into summary sentences with an MSCG.
4. Since there were no human-written query-based summaries for AMI at the time, this system was evaluated manually.
5. The QMSum dataset (Zhong et al., 2021) provides multiple query-summary pairs for each meeting in the AMI and ICSI corpora, covering both general and specific points people might be interest in.
6. They also propose a system with two distinct modules: a locator based on pointer network (Vinyals et al., 2015) and a neural seq2seq summarizer.
7. The former is used to locate the query-related utterances in the meeting transcripts, and the latter is meant to summarize selected utterances into summaries.
8. While versatile in the tasks they could potentially be used to solve, Transformation-focused systems tend to perform worse than Interpretation and Generation-focused systems (cf. Table 1).
9. This can be attributed to the relative age of those methods, as well as their conceptual simplicity that focus on extractive and unsupervised summarization (Shang et al., 2018;Krishna et al., 2021).
10. The spontaneous nature of meetings can lead to meandering structures and information dispersed over a variety of (not necessarily sequential) turns: participants have side conversations, they forget things and come back, they get interrupted, and so on.
11. All of this makes meeting transcripts harder to accurately segment than traditional documents (Xing and Carenini, 2021).
12. Finding ways to preprocess a text with an eye to creating high-quality topic-based segmentations to feed to Interpretation or Generation-focused systems could be a way to improve the state of the art."
251402499,Abstractive Meeting Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/6dd2ffed96ab44004b2e4cb1bf36a44daae14d42,s19,Long input processing,"['p19.0', 'p19.1']","['A straightforward solution to the length problem is to segment a long document into smaller segments to be processed. Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary. Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework. Within each stage, it first splits the source input into sufficiently short segments. Coarse summaries are then generated for each segment and then concatenated as the input to the next stage. This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.', 'While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem. Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022). Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions. Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems. Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription. The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure. First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors. That sequence is then processed by the turn-level encoder. The transformer decoder makes use of both levels of representation via cross-attention layers. Rohde et al. (2021) propose Hierachical Attention Transformer (HAT). Utterances are first prepended with a special BOS token. Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations. Finally, the decoder leverages the outputs at both levels to produce a final summary. Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.']","A straightforward solution to the length problem is to segment a long document into smaller segments to be processed. Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary. Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework. Within each stage, it first splits the source input into sufficiently short segments. Coarse summaries are then generated for each segment and then concatenated as the input to the next stage. This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.

While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem. Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022). Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions. Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems. Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription. The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure. First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors. That sequence is then processed by the turn-level encoder. The transformer decoder makes use of both levels of representation via cross-attention layers. Rohde et al. (2021) propose Hierachical Attention Transformer (HAT). Utterances are first prepended with a special BOS token. Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations. Finally, the decoder leverages the outputs at both levels to produce a final summary. Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.","(p19.0) A straightforward solution to the length problem is to segment a long document into smaller segments to be processed. Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary. Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework. Within each stage, it first splits the source input into sufficiently short segments. Coarse summaries are then generated for each segment and then concatenated as the input to the next stage. This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.

(p19.1) While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem. Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022). Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions. Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems. Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription. The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure. First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors. That sequence is then processed by the turn-level encoder. The transformer decoder makes use of both levels of representation via cross-attention layers. Rohde et al. (2021) propose Hierachical Attention Transformer (HAT). Utterances are first prepended with a special BOS token. Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations. Finally, the decoder leverages the outputs at both levels to produce a final summary. Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.","[[None, 'b54'], ['b56', 'b48', 'b60', 'b4', 'b26', None, 'b22']]","[[None, 'b54'], ['b56', 'b48', 'b60', 'b4', 'b26', None, 'b22']]",9,"1. A straightforward solution to the length problem is to segment a long document into smaller segments to be processed.
2. Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary.
3. Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework.
4. Within each stage, it first splits the source input into sufficiently short segments.
5. Coarse summaries are then generated for each segment and then concatenated as the input to the next stage.
6. This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.
7. While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem.
8. Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022).
9. Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions.
10. Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems.
11. Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription.
12. The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure.
13. First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors.
14. That sequence is then processed by the turn-level encoder.
15. The transformer decoder makes use of both levels of representation via cross-attention layers.
16. Rohde et al. (2021) propose Hierachical Attention Transformer (HAT).
17. Utterances are first prepended with a special BOS token.
18. Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations.
19. Finally, the decoder leverages the outputs at both levels to produce a final summary.
20. Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings."
251402499,Abstractive Meeting Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/6dd2ffed96ab44004b2e4cb1bf36a44daae14d42,s14,Topic segments,['p14.0'],"['Topic segmentation involves dividing text into topically coherent segments of sentences. With the help of a topic segmenter, such as LCSeg (Galley et al., 2003), the work of Banerjee et al. (2015) first separates a meeting transcript into several topic segments. The dependency parse trees of all of the sentences in a topic segment are then merged into a global dependency graph from which the most informative and well-formed subgraph (i.e. sentence) is selected. Finally, the selected sentences are combined to generate a summary. In the larger dialogue domain, topic segments are also useful for summarizing nurse-topatient dialogues (Liu et al., 2019) and unstructured daily chats (Chen and Yang, 2020).']","Topic segmentation involves dividing text into topically coherent segments of sentences. With the help of a topic segmenter, such as LCSeg (Galley et al., 2003), the work of Banerjee et al. (2015) first separates a meeting transcript into several topic segments. The dependency parse trees of all of the sentences in a topic segment are then merged into a global dependency graph from which the most informative and well-formed subgraph (i.e. sentence) is selected. Finally, the selected sentences are combined to generate a summary. In the larger dialogue domain, topic segments are also useful for summarizing nurse-topatient dialogues (Liu et al., 2019) and unstructured daily chats (Chen and Yang, 2020).","(p14.0) Topic segmentation involves dividing text into topically coherent segments of sentences. With the help of a topic segmenter, such as LCSeg (Galley et al., 2003), the work of Banerjee et al. (2015) first separates a meeting transcript into several topic segments. The dependency parse trees of all of the sentences in a topic segment are then merged into a global dependency graph from which the most informative and well-formed subgraph (i.e. sentence) is selected. Finally, the selected sentences are combined to generate a summary. In the larger dialogue domain, topic segments are also useful for summarizing nurse-topatient dialogues (Liu et al., 2019) and unstructured daily chats (Chen and Yang, 2020).","[[None, 'b18', 'b14', 'b3']]","[[None, 'b18', 'b14', 'b3']]",4,"1. Topic segmentation involves dividing text into topically coherent segments of sentences.
2. With the help of a topic segmenter, such as LCSeg (Galley et al., 2003), the work of Banerjee et al. (2015) first separates a meeting transcript into several topic segments.
3. The dependency parse trees of all of the sentences in a topic segment are then merged into a global dependency graph from which the most informative and well-formed subgraph (i.e. sentence) is selected.
4. Finally, the selected sentences are combined to generate a summary.
5. In the larger dialogue domain, topic segments are also useful for summarizing nurse-topatient dialogues (Liu et al., 2019) and unstructured daily chats (Chen and Yang, 2020)."
251719280,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,Computer Science,https://www.semanticscholar.org/paper/a814b76e589ef27e3f4af379d319d02d2110faa1,s16,B.5 Complexity of Natural Language and SQL Query Pairs,['p16.0'],"[""In terms of the complexity for SQL queries, Finegan-Dollak et al. (2018) find that models perform better on shorter SQL queries than longer SQL queries, which indicates that shorter SQL queries are easier in general. Yu et al. (2018c) define the SQL hardness as the number of SQL components. The SQL query is harder when it contains more SQL keywords such as GROUP BY and nested subqueries. Yu et al. (2018c)  In terms of the complexity of natural utterance, there is no qualitative measure of how hard the utterance is. Intuitively, models' performance can decrease when faced with longer questions from users. However, the information conveyed in longer sentences can be more complete, while there can be ambiguity in shorter sentences. Besides, there can be domain-specific phrases that confuse the model in both short and long utterances (Suhr et al., 2020). Thus, researchers need to consider various perspectives to determine the complexity of natural utterance.  Nguyen et al., 2020) and José and Cozman (2021) translate all the English questions in Spider into Chinese, Vietnamese and Portuguese, respectively. TableQA (Sun et al., 2020) follows the data collection method from WikiSQL, while DuSQL (Wang et al., 2020c) follows Spider. Both TableQA and DuSQL collect Chinese utterance and SQL query pairs across different domains. Chen et al. (2021a) propose a Chinese domain-specific dataset, ESQL.""]","In terms of the complexity for SQL queries, Finegan-Dollak et al. (2018) find that models perform better on shorter SQL queries than longer SQL queries, which indicates that shorter SQL queries are easier in general. Yu et al. (2018c) define the SQL hardness as the number of SQL components. The SQL query is harder when it contains more SQL keywords such as GROUP BY and nested subqueries. Yu et al. (2018c)  In terms of the complexity of natural utterance, there is no qualitative measure of how hard the utterance is. Intuitively, models' performance can decrease when faced with longer questions from users. However, the information conveyed in longer sentences can be more complete, while there can be ambiguity in shorter sentences. Besides, there can be domain-specific phrases that confuse the model in both short and long utterances (Suhr et al., 2020). Thus, researchers need to consider various perspectives to determine the complexity of natural utterance.  Nguyen et al., 2020) and José and Cozman (2021) translate all the English questions in Spider into Chinese, Vietnamese and Portuguese, respectively. TableQA (Sun et al., 2020) follows the data collection method from WikiSQL, while DuSQL (Wang et al., 2020c) follows Spider. Both TableQA and DuSQL collect Chinese utterance and SQL query pairs across different domains. Chen et al. (2021a) propose a Chinese domain-specific dataset, ESQL.","(p16.0) In terms of the complexity for SQL queries, Finegan-Dollak et al. (2018) find that models perform better on shorter SQL queries than longer SQL queries, which indicates that shorter SQL queries are easier in general. Yu et al. (2018c) define the SQL hardness as the number of SQL components. The SQL query is harder when it contains more SQL keywords such as GROUP BY and nested subqueries. Yu et al. (2018c)  In terms of the complexity of natural utterance, there is no qualitative measure of how hard the utterance is. Intuitively, models' performance can decrease when faced with longer questions from users. However, the information conveyed in longer sentences can be more complete, while there can be ambiguity in shorter sentences. Besides, there can be domain-specific phrases that confuse the model in both short and long utterances (Suhr et al., 2020). Thus, researchers need to consider various perspectives to determine the complexity of natural utterance.  Nguyen et al., 2020) and José and Cozman (2021) translate all the English questions in Spider into Chinese, Vietnamese and Portuguese, respectively. TableQA (Sun et al., 2020) follows the data collection method from WikiSQL, while DuSQL (Wang et al., 2020c) follows Spider. Both TableQA and DuSQL collect Chinese utterance and SQL query pairs across different domains. Chen et al. (2021a) propose a Chinese domain-specific dataset, ESQL.","[['b28', 'b14', None, 'b11', 'b9']]","[['b28', 'b14', None, 'b11', 'b9']]",5,"1. In terms of the complexity for SQL queries, Finegan-Dollak et al. (2018) find that models perform better on shorter SQL queries than longer SQL queries, which indicates that shorter SQL queries are easier in general.
2. Yu et al. (2018c) define the SQL hardness as the number of SQL components.
3. The SQL query is harder when it contains more SQL keywords such as GROUP BY and nested subqueries.
4. Yu et al. (2018c) In terms of the complexity of natural utterance, there is no qualitative measure of how hard the utterance is.
5. Intuitively, models' performance can decrease when faced with longer questions from users.
6. However, the information conveyed in longer sentences can be more complete, while there can be ambiguity in shorter sentences.
7. Besides, there can be domain-specific phrases that confuse the model in both short and long utterances (Suhr et al., 2020).
8. Thus, researchers need to consider various perspectives to determine the complexity of natural utterance.
9. Nguyen et al., 2020) and José and Cozman (2021) translate all the English questions in Spider into Chinese, Vietnamese and Portuguese, respectively.
10. TableQA (Sun et al., 2020) follows the data collection method from WikiSQL, while DuSQL (Wang et al., 2020c) follows Spider.
11. Both TableQA and DuSQL collect Chinese utterance and SQL query pairs across different domains.
12. Chen et al. (2021a) propose a Chinese domain-specific dataset, ESQL."
251719280,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,Computer Science,https://www.semanticscholar.org/paper/a814b76e589ef27e3f4af379d319d02d2110faa1,s10,Evaluation,['p10.0'],"['Metrics Table 4 shows widely used automatic evaluation metrics for the text-to-SQL task. Early works evaluate SQL queries by comparing the database querying results executed from the predicted SQL query and the ground-truth (or gold) SQL query (Zelle and Mooney, 1996;Yaghmazadeh et al., 2017) (2018) propose SQL query split, where no SQL query is allowed to appear in more than one set among the train, dev, and test sets. Furthermore, Yu et al. (2018c) propose a database split, where the model does not see the databases in the test set in its training time. Other splitting methods also exist to help different research topics (Shaw et al., 2021;.']","Metrics Table 4 shows widely used automatic evaluation metrics for the text-to-SQL task. Early works evaluate SQL queries by comparing the database querying results executed from the predicted SQL query and the ground-truth (or gold) SQL query (Zelle and Mooney, 1996;Yaghmazadeh et al., 2017) (2018) propose SQL query split, where no SQL query is allowed to appear in more than one set among the train, dev, and test sets. Furthermore, Yu et al. (2018c) propose a database split, where the model does not see the databases in the test set in its training time. Other splitting methods also exist to help different research topics (Shaw et al., 2021;.","(p10.0) Metrics Table 4 shows widely used automatic evaluation metrics for the text-to-SQL task. Early works evaluate SQL queries by comparing the database querying results executed from the predicted SQL query and the ground-truth (or gold) SQL query (Zelle and Mooney, 1996;Yaghmazadeh et al., 2017) (2018) propose SQL query split, where no SQL query is allowed to appear in more than one set among the train, dev, and test sets. Furthermore, Yu et al. (2018c) propose a database split, where the model does not see the databases in the test set in its training time. Other splitting methods also exist to help different research topics (Shaw et al., 2021;.","[['b2', None, 'b30', 'b28']]","[['b2', None, 'b30', 'b28']]",4,"1. Metrics Table 4 shows widely used automatic evaluation metrics for the text-to-SQL task.
2. Early works evaluate SQL queries by comparing the database querying results executed from the predicted SQL query and the ground-truth (or gold)
3. SQL query (Zelle and Mooney, 1996;Yaghmazadeh et al., 2017) (2018) propose SQL query split, where no SQL query is allowed to appear in more than one set among the train, dev, and test sets.
4. Furthermore, Yu et al. (2018c) propose a database split, where the model does not see the databases in the test set in its training time.
5. Other splitting methods also exist to help different research topics (Shaw et al., 2021;."
251719280,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,Computer Science,https://www.semanticscholar.org/paper/a814b76e589ef27e3f4af379d319d02d2110faa1,s7,Graph-based Methods,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4', 'p7.5', 'p7.6', 'p7.7']","['Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures. As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing. As a result, modeling DB schema receives little attention. Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas. Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure. In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation. RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.', 'Graphs have also been used to encode questions together with DB schema. Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema. Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently. SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking. Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance. To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.', 'Finally, graph-based techniques are also exploited in context-dependent text-to-SQL. For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.', 'Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.', 'RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns. DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.', 'Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task. PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas. The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021). Other methods adjust the embeddings by PLMs. On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings. Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.', 'HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on. After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction. Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question. They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question. Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns. By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.', 'Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task. For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT. Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered. GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.']","Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures. As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing. As a result, modeling DB schema receives little attention. Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas. Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure. In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation. RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.

Graphs have also been used to encode questions together with DB schema. Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema. Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently. SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking. Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance. To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.

Finally, graph-based techniques are also exploited in context-dependent text-to-SQL. For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.

Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.

RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns. DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.

Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task. PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas. The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021). Other methods adjust the embeddings by PLMs. On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings. Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.

HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on. After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction. Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question. They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question. Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns. By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.

Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task. For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT. Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered. GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.","(p7.0) Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures. As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing. As a result, modeling DB schema receives little attention. Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas. Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure. In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation. RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.

(p7.1) Graphs have also been used to encode questions together with DB schema. Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema. Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently. SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking. Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance. To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.

(p7.2) Finally, graph-based techniques are also exploited in context-dependent text-to-SQL. For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.

(p7.3) Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.

(p7.4) RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns. DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.

(p7.5) Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task. PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas. The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021). Other methods adjust the embeddings by PLMs. On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings. Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.

(p7.6) HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on. After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction. Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question. They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question. Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns. By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.

(p7.7) Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task. For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT. Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered. GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.","[[None, 'b36'], [None], [], [None, 'b40'], ['b38', 'b15'], ['b31', None, 'b40'], ['b42'], ['b43']]","[[None, 'b36'], [None], [], [None, 'b40'], ['b38', 'b15'], ['b31', None, 'b40'], ['b42'], ['b43']]",12,"1. Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures.
2. As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing.
3. As a result, modeling DB schema receives little attention.
4. Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas.
5. Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure.
6. In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation.
7. RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.
8. Graphs have also been used to encode questions together with DB schema.
9. Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema.
10. Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently.
11. SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking.
12. Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance.
13. To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.
14. Finally, graph-based techniques are also exploited in context-dependent text-to-SQL.
15. For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.
16. Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.
17. RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns.
18. DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.
19. Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task.
20. PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas.
21. The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021).
22. Other methods adjust the embeddings by PLMs.
23. On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings.
24. Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.
25. HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on.
26. After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction.
27. Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question.
28. They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question.
29. Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns.
30. By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.
31. Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task.
32. For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT.
33. Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered.
34. GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation."
251719280,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,Computer Science,https://www.semanticscholar.org/paper/a814b76e589ef27e3f4af379d319d02d2110faa1,s8,Decoding,"['p8.0', 'p8.1', 'p8.2', 'p8.3', 'p8.4', 'p8.5', 'p8.6', 'p8.7']","['Various methods have been proposed for decoding to achieve a fine-grained and easier process for SQL generation and bridge the gap between natural language and SQL queries. As shown in Table 3 Table 9 in Appendix D. IR: Intermediate Representation. and other technologies.', 'Tree-based Seq2Tree (Dong and Lapata, 2016) employs a decoder that generates logical forms in a top-down manner. The components in the sub-tree are generated conditioned on their parents apart from the input question. Note that the syntax of the logical forms is implicitly learned from data for Seq2Tree. Similarly, Seq2AST (Yin and Neubig, 2017) uses an abstract syntax tree (AST) for decoding the target programming language, where the syntax is explicitly integrated with AST. Although both Seq2Tree (Dong and Lapata, 2016) and Seq2AST (Yin and Neubig, 2017) do not study text-to-SQL datasets, their uses of trees inspire tree-based decoding in text-to-SQL. SyntaxSQL-Net (Yu et al., 2018b) employs a tree-based decoding method specific to SQL syntax and recursively calls modules to predict different SQL components.', 'Sketch-based SQLNet (Xu et al., 2017) designs a sketch aligned with the SQL grammar, and SQL-Net only needs to fill in the slots in the sketch rather than predict both the output grammar and the content. Besides, the sketch captures the dependency of the predictions. Thus, the prediction of one slot is only conditioned on the slots it depends on, which avoids issues of the same SQL query with varied equivalent serializations. Dong and Lapata (2018) decompose the decoding into two stages, where the first decoder predicts a rough sketch, and the second decoder fills in the lowlevel details conditioned on the question and the sketch. Such coarse-to-fine decoding has also been adopted in other works such as IRNet (Guo et al., 2019). To address the complex SQL queries with nested structures, RYANSQL (Choi et al., 2021) recursively yields SELECT statements and uses a sketch-based slot filling for each of the SELECT statements.', 'Bottom-up Both the tree-based and the sketchbased decoding mechanisms can be viewed as top-down decoding mechanisms. Rubin and Berant (2021) use a bottom-up decoding mechanism. Given K trees of height t, the decoder scores trees with height t + 1 constructed by SQL grammar from the current beam, and K trees with the highest scores are kept. Then, a representation of the new K trees is generated and placed in the new beam.', 'Attention Mechanism To integrate the encoderside information at decoding, an attention score is computed and multiplied with hidden vectors from the encoder to get the context vector, which is then used to generate an output token (Dong and Lapata, 2016;Zhong et al., 2017). Variants of the attention mechanism have been used to better propagate the information encoded from questions and DB schemas to the decoder. SQLNet (Xu et al., 2017) designs column attention, where it uses hidden states from columns multiplied by embeddings for the question to calculate attention scores for a column given the question. Guo and Gao (2018)  Copy Mechanism Seq2AST (Yin and Neubig, 2017) and Seq2SQL (Zhong et al., 2017) employ the pointer network (Vinyals et al., 2015) to compute the probability of copying words from the input. Wang et al. (2018a) use types (e.g., columns, SQL operators, constant from questions) to explicitly restrict locations in the query to copy from and develop a new training objective to only copy from the first occurrence in the input. In addition, the copy mechanism is also adopted in context-dependent text-to-SQL task (Wang et al., 2020b).', 'Intermediate Representations Researchers use intermediate representations to bridge the gap between natural language and SQL queries. Inc-SQL (Shi et al., 2018)  However, the intermediate representations are usually designed for a specific dataset and cannot be easily adapted to others (Suhr et al., 2020). To construct a more generalized intermediate representation, Herzig et al. (2021) propose to omit tokens in the SQL query that do not align to any phrase in the utterance.', 'Inspired by the success of text-to-SQL task, intermediate representations are also studied for SPARQL, another executable language for database systems (Saparina and Osokin, 2021;Herzig et al., 2021).', ""Others PICARD (Scholak et al., 2021b) and UniSAr (Dou et al., 2022) set constraints to the decoder to prevent generating invalid tokens. Several methods adopt an execution-guided decoding mechanism to exclude non-executable partial SQL queries from the output candidates (Wang et al., 2018b;. Global-GNN (Bogin et al., 2019b) employs a separately trained discriminative model to rerank the top-K SQL queries in the decoder's output beam, which is to reason about the complete SQL queries instead of considering each word and DB schemas in isolation.  (2018); Lee (2019) use separate submodules to predict different SQL components, easing the difficulty of generating a complete SQL query. Chen et al. (2020b) employ a gate to select between the output sequence encoded for the question and the output sequence from the previous decoding steps at each step for SQL generation. Inspired by machine translation, Müller and Vlachos (2019) apply byte-pair encoding (BPE) (Sennrich et al., 2016) to compress SQL queries to shorter sequences guided by AST, reducing the difficulties in SQL generation.""]","Various methods have been proposed for decoding to achieve a fine-grained and easier process for SQL generation and bridge the gap between natural language and SQL queries. As shown in Table 3 Table 9 in Appendix D. IR: Intermediate Representation. and other technologies.

Tree-based Seq2Tree (Dong and Lapata, 2016) employs a decoder that generates logical forms in a top-down manner. The components in the sub-tree are generated conditioned on their parents apart from the input question. Note that the syntax of the logical forms is implicitly learned from data for Seq2Tree. Similarly, Seq2AST (Yin and Neubig, 2017) uses an abstract syntax tree (AST) for decoding the target programming language, where the syntax is explicitly integrated with AST. Although both Seq2Tree (Dong and Lapata, 2016) and Seq2AST (Yin and Neubig, 2017) do not study text-to-SQL datasets, their uses of trees inspire tree-based decoding in text-to-SQL. SyntaxSQL-Net (Yu et al., 2018b) employs a tree-based decoding method specific to SQL syntax and recursively calls modules to predict different SQL components.

Sketch-based SQLNet (Xu et al., 2017) designs a sketch aligned with the SQL grammar, and SQL-Net only needs to fill in the slots in the sketch rather than predict both the output grammar and the content. Besides, the sketch captures the dependency of the predictions. Thus, the prediction of one slot is only conditioned on the slots it depends on, which avoids issues of the same SQL query with varied equivalent serializations. Dong and Lapata (2018) decompose the decoding into two stages, where the first decoder predicts a rough sketch, and the second decoder fills in the lowlevel details conditioned on the question and the sketch. Such coarse-to-fine decoding has also been adopted in other works such as IRNet (Guo et al., 2019). To address the complex SQL queries with nested structures, RYANSQL (Choi et al., 2021) recursively yields SELECT statements and uses a sketch-based slot filling for each of the SELECT statements.

Bottom-up Both the tree-based and the sketchbased decoding mechanisms can be viewed as top-down decoding mechanisms. Rubin and Berant (2021) use a bottom-up decoding mechanism. Given K trees of height t, the decoder scores trees with height t + 1 constructed by SQL grammar from the current beam, and K trees with the highest scores are kept. Then, a representation of the new K trees is generated and placed in the new beam.

Attention Mechanism To integrate the encoderside information at decoding, an attention score is computed and multiplied with hidden vectors from the encoder to get the context vector, which is then used to generate an output token (Dong and Lapata, 2016;Zhong et al., 2017). Variants of the attention mechanism have been used to better propagate the information encoded from questions and DB schemas to the decoder. SQLNet (Xu et al., 2017) designs column attention, where it uses hidden states from columns multiplied by embeddings for the question to calculate attention scores for a column given the question. Guo and Gao (2018)  Copy Mechanism Seq2AST (Yin and Neubig, 2017) and Seq2SQL (Zhong et al., 2017) employ the pointer network (Vinyals et al., 2015) to compute the probability of copying words from the input. Wang et al. (2018a) use types (e.g., columns, SQL operators, constant from questions) to explicitly restrict locations in the query to copy from and develop a new training objective to only copy from the first occurrence in the input. In addition, the copy mechanism is also adopted in context-dependent text-to-SQL task (Wang et al., 2020b).

Intermediate Representations Researchers use intermediate representations to bridge the gap between natural language and SQL queries. Inc-SQL (Shi et al., 2018)  However, the intermediate representations are usually designed for a specific dataset and cannot be easily adapted to others (Suhr et al., 2020). To construct a more generalized intermediate representation, Herzig et al. (2021) propose to omit tokens in the SQL query that do not align to any phrase in the utterance.

Inspired by the success of text-to-SQL task, intermediate representations are also studied for SPARQL, another executable language for database systems (Saparina and Osokin, 2021;Herzig et al., 2021).

Others PICARD (Scholak et al., 2021b) and UniSAr (Dou et al., 2022) set constraints to the decoder to prevent generating invalid tokens. Several methods adopt an execution-guided decoding mechanism to exclude non-executable partial SQL queries from the output candidates (Wang et al., 2018b;. Global-GNN (Bogin et al., 2019b) employs a separately trained discriminative model to rerank the top-K SQL queries in the decoder's output beam, which is to reason about the complete SQL queries instead of considering each word and DB schemas in isolation.  (2018); Lee (2019) use separate submodules to predict different SQL components, easing the difficulty of generating a complete SQL query. Chen et al. (2020b) employ a gate to select between the output sequence encoded for the question and the output sequence from the previous decoding steps at each step for SQL generation. Inspired by machine translation, Müller and Vlachos (2019) apply byte-pair encoding (BPE) (Sennrich et al., 2016) to compress SQL queries to shorter sequences guided by AST, reducing the difficulties in SQL generation.","(p8.0) Various methods have been proposed for decoding to achieve a fine-grained and easier process for SQL generation and bridge the gap between natural language and SQL queries. As shown in Table 3 Table 9 in Appendix D. IR: Intermediate Representation. and other technologies.

(p8.1) Tree-based Seq2Tree (Dong and Lapata, 2016) employs a decoder that generates logical forms in a top-down manner. The components in the sub-tree are generated conditioned on their parents apart from the input question. Note that the syntax of the logical forms is implicitly learned from data for Seq2Tree. Similarly, Seq2AST (Yin and Neubig, 2017) uses an abstract syntax tree (AST) for decoding the target programming language, where the syntax is explicitly integrated with AST. Although both Seq2Tree (Dong and Lapata, 2016) and Seq2AST (Yin and Neubig, 2017) do not study text-to-SQL datasets, their uses of trees inspire tree-based decoding in text-to-SQL. SyntaxSQL-Net (Yu et al., 2018b) employs a tree-based decoding method specific to SQL syntax and recursively calls modules to predict different SQL components.

(p8.2) Sketch-based SQLNet (Xu et al., 2017) designs a sketch aligned with the SQL grammar, and SQL-Net only needs to fill in the slots in the sketch rather than predict both the output grammar and the content. Besides, the sketch captures the dependency of the predictions. Thus, the prediction of one slot is only conditioned on the slots it depends on, which avoids issues of the same SQL query with varied equivalent serializations. Dong and Lapata (2018) decompose the decoding into two stages, where the first decoder predicts a rough sketch, and the second decoder fills in the lowlevel details conditioned on the question and the sketch. Such coarse-to-fine decoding has also been adopted in other works such as IRNet (Guo et al., 2019). To address the complex SQL queries with nested structures, RYANSQL (Choi et al., 2021) recursively yields SELECT statements and uses a sketch-based slot filling for each of the SELECT statements.

(p8.3) Bottom-up Both the tree-based and the sketchbased decoding mechanisms can be viewed as top-down decoding mechanisms. Rubin and Berant (2021) use a bottom-up decoding mechanism. Given K trees of height t, the decoder scores trees with height t + 1 constructed by SQL grammar from the current beam, and K trees with the highest scores are kept. Then, a representation of the new K trees is generated and placed in the new beam.

(p8.4) Attention Mechanism To integrate the encoderside information at decoding, an attention score is computed and multiplied with hidden vectors from the encoder to get the context vector, which is then used to generate an output token (Dong and Lapata, 2016;Zhong et al., 2017). Variants of the attention mechanism have been used to better propagate the information encoded from questions and DB schemas to the decoder. SQLNet (Xu et al., 2017) designs column attention, where it uses hidden states from columns multiplied by embeddings for the question to calculate attention scores for a column given the question. Guo and Gao (2018)  Copy Mechanism Seq2AST (Yin and Neubig, 2017) and Seq2SQL (Zhong et al., 2017) employ the pointer network (Vinyals et al., 2015) to compute the probability of copying words from the input. Wang et al. (2018a) use types (e.g., columns, SQL operators, constant from questions) to explicitly restrict locations in the query to copy from and develop a new training objective to only copy from the first occurrence in the input. In addition, the copy mechanism is also adopted in context-dependent text-to-SQL task (Wang et al., 2020b).

(p8.5) Intermediate Representations Researchers use intermediate representations to bridge the gap between natural language and SQL queries. Inc-SQL (Shi et al., 2018)  However, the intermediate representations are usually designed for a specific dataset and cannot be easily adapted to others (Suhr et al., 2020). To construct a more generalized intermediate representation, Herzig et al. (2021) propose to omit tokens in the SQL query that do not align to any phrase in the utterance.

(p8.6) Inspired by the success of text-to-SQL task, intermediate representations are also studied for SPARQL, another executable language for database systems (Saparina and Osokin, 2021;Herzig et al., 2021).

(p8.7) Others PICARD (Scholak et al., 2021b) and UniSAr (Dou et al., 2022) set constraints to the decoder to prevent generating invalid tokens. Several methods adopt an execution-guided decoding mechanism to exclude non-executable partial SQL queries from the output candidates (Wang et al., 2018b;. Global-GNN (Bogin et al., 2019b) employs a separately trained discriminative model to rerank the top-K SQL queries in the decoder's output beam, which is to reason about the complete SQL queries instead of considering each word and DB schemas in isolation.  (2018); Lee (2019) use separate submodules to predict different SQL components, easing the difficulty of generating a complete SQL query. Chen et al. (2020b) employ a gate to select between the output sequence encoded for the question and the output sequence from the previous decoding steps at each step for SQL generation. Inspired by machine translation, Müller and Vlachos (2019) apply byte-pair encoding (BPE) (Sennrich et al., 2016) to compress SQL queries to shorter sequences guided by AST, reducing the difficulties in SQL generation.","[[], ['b22', 'b26', 'b1'], [None], [], ['b22', None, 'b16'], [None, 'b9', 'b5'], [None], [None, 'b26']]","[[], ['b22', 'b26', 'b1'], [None], [], ['b22', None, 'b16'], [None, 'b9', 'b5'], [None], [None, 'b26']]",13,"1. Various methods have been proposed for decoding to achieve a fine-grained and easier process for SQL generation and bridge the gap between natural language and SQL queries.
2. As shown in Table 3 Table 9 in Appendix D. IR: Intermediate Representation.
3. and other technologies. Tree-based Seq2Tree (Dong and Lapata, 2016) employs a decoder that generates logical forms in a top-down manner.
4. The components in the sub-tree are generated conditioned on their parents apart from the input question.
5. Note that the syntax of the logical forms is implicitly learned from data for Seq2Tree.
6. Similarly, Seq2AST (Yin and Neubig, 2017) uses an abstract syntax tree (AST) for decoding the target programming language, where the syntax is explicitly integrated with AST.
7. Although both Seq2Tree (Dong and Lapata, 2016) and Seq2AST (Yin and Neubig, 2017) do not study text-to-SQL datasets, their uses of trees inspire tree-based decoding in text-to-SQL.
8. SyntaxSQL-Net (Yu et al., 2018b) employs a tree-based decoding method specific to SQL syntax and recursively calls modules to predict different SQL components.
9. Sketch-based SQLNet (Xu et al., 2017) designs a sketch aligned with the SQL grammar, and SQL-Net only needs to fill in the slots in the sketch rather than predict both the output grammar and the content.
10. Besides, the sketch captures the dependency of the predictions.
11. Thus, the prediction of one slot is only conditioned on the slots it depends on, which avoids issues of the same SQL query with varied equivalent serializations.
12. Dong and Lapata (2018) decompose the decoding into two stages, where the first decoder predicts a rough sketch, and the second decoder fills in the lowlevel details conditioned on the question and the sketch.
13. Such coarse-to-fine decoding has also been adopted in other works such as IRNet (Guo et al., 2019).
14. To address the complex SQL queries with nested structures, RYANSQL (Choi et al., 2021) recursively yields SELECT statements and uses a sketch-based slot filling for each of the SELECT statements.
15. Bottom-up Both the tree-based and the sketchbased decoding mechanisms can be viewed as top-down decoding mechanisms.
16. Rubin and Berant (2021) use a bottom-up decoding mechanism.
17. Given K trees of height t, the decoder scores trees with height t + 1 constructed by SQL grammar from the current beam, and K trees with the highest scores are kept.
18. Then, a representation of the new K trees is generated and placed in the new beam.
19. Attention Mechanism To integrate the encoderside information at decoding, an attention score is computed and multiplied with hidden vectors from the encoder to get the context vector, which is then used to generate an output token (Dong and Lapata, 2016;Zhong et al., 2017).
20. Variants of the attention mechanism have been used to better propagate the information encoded from questions and DB schemas to the decoder.
21. SQLNet (Xu et al., 2017) designs column attention, where it uses hidden states from columns multiplied by embeddings for the question to calculate attention scores for a column given the question.
22. Guo and Gao (2018)  Copy Mechanism Seq2AST (Yin and Neubig, 2017) and Seq2SQL (Zhong et al., 2017) employ the pointer network (Vinyals et al., 2015) to compute the probability of copying words from the input.
23. Wang et al. (2018a) use types (e.g., columns, SQL operators, constant from questions) to explicitly restrict locations in the query to copy from and develop a new training objective to only copy from the first occurrence in the input.
24. In addition, the copy mechanism is also adopted in context-dependent text-to-SQL task (Wang et al., 2020b).
25. Intermediate Representations Researchers use intermediate representations to bridge the gap between natural language and SQL queries.
26. Inc-SQL (Shi et al., 2018)  However, the intermediate representations are usually designed for a specific dataset and cannot be easily adapted to others (Suhr et al., 2020).
27. To construct a more generalized intermediate representation, Herzig et al. (2021) propose to omit tokens in the SQL query that do not align to any phrase in the utterance.
28. Inspired by the success of text-to-SQL task, intermediate representations are also studied for SPARQL, another executable language for database systems (Saparina and Osokin, 2021;Herzig et al., 2021).
29. Others PICARD (Scholak et al., 2021b) and UniSAr (Dou et al., 2022) set constraints to the decoder to prevent generating invalid tokens.
30. Several methods adopt an execution-guided decoding mechanism to exclude non-executable partial SQL queries from the output candidates (Wang et al., 2018b;. Global-GNN (Bogin et al., 2019b) employs a separately trained discriminative model to rerank the top-K SQL queries in the decoder's output beam, which is to reason about the complete SQL queries instead of considering each word and DB schemas in isolation.
31. (2018); Lee (2019) use separate submodules to predict different SQL components, easing the difficulty of generating a complete SQL query.
32. Chen et al. (2020b) employ a gate to select between the output sequence encoded for the question and the output sequence from the previous decoding steps at each step for SQL generation.
33. Inspired by machine translation, Müller and Vlachos (2019) apply byte-pair encoding (BPE) (Sennrich et al., 2016) to compress SQL queries to shorter sequences guided by AST, reducing the difficulties in SQL generation."
251719280,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,Computer Science,https://www.semanticscholar.org/paper/a814b76e589ef27e3f4af379d319d02d2110faa1,s9,Learning Techniques,"['p9.0', 'p9.1']","['Apart from end-to-end supervised learning, different learning techniques have been proposed to help text-to-SQL research. Here we summarize these learning techniques, each addressing a specific issue for the task. In SQL generation, IncSQL (Shi et al., 2018) allows parsers to explore alternative correct action sequences to generate different SQL queries. Brunner and Stockinger (2021) search values in DB to insert values into SQL query.', 'For context-dependent text-to-SQL, researchers adopt techniques such as turn-level encoder and copy mechanism (Suhr et al., 2018;Zhang et al., 2019;Wang et al., 2020b), constrained decoding (Wang et al., 2020b), dynamic memory decay mechanism (Hui et al., 2021a), treating questions and SQL queries as two modalities, and using bimodal pre-trained models (Zheng et al., 2022).']","Apart from end-to-end supervised learning, different learning techniques have been proposed to help text-to-SQL research. Here we summarize these learning techniques, each addressing a specific issue for the task. In SQL generation, IncSQL (Shi et al., 2018) allows parsers to explore alternative correct action sequences to generate different SQL queries. Brunner and Stockinger (2021) search values in DB to insert values into SQL query.

For context-dependent text-to-SQL, researchers adopt techniques such as turn-level encoder and copy mechanism (Suhr et al., 2018;Zhang et al., 2019;Wang et al., 2020b), constrained decoding (Wang et al., 2020b), dynamic memory decay mechanism (Hui et al., 2021a), treating questions and SQL queries as two modalities, and using bimodal pre-trained models (Zheng et al., 2022).","(p9.0) Apart from end-to-end supervised learning, different learning techniques have been proposed to help text-to-SQL research. Here we summarize these learning techniques, each addressing a specific issue for the task. In SQL generation, IncSQL (Shi et al., 2018) allows parsers to explore alternative correct action sequences to generate different SQL queries. Brunner and Stockinger (2021) search values in DB to insert values into SQL query.

(p9.1) For context-dependent text-to-SQL, researchers adopt techniques such as turn-level encoder and copy mechanism (Suhr et al., 2018;Zhang et al., 2019;Wang et al., 2020b), constrained decoding (Wang et al., 2020b), dynamic memory decay mechanism (Hui et al., 2021a), treating questions and SQL queries as two modalities, and using bimodal pre-trained models (Zheng et al., 2022).","[['b5'], ['b35', None, 'b10', 'b33']]","[['b5'], ['b35', None, 'b10', 'b33']]",5,"1. Apart from end-to-end supervised learning, different learning techniques have been proposed to help text-to-SQL research.
2. Here we summarize these learning techniques, each addressing a specific issue for the task.
3. In SQL generation, IncSQL (Shi et al., 2018) allows parsers to explore alternative correct action sequences to generate different SQL queries.
4. Brunner and Stockinger (2021) search values in DB to insert values into SQL query.
5. For context-dependent text-to-SQL, researchers adopt techniques such as turn-level encoder and copy mechanism (Suhr et al., 2018;Zhang et al., 2019;Wang et al., 2020b), constrained decoding (Wang et al., 2020b), dynamic memory decay mechanism (Hui et al., 2021a), treating questions and SQL queries as two modalities, and using bimodal pre-trained models (Zheng et al., 2022)."
251719280,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,Computer Science,https://www.semanticscholar.org/paper/a814b76e589ef27e3f4af379d319d02d2110faa1,s3,Single-Domain Datasets,"['p3.0', 'p3.1', 'p3.2', 'p3.3', 'p3.4', 'p3.5', 'p3.6']","['Single-domain text-to-SQL datasets typically collect question-SQL pairs for a single database in some real-world tasks, including early ones such as Academic (Li and Jagadish, 2014), Advising (Finegan-Dollak et al., 2018), ATIS (Price, 1990;Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), Yelp and IMDB (Yaghmazadeh et al., 2017), Scholar (Iyer et al., 2017) and Restaurants (Tang and Mooney, 2000;Popescu et al., 2003), as well as recent ones such as SEDE (Hazoom et al., 2021), ESQL (Chen et al., 2021a) and MIMICSQL (Wang et al., 2020d). These single-domain datasets, particularly the early ones, are usually limited in size, containing only a few hundred to a few thousand examples. Because of the limited size and similar SQL patterns in training and testing phases, text-to-SQL models that are trained on these single-domain datasets can achieve decent performance by simply memorizing the SQL patterns and fail to generalize to unseen SQL queries or SQL queries from other unless otherwise specified.', ""domains (Finegan-Dollak et al., 2018;Yu et al., 2018c). However, since these datasets are adapted from real-life applications, most of them contain domain knowledge (Gan et al., 2021b) and dataset conventions (Suhr et al., 2020). Thus, they are still valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions."", 'Appendix B gives a detailed discussion on domain knowledge and dataset convention, and concrete text-to-SQL examples.', ""Large Scale Cross-domain Datasets Large cross-domain datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) are proposed to better evaluate deep neural models. Wik-iSQL uses tables extracted from Wikipedia and lets annotators paraphrase questions generated for the tables. Compared to other datasets, WikiSQL is an order of magnitude larger, containing 80,654 natural utterances in total (Zhong et al., 2017). However, WikiSQL contains only simple SQL queries, and only a single table is queried within each SQL query (Yu et al., 2018c). Yu et al. (2018c) propose Spider, which contains 200 databases with an average of 5 tables for each database, to test models' performance on complicated unseen SQL queries and their ability to generalize to new domains. Furthermore, researchers expand Spider to study various issues of their inter-est (Lei et al., 2020;Zeng et al., 2020;Gan et al., 2021b;Taniguchi et al., 2021;Gan et al., 2021a)."", 'Besides, researchers build several large-scale text-to-SQL datasets in different languages such as CSpider (Min et al., 2019a), TableQA (Sun et al., 2020), DuSQL (Wang et al., 2020c) in Chinese, ViText2SQL (Tuan Nguyen et al., 2020) in Vietnamese, and PortugueseSpider (José and Cozman, 2021) in Portuguese. Given that human translation has shown to be more accurate than machine translation (Min et al., 2019a), these datasets are annotated mainly by human experts based on the English Spider dataset. These Spider-based datasets can serve as potential resources for multi-lingual text-to-SQL research.', 'Other Datasets Several context-dependent textto-SQL datasets have been proposed, which involve user interactions with the text-to-SQL system in English (Price, 1990;Dahl et al., 1994;Yu et al., 2019a,b) and Chinese (Guo et al., 2021). In addition, researchers collect datasets to study questions in text-to-SQL being answerable or not (Zhang et al., 2020), lexicon-level mapping (Shi et al., 2020b) and cross-domain evaluation for real Web databases (Lee et al., 2021).', 'Appendix C.1 discusses more details about datasets mentioned in § 2.']","Single-domain text-to-SQL datasets typically collect question-SQL pairs for a single database in some real-world tasks, including early ones such as Academic (Li and Jagadish, 2014), Advising (Finegan-Dollak et al., 2018), ATIS (Price, 1990;Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), Yelp and IMDB (Yaghmazadeh et al., 2017), Scholar (Iyer et al., 2017) and Restaurants (Tang and Mooney, 2000;Popescu et al., 2003), as well as recent ones such as SEDE (Hazoom et al., 2021), ESQL (Chen et al., 2021a) and MIMICSQL (Wang et al., 2020d). These single-domain datasets, particularly the early ones, are usually limited in size, containing only a few hundred to a few thousand examples. Because of the limited size and similar SQL patterns in training and testing phases, text-to-SQL models that are trained on these single-domain datasets can achieve decent performance by simply memorizing the SQL patterns and fail to generalize to unseen SQL queries or SQL queries from other unless otherwise specified.

domains (Finegan-Dollak et al., 2018;Yu et al., 2018c). However, since these datasets are adapted from real-life applications, most of them contain domain knowledge (Gan et al., 2021b) and dataset conventions (Suhr et al., 2020). Thus, they are still valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions.

Appendix B gives a detailed discussion on domain knowledge and dataset convention, and concrete text-to-SQL examples.

Large Scale Cross-domain Datasets Large cross-domain datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) are proposed to better evaluate deep neural models. Wik-iSQL uses tables extracted from Wikipedia and lets annotators paraphrase questions generated for the tables. Compared to other datasets, WikiSQL is an order of magnitude larger, containing 80,654 natural utterances in total (Zhong et al., 2017). However, WikiSQL contains only simple SQL queries, and only a single table is queried within each SQL query (Yu et al., 2018c). Yu et al. (2018c) propose Spider, which contains 200 databases with an average of 5 tables for each database, to test models' performance on complicated unseen SQL queries and their ability to generalize to new domains. Furthermore, researchers expand Spider to study various issues of their inter-est (Lei et al., 2020;Zeng et al., 2020;Gan et al., 2021b;Taniguchi et al., 2021;Gan et al., 2021a).

Besides, researchers build several large-scale text-to-SQL datasets in different languages such as CSpider (Min et al., 2019a), TableQA (Sun et al., 2020), DuSQL (Wang et al., 2020c) in Chinese, ViText2SQL (Tuan Nguyen et al., 2020) in Vietnamese, and PortugueseSpider (José and Cozman, 2021) in Portuguese. Given that human translation has shown to be more accurate than machine translation (Min et al., 2019a), these datasets are annotated mainly by human experts based on the English Spider dataset. These Spider-based datasets can serve as potential resources for multi-lingual text-to-SQL research.

Other Datasets Several context-dependent textto-SQL datasets have been proposed, which involve user interactions with the text-to-SQL system in English (Price, 1990;Dahl et al., 1994;Yu et al., 2019a,b) and Chinese (Guo et al., 2021). In addition, researchers collect datasets to study questions in text-to-SQL being answerable or not (Zhang et al., 2020), lexicon-level mapping (Shi et al., 2020b) and cross-domain evaluation for real Web databases (Lee et al., 2021).

Appendix C.1 discusses more details about datasets mentioned in § 2.","(p3.0) Single-domain text-to-SQL datasets typically collect question-SQL pairs for a single database in some real-world tasks, including early ones such as Academic (Li and Jagadish, 2014), Advising (Finegan-Dollak et al., 2018), ATIS (Price, 1990;Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), Yelp and IMDB (Yaghmazadeh et al., 2017), Scholar (Iyer et al., 2017) and Restaurants (Tang and Mooney, 2000;Popescu et al., 2003), as well as recent ones such as SEDE (Hazoom et al., 2021), ESQL (Chen et al., 2021a) and MIMICSQL (Wang et al., 2020d). These single-domain datasets, particularly the early ones, are usually limited in size, containing only a few hundred to a few thousand examples. Because of the limited size and similar SQL patterns in training and testing phases, text-to-SQL models that are trained on these single-domain datasets can achieve decent performance by simply memorizing the SQL patterns and fail to generalize to unseen SQL queries or SQL queries from other unless otherwise specified.

(p3.1) domains (Finegan-Dollak et al., 2018;Yu et al., 2018c). However, since these datasets are adapted from real-life applications, most of them contain domain knowledge (Gan et al., 2021b) and dataset conventions (Suhr et al., 2020). Thus, they are still valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions.

(p3.2) Appendix B gives a detailed discussion on domain knowledge and dataset convention, and concrete text-to-SQL examples.

(p3.3) Large Scale Cross-domain Datasets Large cross-domain datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) are proposed to better evaluate deep neural models. Wik-iSQL uses tables extracted from Wikipedia and lets annotators paraphrase questions generated for the tables. Compared to other datasets, WikiSQL is an order of magnitude larger, containing 80,654 natural utterances in total (Zhong et al., 2017). However, WikiSQL contains only simple SQL queries, and only a single table is queried within each SQL query (Yu et al., 2018c). Yu et al. (2018c) propose Spider, which contains 200 databases with an average of 5 tables for each database, to test models' performance on complicated unseen SQL queries and their ability to generalize to new domains. Furthermore, researchers expand Spider to study various issues of their inter-est (Lei et al., 2020;Zeng et al., 2020;Gan et al., 2021b;Taniguchi et al., 2021;Gan et al., 2021a).

(p3.4) Besides, researchers build several large-scale text-to-SQL datasets in different languages such as CSpider (Min et al., 2019a), TableQA (Sun et al., 2020), DuSQL (Wang et al., 2020c) in Chinese, ViText2SQL (Tuan Nguyen et al., 2020) in Vietnamese, and PortugueseSpider (José and Cozman, 2021) in Portuguese. Given that human translation has shown to be more accurate than machine translation (Min et al., 2019a), these datasets are annotated mainly by human experts based on the English Spider dataset. These Spider-based datasets can serve as potential resources for multi-lingual text-to-SQL research.

(p3.5) Other Datasets Several context-dependent textto-SQL datasets have been proposed, which involve user interactions with the text-to-SQL system in English (Price, 1990;Dahl et al., 1994;Yu et al., 2019a,b) and Chinese (Guo et al., 2021). In addition, researchers collect datasets to study questions in text-to-SQL being answerable or not (Zhang et al., 2020), lexicon-level mapping (Shi et al., 2020b) and cross-domain evaluation for real Web databases (Lee et al., 2021).

(p3.6) Appendix C.1 discusses more details about datasets mentioned in § 2.","[[None, 'b30'], [None, 'b9', 'b28'], [], ['b13', None, 'b28', 'b32'], [None, 'b11'], [None, 'b34', 'b6'], []]","[[None, 'b30'], [None, 'b9', 'b28'], [], ['b13', None, 'b28', 'b32'], [None, 'b11'], [None, 'b34', 'b6'], []]",14,"1. Single-domain text-to-SQL datasets typically collect question-SQL pairs for a single database in some real-world tasks, including early ones such as Academic (Li and Jagadish, 2014), Advising (Finegan-Dollak et al., 2018), ATIS (Price, 1990;Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), Yelp and IMDB (Yaghmazadeh et al., 2017), Scholar (Iyer et al., 2017) and Restaurants (Tang and Mooney, 2000;Popescu et al., 2003), as well as recent ones such as SEDE (Hazoom et al., 2021), ESQL (Chen et al., 2021a) and MIMICSQL (Wang et al., 2020d).
2. These single-domain datasets, particularly the early ones, are usually limited in size, containing only a few hundred to a few thousand examples.
3. Because of the limited size and similar SQL patterns in training and testing phases, text-to-SQL models that are trained on these single-domain datasets can achieve decent performance by simply memorizing the SQL patterns and fail to generalize to unseen SQL queries or SQL queries from other unless otherwise specified.domains (Finegan-Dollak et al., 2018;Yu et al., 2018c).
4. However, since these datasets are adapted from real-life applications, most of them contain domain knowledge (Gan et al., 2021b) and dataset conventions (Suhr et al., 2020).
5. Thus, they are still valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions.
6. Appendix B gives a detailed discussion on domain knowledge and dataset convention, and concrete text-to-SQL examples.
7. Large Scale Cross-domain Datasets Large cross-domain datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) are proposed to better evaluate deep neural models.
8. Wik-iSQL uses tables extracted from Wikipedia and lets annotators paraphrase questions generated for the tables.
9. Compared to other datasets, WikiSQL is an order of magnitude larger, containing 80,654 natural utterances in total (Zhong et al., 2017).
10. However, WikiSQL contains only simple SQL queries, and only a single table is queried within each SQL query (Yu et al., 2018c).
11. Yu et al. (2018c) propose Spider, which contains 200 databases with an average of 5 tables for each database, to test models' performance on complicated unseen SQL queries and their ability to generalize to new domains.
12. Furthermore, researchers expand Spider to study various issues of their inter-est (Lei et al., 2020;Zeng et al., 2020;Gan et al., 2021b;Taniguchi et al., 2021;Gan et al., 2021a).
13. Besides, researchers build several large-scale text-to-SQL datasets in different languages such as CSpider (Min et al., 2019a), TableQA (Sun et al., 2020), DuSQL (Wang et al., 2020c) in Chinese, ViText2SQL (Tuan Nguyen et al., 2020) in Vietnamese, and PortugueseSpider (José and Cozman, 2021) in Portuguese.
14. Given that human translation has shown to be more accurate than machine translation (Min et al., 2019a), these datasets are annotated mainly by human experts based on the English Spider dataset.
15. These Spider-based datasets can serve as potential resources for multi-lingual text-to-SQL research.
16. Other Datasets Several context-dependent textto-SQL datasets have been proposed, which involve user interactions with the text-to-SQL system in English (Price, 1990;Dahl et al., 1994;Yu et al., 2019a,b) and Chinese (Guo et al., 2021).
17. In addition, researchers collect datasets to study questions in text-to-SQL being answerable or not (Zhang et al., 2020), lexicon-level mapping (Shi et al., 2020b) and cross-domain evaluation for real Web databases (Lee et al., 2021).
18. Appendix C.1 discusses more details about datasets mentioned in § 2."
252200083,"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives","Computer Science, Linguistics",https://www.semanticscholar.org/paper/77696f90afa79ac06ea58abcc91bf5e10f72b934,s26,New Tasks: Inspiration from Sarcasm,"['p26.0', 'p26.1', 'p26.2', 'p26.3', 'p26.4']","['Compared to sarcasm, irony is rarely seen as a term in NLP conferences. Recently we have witnessed great improvements in sarcasm processing and in this part we will discuss how new tasks in sarcasm could motivate irony research.', 'Data Collection As discussed, datasets are highly dependent on hashtags as a signal to extract ironical expressions. Shmueli et al. (2020) proposed an algorithm to detect sarcastic tweets from a thread based on exterior cue tweets. A distant supervision based method for extracting ironies from platforms is crucial, given ironies in conversational contexts are central topic in the future.', 'Intended and Perceived Irony Oprea and Magdy (2019) explored how author profiling affected the perceived sarcasm (manual labelling) versus the intended sarcasm (hashtags), and verified the difference between both. Further, Oprea and Magdy (2020) introduced iSarcasm dataset which divided intended sarcasms and perceived sarcasms. The state-of-the-art sarcasm detection models performed obviously worse than human evaluation on this dataset. Future work could focus on multimodal perceived and intended irony, especially across various cultures.', 'Target Identification Sarcasm target identification was firstly proposed in Joshi et al. (2018), in which sarcasm targets were classified as one target, several targets and outside. Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, and improved target identification by a lot. For irony processing, most ironical expressions do not equip a specific target in itself as previously discussed. However, its ironical effects are likely in dialogue or visually grounded environment, which encourages us to enhance irony datasets in aforementioned ways.', 'Irony Explanation Irony, according to the definition, have opposite real meanings to literal meanings. However, this does not mean adding a single negation could interpret ironies well. Kumar et al. (2022) proposed a new task, sarcasm explanation in dialogue. Irony explanation might encounter more complex problems due to relatively low proportion of targets. Still, we should include irony explanation as a branch of multimodal irony processing like Desai et al. (2021).']","Compared to sarcasm, irony is rarely seen as a term in NLP conferences. Recently we have witnessed great improvements in sarcasm processing and in this part we will discuss how new tasks in sarcasm could motivate irony research.

Data Collection As discussed, datasets are highly dependent on hashtags as a signal to extract ironical expressions. Shmueli et al. (2020) proposed an algorithm to detect sarcastic tweets from a thread based on exterior cue tweets. A distant supervision based method for extracting ironies from platforms is crucial, given ironies in conversational contexts are central topic in the future.

Intended and Perceived Irony Oprea and Magdy (2019) explored how author profiling affected the perceived sarcasm (manual labelling) versus the intended sarcasm (hashtags), and verified the difference between both. Further, Oprea and Magdy (2020) introduced iSarcasm dataset which divided intended sarcasms and perceived sarcasms. The state-of-the-art sarcasm detection models performed obviously worse than human evaluation on this dataset. Future work could focus on multimodal perceived and intended irony, especially across various cultures.

Target Identification Sarcasm target identification was firstly proposed in Joshi et al. (2018), in which sarcasm targets were classified as one target, several targets and outside. Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, and improved target identification by a lot. For irony processing, most ironical expressions do not equip a specific target in itself as previously discussed. However, its ironical effects are likely in dialogue or visually grounded environment, which encourages us to enhance irony datasets in aforementioned ways.

Irony Explanation Irony, according to the definition, have opposite real meanings to literal meanings. However, this does not mean adding a single negation could interpret ironies well. Kumar et al. (2022) proposed a new task, sarcasm explanation in dialogue. Irony explanation might encounter more complex problems due to relatively low proportion of targets. Still, we should include irony explanation as a branch of multimodal irony processing like Desai et al. (2021).","(p26.0) Compared to sarcasm, irony is rarely seen as a term in NLP conferences. Recently we have witnessed great improvements in sarcasm processing and in this part we will discuss how new tasks in sarcasm could motivate irony research.

(p26.1) Data Collection As discussed, datasets are highly dependent on hashtags as a signal to extract ironical expressions. Shmueli et al. (2020) proposed an algorithm to detect sarcastic tweets from a thread based on exterior cue tweets. A distant supervision based method for extracting ironies from platforms is crucial, given ironies in conversational contexts are central topic in the future.

(p26.2) Intended and Perceived Irony Oprea and Magdy (2019) explored how author profiling affected the perceived sarcasm (manual labelling) versus the intended sarcasm (hashtags), and verified the difference between both. Further, Oprea and Magdy (2020) introduced iSarcasm dataset which divided intended sarcasms and perceived sarcasms. The state-of-the-art sarcasm detection models performed obviously worse than human evaluation on this dataset. Future work could focus on multimodal perceived and intended irony, especially across various cultures.

(p26.3) Target Identification Sarcasm target identification was firstly proposed in Joshi et al. (2018), in which sarcasm targets were classified as one target, several targets and outside. Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, and improved target identification by a lot. For irony processing, most ironical expressions do not equip a specific target in itself as previously discussed. However, its ironical effects are likely in dialogue or visually grounded environment, which encourages us to enhance irony datasets in aforementioned ways.

(p26.4) Irony Explanation Irony, according to the definition, have opposite real meanings to literal meanings. However, this does not mean adding a single negation could interpret ironies well. Kumar et al. (2022) proposed a new task, sarcasm explanation in dialogue. Irony explanation might encounter more complex problems due to relatively low proportion of targets. Still, we should include irony explanation as a branch of multimodal irony processing like Desai et al. (2021).","[[], ['b52'], [None], ['b41', 'b17'], ['b22', 'b0']]","[[], ['b52'], [None], ['b41', 'b17'], ['b22', 'b0']]",6,"1. Compared to sarcasm, irony is rarely seen as a term in NLP conferences.
2. Recently we have witnessed great improvements in sarcasm processing and in this part we will discuss how new tasks in sarcasm could motivate irony research.
3. Data Collection As discussed, datasets are highly dependent on hashtags as a signal to extract ironical expressions.
4. Shmueli et al. (2020) proposed an algorithm to detect sarcastic tweets from a thread based on exterior cue tweets.
5. A distant supervision based method for extracting ironies from platforms is crucial, given ironies in conversational contexts are central topic in the future.
6. Intended and Perceived Irony Oprea and Magdy (2019) explored how author profiling affected the perceived sarcasm (manual labelling) versus the intended sarcasm (hashtags), and verified the difference between both.
7. Further, Oprea and Magdy (2020) introduced iSarcasm dataset which divided intended sarcasms and perceived sarcasms.
8. The state-of-the-art sarcasm detection models performed obviously worse than human evaluation on this dataset.
9. Future work could focus on multimodal perceived and intended irony, especially across various cultures.
10. Target Identification Sarcasm target identification was firstly proposed in Joshi et al. (2018), in which sarcasm targets were classified as one target, several targets and outside.
11. Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, and improved target identification by a lot.
12. For irony processing, most ironical expressions do not equip a specific target in itself as previously discussed.
13. However, its ironical effects are likely in dialogue or visually grounded environment, which encourages us to enhance irony datasets in aforementioned ways.
14. Irony Explanation Irony, according to the definition, have opposite real meanings to literal meanings.
15. However, this does not mean adding a single negation could interpret ironies well.
16. Kumar et al. (2022) proposed a new task, sarcasm explanation in dialogue.
17. Irony explanation might encounter more complex problems due to relatively low proportion of targets.
18. Still, we should include irony explanation as a branch of multimodal irony processing like Desai et al. (2021)."
252200083,"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives","Computer Science, Linguistics",https://www.semanticscholar.org/paper/77696f90afa79ac06ea58abcc91bf5e10f72b934,s22,Multimodal Irony Processing,"['p22.0', 'p22.1', 'p22.2']","['Linguistic interactions are not solely consisted of texts. Besides, facial expressions and speech communications are crucial to convey emotions and feelings. For example, Skrelin et al. (2020) reported people could classify ironies based on phonetic characteristics only. Consequently, it is conceivable that multimodal methods could help with irony detection. Schifanella et al. (2016) made the first attempt in multimodal sarcasm detection, in which they extracted posts from three multimodal social media platforms based on hashtags. Then they used SVMs and neural networks to prove the validity of visual information in enhancing sarcasm detection.', 'Castro et al. (2019) made a great improvement in multimodal sarcasm detection by introducing audio features into the dataset. The experiments also verified the importance of more modalities in sarcasm processing.', 'Future work in multimodal irony processing should include a comprehensive multimodal irony dataset based on MUStARD dataset (Castro et al., 2019) with more fine-grained annotation schemes. Additionally, most methods (Pan et al., 2020;Liu et al., 2021) explored sarcasm by introducing intermodality and intra-modality attention in singlestream setting. How double-stream multimodal pretrained models (MPMs) will encode and interact in complex discourse settings remains an interesting problem to solve.']","Linguistic interactions are not solely consisted of texts. Besides, facial expressions and speech communications are crucial to convey emotions and feelings. For example, Skrelin et al. (2020) reported people could classify ironies based on phonetic characteristics only. Consequently, it is conceivable that multimodal methods could help with irony detection. Schifanella et al. (2016) made the first attempt in multimodal sarcasm detection, in which they extracted posts from three multimodal social media platforms based on hashtags. Then they used SVMs and neural networks to prove the validity of visual information in enhancing sarcasm detection.

Castro et al. (2019) made a great improvement in multimodal sarcasm detection by introducing audio features into the dataset. The experiments also verified the importance of more modalities in sarcasm processing.

Future work in multimodal irony processing should include a comprehensive multimodal irony dataset based on MUStARD dataset (Castro et al., 2019) with more fine-grained annotation schemes. Additionally, most methods (Pan et al., 2020;Liu et al., 2021) explored sarcasm by introducing intermodality and intra-modality attention in singlestream setting. How double-stream multimodal pretrained models (MPMs) will encode and interact in complex discourse settings remains an interesting problem to solve.","(p22.0) Linguistic interactions are not solely consisted of texts. Besides, facial expressions and speech communications are crucial to convey emotions and feelings. For example, Skrelin et al. (2020) reported people could classify ironies based on phonetic characteristics only. Consequently, it is conceivable that multimodal methods could help with irony detection. Schifanella et al. (2016) made the first attempt in multimodal sarcasm detection, in which they extracted posts from three multimodal social media platforms based on hashtags. Then they used SVMs and neural networks to prove the validity of visual information in enhancing sarcasm detection.

(p22.1) Castro et al. (2019) made a great improvement in multimodal sarcasm detection by introducing audio features into the dataset. The experiments also verified the importance of more modalities in sarcasm processing.

(p22.2) Future work in multimodal irony processing should include a comprehensive multimodal irony dataset based on MUStARD dataset (Castro et al., 2019) with more fine-grained annotation schemes. Additionally, most methods (Pan et al., 2020;Liu et al., 2021) explored sarcasm by introducing intermodality and intra-modality attention in singlestream setting. How double-stream multimodal pretrained models (MPMs) will encode and interact in complex discourse settings remains an interesting problem to solve.","[['b53', 'b50'], [], ['b40', 'b32']]","[['b53', 'b50'], [], ['b40', 'b32']]",4,"1. Linguistic interactions are not solely consisted of texts.
2. Besides, facial expressions and speech communications are crucial to convey emotions and feelings.
3. For example, Skrelin et al. (2020) reported people could classify ironies based on phonetic characteristics only.
4. Consequently, it is conceivable that multimodal methods could help with irony detection.
5. Schifanella et al. (2016) made the first attempt in multimodal sarcasm detection, in which they extracted posts from three multimodal social media platforms based on hashtags.
6. Then they used SVMs and neural networks to prove the validity of visual information in enhancing sarcasm detection.
7. Castro et al. (2019) made a great improvement in multimodal sarcasm detection by introducing audio features into the dataset.
8. The experiments also verified the importance of more modalities in sarcasm processing.
9. Future work in multimodal irony processing should include a comprehensive multimodal irony dataset based on MUStARD dataset (Castro et al., 2019) with more fine-grained annotation schemes.
10. Additionally, most methods (Pan et al., 2020;Liu et al., 2021) explored sarcasm by introducing intermodality and intra-modality attention in singlestream setting.
11. How double-stream multimodal pretrained models (MPMs) will encode and interact in complex discourse settings remains an interesting problem to solve."
252200083,"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives","Computer Science, Linguistics",https://www.semanticscholar.org/paper/77696f90afa79ac06ea58abcc91bf5e10f72b934,s6,Irony and Sarcasm,"['p6.0', 'p6.1', 'p6.2', 'p6.3']","['Most of the studies saw sarcasm as a subset of irony (Tannen et al., 2005;Barbe, 1995;Kumon-Nakamura et al., 1995;Leggitt and Gibbs, 2000;Bowes and Katz, 2011). Sarcasm is often recognized as ""a nasty, mean-spirited, or just relatively negative form of verbal irony, used on occasion to enhance the negativity expressed relative to direct, non-figurative criticism"" (Colston, 2017).', 'One of the peculiarities of sarcasm is whether or not the speakers intend to offend the listeners. Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed. The non-sarcastic irony, on the other hand, can communicate either a good or negative attitude, and it is rarely meant to be hurtful. Barbe (1995) concurred that the core difference was ""hurtful"". She claimed that irony is a face-saving strategy while sarcasm is a face-threatening action. Ridicule is another feature of sarcasm. According to Lee and Katz (1998), sarcasm is closer to ridicule than irony. Their experiment revealed that sarcasm is directed at a single person, but irony is directed toward a large group of people. Haiman et al. (1998) claimed that one of the most distinguishing characteristics of sarcasm is that the literal meaning of its words is always positive. However, he did not convey his thoughts on irony. Whereas Littman and Mey (1991) viewed this topic from another angle. While there are many various forms of ironies, they believe that there is only one type of sarcasm because ""sarcasm cannot exist independently of the communication setting.""', 'Cognitive scientists approached the difference in experimental studies. Previous research in child language acquisition (Glenwright and Pexman, 2010) reported that children understood sarcastic criticism later than they could understand the nonliteral meanings of irony and sarcasm, implying different pragmatic purposes of irony and sarcasm. Filik et al. (2019) utilized fMRI and found out sarcasm is associated with wider activation of semantic network in human brains compared to irony.', 'However, most computational linguistics researchers used irony and sarcasm interchangeably, since the boundary between these two concepts is too vague for even human beings, let alone for machines. Joshi et al. (2016) and Sulis et al. (2016) verified this claim from both human annotators and computational perspectives. Although in this paper we will mainly focus on the literal irony processing and discuss the inspirations from recent research output in sarcasm processing, we aim to encourage unify sarcasm under the framework of irony via fine-grained annotation schemes.']","Most of the studies saw sarcasm as a subset of irony (Tannen et al., 2005;Barbe, 1995;Kumon-Nakamura et al., 1995;Leggitt and Gibbs, 2000;Bowes and Katz, 2011). Sarcasm is often recognized as ""a nasty, mean-spirited, or just relatively negative form of verbal irony, used on occasion to enhance the negativity expressed relative to direct, non-figurative criticism"" (Colston, 2017).

One of the peculiarities of sarcasm is whether or not the speakers intend to offend the listeners. Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed. The non-sarcastic irony, on the other hand, can communicate either a good or negative attitude, and it is rarely meant to be hurtful. Barbe (1995) concurred that the core difference was ""hurtful"". She claimed that irony is a face-saving strategy while sarcasm is a face-threatening action. Ridicule is another feature of sarcasm. According to Lee and Katz (1998), sarcasm is closer to ridicule than irony. Their experiment revealed that sarcasm is directed at a single person, but irony is directed toward a large group of people. Haiman et al. (1998) claimed that one of the most distinguishing characteristics of sarcasm is that the literal meaning of its words is always positive. However, he did not convey his thoughts on irony. Whereas Littman and Mey (1991) viewed this topic from another angle. While there are many various forms of ironies, they believe that there is only one type of sarcasm because ""sarcasm cannot exist independently of the communication setting.""

Cognitive scientists approached the difference in experimental studies. Previous research in child language acquisition (Glenwright and Pexman, 2010) reported that children understood sarcastic criticism later than they could understand the nonliteral meanings of irony and sarcasm, implying different pragmatic purposes of irony and sarcasm. Filik et al. (2019) utilized fMRI and found out sarcasm is associated with wider activation of semantic network in human brains compared to irony.

However, most computational linguistics researchers used irony and sarcasm interchangeably, since the boundary between these two concepts is too vague for even human beings, let alone for machines. Joshi et al. (2016) and Sulis et al. (2016) verified this claim from both human annotators and computational perspectives. Although in this paper we will mainly focus on the literal irony processing and discuss the inspirations from recent research output in sarcasm processing, we aim to encourage unify sarcasm under the framework of irony via fine-grained annotation schemes.","(p6.0) Most of the studies saw sarcasm as a subset of irony (Tannen et al., 2005;Barbe, 1995;Kumon-Nakamura et al., 1995;Leggitt and Gibbs, 2000;Bowes and Katz, 2011). Sarcasm is often recognized as ""a nasty, mean-spirited, or just relatively negative form of verbal irony, used on occasion to enhance the negativity expressed relative to direct, non-figurative criticism"" (Colston, 2017).

(p6.1) One of the peculiarities of sarcasm is whether or not the speakers intend to offend the listeners. Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed. The non-sarcastic irony, on the other hand, can communicate either a good or negative attitude, and it is rarely meant to be hurtful. Barbe (1995) concurred that the core difference was ""hurtful"". She claimed that irony is a face-saving strategy while sarcasm is a face-threatening action. Ridicule is another feature of sarcasm. According to Lee and Katz (1998), sarcasm is closer to ridicule than irony. Their experiment revealed that sarcasm is directed at a single person, but irony is directed toward a large group of people. Haiman et al. (1998) claimed that one of the most distinguishing characteristics of sarcasm is that the literal meaning of its words is always positive. However, he did not convey his thoughts on irony. Whereas Littman and Mey (1991) viewed this topic from another angle. While there are many various forms of ironies, they believe that there is only one type of sarcasm because ""sarcasm cannot exist independently of the communication setting.""

(p6.2) Cognitive scientists approached the difference in experimental studies. Previous research in child language acquisition (Glenwright and Pexman, 2010) reported that children understood sarcastic criticism later than they could understand the nonliteral meanings of irony and sarcasm, implying different pragmatic purposes of irony and sarcasm. Filik et al. (2019) utilized fMRI and found out sarcasm is associated with wider activation of semantic network in human brains compared to irony.

(p6.3) However, most computational linguistics researchers used irony and sarcasm interchangeably, since the boundary between these two concepts is too vague for even human beings, let alone for machines. Joshi et al. (2016) and Sulis et al. (2016) verified this claim from both human annotators and computational perspectives. Although in this paper we will mainly focus on the literal irony processing and discuss the inspirations from recent research output in sarcasm processing, we aim to encourage unify sarcasm under the framework of irony via fine-grained annotation schemes.","[['b59', 'b23', 'b26', None], [None, 'b23', 'b14', 'b25'], ['b11', 'b4'], ['b55', None]]","[['b59', 'b23', 'b26', None], [None, 'b23', 'b14', 'b25'], ['b11', 'b4'], ['b55', None]]",12,"1. Most of the studies saw sarcasm as a subset of irony (Tannen et al., 2005;Barbe, 1995;Kumon-Nakamura et al., 1995;Leggitt and Gibbs, 2000;Bowes and Katz, 2011).
2. Sarcasm is often recognized as ""a nasty, mean-spirited, or just relatively negative form of verbal irony, used on occasion to enhance the negativity expressed relative to direct, non-figurative criticism"" (Colston, 2017).
3. One of the peculiarities of sarcasm is whether or not the speakers intend to offend the listeners.
4. Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed.
5. The non-sarcastic irony, on the other hand, can communicate either a good or negative attitude, and it is rarely meant to be hurtful.
6. Barbe (1995) concurred that the core difference was ""hurtful"".
7. She claimed that irony is a face-saving strategy while sarcasm is a face-threatening action.
8. Ridicule is another feature of sarcasm.
9. According to Lee and Katz (1998), sarcasm is closer to ridicule than irony.
10. Their experiment revealed that sarcasm is directed at a single person, but irony is directed toward a large group of people.
11. Haiman et al. (1998) claimed that one of the most distinguishing characteristics of sarcasm is that the literal meaning of its words is always positive.
12. However, he did not convey his thoughts on irony.
13. Whereas Littman and Mey (1991) viewed this topic from another angle.
14. While there are many various forms of ironies, they believe that there is only one type of sarcasm because ""sarcasm cannot exist independently of the communication setting.
15. ""Cognitive scientists approached the difference in experimental studies.
16. Previous research in child language acquisition (Glenwright and Pexman, 2010) reported that children understood sarcastic criticism later than they could understand the nonliteral meanings of irony and sarcasm, implying different pragmatic purposes of irony and sarcasm.
17. Filik et al. (2019) utilized fMRI and found out sarcasm is associated with wider activation of semantic network in human brains compared to irony.
18. However, most computational linguistics researchers used irony and sarcasm interchangeably, since the boundary between these two concepts is too vague for even human beings, let alone for machines.
19. Joshi et al. (2016) and Sulis et al. (2016) verified this claim from both human annotators and computational perspectives.
20. Although in this paper we will mainly focus on the literal irony processing and discuss the inspirations from recent research output in sarcasm processing, we aim to encourage unify sarcasm under the framework of irony via fine-grained annotation schemes."
252200083,"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives","Computer Science, Linguistics",https://www.semanticscholar.org/paper/77696f90afa79ac06ea58abcc91bf5e10f72b934,s5,Irony in Communication,"['p5.0', 'p5.1', 'p5.2', 'p5.3', 'p5.4']","['Researchers claim that by using ironies, people have several kinds of intentions.', ""Be polite: According to Brown et al. (1987), when unfavorable attitudes such as resistance, criticism, and complaints are stated with irony, the threat to the listener's reputation is reduced. The irony, as stated in Giora (1995), is an indirect negation. People prefer to utilize indirect negation to be polite to their listeners because direct negation can generate great unhappiness;"", ""Ease criticisms: As reported by Dews and Winner (1995), irony helps to ease the expression's evaluative function. They believe that the incompatibility between literal meaning and contextual meaning can make it difficult to articulate negative feelings. However, Toplak and Katz (2000) argued that, while irony literally avoids conflict, it is more aggressive from the perspective of the speaker's goal;"", 'Self-protection: Sperber and Wilson (1986) proposed the ""echoic"" idea, which stated that irony is a detached utterance that is simply an echo of another people\'s thought. It\'s a self-protection tactic, especially when the speakers are members of marginalized groups. According to , the irony is an ""off-record"" statement that allows speakers to deny their true intentions and avoid being challenged;', 'Be amusing:  reported that when young people intend to be humorous, 50% of their communication is ironic. It can assist people in creating a dialogue platform on which speakers and listeners can agree and communicate more easily.']","Researchers claim that by using ironies, people have several kinds of intentions.

Be polite: According to Brown et al. (1987), when unfavorable attitudes such as resistance, criticism, and complaints are stated with irony, the threat to the listener's reputation is reduced. The irony, as stated in Giora (1995), is an indirect negation. People prefer to utilize indirect negation to be polite to their listeners because direct negation can generate great unhappiness;

Ease criticisms: As reported by Dews and Winner (1995), irony helps to ease the expression's evaluative function. They believe that the incompatibility between literal meaning and contextual meaning can make it difficult to articulate negative feelings. However, Toplak and Katz (2000) argued that, while irony literally avoids conflict, it is more aggressive from the perspective of the speaker's goal;

Self-protection: Sperber and Wilson (1986) proposed the ""echoic"" idea, which stated that irony is a detached utterance that is simply an echo of another people's thought. It's a self-protection tactic, especially when the speakers are members of marginalized groups. According to , the irony is an ""off-record"" statement that allows speakers to deny their true intentions and avoid being challenged;

Be amusing:  reported that when young people intend to be humorous, 50% of their communication is ironic. It can assist people in creating a dialogue platform on which speakers and listeners can agree and communicate more easily.","(p5.0) Researchers claim that by using ironies, people have several kinds of intentions.

(p5.1) Be polite: According to Brown et al. (1987), when unfavorable attitudes such as resistance, criticism, and complaints are stated with irony, the threat to the listener's reputation is reduced. The irony, as stated in Giora (1995), is an indirect negation. People prefer to utilize indirect negation to be polite to their listeners because direct negation can generate great unhappiness;

(p5.2) Ease criticisms: As reported by Dews and Winner (1995), irony helps to ease the expression's evaluative function. They believe that the incompatibility between literal meaning and contextual meaning can make it difficult to articulate negative feelings. However, Toplak and Katz (2000) argued that, while irony literally avoids conflict, it is more aggressive from the perspective of the speaker's goal;

(p5.3) Self-protection: Sperber and Wilson (1986) proposed the ""echoic"" idea, which stated that irony is a detached utterance that is simply an echo of another people's thought. It's a self-protection tactic, especially when the speakers are members of marginalized groups. According to , the irony is an ""off-record"" statement that allows speakers to deny their true intentions and avoid being challenged;

(p5.4) Be amusing:  reported that when young people intend to be humorous, 50% of their communication is ironic. It can assist people in creating a dialogue platform on which speakers and listeners can agree and communicate more easily.","[[], [None, 'b10'], ['b2', 'b60'], ['b54'], []]","[[], [None, 'b10'], ['b2', 'b60'], ['b54'], []]",5,"1. Researchers claim that by using ironies, people have several kinds of intentions.
2. Be polite: According to Brown et al. (1987), when unfavorable attitudes such as resistance, criticism, and complaints are stated with irony, the threat to the listener's reputation is reduced.
3. The irony, as stated in Giora (1995), is an indirect negation.
4. People prefer to utilize indirect negation to be polite to their listeners because direct negation can generate great unhappiness;Ease criticisms: As reported by Dews and Winner (1995), irony helps to ease the expression's evaluative function.
5. They believe that the incompatibility between literal meaning and contextual meaning can make it difficult to articulate negative feelings.
6. However, Toplak and Katz (2000) argued that, while irony literally avoids conflict, it is more aggressive from the perspective of the speaker's goal;Self-protection: Sperber and Wilson (1986) proposed the ""echoic"" idea, which stated that irony is a detached utterance that is simply an echo of another people's thought.
7. It's a self-protection tactic, especially when the speakers are members of marginalized groups.
8. According to , the irony is an ""off-record"" statement that allows speakers to deny their true intentions and avoid being challenged;Be amusing:  reported that when young people intend to be humorous, 50% of their communication is ironic.
9. It can assist people in creating a dialogue platform on which speakers and listeners can agree and communicate more easily."
252200083,"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives","Computer Science, Linguistics",https://www.semanticscholar.org/paper/77696f90afa79ac06ea58abcc91bf5e10f72b934,s1,Theoretical Research in Irony 2.1 Irony Theories,"['p1.0', 'p1.1', 'p1.2']","['Various definitions have been given to irony. Early studies suggested that irony is the expression whose real meaning is contradictory to its literal meaning (Grice, 1975). The Merriam-Webster Dictionary, The Oxford English Dictionary, and The Collins English Dictionary all adopted this definition and used the words ""opposite"" or ""contrary"" to explain the relationship between the literal and contextual meanings of irony.', 'However, more research into various types of ironic examples revealed that the contextual meaning of irony does not have to be ""opposite"" or ""contrary"" to the literal one. According to Sperber and Wilson (1986); Wilson and Sperber (2012), some expressions have no ""literal meaning"" to be challenged because no ""literal meaning"" is mentioned in the context, based on which they raised relevance theory and the ""echoic"" concept. They considered irony as ""an echoic use of language in which the speaker tacitly dissociates herself from an attributed utterance or thought"" (Wilson, 2006). That is, if the ""echoic use"" is incongruous in some ways, the expression can be ironic. Based on this theory, Seto (1998) put forward that there are some ""echo-markers"" like definitely, really, and indeed.', 'Li and Huang (2020) provided instances to show that ""incongruity"" does not have to be between the literal and contextual meanings of irony in certain circumstances. They believed that irony\'s true nature is a psychological activity as much as a verbal representation. The speaker or listeners must finish the ""reversal"" process on a psychological level for it to be completed. When compared to the concepts of ""echoic"" and ""incongruity,"" ""reversal"" is concerned not only with the results but also with the psychological processes that the speakers/listeners go through.']","Various definitions have been given to irony. Early studies suggested that irony is the expression whose real meaning is contradictory to its literal meaning (Grice, 1975). The Merriam-Webster Dictionary, The Oxford English Dictionary, and The Collins English Dictionary all adopted this definition and used the words ""opposite"" or ""contrary"" to explain the relationship between the literal and contextual meanings of irony.

However, more research into various types of ironic examples revealed that the contextual meaning of irony does not have to be ""opposite"" or ""contrary"" to the literal one. According to Sperber and Wilson (1986); Wilson and Sperber (2012), some expressions have no ""literal meaning"" to be challenged because no ""literal meaning"" is mentioned in the context, based on which they raised relevance theory and the ""echoic"" concept. They considered irony as ""an echoic use of language in which the speaker tacitly dissociates herself from an attributed utterance or thought"" (Wilson, 2006). That is, if the ""echoic use"" is incongruous in some ways, the expression can be ironic. Based on this theory, Seto (1998) put forward that there are some ""echo-markers"" like definitely, really, and indeed.

Li and Huang (2020) provided instances to show that ""incongruity"" does not have to be between the literal and contextual meanings of irony in certain circumstances. They believed that irony's true nature is a psychological activity as much as a verbal representation. The speaker or listeners must finish the ""reversal"" process on a psychological level for it to be completed. When compared to the concepts of ""echoic"" and ""incongruity,"" ""reversal"" is concerned not only with the results but also with the psychological processes that the speakers/listeners go through.","(p1.0) Various definitions have been given to irony. Early studies suggested that irony is the expression whose real meaning is contradictory to its literal meaning (Grice, 1975). The Merriam-Webster Dictionary, The Oxford English Dictionary, and The Collins English Dictionary all adopted this definition and used the words ""opposite"" or ""contrary"" to explain the relationship between the literal and contextual meanings of irony.

(p1.1) However, more research into various types of ironic examples revealed that the contextual meaning of irony does not have to be ""opposite"" or ""contrary"" to the literal one. According to Sperber and Wilson (1986); Wilson and Sperber (2012), some expressions have no ""literal meaning"" to be challenged because no ""literal meaning"" is mentioned in the context, based on which they raised relevance theory and the ""echoic"" concept. They considered irony as ""an echoic use of language in which the speaker tacitly dissociates herself from an attributed utterance or thought"" (Wilson, 2006). That is, if the ""echoic use"" is incongruous in some ways, the expression can be ironic. Based on this theory, Seto (1998) put forward that there are some ""echo-markers"" like definitely, really, and indeed.

(p1.2) Li and Huang (2020) provided instances to show that ""incongruity"" does not have to be between the literal and contextual meanings of irony in certain circumstances. They believed that irony's true nature is a psychological activity as much as a verbal representation. The speaker or listeners must finish the ""reversal"" process on a psychological level for it to be completed. When compared to the concepts of ""echoic"" and ""incongruity,"" ""reversal"" is concerned not only with the results but also with the psychological processes that the speakers/listeners go through.","[['b13'], ['b69', 'b51', 'b54', 'b68'], []]","[['b13'], ['b69', 'b51', 'b54', 'b68'], []]",5,"1. Various definitions have been given to irony.
2. Early studies suggested that irony is the expression whose real meaning is contradictory to its literal meaning (Grice, 1975).
3. The Merriam-Webster Dictionary, The Oxford English Dictionary, and The Collins English Dictionary all adopted this definition and used the words ""opposite"" or ""contrary"" to explain the relationship between the literal and contextual meanings of irony.
4. However, more research into various types of ironic examples revealed that the contextual meaning of irony does not have to be ""opposite"" or ""contrary"" to the literal one.
5. According to Sperber and Wilson (1986); Wilson and Sperber (2012), some expressions have no ""literal meaning"" to be challenged because no ""literal meaning"" is mentioned in the context, based on which they raised relevance theory and the ""echoic"" concept.
6. They considered irony as ""an echoic use of language in which the speaker tacitly dissociates herself from an attributed utterance or thought"" (Wilson, 2006).
7. That is, if the ""echoic use"" is incongruous in some ways, the expression can be ironic.
8. Based on this theory, Seto (1998) put forward that there are some ""echo-markers"" like definitely, really, and indeed.
9. Li and Huang (2020) provided instances to show that ""incongruity"" does not have to be between the literal and contextual meanings of irony in certain circumstances.
10. They believed that irony's true nature is a psychological activity as much as a verbal representation.
11. The speaker or listeners must finish the ""reversal"" process on a psychological level for it to be completed.
12. When compared to the concepts of ""echoic"" and ""incongruity,"" ""reversal"" is concerned not only with the results but also with the psychological processes that the speakers/listeners go through."
252200083,"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives","Computer Science, Linguistics",https://www.semanticscholar.org/paper/77696f90afa79ac06ea58abcc91bf5e10f72b934,s18,Irony Generation,['p18.0'],"['Irony generation is mostly an underexplored research field besides Zhu et al. (2019), in which they defined irony generation as a style transfer problem, and utilized a Seq2Seq framework (Sutskever et al., 2014) with reinforcement learning to generate ironical counterparts from a non-ironic sentence. Concretely, they designed the overall reward as a harmonic mean of irony reward and sentiment reward, which was trying to capture the sentiment incongruity. In terms of the evaluation, besides traditional natural language generation metrics like BLEU, they also designed task-specific evaluation metrics, which shoule be further enhanced in irony and even figurative language research. Future work in irony generation could be advanced in new PLMs and theoretical accounts. For example, no attempts were made to generate ironical expressions after generative PLMs like BART (Lewis et al., 2020). Controllable irony generation and its interaction with agents are interesting topics remaining for future exploration. Besides, irony theories could be further utilized. In recent research on unsupervised sarcasm generation (Mishra et al., 2019;Chakrabarty et al., 2020), context incongruity, valence reversal, and semantic incongruity were merged to enhance the generation.']","Irony generation is mostly an underexplored research field besides Zhu et al. (2019), in which they defined irony generation as a style transfer problem, and utilized a Seq2Seq framework (Sutskever et al., 2014) with reinforcement learning to generate ironical counterparts from a non-ironic sentence. Concretely, they designed the overall reward as a harmonic mean of irony reward and sentiment reward, which was trying to capture the sentiment incongruity. In terms of the evaluation, besides traditional natural language generation metrics like BLEU, they also designed task-specific evaluation metrics, which shoule be further enhanced in irony and even figurative language research. Future work in irony generation could be advanced in new PLMs and theoretical accounts. For example, no attempts were made to generate ironical expressions after generative PLMs like BART (Lewis et al., 2020). Controllable irony generation and its interaction with agents are interesting topics remaining for future exploration. Besides, irony theories could be further utilized. In recent research on unsupervised sarcasm generation (Mishra et al., 2019;Chakrabarty et al., 2020), context incongruity, valence reversal, and semantic incongruity were merged to enhance the generation.","(p18.0) Irony generation is mostly an underexplored research field besides Zhu et al. (2019), in which they defined irony generation as a style transfer problem, and utilized a Seq2Seq framework (Sutskever et al., 2014) with reinforcement learning to generate ironical counterparts from a non-ironic sentence. Concretely, they designed the overall reward as a harmonic mean of irony reward and sentiment reward, which was trying to capture the sentiment incongruity. In terms of the evaluation, besides traditional natural language generation metrics like BLEU, they also designed task-specific evaluation metrics, which shoule be further enhanced in irony and even figurative language research. Future work in irony generation could be advanced in new PLMs and theoretical accounts. For example, no attempts were made to generate ironical expressions after generative PLMs like BART (Lewis et al., 2020). Controllable irony generation and its interaction with agents are interesting topics remaining for future exploration. Besides, irony theories could be further utilized. In recent research on unsupervised sarcasm generation (Mishra et al., 2019;Chakrabarty et al., 2020), context incongruity, valence reversal, and semantic incongruity were merged to enhance the generation.","[['b73', 'b56', None, 'b27', 'b35']]","[['b73', 'b56', None, 'b27', 'b35']]",5,"1. Irony generation is mostly an underexplored research field besides Zhu et al. (2019), in which they defined irony generation as a style transfer problem, and utilized a Seq2Seq framework (Sutskever et al., 2014) with reinforcement learning to generate ironical counterparts from a non-ironic sentence.
2. Concretely, they designed the overall reward as a harmonic mean of irony reward and sentiment reward, which was trying to capture the sentiment incongruity.
3. In terms of the evaluation, besides traditional natural language generation metrics like BLEU, they also designed task-specific evaluation metrics, which shoule be further enhanced in irony and even figurative language research.
4. Future work in irony generation could be advanced in new PLMs and theoretical accounts.
5. For example, no attempts were made to generate ironical expressions after generative PLMs like BART (Lewis et al., 2020).
6. Controllable irony generation and its interaction with agents are interesting topics remaining for future exploration.
7. Besides, irony theories could be further utilized.
8. In recent research on unsupervised sarcasm generation (Mishra et al., 2019;Chakrabarty et al., 2020), context incongruity, valence reversal, and semantic incongruity were merged to enhance the generation."
252200083,"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives","Computer Science, Linguistics",https://www.semanticscholar.org/paper/77696f90afa79ac06ea58abcc91bf5e10f72b934,s15,Supervised non-neural network era,"['p15.0', 'p15.1', 'p15.2', 'p15.3']","['Most research took irony detection as a simple classification problem. Before the popularity of deep learning, feature engineering is crucial for accurate irony detection. Generally, features could be divided into several levels.', 'Lexical features Lexical features are at the foundational level of NLP features, basically divided into bags of words (BOW) sets, word form sets, and conditional n-gram probabilities (Van Hee et al., 2018b). Representative BOW sets mainly include n-grams and character n-grams. Word form sets fo-cus on number and frequency, such as punctuation numbers, emoticon frequencies, character repetitions, etc. Despite their easiness to get, lexical features were proved effective in much research.', 'Syntactic features Syntax is mainly quantified via parts-of-speech and named entities. After tagging, the number and frequency of both characteristics could act as features in classification models. Besides, hand-crafted syntactic features also included clash before verb tenses (Reyes et al., 2013) and dependency parsing (Cignarella et al., 2020a). Linguistic-motivated features Irony processing is deeply associated with sentiments and emotions. Therefore, researchers have offered many characteristics to capture irony patterns. For example, Reyes et al. (2013) proposed the feature of contextual imbalance, which was quantified via measuring the semantic similarity pairwise. Generally, most features could be categorized into ambiguity  and incongruity (Joshi et al., 2015). Take incongruity as an example, implicit incongruity was defined as a boolean feature checking containing implicit sentiment phrases or not; explicit incongruity was defined as number of times a polarity contrast appears. Theoretical research (Ghosh et al., 2020) is encouraging more semantic and pragmatic features to better capture ironies.', 'Features at various levels were concatenated with classifiers, including naive bayes, decision tress, support vector machines (SVMs), etc. (Van Hee et al., 2018b) to get final classification results.']","Most research took irony detection as a simple classification problem. Before the popularity of deep learning, feature engineering is crucial for accurate irony detection. Generally, features could be divided into several levels.

Lexical features Lexical features are at the foundational level of NLP features, basically divided into bags of words (BOW) sets, word form sets, and conditional n-gram probabilities (Van Hee et al., 2018b). Representative BOW sets mainly include n-grams and character n-grams. Word form sets fo-cus on number and frequency, such as punctuation numbers, emoticon frequencies, character repetitions, etc. Despite their easiness to get, lexical features were proved effective in much research.

Syntactic features Syntax is mainly quantified via parts-of-speech and named entities. After tagging, the number and frequency of both characteristics could act as features in classification models. Besides, hand-crafted syntactic features also included clash before verb tenses (Reyes et al., 2013) and dependency parsing (Cignarella et al., 2020a). Linguistic-motivated features Irony processing is deeply associated with sentiments and emotions. Therefore, researchers have offered many characteristics to capture irony patterns. For example, Reyes et al. (2013) proposed the feature of contextual imbalance, which was quantified via measuring the semantic similarity pairwise. Generally, most features could be categorized into ambiguity  and incongruity (Joshi et al., 2015). Take incongruity as an example, implicit incongruity was defined as a boolean feature checking containing implicit sentiment phrases or not; explicit incongruity was defined as number of times a polarity contrast appears. Theoretical research (Ghosh et al., 2020) is encouraging more semantic and pragmatic features to better capture ironies.

Features at various levels were concatenated with classifiers, including naive bayes, decision tress, support vector machines (SVMs), etc. (Van Hee et al., 2018b) to get final classification results.","(p15.0) Most research took irony detection as a simple classification problem. Before the popularity of deep learning, feature engineering is crucial for accurate irony detection. Generally, features could be divided into several levels.

(p15.1) Lexical features Lexical features are at the foundational level of NLP features, basically divided into bags of words (BOW) sets, word form sets, and conditional n-gram probabilities (Van Hee et al., 2018b). Representative BOW sets mainly include n-grams and character n-grams. Word form sets fo-cus on number and frequency, such as punctuation numbers, emoticon frequencies, character repetitions, etc. Despite their easiness to get, lexical features were proved effective in much research.

(p15.2) Syntactic features Syntax is mainly quantified via parts-of-speech and named entities. After tagging, the number and frequency of both characteristics could act as features in classification models. Besides, hand-crafted syntactic features also included clash before verb tenses (Reyes et al., 2013) and dependency parsing (Cignarella et al., 2020a). Linguistic-motivated features Irony processing is deeply associated with sentiments and emotions. Therefore, researchers have offered many characteristics to capture irony patterns. For example, Reyes et al. (2013) proposed the feature of contextual imbalance, which was quantified via measuring the semantic similarity pairwise. Generally, most features could be categorized into ambiguity  and incongruity (Joshi et al., 2015). Take incongruity as an example, implicit incongruity was defined as a boolean feature checking containing implicit sentiment phrases or not; explicit incongruity was defined as number of times a polarity contrast appears. Theoretical research (Ghosh et al., 2020) is encouraging more semantic and pragmatic features to better capture ironies.

(p15.3) Features at various levels were concatenated with classifiers, including naive bayes, decision tress, support vector machines (SVMs), etc. (Van Hee et al., 2018b) to get final classification results.","[[], ['b63'], [None, 'b18', 'b8', 'b46'], []]","[[], ['b63'], [None, 'b18', 'b8', 'b46'], []]",5,"1. Most research took irony detection as a simple classification problem.
2. Before the popularity of deep learning, feature engineering is crucial for accurate irony detection.
3. Generally, features could be divided into several levels.
4. Lexical features Lexical features are at the foundational level of NLP features, basically divided into bags of words (BOW) sets, word form sets, and conditional n-gram probabilities (Van Hee et al., 2018b).
5. Representative BOW sets mainly include n-grams and character n-grams.
6. Word form sets fo-cus on number and frequency, such as punctuation numbers, emoticon frequencies, character repetitions, etc.
7. Despite their easiness to get, lexical features were proved effective in much research.
8. Syntactic features Syntax is mainly quantified via parts-of-speech and named entities.
9. After tagging, the number and frequency of both characteristics could act as features in classification models.
10. Besides, hand-crafted syntactic features also included clash before verb tenses (Reyes et al., 2013) and dependency parsing (Cignarella et al., 2020a).
11. Linguistic-motivated features Irony processing is deeply associated with sentiments and emotions.
12. Therefore, researchers have offered many characteristics to capture irony patterns.
13. For example, Reyes et al. (2013) proposed the feature of contextual imbalance, which was quantified via measuring the semantic similarity pairwise.
14. Generally, most features could be categorized into ambiguity  and incongruity (Joshi et al., 2015).
15. Take incongruity as an example, implicit incongruity was defined as a boolean feature checking containing implicit sentiment phrases or not; explicit incongruity was defined as number of times a polarity contrast appears.
16. Theoretical research (Ghosh et al., 2020) is encouraging more semantic and pragmatic features to better capture ironies.
17. Features at various levels were concatenated with classifiers, including naive bayes, decision tress, support vector machines (SVMs), etc. (Van Hee et al., 2018b) to get final classification results."
252683270,A Decade of Knowledge Graphs in Natural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/2341353cae858ce06225e46356c472b71dc63372,s9,Knowledge Graph Construction,['p9.0'],"['The task of entity extraction is a starting point in constructing KGs and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020). Once the relevant entities are singled out, relationships and interactions between them are found with the task of relation extraction (Zhang et al., 2019a). A lot of papers use both entity extraction and relation extraction to construct new KGs, e.g., for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018). Entity linking is a task of linking entities recognized in some text to already existing entities in KGs (Moon et al., 2018;. Since synonymous or similar entities often exist in different KGs or in different languages, entity alignment can be performed to reduce redundancy and repetition in future tasks (Gangemi et al., 2016;Chen et al., 2018). Coming up with the rules and schemes of KGs, i.e., their structure and format of knowledge presented in it, is done with the task of ontology construction (Haussmann et al., 2019).']","The task of entity extraction is a starting point in constructing KGs and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020). Once the relevant entities are singled out, relationships and interactions between them are found with the task of relation extraction (Zhang et al., 2019a). A lot of papers use both entity extraction and relation extraction to construct new KGs, e.g., for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018). Entity linking is a task of linking entities recognized in some text to already existing entities in KGs (Moon et al., 2018;. Since synonymous or similar entities often exist in different KGs or in different languages, entity alignment can be performed to reduce redundancy and repetition in future tasks (Gangemi et al., 2016;Chen et al., 2018). Coming up with the rules and schemes of KGs, i.e., their structure and format of knowledge presented in it, is done with the task of ontology construction (Haussmann et al., 2019).","(p9.0) The task of entity extraction is a starting point in constructing KGs and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020). Once the relevant entities are singled out, relationships and interactions between them are found with the task of relation extraction (Zhang et al., 2019a). A lot of papers use both entity extraction and relation extraction to construct new KGs, e.g., for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018). Entity linking is a task of linking entities recognized in some text to already existing entities in KGs (Moon et al., 2018;. Since synonymous or similar entities often exist in different KGs or in different languages, entity alignment can be performed to reduce redundancy and repetition in future tasks (Gangemi et al., 2016;Chen et al., 2018). Coming up with the rules and schemes of KGs, i.e., their structure and format of knowledge presented in it, is done with the task of ontology construction (Haussmann et al., 2019).","[['b65', 'b32', 'b46', 'b29', 'b19', 'b6', 'b17', 'b1']]","[['b65', 'b32', 'b46', 'b29', 'b19', 'b6', 'b17', 'b1']]",8,"1. The task of entity extraction is a starting point in constructing KGs and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020).
2. Once the relevant entities are singled out, relationships and interactions between them are found with the task of relation extraction (Zhang et al., 2019a).
3. A lot of papers use both entity extraction and relation extraction to construct new KGs, e.g., for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018).
4. Entity linking is a task of linking entities recognized in some text to already existing entities in KGs (Moon et al., 2018;. Since synonymous or similar entities often exist in different KGs or in different languages, entity alignment can be performed to reduce redundancy and repetition in future tasks (Gangemi et al., 2016;Chen et al., 2018).
5. Coming up with the rules and schemes of KGs, i.e., their structure and format of knowledge presented in it, is done with the task of ontology construction (Haussmann et al., 2019)."
252683270,A Decade of Knowledge Graphs in Natural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/2341353cae858ce06225e46356c472b71dc63372,s11,Knowledge Application,"['p11.0', 'p11.1', 'p11.2', 'p11.3', 'p11.4', 'p11.5']","['Existing KGs can be used in a multitude of popular NLP tasks. Here we outline the most popular ones.', 'Question answering (QA) was found to be the most common NLP task using KGs. This task is typically divided into textual QA and question answering over knowledge bases (KBQA). Textual QA derives answers from unstructured documents while KBQA does so from predefined knowledge bases (Fu et al., 2020). KBQA is naturally tied to KGs while textual QA can also be approached by using KGs as a source of common-sense knowledge when answering questions. As Zhu et al. (2021) conclude, this approach is desired not only because it is helpful for generating answers, but also because it makes answers more interpretable.', 'Semantic search refers to ""search with meaning"", where the goal is not just to search for literal matches, but to understand the search intent and query context as well (Bast et al., 2016). This label denoted studies that use KGs for search, recommendations, and analytics. Examples are a big semantic network of everyday concepts called ConceptNet (Speer et al., 2017) and a KG of scholarly communications and the relationships, among them the Microsoft Academic Graph .', 'Conversational interfaces constitute another NLP field that can benefit from world knowledge contained in KGs. Zhou et al. (2018)  Natural language generation (NLG) is a subfield of NLP and computational linguistics that is concerned with models which generate natural language output from scratch. KGs are used in this subfield for producing natural language text from KGs (Koncel-Kedziorski et al., 2019), generating question-answer pairs (Reddy et al., 2017), the multi-modal task of image captioning (Lu et al., 2018), or data augmentation in low-resource settings (Sharifirad et al., 2018).', 'Text analysis combines various analytical NLP techniques and methods that are applied to process and understand textual data. Exemplary tasks are sentiment detection (Kumar et al., 2018), topic modeling , or word sense disambiguation .', 'Augmented language models are a combination of large pretrained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) with knowledge contained in KGs. Since PLMs derive their knowledge from huge amounts of unstructured training data, a rising research trend is in combining them with structured knowledge. Knowledge from KGs can be infused into language models in their input, architecture, output, or some combination thereof (Colon-Hernandez et al., 2021). Some notable examples we outline are ERNIE (Zhang et al., 2019b), COMET (Bosselut et al., 2019), K-BERT (Liu et al., 2020b), and KEPLER (Wang et al., 2021b). Table 3 shows the distribution of papers according to the different research and contribution types as defined in Table 4 and 5 in the Appendix. It shows that most papers conduct validation research, investigating new techniques or methods that have not yet been implemented in practice. A considerable number of papers, although significantly less, focus on solution proposals of approaches by demonstrating their advantages and applicability by a small example or argumentation. However, these papers usually lack a profound empirical evaluation. Secondary research accounts for only a small number of papers and is severely underrepresented in the research field of KGs in NLP. As already mentioned in Section 1 and Section 2, there is a notable lack of studies that summarize, compile, or synthesize existing research regarding KGs in NLP. Moreover, evaluation research papers that implement and evaluate approaches in an industry context are equally scarce. Opinion papers are almost non-existent.']","Existing KGs can be used in a multitude of popular NLP tasks. Here we outline the most popular ones.

Question answering (QA) was found to be the most common NLP task using KGs. This task is typically divided into textual QA and question answering over knowledge bases (KBQA). Textual QA derives answers from unstructured documents while KBQA does so from predefined knowledge bases (Fu et al., 2020). KBQA is naturally tied to KGs while textual QA can also be approached by using KGs as a source of common-sense knowledge when answering questions. As Zhu et al. (2021) conclude, this approach is desired not only because it is helpful for generating answers, but also because it makes answers more interpretable.

Semantic search refers to ""search with meaning"", where the goal is not just to search for literal matches, but to understand the search intent and query context as well (Bast et al., 2016). This label denoted studies that use KGs for search, recommendations, and analytics. Examples are a big semantic network of everyday concepts called ConceptNet (Speer et al., 2017) and a KG of scholarly communications and the relationships, among them the Microsoft Academic Graph .

Conversational interfaces constitute another NLP field that can benefit from world knowledge contained in KGs. Zhou et al. (2018)  Natural language generation (NLG) is a subfield of NLP and computational linguistics that is concerned with models which generate natural language output from scratch. KGs are used in this subfield for producing natural language text from KGs (Koncel-Kedziorski et al., 2019), generating question-answer pairs (Reddy et al., 2017), the multi-modal task of image captioning (Lu et al., 2018), or data augmentation in low-resource settings (Sharifirad et al., 2018).

Text analysis combines various analytical NLP techniques and methods that are applied to process and understand textual data. Exemplary tasks are sentiment detection (Kumar et al., 2018), topic modeling , or word sense disambiguation .

Augmented language models are a combination of large pretrained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) with knowledge contained in KGs. Since PLMs derive their knowledge from huge amounts of unstructured training data, a rising research trend is in combining them with structured knowledge. Knowledge from KGs can be infused into language models in their input, architecture, output, or some combination thereof (Colon-Hernandez et al., 2021). Some notable examples we outline are ERNIE (Zhang et al., 2019b), COMET (Bosselut et al., 2019), K-BERT (Liu et al., 2020b), and KEPLER (Wang et al., 2021b). Table 3 shows the distribution of papers according to the different research and contribution types as defined in Table 4 and 5 in the Appendix. It shows that most papers conduct validation research, investigating new techniques or methods that have not yet been implemented in practice. A considerable number of papers, although significantly less, focus on solution proposals of approaches by demonstrating their advantages and applicability by a small example or argumentation. However, these papers usually lack a profound empirical evaluation. Secondary research accounts for only a small number of papers and is severely underrepresented in the research field of KGs in NLP. As already mentioned in Section 1 and Section 2, there is a notable lack of studies that summarize, compile, or synthesize existing research regarding KGs in NLP. Moreover, evaluation research papers that implement and evaluate approaches in an industry context are equally scarce. Opinion papers are almost non-existent.","(p11.0) Existing KGs can be used in a multitude of popular NLP tasks. Here we outline the most popular ones.

(p11.1) Question answering (QA) was found to be the most common NLP task using KGs. This task is typically divided into textual QA and question answering over knowledge bases (KBQA). Textual QA derives answers from unstructured documents while KBQA does so from predefined knowledge bases (Fu et al., 2020). KBQA is naturally tied to KGs while textual QA can also be approached by using KGs as a source of common-sense knowledge when answering questions. As Zhu et al. (2021) conclude, this approach is desired not only because it is helpful for generating answers, but also because it makes answers more interpretable.

(p11.2) Semantic search refers to ""search with meaning"", where the goal is not just to search for literal matches, but to understand the search intent and query context as well (Bast et al., 2016). This label denoted studies that use KGs for search, recommendations, and analytics. Examples are a big semantic network of everyday concepts called ConceptNet (Speer et al., 2017) and a KG of scholarly communications and the relationships, among them the Microsoft Academic Graph .

(p11.3) Conversational interfaces constitute another NLP field that can benefit from world knowledge contained in KGs. Zhou et al. (2018)  Natural language generation (NLG) is a subfield of NLP and computational linguistics that is concerned with models which generate natural language output from scratch. KGs are used in this subfield for producing natural language text from KGs (Koncel-Kedziorski et al., 2019), generating question-answer pairs (Reddy et al., 2017), the multi-modal task of image captioning (Lu et al., 2018), or data augmentation in low-resource settings (Sharifirad et al., 2018).

(p11.4) Text analysis combines various analytical NLP techniques and methods that are applied to process and understand textual data. Exemplary tasks are sentiment detection (Kumar et al., 2018), topic modeling , or word sense disambiguation .

(p11.5) Augmented language models are a combination of large pretrained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) with knowledge contained in KGs. Since PLMs derive their knowledge from huge amounts of unstructured training data, a rising research trend is in combining them with structured knowledge. Knowledge from KGs can be infused into language models in their input, architecture, output, or some combination thereof (Colon-Hernandez et al., 2021). Some notable examples we outline are ERNIE (Zhang et al., 2019b), COMET (Bosselut et al., 2019), K-BERT (Liu et al., 2020b), and KEPLER (Wang et al., 2021b). Table 3 shows the distribution of papers according to the different research and contribution types as defined in Table 4 and 5 in the Appendix. It shows that most papers conduct validation research, investigating new techniques or methods that have not yet been implemented in practice. A considerable number of papers, although significantly less, focus on solution proposals of approaches by demonstrating their advantages and applicability by a small example or argumentation. However, these papers usually lack a profound empirical evaluation. Secondary research accounts for only a small number of papers and is severely underrepresented in the research field of KGs in NLP. As already mentioned in Section 1 and Section 2, there is a notable lack of studies that summarize, compile, or synthesize existing research regarding KGs in NLP. Moreover, evaluation research papers that implement and evaluate approaches in an industry context are equally scarce. Opinion papers are almost non-existent.","[[], [None], ['b53', 'b4'], ['b44', 'b49', 'b28', 'b68'], ['b21'], ['b11', 'b12', None, 'b67', 'b59', 'b5', 'b43']]","[[], [None], ['b53', 'b4'], ['b44', 'b49', 'b28', 'b68'], ['b21'], ['b11', 'b12', None, 'b67', 'b59', 'b5', 'b43']]",15,"1. Existing KGs can be used in a multitude of popular NLP tasks.
2. Here we outline the most popular ones.
3. Question answering (QA) was found to be the most common NLP task using KGs.
4. This task is typically divided into textual QA and question answering over knowledge bases (KBQA).
5. Textual QA derives answers from unstructured documents while KBQA does so from predefined knowledge bases (Fu et al., 2020).
6. KBQA is naturally tied to KGs while textual QA can also be approached by using KGs as a source of common-sense knowledge when answering questions.
7. As Zhu et al. (2021) conclude, this approach is desired not only because it is helpful for generating answers, but also because it makes answers more interpretable.
8. Semantic search refers to ""search with meaning"", where the goal is not just to search for literal matches, but to understand the search intent and query context as well (Bast et al., 2016).
9. This label denoted studies that use KGs for search, recommendations, and analytics.
10. Examples are a big semantic network of everyday concepts called ConceptNet (Speer et al., 2017) and a KG of scholarly communications and the relationships, among them the Microsoft Academic Graph .
11. Conversational interfaces constitute another NLP field that can benefit from world knowledge contained in KGs.
12. Zhou et al. (2018)  Natural language generation (NLG) is a subfield of NLP and computational linguistics that is concerned with models which generate natural language output from scratch.
13. KGs are used in this subfield for producing natural language text from KGs (Koncel-Kedziorski et al., 2019), generating question-answer pairs (Reddy et al., 2017), the multi-modal task of image captioning (Lu et al., 2018), or data augmentation in low-resource settings (Sharifirad et al., 2018).
14. Text analysis combines various analytical NLP techniques and methods that are applied to process and understand textual data.
15. Exemplary tasks are sentiment detection (Kumar et al., 2018), topic modeling , or word sense disambiguation .
16. Augmented language models are a combination of large pretrained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) with knowledge contained in KGs.
17. Since PLMs derive their knowledge from huge amounts of unstructured training data, a rising research trend is in combining them with structured knowledge.
18. Knowledge from KGs can be infused into language models in their input, architecture, output, or some combination thereof (Colon-Hernandez et al., 2021).
19. Some notable examples we outline are ERNIE (Zhang et al., 2019b), COMET (Bosselut et al., 2019), K-BERT (Liu et al., 2020b), and KEPLER (Wang et al., 2021b).
20. Table 3 shows the distribution of papers according to the different research and contribution types as defined in Table 4 and 5 in the Appendix.
21. It shows that most papers conduct validation research, investigating new techniques or methods that have not yet been implemented in practice.
22. A considerable number of papers, although significantly less, focus on solution proposals of approaches by demonstrating their advantages and applicability by a small example or argumentation.
23. However, these papers usually lack a profound empirical evaluation.
24. Secondary research accounts for only a small number of papers and is severely underrepresented in the research field of KGs in NLP.
25. As already mentioned in Section 1 and Section 2, there is a notable lack of studies that summarize, compile, or synthesize existing research regarding KGs in NLP.
26. Moreover, evaluation research papers that implement and evaluate approaches in an industry context are equally scarce.
27. Opinion papers are almost non-existent."
252683270,A Decade of Knowledge Graphs in Natural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/2341353cae858ce06225e46356c472b71dc63372,s10,Knowledge Graph Reasoning,"['p10.0', 'p10.1']","['Once constructed, KGs contain structured world knowledge and can be used to infer new knowledge by reasoning over them. Thereby, the task of classifying entities is called entity classification, while link prediction is the task of inferring missing links between entities in existing KGs often performed via ranking entities as possible answers to queries (Shi and Weninger, 2018;Bosselut et al., 2019;Ali et al., 2021).', 'Knowledge graph embedding techniques are used to create dense vector representations of a graph so that they can then be used for downstream machine learning tasks. While this problem can be related solely to KGs, in our survey this label refers to approaches that jointly learn text and graph embeddings (Chen et al., 2018;Wang et al., 2021b).']","Once constructed, KGs contain structured world knowledge and can be used to infer new knowledge by reasoning over them. Thereby, the task of classifying entities is called entity classification, while link prediction is the task of inferring missing links between entities in existing KGs often performed via ranking entities as possible answers to queries (Shi and Weninger, 2018;Bosselut et al., 2019;Ali et al., 2021).

Knowledge graph embedding techniques are used to create dense vector representations of a graph so that they can then be used for downstream machine learning tasks. While this problem can be related solely to KGs, in our survey this label refers to approaches that jointly learn text and graph embeddings (Chen et al., 2018;Wang et al., 2021b).","(p10.0) Once constructed, KGs contain structured world knowledge and can be used to infer new knowledge by reasoning over them. Thereby, the task of classifying entities is called entity classification, while link prediction is the task of inferring missing links between entities in existing KGs often performed via ranking entities as possible answers to queries (Shi and Weninger, 2018;Bosselut et al., 2019;Ali et al., 2021).

(p10.1) Knowledge graph embedding techniques are used to create dense vector representations of a graph so that they can then be used for downstream machine learning tasks. While this problem can be related solely to KGs, in our survey this label refers to approaches that jointly learn text and graph embeddings (Chen et al., 2018;Wang et al., 2021b).","[['b51', 'b2', 'b5'], ['b59', 'b6']]","[['b51', 'b2', 'b5'], ['b59', 'b6']]",5,"1. Once constructed, KGs contain structured world knowledge and can be used to infer new knowledge by reasoning over them.
2. Thereby, the task of classifying entities is called entity classification, while link prediction is the task of inferring missing links between entities in existing KGs often performed via ranking entities as possible answers to queries (Shi and Weninger, 2018;Bosselut et al., 2019;Ali et al., 2021).
3. Knowledge graph embedding techniques are used to create dense vector representations of a graph so that they can then be used for downstream machine learning tasks.
4. While this problem can be related solely to KGs, in our survey this label refers to approaches that jointly learn text and graph embeddings (Chen et al., 2018;Wang et al., 2021b)."
252683270,A Decade of Knowledge Graphs in Natural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/2341353cae858ce06225e46356c472b71dc63372,s5,Classification Scheme and Data Extraction,['p5.0'],"[""According to our RQs, the included papers had to be categorized with respect to three facets: task, research type, and contribution. Established classification schemes from Wieringa et al. (2006) and Shaw (2003) were adapted for the research and contribution type as presented in Appendix A. For classifying tasks, we constructed a task taxonomy, following the iterative procedure suggested by Petersen et al. (2008), in which an initial classification scheme derived from keywords continuously evolves through adding, merging, or splitting categories during the classification process. Our task taxonomy is based on existing schemes from Paulheim (2017) Once the initial schemes were set up, all papers were sorted into the classes as part of the data extraction process. The 507 included studies were divided between two of the authors. In regular sessions, they discussed changes to the classification schemes or clarified uncertain labels. While each paper got assigned one label for the research type assigned, multiple labels were possible with regard to tasks and contributions. To assess the reliability of the inter-annotator agreement, the two authors independently classified a random sample of 50 papers. We calculated Cohen's Kappa coefficient of these annotations for each facet (Cohen, 1960). ""]","According to our RQs, the included papers had to be categorized with respect to three facets: task, research type, and contribution. Established classification schemes from Wieringa et al. (2006) and Shaw (2003) were adapted for the research and contribution type as presented in Appendix A. For classifying tasks, we constructed a task taxonomy, following the iterative procedure suggested by Petersen et al. (2008), in which an initial classification scheme derived from keywords continuously evolves through adding, merging, or splitting categories during the classification process. Our task taxonomy is based on existing schemes from Paulheim (2017) Once the initial schemes were set up, all papers were sorted into the classes as part of the data extraction process. The 507 included studies were divided between two of the authors. In regular sessions, they discussed changes to the classification schemes or clarified uncertain labels. While each paper got assigned one label for the research type assigned, multiple labels were possible with regard to tasks and contributions. To assess the reliability of the inter-annotator agreement, the two authors independently classified a random sample of 50 papers. We calculated Cohen's Kappa coefficient of these annotations for each facet (Cohen, 1960). ","(p5.0) According to our RQs, the included papers had to be categorized with respect to three facets: task, research type, and contribution. Established classification schemes from Wieringa et al. (2006) and Shaw (2003) were adapted for the research and contribution type as presented in Appendix A. For classifying tasks, we constructed a task taxonomy, following the iterative procedure suggested by Petersen et al. (2008), in which an initial classification scheme derived from keywords continuously evolves through adding, merging, or splitting categories during the classification process. Our task taxonomy is based on existing schemes from Paulheim (2017) Once the initial schemes were set up, all papers were sorted into the classes as part of the data extraction process. The 507 included studies were divided between two of the authors. In regular sessions, they discussed changes to the classification schemes or clarified uncertain labels. While each paper got assigned one label for the research type assigned, multiple labels were possible with regard to tasks and contributions. To assess the reliability of the inter-annotator agreement, the two authors independently classified a random sample of 50 papers. We calculated Cohen's Kappa coefficient of these annotations for each facet (Cohen, 1960). ","[['b61', 'b9', 'b40', 'b50']]","[['b61', 'b9', 'b40', 'b50']]",4,"1. According to our RQs, the included papers had to be categorized with respect to three facets: task, research type, and contribution.
2. Established classification schemes from Wieringa et al. (2006) and Shaw (2003) were adapted for the research and contribution type as presented in Appendix A. For classifying tasks, we constructed a task taxonomy, following the iterative procedure suggested by Petersen et al. (2008), in which an initial classification scheme derived from keywords continuously evolves through adding, merging, or splitting categories during the classification process.
3. Our task taxonomy is based on existing schemes from Paulheim (2017)
4. Once the initial schemes were set up, all papers were sorted into the classes as part of the data extraction process.
5. The 507 included studies were divided between two of the authors.
6. In regular sessions, they discussed changes to the classification schemes or clarified uncertain labels.
7. While each paper got assigned one label for the research type assigned, multiple labels were possible with regard to tasks and contributions.
8. To assess the reliability of the inter-annotator agreement, the two authors independently classified a random sample of 50 papers.
9. We calculated Cohen's Kappa coefficient of these annotations for each facet (Cohen, 1960)."
252683270,A Decade of Knowledge Graphs in Natural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/2341353cae858ce06225e46356c472b71dc63372,s1,Related Work,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4']","['Related literature that includes both KGs and NLP seems to be relatively scarce. Most survey papers focus either only on KGs or only on NLP. In their broad introduction to KGs, Hogan et al. (2022) point out that existing surveys on KGs tend to revolve around specific aspects of KGs, most commonly their construction and embedding.', 'Such surveys with a KG focus usually bring up NLP only in the context of employed NLP methods, like information extraction, being used to populate and refine graphs (Nickel et al., 2016). Other surveys on KGs mention some downstream applications of KGs for NLP tasks, such as for con-structing augmented language models, question answering over knowledge bases (KBQA), or recommender systems (Ji et al., 2021).', 'As noted previously, related work that includes both KGs and NLP strictly focus on a specific application or task. For example, Safavi and Koutra (2021) provide an overview on applying relational world knowledge from KGs to augment large contextual language models. Other surveys on specific applications include KG reasoning , biomedical KGs (Nicholson and Greene, 2020), and the task of KBQA (Fu et al., 2020).', 'The survey on graphs in NLP by Nastase et al. (2015) covers only smaller graphs such as dependency graphs and dialogue trees. Even though it does not include KGs, the survey concludes that graphs are a powerful representation formalism and how NLP tasks can benefit from harnessing the potential of data presented in graph structures.', 'To the best of our knowledge, this is the first survey covering a wide spectrum of techniques, methods as well as applications of KGs within the NLP research field.']","Related literature that includes both KGs and NLP seems to be relatively scarce. Most survey papers focus either only on KGs or only on NLP. In their broad introduction to KGs, Hogan et al. (2022) point out that existing surveys on KGs tend to revolve around specific aspects of KGs, most commonly their construction and embedding.

Such surveys with a KG focus usually bring up NLP only in the context of employed NLP methods, like information extraction, being used to populate and refine graphs (Nickel et al., 2016). Other surveys on KGs mention some downstream applications of KGs for NLP tasks, such as for con-structing augmented language models, question answering over knowledge bases (KBQA), or recommender systems (Ji et al., 2021).

As noted previously, related work that includes both KGs and NLP strictly focus on a specific application or task. For example, Safavi and Koutra (2021) provide an overview on applying relational world knowledge from KGs to augment large contextual language models. Other surveys on specific applications include KG reasoning , biomedical KGs (Nicholson and Greene, 2020), and the task of KBQA (Fu et al., 2020).

The survey on graphs in NLP by Nastase et al. (2015) covers only smaller graphs such as dependency graphs and dialogue trees. Even though it does not include KGs, the survey concludes that graphs are a powerful representation formalism and how NLP tasks can benefit from harnessing the potential of data presented in graph structures.

To the best of our knowledge, this is the first survey covering a wide spectrum of techniques, methods as well as applications of KGs within the NLP research field.","(p1.0) Related literature that includes both KGs and NLP seems to be relatively scarce. Most survey papers focus either only on KGs or only on NLP. In their broad introduction to KGs, Hogan et al. (2022) point out that existing surveys on KGs tend to revolve around specific aspects of KGs, most commonly their construction and embedding.

(p1.1) Such surveys with a KG focus usually bring up NLP only in the context of employed NLP methods, like information extraction, being used to populate and refine graphs (Nickel et al., 2016). Other surveys on KGs mention some downstream applications of KGs for NLP tasks, such as for con-structing augmented language models, question answering over knowledge bases (KBQA), or recommender systems (Ji et al., 2021).

(p1.2) As noted previously, related work that includes both KGs and NLP strictly focus on a specific application or task. For example, Safavi and Koutra (2021) provide an overview on applying relational world knowledge from KGs to augment large contextual language models. Other surveys on specific applications include KG reasoning , biomedical KGs (Nicholson and Greene, 2020), and the task of KBQA (Fu et al., 2020).

(p1.3) The survey on graphs in NLP by Nastase et al. (2015) covers only smaller graphs such as dependency graphs and dialogue trees. Even though it does not include KGs, the survey concludes that graphs are a powerful representation formalism and how NLP tasks can benefit from harnessing the potential of data presented in graph structures.

(p1.4) To the best of our knowledge, this is the first survey covering a wide spectrum of techniques, methods as well as applications of KGs within the NLP research field.","[[None], ['b37', None], [None, 'b47'], ['b35'], []]","[[None], ['b37', None], [None, 'b47'], ['b35'], []]",6,"1. Related literature that includes both KGs and NLP seems to be relatively scarce.
2. Most survey papers focus either only on KGs or only on NLP.
3. In their broad introduction to KGs, Hogan et al. (2022) point out that existing surveys on KGs tend to revolve around specific aspects of KGs, most commonly their construction and embedding.
4. Such surveys with a KG focus usually bring up NLP only in the context of employed NLP methods, like information extraction, being used to populate and refine graphs (Nickel et al., 2016).
5. Other surveys on KGs mention some downstream applications of KGs for NLP tasks, such as for con-structing augmented language models, question answering over knowledge bases (KBQA), or recommender systems (Ji et al., 2021).
6. As noted previously, related work that includes both KGs and NLP strictly focus on a specific application or task.
7. For example, Safavi and Koutra (2021) provide an overview on applying relational world knowledge from KGs to augment large contextual language models.
8. Other surveys on specific applications include KG reasoning , biomedical KGs (Nicholson and Greene, 2020), and the task of KBQA (Fu et al., 2020).
9. The survey on graphs in NLP by Nastase et al. (2015) covers only smaller graphs such as dependency graphs and dialogue trees.
10. Even though it does not include KGs, the survey concludes that graphs are a powerful representation formalism and how NLP tasks can benefit from harnessing the potential of data presented in graph structures.
11. To the best of our knowledge, this is the first survey covering a wide spectrum of techniques, methods as well as applications of KGs within the NLP research field."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Computer Science,https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,s11,Semantic Polarity,['p11.0'],"['Mehrabi et al. (2021b) focused on biases in common sense KGs like ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) (contains sentences) which are at risk of causing representational harms (see Section 2.2). They utilized regard (Sheng et al., 2019) and sentiment as intermediate bias proxies. Both concepts express the polarity of statements and can be measured via classifiers that predict a neutral, negative, or positive label (Sheng et al., 2019;Dhamala et al., 2021). Groups that are referred to in a mostly positive way are interpreted as favored and vice versa. Mehrabi et al. (2021b) applied this principle to natural language statements generated from Con-ceptNet triples. They found that subject and object entities relating to the professions CEO, nurse, and physician were more often favored while performing artist, politician, and prisoner were more often disfavored. Similarly, several Islam-related entities were on the negative end while Christian and Hindu were more ambiguously valuated. As for gender, no significant difference was found.']","Mehrabi et al. (2021b) focused on biases in common sense KGs like ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) (contains sentences) which are at risk of causing representational harms (see Section 2.2). They utilized regard (Sheng et al., 2019) and sentiment as intermediate bias proxies. Both concepts express the polarity of statements and can be measured via classifiers that predict a neutral, negative, or positive label (Sheng et al., 2019;Dhamala et al., 2021). Groups that are referred to in a mostly positive way are interpreted as favored and vice versa. Mehrabi et al. (2021b) applied this principle to natural language statements generated from Con-ceptNet triples. They found that subject and object entities relating to the professions CEO, nurse, and physician were more often favored while performing artist, politician, and prisoner were more often disfavored. Similarly, several Islam-related entities were on the negative end while Christian and Hindu were more ambiguously valuated. As for gender, no significant difference was found.","(p11.0) Mehrabi et al. (2021b) focused on biases in common sense KGs like ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) (contains sentences) which are at risk of causing representational harms (see Section 2.2). They utilized regard (Sheng et al., 2019) and sentiment as intermediate bias proxies. Both concepts express the polarity of statements and can be measured via classifiers that predict a neutral, negative, or positive label (Sheng et al., 2019;Dhamala et al., 2021). Groups that are referred to in a mostly positive way are interpreted as favored and vice versa. Mehrabi et al. (2021b) applied this principle to natural language statements generated from Con-ceptNet triples. They found that subject and object entities relating to the professions CEO, nurse, and physician were more often favored while performing artist, politician, and prisoner were more often disfavored. Similarly, several Islam-related entities were on the negative end while Christian and Hindu were more ambiguously valuated. As for gender, no significant difference was found.","[['b77', 'b20', 'b7', None, 'b78']]","[['b77', 'b20', 'b7', None, 'b78']]",5,"1. Mehrabi et al. (2021b) focused on biases in common sense KGs like ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) (contains sentences) which are at risk of causing representational harms (see Section 2.2).
2. They utilized regard (Sheng et al., 2019) and sentiment as intermediate bias proxies.
3. Both concepts express the polarity of statements and can be measured via classifiers that predict a neutral, negative, or positive label (Sheng et al., 2019;Dhamala et al., 2021).
4. Groups that are referred to in a mostly positive way are interpreted as favored and vice versa.
5. Mehrabi et al. (2021b) applied this principle to natural language statements generated from Con-ceptNet triples.
6. They found that subject and object entities relating to the professions CEO, nurse, and physician were more often favored while performing artist, politician, and prisoner were more often disfavored.
7. Similarly, several Islam-related entities were on the negative end while Christian and Hindu were more ambiguously valuated.
8. As for gender, no significant difference was found."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Computer Science,https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,s25,Recommendations,"['p25.0', 'p25.1', 'p25.2', 'p25.3']","['To avoid harms caused by biases in KGs and their embeddings, we identify and recommend several actions for practitioners and researchers.', 'Transparency and Accountability KGs should by default be published with bias-sensitive documentation to facilitate transparency and accountability regarding potential risks. Data Statements (Bender and Friedman, 2018) report curation criteria, language variety, demographics of the data authors and annotators, relevant indicators of context, quality, and provenance. Datasheets for Datasets (Gebru et al., 2021) additionally state motivation, composition, preparation, distribution, and maintenance. The associated questionnaire can accompany the dataset creation process to avoid risks early on. Especially in the case of ongoing crowdsourcing efforts for encyclopedic KGs the demographic background of contributors should be reported (Demartini, 2019). Researchers using subsets of these KGs, should investigate respective data dumps for potential biases and report limitations transparently. Similarly, KG embedding models should be published with Model Cards (Mitchell et al., 2019) documenting intended use, underlying data, ethical considerations, and limitations. Stating the contact details for reporting problems and concerns establishes accountability (Mitchell et al., 2019;Gebru et al., 2021).', 'Improving Representativeness To tackle selection bias, data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints. Annotations should be determined via aggregation (see Hovy and Prabhumoye, 2021). For open editable KGs, interventions like edit-a-thons are helpful to introduce more authors from underrepresented groups (Vetter et al., 2022) (e.g., the Art+Feminism campaign aims to fill the gender gap in Wikimedia knowledge bases 8 ). In order for such interventions to take effect, research must update data bases and benchmarks frequently (see Koch et al., 2021). In addition, the timeliness of encyclopedic data is necessary to avoid perpetuating historic biases.', 'Tackling Algorithmic Bias Evaluation and prevention of harmful biases must become part of the development pipeline (Stanczak and Augenstein, 2021). Algorithmic biases are best evaluated with a combination of multiple quantitative (Section 5) and qualitative measures (Kraft et al., 2022;Dev et al., 2021), considering multiple demographic dimensions (beyond gender and occupation). Evaluating the content of attributions in light of social discourse and the intended use of a technology facilitates an assessment of potential harms (Selbst et al., 2019). Downstream task bias may exist independently from a measured embedding bias (Goldfarb-Tarrant et al., 2021), therefore a taskand context-oriented evaluation is preferred (Section 6). We have presented several bias-mitigating strategies for different KGEs, which might alleviate the issue in some cases (Section 7). However, more research is needed to establish more effective and robust mitigation methods, as well as metrics used to evaluate their impact (Gonen and Goldberg, 2019;Blodgett et al., 2020).']","To avoid harms caused by biases in KGs and their embeddings, we identify and recommend several actions for practitioners and researchers.

Transparency and Accountability KGs should by default be published with bias-sensitive documentation to facilitate transparency and accountability regarding potential risks. Data Statements (Bender and Friedman, 2018) report curation criteria, language variety, demographics of the data authors and annotators, relevant indicators of context, quality, and provenance. Datasheets for Datasets (Gebru et al., 2021) additionally state motivation, composition, preparation, distribution, and maintenance. The associated questionnaire can accompany the dataset creation process to avoid risks early on. Especially in the case of ongoing crowdsourcing efforts for encyclopedic KGs the demographic background of contributors should be reported (Demartini, 2019). Researchers using subsets of these KGs, should investigate respective data dumps for potential biases and report limitations transparently. Similarly, KG embedding models should be published with Model Cards (Mitchell et al., 2019) documenting intended use, underlying data, ethical considerations, and limitations. Stating the contact details for reporting problems and concerns establishes accountability (Mitchell et al., 2019;Gebru et al., 2021).

Improving Representativeness To tackle selection bias, data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints. Annotations should be determined via aggregation (see Hovy and Prabhumoye, 2021). For open editable KGs, interventions like edit-a-thons are helpful to introduce more authors from underrepresented groups (Vetter et al., 2022) (e.g., the Art+Feminism campaign aims to fill the gender gap in Wikimedia knowledge bases 8 ). In order for such interventions to take effect, research must update data bases and benchmarks frequently (see Koch et al., 2021). In addition, the timeliness of encyclopedic data is necessary to avoid perpetuating historic biases.

Tackling Algorithmic Bias Evaluation and prevention of harmful biases must become part of the development pipeline (Stanczak and Augenstein, 2021). Algorithmic biases are best evaluated with a combination of multiple quantitative (Section 5) and qualitative measures (Kraft et al., 2022;Dev et al., 2021), considering multiple demographic dimensions (beyond gender and occupation). Evaluating the content of attributions in light of social discourse and the intended use of a technology facilitates an assessment of potential harms (Selbst et al., 2019). Downstream task bias may exist independently from a measured embedding bias (Goldfarb-Tarrant et al., 2021), therefore a taskand context-oriented evaluation is preferred (Section 6). We have presented several bias-mitigating strategies for different KGEs, which might alleviate the issue in some cases (Section 7). However, more research is needed to establish more effective and robust mitigation methods, as well as metrics used to evaluate their impact (Gonen and Goldberg, 2019;Blodgett et al., 2020).","(p25.0) To avoid harms caused by biases in KGs and their embeddings, we identify and recommend several actions for practitioners and researchers.

(p25.1) Transparency and Accountability KGs should by default be published with bias-sensitive documentation to facilitate transparency and accountability regarding potential risks. Data Statements (Bender and Friedman, 2018) report curation criteria, language variety, demographics of the data authors and annotators, relevant indicators of context, quality, and provenance. Datasheets for Datasets (Gebru et al., 2021) additionally state motivation, composition, preparation, distribution, and maintenance. The associated questionnaire can accompany the dataset creation process to avoid risks early on. Especially in the case of ongoing crowdsourcing efforts for encyclopedic KGs the demographic background of contributors should be reported (Demartini, 2019). Researchers using subsets of these KGs, should investigate respective data dumps for potential biases and report limitations transparently. Similarly, KG embedding models should be published with Model Cards (Mitchell et al., 2019) documenting intended use, underlying data, ethical considerations, and limitations. Stating the contact details for reporting problems and concerns establishes accountability (Mitchell et al., 2019;Gebru et al., 2021).

(p25.2) Improving Representativeness To tackle selection bias, data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints. Annotations should be determined via aggregation (see Hovy and Prabhumoye, 2021). For open editable KGs, interventions like edit-a-thons are helpful to introduce more authors from underrepresented groups (Vetter et al., 2022) (e.g., the Art+Feminism campaign aims to fill the gender gap in Wikimedia knowledge bases 8 ). In order for such interventions to take effect, research must update data bases and benchmarks frequently (see Koch et al., 2021). In addition, the timeliness of encyclopedic data is necessary to avoid perpetuating historic biases.

(p25.3) Tackling Algorithmic Bias Evaluation and prevention of harmful biases must become part of the development pipeline (Stanczak and Augenstein, 2021). Algorithmic biases are best evaluated with a combination of multiple quantitative (Section 5) and qualitative measures (Kraft et al., 2022;Dev et al., 2021), considering multiple demographic dimensions (beyond gender and occupation). Evaluating the content of attributions in light of social discourse and the intended use of a technology facilitates an assessment of potential harms (Selbst et al., 2019). Downstream task bias may exist independently from a measured embedding bias (Goldfarb-Tarrant et al., 2021), therefore a taskand context-oriented evaluation is preferred (Section 6). We have presented several bias-mitigating strategies for different KGEs, which might alleviate the issue in some cases (Section 7). However, more research is needed to establish more effective and robust mitigation methods, as well as metrics used to evaluate their impact (Gonen and Goldberg, 2019;Blodgett et al., 2020).","[[], ['b29', 'b61', 'b5', 'b17'], ['b48', 'b83', 'b39'], ['b49', 'b18', 'b34', 'b8', 'b79', 'b35', 'b74']]","[[], ['b29', 'b61', 'b5', 'b17'], ['b48', 'b83', 'b39'], ['b49', 'b18', 'b34', 'b8', 'b79', 'b35', 'b74']]",14,"1. To avoid harms caused by biases in KGs and their embeddings, we identify and recommend several actions for practitioners and researchers.
2. Transparency and Accountability KGs should by default be published with bias-sensitive documentation to facilitate transparency and accountability regarding potential risks.
3. Data Statements (Bender and Friedman, 2018) report curation criteria, language variety, demographics of the data authors and annotators, relevant indicators of context, quality, and provenance.
4. Datasheets for Datasets (Gebru et al., 2021) additionally state motivation, composition, preparation, distribution, and maintenance.
5. The associated questionnaire can accompany the dataset creation process to avoid risks early on.
6. Especially in the case of ongoing crowdsourcing efforts for encyclopedic KGs the demographic background of contributors should be reported (Demartini, 2019).
7. Researchers using subsets of these KGs, should investigate respective data dumps for potential biases and report limitations transparently.
8. Similarly, KG embedding models should be published with Model Cards (Mitchell et al., 2019) documenting intended use, underlying data, ethical considerations, and limitations.
9. Stating the contact details for reporting problems and concerns establishes accountability (Mitchell et al., 2019;Gebru et al., 2021).
10. Improving Representativeness To tackle selection bias, data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints.
11. Annotations should be determined via aggregation (see Hovy and Prabhumoye, 2021).
12. For open editable KGs, interventions like edit-a-thons are helpful to introduce more authors from underrepresented groups (Vetter et al., 2022) (e.g., the Art+Feminism campaign aims to fill the gender gap in Wikimedia knowledge bases 8 ).
13. In order for such interventions to take effect, research must update data bases and benchmarks frequently (see Koch et al., 2021).
14. In addition, the timeliness of encyclopedic data is necessary to avoid perpetuating historic biases.
15. Tackling Algorithmic Bias Evaluation and prevention of harmful biases must become part of the development pipeline (Stanczak and Augenstein, 2021).
16. Algorithmic biases are best evaluated with a combination of multiple quantitative (Section 5) and qualitative measures (Kraft et al., 2022;Dev et al., 2021), considering multiple demographic dimensions (beyond gender and occupation).
17. Evaluating the content of attributions in light of social discourse and the intended use of a technology facilitates an assessment of potential harms (Selbst et al., 2019).
18. Downstream task bias may exist independently from a measured embedding bias (Goldfarb-Tarrant et al., 2021), therefore a taskand context-oriented evaluation is preferred (Section 6).
19. We have presented several bias-mitigating strategies for different KGEs, which might alleviate the issue in some cases (Section 7).
20. However, more research is needed to establish more effective and robust mitigation methods, as well as metrics used to evaluate their impact (Gonen and Goldberg, 2019;Blodgett et al., 2020)."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Computer Science,https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,s26,Related Work,['p26.0'],"['Although a wide range of surveys investigates biases in NLP, none of them addresses KG-based methods, in particular. Blodgett et al. (2020) critically investigated the theoretical foundation of works analyzing bias in NLP. The authors claim that most works lack a clear taxonomy. We came to a similar conclusion with respect to evaluations of KGs and their embeddings.  and Stanczak and Augenstein (2021) surveyed algorithmic measurement and mitigation strategies for gender bias in NLP.  summarized approaches for the measurement and mitigation of bias in generative language models. Some of the methods presented earlier are derived from works discussed in these surveys and adapted to the constraints of KG embeddings (e.g., Bourli and Pitoura (2020) adapted hard debiasing (Bolukbasi et al., 2016)). Criticisms point to the monolingual focus on the English language, the predominant assumption of a gender binary, and a lack of interdisciplinary collaboration. Shah et al. (2020) identified four sources of predictive biases: label bias (label distributions are imbalanced and erroneous regarding certain demographics), selection bias (the data sample is not representative of the real world distribution), semantic bias/input representation bias (e.g., feature creation with biased embeddings), and overamplification through the predictive model (slight differences between human attributes are overempha-sized by the model). All of these factors are reflected in the lifecycle as discussed in this article. To counter the risks, Shah et al. (2020) suggest employing multiple annotators and methods of aggregation (see also Hovy and Prabhumoye, 2021), re-stratification, re-weighting, or data augmentation, debiasing of models, and, finally, standardized data and model documentation.']","Although a wide range of surveys investigates biases in NLP, none of them addresses KG-based methods, in particular. Blodgett et al. (2020) critically investigated the theoretical foundation of works analyzing bias in NLP. The authors claim that most works lack a clear taxonomy. We came to a similar conclusion with respect to evaluations of KGs and their embeddings.  and Stanczak and Augenstein (2021) surveyed algorithmic measurement and mitigation strategies for gender bias in NLP.  summarized approaches for the measurement and mitigation of bias in generative language models. Some of the methods presented earlier are derived from works discussed in these surveys and adapted to the constraints of KG embeddings (e.g., Bourli and Pitoura (2020) adapted hard debiasing (Bolukbasi et al., 2016)). Criticisms point to the monolingual focus on the English language, the predominant assumption of a gender binary, and a lack of interdisciplinary collaboration. Shah et al. (2020) identified four sources of predictive biases: label bias (label distributions are imbalanced and erroneous regarding certain demographics), selection bias (the data sample is not representative of the real world distribution), semantic bias/input representation bias (e.g., feature creation with biased embeddings), and overamplification through the predictive model (slight differences between human attributes are overempha-sized by the model). All of these factors are reflected in the lifecycle as discussed in this article. To counter the risks, Shah et al. (2020) suggest employing multiple annotators and methods of aggregation (see also Hovy and Prabhumoye, 2021), re-stratification, re-weighting, or data augmentation, debiasing of models, and, finally, standardized data and model documentation.","(p26.0) Although a wide range of surveys investigates biases in NLP, none of them addresses KG-based methods, in particular. Blodgett et al. (2020) critically investigated the theoretical foundation of works analyzing bias in NLP. The authors claim that most works lack a clear taxonomy. We came to a similar conclusion with respect to evaluations of KGs and their embeddings.  and Stanczak and Augenstein (2021) surveyed algorithmic measurement and mitigation strategies for gender bias in NLP.  summarized approaches for the measurement and mitigation of bias in generative language models. Some of the methods presented earlier are derived from works discussed in these surveys and adapted to the constraints of KG embeddings (e.g., Bourli and Pitoura (2020) adapted hard debiasing (Bolukbasi et al., 2016)). Criticisms point to the monolingual focus on the English language, the predominant assumption of a gender binary, and a lack of interdisciplinary collaboration. Shah et al. (2020) identified four sources of predictive biases: label bias (label distributions are imbalanced and erroneous regarding certain demographics), selection bias (the data sample is not representative of the real world distribution), semantic bias/input representation bias (e.g., feature creation with biased embeddings), and overamplification through the predictive model (slight differences between human attributes are overempha-sized by the model). All of these factors are reflected in the lifecycle as discussed in this article. To counter the risks, Shah et al. (2020) suggest employing multiple annotators and methods of aggregation (see also Hovy and Prabhumoye, 2021), re-stratification, re-weighting, or data augmentation, debiasing of models, and, finally, standardized data and model documentation.","[['b8', 'b39', 'b79', 'b12', 'b75', 'b9']]","[['b8', 'b39', 'b79', 'b12', 'b75', 'b9']]",6,"1. Although a wide range of surveys investigates biases in NLP, none of them addresses KG-based methods, in particular.
2. Blodgett et al. (2020) critically investigated the theoretical foundation of works analyzing bias in NLP.
3. The authors claim that most works lack a clear taxonomy.
4. We came to a similar conclusion with respect to evaluations of KGs and their embeddings.  and Stanczak and Augenstein (2021) surveyed algorithmic measurement and mitigation strategies for gender bias in NLP.  summarized approaches for the measurement and mitigation of bias in generative language models.
5. Some of the methods presented earlier are derived from works discussed in these surveys and adapted to the constraints of KG embeddings (e.g., Bourli and Pitoura (2020) adapted hard debiasing (Bolukbasi et al., 2016)).
6. Criticisms point to the monolingual focus on the English language, the predominant assumption of a gender binary, and a lack of interdisciplinary collaboration.
7. Shah et al. (2020) identified four sources of predictive biases: label bias (label distributions are imbalanced and erroneous regarding certain demographics), selection bias (the data sample is not representative of the real world distribution), semantic bias/input representation bias (e.g., feature creation with biased embeddings), and overamplification through the predictive model (slight differences between human attributes are overempha-sized by the model).
8. All of these factors are reflected in the lifecycle as discussed in this article.
9. To counter the risks, Shah et al. (2020) suggest employing multiple annotators and methods of aggregation (see also Hovy and Prabhumoye, 2021), re-stratification, re-weighting, or data augmentation, debiasing of models, and, finally, standardized data and model documentation."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Computer Science,https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,s24,Reported Motivations,['p24.0'],"['Many, yet not all works in the field name potential social harms as a motivator for their research on social bias in KGs (Mehrabi et al., 2021b;Fisher et al., 2020a,b;Radstok et al., 2021). Only Mehrabi et al. (2021b) drew from established taxonomies and targeted biases associated with representational harms (Barocas et al., as cited in, Blodgett et al., 2020). Similarly, most works lack a clear working definition of social bias. For example, aspects of pre-existing societal biases captured in the data and biases arising through the algorithm (Friedman and Nissenbaum, 1996) are usually not disentangled. Only Bourli and Pitoura (2020) compared model bias to the original KG frequencies and showed that the statistical modeling caused an amplification.']","Many, yet not all works in the field name potential social harms as a motivator for their research on social bias in KGs (Mehrabi et al., 2021b;Fisher et al., 2020a,b;Radstok et al., 2021). Only Mehrabi et al. (2021b) drew from established taxonomies and targeted biases associated with representational harms (Barocas et al., as cited in, Blodgett et al., 2020). Similarly, most works lack a clear working definition of social bias. For example, aspects of pre-existing societal biases captured in the data and biases arising through the algorithm (Friedman and Nissenbaum, 1996) are usually not disentangled. Only Bourli and Pitoura (2020) compared model bias to the original KG frequencies and showed that the statistical modeling caused an amplification.","(p24.0) Many, yet not all works in the field name potential social harms as a motivator for their research on social bias in KGs (Mehrabi et al., 2021b;Fisher et al., 2020a,b;Radstok et al., 2021). Only Mehrabi et al. (2021b) drew from established taxonomies and targeted biases associated with representational harms (Barocas et al., as cited in, Blodgett et al., 2020). Similarly, most works lack a clear working definition of social bias. For example, aspects of pre-existing societal biases captured in the data and biases arising through the algorithm (Friedman and Nissenbaum, 1996) are usually not disentangled. Only Bourli and Pitoura (2020) compared model bias to the original KG frequencies and showed that the statistical modeling caused an amplification.","[[None, 'b12', 'b70', 'b26']]","[[None, 'b12', 'b70', 'b26']]",4,"1. Many, yet not all works in the field name potential social harms as a motivator for their research on social bias in KGs (Mehrabi et al., 2021b;Fisher et al., 2020a,b;Radstok et al., 2021).
2. Only Mehrabi et al. (2021b) drew from established taxonomies and targeted biases associated with representational harms (Barocas et al., as cited in, Blodgett et al., 2020).
3. Similarly, most works lack a clear working definition of social bias.
4. For example, aspects of pre-existing societal biases captured in the data and biases arising through the algorithm (Friedman and Nissenbaum, 1996) are usually not disentangled.
5. Only Bourli and Pitoura (2020) compared model bias to the original KG frequencies and showed that the statistical modeling caused an amplification."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Computer Science,https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,s23,(In-)Effectiveness of Mitigation Strategies,['p23.0'],"['Data balancing is the most intuitive approach to bias mitigation and was proven to be effective in the context of text processing (Meade et al., 2022). However, for KGEs, data balancing methods were found to inconsistently reduce bias (Section 7.1). Adversarial learning yielded promising outcomes in the study by Arduini et al. (2020). Their FAN approach does not rely on pre-specified attributes. This is in contrast to Fisher et al. (2020a), whose intervention was found to miss non-targeted, yet bias-related information. This problem relates to one of the main criticisms of hard and soft debiasing: instead of alleviating the problem, these techniques risk concealing the full extent of the bias (Gonen and Goldberg, 2019).']","Data balancing is the most intuitive approach to bias mitigation and was proven to be effective in the context of text processing (Meade et al., 2022). However, for KGEs, data balancing methods were found to inconsistently reduce bias (Section 7.1). Adversarial learning yielded promising outcomes in the study by Arduini et al. (2020). Their FAN approach does not rely on pre-specified attributes. This is in contrast to Fisher et al. (2020a), whose intervention was found to miss non-targeted, yet bias-related information. This problem relates to one of the main criticisms of hard and soft debiasing: instead of alleviating the problem, these techniques risk concealing the full extent of the bias (Gonen and Goldberg, 2019).","(p23.0) Data balancing is the most intuitive approach to bias mitigation and was proven to be effective in the context of text processing (Meade et al., 2022). However, for KGEs, data balancing methods were found to inconsistently reduce bias (Section 7.1). Adversarial learning yielded promising outcomes in the study by Arduini et al. (2020). Their FAN approach does not rely on pre-specified attributes. This is in contrast to Fisher et al. (2020a), whose intervention was found to miss non-targeted, yet bias-related information. This problem relates to one of the main criticisms of hard and soft debiasing: instead of alleviating the problem, these techniques risk concealing the full extent of the bias (Gonen and Goldberg, 2019).","[['b24', 'b55', 'b1', 'b35']]","[['b24', 'b55', 'b1', 'b35']]",4,"1. Data balancing is the most intuitive approach to bias mitigation and was proven to be effective in the context of text processing (Meade et al., 2022).
2. However, for KGEs, data balancing methods were found to inconsistently reduce bias (Section 7.1).
3. Adversarial learning yielded promising outcomes in the study by Arduini et al. (2020).
4. Their FAN approach does not rely on pre-specified attributes.
5. This is in contrast to Fisher et al. (2020a), whose intervention was found to miss non-targeted, yet bias-related information.
6. This problem relates to one of the main criticisms of hard and soft debiasing: instead of alleviating the problem, these techniques risk concealing the full extent of the bias (Gonen and Goldberg, 2019)."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Computer Science,https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,s22,Lack of Validation,['p22.0'],"['Most of the KGE bias metrics presented here are interpreted as valid if they detect unfairly discriminating association patterns that intuitively align with existing stereotypes. Besides that, several works investigate the comparability between different metrics. Although both of these practices deliver valuable information on validity, they largely ignore the societal context. Only Du et al. (2022) compared embedding-level bias metrics with census-aligned data to assess compatibility with real-world inequalities. We suggest that future work consider a more comprehensive study of construct validity (Does the measurement instrument measure the construct in a meaningful and useful capacity?) (Jacobs and Wallach, 2021). One requirement is that the obtained measurements capture all relevant aspects of the construct the instrument claims to measure. That is, a gender bias measure must measure all relevant aspects of gender bias (Stanczak and Augenstein, 2021) (including, e.g., nonbinary gender and a distinction between benevolent and hostile forms of sexist stereotyping (Glick and Fiske, 1997)). Unless proven otherwise, we must be skeptical that this is achieved by existing approaches (Gonen and Goldberg, 2019). As a result of minimal validation, detailed interpretation guidelines are generally not provided. Therefore, the distinctions between strong and weak bias or weak bias and random variation are mostly vague.']","Most of the KGE bias metrics presented here are interpreted as valid if they detect unfairly discriminating association patterns that intuitively align with existing stereotypes. Besides that, several works investigate the comparability between different metrics. Although both of these practices deliver valuable information on validity, they largely ignore the societal context. Only Du et al. (2022) compared embedding-level bias metrics with census-aligned data to assess compatibility with real-world inequalities. We suggest that future work consider a more comprehensive study of construct validity (Does the measurement instrument measure the construct in a meaningful and useful capacity?) (Jacobs and Wallach, 2021). One requirement is that the obtained measurements capture all relevant aspects of the construct the instrument claims to measure. That is, a gender bias measure must measure all relevant aspects of gender bias (Stanczak and Augenstein, 2021) (including, e.g., nonbinary gender and a distinction between benevolent and hostile forms of sexist stereotyping (Glick and Fiske, 1997)). Unless proven otherwise, we must be skeptical that this is achieved by existing approaches (Gonen and Goldberg, 2019). As a result of minimal validation, detailed interpretation guidelines are generally not provided. Therefore, the distinctions between strong and weak bias or weak bias and random variation are mostly vague.","(p22.0) Most of the KGE bias metrics presented here are interpreted as valid if they detect unfairly discriminating association patterns that intuitively align with existing stereotypes. Besides that, several works investigate the comparability between different metrics. Although both of these practices deliver valuable information on validity, they largely ignore the societal context. Only Du et al. (2022) compared embedding-level bias metrics with census-aligned data to assess compatibility with real-world inequalities. We suggest that future work consider a more comprehensive study of construct validity (Does the measurement instrument measure the construct in a meaningful and useful capacity?) (Jacobs and Wallach, 2021). One requirement is that the obtained measurements capture all relevant aspects of the construct the instrument claims to measure. That is, a gender bias measure must measure all relevant aspects of gender bias (Stanczak and Augenstein, 2021) (including, e.g., nonbinary gender and a distinction between benevolent and hostile forms of sexist stereotyping (Glick and Fiske, 1997)). Unless proven otherwise, we must be skeptical that this is achieved by existing approaches (Gonen and Goldberg, 2019). As a result of minimal validation, detailed interpretation guidelines are generally not provided. Therefore, the distinctions between strong and weak bias or weak bias and random variation are mostly vague.","[['b33', 'b41', 'b79', 'b35', 'b22']]","[['b33', 'b41', 'b79', 'b35', 'b22']]",5,"1. Most of the KGE bias metrics presented here are interpreted as valid if they detect unfairly discriminating association patterns that intuitively align with existing stereotypes.
2. Besides that, several works investigate the comparability between different metrics.
3. Although both of these practices deliver valuable information on validity, they largely ignore the societal context.
4. Only Du et al. (2022) compared embedding-level bias metrics with census-aligned data to assess compatibility with real-world inequalities.
5. We suggest that future work consider a more comprehensive study of construct validity (Does the measurement instrument measure the construct in a meaningful and useful capacity?)
6. (Jacobs and Wallach, 2021). One requirement is that the obtained measurements capture all relevant aspects of the construct the instrument claims to measure.
7. That is, a gender bias measure must measure all relevant aspects of gender bias (Stanczak and Augenstein, 2021) (including, e.g., nonbinary gender and a distinction between benevolent and hostile forms of sexist stereotyping (Glick and Fiske, 1997)).
8. Unless proven otherwise, we must be skeptical that this is achieved by existing approaches (Gonen and Goldberg, 2019).
9. As a result of minimal validation, detailed interpretation guidelines are generally not provided.
10. Therefore, the distinctions between strong and weak bias or weak bias and random variation are mostly vague."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Computer Science,https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,s21,Discussion,"['p21.0', 'p21.1']","['In this article, we cover a wide range of evidence for harmful biases at different stages during the lifecycle of ""facts"" as represented in KGs. Some of the most influential graphs misrepresent the world as it is due to sampling and algorithmic biases at the creation step. Pre-existing biases are exaggerated in these representations. Embedding models learn to encode the same or further amplified versions of these biases. Since the training of high-quality embeddings is costly, they are, in practice, pretrained once and afterward reused and fine-tuned for different systems. These systems preserve the inherited biases over long periods, exacerbating the issue further. Our survey shows that KGs may qualify as resources for historic facts, but they do not qualify for inference regarding various human attributes. Future work on biases in KGs and KGEs should aim for improvement in the following areas:', 'Attribute and Seed Choices Bias metrics usually examine one or a few specific attributes (e.g., occupation) and their correlations with selected seed dimensions (e.g., gender). Occupation is by far the most researched attribute in the articles we found (Arduini et al., 2020;Radstok et al., 2021;Bourli and Pitoura, 2020;Fisher et al., 2020a,b). Only Keidar et al. (2021) propose to aggregate the correlations between a set of seed dimensions and all relations in a graph. All the works used binary gender as the seed dimension and some additionally addressed ethnicity, religion, and nationality (Fisher et al., 2020a,b;Mehrabi et al., 2021b).']","In this article, we cover a wide range of evidence for harmful biases at different stages during the lifecycle of ""facts"" as represented in KGs. Some of the most influential graphs misrepresent the world as it is due to sampling and algorithmic biases at the creation step. Pre-existing biases are exaggerated in these representations. Embedding models learn to encode the same or further amplified versions of these biases. Since the training of high-quality embeddings is costly, they are, in practice, pretrained once and afterward reused and fine-tuned for different systems. These systems preserve the inherited biases over long periods, exacerbating the issue further. Our survey shows that KGs may qualify as resources for historic facts, but they do not qualify for inference regarding various human attributes. Future work on biases in KGs and KGEs should aim for improvement in the following areas:

Attribute and Seed Choices Bias metrics usually examine one or a few specific attributes (e.g., occupation) and their correlations with selected seed dimensions (e.g., gender). Occupation is by far the most researched attribute in the articles we found (Arduini et al., 2020;Radstok et al., 2021;Bourli and Pitoura, 2020;Fisher et al., 2020a,b). Only Keidar et al. (2021) propose to aggregate the correlations between a set of seed dimensions and all relations in a graph. All the works used binary gender as the seed dimension and some additionally addressed ethnicity, religion, and nationality (Fisher et al., 2020a,b;Mehrabi et al., 2021b).","(p21.0) In this article, we cover a wide range of evidence for harmful biases at different stages during the lifecycle of ""facts"" as represented in KGs. Some of the most influential graphs misrepresent the world as it is due to sampling and algorithmic biases at the creation step. Pre-existing biases are exaggerated in these representations. Embedding models learn to encode the same or further amplified versions of these biases. Since the training of high-quality embeddings is costly, they are, in practice, pretrained once and afterward reused and fine-tuned for different systems. These systems preserve the inherited biases over long periods, exacerbating the issue further. Our survey shows that KGs may qualify as resources for historic facts, but they do not qualify for inference regarding various human attributes. Future work on biases in KGs and KGEs should aim for improvement in the following areas:

(p21.1) Attribute and Seed Choices Bias metrics usually examine one or a few specific attributes (e.g., occupation) and their correlations with selected seed dimensions (e.g., gender). Occupation is by far the most researched attribute in the articles we found (Arduini et al., 2020;Radstok et al., 2021;Bourli and Pitoura, 2020;Fisher et al., 2020a,b). Only Keidar et al. (2021) propose to aggregate the correlations between a set of seed dimensions and all relations in a graph. All the works used binary gender as the seed dimension and some additionally addressed ethnicity, religion, and nationality (Fisher et al., 2020a,b;Mehrabi et al., 2021b).","[[], [None, 'b12', 'b70', 'b1']]","[[], [None, 'b12', 'b70', 'b1']]",4,"1. In this article, we cover a wide range of evidence for harmful biases at different stages during the lifecycle of ""facts"" as represented in KGs.
2. Some of the most influential graphs misrepresent the world as it is due to sampling and algorithmic biases at the creation step.
3. Pre-existing biases are exaggerated in these representations.
4. Embedding models learn to encode the same or further amplified versions of these biases.
5. Since the training of high-quality embeddings is costly, they are, in practice, pretrained once and afterward reused and fine-tuned for different systems.
6. These systems preserve the inherited biases over long periods, exacerbating the issue further.
7. Our survey shows that KGs may qualify as resources for historic facts, but they do not qualify for inference regarding various human attributes.
8. Future work on biases in KGs and KGEs should aim for improvement in the following areas:Attribute and Seed Choices Bias metrics usually examine one or a few specific attributes (e.g., occupation) and their correlations with selected seed dimensions (e.g., gender).
9. Occupation is by far the most researched attribute in the articles we found (Arduini et al., 2020;Radstok et al., 2021;Bourli and Pitoura, 2020;Fisher et al., 2020a,b).
10. Only Keidar et al. (2021) propose to aggregate the correlations between a set of seed dimensions and all relations in a graph.
11. All the works used binary gender as the seed dimension and some additionally addressed ethnicity, religion, and nationality (Fisher et al., 2020a,b;Mehrabi et al., 2021b)."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Computer Science,https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,s6,Triples: Crowd-Sourcing of Facts,['p6.0'],"[""Popular large-scale KGs, like Wikidata (Vrandecic and Krötzsch, 2014) and DBpedia (Auer et al., 2007) are the products of continuous crowdsourcing efforts. Both of these examples are closely related to Wikipedia, where the top five languages (English, Cebuano, German, Swedish, and French) constitute 35% of all articles on this platform. 3 It can be said that Wikipedia is Euro-centric in tendency. Moreover, the majority of authors are white males. 4 As a result, the data transport a particular homogeneous set of interests and knowledge (Beytía et al., 2022;Wagner et al., 2015). This sampling bias affects the geospatial coverage of information (Janowicz et al., 2018) and leads to higher barriers for female personalities to receive a biographic entry (Beytía et al., 2022). In an experiment, Demartini (2019) asked crowd contributors to provide a factual answer to the (politically charged) question of whether or not Catalonia is a part of Spain. The diverging responses indicated that participants' beliefs of what counts as true differed largely. This is an example of bias that is beyond a subliminal psychological level. In this case, structural aspects like consumed media and social discourse play an important role. To counter this problem, Demartini (2019) suggests actively asking contributors for evidence supporting their statements, as well as keeping track of their demographic backgrounds. This makes underlying motivations and possible sources for bias traceable.""]","Popular large-scale KGs, like Wikidata (Vrandecic and Krötzsch, 2014) and DBpedia (Auer et al., 2007) are the products of continuous crowdsourcing efforts. Both of these examples are closely related to Wikipedia, where the top five languages (English, Cebuano, German, Swedish, and French) constitute 35% of all articles on this platform. 3 It can be said that Wikipedia is Euro-centric in tendency. Moreover, the majority of authors are white males. 4 As a result, the data transport a particular homogeneous set of interests and knowledge (Beytía et al., 2022;Wagner et al., 2015). This sampling bias affects the geospatial coverage of information (Janowicz et al., 2018) and leads to higher barriers for female personalities to receive a biographic entry (Beytía et al., 2022). In an experiment, Demartini (2019) asked crowd contributors to provide a factual answer to the (politically charged) question of whether or not Catalonia is a part of Spain. The diverging responses indicated that participants' beliefs of what counts as true differed largely. This is an example of bias that is beyond a subliminal psychological level. In this case, structural aspects like consumed media and social discourse play an important role. To counter this problem, Demartini (2019) suggests actively asking contributors for evidence supporting their statements, as well as keeping track of their demographic backgrounds. This makes underlying motivations and possible sources for bias traceable.","(p6.0) Popular large-scale KGs, like Wikidata (Vrandecic and Krötzsch, 2014) and DBpedia (Auer et al., 2007) are the products of continuous crowdsourcing efforts. Both of these examples are closely related to Wikipedia, where the top five languages (English, Cebuano, German, Swedish, and French) constitute 35% of all articles on this platform. 3 It can be said that Wikipedia is Euro-centric in tendency. Moreover, the majority of authors are white males. 4 As a result, the data transport a particular homogeneous set of interests and knowledge (Beytía et al., 2022;Wagner et al., 2015). This sampling bias affects the geospatial coverage of information (Janowicz et al., 2018) and leads to higher barriers for female personalities to receive a biographic entry (Beytía et al., 2022). In an experiment, Demartini (2019) asked crowd contributors to provide a factual answer to the (politically charged) question of whether or not Catalonia is a part of Spain. The diverging responses indicated that participants' beliefs of what counts as true differed largely. This is an example of bias that is beyond a subliminal psychological level. In this case, structural aspects like consumed media and social discourse play an important role. To counter this problem, Demartini (2019) suggests actively asking contributors for evidence supporting their statements, as well as keeping track of their demographic backgrounds. This makes underlying motivations and possible sources for bias traceable.","[['b85', 'b3', 'b6', 'b42']]","[['b85', 'b3', 'b6', 'b42']]",4,"1. Popular large-scale KGs, like Wikidata (Vrandecic and Krötzsch, 2014) and DBpedia (Auer et al., 2007) are the products of continuous crowdsourcing efforts.
2. Both of these examples are closely related to Wikipedia, where the top five languages (English, Cebuano, German, Swedish, and French) constitute 35% of all articles on this platform.
3. 3 It can be said that Wikipedia is Euro-centric in tendency.
4. Moreover, the majority of authors are white males.
5. 4 As a result, the data transport a particular homogeneous set of interests and knowledge (Beytía et al., 2022;Wagner et al., 2015).
6. This sampling bias affects the geospatial coverage of information (Janowicz et al., 2018) and leads to higher barriers for female personalities to receive a biographic entry (Beytía et al., 2022).
7. In an experiment, Demartini (2019) asked crowd contributors to provide a factual answer to the (politically charged) question of whether or not Catalonia is a part of Spain.
8. The diverging responses indicated that participants' beliefs of what counts as true differed largely.
9. This is an example of bias that is beyond a subliminal psychological level.
10. In this case, structural aspects like consumed media and social discourse play an important role.
11. To counter this problem, Demartini (2019) suggests actively asking contributors for evidence supporting their statements, as well as keeping track of their demographic backgrounds.
12. This makes underlying motivations and possible sources for bias traceable."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Computer Science,https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,s7,Ontologies: Manual Creation of Rules,['p7.0'],"[""Ontologies determine rules regarding allowed types of entities and relations or their usage. They are often hand-made and a source of bias (Janowicz et al., 2018) due to the influence of opinions, motivations, and personal choices (Keet, 2021): Factors like scientific opinions (e.g., historical ideas about race), socio-culture (e.g., how many people a person can be married to), or political and religious views (e.g., classifying a person of type X as a terrorist or a protestor) can proximately lead to an encoding of social bias. Also structural constraints like the ontologies' granularity levels can induce bias (Keet, 2021). Furthermore, issues can arise from the types of information used to characterize a person entity. Whether one attributes the person with their skin color or not could theoretically determine the emergence of racist bias in a downstream application (Paparidis and Kotis, 2021). Geller and Kollapally (2021) give a practical example for detection and alleviation of ontology bias in a real-world scenario. The authors discovered that ontological gaps in the medical context lead to an under-reporting of racespecific incidents. They were able to suggest countermeasures based on a structured analysis of real incidents and external terminological resources.""]","Ontologies determine rules regarding allowed types of entities and relations or their usage. They are often hand-made and a source of bias (Janowicz et al., 2018) due to the influence of opinions, motivations, and personal choices (Keet, 2021): Factors like scientific opinions (e.g., historical ideas about race), socio-culture (e.g., how many people a person can be married to), or political and religious views (e.g., classifying a person of type X as a terrorist or a protestor) can proximately lead to an encoding of social bias. Also structural constraints like the ontologies' granularity levels can induce bias (Keet, 2021). Furthermore, issues can arise from the types of information used to characterize a person entity. Whether one attributes the person with their skin color or not could theoretically determine the emergence of racist bias in a downstream application (Paparidis and Kotis, 2021). Geller and Kollapally (2021) give a practical example for detection and alleviation of ontology bias in a real-world scenario. The authors discovered that ontological gaps in the medical context lead to an under-reporting of racespecific incidents. They were able to suggest countermeasures based on a structured analysis of real incidents and external terminological resources.","(p7.0) Ontologies determine rules regarding allowed types of entities and relations or their usage. They are often hand-made and a source of bias (Janowicz et al., 2018) due to the influence of opinions, motivations, and personal choices (Keet, 2021): Factors like scientific opinions (e.g., historical ideas about race), socio-culture (e.g., how many people a person can be married to), or political and religious views (e.g., classifying a person of type X as a terrorist or a protestor) can proximately lead to an encoding of social bias. Also structural constraints like the ontologies' granularity levels can induce bias (Keet, 2021). Furthermore, issues can arise from the types of information used to characterize a person entity. Whether one attributes the person with their skin color or not could theoretically determine the emergence of racist bias in a downstream application (Paparidis and Kotis, 2021). Geller and Kollapally (2021) give a practical example for detection and alleviation of ontology bias in a real-world scenario. The authors discovered that ontological gaps in the medical context lead to an under-reporting of racespecific incidents. They were able to suggest countermeasures based on a structured analysis of real incidents and external terminological resources.","[['b45', 'b64', 'b30', 'b42']]","[['b45', 'b64', 'b30', 'b42']]",4,"1. Ontologies determine rules regarding allowed types of entities and relations or their usage.
2. They are often hand-made and a source of bias (Janowicz et al., 2018) due to the influence of opinions, motivations, and personal choices (Keet, 2021): Factors like scientific opinions (e.g., historical ideas about race), socio-culture (e.g., how many people a person can be married to), or political and religious views (e.g., classifying a person of type X as a terrorist or a protestor) can proximately lead to an encoding of social bias.
3. Also structural constraints like the ontologies' granularity levels can induce bias (Keet, 2021).
4. Furthermore, issues can arise from the types of information used to characterize a person entity.
5. Whether one attributes the person with their skin color or not could theoretically determine the emergence of racist bias in a downstream application (Paparidis and Kotis, 2021).
6. Geller and Kollapally (2021) give a practical example for detection and alleviation of ontology bias in a real-world scenario.
7. The authors discovered that ontological gaps in the medical context lead to an under-reporting of racespecific incidents.
8. They were able to suggest countermeasures based on a structured analysis of real incidents and external terminological resources."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Computer Science,https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,s8,Extraction: Automated Extraction of Information,"['p8.0', 'p8.1', 'p8.2']","['Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples (or as [subject, predicate, object], respectively). Mehrabi et al. (2020) showed that the NER system CoreNLP  exhibits binary gender bias. They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names 5 from 139 years of census data. The model returned more erroneous tags for female names. Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders. A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF) (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy 6 NER models). Across models, nonwhite names yielded on average lower performance scores than white names. Generally, ELMo exhibited the least bias. Although ConceptNet is debiased for gender and ethnicity 7 , it was found to produce strongly varied accuracy values. Gaut et al. (2020) analyzed binary gender bias in a popular open-source neural relation extraction (NRE) model, OpenNRE (Han et al., 2019). For this purpose, the authors created a new dataset, named WikiGenderBias (sourced from Wikipedia and DBpedia). All sentences describe a gendered subject with one of four relations: spouse, hypernym, birthData, or birthPlace (DBpedia mostly uses occupation-related hypernyms). The most notable bias found was the spouse relation. It was more reliably predicted for male than female entities. This observation stands in contrast to the predominance of female instances with spouse relation in WikiGenderBias. The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016). Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.', 'Nowadays, contextualized transformer-based en-coders are used in various NLP applications, including NER and NRE. Several works have analyzed the various societal biases encoded in large-scale word embeddings (like word2vec (Mikolov et al., 2013;Bolukbasi et al., 2016) or BERT (Devlin et al., 2019;Kurita et al., 2019)) or language models (like GPT-2 (Radford et al., 2019;Kirk et al., 2021) and GPT-3 (Brown et al., 2020;Abid et al., 2021)). Thus, it is likely that these biases also affect the downstream tasks discussed here.  used two types of tasks to analyze bias in BERT-based RE on the newly created Wiki80 and TACRED (Zhang et al., 2017) benchmarks. For the first task, they masked only entity names with a special token (masked-entity; ME), whereas for the second task, only the entity names were given (onlyentity; OE). The model maintained higher performances in the OE setting, indicating that the entity names were more informative of the predicted relation than the contextual information. This hints at what the authors call semantic bias.', 'A Note on Reporting Bias Generally, when extracting knowledge from text, one should be aware that the frequency with which facts are reported is not representative of their real-world prevalence. Humans tend to mention only events, outcomes, or properties that are out of their perceived ordinary (Gordon and Van Durme, 2013) (e.g., ""a banana is yellow"" is too trivial to be reported). This phenomenon is called reporting bias and likely stems from a need to be as informative and non-redundant as possible when sharing knowledge.']","Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples (or as [subject, predicate, object], respectively). Mehrabi et al. (2020) showed that the NER system CoreNLP  exhibits binary gender bias. They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names 5 from 139 years of census data. The model returned more erroneous tags for female names. Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders. A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF) (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy 6 NER models). Across models, nonwhite names yielded on average lower performance scores than white names. Generally, ELMo exhibited the least bias. Although ConceptNet is debiased for gender and ethnicity 7 , it was found to produce strongly varied accuracy values. Gaut et al. (2020) analyzed binary gender bias in a popular open-source neural relation extraction (NRE) model, OpenNRE (Han et al., 2019). For this purpose, the authors created a new dataset, named WikiGenderBias (sourced from Wikipedia and DBpedia). All sentences describe a gendered subject with one of four relations: spouse, hypernym, birthData, or birthPlace (DBpedia mostly uses occupation-related hypernyms). The most notable bias found was the spouse relation. It was more reliably predicted for male than female entities. This observation stands in contrast to the predominance of female instances with spouse relation in WikiGenderBias. The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016). Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.

Nowadays, contextualized transformer-based en-coders are used in various NLP applications, including NER and NRE. Several works have analyzed the various societal biases encoded in large-scale word embeddings (like word2vec (Mikolov et al., 2013;Bolukbasi et al., 2016) or BERT (Devlin et al., 2019;Kurita et al., 2019)) or language models (like GPT-2 (Radford et al., 2019;Kirk et al., 2021) and GPT-3 (Brown et al., 2020;Abid et al., 2021)). Thus, it is likely that these biases also affect the downstream tasks discussed here.  used two types of tasks to analyze bias in BERT-based RE on the newly created Wiki80 and TACRED (Zhang et al., 2017) benchmarks. For the first task, they masked only entity names with a special token (masked-entity; ME), whereas for the second task, only the entity names were given (onlyentity; OE). The model maintained higher performances in the OE setting, indicating that the entity names were more informative of the predicted relation than the contextual information. This hints at what the authors call semantic bias.

A Note on Reporting Bias Generally, when extracting knowledge from text, one should be aware that the frequency with which facts are reported is not representative of their real-world prevalence. Humans tend to mention only events, outcomes, or properties that are out of their perceived ordinary (Gordon and Van Durme, 2013) (e.g., ""a banana is yellow"" is too trivial to be reported). This phenomenon is called reporting bias and likely stems from a need to be as informative and non-redundant as possible when sharing knowledge.","(p8.0) Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples (or as [subject, predicate, object], respectively). Mehrabi et al. (2020) showed that the NER system CoreNLP  exhibits binary gender bias. They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names 5 from 139 years of census data. The model returned more erroneous tags for female names. Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders. A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF) (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy 6 NER models). Across models, nonwhite names yielded on average lower performance scores than white names. Generally, ELMo exhibited the least bias. Although ConceptNet is debiased for gender and ethnicity 7 , it was found to produce strongly varied accuracy values. Gaut et al. (2020) analyzed binary gender bias in a popular open-source neural relation extraction (NRE) model, OpenNRE (Han et al., 2019). For this purpose, the authors created a new dataset, named WikiGenderBias (sourced from Wikipedia and DBpedia). All sentences describe a gendered subject with one of four relations: spouse, hypernym, birthData, or birthPlace (DBpedia mostly uses occupation-related hypernyms). The most notable bias found was the spouse relation. It was more reliably predicted for male than female entities. This observation stands in contrast to the predominance of female instances with spouse relation in WikiGenderBias. The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016). Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.

(p8.1) Nowadays, contextualized transformer-based en-coders are used in various NLP applications, including NER and NRE. Several works have analyzed the various societal biases encoded in large-scale word embeddings (like word2vec (Mikolov et al., 2013;Bolukbasi et al., 2016) or BERT (Devlin et al., 2019;Kurita et al., 2019)) or language models (like GPT-2 (Radford et al., 2019;Kirk et al., 2021) and GPT-3 (Brown et al., 2020;Abid et al., 2021)). Thus, it is likely that these biases also affect the downstream tasks discussed here.  used two types of tasks to analyze bias in BERT-based RE on the newly created Wiki80 and TACRED (Zhang et al., 2017) benchmarks. For the first task, they masked only entity names with a special token (masked-entity; ME), whereas for the second task, only the entity names were given (onlyentity; OE). The model maintained higher performances in the OE setting, indicating that the entity names were more informative of the predicted relation than the contextual information. This hints at what the authors call semantic bias.

(p8.2) A Note on Reporting Bias Generally, when extracting knowledge from text, one should be aware that the frequency with which facts are reported is not representative of their real-world prevalence. Humans tend to mention only events, outcomes, or properties that are out of their perceived ordinary (Gordon and Van Durme, 2013) (e.g., ""a banana is yellow"" is too trivial to be reported). This phenomenon is called reporting bias and likely stems from a need to be as informative and non-redundant as possible when sharing knowledge.","[['b73', 'b28', 'b40', 'b56', 'b60', 'b66', 'b67', 'b37', None, 'b78', 'b9'], ['b69', 'b50', 'b0', 'b90', None, 'b47', 'b19', 'b59', 'b9'], []]","[['b73', 'b28', 'b40', 'b56', 'b60', 'b66', 'b67', 'b37', None, 'b78', 'b9'], ['b69', 'b50', 'b0', 'b90', None, 'b47', 'b19', 'b59', 'b9'], []]",20,"1. Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples (or as [subject, predicate, object], respectively).
2. Mehrabi et al. (2020) showed that the NER system CoreNLP  exhibits binary gender bias.
3. They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names 5 from 139 years of census data.
4. The model returned more erroneous tags for female names.
5. Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders.
6. A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF)
7. (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy 6 NER models).
8. Across models, nonwhite names yielded on average lower performance scores than white names.
9. Generally, ELMo exhibited the least bias.
10. Although ConceptNet is debiased for gender and ethnicity 7 , it was found to produce strongly varied accuracy values.
11. Gaut et al. (2020) analyzed binary gender bias in a popular open-source neural relation extraction (NRE) model, OpenNRE (Han et al., 2019).
12. For this purpose, the authors created a new dataset, named WikiGenderBias (sourced from Wikipedia and DBpedia).
13. All sentences describe a gendered subject with one of four relations: spouse, hypernym, birthData, or birthPlace (DBpedia mostly uses occupation-related hypernyms).
14. The most notable bias found was the spouse relation.
15. It was more reliably predicted for male than female entities.
16. This observation stands in contrast to the predominance of female instances with spouse relation in WikiGenderBias.
17. The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016).
18. Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.
19. Nowadays, contextualized transformer-based en-coders are used in various NLP applications, including NER and NRE.
20. Several works have analyzed the various societal biases encoded in large-scale word embeddings (like word2vec (Mikolov et al., 2013;Bolukbasi et al., 2016) or BERT (Devlin et al., 2019;Kurita et al., 2019)) or language models (like GPT-2 (Radford et al., 2019;Kirk et al., 2021) and GPT-3 (Brown et al., 2020;Abid et al., 2021)).
21. Thus, it is likely that these biases also affect the downstream tasks discussed here.
22. used two types of tasks to analyze bias in BERT-based RE on the newly created Wiki80 and TACRED (Zhang et al., 2017) benchmarks.
23. For the first task, they masked only entity names with a special token (masked-entity; ME), whereas for the second task, only the entity names were given (onlyentity; OE).
24. The model maintained higher performances in the OE setting, indicating that the entity names were more informative of the predicted relation than the contextual information.
25. This hints at what the authors call semantic bias.
26. A Note on Reporting Bias Generally, when extracting knowledge from text, one should be aware that the frequency with which facts are reported is not representative of their real-world prevalence.
27. Humans tend to mention only events, outcomes, or properties that are out of their perceived ordinary (Gordon and Van Durme, 2013) (e.g., ""a banana is yellow"" is too trivial to be reported).
28. This phenomenon is called reporting bias and likely stems from a need to be as informative and non-redundant as possible when sharing knowledge."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Computer Science,https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,s18,Data Balancing,['p18.0'],"[""Radstok et al. (2021) explored the effects of training an embedding model on a gender-balanced subset of Wikidata triples. First, the authors worked with the originally gender-imbalanced Wikidata12k (Leblay and Chekol, 2018;Dasgupta et al., 2018) and DBpedia15k (Sun et al., 2017) on which they fit a TransE and a DistMult model (Yang et al., 2015). They then added more female triples from the Wikidata/DBpedia graph to even out the binary gender distribution among the top-5 most common occupations. Through link prediction, they compared the number of male and female predictions with the ground truth frequencies. More female entities were predicted after the data balancing intervention. However, the absolute difference between the female ratios in the data and the predictions increased, causing the model to be less accurate and fair. Moreover, the authors note that this process is not scalable since for some domains there are no or only a limited amount of female entities (e.g., female U.S. presidents do not exist in Wikidata). Du et al. (2022) experimented with adding and removing triples to gender-balance a Freebase subset (Bordes et al., 2015). For the first approach, the authors added synthetic triples (as opposed to real entities from another source as was done by Radstok et al. (2021)) for occupations with a higher male ratio. The resulting bias change was inconsis-tent across occupations. This appears in line with the authors' finding that ground truth gender ratios are not perfectly predictive of downstream task bias (Section 6). For the second strategy, the triples that most strongly influenced an existing bias were determined and removed. This outperformed random triple removal.""]","Radstok et al. (2021) explored the effects of training an embedding model on a gender-balanced subset of Wikidata triples. First, the authors worked with the originally gender-imbalanced Wikidata12k (Leblay and Chekol, 2018;Dasgupta et al., 2018) and DBpedia15k (Sun et al., 2017) on which they fit a TransE and a DistMult model (Yang et al., 2015). They then added more female triples from the Wikidata/DBpedia graph to even out the binary gender distribution among the top-5 most common occupations. Through link prediction, they compared the number of male and female predictions with the ground truth frequencies. More female entities were predicted after the data balancing intervention. However, the absolute difference between the female ratios in the data and the predictions increased, causing the model to be less accurate and fair. Moreover, the authors note that this process is not scalable since for some domains there are no or only a limited amount of female entities (e.g., female U.S. presidents do not exist in Wikidata). Du et al. (2022) experimented with adding and removing triples to gender-balance a Freebase subset (Bordes et al., 2015). For the first approach, the authors added synthetic triples (as opposed to real entities from another source as was done by Radstok et al. (2021)) for occupations with a higher male ratio. The resulting bias change was inconsis-tent across occupations. This appears in line with the authors' finding that ground truth gender ratios are not perfectly predictive of downstream task bias (Section 6). For the second strategy, the triples that most strongly influenced an existing bias were determined and removed. This outperformed random triple removal.","(p18.0) Radstok et al. (2021) explored the effects of training an embedding model on a gender-balanced subset of Wikidata triples. First, the authors worked with the originally gender-imbalanced Wikidata12k (Leblay and Chekol, 2018;Dasgupta et al., 2018) and DBpedia15k (Sun et al., 2017) on which they fit a TransE and a DistMult model (Yang et al., 2015). They then added more female triples from the Wikidata/DBpedia graph to even out the binary gender distribution among the top-5 most common occupations. Through link prediction, they compared the number of male and female predictions with the ground truth frequencies. More female entities were predicted after the data balancing intervention. However, the absolute difference between the female ratios in the data and the predictions increased, causing the model to be less accurate and fair. Moreover, the authors note that this process is not scalable since for some domains there are no or only a limited amount of female entities (e.g., female U.S. presidents do not exist in Wikidata). Du et al. (2022) experimented with adding and removing triples to gender-balance a Freebase subset (Bordes et al., 2015). For the first approach, the authors added synthetic triples (as opposed to real entities from another source as was done by Radstok et al. (2021)) for occupations with a higher male ratio. The resulting bias change was inconsis-tent across occupations. This appears in line with the authors' finding that ground truth gender ratios are not perfectly predictive of downstream task bias (Section 6). For the second strategy, the triples that most strongly influenced an existing bias were determined and removed. This outperformed random triple removal.","[['b88', 'b70', 'b16', 'b81', 'b10', 'b22', 'b51']]","[['b88', 'b70', 'b16', 'b81', 'b10', 'b22', 'b51']]",7,"1. Radstok et al. (2021) explored the effects of training an embedding model on a gender-balanced subset of Wikidata triples.
2. First, the authors worked with the originally gender-imbalanced Wikidata12k (Leblay and Chekol, 2018;Dasgupta et al., 2018) and DBpedia15k (Sun et al., 2017) on which they fit a TransE and a DistMult model (Yang et al., 2015).
3. They then added more female triples from the Wikidata/DBpedia graph to even out the binary gender distribution among the top-5 most common occupations.
4. Through link prediction, they compared the number of male and female predictions with the ground truth frequencies.
5. More female entities were predicted after the data balancing intervention.
6. However, the absolute difference between the female ratios in the data and the predictions increased, causing the model to be less accurate and fair.
7. Moreover, the authors note that this process is not scalable since for some domains there are no or only a limited amount of female entities (e.g., female U.S. presidents do not exist in Wikidata).
8. Du et al. (2022) experimented with adding and removing triples to gender-balance a Freebase subset (Bordes et al., 2015).
9. For the first approach, the authors added synthetic triples (as opposed to real entities from another source as was done by Radstok et al. (2021)) for occupations with a higher male ratio.
10. The resulting bias change was inconsis-tent across occupations.
11. This appears in line with the authors' finding that ground truth gender ratios are not perfectly predictive of downstream task bias (Section 6).
12. For the second strategy, the triples that most strongly influenced an existing bias were determined and removed.
13. This outperformed random triple removal."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Computer Science,https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,s13,Stereotypical Analogies,"['p13.0', 'p13.1']","['The idea behind analogy tests is to see whether demographics are associated with attributes in stereotypical ways (e.g., ""Man is to computer programmer as woman is to homemaker"" (Bolukbasi et al., 2016)). In their in-depth analysis of a TransEembedded Wikidata KG, Bourli and Pitoura (2020) investigated occupational analogies for binary gender seeds. TransE (Bordes et al., 2013) represents (h, r, t) (with head h, relation r, tail t) in a single space such that h+r ≈ t. The authors identified the model\'s most likely instance of the claim ""a is to x as b is to y"" (with (a,b) being a set of demographics seeds and (x,y) a set of attributes) via a cosine score:', 'where r is the relation has_occupation. In their study, the highest scoring analogy was ""woman is to fashion model as man is to businessperson"". This example appears rather stereotypical, but other highly ranked analogies less so, like ""Japanese entertainer"" versus ""businessperson"" (Bourli and Pitoura, 2020). A systematic evaluation of how stereotypical the results are is missing here. In comparison, the work that originally introduced analogy testing for word2vec (Bolukbasi et al., 2016) employed human annotators to rate stereotypical and gender-appropriate analogies (e.g., ""sister"" versus ""brother"").']","The idea behind analogy tests is to see whether demographics are associated with attributes in stereotypical ways (e.g., ""Man is to computer programmer as woman is to homemaker"" (Bolukbasi et al., 2016)). In their in-depth analysis of a TransEembedded Wikidata KG, Bourli and Pitoura (2020) investigated occupational analogies for binary gender seeds. TransE (Bordes et al., 2013) represents (h, r, t) (with head h, relation r, tail t) in a single space such that h+r ≈ t. The authors identified the model's most likely instance of the claim ""a is to x as b is to y"" (with (a,b) being a set of demographics seeds and (x,y) a set of attributes) via a cosine score:

where r is the relation has_occupation. In their study, the highest scoring analogy was ""woman is to fashion model as man is to businessperson"". This example appears rather stereotypical, but other highly ranked analogies less so, like ""Japanese entertainer"" versus ""businessperson"" (Bourli and Pitoura, 2020). A systematic evaluation of how stereotypical the results are is missing here. In comparison, the work that originally introduced analogy testing for word2vec (Bolukbasi et al., 2016) employed human annotators to rate stereotypical and gender-appropriate analogies (e.g., ""sister"" versus ""brother"").","(p13.0) The idea behind analogy tests is to see whether demographics are associated with attributes in stereotypical ways (e.g., ""Man is to computer programmer as woman is to homemaker"" (Bolukbasi et al., 2016)). In their in-depth analysis of a TransEembedded Wikidata KG, Bourli and Pitoura (2020) investigated occupational analogies for binary gender seeds. TransE (Bordes et al., 2013) represents (h, r, t) (with head h, relation r, tail t) in a single space such that h+r ≈ t. The authors identified the model's most likely instance of the claim ""a is to x as b is to y"" (with (a,b) being a set of demographics seeds and (x,y) a set of attributes) via a cosine score:

(p13.1) where r is the relation has_occupation. In their study, the highest scoring analogy was ""woman is to fashion model as man is to businessperson"". This example appears rather stereotypical, but other highly ranked analogies less so, like ""Japanese entertainer"" versus ""businessperson"" (Bourli and Pitoura, 2020). A systematic evaluation of how stereotypical the results are is missing here. In comparison, the work that originally introduced analogy testing for word2vec (Bolukbasi et al., 2016) employed human annotators to rate stereotypical and gender-appropriate analogies (e.g., ""sister"" versus ""brother"").","[['b11', 'b12', 'b9'], ['b12', 'b9']]","[['b11', 'b12', 'b9'], ['b12', 'b9']]",5,"1. The idea behind analogy tests is to see whether demographics are associated with attributes in stereotypical ways (e.g., ""Man is to computer programmer as woman is to homemaker"" (Bolukbasi et al., 2016)).
2. In their in-depth analysis of a TransEembedded Wikidata KG, Bourli and Pitoura (2020) investigated occupational analogies for binary gender seeds.
3. TransE (Bordes et al., 2013) represents (h, r, t) (with head h, relation r, tail t) in a single space such that h+r
4. ≈ t. The authors identified the model's most likely instance of the claim ""a is to x as b is to y""
5. (with (a,b) being a set of demographics seeds and (x,y) a set of attributes) via a cosine score:where r is the relation has_occupation.
6. In their study, the highest scoring analogy was ""woman is to fashion model as man is to businessperson"".
7. This example appears rather stereotypical, but other highly ranked analogies less so, like ""Japanese entertainer"" versus ""businessperson"" (Bourli and Pitoura, 2020).
8. A systematic evaluation of how stereotypical the results are is missing here.
9. In comparison, the work that originally introduced analogy testing for word2vec (Bolukbasi et al., 2016) employed human annotators to rate stereotypical and gender-appropriate analogies (e.g., ""sister"" versus ""brother"")."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Computer Science,https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,s16,Downstream Task Bias: Link Prediction,['p16.0'],"['Link prediction is a standard downstream task that targets the prediction of relations between entities in a given KG. Systematic deviations in the relations suggested for entities with different demographics indicate reproduced social bias. For the measurement of fairness or bias in link prediction, Keidar et al. (2021) distinguish between demographic parity versus predictive parity. The assumption underlying demographic parity is that the equality between predictions for demographic counterfactuals (opposite demographics, for example, female versus male in binary understanding) is the ideal state (Dwork et al., 2012). That is, the probability of predicting a label should be the same for both groups. Predictive parity is given, on the other hand, if the probability of true positive predictions (positive predictive value or precision) is equal between groups (Chouldechova, 2017). Hence, this measure factors in the label  Bourli and Pitoura (2020) distribution by demographic. With these metrics, Keidar et al. (2021) analyzed different embedding types, namely TransE, ComplEx, RotatE, and Dist-Mult, each fit on the benchmark datasets FB15k-237 (Toutanova and Chen, 2015) and Wikidata5m (Wang et al., 2021). They averaged the scores across a large set of human-associated relations to detect automatically which relations are most biased. The results showed that position played on a sports team was most consistently genderbiased across embeddings. Arduini et al. (2020) analyzed link prediction parity regarding the relations gender and occupation to estimate debiasing effects on TransH (Wang et al., 2014) and TransD (Ji et al., 2015). The comparability between different forms of vector representations is a strength of downstream metrics. In contrast, measures like the analogy test or projection score (Bourli and Pitoura, 2020) are based on specific distance metrics and TL (Fisher et al., 2020b) was shown to lack transferability across representations (Keidar et al., 2021) (Section 5.3). Du et al. (2022) interpret the correlation between gender and link prediction errors as an indicator of group bias. With this, they found, for example, that engineer and nurse are stereotypically biased in FB5M. However, the ground truth gender ratio was found not predictive of the bias metric (e.g., despite its higher male ratio, animator produced a stronger female bias value). For validation, it was shown that the predicted bias values correlate to the gender distributions of occupations according to U.S. census (again, on TransE). Furthermore, the authors investigated how much single triples contribute to group bias via an influence function. They found that gender bias is mostly driven by triples containing gendered entities and triples of low degree.']","Link prediction is a standard downstream task that targets the prediction of relations between entities in a given KG. Systematic deviations in the relations suggested for entities with different demographics indicate reproduced social bias. For the measurement of fairness or bias in link prediction, Keidar et al. (2021) distinguish between demographic parity versus predictive parity. The assumption underlying demographic parity is that the equality between predictions for demographic counterfactuals (opposite demographics, for example, female versus male in binary understanding) is the ideal state (Dwork et al., 2012). That is, the probability of predicting a label should be the same for both groups. Predictive parity is given, on the other hand, if the probability of true positive predictions (positive predictive value or precision) is equal between groups (Chouldechova, 2017). Hence, this measure factors in the label  Bourli and Pitoura (2020) distribution by demographic. With these metrics, Keidar et al. (2021) analyzed different embedding types, namely TransE, ComplEx, RotatE, and Dist-Mult, each fit on the benchmark datasets FB15k-237 (Toutanova and Chen, 2015) and Wikidata5m (Wang et al., 2021). They averaged the scores across a large set of human-associated relations to detect automatically which relations are most biased. The results showed that position played on a sports team was most consistently genderbiased across embeddings. Arduini et al. (2020) analyzed link prediction parity regarding the relations gender and occupation to estimate debiasing effects on TransH (Wang et al., 2014) and TransD (Ji et al., 2015). The comparability between different forms of vector representations is a strength of downstream metrics. In contrast, measures like the analogy test or projection score (Bourli and Pitoura, 2020) are based on specific distance metrics and TL (Fisher et al., 2020b) was shown to lack transferability across representations (Keidar et al., 2021) (Section 5.3). Du et al. (2022) interpret the correlation between gender and link prediction errors as an indicator of group bias. With this, they found, for example, that engineer and nurse are stereotypically biased in FB5M. However, the ground truth gender ratio was found not predictive of the bias metric (e.g., despite its higher male ratio, animator produced a stronger female bias value). For validation, it was shown that the predicted bias values correlate to the gender distributions of occupations according to U.S. census (again, on TransE). Furthermore, the authors investigated how much single triples contribute to group bias via an influence function. They found that gender bias is mostly driven by triples containing gendered entities and triples of low degree.","(p16.0) Link prediction is a standard downstream task that targets the prediction of relations between entities in a given KG. Systematic deviations in the relations suggested for entities with different demographics indicate reproduced social bias. For the measurement of fairness or bias in link prediction, Keidar et al. (2021) distinguish between demographic parity versus predictive parity. The assumption underlying demographic parity is that the equality between predictions for demographic counterfactuals (opposite demographics, for example, female versus male in binary understanding) is the ideal state (Dwork et al., 2012). That is, the probability of predicting a label should be the same for both groups. Predictive parity is given, on the other hand, if the probability of true positive predictions (positive predictive value or precision) is equal between groups (Chouldechova, 2017). Hence, this measure factors in the label  Bourli and Pitoura (2020) distribution by demographic. With these metrics, Keidar et al. (2021) analyzed different embedding types, namely TransE, ComplEx, RotatE, and Dist-Mult, each fit on the benchmark datasets FB15k-237 (Toutanova and Chen, 2015) and Wikidata5m (Wang et al., 2021). They averaged the scores across a large set of human-associated relations to detect automatically which relations are most biased. The results showed that position played on a sports team was most consistently genderbiased across embeddings. Arduini et al. (2020) analyzed link prediction parity regarding the relations gender and occupation to estimate debiasing effects on TransH (Wang et al., 2014) and TransD (Ji et al., 2015). The comparability between different forms of vector representations is a strength of downstream metrics. In contrast, measures like the analogy test or projection score (Bourli and Pitoura, 2020) are based on specific distance metrics and TL (Fisher et al., 2020b) was shown to lack transferability across representations (Keidar et al., 2021) (Section 5.3). Du et al. (2022) interpret the correlation between gender and link prediction errors as an indicator of group bias. With this, they found, for example, that engineer and nurse are stereotypically biased in FB5M. However, the ground truth gender ratio was found not predictive of the bias metric (e.g., despite its higher male ratio, animator produced a stronger female bias value). For validation, it was shown that the predicted bias values correlate to the gender distributions of occupations according to U.S. census (again, on TransE). Furthermore, the authors investigated how much single triples contribute to group bias via an influence function. They found that gender bias is mostly driven by triples containing gendered entities and triples of low degree.","[['b15', 'b86', 'b82', None, 'b12', 'b87', 'b22', 'b23', 'b1', 'b43']]","[['b15', 'b86', 'b82', None, 'b12', 'b87', 'b22', 'b23', 'b1', 'b43']]",10,"1. Link prediction is a standard downstream task that targets the prediction of relations between entities in a given KG.
2. Systematic deviations in the relations suggested for entities with different demographics indicate reproduced social bias.
3. For the measurement of fairness or bias in link prediction, Keidar et al. (2021) distinguish between demographic parity versus predictive parity.
4. The assumption underlying demographic parity is that the equality between predictions for demographic counterfactuals (opposite demographics, for example, female versus male in binary understanding) is the ideal state (Dwork et al., 2012).
5. That is, the probability of predicting a label should be the same for both groups.
6. Predictive parity is given, on the other hand, if the probability of true positive predictions (positive predictive value or precision) is equal between groups (Chouldechova, 2017).
7. Hence, this measure factors in the label  Bourli and Pitoura (2020) distribution by demographic.
8. With these metrics, Keidar et al. (2021) analyzed different embedding types, namely TransE, ComplEx, RotatE, and Dist-Mult, each fit on the benchmark datasets FB15k-237 (Toutanova and Chen, 2015) and Wikidata5m (Wang et al., 2021).
9. They averaged the scores across a large set of human-associated relations to detect automatically which relations are most biased.
10. The results showed that position played on a sports team was most consistently genderbiased across embeddings.
11. Arduini et al. (2020) analyzed link prediction parity regarding the relations gender and occupation to estimate debiasing effects on TransH (Wang et al., 2014) and TransD (Ji et al., 2015).
12. The comparability between different forms of vector representations is a strength of downstream metrics.
13. In contrast, measures like the analogy test or projection score (Bourli and Pitoura, 2020) are based on specific distance metrics and TL (Fisher et al., 2020b) was shown to lack transferability across representations (Keidar et al., 2021) (Section 5.3).
14. Du et al. (2022) interpret the correlation between gender and link prediction errors as an indicator of group bias.
15. With this, they found, for example, that engineer and nurse are stereotypically biased in FB5M. However, the ground truth gender ratio was found not predictive of the bias metric (e.g., despite its higher male ratio, animator produced a stronger female bias value).
16. For validation, it was shown that the predicted bias values correlate to the gender distributions of occupations according to U.S. census (again, on TransE).
17. Furthermore, the authors investigated how much single triples contribute to group bias via an influence function.
18. They found that gender bias is mostly driven by triples containing gendered entities and triples of low degree."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,Computer Science,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,s2,Related Surveys,"['p2.0', 'p2.1', 'p2.2', 'p2.3', 'p2.4', 'p2.5', 'p2.6', 'p2.7']","['As the application of GNNs to ATS is rather novel, to the best of our knowledge there is only one survey on the topic. The survey by  gives an introduction to the topic. However, it does not provide much detail on GNNs in the context of ATS, nor does it cover all models in the space with a taxonomy. As for GNNs themselves, there exists a large number of surveys on GNNs as a technology. In particular, we want to highlight the surveys by  and  which provide a general overview on GNNs and their applications in different fields. The survey by Abadal et al.', '(2021) provides more technical and theoretical details on GNNs. The survey by Wu et al. (2021a) is of particular interest as it focuses on the usage of GNNs for NLP as a domain. Additionally, we want to note here the survey by Wu et al. (2020a) on the usage of GNNs in recommender systems and the review by Malekzadeh et al. (2021) on the usage of GNNs for text classification.', 'For a more theoretical approach we point the reader to the analysis by Xu et al. (2018) which establishes a number of important theoretical properties for GNNs. An analysis of the Vapnik-Chervonenkis (VC) dimension of GNNs was performed by Veličković et al. (2018).', 'In general, the contributions of our survey are as follows:', '1. We provide a detailed explanation of GNNs in the context of ATS.', '2. We introduce a simple taxonomy for GNN models used for ATS.', '3. We provide a comprehensive overview of innovative GNN models for ATS and discuss further directions for future research.', 'The rest of survey is structured as follows. First we will give a comprehensive explanation of GNNs in the context of ATS. Next, we will explore a number of interesting and innovative models. Finally, we will finish with a conclusion and an outlook on the future usage of GNNs for ATS.']","As the application of GNNs to ATS is rather novel, to the best of our knowledge there is only one survey on the topic. The survey by  gives an introduction to the topic. However, it does not provide much detail on GNNs in the context of ATS, nor does it cover all models in the space with a taxonomy. As for GNNs themselves, there exists a large number of surveys on GNNs as a technology. In particular, we want to highlight the surveys by  and  which provide a general overview on GNNs and their applications in different fields. The survey by Abadal et al.

(2021) provides more technical and theoretical details on GNNs. The survey by Wu et al. (2021a) is of particular interest as it focuses on the usage of GNNs for NLP as a domain. Additionally, we want to note here the survey by Wu et al. (2020a) on the usage of GNNs in recommender systems and the review by Malekzadeh et al. (2021) on the usage of GNNs for text classification.

For a more theoretical approach we point the reader to the analysis by Xu et al. (2018) which establishes a number of important theoretical properties for GNNs. An analysis of the Vapnik-Chervonenkis (VC) dimension of GNNs was performed by Veličković et al. (2018).

In general, the contributions of our survey are as follows:

1. We provide a detailed explanation of GNNs in the context of ATS.

2. We introduce a simple taxonomy for GNN models used for ATS.

3. We provide a comprehensive overview of innovative GNN models for ATS and discuss further directions for future research.

The rest of survey is structured as follows. First we will give a comprehensive explanation of GNNs in the context of ATS. Next, we will explore a number of interesting and innovative models. Finally, we will finish with a conclusion and an outlook on the future usage of GNNs for ATS.","(p2.0) As the application of GNNs to ATS is rather novel, to the best of our knowledge there is only one survey on the topic. The survey by  gives an introduction to the topic. However, it does not provide much detail on GNNs in the context of ATS, nor does it cover all models in the space with a taxonomy. As for GNNs themselves, there exists a large number of surveys on GNNs as a technology. In particular, we want to highlight the surveys by  and  which provide a general overview on GNNs and their applications in different fields. The survey by Abadal et al.

(p2.1) (2021) provides more technical and theoretical details on GNNs. The survey by Wu et al. (2021a) is of particular interest as it focuses on the usage of GNNs for NLP as a domain. Additionally, we want to note here the survey by Wu et al. (2020a) on the usage of GNNs in recommender systems and the review by Malekzadeh et al. (2021) on the usage of GNNs for text classification.

(p2.2) For a more theoretical approach we point the reader to the analysis by Xu et al. (2018) which establishes a number of important theoretical properties for GNNs. An analysis of the Vapnik-Chervonenkis (VC) dimension of GNNs was performed by Veličković et al. (2018).

(p2.3) In general, the contributions of our survey are as follows:

(p2.4) 1. We provide a detailed explanation of GNNs in the context of ATS.

(p2.5) 2. We introduce a simple taxonomy for GNN models used for ATS.

(p2.6) 3. We provide a comprehensive overview of innovative GNN models for ATS and discuss further directions for future research.

(p2.7) The rest of survey is structured as follows. First we will give a comprehensive explanation of GNNs in the context of ATS. Next, we will explore a number of interesting and innovative models. Finally, we will finish with a conclusion and an outlook on the future usage of GNNs for ATS.","[[], ['b27', 'b36', 'b35'], ['b32', 'b41'], [], [], [], [], []]","[[], ['b27', 'b36', 'b35'], ['b32', 'b41'], [], [], [], [], []]",5,"1. As the application of GNNs to ATS is rather novel, to the best of our knowledge there is only one survey on the topic.
2. The survey by  gives an introduction to the topic.
3. However, it does not provide much detail on GNNs in the context of ATS, nor does it cover all models in the space with a taxonomy.
4. As for GNNs themselves, there exists a large number of surveys on GNNs as a technology.
5. In particular, we want to highlight the surveys by  and  which provide a general overview on GNNs and their applications in different fields.
6. The survey by Abadal et al.(2021) provides more technical and theoretical details on GNNs.
7. The survey by Wu et al. (2021a) is of particular interest as it focuses on the usage of GNNs for NLP as a domain.
8. Additionally, we want to note here the survey by Wu et al. (2020a) on the usage of GNNs in recommender systems and the review by Malekzadeh et al. (2021) on the usage of GNNs for text classification.
9. For a more theoretical approach we point the reader to the analysis by Xu et al. (2018) which establishes a number of important theoretical properties for GNNs.
10. An analysis of the Vapnik-Chervonenkis (VC) dimension of GNNs was performed by Veličković et al. (2018).
11. In general, the contributions of our survey are as follows:1.
12. We provide a detailed explanation of GNNs in the context of ATS.
13. 2. We introduce a simple taxonomy for GNN models used for ATS.3.
14. We provide a comprehensive overview of innovative GNN models for ATS and discuss further directions for future research.
15. The rest of survey is structured as follows.
16. First we will give a comprehensive explanation of GNNs in the context of ATS.
17. Next, we will explore a number of interesting and innovative models.
18. Finally, we will finish with a conclusion and an outlook on the future usage of GNNs for ATS."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,Computer Science,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,s6,Spatial Convolution and Message Passing,"['p6.0', 'p6.1', 'p6.2', 'p6.3', 'p6.4', 'p6.5', 'p6.6', 'p6.7', 'p6.8', 'p6.9', 'p6.10', 'p6.11', 'p6.12', 'p6.13']","['One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.', 'Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:', 'This first equation describes how messages are generated. A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.', 'The above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function ρ. This function aggregates all incoming messages to a node. Then another differen-tiable function ψ combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.', 'The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.', 'where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.', 'The above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1', 'where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with', '. This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows', '). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.', 'There are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.', 'In ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.', 'Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.', 'We want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.']","One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.

Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:

This first equation describes how messages are generated. A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.

The above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function ρ. This function aggregates all incoming messages to a node. Then another differen-tiable function ψ combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.

The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.

where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.

The above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1

where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with

. This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows

). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.

There are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.

In ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.

Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.

We want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","(p6.0) One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.

(p6.1) Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:

(p6.2) This first equation describes how messages are generated. A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.

(p6.3) The above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function ρ. This function aggregates all incoming messages to a node. Then another differen-tiable function ψ combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.

(p6.4) The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.

(p6.5) where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.

(p6.6) The above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1

(p6.7) where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with

(p6.8) . This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows

(p6.9) ). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.

(p6.10) There are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.

(p6.11) In ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.

(p6.12) Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.

(p6.13) We want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","[[], ['b11'], [], [], [], [], ['b31', 'b32'], [], ['b3'], [], [], [], [], []]","[[], ['b11'], [], [], [], [], ['b31', 'b32'], [], ['b3'], [], [], [], [], []]",4,"1. One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs.
2. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image.
3. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node.
4. Generalizing this idea to a non-regular grids leads to spatial convolution.
5. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation.
6. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.
7. Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme.
8. However, such a description suffers from scalability issues due to it directly operating over the entire graph.
9. As such modern GNNs use, what is commonly referred to as, message passing.
10. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations.
11. Thereby the network is able to diffuse information throughout the graph.
12. Consequently, the more iterations, the further outwards information is propagated throughout the graph.
13. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution.
14. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:This first equation describes how messages are generated.
15. A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.
16. The above equation is the core of the message passing framework and describes how each node feature is updated.
17. The first part consists in the application of a permutation-invariant reduction function ρ.
18. This function aggregates all incoming messages to a node.
19. Then another differen-tiable function ψ combines the reduced messages received with the previous state.
20. Using these two equations one can utilize message passing for learnable layers.
21. The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1
22. i,j = x t j and the aggregation is the normalized sum of messages, i.e.where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree.
23. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.
24. The above presented convolution layer does not allow the model to filter unimportant neighbours.
25. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour.
26. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure.
27. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages.
28. Just as in transformers GAT is formulated with multi-head attention.
29. The modification to the previously presented convolution layer follows closely the common attention formulation.
30. Formally, x t+1where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads.
31. The attention scores are computed with.
32. This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ).
33. We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2.
34. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be.
35. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity.
36. Taking the above description the attention score for GATv2 is modified as follows).
37. In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.
38. There are numerous modifications and extensions to the basic convolution presented here.
39. However, for ATS models, GAT layers are dominating as the workhorse for most models.
40. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST.
41. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT.
42. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.
43. In ATS the graphs used are in nearly all cases not homogeneous.
44. However, the equations presented here do not work for heterogeneous graphs.
45. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph.
46. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.
47. Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models.
48. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes.
49. This is usually done with some function f where f is commonly the mean, max or sum.
50. We want to explicitly point out to the reader that the construction of GNNs does not require special datasets.
51. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail.
52. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,Computer Science,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,s8,Standalone GNNs,"['p8.0', 'p8.1', 'p8.2', 'p8.3', 'p8.4', 'p8.5', 'p8.6', 'p8.7', 'p8.8', 'p8.9', 'p8.10']","['We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.', 'The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.', 'The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.', 'The classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.', 'The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.', 'An older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.', 'A model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.', 'This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.', 'Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.', 'HAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.', 'The results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.']","We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.

The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.

The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.

The classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.

The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.

An older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.

A model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.

This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.

Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.

HAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.

The results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","(p8.0) We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.

(p8.1) The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.

(p8.2) The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.

(p8.3) The classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.

(p8.4) The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.

(p8.5) An older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.

(p8.6) A model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.

(p8.7) This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.

(p8.8) Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.

(p8.9) HAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.

(p8.10) The results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","[[], [], [], ['b30'], [], ['b29', 'b43'], ['b22', 'b43'], ['b1'], [None], [], ['b52']]","[[], [], [], ['b30'], [], ['b29', 'b43'], ['b22', 'b43'], ['b1'], [None], [], ['b52']]",8,"1. We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by .
2. We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS.
3. An illustration of the general concepts presented here can be seen in Figure 3.
4. The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes.
5. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge.
6. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document.
7. This is a flexible structure, as it can be used in a single-document but also multidocument setting.
8. Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism.
9. Inspired by HSG. The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word.
10. The neural network consists of a modified GAT layer.
11. The GAT is modified to consider the TF-IDF value of the connecting edge.
12. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution.
13. In total three convolution layers are used, word-sentence, sentence-word and word-document.
14. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.
15. The classification itself is done by a single linear layer.
16. The model then does not directly use the predicted nodes to produce the summary.
17. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.
18. The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset.
19. One should especially note the flexibility and ability to use this model for two tasks.
20. An older model by Muratore et al. (2010) can be considered a precursor to this architecture.
21. A simple extension to the HSG model is proposed by Ya et al. (2021).
22. In their extension they modify the model for query constraints for the summary.
23. This is achieved by adding a query node to the graph structure.
24. Additionally, they introduce a mu-tual information maximization mechanism during training.
25. A model which further follows this structure is the one by Linmei et al. (2019).
26. The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution.
27. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels.
28. In particular, they encode the semantic and syntactical relationship between sentences within the graph.
29. This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019).
30. They introduce an additional universal feature vector which is added to each sentence node embedding.
31. This universal feature vector is learned from a large unrelated and general corpus.
32. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.
33. Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a).
34. The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities.
35. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens.
36. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence.
37. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram.
38. Additionally, sequentially occurring words and entities are connected with a directed edge.
39. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.
40. HAHSum uses a GAT for each of the five node type combinations found within the graph.
41. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.
42. The results for HAHSum show that GNNs can perform very well.
43. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset.
44. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS .
45. The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness.
46. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by .
47. Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents.
48. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution.
49. These sub-graphs are then ranked and thereby selected for a summary.
50. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem.
51. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,Computer Science,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,s1,Why graph neural networks ?,"['p1.0', 'p1.1', 'p1.2']","['Contemporary solutions to the task of ATS suffer from a number of issues, chiefly an inconsistent evaluation protocol and, somewhat, a lack of progress, as noted by Kryściński et al. (2019). In recent years GNNs have been successfully applied to a number of downstream NLP tasks such as classification    and translation (Xu et al., 2021) (Yin et al., 2020). Although GNNs may not be able to solve all problems related to the task of ATS, we believe that they can at least give a new perspective to this task. Generally GNNs bring a number of advantages to ATS which we believe to be significant enough to warrant further research, and this survey. In particular we want to highlight the following aspects of GNNs:', ""• Scalability and Flexibility. A vast number of ATS models are based on BERT (Devlin et al., 2019). However, the computational complexity of BERT-based ATS models grows quadratic with the input length; due to the selfattention operation. This fact renders them impractical for long, or even medium sized text documents. Recently some work has been done in order to circumvent this limiting factor (Ding et al., 2020) (Zhang et al., 2021). In contrast, GNNs can scale by their nature to graphs of thousands of nodes and more. This is in part due to the linear scaling of the memory cost with regards to the input size. The total memory cost of a GNN model depends on the size of the graph, the number of layers and the feature vector size of the nodes present. Formally, for L layers and an input of N nodes with each node's feature vector being of size H the memory complexity is O(LN H). But even for very large graphs on the scale of millions of nodes one can utilize GNNs. This can be achieved using methods such as neighbour sampling or distributing the graph over multiple GPUs, as done for example by Jia et al. (2020b). We recommend the paper by  for insights as to how one can train large and very deep GNNs. As the input of a GNN is a graph, the input can vary in size, therefore GNNs are also able to cope with changing text sizes and structures. Both of these aspects combined allow GNNs to produce summaries which are not restricted by hard-coded limits related to input or output size."", '• Understanding and Explainability. It is often difficult to understand why a model arrived at a certain conclusion. Additionally it is often difficult to see how the model aggregates information. This is not the case with GNNs, as with the help of methods such as GNN Explainer (Ying et al., 2019) one can understand which nodes were used by the model to reach its output. This removes a layer of the blackbox magic present in many current non-GNN models. We recommend the survey by Yuan et al. (2020) for an overview of methods for generating explanations for GNNs.']","Contemporary solutions to the task of ATS suffer from a number of issues, chiefly an inconsistent evaluation protocol and, somewhat, a lack of progress, as noted by Kryściński et al. (2019). In recent years GNNs have been successfully applied to a number of downstream NLP tasks such as classification    and translation (Xu et al., 2021) (Yin et al., 2020). Although GNNs may not be able to solve all problems related to the task of ATS, we believe that they can at least give a new perspective to this task. Generally GNNs bring a number of advantages to ATS which we believe to be significant enough to warrant further research, and this survey. In particular we want to highlight the following aspects of GNNs:

• Scalability and Flexibility. A vast number of ATS models are based on BERT (Devlin et al., 2019). However, the computational complexity of BERT-based ATS models grows quadratic with the input length; due to the selfattention operation. This fact renders them impractical for long, or even medium sized text documents. Recently some work has been done in order to circumvent this limiting factor (Ding et al., 2020) (Zhang et al., 2021). In contrast, GNNs can scale by their nature to graphs of thousands of nodes and more. This is in part due to the linear scaling of the memory cost with regards to the input size. The total memory cost of a GNN model depends on the size of the graph, the number of layers and the feature vector size of the nodes present. Formally, for L layers and an input of N nodes with each node's feature vector being of size H the memory complexity is O(LN H). But even for very large graphs on the scale of millions of nodes one can utilize GNNs. This can be achieved using methods such as neighbour sampling or distributing the graph over multiple GPUs, as done for example by Jia et al. (2020b). We recommend the paper by  for insights as to how one can train large and very deep GNNs. As the input of a GNN is a graph, the input can vary in size, therefore GNNs are also able to cope with changing text sizes and structures. Both of these aspects combined allow GNNs to produce summaries which are not restricted by hard-coded limits related to input or output size.

• Understanding and Explainability. It is often difficult to understand why a model arrived at a certain conclusion. Additionally it is often difficult to see how the model aggregates information. This is not the case with GNNs, as with the help of methods such as GNN Explainer (Ying et al., 2019) one can understand which nodes were used by the model to reach its output. This removes a layer of the blackbox magic present in many current non-GNN models. We recommend the survey by Yuan et al. (2020) for an overview of methods for generating explanations for GNNs.","(p1.0) Contemporary solutions to the task of ATS suffer from a number of issues, chiefly an inconsistent evaluation protocol and, somewhat, a lack of progress, as noted by Kryściński et al. (2019). In recent years GNNs have been successfully applied to a number of downstream NLP tasks such as classification    and translation (Xu et al., 2021) (Yin et al., 2020). Although GNNs may not be able to solve all problems related to the task of ATS, we believe that they can at least give a new perspective to this task. Generally GNNs bring a number of advantages to ATS which we believe to be significant enough to warrant further research, and this survey. In particular we want to highlight the following aspects of GNNs:

(p1.1) • Scalability and Flexibility. A vast number of ATS models are based on BERT (Devlin et al., 2019). However, the computational complexity of BERT-based ATS models grows quadratic with the input length; due to the selfattention operation. This fact renders them impractical for long, or even medium sized text documents. Recently some work has been done in order to circumvent this limiting factor (Ding et al., 2020) (Zhang et al., 2021). In contrast, GNNs can scale by their nature to graphs of thousands of nodes and more. This is in part due to the linear scaling of the memory cost with regards to the input size. The total memory cost of a GNN model depends on the size of the graph, the number of layers and the feature vector size of the nodes present. Formally, for L layers and an input of N nodes with each node's feature vector being of size H the memory complexity is O(LN H). But even for very large graphs on the scale of millions of nodes one can utilize GNNs. This can be achieved using methods such as neighbour sampling or distributing the graph over multiple GPUs, as done for example by Jia et al. (2020b). We recommend the paper by  for insights as to how one can train large and very deep GNNs. As the input of a GNN is a graph, the input can vary in size, therefore GNNs are also able to cope with changing text sizes and structures. Both of these aspects combined allow GNNs to produce summaries which are not restricted by hard-coded limits related to input or output size.

(p1.2) • Understanding and Explainability. It is often difficult to understand why a model arrived at a certain conclusion. Additionally it is often difficult to see how the model aggregates information. This is not the case with GNNs, as with the help of methods such as GNN Explainer (Ying et al., 2019) one can understand which nodes were used by the model to reach its output. This removes a layer of the blackbox magic present in many current non-GNN models. We recommend the survey by Yuan et al. (2020) for an overview of methods for generating explanations for GNNs.","[['b45', 'b16', 'b42'], ['b7', 'b6', 'b48', 'b14'], ['b47', 'b46']]","[['b45', 'b16', 'b42'], ['b7', 'b6', 'b48', 'b14'], ['b47', 'b46']]",9,"1. Contemporary solutions to the task of ATS suffer from a number of issues, chiefly an inconsistent evaluation protocol and, somewhat, a lack of progress, as noted by Kryściński et al. (2019).
2. In recent years GNNs have been successfully applied to a number of downstream NLP tasks such as classification    and translation (Xu et al., 2021) (Yin et al., 2020).
3. Although GNNs may not be able to solve all problems related to the task of ATS, we believe that they can at least give a new perspective to this task.
4. Generally GNNs bring a number of advantages to ATS which we believe to be significant enough to warrant further research, and this survey.
5. In particular we want to highlight the following aspects of GNNs:• Scalability and Flexibility.
6. A vast number of ATS models are based on BERT (Devlin et al., 2019).
7. However, the computational complexity of BERT-based ATS models grows quadratic with the input length; due to the selfattention operation.
8. This fact renders them impractical for long, or even medium sized text documents.
9. Recently some work has been done in order to circumvent this limiting factor (Ding et al., 2020) (Zhang et al., 2021).
10. In contrast, GNNs can scale by their nature to graphs of thousands of nodes and more.
11. This is in part due to the linear scaling of the memory cost with regards to the input size.
12. The total memory cost of a GNN model depends on the size of the graph, the number of layers and the feature vector size of the nodes present.
13. Formally, for L layers and an input of N nodes with each node's feature vector being of size H the memory complexity is O(LN H).
14. But even for very large graphs on the scale of millions of nodes one can utilize GNNs.
15. This can be achieved using methods such as neighbour sampling or distributing the graph over multiple GPUs, as done for example by Jia et al. (2020b).
16. We recommend the paper by  for insights as to how one can train large and very deep GNNs.
17. As the input of a GNN is a graph, the input can vary in size, therefore GNNs are also able to cope with changing text sizes and structures.
18. Both of these aspects combined allow GNNs to produce summaries which are not restricted by hard-coded limits related to input or output size.
19. • Understanding and Explainability.
20. It is often difficult to understand why a model arrived at a certain conclusion.
21. Additionally it is often difficult to see how the model aggregates information.
22. This is not the case with GNNs, as with the help of methods such as GNN Explainer (Ying et al., 2019) one can understand which nodes were used by the model to reach its output.
23. This removes a layer of the blackbox magic present in many current non-GNN models.
24. We recommend the survey by Yuan et al. (2020) for an overview of methods for generating explanations for GNNs."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,Computer Science,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,s9,Embedded GNNs,"['p9.0', 'p9.1', 'p9.2', 'p9.3', 'p9.4', 'p9.5', 'p9.6']","['The first embedded GNN model we will present is the GRU-GCN model by Yasunaga et al. (2017). Despite being an older model it provides an illustrative introduction as to how one can effectively incorporate GNNs into established deep learning methods. As this model utilizes GNNs within a sequence-sequence architecture it falls into the encoder-decoder category of embedded GNNs. The model works exclusively with multi-document summarization. We want to note that due to the age of this model the GNN uses spectral-based convolution instead of spatial-based convolution.', 'As the model is an encoding-decoding embedded GNN it features three parts: the encoding part, the GNN and a decoding part. The encoding and decod-GNN CNN/DailyMail Performance Overview Model Name Type Description R-1 R-2 R-L BERTSUMEXT (Liu and Lapata, 2019) Extractive BERT baseline  Topic-Graphsum  Extractive (   Extractive (Standalone) Sentence information encoding 43.16 20.14 39.49 HSG  Extractive ( ing are done by gated recurrent networks (GRUs). More specifically, sentence encodings are produced by the encoding GRU network. These sentence encodings are then used by the GNN whose input graph consists solely of sentence nodes. The edges are determined by semantic relatedness of the sentences. The resulting sentence node feature vectors produced by the GNN are passed to the decoding GRU which computes salience scores for each sentence. The results for this model have been surpassed by other models. Similar to the GRU-GCN model, the Topic-GraphSum model by  combines an established deep neural network method with a GNN. In it a variational autoencoder is utilized for modeling topics within a given text, that is, it learns latent topics via encoding-decoding. The GNN is fed a graph consisting of topic nodes and sentence nodes. The topic node embeddings are produced by the autoencoder and the sentence node embeddings are produced by BERT. The GNN utilizes a GAT for the prediction of sentences to be used for the summary. Note that the autoencoder and the GNN are trained jointly, which is why this model is classified as an embedded model. However, as it does not utilize the encoder-decoder architecture it falls into the category of other embedded GNNs.', 'Another embedded GNN which does not follow the encoder-decoder schema is the model by Xu et al. (2020b). Their DISCOBERT model incorporates a GNN into a BERTSUM (Liu and Lapata, 2019) like architecture.', 'The DSGSum model developed by Bi et al. (2021) utilizes a GNN to enhance the semantic information provided to the model. DSGSum uses the GNN mainly for encoding entity information. The input to the GNN consists of an entity graph which has been enriched with a knowledge graph (KG); specifically, the entities and their relations as defined in the KG are encoded into the graph. The GNN then utilizes GAT to produce entity embeddings which are directly used by the decoder part of the architecture. Hence DSGSum is an embedded encoder-decoder model. All of the following embedded GNN ATS models follow the ATS GNN encoder-decoder pattern. The general principle of these models is illustrated by Figure 4. This principle being the usage of the GNN to supplement the information provided to the decoder, while also utilizing an encoder for generating the input to the GNN.', 'The authors of (Wu et al., 2021b) produce a graph based on the semantics of each sentence. The encoding produced by the GNN and a textual encoding of each sentence are then passed to a decoder. Similarly, the model by  uses a GNN as an encoding component. Their model utilizes the dependency tree of the input as a graph input to the GNN. It also uses a modified attention mechanism which is used to decode the attention of the GAT directly into the decoder part of the architecture. Another model in this category is the model by Liang et al. (2021). Following this idea even further is the model by Li et al. (2020). However, in contrast to DSGSum or others, their model uses a GNN directly within the transformer based encoder and decoder blocks, and not as an outside component providing additional encodings.', 'Another area where GNNs have found some usage is in the abstractive summarization of di-alogues, which although a niche area, is still part of the ATS task. The model by  uses a GNN to encode the structure of the conversation into their sequence-to-sequence architecture. This is also done by Feng et al. (2020). Also for dialogue summarization Feng et al. (2021) introduce a special type of graph encoding to a sequenceto-sequence architecture. They utilize a GNN as an encoder and their input graph links information about the speaker, the spoken sentences and other information together.', 'At this point we also want to shortly mention a few models which do not perform classical ATS but do perform a specialized form of summarization with the help of a GNN. The models by  and LeClair et al. (2020) both perform code summarization, that is they generate a natural language description/summarization of code written in a programming language. They both utilize a GNN, and also both leverage the abstract syntax tree of the program given. Another interesting model is the model by Wu et al. (2019). Their model performs multi-video summarization with the help of a GNN.']","The first embedded GNN model we will present is the GRU-GCN model by Yasunaga et al. (2017). Despite being an older model it provides an illustrative introduction as to how one can effectively incorporate GNNs into established deep learning methods. As this model utilizes GNNs within a sequence-sequence architecture it falls into the encoder-decoder category of embedded GNNs. The model works exclusively with multi-document summarization. We want to note that due to the age of this model the GNN uses spectral-based convolution instead of spatial-based convolution.

As the model is an encoding-decoding embedded GNN it features three parts: the encoding part, the GNN and a decoding part. The encoding and decod-GNN CNN/DailyMail Performance Overview Model Name Type Description R-1 R-2 R-L BERTSUMEXT (Liu and Lapata, 2019) Extractive BERT baseline  Topic-Graphsum  Extractive (   Extractive (Standalone) Sentence information encoding 43.16 20.14 39.49 HSG  Extractive ( ing are done by gated recurrent networks (GRUs). More specifically, sentence encodings are produced by the encoding GRU network. These sentence encodings are then used by the GNN whose input graph consists solely of sentence nodes. The edges are determined by semantic relatedness of the sentences. The resulting sentence node feature vectors produced by the GNN are passed to the decoding GRU which computes salience scores for each sentence. The results for this model have been surpassed by other models. Similar to the GRU-GCN model, the Topic-GraphSum model by  combines an established deep neural network method with a GNN. In it a variational autoencoder is utilized for modeling topics within a given text, that is, it learns latent topics via encoding-decoding. The GNN is fed a graph consisting of topic nodes and sentence nodes. The topic node embeddings are produced by the autoencoder and the sentence node embeddings are produced by BERT. The GNN utilizes a GAT for the prediction of sentences to be used for the summary. Note that the autoencoder and the GNN are trained jointly, which is why this model is classified as an embedded model. However, as it does not utilize the encoder-decoder architecture it falls into the category of other embedded GNNs.

Another embedded GNN which does not follow the encoder-decoder schema is the model by Xu et al. (2020b). Their DISCOBERT model incorporates a GNN into a BERTSUM (Liu and Lapata, 2019) like architecture.

The DSGSum model developed by Bi et al. (2021) utilizes a GNN to enhance the semantic information provided to the model. DSGSum uses the GNN mainly for encoding entity information. The input to the GNN consists of an entity graph which has been enriched with a knowledge graph (KG); specifically, the entities and their relations as defined in the KG are encoded into the graph. The GNN then utilizes GAT to produce entity embeddings which are directly used by the decoder part of the architecture. Hence DSGSum is an embedded encoder-decoder model. All of the following embedded GNN ATS models follow the ATS GNN encoder-decoder pattern. The general principle of these models is illustrated by Figure 4. This principle being the usage of the GNN to supplement the information provided to the decoder, while also utilizing an encoder for generating the input to the GNN.

The authors of (Wu et al., 2021b) produce a graph based on the semantics of each sentence. The encoding produced by the GNN and a textual encoding of each sentence are then passed to a decoder. Similarly, the model by  uses a GNN as an encoding component. Their model utilizes the dependency tree of the input as a graph input to the GNN. It also uses a modified attention mechanism which is used to decode the attention of the GAT directly into the decoder part of the architecture. Another model in this category is the model by Liang et al. (2021). Following this idea even further is the model by Li et al. (2020). However, in contrast to DSGSum or others, their model uses a GNN directly within the transformer based encoder and decoder blocks, and not as an outside component providing additional encodings.

Another area where GNNs have found some usage is in the abstractive summarization of di-alogues, which although a niche area, is still part of the ATS task. The model by  uses a GNN to encode the structure of the conversation into their sequence-to-sequence architecture. This is also done by Feng et al. (2020). Also for dialogue summarization Feng et al. (2021) introduce a special type of graph encoding to a sequenceto-sequence architecture. They utilize a GNN as an encoder and their input graph links information about the speaker, the spoken sentences and other information together.

At this point we also want to shortly mention a few models which do not perform classical ATS but do perform a specialized form of summarization with the help of a GNN. The models by  and LeClair et al. (2020) both perform code summarization, that is they generate a natural language description/summarization of code written in a programming language. They both utilize a GNN, and also both leverage the abstract syntax tree of the program given. Another interesting model is the model by Wu et al. (2019). Their model performs multi-video summarization with the help of a GNN.","(p9.0) The first embedded GNN model we will present is the GRU-GCN model by Yasunaga et al. (2017). Despite being an older model it provides an illustrative introduction as to how one can effectively incorporate GNNs into established deep learning methods. As this model utilizes GNNs within a sequence-sequence architecture it falls into the encoder-decoder category of embedded GNNs. The model works exclusively with multi-document summarization. We want to note that due to the age of this model the GNN uses spectral-based convolution instead of spatial-based convolution.

(p9.1) As the model is an encoding-decoding embedded GNN it features three parts: the encoding part, the GNN and a decoding part. The encoding and decod-GNN CNN/DailyMail Performance Overview Model Name Type Description R-1 R-2 R-L BERTSUMEXT (Liu and Lapata, 2019) Extractive BERT baseline  Topic-Graphsum  Extractive (   Extractive (Standalone) Sentence information encoding 43.16 20.14 39.49 HSG  Extractive ( ing are done by gated recurrent networks (GRUs). More specifically, sentence encodings are produced by the encoding GRU network. These sentence encodings are then used by the GNN whose input graph consists solely of sentence nodes. The edges are determined by semantic relatedness of the sentences. The resulting sentence node feature vectors produced by the GNN are passed to the decoding GRU which computes salience scores for each sentence. The results for this model have been surpassed by other models. Similar to the GRU-GCN model, the Topic-GraphSum model by  combines an established deep neural network method with a GNN. In it a variational autoencoder is utilized for modeling topics within a given text, that is, it learns latent topics via encoding-decoding. The GNN is fed a graph consisting of topic nodes and sentence nodes. The topic node embeddings are produced by the autoencoder and the sentence node embeddings are produced by BERT. The GNN utilizes a GAT for the prediction of sentences to be used for the summary. Note that the autoencoder and the GNN are trained jointly, which is why this model is classified as an embedded model. However, as it does not utilize the encoder-decoder architecture it falls into the category of other embedded GNNs.

(p9.2) Another embedded GNN which does not follow the encoder-decoder schema is the model by Xu et al. (2020b). Their DISCOBERT model incorporates a GNN into a BERTSUM (Liu and Lapata, 2019) like architecture.

(p9.3) The DSGSum model developed by Bi et al. (2021) utilizes a GNN to enhance the semantic information provided to the model. DSGSum uses the GNN mainly for encoding entity information. The input to the GNN consists of an entity graph which has been enriched with a knowledge graph (KG); specifically, the entities and their relations as defined in the KG are encoded into the graph. The GNN then utilizes GAT to produce entity embeddings which are directly used by the decoder part of the architecture. Hence DSGSum is an embedded encoder-decoder model. All of the following embedded GNN ATS models follow the ATS GNN encoder-decoder pattern. The general principle of these models is illustrated by Figure 4. This principle being the usage of the GNN to supplement the information provided to the decoder, while also utilizing an encoder for generating the input to the GNN.

(p9.4) The authors of (Wu et al., 2021b) produce a graph based on the semantics of each sentence. The encoding produced by the GNN and a textual encoding of each sentence are then passed to a decoder. Similarly, the model by  uses a GNN as an encoding component. Their model utilizes the dependency tree of the input as a graph input to the GNN. It also uses a modified attention mechanism which is used to decode the attention of the GAT directly into the decoder part of the architecture. Another model in this category is the model by Liang et al. (2021). Following this idea even further is the model by Li et al. (2020). However, in contrast to DSGSum or others, their model uses a GNN directly within the transformer based encoder and decoder blocks, and not as an outside component providing additional encodings.

(p9.5) Another area where GNNs have found some usage is in the abstractive summarization of di-alogues, which although a niche area, is still part of the ATS task. The model by  uses a GNN to encode the structure of the conversation into their sequence-to-sequence architecture. This is also done by Feng et al. (2020). Also for dialogue summarization Feng et al. (2021) introduce a special type of graph encoding to a sequenceto-sequence architecture. They utilize a GNN as an encoder and their input graph links information about the speaker, the spoken sentences and other information together.

(p9.6) At this point we also want to shortly mention a few models which do not perform classical ATS but do perform a specialized form of summarization with the help of a GNN. The models by  and LeClair et al. (2020) both perform code summarization, that is they generate a natural language description/summarization of code written in a programming language. They both utilize a GNN, and also both leverage the abstract syntax tree of the program given. Another interesting model is the model by Wu et al. (2019). Their model performs multi-video summarization with the help of a GNN.","[['b44'], ['b25'], ['b40'], ['b2'], ['b53', 'b21', 'b37'], ['b10', 'b9'], ['b18', 'b34']]","[['b44'], ['b25'], ['b40'], ['b2'], ['b53', 'b21', 'b37'], ['b10', 'b9'], ['b18', 'b34']]",11,"1. The first embedded GNN model we will present is the GRU-GCN model by Yasunaga et al. (2017).
2. Despite being an older model it provides an illustrative introduction as to how one can effectively incorporate GNNs into established deep learning methods.
3. As this model utilizes GNNs within a sequence-sequence architecture it falls into the encoder-decoder category of embedded GNNs.
4. The model works exclusively with multi-document summarization.
5. We want to note that due to the age of this model the GNN uses spectral-based convolution instead of spatial-based convolution.
6. As the model is an encoding-decoding embedded GNN it features three parts: the encoding part, the GNN and a decoding part.
7. The encoding and decod-GNN CNN/DailyMail Performance Overview Model Name Type Description R-1 R-2 R-L BERTSUMEXT (Liu and Lapata, 2019) Extractive BERT baseline  Topic-Graphsum  Extractive (   Extractive (Standalone) Sentence information encoding 43.16 20.14 39.49 HSG  Extractive ( ing are done by gated recurrent networks (GRUs).
8. More specifically, sentence encodings are produced by the encoding GRU network.
9. These sentence encodings are then used by the GNN whose input graph consists solely of sentence nodes.
10. The edges are determined by semantic relatedness of the sentences.
11. The resulting sentence node feature vectors produced by the GNN are passed to the decoding GRU which computes salience scores for each sentence.
12. The results for this model have been surpassed by other models.
13. Similar to the GRU-GCN model, the Topic-GraphSum model by  combines an established deep neural network method with a GNN.
14. In it a variational autoencoder is utilized for modeling topics within a given text, that is, it learns latent topics via encoding-decoding.
15. The GNN is fed a graph consisting of topic nodes and sentence nodes.
16. The topic node embeddings are produced by the autoencoder and the sentence node embeddings are produced by BERT.
17. The GNN utilizes a GAT for the prediction of sentences to be used for the summary.
18. Note that the autoencoder and the GNN are trained jointly, which is why this model is classified as an embedded model.
19. However, as it does not utilize the encoder-decoder architecture it falls into the category of other embedded GNNs.
20. Another embedded GNN which does not follow the encoder-decoder schema is the model by Xu et al. (2020b).
21. Their DISCOBERT model incorporates a GNN into a BERTSUM (Liu and Lapata, 2019) like architecture.
22. The DSGSum model developed by Bi et al. (2021) utilizes a GNN to enhance the semantic information provided to the model.
23. DSGSum uses the GNN mainly for encoding entity information.
24. The input to the GNN consists of an entity graph which has been enriched with a knowledge graph (KG); specifically, the entities and their relations as defined in the KG are encoded into the graph.
25. The GNN then utilizes GAT to produce entity embeddings which are directly used by the decoder part of the architecture.
26. Hence DSGSum is an embedded encoder-decoder model.
27. All of the following embedded GNN ATS models follow the ATS GNN encoder-decoder pattern.
28. The general principle of these models is illustrated by Figure 4.
29. This principle being the usage of the GNN to supplement the information provided to the decoder, while also utilizing an encoder for generating the input to the GNN.
30. The authors of (Wu et al., 2021b) produce a graph based on the semantics of each sentence.
31. The encoding produced by the GNN and a textual encoding of each sentence are then passed to a decoder.
32. Similarly, the model by  uses a GNN as an encoding component.
33. Their model utilizes the dependency tree of the input as a graph input to the GNN.
34. It also uses a modified attention mechanism which is used to decode the attention of the GAT directly into the decoder part of the architecture.
35. Another model in this category is the model by Liang et al. (2021).
36. Following this idea even further is the model by Li et al. (2020).
37. However, in contrast to DSGSum or others, their model uses a GNN directly within the transformer based encoder and decoder blocks, and not as an outside component providing additional encodings.
38. Another area where GNNs have found some usage is in the abstractive summarization of di-alogues, which although a niche area, is still part of the ATS task.
39. The model by  uses a GNN to encode the structure of the conversation into their sequence-to-sequence architecture.
40. This is also done by Feng et al. (2020).
41. Also for dialogue summarization Feng et al. (2021) introduce a special type of graph encoding to a sequenceto-sequence architecture.
42. They utilize a GNN as an encoder and their input graph links information about the speaker, the spoken sentences and other information together.
43. At this point we also want to shortly mention a few models which do not perform classical ATS but do perform a specialized form of summarization with the help of a GNN.
44. The models by  and LeClair et al. (2020) both perform code summarization, that is they generate a natural language description/summarization of code written in a programming language.
45. They both utilize a GNN, and also both leverage the abstract syntax tree of the program given.
46. Another interesting model is the model by Wu et al. (2019).
47. Their model performs multi-video summarization with the help of a GNN."
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s6,Performance Prediction,"['p6.0', 'p6.1', 'p6.2']","['Predicting performance can be another indicator for querying. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model. This strategy can be computationally costly since retraining is needed for each candidate.', 'Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b). This learning-to-select strategy may have some constraints. First, it requires labeled data (maybe from another domain) to train the policy. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.', 'A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).']","Predicting performance can be another indicator for querying. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model. This strategy can be computationally costly since retraining is needed for each candidate.

Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b). This learning-to-select strategy may have some constraints. First, it requires labeled data (maybe from another domain) to train the policy. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.

A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).","(p6.0) Predicting performance can be another indicator for querying. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model. This strategy can be computationally costly since retraining is needed for each candidate.

(p6.1) Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b). This learning-to-select strategy may have some constraints. First, it requires labeled data (maybe from another domain) to train the policy. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.

(p6.2) A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).","[[None], ['b53', None, 'b98'], [None, 'b38', 'b106', 'b62']]","[[None], ['b53', None, 'b98'], [None, 'b38', 'b106', 'b62']]",8,"1. Predicting performance can be another indicator for querying.
2. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set.
3. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model.
4. This strategy can be computationally costly since retraining is needed for each candidate.
5. Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set.
6. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b).
7. This learning-to-select strategy may have some constraints.
8. First, it requires labeled data (maybe from another domain) to train the policy.
9. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations.
10. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.
11. A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model.
12. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling.
13. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold.
14. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b)."
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s8,Density,['p8.0'],"[""With the motivation to avoid outliers, density-based strategies prefer instances that are more representative of the unlabeled set. Selecting by n-gram or word counts (Ambati et al., 2010a;Zhao et al., 2020b) can be regarded as a simple way of density measurement. Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998;. While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., , 2009).""]","With the motivation to avoid outliers, density-based strategies prefer instances that are more representative of the unlabeled set. Selecting by n-gram or word counts (Ambati et al., 2010a;Zhao et al., 2020b) can be regarded as a simple way of density measurement. Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998;. While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., , 2009).","(p8.0) With the motivation to avoid outliers, density-based strategies prefer instances that are more representative of the unlabeled set. Selecting by n-gram or word counts (Ambati et al., 2010a;Zhao et al., 2020b) can be regarded as a simple way of density measurement. Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998;. While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., , 2009).","[[None, 'b107', 'b84', 'b111']]","[[None, 'b107', 'b84', 'b111']]",4,"1. With the motivation to avoid outliers, density-based strategies prefer instances that are more representative of the unlabeled set.
2. Selecting by n-gram or word counts (Ambati et al., 2010a;Zhao et al., 2020b) can be regarded as a simple way of density measurement.
3. Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998;. While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., , 2009)."
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s3,Output Uncertainty,"['p3.0', 'p3.1']","['Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).', ""Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).""]","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","(p3.0) Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

(p3.1) Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","[['b49', 'b14', None, 'b11', 'b10', 'b25', 'b19', 'b91', 'b12'], ['b72', None]]","[['b49', 'b14', None, 'b11', 'b10', 'b25', 'b19', 'b91', 'b12'], ['b72', None]]",11,"1. Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.
2. It prefers the most uncertain instances judged by the model outputs.
3. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).
4. Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.
5. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).
6. Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.
7. If an instance is near the decision boundary, the model's outputs may be different within its local region.
8. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020)."
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s10,Batch Diversity,"['p10.0', 'p10.1', 'p10.2']","['Ideally, only one most useful instance would be selected in each iteration. However, it is more efficient and practical to adopt batch-mode AL (Settles, 2009), where each time a batch of instances is selected. In this case, we need to consider the dissimilarities not only between selected instances and labeled ones but also within the selected batch.', 'To select a batch of diverse instances, there are two common approaches. 1) Iterative selection collects the batch in an iterative greedy way (Brinker, 2003;. In each iteration, an instance is selected by comparing it with previously chosen instances to avoid redundancy. Some more advanced diversity-based criteria, like coreset (Geifman and El-Yaniv, 2017;Sener and Savarese, 2018) and determinantal point processes , can also be approximated in a similar way. 2) Clustering-based methods partition the unlabeled data into clusters and select instances among them Xu et al., 2003;Nguyen and Smeulders, 2004;Zhdanov, 2019;Yu et al., 2022). Since the chosen instances come from different clusters, diversity can be achieved to some extent.', 'For the calculation of similarity, in addition to comparing the input features or intermediate neural representations, other methods are also investigated, such as utilizing model-based similarity ), gradients (Ash et al., 2020Kim, 2020), and masked LM surprisal embeddings .']","Ideally, only one most useful instance would be selected in each iteration. However, it is more efficient and practical to adopt batch-mode AL (Settles, 2009), where each time a batch of instances is selected. In this case, we need to consider the dissimilarities not only between selected instances and labeled ones but also within the selected batch.

To select a batch of diverse instances, there are two common approaches. 1) Iterative selection collects the batch in an iterative greedy way (Brinker, 2003;. In each iteration, an instance is selected by comparing it with previously chosen instances to avoid redundancy. Some more advanced diversity-based criteria, like coreset (Geifman and El-Yaniv, 2017;Sener and Savarese, 2018) and determinantal point processes , can also be approximated in a similar way. 2) Clustering-based methods partition the unlabeled data into clusters and select instances among them Xu et al., 2003;Nguyen and Smeulders, 2004;Zhdanov, 2019;Yu et al., 2022). Since the chosen instances come from different clusters, diversity can be achieved to some extent.

For the calculation of similarity, in addition to comparing the input features or intermediate neural representations, other methods are also investigated, such as utilizing model-based similarity ), gradients (Ash et al., 2020Kim, 2020), and masked LM surprisal embeddings .","(p10.0) Ideally, only one most useful instance would be selected in each iteration. However, it is more efficient and practical to adopt batch-mode AL (Settles, 2009), where each time a batch of instances is selected. In this case, we need to consider the dissimilarities not only between selected instances and labeled ones but also within the selected batch.

(p10.1) To select a batch of diverse instances, there are two common approaches. 1) Iterative selection collects the batch in an iterative greedy way (Brinker, 2003;. In each iteration, an instance is selected by comparing it with previously chosen instances to avoid redundancy. Some more advanced diversity-based criteria, like coreset (Geifman and El-Yaniv, 2017;Sener and Savarese, 2018) and determinantal point processes , can also be approximated in a similar way. 2) Clustering-based methods partition the unlabeled data into clusters and select instances among them Xu et al., 2003;Nguyen and Smeulders, 2004;Zhdanov, 2019;Yu et al., 2022). Since the chosen instances come from different clusters, diversity can be achieved to some extent.

(p10.2) For the calculation of similarity, in addition to comparing the input features or intermediate neural representations, other methods are also investigated, such as utilizing model-based similarity ), gradients (Ash et al., 2020Kim, 2020), and masked LM surprisal embeddings .","[[], ['b60', 'b63', None, 'b17', 'b78'], [None, 'b91']]","[[], ['b60', 'b63', None, 'b17', 'b78'], [None, 'b91']]",7,"1. Ideally, only one most useful instance would be selected in each iteration.
2. However, it is more efficient and practical to adopt batch-mode AL (Settles, 2009), where each time a batch of instances is selected.
3. In this case, we need to consider the dissimilarities not only between selected instances and labeled ones but also within the selected batch.
4. To select a batch of diverse instances, there are two common approaches.
5. 1) Iterative selection collects the batch in an iterative greedy way (Brinker, 2003;. In each iteration, an instance is selected by comparing it with previously chosen instances to avoid redundancy. Some more advanced diversity-based criteria, like coreset (Geifman and El-Yaniv, 2017;Sener and Savarese, 2018) and determinantal point processes , can also be approximated in a similar way.
6. 2) Clustering-based methods partition the unlabeled data into clusters and select instances among them Xu et al., 2003;Nguyen and Smeulders, 2004;Zhdanov, 2019;Yu et al., 2022).
7. Since the chosen instances come from different clusters, diversity can be achieved to some extent.
8. For the calculation of similarity, in addition to comparing the input features or intermediate neural representations, other methods are also investigated, such as utilizing model-based similarity ), gradients (Ash et al., 2020Kim, 2020), and masked LM surprisal embeddings ."
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s26,Stopping AL,"['p26.0', 'p26.1', 'p26.2', 'p26.3', 'p26.4']","['When adopting AL in practice, it would be desirable to know the time to stop AL when the model performance is already near the upper limits, before running out of all the budgets. For this purpose, a stopping criterion is needed, which checks certain metrics satisfying certain conditions. There can be simple heuristics. For example, AL can be stopped when all unlabeled instances are no closer than any of the support vectors with an SVM (Schohn and Cohn, 2000;Ertekin et al., 2007) or no new n-grams remain in the unlabeled set for MT (Bloodgood and Callison-Burch, 2010). Nevertheless, these are specific to the underlying models or target tasks. For the design of a general stopping criterion, there are three main aspects to consider: metric, dataset and condition.', 'For the metric, measuring performance on a development set seems a natural option. However, the results would be unstable if this set is too small and it would be impractical to assume a large development set. Cross-validation on the training set also has problems since the labeled data by AL is usually biased. In this case, metrics from the query strategies can be utilized. Examples include uncertainty or confidence (Zhu and Hovy, 2007;Vlachos, 2008), disagreement (Tomanek et al., 2007;Olsson and Tomanek, 2009), estimated performance (Laws and Schütze, 2008), expected error (Zhu et al., 2008a), confidence variation (Ghayoomi, 2010), as well as actual performance on the selected instances (Zhu and Hovy, 2007). Moreover, comparing the predictions between consecutive AL iterations is another reasonable option (Zhu et al., 2008b;Bloodgood and Vijay-Shanker, 2009a).', 'The dataset to calculate the stopping metric requires careful choosing. The results could be unstable if not adopting a proper set . Many works suggest that a separate unlabeled dataset should be utilized Vlachos, 2008;Bloodgood and Vijay-Shanker, 2009a;Beatty et al., 2019;Kurlandski and Bloodgood, 2022). Since the stopping metrics usually do not rely on gold labels, this dataset could potentially be very large to provide more stable results, though wait time would be another factor to consider in this case ( §3.2.4).', 'The condition to stop AL is usually comparing the metrics to a pre-defined threshold. Earlier works only look at the metric at the current iteration, for example, stopping if the uncertainty or the error is less than the threshold (Zhu and Hovy, 2007). In this case, the threshold is hard to specify since it relies on the model and the task. (Zhu et al., 2008b) cascade multiple stopping criteria to mitigate this reliance. A more stable option is to track the change of the metrics over several AL iterations, such as stopping when the confidence consistently drops (Vlachos, 2008), the changing rate flattens (Laws and Schütze, 2008) or the predictions stabilize across iterations (Bloodgood and Vijay-Shanker, 2009a;Bloodgood and Grothendieck, 2013). Pullar-Strecker et al. (2021) provide an empirical comparison over common stopping criteria and would be a nice reference. Moreover, stopping AL can be closely related to performance prediction and early stopping. Especially, the latter can be of particular interest to AL since learning in early AL stages need to face the low-resource problem and how to perform early stopping may also require careful considerations.', '6 Related Topics and Future Directions']","When adopting AL in practice, it would be desirable to know the time to stop AL when the model performance is already near the upper limits, before running out of all the budgets. For this purpose, a stopping criterion is needed, which checks certain metrics satisfying certain conditions. There can be simple heuristics. For example, AL can be stopped when all unlabeled instances are no closer than any of the support vectors with an SVM (Schohn and Cohn, 2000;Ertekin et al., 2007) or no new n-grams remain in the unlabeled set for MT (Bloodgood and Callison-Burch, 2010). Nevertheless, these are specific to the underlying models or target tasks. For the design of a general stopping criterion, there are three main aspects to consider: metric, dataset and condition.

For the metric, measuring performance on a development set seems a natural option. However, the results would be unstable if this set is too small and it would be impractical to assume a large development set. Cross-validation on the training set also has problems since the labeled data by AL is usually biased. In this case, metrics from the query strategies can be utilized. Examples include uncertainty or confidence (Zhu and Hovy, 2007;Vlachos, 2008), disagreement (Tomanek et al., 2007;Olsson and Tomanek, 2009), estimated performance (Laws and Schütze, 2008), expected error (Zhu et al., 2008a), confidence variation (Ghayoomi, 2010), as well as actual performance on the selected instances (Zhu and Hovy, 2007). Moreover, comparing the predictions between consecutive AL iterations is another reasonable option (Zhu et al., 2008b;Bloodgood and Vijay-Shanker, 2009a).

The dataset to calculate the stopping metric requires careful choosing. The results could be unstable if not adopting a proper set . Many works suggest that a separate unlabeled dataset should be utilized Vlachos, 2008;Bloodgood and Vijay-Shanker, 2009a;Beatty et al., 2019;Kurlandski and Bloodgood, 2022). Since the stopping metrics usually do not rely on gold labels, this dataset could potentially be very large to provide more stable results, though wait time would be another factor to consider in this case ( §3.2.4).

The condition to stop AL is usually comparing the metrics to a pre-defined threshold. Earlier works only look at the metric at the current iteration, for example, stopping if the uncertainty or the error is less than the threshold (Zhu and Hovy, 2007). In this case, the threshold is hard to specify since it relies on the model and the task. (Zhu et al., 2008b) cascade multiple stopping criteria to mitigate this reliance. A more stable option is to track the change of the metrics over several AL iterations, such as stopping when the confidence consistently drops (Vlachos, 2008), the changing rate flattens (Laws and Schütze, 2008) or the predictions stabilize across iterations (Bloodgood and Vijay-Shanker, 2009a;Bloodgood and Grothendieck, 2013). Pullar-Strecker et al. (2021) provide an empirical comparison over common stopping criteria and would be a nice reference. Moreover, stopping AL can be closely related to performance prediction and early stopping. Especially, the latter can be of particular interest to AL since learning in early AL stages need to face the low-resource problem and how to perform early stopping may also require careful considerations.

6 Related Topics and Future Directions","(p26.0) When adopting AL in practice, it would be desirable to know the time to stop AL when the model performance is already near the upper limits, before running out of all the budgets. For this purpose, a stopping criterion is needed, which checks certain metrics satisfying certain conditions. There can be simple heuristics. For example, AL can be stopped when all unlabeled instances are no closer than any of the support vectors with an SVM (Schohn and Cohn, 2000;Ertekin et al., 2007) or no new n-grams remain in the unlabeled set for MT (Bloodgood and Callison-Burch, 2010). Nevertheless, these are specific to the underlying models or target tasks. For the design of a general stopping criterion, there are three main aspects to consider: metric, dataset and condition.

(p26.1) For the metric, measuring performance on a development set seems a natural option. However, the results would be unstable if this set is too small and it would be impractical to assume a large development set. Cross-validation on the training set also has problems since the labeled data by AL is usually biased. In this case, metrics from the query strategies can be utilized. Examples include uncertainty or confidence (Zhu and Hovy, 2007;Vlachos, 2008), disagreement (Tomanek et al., 2007;Olsson and Tomanek, 2009), estimated performance (Laws and Schütze, 2008), expected error (Zhu et al., 2008a), confidence variation (Ghayoomi, 2010), as well as actual performance on the selected instances (Zhu and Hovy, 2007). Moreover, comparing the predictions between consecutive AL iterations is another reasonable option (Zhu et al., 2008b;Bloodgood and Vijay-Shanker, 2009a).

(p26.2) The dataset to calculate the stopping metric requires careful choosing. The results could be unstable if not adopting a proper set . Many works suggest that a separate unlabeled dataset should be utilized Vlachos, 2008;Bloodgood and Vijay-Shanker, 2009a;Beatty et al., 2019;Kurlandski and Bloodgood, 2022). Since the stopping metrics usually do not rely on gold labels, this dataset could potentially be very large to provide more stable results, though wait time would be another factor to consider in this case ( §3.2.4).

(p26.3) The condition to stop AL is usually comparing the metrics to a pre-defined threshold. Earlier works only look at the metric at the current iteration, for example, stopping if the uncertainty or the error is less than the threshold (Zhu and Hovy, 2007). In this case, the threshold is hard to specify since it relies on the model and the task. (Zhu et al., 2008b) cascade multiple stopping criteria to mitigate this reliance. A more stable option is to track the change of the metrics over several AL iterations, such as stopping when the confidence consistently drops (Vlachos, 2008), the changing rate flattens (Laws and Schütze, 2008) or the predictions stabilize across iterations (Bloodgood and Vijay-Shanker, 2009a;Bloodgood and Grothendieck, 2013). Pullar-Strecker et al. (2021) provide an empirical comparison over common stopping criteria and would be a nice reference. Moreover, stopping AL can be closely related to performance prediction and early stopping. Especially, the latter can be of particular interest to AL since learning in early AL stages need to face the low-resource problem and how to perform early stopping may also require careful considerations.

(p26.4) 6 Related Topics and Future Directions","[[None, 'b12', 'b106'], ['b48', 'b82', 'b81', None, 'b47', 'b83', 'b52'], [None, 'b52'], [None, 'b83', 'b81', 'b52'], []]","[[None, 'b12', 'b106'], ['b48', 'b82', 'b81', None, 'b47', 'b83', 'b52'], [None, 'b52'], [None, 'b83', 'b81', 'b52'], []]",16,"1. When adopting AL in practice, it would be desirable to know the time to stop AL when the model performance is already near the upper limits, before running out of all the budgets.
2. For this purpose, a stopping criterion is needed, which checks certain metrics satisfying certain conditions.
3. There can be simple heuristics.
4. For example, AL can be stopped when all unlabeled instances are no closer than any of the support vectors with an SVM (Schohn and Cohn, 2000;Ertekin et al., 2007) or no new n-grams remain in the unlabeled set for MT (Bloodgood and Callison-Burch, 2010).
5. Nevertheless, these are specific to the underlying models or target tasks.
6. For the design of a general stopping criterion, there are three main aspects to consider: metric, dataset and condition.
7. For the metric, measuring performance on a development set seems a natural option.
8. However, the results would be unstable if this set is too small and it would be impractical to assume a large development set.
9. Cross-validation on the training set also has problems since the labeled data by AL is usually biased.
10. In this case, metrics from the query strategies can be utilized.
11. Examples include uncertainty or confidence (Zhu and Hovy, 2007;Vlachos, 2008), disagreement (Tomanek et al., 2007;Olsson and Tomanek, 2009), estimated performance (Laws and Schütze, 2008), expected error (Zhu et al., 2008a), confidence variation (Ghayoomi, 2010), as well as actual performance on the selected instances (Zhu and Hovy, 2007).
12. Moreover, comparing the predictions between consecutive AL iterations is another reasonable option (Zhu et al., 2008b;Bloodgood and Vijay-Shanker, 2009a).
13. The dataset to calculate the stopping metric requires careful choosing.
14. The results could be unstable if not adopting a proper set .
15. Many works suggest that a separate unlabeled dataset should be utilized Vlachos, 2008;Bloodgood and Vijay-Shanker, 2009a;Beatty et al., 2019;Kurlandski and Bloodgood, 2022).
16. Since the stopping metrics usually do not rely on gold labels, this dataset could potentially be very large to provide more stable results, though wait time would be another factor to consider in this case ( §3.2.4).
17. The condition to stop AL is usually comparing the metrics to a pre-defined threshold.
18. Earlier works only look at the metric at the current iteration, for example, stopping if the uncertainty or the error is less than the threshold (Zhu and Hovy, 2007).
19. In this case, the threshold is hard to specify since it relies on the model and the task.
20. (Zhu et al., 2008b) cascade multiple stopping criteria to mitigate this reliance.
21. A more stable option is to track the change of the metrics over several AL iterations, such as stopping when the confidence consistently drops (Vlachos, 2008), the changing rate flattens (Laws and Schütze, 2008) or the predictions stabilize across iterations (Bloodgood and Vijay-Shanker, 2009a;Bloodgood and Grothendieck, 2013).
22. Pullar-Strecker et al. (2021) provide an empirical comparison over common stopping criteria and would be a nice reference.
23. Moreover, stopping AL can be closely related to performance prediction and early stopping.
24. Especially, the latter can be of particular interest to AL since learning in early AL stages need to face the low-resource problem and how to perform early stopping may also require careful considerations.
25. 6 Related Topics and Future Directions"
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s11,Hybrid,"['p11.0', 'p11.1', 'p11.2', 'p11.3']","['There is no surprise that informativeness and representativeness can be combined for instance querying, leading to hybrid strategies. A simple combination can be used to merge multiple criteria into one. This can be achieved by a weighted sum (Kim et al., 2006;Chen et al., 2011) or multiplication .', 'There are several strategies to naturally integrate multiple criteria. Examples include (uncertainty) weighted clustering (Zhdanov, 2019), diverse gradient selection (Ash et al., 2020;Kim, 2020) where the gradients themselves contain uncertainty information ( §2.1.3) and determinantal point processes (DPP) with quality-diversity decomposition .', 'Moreover, multi-step querying, which applies multiple criteria in series, is another natural hybrid method. For example, one can consider first filtering certain highly uncertain instances and then performing clustering to select a diverse batch from them (Xu et al., 2003;Mirroshandel et al., 2011). An alternative strategy of selecting the most uncertain instances per cluster has also been utilized .', 'Instead of statically merging into one query strategy, dynamic combination may better fit the AL learning process, since different strategies may excel at different AL phases. For example, at the start of AL, uncertainty sampling may be unreliable due to little labeled data, and representativenessbased methods could be preferable, whereas in later stages where we have enough data and target finergrained decision boundaries, uncertainty may be a suitable strategy. DUAL (Donmez et al., 2007) is such a dynamic strategy that can switch from a density-based selector to an uncertainty-based one. Ambati et al. (2011b) further propose GraDUAL, which gradually switches strategies within a switching range. Wu et al. (2017) adopt a similar idea with a pre-defined monotonic function to control the combination weights.']","There is no surprise that informativeness and representativeness can be combined for instance querying, leading to hybrid strategies. A simple combination can be used to merge multiple criteria into one. This can be achieved by a weighted sum (Kim et al., 2006;Chen et al., 2011) or multiplication .

There are several strategies to naturally integrate multiple criteria. Examples include (uncertainty) weighted clustering (Zhdanov, 2019), diverse gradient selection (Ash et al., 2020;Kim, 2020) where the gradients themselves contain uncertainty information ( §2.1.3) and determinantal point processes (DPP) with quality-diversity decomposition .

Moreover, multi-step querying, which applies multiple criteria in series, is another natural hybrid method. For example, one can consider first filtering certain highly uncertain instances and then performing clustering to select a diverse batch from them (Xu et al., 2003;Mirroshandel et al., 2011). An alternative strategy of selecting the most uncertain instances per cluster has also been utilized .

Instead of statically merging into one query strategy, dynamic combination may better fit the AL learning process, since different strategies may excel at different AL phases. For example, at the start of AL, uncertainty sampling may be unreliable due to little labeled data, and representativenessbased methods could be preferable, whereas in later stages where we have enough data and target finergrained decision boundaries, uncertainty may be a suitable strategy. DUAL (Donmez et al., 2007) is such a dynamic strategy that can switch from a density-based selector to an uncertainty-based one. Ambati et al. (2011b) further propose GraDUAL, which gradually switches strategies within a switching range. Wu et al. (2017) adopt a similar idea with a pre-defined monotonic function to control the combination weights.","(p11.0) There is no surprise that informativeness and representativeness can be combined for instance querying, leading to hybrid strategies. A simple combination can be used to merge multiple criteria into one. This can be achieved by a weighted sum (Kim et al., 2006;Chen et al., 2011) or multiplication .

(p11.1) There are several strategies to naturally integrate multiple criteria. Examples include (uncertainty) weighted clustering (Zhdanov, 2019), diverse gradient selection (Ash et al., 2020;Kim, 2020) where the gradients themselves contain uncertainty information ( §2.1.3) and determinantal point processes (DPP) with quality-diversity decomposition .

(p11.2) Moreover, multi-step querying, which applies multiple criteria in series, is another natural hybrid method. For example, one can consider first filtering certain highly uncertain instances and then performing clustering to select a diverse batch from them (Xu et al., 2003;Mirroshandel et al., 2011). An alternative strategy of selecting the most uncertain instances per cluster has also been utilized .

(p11.3) Instead of statically merging into one query strategy, dynamic combination may better fit the AL learning process, since different strategies may excel at different AL phases. For example, at the start of AL, uncertainty sampling may be unreliable due to little labeled data, and representativenessbased methods could be preferable, whereas in later stages where we have enough data and target finergrained decision boundaries, uncertainty may be a suitable strategy. DUAL (Donmez et al., 2007) is such a dynamic strategy that can switch from a density-based selector to an uncertainty-based one. Ambati et al. (2011b) further propose GraDUAL, which gradually switches strategies within a switching range. Wu et al. (2017) adopt a similar idea with a pre-defined monotonic function to control the combination weights.","[['b91'], ['b78', None, 'b91'], ['b60', 'b102'], [None, 'b57']]","[['b91'], ['b78', None, 'b91'], ['b60', 'b102'], [None, 'b57']]",8,"1. There is no surprise that informativeness and representativeness can be combined for instance querying, leading to hybrid strategies.
2. A simple combination can be used to merge multiple criteria into one.
3. This can be achieved by a weighted sum (Kim et al., 2006;Chen et al., 2011) or multiplication .
4. There are several strategies to naturally integrate multiple criteria.
5. Examples include (uncertainty) weighted clustering (Zhdanov, 2019), diverse gradient selection (Ash et al., 2020;Kim, 2020) where the gradients themselves contain uncertainty information ( §2.1.3) and determinantal point processes (DPP) with quality-diversity decomposition .
6. Moreover, multi-step querying, which applies multiple criteria in series, is another natural hybrid method.
7. For example, one can consider first filtering certain highly uncertain instances and then performing clustering to select a diverse batch from them (Xu et al., 2003;Mirroshandel et al., 2011).
8. An alternative strategy of selecting the most uncertain instances per cluster has also been utilized .
9. Instead of statically merging into one query strategy, dynamic combination may better fit the AL learning process, since different strategies may excel at different AL phases.
10. For example, at the start of AL, uncertainty sampling may be unreliable due to little labeled data, and representativenessbased methods could be preferable, whereas in later stages where we have enough data and target finergrained decision boundaries, uncertainty may be a suitable strategy.
11. DUAL (Donmez et al., 2007) is such a dynamic strategy that can switch from a density-based selector to an uncertainty-based one.
12. Ambati et al. (2011b) further propose GraDUAL, which gradually switches strategies within a switching range.
13. Wu et al. (2017) adopt a similar idea with a pre-defined monotonic function to control the combination weights."
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s14,Full-structure AL,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4']","['First, if we regard the full output structure of an instance as a whole and perform query and annotation at the full-instance level, then AL for structured prediction tasks is not very different than for simpler classification tasks. Nevertheless, considering that the output space is usually exponentially large and infeasible to explicitly enumerate, querying may require further inspection.', 'Some uncertainty sampling strategies, such as entropy, need to consider the full output space. Instead of the infeasible explicit enumeration, dynamic-programming algorithms that are similar to the ones in decoding and inference processes can be utilized, such as algorithms for tree-entropy (Hwa, 2000(Hwa, , 2004 and sequence-entropy (Mann and McCallum, 2007;. Instead of considering the full output space, topk approximation is a simpler alternative that takes k-best predicted structures as a proxy. This is also a frequently utilized method Kim et al., 2006;Rocha and Sanchez, 2013).', 'For disagreement-based strategies, the measurement of partial disagreement may be required, since full-match can be too strict for structured objects. Fine-grained evaluation scores can be reasonable choices for this purpose, such as F1 score for sequence labeling (Ngai and Yarowsky, 2000).', 'Since longer instances usually have larger uncertainties and might be preferred, length normalization is a commonly-used heuristic to avoid this bias Hwa, 2000Hwa, , 2004). Yet,  argue that longer sequences should not be discouraged and may contain more information.', 'Instead of directly specifying the full utility of an instance, aggregation is also often utilized by gathering utilities of its sub-structures, usually along the factorization of the structured modeling. For example, the sequence uncertainty can be obtained by summing or averaging the uncertainties of all the tokens . Other aggregation methods are also applicable, such as weighted sum by word frequency  or using only the most uncertain (least probable) one (Myers and Palmer, 2021;Liu et al., 2022).']","First, if we regard the full output structure of an instance as a whole and perform query and annotation at the full-instance level, then AL for structured prediction tasks is not very different than for simpler classification tasks. Nevertheless, considering that the output space is usually exponentially large and infeasible to explicitly enumerate, querying may require further inspection.

Some uncertainty sampling strategies, such as entropy, need to consider the full output space. Instead of the infeasible explicit enumeration, dynamic-programming algorithms that are similar to the ones in decoding and inference processes can be utilized, such as algorithms for tree-entropy (Hwa, 2000(Hwa, , 2004 and sequence-entropy (Mann and McCallum, 2007;. Instead of considering the full output space, topk approximation is a simpler alternative that takes k-best predicted structures as a proxy. This is also a frequently utilized method Kim et al., 2006;Rocha and Sanchez, 2013).

For disagreement-based strategies, the measurement of partial disagreement may be required, since full-match can be too strict for structured objects. Fine-grained evaluation scores can be reasonable choices for this purpose, such as F1 score for sequence labeling (Ngai and Yarowsky, 2000).

Since longer instances usually have larger uncertainties and might be preferred, length normalization is a commonly-used heuristic to avoid this bias Hwa, 2000Hwa, , 2004). Yet,  argue that longer sequences should not be discouraged and may contain more information.

Instead of directly specifying the full utility of an instance, aggregation is also often utilized by gathering utilities of its sub-structures, usually along the factorization of the structured modeling. For example, the sequence uncertainty can be obtained by summing or averaging the uncertainties of all the tokens . Other aggregation methods are also applicable, such as weighted sum by word frequency  or using only the most uncertain (least probable) one (Myers and Palmer, 2021;Liu et al., 2022).","(p14.0) First, if we regard the full output structure of an instance as a whole and perform query and annotation at the full-instance level, then AL for structured prediction tasks is not very different than for simpler classification tasks. Nevertheless, considering that the output space is usually exponentially large and infeasible to explicitly enumerate, querying may require further inspection.

(p14.1) Some uncertainty sampling strategies, such as entropy, need to consider the full output space. Instead of the infeasible explicit enumeration, dynamic-programming algorithms that are similar to the ones in decoding and inference processes can be utilized, such as algorithms for tree-entropy (Hwa, 2000(Hwa, , 2004 and sequence-entropy (Mann and McCallum, 2007;. Instead of considering the full output space, topk approximation is a simpler alternative that takes k-best predicted structures as a proxy. This is also a frequently utilized method Kim et al., 2006;Rocha and Sanchez, 2013).

(p14.2) For disagreement-based strategies, the measurement of partial disagreement may be required, since full-match can be too strict for structured objects. Fine-grained evaluation scores can be reasonable choices for this purpose, such as F1 score for sequence labeling (Ngai and Yarowsky, 2000).

(p14.3) Since longer instances usually have larger uncertainties and might be preferred, length normalization is a commonly-used heuristic to avoid this bias Hwa, 2000Hwa, , 2004). Yet,  argue that longer sequences should not be discouraged and may contain more information.

(p14.4) Instead of directly specifying the full utility of an instance, aggregation is also often utilized by gathering utilities of its sub-structures, usually along the factorization of the structured modeling. For example, the sequence uncertainty can be obtained by summing or averaging the uncertainties of all the tokens . Other aggregation methods are also applicable, such as weighted sum by word frequency  or using only the most uncertain (least probable) one (Myers and Palmer, 2021;Liu et al., 2022).","[[], [None, 'b104', 'b91'], [None], [None], [None, 'b99']]","[[], [None, 'b104', 'b91'], [None], [None], [None, 'b99']]",7,"1. First, if we regard the full output structure of an instance as a whole and perform query and annotation at the full-instance level, then AL for structured prediction tasks is not very different than for simpler classification tasks.
2. Nevertheless, considering that the output space is usually exponentially large and infeasible to explicitly enumerate, querying may require further inspection.
3. Some uncertainty sampling strategies, such as entropy, need to consider the full output space.
4. Instead of the infeasible explicit enumeration, dynamic-programming algorithms that are similar to the ones in decoding and inference processes can be utilized, such as algorithms for tree-entropy (Hwa, 2000(Hwa, , 2004 and sequence-entropy (Mann and McCallum, 2007;. Instead of considering the full output space, topk approximation is a simpler alternative that takes k-best predicted structures as a proxy. This is also a frequently utilized method Kim et al., 2006;Rocha and Sanchez, 2013).
5. For disagreement-based strategies, the measurement of partial disagreement may be required, since full-match can be too strict for structured objects.
6. Fine-grained evaluation scores can be reasonable choices for this purpose, such as F1 score for sequence labeling (Ngai and Yarowsky, 2000).
7. Since longer instances usually have larger uncertainties and might be preferred, length normalization is a commonly-used heuristic to avoid this bias Hwa, 2000Hwa, , 2004).
8. Yet,  argue that longer sequences should not be discouraged and may contain more information.
9. Instead of directly specifying the full utility of an instance, aggregation is also often utilized by gathering utilities of its sub-structures, usually along the factorization of the structured modeling.
10. For example, the sequence uncertainty can be obtained by summing or averaging the uncertainties of all the tokens .
11. Other aggregation methods are also applicable, such as weighted sum by word frequency  or using only the most uncertain (least probable) one (Myers and Palmer, 2021;Liu et al., 2022)."
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s15,Partial-structure AL,"['p15.0', 'p15.1', 'p15.2', 'p15.3', 'p15.4']","['A structured object can be decomposed into smaller sub-structures with different training utilities. For example, in a dependency tree, functional relations are usually easier to judge while prepositional attachment links may be more informative for the learning purpose. This naturally leads to AL with partial structures, where querying and annotating can be performed at the sub-structure level.', 'Factorizing full structures into the finestgrained sub-structures and regarding them as the annotation units could be a natural choice. Typical examples include individual tokens for sequence labeling , word boundaries for segmentation (Neubig et al., 2011;Li et al., 2012b), syntactic-unit pairs for dependency parsing (Sassano and Kurohashi, 2010) and mention pairs for coreference (Gasperin, 2009;Miller et al., 2012;Sachan et al., 2015). The querying strategy for the sub-structures can be similar to the classification cases, though inferences are usually needed to calculate marginal probabilities. Moreover, if full structures are desired as annotation outputs, semi-supervised techniques such as self-training ( §4.2) could be utilized to assign pseudo labels to the unannotated parts Majidi and Crane, 2013).', 'At many times, choosing larger sub-structures is preferable, since partial annotation still needs the understanding of larger contexts and frequently jumping among different contexts may require more reading time ( §3.2.1). Moreover, increasing the sampling granularity may mitigate the missed class effect, where certain classes may be overlooked . Typical examples of larger sub-structures include sub-sequences for sequence labeling Chaudhary et al., 2019;, word-wise head edges for dependency parsing (Flannery and Mori, 2015;Li et al., 2016), neighborhood pools (Laws et al., 2012) or mention-wise anaphoric links Espeland et al., 2020) for coreference, and phrases for MT (Bloodgood and Callison-Burch, 2010;Miura et al., 2016;Hu and Neubig, 2021). In addition to increasing granularity, grouping queries can also help to make annotation easier, such as adopting a two-stage selection of choosing uncertain tokens from uncertain sentences (Mirroshandel and Nasr, 2011;Flannery and Mori, 2015) and selecting nearby instances in a row (Miller et al., 2012).', 'For AL with partial structures, output modeling is of particular interest since the model needs to learn from partial annotations. If directly using local discriminative models where each substructure is decided independently, learning with partial annotations is straightforward since the annotations are already complete to the models (Neubig et al., 2011;Flannery and Mori, 2015). For more complex models that consider interactions among output sub-structures, such as global models, special algorithms are required to learn from incomplete annotations (Scheffer et al., 2001;Wanvarie et al., 2011;Li et al., 2016). One advantage of these more complex models is the interaction of the partial labels and the remaining parts. For example, considering the output constraints for structured prediction tasks, combining the annotated parts and the constraints may reduce the output space of other parts and thus lower their uncertainties, leading to better queries (Roth and Small, 2006;Sassano and Kurohashi, 2010;Mirroshandel and Nasr, 2011). More generally, the annotation of one label can intermediately influence others with cheap re-inference, which can help batch-mode selection (Marcheggiani and Artières, 2014) and interactive correction (Culotta and McCallum, 2005).', 'In addition to classical structured-prediction tasks, classification tasks can also be cast as structured predictions with partial labeling. Partial feedback is an example that is adopted to make the annotating of classification tasks simpler, especially when there are a large number of target labels. For example, annotators may find it much easier to answer yes/no questions (Hu et al., 2019) or rule out negative classes (Lippincott and Van Durme, 2021) than to identify the correct one.']","A structured object can be decomposed into smaller sub-structures with different training utilities. For example, in a dependency tree, functional relations are usually easier to judge while prepositional attachment links may be more informative for the learning purpose. This naturally leads to AL with partial structures, where querying and annotating can be performed at the sub-structure level.

Factorizing full structures into the finestgrained sub-structures and regarding them as the annotation units could be a natural choice. Typical examples include individual tokens for sequence labeling , word boundaries for segmentation (Neubig et al., 2011;Li et al., 2012b), syntactic-unit pairs for dependency parsing (Sassano and Kurohashi, 2010) and mention pairs for coreference (Gasperin, 2009;Miller et al., 2012;Sachan et al., 2015). The querying strategy for the sub-structures can be similar to the classification cases, though inferences are usually needed to calculate marginal probabilities. Moreover, if full structures are desired as annotation outputs, semi-supervised techniques such as self-training ( §4.2) could be utilized to assign pseudo labels to the unannotated parts Majidi and Crane, 2013).

At many times, choosing larger sub-structures is preferable, since partial annotation still needs the understanding of larger contexts and frequently jumping among different contexts may require more reading time ( §3.2.1). Moreover, increasing the sampling granularity may mitigate the missed class effect, where certain classes may be overlooked . Typical examples of larger sub-structures include sub-sequences for sequence labeling Chaudhary et al., 2019;, word-wise head edges for dependency parsing (Flannery and Mori, 2015;Li et al., 2016), neighborhood pools (Laws et al., 2012) or mention-wise anaphoric links Espeland et al., 2020) for coreference, and phrases for MT (Bloodgood and Callison-Burch, 2010;Miura et al., 2016;Hu and Neubig, 2021). In addition to increasing granularity, grouping queries can also help to make annotation easier, such as adopting a two-stage selection of choosing uncertain tokens from uncertain sentences (Mirroshandel and Nasr, 2011;Flannery and Mori, 2015) and selecting nearby instances in a row (Miller et al., 2012).

For AL with partial structures, output modeling is of particular interest since the model needs to learn from partial annotations. If directly using local discriminative models where each substructure is decided independently, learning with partial annotations is straightforward since the annotations are already complete to the models (Neubig et al., 2011;Flannery and Mori, 2015). For more complex models that consider interactions among output sub-structures, such as global models, special algorithms are required to learn from incomplete annotations (Scheffer et al., 2001;Wanvarie et al., 2011;Li et al., 2016). One advantage of these more complex models is the interaction of the partial labels and the remaining parts. For example, considering the output constraints for structured prediction tasks, combining the annotated parts and the constraints may reduce the output space of other parts and thus lower their uncertainties, leading to better queries (Roth and Small, 2006;Sassano and Kurohashi, 2010;Mirroshandel and Nasr, 2011). More generally, the annotation of one label can intermediately influence others with cheap re-inference, which can help batch-mode selection (Marcheggiani and Artières, 2014) and interactive correction (Culotta and McCallum, 2005).

In addition to classical structured-prediction tasks, classification tasks can also be cast as structured predictions with partial labeling. Partial feedback is an example that is adopted to make the annotating of classification tasks simpler, especially when there are a large number of target labels. For example, annotators may find it much easier to answer yes/no questions (Hu et al., 2019) or rule out negative classes (Lippincott and Van Durme, 2021) than to identify the correct one.","(p15.0) A structured object can be decomposed into smaller sub-structures with different training utilities. For example, in a dependency tree, functional relations are usually easier to judge while prepositional attachment links may be more informative for the learning purpose. This naturally leads to AL with partial structures, where querying and annotating can be performed at the sub-structure level.

(p15.1) Factorizing full structures into the finestgrained sub-structures and regarding them as the annotation units could be a natural choice. Typical examples include individual tokens for sequence labeling , word boundaries for segmentation (Neubig et al., 2011;Li et al., 2012b), syntactic-unit pairs for dependency parsing (Sassano and Kurohashi, 2010) and mention pairs for coreference (Gasperin, 2009;Miller et al., 2012;Sachan et al., 2015). The querying strategy for the sub-structures can be similar to the classification cases, though inferences are usually needed to calculate marginal probabilities. Moreover, if full structures are desired as annotation outputs, semi-supervised techniques such as self-training ( §4.2) could be utilized to assign pseudo labels to the unannotated parts Majidi and Crane, 2013).

(p15.2) At many times, choosing larger sub-structures is preferable, since partial annotation still needs the understanding of larger contexts and frequently jumping among different contexts may require more reading time ( §3.2.1). Moreover, increasing the sampling granularity may mitigate the missed class effect, where certain classes may be overlooked . Typical examples of larger sub-structures include sub-sequences for sequence labeling Chaudhary et al., 2019;, word-wise head edges for dependency parsing (Flannery and Mori, 2015;Li et al., 2016), neighborhood pools (Laws et al., 2012) or mention-wise anaphoric links Espeland et al., 2020) for coreference, and phrases for MT (Bloodgood and Callison-Burch, 2010;Miura et al., 2016;Hu and Neubig, 2021). In addition to increasing granularity, grouping queries can also help to make annotation easier, such as adopting a two-stage selection of choosing uncertain tokens from uncertain sentences (Mirroshandel and Nasr, 2011;Flannery and Mori, 2015) and selecting nearby instances in a row (Miller et al., 2012).

(p15.3) For AL with partial structures, output modeling is of particular interest since the model needs to learn from partial annotations. If directly using local discriminative models where each substructure is decided independently, learning with partial annotations is straightforward since the annotations are already complete to the models (Neubig et al., 2011;Flannery and Mori, 2015). For more complex models that consider interactions among output sub-structures, such as global models, special algorithms are required to learn from incomplete annotations (Scheffer et al., 2001;Wanvarie et al., 2011;Li et al., 2016). One advantage of these more complex models is the interaction of the partial labels and the remaining parts. For example, considering the output constraints for structured prediction tasks, combining the annotated parts and the constraints may reduce the output space of other parts and thus lower their uncertainties, leading to better queries (Roth and Small, 2006;Sassano and Kurohashi, 2010;Mirroshandel and Nasr, 2011). More generally, the annotation of one label can intermediately influence others with cheap re-inference, which can help batch-mode selection (Marcheggiani and Artières, 2014) and interactive correction (Culotta and McCallum, 2005).

(p15.4) In addition to classical structured-prediction tasks, classification tasks can also be cast as structured predictions with partial labeling. Partial feedback is an example that is adopted to make the annotating of classification tasks simpler, especially when there are a large number of target labels. For example, annotators may find it much easier to answer yes/no questions (Hu et al., 2019) or rule out negative classes (Lippincott and Van Durme, 2021) than to identify the correct one.","[[], [None, 'b100', 'b101', 'b91'], ['b109', 'b100', 'b101', 'b95', None, 'b91', 'b106'], ['b56', 'b95', None, 'b10', 'b91'], []]","[[], [None, 'b100', 'b101', 'b91'], ['b109', 'b100', 'b101', 'b95', None, 'b91', 'b106'], ['b56', 'b95', None, 'b10', 'b91'], []]",16,"1. A structured object can be decomposed into smaller sub-structures with different training utilities.
2. For example, in a dependency tree, functional relations are usually easier to judge while prepositional attachment links may be more informative for the learning purpose.
3. This naturally leads to AL with partial structures, where querying and annotating can be performed at the sub-structure level.
4. Factorizing full structures into the finestgrained sub-structures and regarding them as the annotation units could be a natural choice.
5. Typical examples include individual tokens for sequence labeling , word boundaries for segmentation (Neubig et al., 2011;Li et al., 2012b), syntactic-unit pairs for dependency parsing (Sassano and Kurohashi, 2010) and mention pairs for coreference (Gasperin, 2009;Miller et al., 2012;Sachan et al., 2015).
6. The querying strategy for the sub-structures can be similar to the classification cases, though inferences are usually needed to calculate marginal probabilities.
7. Moreover, if full structures are desired as annotation outputs, semi-supervised techniques such as self-training ( §4.2) could be utilized to assign pseudo labels to the unannotated parts Majidi and Crane, 2013).
8. At many times, choosing larger sub-structures is preferable, since partial annotation still needs the understanding of larger contexts and frequently jumping among different contexts may require more reading time ( §3.2.1).
9. Moreover, increasing the sampling granularity may mitigate the missed class effect, where certain classes may be overlooked .
10. Typical examples of larger sub-structures include sub-sequences for sequence labeling Chaudhary et al., 2019;, word-wise head edges for dependency parsing (Flannery and Mori, 2015;Li et al., 2016), neighborhood pools (Laws et al., 2012) or mention-wise anaphoric links Espeland et al., 2020) for coreference, and phrases for MT (Bloodgood and Callison-Burch, 2010;Miura et al., 2016;Hu and Neubig, 2021).
11. In addition to increasing granularity, grouping queries can also help to make annotation easier, such as adopting a two-stage selection of choosing uncertain tokens from uncertain sentences (Mirroshandel and Nasr, 2011;Flannery and Mori, 2015) and selecting nearby instances in a row (Miller et al., 2012).
12. For AL with partial structures, output modeling is of particular interest since the model needs to learn from partial annotations.
13. If directly using local discriminative models where each substructure is decided independently, learning with partial annotations is straightforward since the annotations are already complete to the models (Neubig et al., 2011;Flannery and Mori, 2015).
14. For more complex models that consider interactions among output sub-structures, such as global models, special algorithms are required to learn from incomplete annotations (Scheffer et al., 2001;Wanvarie et al., 2011;Li et al., 2016).
15. One advantage of these more complex models is the interaction of the partial labels and the remaining parts.
16. For example, considering the output constraints for structured prediction tasks, combining the annotated parts and the constraints may reduce the output space of other parts and thus lower their uncertainties, leading to better queries (Roth and Small, 2006;Sassano and Kurohashi, 2010;Mirroshandel and Nasr, 2011).
17. More generally, the annotation of one label can intermediately influence others with cheap re-inference, which can help batch-mode selection (Marcheggiani and Artières, 2014) and interactive correction (Culotta and McCallum, 2005).
18. In addition to classical structured-prediction tasks, classification tasks can also be cast as structured predictions with partial labeling.
19. Partial feedback is an example that is adopted to make the annotating of classification tasks simpler, especially when there are a large number of target labels.
20. For example, annotators may find it much easier to answer yes/no questions (Hu et al., 2019) or rule out negative classes (Lippincott and Van Durme, 2021) than to identify the correct one."
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s25,Starting AL,"['p25.0', 'p25.1']","['While there are cases where there are already enough labeled data to train a reasonable model and AL is utilized to provide further improvements (Bloodgood and Callison-Burch, 2010;Geifman and El-Yaniv, 2017), at many times we are facing the cold-start problem, where instances need to be selected without a reasonable model. Especially, how to select the seed data to start the AL process is an interesting question, which may greatly influence the performance in initial AL stages Horbach and Palmer, 2016).', 'Random sampling is probably the most commonly utilized strategy, which is reasonable since it preserves the original data distribution. Some representativeness-based querying strategies ( §2.2) can also be utilized, for example, selecting points near the clustering centroids is a way to obtain representative and diverse seeds (Kang et al., 2004;Hu et al., 2010). Moreover, some advanced learning techniques ( §4.2) can also be helpful here, such as transfer learning (Wu et al., 2017) and unsupervised methods (Vlachos, 2006;Dasgupta and Ng, 2009). In addition, language model can be a useful tool, with which Dligach and Palmer (2011) select low-probability words in the context of word sense disambiguation and  choose cluster centers with surprisal embeddings by pre-trained contextualized LMs.']","While there are cases where there are already enough labeled data to train a reasonable model and AL is utilized to provide further improvements (Bloodgood and Callison-Burch, 2010;Geifman and El-Yaniv, 2017), at many times we are facing the cold-start problem, where instances need to be selected without a reasonable model. Especially, how to select the seed data to start the AL process is an interesting question, which may greatly influence the performance in initial AL stages Horbach and Palmer, 2016).

Random sampling is probably the most commonly utilized strategy, which is reasonable since it preserves the original data distribution. Some representativeness-based querying strategies ( §2.2) can also be utilized, for example, selecting points near the clustering centroids is a way to obtain representative and diverse seeds (Kang et al., 2004;Hu et al., 2010). Moreover, some advanced learning techniques ( §4.2) can also be helpful here, such as transfer learning (Wu et al., 2017) and unsupervised methods (Vlachos, 2006;Dasgupta and Ng, 2009). In addition, language model can be a useful tool, with which Dligach and Palmer (2011) select low-probability words in the context of word sense disambiguation and  choose cluster centers with surprisal embeddings by pre-trained contextualized LMs.","(p25.0) While there are cases where there are already enough labeled data to train a reasonable model and AL is utilized to provide further improvements (Bloodgood and Callison-Burch, 2010;Geifman and El-Yaniv, 2017), at many times we are facing the cold-start problem, where instances need to be selected without a reasonable model. Especially, how to select the seed data to start the AL process is an interesting question, which may greatly influence the performance in initial AL stages Horbach and Palmer, 2016).

(p25.1) Random sampling is probably the most commonly utilized strategy, which is reasonable since it preserves the original data distribution. Some representativeness-based querying strategies ( §2.2) can also be utilized, for example, selecting points near the clustering centroids is a way to obtain representative and diverse seeds (Kang et al., 2004;Hu et al., 2010). Moreover, some advanced learning techniques ( §4.2) can also be helpful here, such as transfer learning (Wu et al., 2017) and unsupervised methods (Vlachos, 2006;Dasgupta and Ng, 2009). In addition, language model can be a useful tool, with which Dligach and Palmer (2011) select low-probability words in the context of word sense disambiguation and  choose cluster centers with surprisal embeddings by pre-trained contextualized LMs.","[[None, 'b106'], [None, 'b57', 'b51']]","[[None, 'b106'], [None, 'b57', 'b51']]",5,"1. While there are cases where there are already enough labeled data to train a reasonable model and AL is utilized to provide further improvements (Bloodgood and Callison-Burch, 2010;Geifman and El-Yaniv, 2017), at many times we are facing the cold-start problem, where instances need to be selected without a reasonable model.
2. Especially, how to select the seed data to start the AL process is an interesting question, which may greatly influence the performance in initial AL stages Horbach and Palmer, 2016).
3. Random sampling is probably the most commonly utilized strategy, which is reasonable since it preserves the original data distribution.
4. Some representativeness-based querying strategies ( §2.2) can also be utilized, for example, selecting points near the clustering centroids is a way to obtain representative and diverse seeds (Kang et al., 2004;Hu et al., 2010).
5. Moreover, some advanced learning techniques ( §4.2) can also be helpful here, such as transfer learning (Wu et al., 2017) and unsupervised methods (Vlachos, 2006;Dasgupta and Ng, 2009).
6. In addition, language model can be a useful tool, with which Dligach and Palmer (2011) select low-probability words in the context of word sense disambiguation and  choose cluster centers with surprisal embeddings by pre-trained contextualized LMs."
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s17,Cost Measurement,"['p17.0', 'p17.1', 'p17.2']","['Most AL works adopt simple measurements of unit cost, that is, assuming that annotating each instance requires the same cost. Nevertheless, the annotation efforts for different instances may vary . For example, longer sentences may cost more to annotate than shorter ones. Because of this, many works assume unit costs to tokens instead of sequences, which may still be inaccurate. Especially, AL tends to select difficult and ambiguous instances, which may require more annotation efforts (Hachey et al., 2005;Lynn et al., 2012). It is important to properly measure annotation cost since the measurement directly affects the evaluation of AL algorithms. The comparisons of query strategies may vary if adopting different cost measurement (Haertel et al., 2008a;Bloodgood and Callison-Burch, 2010;Chen et al., 2015).', 'Probably the best cost measurement is the actual annotation time (Baldridge and Palmer, 2009). Especially, when the cost comparisons are not that straightforward, such as comparing annotating data against writing rules (Ngai and Yarowsky, 2000) or partial against full annotations ( §3.1; Flannery and Mori, 2015;Li et al., 2016, time-based evaluation is an ideal choice. This requires actual annotating exercises rather than simulations.', 'Since cost measurement can also be used for querying ( §3.2.2), it would be helpful to be able to predict the real cost before annotating. This can be cast as a regression problem, for which several works learn a linear cost model based on input features Ringger et al., 2008;Haertel et al., 2008a;Arora et al., 2009).']","Most AL works adopt simple measurements of unit cost, that is, assuming that annotating each instance requires the same cost. Nevertheless, the annotation efforts for different instances may vary . For example, longer sentences may cost more to annotate than shorter ones. Because of this, many works assume unit costs to tokens instead of sequences, which may still be inaccurate. Especially, AL tends to select difficult and ambiguous instances, which may require more annotation efforts (Hachey et al., 2005;Lynn et al., 2012). It is important to properly measure annotation cost since the measurement directly affects the evaluation of AL algorithms. The comparisons of query strategies may vary if adopting different cost measurement (Haertel et al., 2008a;Bloodgood and Callison-Burch, 2010;Chen et al., 2015).

Probably the best cost measurement is the actual annotation time (Baldridge and Palmer, 2009). Especially, when the cost comparisons are not that straightforward, such as comparing annotating data against writing rules (Ngai and Yarowsky, 2000) or partial against full annotations ( §3.1; Flannery and Mori, 2015;Li et al., 2016, time-based evaluation is an ideal choice. This requires actual annotating exercises rather than simulations.

Since cost measurement can also be used for querying ( §3.2.2), it would be helpful to be able to predict the real cost before annotating. This can be cast as a regression problem, for which several works learn a linear cost model based on input features Ringger et al., 2008;Haertel et al., 2008a;Arora et al., 2009).","(p17.0) Most AL works adopt simple measurements of unit cost, that is, assuming that annotating each instance requires the same cost. Nevertheless, the annotation efforts for different instances may vary . For example, longer sentences may cost more to annotate than shorter ones. Because of this, many works assume unit costs to tokens instead of sequences, which may still be inaccurate. Especially, AL tends to select difficult and ambiguous instances, which may require more annotation efforts (Hachey et al., 2005;Lynn et al., 2012). It is important to properly measure annotation cost since the measurement directly affects the evaluation of AL algorithms. The comparisons of query strategies may vary if adopting different cost measurement (Haertel et al., 2008a;Bloodgood and Callison-Burch, 2010;Chen et al., 2015).

(p17.1) Probably the best cost measurement is the actual annotation time (Baldridge and Palmer, 2009). Especially, when the cost comparisons are not that straightforward, such as comparing annotating data against writing rules (Ngai and Yarowsky, 2000) or partial against full annotations ( §3.1; Flannery and Mori, 2015;Li et al., 2016, time-based evaluation is an ideal choice. This requires actual annotating exercises rather than simulations.

(p17.2) Since cost measurement can also be used for querying ( §3.2.2), it would be helpful to be able to predict the real cost before annotating. This can be cast as a regression problem, for which several works learn a linear cost model based on input features Ringger et al., 2008;Haertel et al., 2008a;Arora et al., 2009).","[['b89', None, 'b106', 'b91'], [None, 'b95'], ['b89', None, 'b8']]","[['b89', None, 'b106', 'b91'], [None, 'b95'], ['b89', None, 'b8']]",9,"1. Most AL works adopt simple measurements of unit cost, that is, assuming that annotating each instance requires the same cost.
2. Nevertheless, the annotation efforts for different instances may vary .
3. For example, longer sentences may cost more to annotate than shorter ones.
4. Because of this, many works assume unit costs to tokens instead of sequences, which may still be inaccurate.
5. Especially, AL tends to select difficult and ambiguous instances, which may require more annotation efforts (Hachey et al., 2005;Lynn et al., 2012).
6. It is important to properly measure annotation cost since the measurement directly affects the evaluation of AL algorithms.
7. The comparisons of query strategies may vary if adopting different cost measurement (Haertel et al., 2008a;Bloodgood and Callison-Burch, 2010;Chen et al., 2015).
8. Probably the best cost measurement is the actual annotation time (Baldridge and Palmer, 2009).
9. Especially, when the cost comparisons are not that straightforward, such as comparing annotating data against writing rules (Ngai and Yarowsky, 2000) or partial against full annotations ( §3.1; Flannery and Mori, 2015;Li et al., 2016, time-based evaluation is an ideal choice.
10. This requires actual annotating exercises rather than simulations.
11. Since cost measurement can also be used for querying ( §3.2.2), it would be helpful to be able to predict the real cost before annotating.
12. This can be cast as a regression problem, for which several works learn a linear cost model based on input features Ringger et al., 2008;Haertel et al., 2008a;Arora et al., 2009)."
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s23,Learning,"['p23.0', 'p23.1', 'p23.2', 'p23.3', 'p23.4']","['AL can be combined with other advanced learning techniques to further reduce required annotations.', 'Semi-supervised learning. Since AL usually assumes an unlabeled pool, semi-supervised learning can be a natural fit. Combining these two is not a new idea: (McCallum and Nigam, 1998) adopt the EM algorithm to estimate the outputs of unlabeled data and utilize them for learning. This type of self-training or pseudo-labeling technique is often utilized in AL Majidi and Crane, 2013;Yu et al., 2022). With a similar motivation, (Dasgupta and Ng, 2009) use an unsupervised algorithm to identify the unambiguous instances to train an active learner. For the task of word alignment, which can be learned in an unsupervised manner, incorporating supervision with AL can bring further improvements in a data-efficient way (Ambati et al., 2010b,c).', 'Transfer learning. AL can be easily combined with transfer learning, another technique to reduce required annotations. Utilizing pre-trained models is already a good example (Ein-Dor et al., 2020;Tamkin et al., 2022) and continual training (Gururangan et al., 2020) can also be applied (Hua and Wang, 2022;Margatina et al., 2022). Moreover, transductive learning is commonly combined with AL by transferring learning signals from different domains (Chan and Ng, 2007;Shi et al., 2008;Rai et al., 2010;Saha et al., 2011;Wu et al., 2017;Kasai et al., 2019; or languages (Qian et al., 2014;Fang and Cohn, 2017;Fang et al., 2017;Chaudhary et al., 2019Moniz et al., 2022). In addition to the task model, the model-based query policy ( §2.1.4) is also often obtained with transfer learning.', 'Weak supervision. AL can also be combined with weakly supervised learning. Examples include learning from inputs and execution results for semantic parsing (Ni et al., 2020), labeling based on identical structure vectors for entity representations (Qian et al., 2020), learning from gazetteers and dictionaries for sequence labeling  and interactively discovering labeling rules .', 'Data augmentation. Augmentation is also applicable in AL and has been explored with iterative back-translation , mixup for sequence labeling  and phraseto-sentence augmentation for MT (Hu and Neubig, 2021). As discussed in §2.1.1, augmentation can also be helpful for instance querying (Jiang et al., 2020;Zhang et al., 2022b). Another interesting scenario involving augmentation and AL is query synthesis, which directly generates instances to be annotated instead of selecting existing unlabeled ones. Though synthesizing texts is still a hard problem generally, there have been successful applications for simple classification tasks (Schumann and Rehbein, 2019;Quteineh et al., 2020).']","AL can be combined with other advanced learning techniques to further reduce required annotations.

Semi-supervised learning. Since AL usually assumes an unlabeled pool, semi-supervised learning can be a natural fit. Combining these two is not a new idea: (McCallum and Nigam, 1998) adopt the EM algorithm to estimate the outputs of unlabeled data and utilize them for learning. This type of self-training or pseudo-labeling technique is often utilized in AL Majidi and Crane, 2013;Yu et al., 2022). With a similar motivation, (Dasgupta and Ng, 2009) use an unsupervised algorithm to identify the unambiguous instances to train an active learner. For the task of word alignment, which can be learned in an unsupervised manner, incorporating supervision with AL can bring further improvements in a data-efficient way (Ambati et al., 2010b,c).

Transfer learning. AL can be easily combined with transfer learning, another technique to reduce required annotations. Utilizing pre-trained models is already a good example (Ein-Dor et al., 2020;Tamkin et al., 2022) and continual training (Gururangan et al., 2020) can also be applied (Hua and Wang, 2022;Margatina et al., 2022). Moreover, transductive learning is commonly combined with AL by transferring learning signals from different domains (Chan and Ng, 2007;Shi et al., 2008;Rai et al., 2010;Saha et al., 2011;Wu et al., 2017;Kasai et al., 2019; or languages (Qian et al., 2014;Fang and Cohn, 2017;Fang et al., 2017;Chaudhary et al., 2019Moniz et al., 2022). In addition to the task model, the model-based query policy ( §2.1.4) is also often obtained with transfer learning.

Weak supervision. AL can also be combined with weakly supervised learning. Examples include learning from inputs and execution results for semantic parsing (Ni et al., 2020), labeling based on identical structure vectors for entity representations (Qian et al., 2020), learning from gazetteers and dictionaries for sequence labeling  and interactively discovering labeling rules .

Data augmentation. Augmentation is also applicable in AL and has been explored with iterative back-translation , mixup for sequence labeling  and phraseto-sentence augmentation for MT (Hu and Neubig, 2021). As discussed in §2.1.1, augmentation can also be helpful for instance querying (Jiang et al., 2020;Zhang et al., 2022b). Another interesting scenario involving augmentation and AL is query synthesis, which directly generates instances to be annotated instead of selecting existing unlabeled ones. Though synthesizing texts is still a hard problem generally, there have been successful applications for simple classification tasks (Schumann and Rehbein, 2019;Quteineh et al., 2020).","(p23.0) AL can be combined with other advanced learning techniques to further reduce required annotations.

(p23.1) Semi-supervised learning. Since AL usually assumes an unlabeled pool, semi-supervised learning can be a natural fit. Combining these two is not a new idea: (McCallum and Nigam, 1998) adopt the EM algorithm to estimate the outputs of unlabeled data and utilize them for learning. This type of self-training or pseudo-labeling technique is often utilized in AL Majidi and Crane, 2013;Yu et al., 2022). With a similar motivation, (Dasgupta and Ng, 2009) use an unsupervised algorithm to identify the unambiguous instances to train an active learner. For the task of word alignment, which can be learned in an unsupervised manner, incorporating supervision with AL can bring further improvements in a data-efficient way (Ambati et al., 2010b,c).

(p23.2) Transfer learning. AL can be easily combined with transfer learning, another technique to reduce required annotations. Utilizing pre-trained models is already a good example (Ein-Dor et al., 2020;Tamkin et al., 2022) and continual training (Gururangan et al., 2020) can also be applied (Hua and Wang, 2022;Margatina et al., 2022). Moreover, transductive learning is commonly combined with AL by transferring learning signals from different domains (Chan and Ng, 2007;Shi et al., 2008;Rai et al., 2010;Saha et al., 2011;Wu et al., 2017;Kasai et al., 2019; or languages (Qian et al., 2014;Fang and Cohn, 2017;Fang et al., 2017;Chaudhary et al., 2019Moniz et al., 2022). In addition to the task model, the model-based query policy ( §2.1.4) is also often obtained with transfer learning.

(p23.3) Weak supervision. AL can also be combined with weakly supervised learning. Examples include learning from inputs and execution results for semantic parsing (Ni et al., 2020), labeling based on identical structure vectors for entity representations (Qian et al., 2020), learning from gazetteers and dictionaries for sequence labeling  and interactively discovering labeling rules .

(p23.4) Data augmentation. Augmentation is also applicable in AL and has been explored with iterative back-translation , mixup for sequence labeling  and phraseto-sentence augmentation for MT (Hu and Neubig, 2021). As discussed in §2.1.1, augmentation can also be helpful for instance querying (Jiang et al., 2020;Zhang et al., 2022b). Another interesting scenario involving augmentation and AL is query synthesis, which directly generates instances to be annotated instead of selecting existing unlabeled ones. Though synthesizing texts is still a hard problem generally, there have been successful applications for simple classification tasks (Schumann and Rehbein, 2019;Quteineh et al., 2020).","[[], ['b63', None], ['b57', 'b32', 'b39', None, 'b91', 'b67', 'b2'], ['b112'], ['b72', 'b0', None, 'b15']]","[[], ['b63', None], ['b57', 'b32', 'b39', None, 'b91', 'b67', 'b2'], ['b112'], ['b72', 'b0', None, 'b15']]",14,"1. AL can be combined with other advanced learning techniques to further reduce required annotations.
2. Semi-supervised learning. Since AL usually assumes an unlabeled pool, semi-supervised learning can be a natural fit.
3. Combining these two is not a new idea: (McCallum and Nigam, 1998) adopt the EM algorithm to estimate the outputs of unlabeled data and utilize them for learning.
4. This type of self-training or pseudo-labeling technique is often utilized in AL Majidi and Crane, 2013;Yu et al., 2022).
5. With a similar motivation, (Dasgupta and Ng, 2009) use an unsupervised algorithm to identify the unambiguous instances to train an active learner.
6. For the task of word alignment, which can be learned in an unsupervised manner, incorporating supervision with AL can bring further improvements in a data-efficient way (Ambati et al., 2010b,c).
7. Transfer learning. AL can be easily combined with transfer learning, another technique to reduce required annotations.
8. Utilizing pre-trained models is already a good example (Ein-Dor et al., 2020;Tamkin et al., 2022) and continual training (Gururangan et al., 2020) can also be applied (Hua and Wang, 2022;Margatina et al., 2022).
9. Moreover, transductive learning is commonly combined with AL by transferring learning signals from different domains (Chan and Ng, 2007;Shi et al., 2008;Rai et al., 2010;Saha et al., 2011;Wu et al., 2017;Kasai et al., 2019; or languages (Qian et al., 2014;Fang and Cohn, 2017;Fang et al., 2017;Chaudhary et al., 2019Moniz et al., 2022).
10. In addition to the task model, the model-based query policy ( §2.1.4) is also often obtained with transfer learning.
11. Weak supervision. AL can also be combined with weakly supervised learning.
12. Examples include learning from inputs and execution results for semantic parsing (Ni et al., 2020), labeling based on identical structure vectors for entity representations (Qian et al., 2020), learning from gazetteers and dictionaries for sequence labeling  and interactively discovering labeling rules .
13. Data augmentation. Augmentation is also applicable in AL and has been explored with iterative back-translation , mixup for sequence labeling  and phraseto-sentence augmentation for MT (Hu and Neubig, 2021).
14. As discussed in §2.1.1, augmentation can also be helpful for instance querying (Jiang et al., 2020;Zhang et al., 2022b).
15. Another interesting scenario involving augmentation and AL is query synthesis, which directly generates instances to be annotated instead of selecting existing unlabeled ones.
16. Though synthesizing texts is still a hard problem generally, there have been successful applications for simple classification tasks (Schumann and Rehbein, 2019;Quteineh et al., 2020)."
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s22,Model Mismatch,"['p22.0', 'p22.1']","['While it is natural to adopt the same bestperforming model throughout the AL process, there are cases where the query and final (successor) models can mismatch (Lewis and Catlett, 1994). Firstly, more efficient models are preferable for querying to reduce wait time ( §3.2.4). Moreover, since data usually outlive models, re-using ALbase data to train another model would be desired (Baldridge and Osborne, 2004;Tomanek et al., 2007). Several works show that model mismatch may make the gains from AL be negligible or even negative (Baldridge and Osborne, 2004;Lowell et al., 2019;, which raises concerns about the utilization of AL in practice.', 'For efficiency purposes, distillation can be utilized to improve querying efficiency while keeping reasonable AL performance.  show that using a smaller distilled version of a pre-trained model for querying does not lead to too much performance drop. Tsvigun et al. (2022) combine this idea with pseudo-labeling and sub-sampling to further reduce computational cost. Similarly, Nguyen et al. (2022) keep a smaller proxy model for query and synchronize the proxy with the main model by distillation.']","While it is natural to adopt the same bestperforming model throughout the AL process, there are cases where the query and final (successor) models can mismatch (Lewis and Catlett, 1994). Firstly, more efficient models are preferable for querying to reduce wait time ( §3.2.4). Moreover, since data usually outlive models, re-using ALbase data to train another model would be desired (Baldridge and Osborne, 2004;Tomanek et al., 2007). Several works show that model mismatch may make the gains from AL be negligible or even negative (Baldridge and Osborne, 2004;Lowell et al., 2019;, which raises concerns about the utilization of AL in practice.

For efficiency purposes, distillation can be utilized to improve querying efficiency while keeping reasonable AL performance.  show that using a smaller distilled version of a pre-trained model for querying does not lead to too much performance drop. Tsvigun et al. (2022) combine this idea with pseudo-labeling and sub-sampling to further reduce computational cost. Similarly, Nguyen et al. (2022) keep a smaller proxy model for query and synchronize the proxy with the main model by distillation.","(p22.0) While it is natural to adopt the same bestperforming model throughout the AL process, there are cases where the query and final (successor) models can mismatch (Lewis and Catlett, 1994). Firstly, more efficient models are preferable for querying to reduce wait time ( §3.2.4). Moreover, since data usually outlive models, re-using ALbase data to train another model would be desired (Baldridge and Osborne, 2004;Tomanek et al., 2007). Several works show that model mismatch may make the gains from AL be negligible or even negative (Baldridge and Osborne, 2004;Lowell et al., 2019;, which raises concerns about the utilization of AL in practice.

(p22.1) For efficiency purposes, distillation can be utilized to improve querying efficiency while keeping reasonable AL performance.  show that using a smaller distilled version of a pre-trained model for querying does not lead to too much performance drop. Tsvigun et al. (2022) combine this idea with pseudo-labeling and sub-sampling to further reduce computational cost. Similarly, Nguyen et al. (2022) keep a smaller proxy model for query and synchronize the proxy with the main model by distillation.","[['b94', None, 'b48'], ['b50']]","[['b94', None, 'b48'], ['b50']]",4,"1. While it is natural to adopt the same bestperforming model throughout the AL process, there are cases where the query and final (successor) models can mismatch (Lewis and Catlett, 1994).
2. Firstly, more efficient models are preferable for querying to reduce wait time ( §3.2.4).
3. Moreover, since data usually outlive models, re-using ALbase data to train another model would be desired (Baldridge and Osborne, 2004;Tomanek et al., 2007).
4. Several works show that model mismatch may make the gains from AL be negligible or even negative (Baldridge and Osborne, 2004;Lowell et al., 2019;, which raises concerns about the utilization of AL in practice.
5. For efficiency purposes, distillation can be utilized to improve querying efficiency while keeping reasonable AL performance.
6. show that using a smaller distilled version of a pre-trained model for querying does not lead to too much performance drop.
7. Tsvigun et al. (2022) combine this idea with pseudo-labeling and sub-sampling to further reduce computational cost.
8. Similarly, Nguyen et al. (2022) keep a smaller proxy model for query and synchronize the proxy with the main model by distillation."
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s20,Wait Time,"['p20.0', 'p20.1', 'p20.2', 'p20.3']","['In AL iterations, the annotators may need to wait for the training and querying steps (Line 3 and 4 in Algorithm 1). This wait time may bring some hidden costs, thus more efficient querying and training would be preferable for faster turnarounds.', 'To speed up querying, sub-sampling is a simple method to deal with large unlabeled pools (Roy and McCallum, 2001;Ertekin et al., 2007;Tsvigun et al., 2022). For some querying strategies, precalculating and caching unchanging information can also help to speed up (Ashrafi Asli et al., 2020;Citovsky et al., 2021). In addition, approximation with k-nearest neighbours can also be utilized to calculate density (Zhu et al., 2009) or search for instances after adversarial attacks (Ru et al., 2020).', 'To reduce training time, a seemingly reasonable strategy is to apply incremental training across AL iterations, that is, continuing training previous models on the new instances. However, Ash and Adams (2020) show that this type of warm-start may lead to sub-optimal performance for neural models and many recent AL works usually train models from scratch (Hu et al., 2019;Ein-Dor et al., 2020). Another method is to use an efficient model for querying and a more powerful model for final training. However, this might lead to sub-optimal results, which will be discussed in §4.1.', 'Another idea to reduce wait time is to simply allow querying with stale information. Actually, batch-mode AL ( §2.2.3) is such an example where instances in the same batch are queried with the same model. Haertel et al. (2010) propose parallel AL, which maintains separate loops of annotating, training, and scoring, and allows dynamic and parameterless instance selection at any time.']","In AL iterations, the annotators may need to wait for the training and querying steps (Line 3 and 4 in Algorithm 1). This wait time may bring some hidden costs, thus more efficient querying and training would be preferable for faster turnarounds.

To speed up querying, sub-sampling is a simple method to deal with large unlabeled pools (Roy and McCallum, 2001;Ertekin et al., 2007;Tsvigun et al., 2022). For some querying strategies, precalculating and caching unchanging information can also help to speed up (Ashrafi Asli et al., 2020;Citovsky et al., 2021). In addition, approximation with k-nearest neighbours can also be utilized to calculate density (Zhu et al., 2009) or search for instances after adversarial attacks (Ru et al., 2020).

To reduce training time, a seemingly reasonable strategy is to apply incremental training across AL iterations, that is, continuing training previous models on the new instances. However, Ash and Adams (2020) show that this type of warm-start may lead to sub-optimal performance for neural models and many recent AL works usually train models from scratch (Hu et al., 2019;Ein-Dor et al., 2020). Another method is to use an efficient model for querying and a more powerful model for final training. However, this might lead to sub-optimal results, which will be discussed in §4.1.

Another idea to reduce wait time is to simply allow querying with stale information. Actually, batch-mode AL ( §2.2.3) is such an example where instances in the same batch are queried with the same model. Haertel et al. (2010) propose parallel AL, which maintains separate loops of annotating, training, and scoring, and allows dynamic and parameterless instance selection at any time.","(p20.0) In AL iterations, the annotators may need to wait for the training and querying steps (Line 3 and 4 in Algorithm 1). This wait time may bring some hidden costs, thus more efficient querying and training would be preferable for faster turnarounds.

(p20.1) To speed up querying, sub-sampling is a simple method to deal with large unlabeled pools (Roy and McCallum, 2001;Ertekin et al., 2007;Tsvigun et al., 2022). For some querying strategies, precalculating and caching unchanging information can also help to speed up (Ashrafi Asli et al., 2020;Citovsky et al., 2021). In addition, approximation with k-nearest neighbours can also be utilized to calculate density (Zhu et al., 2009) or search for instances after adversarial attacks (Ru et al., 2020).

(p20.2) To reduce training time, a seemingly reasonable strategy is to apply incremental training across AL iterations, that is, continuing training previous models on the new instances. However, Ash and Adams (2020) show that this type of warm-start may lead to sub-optimal performance for neural models and many recent AL works usually train models from scratch (Hu et al., 2019;Ein-Dor et al., 2020). Another method is to use an efficient model for querying and a more powerful model for final training. However, this might lead to sub-optimal results, which will be discussed in §4.1.

(p20.3) Another idea to reduce wait time is to simply allow querying with stale information. Actually, batch-mode AL ( §2.2.3) is such an example where instances in the same batch are queried with the same model. Haertel et al. (2010) propose parallel AL, which maintains separate loops of annotating, training, and scoring, and allows dynamic and parameterless instance selection at any time.","[[], [None, 'b84', 'b50'], [None], [None]]","[[], [None, 'b84', 'b50'], [None], [None]]",5,"1. In AL iterations, the annotators may need to wait for the training and querying steps (Line 3 and 4 in Algorithm 1).
2. This wait time may bring some hidden costs, thus more efficient querying and training would be preferable for faster turnarounds.
3. To speed up querying, sub-sampling is a simple method to deal with large unlabeled pools (Roy and McCallum, 2001;Ertekin et al., 2007;Tsvigun et al., 2022).
4. For some querying strategies, precalculating and caching unchanging information can also help to speed up (Ashrafi Asli et al., 2020;Citovsky et al., 2021).
5. In addition, approximation with k-nearest neighbours can also be utilized to calculate density (Zhu et al., 2009) or search for instances after adversarial attacks (Ru et al., 2020).
6. To reduce training time, a seemingly reasonable strategy is to apply incremental training across AL iterations, that is, continuing training previous models on the new instances.
7. However, Ash and Adams (2020) show that this type of warm-start may lead to sub-optimal performance for neural models and many recent AL works usually train models from scratch (Hu et al., 2019;Ein-Dor et al., 2020).
8. Another method is to use an efficient model for querying and a more powerful model for final training.
9. However, this might lead to sub-optimal results, which will be discussed in §4.1.
10. Another idea to reduce wait time is to simply allow querying with stale information.
11. Actually, batch-mode AL ( §2.2.3) is such an example where instances in the same batch are queried with the same model.
12. Haertel et al. (2010) propose parallel AL, which maintains separate loops of annotating, training, and scoring, and allows dynamic and parameterless instance selection at any time."
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s9,Discriminative 3,['p9.0'],"['Another direction is to select instances that are different from already labeled instances. Again, for NLP tasks, simple feature-based metrics can be utilized for this purpose by preferring instances with more unseen n-grams or out-of-vocabulary words (Eck et al., 2005;Bloodgood and Callison-Burch, 2010;Erdmann et al., 2019). Generally, similarity scores can also be utilized to select the instances that are less similar to the labeled set (Kim et al., 2006;). Another interesting idea is to train a model to discriminate the labeled and unlabeled sets. Gissin and Shalev-Shwartz (2019) directly train a classifier for this purpose, while naturally adversarial training can be also adopted (Sinha et al., 2019;. In domain adaptation scenarios, the same motivation leads to the usage of a domain separator to filter instances (Rai et al., 2010).']","Another direction is to select instances that are different from already labeled instances. Again, for NLP tasks, simple feature-based metrics can be utilized for this purpose by preferring instances with more unseen n-grams or out-of-vocabulary words (Eck et al., 2005;Bloodgood and Callison-Burch, 2010;Erdmann et al., 2019). Generally, similarity scores can also be utilized to select the instances that are less similar to the labeled set (Kim et al., 2006;). Another interesting idea is to train a model to discriminate the labeled and unlabeled sets. Gissin and Shalev-Shwartz (2019) directly train a classifier for this purpose, while naturally adversarial training can be also adopted (Sinha et al., 2019;. In domain adaptation scenarios, the same motivation leads to the usage of a domain separator to filter instances (Rai et al., 2010).","(p9.0) Another direction is to select instances that are different from already labeled instances. Again, for NLP tasks, simple feature-based metrics can be utilized for this purpose by preferring instances with more unseen n-grams or out-of-vocabulary words (Eck et al., 2005;Bloodgood and Callison-Burch, 2010;Erdmann et al., 2019). Generally, similarity scores can also be utilized to select the instances that are less similar to the labeled set (Kim et al., 2006;). Another interesting idea is to train a model to discriminate the labeled and unlabeled sets. Gissin and Shalev-Shwartz (2019) directly train a classifier for this purpose, while naturally adversarial training can be also adopted (Sinha et al., 2019;. In domain adaptation scenarios, the same motivation leads to the usage of a domain separator to filter instances (Rai et al., 2010).","[['b106', 'b34', 'b2', 'b91']]","[['b106', 'b34', 'b2', 'b91']]",4,"1. Another direction is to select instances that are different from already labeled instances.
2. Again, for NLP tasks, simple feature-based metrics can be utilized for this purpose by preferring instances with more unseen n-grams or out-of-vocabulary words (Eck et al., 2005;Bloodgood and Callison-Burch, 2010;Erdmann et al., 2019).
3. Generally, similarity scores can also be utilized to select the instances that are less similar to the labeled set (Kim et al., 2006;).
4. Another interesting idea is to train a model to discriminate the labeled and unlabeled sets.
5. Gissin and Shalev-Shwartz (2019) directly train a classifier for this purpose, while naturally adversarial training can be also adopted (Sinha et al., 2019;. In domain adaptation scenarios, the same motivation leads to the usage of a domain separator to filter instances (Rai et al., 2010)."
252992688,A Survey of Active Learning for Natural Language Processing,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s19,Directly Reducing Cost,"['p19.0', 'p19.1', 'p19.2']","['In addition to better query strategies, there are other ways of directly reducing annotation cost, such as computer-assisted annotation. In AL, models and annotators usually interact in an indirect way where models only query the instances to present to the annotators, while there could be closer interactions.', ""Pre-annotation is such an idea, where not only the raw data instances but also the model's best or top-k predictions are sent to the annotators to help them make decisions. If the model's predictions are reasonable, the annotators can simply select or make a few corrections to obtain the gold annotations rather than creating from scratch. This method has been shown effective when combined with AL (Baldridge and Osborne, 2004;Vlachos, 2006;Ringger et al., 2008;Skeppstedt, 2013;Cañizares-Díaz et al., 2021). Post-editing for MT is also a typical example (Dara et al., 2014)."", ""Moreover, the models could provide help at real annotating time. For example, Culotta and Mc-Callum (2005) present an interactive AL system where the user's corrections can propagate to the model, which generates new predictions for the user to further refine. Interactive machine translation (IMT) adopts a similar idea, where the annotator corrects the first erroneous character, based on which the model reproduces the prediction. AL has also been combined with IMT to further reduce manual efforts (González-Rubio et al., 2012;Peris and Casacuberta, 2018;.""]","In addition to better query strategies, there are other ways of directly reducing annotation cost, such as computer-assisted annotation. In AL, models and annotators usually interact in an indirect way where models only query the instances to present to the annotators, while there could be closer interactions.

Pre-annotation is such an idea, where not only the raw data instances but also the model's best or top-k predictions are sent to the annotators to help them make decisions. If the model's predictions are reasonable, the annotators can simply select or make a few corrections to obtain the gold annotations rather than creating from scratch. This method has been shown effective when combined with AL (Baldridge and Osborne, 2004;Vlachos, 2006;Ringger et al., 2008;Skeppstedt, 2013;Cañizares-Díaz et al., 2021). Post-editing for MT is also a typical example (Dara et al., 2014).

Moreover, the models could provide help at real annotating time. For example, Culotta and Mc-Callum (2005) present an interactive AL system where the user's corrections can propagate to the model, which generates new predictions for the user to further refine. Interactive machine translation (IMT) adopts a similar idea, where the annotator corrects the first erroneous character, based on which the model reproduces the prediction. AL has also been combined with IMT to further reduce manual efforts (González-Rubio et al., 2012;Peris and Casacuberta, 2018;.","(p19.0) In addition to better query strategies, there are other ways of directly reducing annotation cost, such as computer-assisted annotation. In AL, models and annotators usually interact in an indirect way where models only query the instances to present to the annotators, while there could be closer interactions.

(p19.1) Pre-annotation is such an idea, where not only the raw data instances but also the model's best or top-k predictions are sent to the annotators to help them make decisions. If the model's predictions are reasonable, the annotators can simply select or make a few corrections to obtain the gold annotations rather than creating from scratch. This method has been shown effective when combined with AL (Baldridge and Osborne, 2004;Vlachos, 2006;Ringger et al., 2008;Skeppstedt, 2013;Cañizares-Díaz et al., 2021). Post-editing for MT is also a typical example (Dara et al., 2014).

(p19.2) Moreover, the models could provide help at real annotating time. For example, Culotta and Mc-Callum (2005) present an interactive AL system where the user's corrections can propagate to the model, which generates new predictions for the user to further refine. Interactive machine translation (IMT) adopts a similar idea, where the annotator corrects the first erroneous character, based on which the model reproduces the prediction. AL has also been combined with IMT to further reduce manual efforts (González-Rubio et al., 2012;Peris and Casacuberta, 2018;.","[[], ['b8', 'b102', None, 'b35', 'b94', 'b51'], [None, 'b107']]","[[], ['b8', 'b102', None, 'b35', 'b94', 'b51'], [None, 'b107']]",8,"1. In addition to better query strategies, there are other ways of directly reducing annotation cost, such as computer-assisted annotation.
2. In AL, models and annotators usually interact in an indirect way where models only query the instances to present to the annotators, while there could be closer interactions.
3. Pre-annotation is such an idea, where not only the raw data instances but also the model's best or top-k predictions are sent to the annotators to help them make decisions.
4. If the model's predictions are reasonable, the annotators can simply select or make a few corrections to obtain the gold annotations rather than creating from scratch.
5. This method has been shown effective when combined with AL (Baldridge and Osborne, 2004;Vlachos, 2006;Ringger et al., 2008;Skeppstedt, 2013;Cañizares-Díaz et al., 2021).
6. Post-editing for MT is also a typical example (Dara et al., 2014).
7. Moreover, the models could provide help at real annotating time.
8. For example, Culotta and Mc-Callum (2005) present an interactive AL system where the user's corrections can propagate to the model, which generates new predictions for the user to further refine.
9. Interactive machine translation (IMT) adopts a similar idea, where the annotator corrects the first erroneous character, based on which the model reproduces the prediction.
10. AL has also been combined with IMT to further reduce manual efforts (González-Rubio et al., 2012;Peris and Casacuberta, 2018;."
253736389,Transformers for Tabular Data Representation: A Survey of Models and Applications,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18ff1542d5a2a4490c7b3f21522bf1343889f700,s16,Downstream Tasks,"['p16.0', 'p16.1', 'p16.2', 'p16.3', 'p16.4']","['Using neural representations for tabular data show, improvements in performance in several downstream tasks. In this section, we describe the tasks and define their input and output. While they all consume tables, settings can be quite heterogeneous, with systems exploiting different information, even for the same task. A summary of the covered tasks along their input, output, and some representative systems addressing them is shown in Table 4. We detail next the mandatory input elements and the different contexts.  (Dagan et al., 2013;Korman et al., 2018), checking facts with tables consists of verifying if a textual input claim is true or false against a trusted database (TAPEX, DECO, TABFACT), also provided as input. Some fact-checking systems, such as FEVEROUS, also output the cells used for verification as evidence (Nakov et al., 2021;Karagiannis et al., 2020).', 'Question Answering (QA): In the free text setting, QA aims at retrieving passages that include the answer to a given question. In the tabular data setting, it consists of returning as output the cells that answer an input consisting of a question and a table. One can distinguish two levels of complexity. Simple QA involves lookup queries on tables (DTR, CLTR), while a more complex QA  Most of the systems in this survey aim at improving accuracy in QA with respect to hand-crafted embeddings.', ""Semantic Parsing (SP): In the tabular data setting, given a question and a table as input, SP generates a declarative query in SQL over the table's schema to retrieve the answer to the question. While in QA the interest is in directly getting the answer, SP produces the (interpretable) query to obtain it (TABERT, GRAPPA, TAPEX).  table that can be used to answer the question. TR is helpful when trying to reduce the search space for a QA task (GTR, DTR, MMR). It is a challenging task given the limited input size of transformers, that is, their constraint to sequences of 512 tokens.  We observe that most tasks can be seen as traditional NLP problems where structured data replace free text, such as the case of QA where answers are located in tabular data instead of documents (Gupta and Gupta, 2012). TFC involves retrieving cells that entail or refute a given statement, whereas on free text the corresponding objective is to select sentences as evidence (Thorne et al., 2018). SP is the task of converting natural language utterance into a logical form (Berant and Liang, 2014), which in this setting is expressed as a declarative query over a relational table. TR on tabular data corresponds to passage retrieval on free text (Kaszkiel and Zobel, 1997). TCP is analogous to predicting missing words or values in a sentence . Finally, TMP can be related to syntactic parsing in NLP (Van Gompel and Pickering, 2007), where relationships between different tokens are depicted."", 'We conclude this section with an analysis of the performance of the systems over the different downstream tasks. For every task, we selected datasets for which at least two systems have reported results. All datasets reported in Table 5 (Ghasemi-Gol and Szekely, 2018), which contains web tables annotated with their type (relational, entity, matrix, list, and non-data), and EntiTab (Zhang and Balog, 2017), which contains web tables annotated with possible header labels for the column population task. Table 5 also contains the size, expressed as number of parameters, of the largest model used by every system. As some systems are not comparable on any shared datasets, we report here their size: TABFACT (110M), MMR (87M), TURL (314M), RPT (139M), and CLTR (235M). Larger models do not correlate with better performance across different systems for the same task, but a larger model always brings higher scores for the same system, as expected. Execution times for training and testing depend on the size of the model and the computing architecture.', 'The results in the table show that some tasks, such as TFC and TMP, can already be handled successfully by the systems, while some tasks, such as TCP, TR, and SP, are harder. QA is the task supported by most systems and the quality of the results vary depending on the dataset at hand. Differences in performance can be explained with different improvements across the systems. For example, MATE has better performance with respect to TAPAS in two tasks because of its mechanism to deal with larger input tables. Similarly, TUTA improves over TABERT because it handles tabular data beyond relational tables. Finally, TABERT and TAPAS are the systems that show most coverage in terms of tasks, with multiple papers using them as baselines in their experiments. For the systems that are not reported in Table 5, we notice that TURL obtains similar F1 results for column type prediction, but on a dataset different from VizNet; TURL, TABBIE, and TABERT also report comparable MAP results for the row population task (not in Table 5) over different datasets.']","Using neural representations for tabular data show, improvements in performance in several downstream tasks. In this section, we describe the tasks and define their input and output. While they all consume tables, settings can be quite heterogeneous, with systems exploiting different information, even for the same task. A summary of the covered tasks along their input, output, and some representative systems addressing them is shown in Table 4. We detail next the mandatory input elements and the different contexts.  (Dagan et al., 2013;Korman et al., 2018), checking facts with tables consists of verifying if a textual input claim is true or false against a trusted database (TAPEX, DECO, TABFACT), also provided as input. Some fact-checking systems, such as FEVEROUS, also output the cells used for verification as evidence (Nakov et al., 2021;Karagiannis et al., 2020).

Question Answering (QA): In the free text setting, QA aims at retrieving passages that include the answer to a given question. In the tabular data setting, it consists of returning as output the cells that answer an input consisting of a question and a table. One can distinguish two levels of complexity. Simple QA involves lookup queries on tables (DTR, CLTR), while a more complex QA  Most of the systems in this survey aim at improving accuracy in QA with respect to hand-crafted embeddings.

Semantic Parsing (SP): In the tabular data setting, given a question and a table as input, SP generates a declarative query in SQL over the table's schema to retrieve the answer to the question. While in QA the interest is in directly getting the answer, SP produces the (interpretable) query to obtain it (TABERT, GRAPPA, TAPEX).  table that can be used to answer the question. TR is helpful when trying to reduce the search space for a QA task (GTR, DTR, MMR). It is a challenging task given the limited input size of transformers, that is, their constraint to sequences of 512 tokens.  We observe that most tasks can be seen as traditional NLP problems where structured data replace free text, such as the case of QA where answers are located in tabular data instead of documents (Gupta and Gupta, 2012). TFC involves retrieving cells that entail or refute a given statement, whereas on free text the corresponding objective is to select sentences as evidence (Thorne et al., 2018). SP is the task of converting natural language utterance into a logical form (Berant and Liang, 2014), which in this setting is expressed as a declarative query over a relational table. TR on tabular data corresponds to passage retrieval on free text (Kaszkiel and Zobel, 1997). TCP is analogous to predicting missing words or values in a sentence . Finally, TMP can be related to syntactic parsing in NLP (Van Gompel and Pickering, 2007), where relationships between different tokens are depicted.

We conclude this section with an analysis of the performance of the systems over the different downstream tasks. For every task, we selected datasets for which at least two systems have reported results. All datasets reported in Table 5 (Ghasemi-Gol and Szekely, 2018), which contains web tables annotated with their type (relational, entity, matrix, list, and non-data), and EntiTab (Zhang and Balog, 2017), which contains web tables annotated with possible header labels for the column population task. Table 5 also contains the size, expressed as number of parameters, of the largest model used by every system. As some systems are not comparable on any shared datasets, we report here their size: TABFACT (110M), MMR (87M), TURL (314M), RPT (139M), and CLTR (235M). Larger models do not correlate with better performance across different systems for the same task, but a larger model always brings higher scores for the same system, as expected. Execution times for training and testing depend on the size of the model and the computing architecture.

The results in the table show that some tasks, such as TFC and TMP, can already be handled successfully by the systems, while some tasks, such as TCP, TR, and SP, are harder. QA is the task supported by most systems and the quality of the results vary depending on the dataset at hand. Differences in performance can be explained with different improvements across the systems. For example, MATE has better performance with respect to TAPAS in two tasks because of its mechanism to deal with larger input tables. Similarly, TUTA improves over TABERT because it handles tabular data beyond relational tables. Finally, TABERT and TAPAS are the systems that show most coverage in terms of tasks, with multiple papers using them as baselines in their experiments. For the systems that are not reported in Table 5, we notice that TURL obtains similar F1 results for column type prediction, but on a dataset different from VizNet; TURL, TABBIE, and TABERT also report comparable MAP results for the row population task (not in Table 5) over different datasets.","(p16.0) Using neural representations for tabular data show, improvements in performance in several downstream tasks. In this section, we describe the tasks and define their input and output. While they all consume tables, settings can be quite heterogeneous, with systems exploiting different information, even for the same task. A summary of the covered tasks along their input, output, and some representative systems addressing them is shown in Table 4. We detail next the mandatory input elements and the different contexts.  (Dagan et al., 2013;Korman et al., 2018), checking facts with tables consists of verifying if a textual input claim is true or false against a trusted database (TAPEX, DECO, TABFACT), also provided as input. Some fact-checking systems, such as FEVEROUS, also output the cells used for verification as evidence (Nakov et al., 2021;Karagiannis et al., 2020).

(p16.1) Question Answering (QA): In the free text setting, QA aims at retrieving passages that include the answer to a given question. In the tabular data setting, it consists of returning as output the cells that answer an input consisting of a question and a table. One can distinguish two levels of complexity. Simple QA involves lookup queries on tables (DTR, CLTR), while a more complex QA  Most of the systems in this survey aim at improving accuracy in QA with respect to hand-crafted embeddings.

(p16.2) Semantic Parsing (SP): In the tabular data setting, given a question and a table as input, SP generates a declarative query in SQL over the table's schema to retrieve the answer to the question. While in QA the interest is in directly getting the answer, SP produces the (interpretable) query to obtain it (TABERT, GRAPPA, TAPEX).  table that can be used to answer the question. TR is helpful when trying to reduce the search space for a QA task (GTR, DTR, MMR). It is a challenging task given the limited input size of transformers, that is, their constraint to sequences of 512 tokens.  We observe that most tasks can be seen as traditional NLP problems where structured data replace free text, such as the case of QA where answers are located in tabular data instead of documents (Gupta and Gupta, 2012). TFC involves retrieving cells that entail or refute a given statement, whereas on free text the corresponding objective is to select sentences as evidence (Thorne et al., 2018). SP is the task of converting natural language utterance into a logical form (Berant and Liang, 2014), which in this setting is expressed as a declarative query over a relational table. TR on tabular data corresponds to passage retrieval on free text (Kaszkiel and Zobel, 1997). TCP is analogous to predicting missing words or values in a sentence . Finally, TMP can be related to syntactic parsing in NLP (Van Gompel and Pickering, 2007), where relationships between different tokens are depicted.

(p16.3) We conclude this section with an analysis of the performance of the systems over the different downstream tasks. For every task, we selected datasets for which at least two systems have reported results. All datasets reported in Table 5 (Ghasemi-Gol and Szekely, 2018), which contains web tables annotated with their type (relational, entity, matrix, list, and non-data), and EntiTab (Zhang and Balog, 2017), which contains web tables annotated with possible header labels for the column population task. Table 5 also contains the size, expressed as number of parameters, of the largest model used by every system. As some systems are not comparable on any shared datasets, we report here their size: TABFACT (110M), MMR (87M), TURL (314M), RPT (139M), and CLTR (235M). Larger models do not correlate with better performance across different systems for the same task, but a larger model always brings higher scores for the same system, as expected. Execution times for training and testing depend on the size of the model and the computing architecture.

(p16.4) The results in the table show that some tasks, such as TFC and TMP, can already be handled successfully by the systems, while some tasks, such as TCP, TR, and SP, are harder. QA is the task supported by most systems and the quality of the results vary depending on the dataset at hand. Differences in performance can be explained with different improvements across the systems. For example, MATE has better performance with respect to TAPAS in two tasks because of its mechanism to deal with larger input tables. Similarly, TUTA improves over TABERT because it handles tabular data beyond relational tables. Finally, TABERT and TAPAS are the systems that show most coverage in terms of tasks, with multiple papers using them as baselines in their experiments. For the systems that are not reported in Table 5, we notice that TURL obtains similar F1 results for column type prediction, but on a dataset different from VizNet; TURL, TABBIE, and TABERT also report comparable MAP results for the row population task (not in Table 5) over different datasets.","[['b61', 'b30', 'b72', 'b54'], [], ['b55', 'b93', 'b47', 'b10', 'b96'], ['b111', 'b42'], []]","[['b61', 'b30', 'b72', 'b54'], [], ['b55', 'b93', 'b47', 'b10', 'b96'], ['b111', 'b42'], []]",11,"1. Using neural representations for tabular data show, improvements in performance in several downstream tasks.
2. In this section, we describe the tasks and define their input and output.
3. While they all consume tables, settings can be quite heterogeneous, with systems exploiting different information, even for the same task.
4. A summary of the covered tasks along their input, output, and some representative systems addressing them is shown in Table 4.
5. We detail next the mandatory input elements and the different contexts.
6. (Dagan et al., 2013;Korman et al., 2018), checking facts with tables consists of verifying if a textual input claim is true or false against a trusted database (TAPEX, DECO, TABFACT), also provided as input.
7. Some fact-checking systems, such as FEVEROUS, also output the cells used for verification as evidence (Nakov et al., 2021;Karagiannis et al., 2020).
8. Question Answering (QA): In the free text setting, QA aims at retrieving passages that include the answer to a given question.
9. In the tabular data setting, it consists of returning as output the cells that answer an input consisting of a question and a table.
10. One can distinguish two levels of complexity.
11. Simple QA involves lookup queries on tables (DTR, CLTR), while a more complex QA  Most of the systems in this survey aim at improving accuracy in QA with respect to hand-crafted embeddings.
12. Semantic Parsing (SP): In the tabular data setting, given a question and a table as input, SP generates a declarative query in SQL over the table's schema to retrieve the answer to the question.
13. While in QA the interest is in directly getting the answer, SP produces the (interpretable) query to obtain it (TABERT, GRAPPA, TAPEX).  table that can be used to answer the question.
14. TR is helpful when trying to reduce the search space for a QA task (GTR, DTR, MMR).
15. It is a challenging task given the limited input size of transformers, that is, their constraint to sequences of 512 tokens.
16. We observe that most tasks can be seen as traditional NLP problems where structured data replace free text, such as the case of QA where answers are located in tabular data instead of documents (Gupta and Gupta, 2012).
17. TFC involves retrieving cells that entail or refute a given statement, whereas on free text the corresponding objective is to select sentences as evidence (Thorne et al., 2018).
18. SP is the task of converting natural language utterance into a logical form (Berant and Liang, 2014), which in this setting is expressed as a declarative query over a relational table.
19. TR on tabular data corresponds to passage retrieval on free text (Kaszkiel and Zobel, 1997).
20. TCP is analogous to predicting missing words or values in a sentence .
21. Finally, TMP can be related to syntactic parsing in NLP (Van Gompel and Pickering, 2007), where relationships between different tokens are depicted.
22. We conclude this section with an analysis of the performance of the systems over the different downstream tasks.
23. For every task, we selected datasets for which at least two systems have reported results.
24. All datasets reported in Table 5 (Ghasemi-Gol and Szekely, 2018), which contains web tables annotated with their type (relational, entity, matrix, list, and non-data), and EntiTab (Zhang and Balog, 2017), which contains web tables annotated with possible header labels for the column population task.
25. Table 5 also contains the size, expressed as number of parameters, of the largest model used by every system.
26. As some systems are not comparable on any shared datasets, we report here their size: TABFACT (110M), MMR (87M), TURL (314M), RPT (139M), and CLTR (235M).
27. Larger models do not correlate with better performance across different systems for the same task, but a larger model always brings higher scores for the same system, as expected.
28. Execution times for training and testing depend on the size of the model and the computing architecture.
29. The results in the table show that some tasks, such as TFC and TMP, can already be handled successfully by the systems, while some tasks, such as TCP, TR, and SP, are harder.
30. QA is the task supported by most systems and the quality of the results vary depending on the dataset at hand.
31. Differences in performance can be explained with different improvements across the systems.
32. For example, MATE has better performance with respect to TAPAS in two tasks because of its mechanism to deal with larger input tables.
33. Similarly, TUTA improves over TABERT because it handles tabular data beyond relational tables.
34. Finally, TABERT and TAPAS are the systems that show most coverage in terms of tasks, with multiple papers using them as baselines in their experiments.
35. For the systems that are not reported in Table 5, we notice that TURL obtains similar F1 results for column type prediction, but on a dataset different from VizNet; TURL, TABBIE, and TABERT also report comparable MAP results for the row population task (not in Table 5) over different datasets."
253736389,Transformers for Tabular Data Representation: A Survey of Models and Applications,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18ff1542d5a2a4490c7b3f21522bf1343889f700,s10,Vanilla Transformer,"['p10.0', 'p10.1']","['The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules. The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Residual connections and layer-normalization modules are also used. Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions.', 'The transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017;Raffel et al., 2020), an encoder-only , or decoder-only (Radford et al., 2019;Brown et al., 2020) model. The choice of the architecture depends on the final task. Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data. In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input. The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).']","The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules. The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Residual connections and layer-normalization modules are also used. Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions.

The transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017;Raffel et al., 2020), an encoder-only , or decoder-only (Radford et al., 2019;Brown et al., 2020) model. The choice of the architecture depends on the final task. Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data. In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input. The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).","(p10.0) The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules. The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Residual connections and layer-normalization modules are also used. Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions.

(p10.1) The transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017;Raffel et al., 2020), an encoder-only , or decoder-only (Radford et al., 2019;Brown et al., 2020) model. The choice of the architecture depends on the final task. Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data. In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input. The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).","[['b97', 'b90'], ['b78', 'b77', 'b97', None]]","[['b97', 'b90'], ['b78', 'b77', 'b97', None]]",6,"1. The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules.
2. The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network.
3. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.
4. Residual connections and layer-normalization modules are also used.
5. Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions.
6. The transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017;Raffel et al., 2020), an encoder-only , or decoder-only (Radford et al., 2019;Brown et al., 2020) model.
7. The choice of the architecture depends on the final task.
8. Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data.
9. In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input.
10. The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX)."
253736389,Transformers for Tabular Data Representation: A Survey of Models and Applications,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18ff1542d5a2a4490c7b3f21522bf1343889f700,s2,Training Datasets,['p2.0'],"[""We present both the datasets used for pre-training and for fine-tuning in the downstream tasks. Pre-training tables are not annotated, in some cases scraped from the web, while data used for fine-tuning have task-dependent annotation labels. The datasets consist of tables and their context, such as table metadata, surrounding texts, claims or questions. To construct large pre-training datasets and in an attempt to reduce bias, multiple sources can be used, independently of the target task at hand. For instance, unlike TAPAS (Herzig et al., 2020), which only uses Wikipedia Tables for QA, and TABULARNET , which only uses Spreadsheets for TMP, TABERT  uses Wikipedia Tables and WDC  for SP; GRAPPA uses Wikipedia Tables, Spider, and WikiSQL for SP; MMR (Kostić et al., 2021) uses NQ, OTT-QA, and WikiSQL for TR; MATE  uses Wikipedia Tables and HybridQA for QA; and TUTA  uses Wikipedia Tables, WDC, and spreadsheets for TMP. In general, it is recommended to utilize different data sources for pre-training to ensure covering different kinds and content, and thus, improve the scope of representations. For instance, Wikipedia tables has a large number of relational tables (Bhagavatula et al., 2015), while WDC and Spreadsheets include also entity tables and spreadsheets with complex structure. Table 2 summarizes the main characteristics for the most common datasets. We mark the tasks for which the dataset has been used by ✔ under the column ''Task''. We note that the top four datasets are mostly used for pre-training, while the others can be used for fine-tuning as well since they include annotations for the target task, for example, questions/answers for QA. The column ''Large Tables'' is a binary indicator, where ✔ and ✘ indicate whether or not the tabular corpus include large tables and hence whether or not some pre-processing is needed to reduce table content to meet the limits of the transformer architecture (512 input tokens in most cases). Some works, such as TABERT, TAPEX (Liu et al., 2021a), and CLTR (Pan et al., 2021), apply filtering in any case to reduce noisy input. Finally, the ''Context'' column describes additional text that come with the tables. This can be text describing the table, such as caption or title of the document containing the table; table metadata, such as table orientation, header row, and keys; or questions and claims that can be addressed with the table.""]","We present both the datasets used for pre-training and for fine-tuning in the downstream tasks. Pre-training tables are not annotated, in some cases scraped from the web, while data used for fine-tuning have task-dependent annotation labels. The datasets consist of tables and their context, such as table metadata, surrounding texts, claims or questions. To construct large pre-training datasets and in an attempt to reduce bias, multiple sources can be used, independently of the target task at hand. For instance, unlike TAPAS (Herzig et al., 2020), which only uses Wikipedia Tables for QA, and TABULARNET , which only uses Spreadsheets for TMP, TABERT  uses Wikipedia Tables and WDC  for SP; GRAPPA uses Wikipedia Tables, Spider, and WikiSQL for SP; MMR (Kostić et al., 2021) uses NQ, OTT-QA, and WikiSQL for TR; MATE  uses Wikipedia Tables and HybridQA for QA; and TUTA  uses Wikipedia Tables, WDC, and spreadsheets for TMP. In general, it is recommended to utilize different data sources for pre-training to ensure covering different kinds and content, and thus, improve the scope of representations. For instance, Wikipedia tables has a large number of relational tables (Bhagavatula et al., 2015), while WDC and Spreadsheets include also entity tables and spreadsheets with complex structure. Table 2 summarizes the main characteristics for the most common datasets. We mark the tasks for which the dataset has been used by ✔ under the column ''Task''. We note that the top four datasets are mostly used for pre-training, while the others can be used for fine-tuning as well since they include annotations for the target task, for example, questions/answers for QA. The column ''Large Tables'' is a binary indicator, where ✔ and ✘ indicate whether or not the tabular corpus include large tables and hence whether or not some pre-processing is needed to reduce table content to meet the limits of the transformer architecture (512 input tokens in most cases). Some works, such as TABERT, TAPEX (Liu et al., 2021a), and CLTR (Pan et al., 2021), apply filtering in any case to reduce noisy input. Finally, the ''Context'' column describes additional text that come with the tables. This can be text describing the table, such as caption or title of the document containing the table; table metadata, such as table orientation, header row, and keys; or questions and claims that can be addressed with the table.","(p2.0) We present both the datasets used for pre-training and for fine-tuning in the downstream tasks. Pre-training tables are not annotated, in some cases scraped from the web, while data used for fine-tuning have task-dependent annotation labels. The datasets consist of tables and their context, such as table metadata, surrounding texts, claims or questions. To construct large pre-training datasets and in an attempt to reduce bias, multiple sources can be used, independently of the target task at hand. For instance, unlike TAPAS (Herzig et al., 2020), which only uses Wikipedia Tables for QA, and TABULARNET , which only uses Spreadsheets for TMP, TABERT  uses Wikipedia Tables and WDC  for SP; GRAPPA uses Wikipedia Tables, Spider, and WikiSQL for SP; MMR (Kostić et al., 2021) uses NQ, OTT-QA, and WikiSQL for TR; MATE  uses Wikipedia Tables and HybridQA for QA; and TUTA  uses Wikipedia Tables, WDC, and spreadsheets for TMP. In general, it is recommended to utilize different data sources for pre-training to ensure covering different kinds and content, and thus, improve the scope of representations. For instance, Wikipedia tables has a large number of relational tables (Bhagavatula et al., 2015), while WDC and Spreadsheets include also entity tables and spreadsheets with complex structure. Table 2 summarizes the main characteristics for the most common datasets. We mark the tasks for which the dataset has been used by ✔ under the column ''Task''. We note that the top four datasets are mostly used for pre-training, while the others can be used for fine-tuning as well since they include annotations for the target task, for example, questions/answers for QA. The column ''Large Tables'' is a binary indicator, where ✔ and ✘ indicate whether or not the tabular corpus include large tables and hence whether or not some pre-processing is needed to reduce table content to meet the limits of the transformer architecture (512 input tokens in most cases). Some works, such as TABERT, TAPEX (Liu et al., 2021a), and CLTR (Pan et al., 2021), apply filtering in any case to reduce noisy input. Finally, the ''Context'' column describes additional text that come with the tables. This can be text describing the table, such as caption or title of the document containing the table; table metadata, such as table orientation, header row, and keys; or questions and claims that can be addressed with the table.","[['b49', 'b62', 'b11', 'b68', 'b74']]","[['b49', 'b62', 'b11', 'b68', 'b74']]",5,"1. We present both the datasets used for pre-training and for fine-tuning in the downstream tasks.
2. Pre-training tables are not annotated, in some cases scraped from the web, while data used for fine-tuning have task-dependent annotation labels.
3. The datasets consist of tables and their context, such as table metadata, surrounding texts, claims or questions.
4. To construct large pre-training datasets and in an attempt to reduce bias, multiple sources can be used, independently of the target task at hand.
5. For instance, unlike TAPAS (Herzig et al., 2020), which only uses Wikipedia Tables for QA, and TABULARNET , which only uses Spreadsheets for TMP, TABERT  uses Wikipedia Tables and WDC  for SP; GRAPPA uses Wikipedia Tables, Spider, and WikiSQL for SP; MMR (Kostić et al., 2021) uses NQ, OTT-QA, and WikiSQL for TR; MATE  uses Wikipedia Tables and HybridQA for QA; and TUTA  uses Wikipedia Tables, WDC, and spreadsheets for TMP.
6. In general, it is recommended to utilize different data sources for pre-training to ensure covering different kinds and content, and thus, improve the scope of representations.
7. For instance, Wikipedia tables has a large number of relational tables (Bhagavatula et al., 2015), while WDC and Spreadsheets include also entity tables and spreadsheets with complex structure.
8. Table 2 summarizes the main characteristics for the most common datasets.
9. We mark the tasks for which the dataset has been used by ✔ under the column ''Task''.
10. We note that the top four datasets are mostly used for pre-training, while the others can be used for fine-tuning as well since they include annotations for the target task, for example, questions/answers for QA.
11. The column ''Large Tables'' is a binary indicator, where ✔ and ✘ indicate whether or not the tabular corpus include large tables and hence whether or not some pre-processing is needed to reduce table content to meet the limits of the transformer architecture (512 input tokens in most cases).
12. Some works, such as TABERT, TAPEX (Liu et al., 2021a), and CLTR (Pan et al., 2021), apply filtering in any case to reduce noisy input.
13. Finally, the ''Context'' column describes additional text that come with the tables.
14. This can be text describing the table, such as caption or title of the document containing the table; table metadata, such as table orientation, header row, and keys; or questions and claims that can be addressed with the table."
254043519,Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/32cdcf593fef78dcf9e1b6aba7f2345310d7cc60,s15,Limitations,"['p15.0', 'p15.1', 'p15.2', 'p15.3', 'p15.4', 'p15.5']","['On Dataset Documentation. Throughout our survey process, we found inadequate dataset documentation, limiting the coverage of our survey. We suggest that individual researchers provide the input data source and the labeling methodology; if people were involved in dataset creation, their demographic information should be provided, as well. Such information can help researchers analyze potential bias embedded in the dataset (Bender and Friedman, 2018).', 'Surveyed Dataset Collection Process. Despite our best efforts, we do not claim to cover all relevant datasets. Our collection process overlooks datasets that are published at non-ACL venues and not in Hugging Face as well as papers that do not match our search keywords. For instance, we missed multilingual machine reading comprehension datasets (Gupta and Khade, 2020;Asai et al., 2018) and morphology datasets (McCarthy et al., 2020). We also found a very low presence of indigenous language datasets. None of 10 indigenous American languages from a recent study (Ebrahimi et al., 2022) was represented in our survey. That said, we host http: //multilingual-dataset-survey.github.io where researchers can submit their dataset information and periodically update our analysis. Furthermore, we constantly encountered poorly written documentation or unavailable datasets during our annotation processes. During annotation, whenever this paper\'s dataset annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories. If no evidence could be found for the inference, they put ""not mentioned"" as a result. All unclear decision were adjudicated by at least three annotators.', 'Using Country Names as a Proxy for Languages Spoken. In Section 5, we attempted to approximate the number of NLP researchers with language proficiency in different languages. To do this, we mapped the names of ACL submission country to the most commonly spoken languages in those countries. We acknowledge that (1) the country of origin of researchers might be different from the country of submission, (2) researchers native language might not be listed in the commonly spoken languages and (3) the mapping might be incomprehensive.', 'MTurk Pilot Study. Due to our limited data points, although our MTurk study showed that data quality could be improved if the language qualification were applied in the collection process on MTurk, and our previous recommendations do not currently apply, we acknowledge that more research at scale should be done to statistically confirm the conclusion. Furthermore, languages other than the supported 5 languages might still be unsuitable for gathering multilingual data on MTurk As we identify that many datasets use texts beyond Wikipedia (see Table 4), we instead use the mC4 corpora (Xue et al., 2021), a much larger collection of texts in 101 languages drawn from the public Common Crawl web scrape and used for training the mT5 model. Specifically, we use the number of tokens in mC4 to estimate the amount of unlabeled data. Figure 7a shows a scatter plot where the x-axis represents the number of tokens in mC4 and y-axis represents the number of labeled datasets available in the languages. The availability of unlabeled text corpora and the number of labeled datasets show a high correlation (ρ = 0.794). We also analyze the relationship between the number of labeled datasets and the number of Wikipedia articles in Figure 7b. Again we observe a high correlation of ρ = 0.767. Classification (sentiment analysis) annotated (authors, linguists) collected from Wikipedia collected from curated source (exams, scientific papers, etc) collected from media (news) collected from social media or commercial sources collected from web crowdsourced curated linguistic resources multiple modes template-based Figure 6: The distribution over input source per task type. The size of bar represents the # of datasets of that task type and input source.', '(a) The number of tokens (billions) in mC4 (Xue et al., 2021).', '(b) The number of Wikipedia articles. Figure 7: Relationships between the number of datasets and number of tokens in mC4 and Wikipedia articles. English is removed.']","On Dataset Documentation. Throughout our survey process, we found inadequate dataset documentation, limiting the coverage of our survey. We suggest that individual researchers provide the input data source and the labeling methodology; if people were involved in dataset creation, their demographic information should be provided, as well. Such information can help researchers analyze potential bias embedded in the dataset (Bender and Friedman, 2018).

Surveyed Dataset Collection Process. Despite our best efforts, we do not claim to cover all relevant datasets. Our collection process overlooks datasets that are published at non-ACL venues and not in Hugging Face as well as papers that do not match our search keywords. For instance, we missed multilingual machine reading comprehension datasets (Gupta and Khade, 2020;Asai et al., 2018) and morphology datasets (McCarthy et al., 2020). We also found a very low presence of indigenous language datasets. None of 10 indigenous American languages from a recent study (Ebrahimi et al., 2022) was represented in our survey. That said, we host http: //multilingual-dataset-survey.github.io where researchers can submit their dataset information and periodically update our analysis. Furthermore, we constantly encountered poorly written documentation or unavailable datasets during our annotation processes. During annotation, whenever this paper's dataset annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories. If no evidence could be found for the inference, they put ""not mentioned"" as a result. All unclear decision were adjudicated by at least three annotators.

Using Country Names as a Proxy for Languages Spoken. In Section 5, we attempted to approximate the number of NLP researchers with language proficiency in different languages. To do this, we mapped the names of ACL submission country to the most commonly spoken languages in those countries. We acknowledge that (1) the country of origin of researchers might be different from the country of submission, (2) researchers native language might not be listed in the commonly spoken languages and (3) the mapping might be incomprehensive.

MTurk Pilot Study. Due to our limited data points, although our MTurk study showed that data quality could be improved if the language qualification were applied in the collection process on MTurk, and our previous recommendations do not currently apply, we acknowledge that more research at scale should be done to statistically confirm the conclusion. Furthermore, languages other than the supported 5 languages might still be unsuitable for gathering multilingual data on MTurk As we identify that many datasets use texts beyond Wikipedia (see Table 4), we instead use the mC4 corpora (Xue et al., 2021), a much larger collection of texts in 101 languages drawn from the public Common Crawl web scrape and used for training the mT5 model. Specifically, we use the number of tokens in mC4 to estimate the amount of unlabeled data. Figure 7a shows a scatter plot where the x-axis represents the number of tokens in mC4 and y-axis represents the number of labeled datasets available in the languages. The availability of unlabeled text corpora and the number of labeled datasets show a high correlation (ρ = 0.794). We also analyze the relationship between the number of labeled datasets and the number of Wikipedia articles in Figure 7b. Again we observe a high correlation of ρ = 0.767. Classification (sentiment analysis) annotated (authors, linguists) collected from Wikipedia collected from curated source (exams, scientific papers, etc) collected from media (news) collected from social media or commercial sources collected from web crowdsourced curated linguistic resources multiple modes template-based Figure 6: The distribution over input source per task type. The size of bar represents the # of datasets of that task type and input source.

(a) The number of tokens (billions) in mC4 (Xue et al., 2021).

(b) The number of Wikipedia articles. Figure 7: Relationships between the number of datasets and number of tokens in mC4 and Wikipedia articles. English is removed.","(p15.0) On Dataset Documentation. Throughout our survey process, we found inadequate dataset documentation, limiting the coverage of our survey. We suggest that individual researchers provide the input data source and the labeling methodology; if people were involved in dataset creation, their demographic information should be provided, as well. Such information can help researchers analyze potential bias embedded in the dataset (Bender and Friedman, 2018).

(p15.1) Surveyed Dataset Collection Process. Despite our best efforts, we do not claim to cover all relevant datasets. Our collection process overlooks datasets that are published at non-ACL venues and not in Hugging Face as well as papers that do not match our search keywords. For instance, we missed multilingual machine reading comprehension datasets (Gupta and Khade, 2020;Asai et al., 2018) and morphology datasets (McCarthy et al., 2020). We also found a very low presence of indigenous language datasets. None of 10 indigenous American languages from a recent study (Ebrahimi et al., 2022) was represented in our survey. That said, we host http: //multilingual-dataset-survey.github.io where researchers can submit their dataset information and periodically update our analysis. Furthermore, we constantly encountered poorly written documentation or unavailable datasets during our annotation processes. During annotation, whenever this paper's dataset annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories. If no evidence could be found for the inference, they put ""not mentioned"" as a result. All unclear decision were adjudicated by at least three annotators.

(p15.2) Using Country Names as a Proxy for Languages Spoken. In Section 5, we attempted to approximate the number of NLP researchers with language proficiency in different languages. To do this, we mapped the names of ACL submission country to the most commonly spoken languages in those countries. We acknowledge that (1) the country of origin of researchers might be different from the country of submission, (2) researchers native language might not be listed in the commonly spoken languages and (3) the mapping might be incomprehensive.

(p15.3) MTurk Pilot Study. Due to our limited data points, although our MTurk study showed that data quality could be improved if the language qualification were applied in the collection process on MTurk, and our previous recommendations do not currently apply, we acknowledge that more research at scale should be done to statistically confirm the conclusion. Furthermore, languages other than the supported 5 languages might still be unsuitable for gathering multilingual data on MTurk As we identify that many datasets use texts beyond Wikipedia (see Table 4), we instead use the mC4 corpora (Xue et al., 2021), a much larger collection of texts in 101 languages drawn from the public Common Crawl web scrape and used for training the mT5 model. Specifically, we use the number of tokens in mC4 to estimate the amount of unlabeled data. Figure 7a shows a scatter plot where the x-axis represents the number of tokens in mC4 and y-axis represents the number of labeled datasets available in the languages. The availability of unlabeled text corpora and the number of labeled datasets show a high correlation (ρ = 0.794). We also analyze the relationship between the number of labeled datasets and the number of Wikipedia articles in Figure 7b. Again we observe a high correlation of ρ = 0.767. Classification (sentiment analysis) annotated (authors, linguists) collected from Wikipedia collected from curated source (exams, scientific papers, etc) collected from media (news) collected from social media or commercial sources collected from web crowdsourced curated linguistic resources multiple modes template-based Figure 6: The distribution over input source per task type. The size of bar represents the # of datasets of that task type and input source.

(p15.4) (a) The number of tokens (billions) in mC4 (Xue et al., 2021).

(p15.5) (b) The number of Wikipedia articles. Figure 7: Relationships between the number of datasets and number of tokens in mC4 and Wikipedia articles. English is removed.","[[None], [None], [], [None], [None], []]","[[None], [None], [], [None], [None], []]",4,"1. On Dataset Documentation. Throughout our survey process, we found inadequate dataset documentation, limiting the coverage of our survey.
2. We suggest that individual researchers provide the input data source and the labeling methodology; if people were involved in dataset creation, their demographic information should be provided, as well.
3. Such information can help researchers analyze potential bias embedded in the dataset (Bender and Friedman, 2018).Surveyed Dataset Collection Process.
4. Despite our best efforts, we do not claim to cover all relevant datasets.
5. Our collection process overlooks datasets that are published at non-ACL venues and not in Hugging Face as well as papers that do not match our search keywords.
6. For instance, we missed multilingual machine reading comprehension datasets (Gupta and Khade, 2020;Asai et al., 2018) and morphology datasets (McCarthy et al., 2020).
7. We also found a very low presence of indigenous language datasets.
8. None of 10 indigenous American languages from a recent study (Ebrahimi et al., 2022) was represented in our survey.
9. That said, we host http: //multilingual-dataset-survey.github.io where researchers can submit their dataset information and periodically update our analysis.
10. Furthermore, we constantly encountered poorly written documentation or unavailable datasets during our annotation processes.
11. During annotation, whenever this paper's dataset annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories.
12. If no evidence could be found for the inference, they put ""not mentioned"" as a result.
13. All unclear decision were adjudicated by at least three annotators.
14. Using Country Names as a Proxy for Languages Spoken.
15. In Section 5, we attempted to approximate the number of NLP researchers with language proficiency in different languages.
16. To do this, we mapped the names of ACL submission country to the most commonly spoken languages in those countries.
17. We acknowledge that (1) the country of origin of researchers might be different from the country of submission, (2) researchers native language might not be listed in the commonly spoken languages and (3) the mapping might be incomprehensive.
18. MTurk Pilot Study. Due to our limited data points, although our MTurk study showed that data quality could be improved if the language qualification were applied in the collection process on MTurk, and our previous recommendations do not currently apply, we acknowledge that more research at scale should be done to statistically confirm the conclusion.
19. Furthermore, languages other than the supported 5 languages might still be unsuitable for gathering multilingual data on MTurk As we identify that many datasets use texts beyond Wikipedia (see Table 4), we instead use the mC4 corpora (Xue et al., 2021), a much larger collection of texts in 101 languages drawn from the public Common Crawl web scrape and used for training the mT5 model.
20. Specifically, we use the number of tokens in mC4 to estimate the amount of unlabeled data.
21. Figure 7a shows a scatter plot where the x-axis represents the number of tokens in mC4 and y-axis represents the number of labeled datasets available in the languages.
22. The availability of unlabeled text corpora and the number of labeled datasets show a high correlation (ρ = 0.794).
23. We also analyze the relationship between the number of labeled datasets and the number of Wikipedia articles in Figure 7b.
24. Again we observe a high correlation of ρ = 0.767.
25. Classification (sentiment analysis) annotated (authors, linguists) collected from Wikipedia collected from curated source (exams, scientific papers, etc) collected from media (news) collected from social media or commercial sources collected from web crowdsourced curated linguistic resources multiple modes template-based Figure 6: The distribution over input source per task type.
26. The size of bar represents the # of datasets of that task type and input source.
27. (a) The number of tokens (billions) in mC4 (Xue et al., 2021).
28. (b) The number of Wikipedia articles.
29. Figure 7: Relationships between the number of datasets and number of tokens in mC4 and Wikipedia articles.
30. English is removed."
254043519,Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/32cdcf593fef78dcf9e1b6aba7f2345310d7cc60,s12,Suggestions for the NLP Community,"['p12.0', 'p12.1']","['To Foster Language-proficient Researchers and Community Efforts. Our analysis shows that the availability of NLP researchers who are fluent in languages highly correlates with the availability of datasets. Moreover, monolingual test suites cover only 16 languages, such as Chinese (Xu et al., 2020), Indic Languages (Kakwani et al., 2020), Polish (Rybak et al., 2020), Persian (Khashabi et al., 2021), Russian (Shavrina et al., 2020) or Arabic (Seelawi et al., 2021), where efforts are driven by language-proficient NLP researchers. Organizing these large-scale, inter-organization efforts can be challenging but have profound effects. Recent community efforts such as Masakhane * spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021). Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation. In the long run, globalized NLP education like AFIRM * will be necessary. A directory of potential funding sources to support multlingual data collection can also be helpful.', 'On Inclusive Venues. The academic publication/conference reviewing system should also reward efforts to develop language-specific resources, without perceiving this as a niche, low-impact effort (Rogers et al., 2022). As a community, we should encourage efforts to create and provide region-specific (e.g., Nordic Conference on Computational Linguistics, Pacific Asia Conference on Language, Information and Computation), language-oriented (e.g., Deep Learning for Low-Resource NLP, AfricaNLP, Workshop on Indian Language Data: Resources and Evaluation), and data-oriented (e.g., NeurIPS dataset and benchmark track) venues for introducing multilingual datasets. Adapting existing systems to new and low-resource languages poses a challenging and intriguing task as well as substantial research inquiries. The community should continue supporting such efforts and expand evaluation data for diverse target languages.']","To Foster Language-proficient Researchers and Community Efforts. Our analysis shows that the availability of NLP researchers who are fluent in languages highly correlates with the availability of datasets. Moreover, monolingual test suites cover only 16 languages, such as Chinese (Xu et al., 2020), Indic Languages (Kakwani et al., 2020), Polish (Rybak et al., 2020), Persian (Khashabi et al., 2021), Russian (Shavrina et al., 2020) or Arabic (Seelawi et al., 2021), where efforts are driven by language-proficient NLP researchers. Organizing these large-scale, inter-organization efforts can be challenging but have profound effects. Recent community efforts such as Masakhane * spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021). Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation. In the long run, globalized NLP education like AFIRM * will be necessary. A directory of potential funding sources to support multlingual data collection can also be helpful.

On Inclusive Venues. The academic publication/conference reviewing system should also reward efforts to develop language-specific resources, without perceiving this as a niche, low-impact effort (Rogers et al., 2022). As a community, we should encourage efforts to create and provide region-specific (e.g., Nordic Conference on Computational Linguistics, Pacific Asia Conference on Language, Information and Computation), language-oriented (e.g., Deep Learning for Low-Resource NLP, AfricaNLP, Workshop on Indian Language Data: Resources and Evaluation), and data-oriented (e.g., NeurIPS dataset and benchmark track) venues for introducing multilingual datasets. Adapting existing systems to new and low-resource languages poses a challenging and intriguing task as well as substantial research inquiries. The community should continue supporting such efforts and expand evaluation data for diverse target languages.","(p12.0) To Foster Language-proficient Researchers and Community Efforts. Our analysis shows that the availability of NLP researchers who are fluent in languages highly correlates with the availability of datasets. Moreover, monolingual test suites cover only 16 languages, such as Chinese (Xu et al., 2020), Indic Languages (Kakwani et al., 2020), Polish (Rybak et al., 2020), Persian (Khashabi et al., 2021), Russian (Shavrina et al., 2020) or Arabic (Seelawi et al., 2021), where efforts are driven by language-proficient NLP researchers. Organizing these large-scale, inter-organization efforts can be challenging but have profound effects. Recent community efforts such as Masakhane * spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021). Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation. In the long run, globalized NLP education like AFIRM * will be necessary. A directory of potential funding sources to support multlingual data collection can also be helpful.

(p12.1) On Inclusive Venues. The academic publication/conference reviewing system should also reward efforts to develop language-specific resources, without perceiving this as a niche, low-impact effort (Rogers et al., 2022). As a community, we should encourage efforts to create and provide region-specific (e.g., Nordic Conference on Computational Linguistics, Pacific Asia Conference on Language, Information and Computation), language-oriented (e.g., Deep Learning for Low-Resource NLP, AfricaNLP, Workshop on Indian Language Data: Resources and Evaluation), and data-oriented (e.g., NeurIPS dataset and benchmark track) venues for introducing multilingual datasets. Adapting existing systems to new and low-resource languages poses a challenging and intriguing task as well as substantial research inquiries. The community should continue supporting such efforts and expand evaluation data for diverse target languages.","[[None, 'b12', 'b10', 'b15'], ['b9']]","[[None, 'b12', 'b10', 'b15'], ['b9']]",5,"1. To Foster Language-proficient Researchers and Community Efforts.
2. Our analysis shows that the availability of NLP researchers who are fluent in languages highly correlates with the availability of datasets.
3. Moreover, monolingual test suites cover only 16 languages, such as Chinese (Xu et al., 2020), Indic Languages (Kakwani et al., 2020), Polish (Rybak et al., 2020), Persian (Khashabi et al., 2021), Russian (Shavrina et al., 2020) or Arabic (Seelawi et al., 2021), where efforts are driven by language-proficient NLP researchers.
4. Organizing these large-scale, inter-organization efforts can be challenging but have profound effects.
5. Recent community efforts such as Masakhane * spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021).
6. Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation.
7. In the long run, globalized NLP education like AFIRM * will be necessary.
8. A directory of potential funding sources to support multlingual data collection can also be helpful.
9. On Inclusive Venues. The academic publication/conference reviewing system should also reward efforts to develop language-specific resources, without perceiving this as a niche, low-impact effort (Rogers et al., 2022).
10. As a community, we should encourage efforts to create and provide region-specific (e.g., Nordic Conference on Computational Linguistics, Pacific Asia Conference on Language, Information and Computation), language-oriented (e.g., Deep Learning for Low-Resource NLP, AfricaNLP, Workshop on Indian Language Data: Resources and Evaluation), and data-oriented (e.g., NeurIPS dataset and benchmark track) venues for introducing multilingual datasets.
11. Adapting existing systems to new and low-resource languages poses a challenging and intriguing task as well as substantial research inquiries.
12. The community should continue supporting such efforts and expand evaluation data for diverse target languages."
254854317,The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/44ebfdb670007b3949507be0d1a1fca93bc3d5d5,s11,Linguistic-Driven Approaches,"['p11.0', 'p11.1', 'p11.2']","['Equivalence Constraint In a well-formed codeswitched sentence, the switching takes place at those points where the grammatical constraints of both languages are satisfied (Poplack, 1980). Fung (2012, 2013) incorporate this syntactic constraint to a statistical code-switch language model (LM) and evaluate the model on Chinese-English code-switched speech recognition. On the same line of work, Pratapa et al. (2018a); Pratapa and Choudhury (2021) implement the same constraint to Hindi-English CSW data by producing parse trees of parallel sentences and matching the surface order of child nodes in the trees. Winata et al. (2019c) apply the constraint to generate synthetic CSW text and find that combining the real CSW data with synthetic CSW data can effectively improve the perplexity. They also treat parallel sentences as a linear structure and only allow switching on non-crossing alignments.', 'Matrix-Embedded Language Framework (MLF) Myers-Scotton (1997) proposed that in bilingual CSW, there exists an asymmetrical relationship between the dominant matrix language and the subordinate embedded language.', 'Matrix language provides the frame of the sentence by governing all or most of the most of the grammatical morphemes as well as word order, whereas syntactic elements that bear no or only limited grammatical function can be provided by the embedded language (Johanson, 1999;Myers-Scotton, 2005 Functional Head Constraint Belazi et al. (1994) posit that it is impossible to switch languages between a functional head and its complement because of the strong relationship between the two constituents. Li and Fung (2014) use the constraint of the LM by first expanding the search network with a translation model and then using parsing to restrict paths to those permissible under the constraint.']","Equivalence Constraint In a well-formed codeswitched sentence, the switching takes place at those points where the grammatical constraints of both languages are satisfied (Poplack, 1980). Fung (2012, 2013) incorporate this syntactic constraint to a statistical code-switch language model (LM) and evaluate the model on Chinese-English code-switched speech recognition. On the same line of work, Pratapa et al. (2018a); Pratapa and Choudhury (2021) implement the same constraint to Hindi-English CSW data by producing parse trees of parallel sentences and matching the surface order of child nodes in the trees. Winata et al. (2019c) apply the constraint to generate synthetic CSW text and find that combining the real CSW data with synthetic CSW data can effectively improve the perplexity. They also treat parallel sentences as a linear structure and only allow switching on non-crossing alignments.

Matrix-Embedded Language Framework (MLF) Myers-Scotton (1997) proposed that in bilingual CSW, there exists an asymmetrical relationship between the dominant matrix language and the subordinate embedded language.

Matrix language provides the frame of the sentence by governing all or most of the most of the grammatical morphemes as well as word order, whereas syntactic elements that bear no or only limited grammatical function can be provided by the embedded language (Johanson, 1999;Myers-Scotton, 2005 Functional Head Constraint Belazi et al. (1994) posit that it is impossible to switch languages between a functional head and its complement because of the strong relationship between the two constituents. Li and Fung (2014) use the constraint of the LM by first expanding the search network with a translation model and then using parsing to restrict paths to those permissible under the constraint.","(p11.0) Equivalence Constraint In a well-formed codeswitched sentence, the switching takes place at those points where the grammatical constraints of both languages are satisfied (Poplack, 1980). Fung (2012, 2013) incorporate this syntactic constraint to a statistical code-switch language model (LM) and evaluate the model on Chinese-English code-switched speech recognition. On the same line of work, Pratapa et al. (2018a); Pratapa and Choudhury (2021) implement the same constraint to Hindi-English CSW data by producing parse trees of parallel sentences and matching the surface order of child nodes in the trees. Winata et al. (2019c) apply the constraint to generate synthetic CSW text and find that combining the real CSW data with synthetic CSW data can effectively improve the perplexity. They also treat parallel sentences as a linear structure and only allow switching on non-crossing alignments.

(p11.1) Matrix-Embedded Language Framework (MLF) Myers-Scotton (1997) proposed that in bilingual CSW, there exists an asymmetrical relationship between the dominant matrix language and the subordinate embedded language.

(p11.2) Matrix language provides the frame of the sentence by governing all or most of the most of the grammatical morphemes as well as word order, whereas syntactic elements that bear no or only limited grammatical function can be provided by the embedded language (Johanson, 1999;Myers-Scotton, 2005 Functional Head Constraint Belazi et al. (1994) posit that it is impossible to switch languages between a functional head and its complement because of the strong relationship between the two constituents. Li and Fung (2014) use the constraint of the LM by first expanding the search network with a translation model and then using parsing to restrict paths to those permissible under the constraint.","[[None, 'b18'], [None], [None]]","[[None, 'b18'], [None], [None]]",4,"1. Equivalence Constraint In a well-formed codeswitched sentence, the switching takes place at those points where the grammatical constraints of both languages are satisfied (Poplack, 1980).
2. Fung (2012, 2013) incorporate this syntactic constraint to a statistical code-switch language model (LM) and evaluate the model on Chinese-English code-switched speech recognition.
3. On the same line of work, Pratapa et al. (2018a); Pratapa and Choudhury (2021) implement the same constraint to Hindi-English CSW data by producing parse trees of parallel sentences and matching the surface order of child nodes in the trees.
4. Winata et al. (2019c) apply the constraint to generate synthetic CSW text and find that combining the real CSW data with synthetic CSW data can effectively improve the perplexity.
5. They also treat parallel sentences as a linear structure and only allow switching on non-crossing alignments.
6. Matrix-Embedded Language Framework (MLF) Myers-Scotton (1997) proposed that in bilingual CSW, there exists an asymmetrical relationship between the dominant matrix language and the subordinate embedded language.
7. Matrix language provides the frame of the sentence by governing all or most of the most of the grammatical morphemes as well as word order, whereas syntactic elements that bear no or only limited grammatical function can be provided by the embedded language
8. (Johanson, 1999;Myers-Scotton, 2005 Functional Head Constraint Belazi et al. (1994) posit that it is impossible to switch languages between a functional head and its complement because of the strong relationship between the two constituents.
9. Li and Fung (2014) use the constraint of the LM by first expanding the search network with a translation model and then using parsing to restrict paths to those permissible under the constraint."
254854317,The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/44ebfdb670007b3949507be0d1a1fca93bc3d5d5,s14,Utilizing Neural Networks,"['p14.0', 'p14.1', 'p14.2', 'p14.3']","['Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time. In contrast, the statistical and rule-based approaches are diminishing. Compared to ISCA, we see more adaptation of the pre-training model. This is because ACL work is more text-based focused, where pre-trained LMs are more widely available.', 'Neural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage. RNN and LSTM architectures are commonly used in sequence modeling, such as language modeling (Adel et al., 2013;Vu and Schultz, 2014;Adel et al., 2014c;Winata et al., 2018a;Garg et al., 2018a;Winata et al., 2019c) and CSW identification (Samih et al., 2016a). DNN-based and hybrid HMM-DNN models are used in speech recognition models .', 'Pre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer. Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017). A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018). A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b). Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021). Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b). In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.', 'Language Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a). These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages. Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets. Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text. Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context. While XLM-R provides the best result, it is also computationally heavy. There needed to be more exploration of larger models. We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach. Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available. However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations. CSW style can vary in different regions of the world, and it would be interesting to gather more datasets on unexplored and unknown styles, which can be useful for further research and investigation on linguistics and NLP. Therefore, one future direction is to broaden the language scope of CSW research.']","Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time. In contrast, the statistical and rule-based approaches are diminishing. Compared to ISCA, we see more adaptation of the pre-training model. This is because ACL work is more text-based focused, where pre-trained LMs are more widely available.

Neural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage. RNN and LSTM architectures are commonly used in sequence modeling, such as language modeling (Adel et al., 2013;Vu and Schultz, 2014;Adel et al., 2014c;Winata et al., 2018a;Garg et al., 2018a;Winata et al., 2019c) and CSW identification (Samih et al., 2016a). DNN-based and hybrid HMM-DNN models are used in speech recognition models .

Pre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer. Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017). A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018). A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b). Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021). Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b). In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.

Language Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a). These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages. Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets. Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text. Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context. While XLM-R provides the best result, it is also computationally heavy. There needed to be more exploration of larger models. We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach. Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available. However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations. CSW style can vary in different regions of the world, and it would be interesting to gather more datasets on unexplored and unknown styles, which can be useful for further research and investigation on linguistics and NLP. Therefore, one future direction is to broaden the language scope of CSW research.","(p14.0) Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time. In contrast, the statistical and rule-based approaches are diminishing. Compared to ISCA, we see more adaptation of the pre-training model. This is because ACL work is more text-based focused, where pre-trained LMs are more widely available.

(p14.1) Neural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage. RNN and LSTM architectures are commonly used in sequence modeling, such as language modeling (Adel et al., 2013;Vu and Schultz, 2014;Adel et al., 2014c;Winata et al., 2018a;Garg et al., 2018a;Winata et al., 2019c) and CSW identification (Samih et al., 2016a). DNN-based and hybrid HMM-DNN models are used in speech recognition models .

(p14.2) Pre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer. Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017). A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018). A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b). Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021). Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b). In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.

(p14.3) Language Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a). These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages. Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets. Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text. Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context. While XLM-R provides the best result, it is also computationally heavy. There needed to be more exploration of larger models. We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach. Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available. However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations. CSW style can vary in different regions of the world, and it would be interesting to gather more datasets on unexplored and unknown styles, which can be useful for further research and investigation on linguistics and NLP. Therefore, one future direction is to broaden the language scope of CSW research.","[[], [None, 'b18', 'b17'], [None, 'b19'], ['b13', None]]","[[], [None, 'b18', 'b17'], [None, 'b19'], ['b13', None]]",7,"1. Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time.
2. In contrast, the statistical and rule-based approaches are diminishing.
3. Compared to ISCA, we see more adaptation of the pre-training model.
4. This is because ACL work is more text-based focused, where pre-trained LMs are more widely available.
5. Neural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage.
6. RNN and LSTM architectures are commonly used in sequence modeling, such as language modeling (Adel et al., 2013;Vu and Schultz, 2014;Adel et al., 2014c;Winata et al., 2018a;Garg et al., 2018a;Winata et al., 2019c) and CSW identification (Samih et al., 2016a).
7. DNN-based and hybrid HMM-DNN models are used in speech recognition models .
8. Pre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer.
9. Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017).
10. A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018).
11. A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b).
12. Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021).
13. Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b).
14. In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.
15. Language Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a).
16. These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages.
17. Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets.
18. Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text.
19. Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context.
20. While XLM-R provides the best result, it is also computationally heavy.
21. There needed to be more exploration of larger models.
22. We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach.
23. Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available.
24. However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations.
25. CSW style can vary in different regions of the world, and it would be interesting to gather more datasets on unexplored and unknown styles, which can be useful for further research and investigation on linguistics and NLP.
26. Therefore, one future direction is to broaden the language scope of CSW research."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,Computer Science,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,s3,Negotiation in Human,"['p3.0', 'p3.1']","['Humans negotiate everyday in their daily routines. Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011;Lewicki et al., 2011). Research on negotiation has been conducted for almost 60 years in the field of psychology, political science, and communication. It has evolved over the past decades from exploring game theory (Walton and McKersie, 1991), behavior decisions driven by the cognitive revolution in psychology (Bazerman and Neale, 1993), to cultural differences in the 2000s (Bazerman et al., 2000). Negotiation research, however, is now forced to confront the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010). Converging efforts from social scientists and data scientists who incorporate insights from both fields will be fruitful in maximizing expectations and outcomes during negotiation processes.', 'Negotiation is a process by which two or more parties attempt to resolve their opposing interests. Strategy of negotiation can be distributive, such as bargaining (Fershtman, 1990) and integrative, such as maximizing unilateral interests (Bazerman and Neale, 1993), both of which are used in various social situations such as informal, peer to peer, organizational, and diplomatic country to country settings. The implications for enhancing outcomes are thus large and important to understand. Research from psychology demonstrates that the negotiation process can be affected by psychological factors, such as personality (Sharma et al., 2013), relationship (Olekalns and Smith, 2003), social status (Blader and Chen, 2012), and cultural background (Leung and Cohen, 2011).']","Humans negotiate everyday in their daily routines. Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011;Lewicki et al., 2011). Research on negotiation has been conducted for almost 60 years in the field of psychology, political science, and communication. It has evolved over the past decades from exploring game theory (Walton and McKersie, 1991), behavior decisions driven by the cognitive revolution in psychology (Bazerman and Neale, 1993), to cultural differences in the 2000s (Bazerman et al., 2000). Negotiation research, however, is now forced to confront the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010). Converging efforts from social scientists and data scientists who incorporate insights from both fields will be fruitful in maximizing expectations and outcomes during negotiation processes.

Negotiation is a process by which two or more parties attempt to resolve their opposing interests. Strategy of negotiation can be distributive, such as bargaining (Fershtman, 1990) and integrative, such as maximizing unilateral interests (Bazerman and Neale, 1993), both of which are used in various social situations such as informal, peer to peer, organizational, and diplomatic country to country settings. The implications for enhancing outcomes are thus large and important to understand. Research from psychology demonstrates that the negotiation process can be affected by psychological factors, such as personality (Sharma et al., 2013), relationship (Olekalns and Smith, 2003), social status (Blader and Chen, 2012), and cultural background (Leung and Cohen, 2011).","(p3.0) Humans negotiate everyday in their daily routines. Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011;Lewicki et al., 2011). Research on negotiation has been conducted for almost 60 years in the field of psychology, political science, and communication. It has evolved over the past decades from exploring game theory (Walton and McKersie, 1991), behavior decisions driven by the cognitive revolution in psychology (Bazerman and Neale, 1993), to cultural differences in the 2000s (Bazerman et al., 2000). Negotiation research, however, is now forced to confront the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010). Converging efforts from social scientists and data scientists who incorporate insights from both fields will be fruitful in maximizing expectations and outcomes during negotiation processes.

(p3.1) Negotiation is a process by which two or more parties attempt to resolve their opposing interests. Strategy of negotiation can be distributive, such as bargaining (Fershtman, 1990) and integrative, such as maximizing unilateral interests (Bazerman and Neale, 1993), both of which are used in various social situations such as informal, peer to peer, organizational, and diplomatic country to country settings. The implications for enhancing outcomes are thus large and important to understand. Research from psychology demonstrates that the negotiation process can be affected by psychological factors, such as personality (Sharma et al., 2013), relationship (Olekalns and Smith, 2003), social status (Blader and Chen, 2012), and cultural background (Leung and Cohen, 2011).","[['b3', 'b18', 'b33', 'b4', 'b47', 'b51'], ['b43', 'b4', 'b42', 'b17']]","[['b3', 'b18', 'b33', 'b4', 'b47', 'b51'], ['b43', 'b4', 'b42', 'b17']]",10,"1. Humans negotiate everyday in their daily routines.
2. Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011;Lewicki et al., 2011).
3. Research on negotiation has been conducted for almost 60 years in the field of psychology, political science, and communication.
4. It has evolved over the past decades from exploring game theory (Walton and McKersie, 1991), behavior decisions driven by the cognitive revolution in psychology (Bazerman and Neale, 1993), to cultural differences in the 2000s (Bazerman et al., 2000).
5. Negotiation research, however, is now forced to confront the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010).
6. Converging efforts from social scientists and data scientists who incorporate insights from both fields will be fruitful in maximizing expectations and outcomes during negotiation processes.
7. Negotiation is a process by which two or more parties attempt to resolve their opposing interests.
8. Strategy of negotiation can be distributive, such as bargaining (Fershtman, 1990) and integrative, such as maximizing unilateral interests (Bazerman and Neale, 1993), both of which are used in various social situations such as informal, peer to peer, organizational, and diplomatic country to country settings.
9. The implications for enhancing outcomes are thus large and important to understand.
10. Research from psychology demonstrates that the negotiation process can be affected by psychological factors, such as personality (Sharma et al., 2013), relationship (Olekalns and Smith, 2003), social status (Blader and Chen, 2012), and cultural background (Leung and Cohen, 2011)."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,Computer Science,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,s23,Supervised Learning,['p23.0'],"[""Supervised learning (SL) is another popular paradigm for policy learning. (Lewis et al., 2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data. However, supervised learning only aims to mimic the average human behavior, so He et al. (2018) propose to finetune the supervised model to directly optimize for a particular dialogue reward function, which is defined as i) the utility function of the final price for the buyer and seller ii) the difference between two agents' utilities iii) the number of utterances in the dialogue. Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training. The system response would be generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy. In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm. Recently, Dutt et al. (2021b) propose a generalised framework for identifying resisting strategies in persuasive negotiations using a pre-trained BERT model (Devlin et al., 2019).""]","Supervised learning (SL) is another popular paradigm for policy learning. (Lewis et al., 2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data. However, supervised learning only aims to mimic the average human behavior, so He et al. (2018) propose to finetune the supervised model to directly optimize for a particular dialogue reward function, which is defined as i) the utility function of the final price for the buyer and seller ii) the difference between two agents' utilities iii) the number of utterances in the dialogue. Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training. The system response would be generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy. In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm. Recently, Dutt et al. (2021b) propose a generalised framework for identifying resisting strategies in persuasive negotiations using a pre-trained BERT model (Devlin et al., 2019).","(p23.0) Supervised learning (SL) is another popular paradigm for policy learning. (Lewis et al., 2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data. However, supervised learning only aims to mimic the average human behavior, so He et al. (2018) propose to finetune the supervised model to directly optimize for a particular dialogue reward function, which is defined as i) the utility function of the final price for the buyer and seller ii) the difference between two agents' utilities iii) the number of utterances in the dialogue. Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training. The system response would be generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy. In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm. Recently, Dutt et al. (2021b) propose a generalised framework for identifying resisting strategies in persuasive negotiations using a pre-trained BERT model (Devlin et al., 2019).","[['b34', 'b26', None, 'b12', 'b24', 'b59']]","[['b34', 'b26', None, 'b12', 'b24', 'b59']]",6,"1. Supervised learning (SL) is another popular paradigm for policy learning.
2. (Lewis et al., 2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data.
3. However, supervised learning only aims to mimic the average human behavior, so He et al. (2018) propose to finetune the supervised model to directly optimize for a particular dialogue reward function, which is defined as i) the utility function of the final price for the buyer and seller ii) the difference between two agents' utilities iii) the number of utterances in the dialogue.
4. Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training.
5. The system response would be generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy.
6. In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm.
7. Recently, Dutt et al. (2021b) propose a generalised framework for identifying resisting strategies in persuasive negotiations using a pre-trained BERT model (Devlin et al., 2019)."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,Computer Science,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,s22,Reinforcement Learning,"['p22.0', 'p22.1']","['Reinforcement learning (RL) is one of the most common frameworks chosen for policy learning. English and Heeman (2005) are the first to use RL techniques for negotiation dialogue systems. They employed a single-agent pattern to learn the policy of two opponents individually. But singleagent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment. Therefore, Georgila et al. (2014) further advances the framework with concurrent progress using multi-agent RL techniques, which simultaneously model two parties and provide a way to deal with multi-issues scenarios. Besides, Keizer et al. (2017) propose to learn the action of the target agents with a Qlearning reward function. They further propose a method based on hand-crafted rules and a method using Random Forest trained on a large human negotiation corpus from (Afantenos et al., 2012).', ""Most recent works try to equip RL with deep learning techniques. For instance, Zhang et al. (2020) propose OPPA, which lets the target agent behave given the system actions. The system actions are predicted and conditioned on the target agent's actions. The reward of the actions for the target agent is obtained by predicting a structured output given the whole dialogue. Besides, Shi et al. (2021) use a modular framework containing a language model to generate responses, a response detector would automatically annotate the response with a negotiation strategy, and an RL-based reward function to assign a score to the strategy. Instead of the modular framework which separates policy learning and response generation, Gao et al. (2021) propose an integrated framework with deep Q-learning, which includes multiple channel negotiation skills. It allows agents to leverage parameterized DQN to learn a comprehensive negotiation strategy that integrates linguistic communication skills and bidding strategies.""]","Reinforcement learning (RL) is one of the most common frameworks chosen for policy learning. English and Heeman (2005) are the first to use RL techniques for negotiation dialogue systems. They employed a single-agent pattern to learn the policy of two opponents individually. But singleagent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment. Therefore, Georgila et al. (2014) further advances the framework with concurrent progress using multi-agent RL techniques, which simultaneously model two parties and provide a way to deal with multi-issues scenarios. Besides, Keizer et al. (2017) propose to learn the action of the target agents with a Qlearning reward function. They further propose a method based on hand-crafted rules and a method using Random Forest trained on a large human negotiation corpus from (Afantenos et al., 2012).

Most recent works try to equip RL with deep learning techniques. For instance, Zhang et al. (2020) propose OPPA, which lets the target agent behave given the system actions. The system actions are predicted and conditioned on the target agent's actions. The reward of the actions for the target agent is obtained by predicting a structured output given the whole dialogue. Besides, Shi et al. (2021) use a modular framework containing a language model to generate responses, a response detector would automatically annotate the response with a negotiation strategy, and an RL-based reward function to assign a score to the strategy. Instead of the modular framework which separates policy learning and response generation, Gao et al. (2021) propose an integrated framework with deep Q-learning, which includes multiple channel negotiation skills. It allows agents to leverage parameterized DQN to learn a comprehensive negotiation strategy that integrates linguistic communication skills and bidding strategies.","(p22.0) Reinforcement learning (RL) is one of the most common frameworks chosen for policy learning. English and Heeman (2005) are the first to use RL techniques for negotiation dialogue systems. They employed a single-agent pattern to learn the policy of two opponents individually. But singleagent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment. Therefore, Georgila et al. (2014) further advances the framework with concurrent progress using multi-agent RL techniques, which simultaneously model two parties and provide a way to deal with multi-issues scenarios. Besides, Keizer et al. (2017) propose to learn the action of the target agents with a Qlearning reward function. They further propose a method based on hand-crafted rules and a method using Random Forest trained on a large human negotiation corpus from (Afantenos et al., 2012).

(p22.1) Most recent works try to equip RL with deep learning techniques. For instance, Zhang et al. (2020) propose OPPA, which lets the target agent behave given the system actions. The system actions are predicted and conditioned on the target agent's actions. The reward of the actions for the target agent is obtained by predicting a structured output given the whole dialogue. Besides, Shi et al. (2021) use a modular framework containing a language model to generate responses, a response detector would automatically annotate the response with a negotiation strategy, and an RL-based reward function to assign a score to the strategy. Instead of the modular framework which separates policy learning and response generation, Gao et al. (2021) propose an integrated framework with deep Q-learning, which includes multiple channel negotiation skills. It allows agents to leverage parameterized DQN to learn a comprehensive negotiation strategy that integrates linguistic communication skills and bidding strategies.","[['b22', 'b0', 'b16', 'b27'], ['b21', 'b44', 'b56']]","[['b22', 'b0', 'b16', 'b27'], ['b21', 'b44', 'b56']]",7,"1. Reinforcement learning (RL) is one of the most common frameworks chosen for policy learning.
2. English and Heeman (2005) are the first to use RL techniques for negotiation dialogue systems.
3. They employed a single-agent pattern to learn the policy of two opponents individually.
4. But singleagent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment.
5. Therefore, Georgila et al. (2014) further advances the framework with concurrent progress using multi-agent RL techniques, which simultaneously model two parties and provide a way to deal with multi-issues scenarios.
6. Besides, Keizer et al. (2017) propose to learn the action of the target agents with a Qlearning reward function.
7. They further propose a method based on hand-crafted rules and a method using Random Forest trained on a large human negotiation corpus from (Afantenos et al., 2012).
8. Most recent works try to equip RL with deep learning techniques.
9. For instance, Zhang et al. (2020) propose OPPA, which lets the target agent behave given the system actions.
10. The system actions are predicted and conditioned on the target agent's actions.
11. The reward of the actions for the target agent is obtained by predicting a structured output given the whole dialogue.
12. Besides, Shi et al. (2021) use a modular framework containing a language model to generate responses, a response detector would automatically annotate the response with a negotiation strategy, and an RL-based reward function to assign a score to the strategy.
13. Instead of the modular framework which separates policy learning and response generation, Gao et al. (2021) propose an integrated framework with deep Q-learning, which includes multiple channel negotiation skills.
14. It allows agents to leverage parameterized DQN to learn a comprehensive negotiation strategy that integrates linguistic communication skills and bidding strategies."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,Computer Science,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,s6,Integrative Negotiation Datasets,"['p6.0', 'p6.1', 'p6.2', 'p6.3']","['In integrative negotiations, there is normally more than one issue available to be negotiated. To achieve optimal negotiation goals, the involved players should make trade-offs for multiple issues.', 'Multi-player Strategy Games The strategy video games provide ideal platforms for people to verbally communicate with other players to accomplish their missions and goals. Asher et al. (2016) propose the STAC benchmark, which is the player dialogue in the game of Catan. In this game, players need to gather resources, including wood, wheat, sheep, and more, with each other to purchase settlements, roads and cities. As each player only has access to their own resources, they have to communicate with each other. To investigate the linguistic strategies used in this situation, STAC also includes an SDRT-styled discourse structure. Boritchev and Amblard (2022) also collect a DinG dataset from French-speaking players in this game. The participants are instructed to focus on the game, rather than talk about themselves. As a result, the collected dialogues can better reflect the negotiation strategy used in the game process.', 'Negotiation for Item Assignment The item assignment scenarios involve a fixed set of items as well as a predefined priority for each player in the dialogue. As the players only have access to their own priority, they need to negotiate with each other to exchange the items they prefer. Nouri and Traum (2014) propose InitiativeTalking, occurring between the owners of two restaurants. They discuss how to distribute the fruits (i.e., apples, bananas, and strawberries) and try to reach an agreement. Lewis et al. (2017) propose DealorNoDeal, a similar two-party negotiation dialogue benchmark where both participants are only shown their own sets of items with a value for each and both of them are asked to maximize their total score after negotiation. Chawla et al. (2021c) propose CaSiNo, a dataset on campsite scenarios involving campsite neighbors negotiating for additional food, water, and firewood packages. Both parties have different priorities over different items.', ""Negotiation for Job Interview Another commonly encountered negotiation scenario is job offer negotiation with recruiters. Yamaguchi et al. (2021a) fill this gap and propose the JobInterview dataset. JobInterview includes recruiter-applicant interactions over salary, day off, position, company, and workplace. The participants are shown negotiators' preferences and the corresponding issues and options and are given feedback in the middle of the negotiation.""]","In integrative negotiations, there is normally more than one issue available to be negotiated. To achieve optimal negotiation goals, the involved players should make trade-offs for multiple issues.

Multi-player Strategy Games The strategy video games provide ideal platforms for people to verbally communicate with other players to accomplish their missions and goals. Asher et al. (2016) propose the STAC benchmark, which is the player dialogue in the game of Catan. In this game, players need to gather resources, including wood, wheat, sheep, and more, with each other to purchase settlements, roads and cities. As each player only has access to their own resources, they have to communicate with each other. To investigate the linguistic strategies used in this situation, STAC also includes an SDRT-styled discourse structure. Boritchev and Amblard (2022) also collect a DinG dataset from French-speaking players in this game. The participants are instructed to focus on the game, rather than talk about themselves. As a result, the collected dialogues can better reflect the negotiation strategy used in the game process.

Negotiation for Item Assignment The item assignment scenarios involve a fixed set of items as well as a predefined priority for each player in the dialogue. As the players only have access to their own priority, they need to negotiate with each other to exchange the items they prefer. Nouri and Traum (2014) propose InitiativeTalking, occurring between the owners of two restaurants. They discuss how to distribute the fruits (i.e., apples, bananas, and strawberries) and try to reach an agreement. Lewis et al. (2017) propose DealorNoDeal, a similar two-party negotiation dialogue benchmark where both participants are only shown their own sets of items with a value for each and both of them are asked to maximize their total score after negotiation. Chawla et al. (2021c) propose CaSiNo, a dataset on campsite scenarios involving campsite neighbors negotiating for additional food, water, and firewood packages. Both parties have different priorities over different items.

Negotiation for Job Interview Another commonly encountered negotiation scenario is job offer negotiation with recruiters. Yamaguchi et al. (2021a) fill this gap and propose the JobInterview dataset. JobInterview includes recruiter-applicant interactions over salary, day off, position, company, and workplace. The participants are shown negotiators' preferences and the corresponding issues and options and are given feedback in the middle of the negotiation.","(p6.0) In integrative negotiations, there is normally more than one issue available to be negotiated. To achieve optimal negotiation goals, the involved players should make trade-offs for multiple issues.

(p6.1) Multi-player Strategy Games The strategy video games provide ideal platforms for people to verbally communicate with other players to accomplish their missions and goals. Asher et al. (2016) propose the STAC benchmark, which is the player dialogue in the game of Catan. In this game, players need to gather resources, including wood, wheat, sheep, and more, with each other to purchase settlements, roads and cities. As each player only has access to their own resources, they have to communicate with each other. To investigate the linguistic strategies used in this situation, STAC also includes an SDRT-styled discourse structure. Boritchev and Amblard (2022) also collect a DinG dataset from French-speaking players in this game. The participants are instructed to focus on the game, rather than talk about themselves. As a result, the collected dialogues can better reflect the negotiation strategy used in the game process.

(p6.2) Negotiation for Item Assignment The item assignment scenarios involve a fixed set of items as well as a predefined priority for each player in the dialogue. As the players only have access to their own priority, they need to negotiate with each other to exchange the items they prefer. Nouri and Traum (2014) propose InitiativeTalking, occurring between the owners of two restaurants. They discuss how to distribute the fruits (i.e., apples, bananas, and strawberries) and try to reach an agreement. Lewis et al. (2017) propose DealorNoDeal, a similar two-party negotiation dialogue benchmark where both participants are only shown their own sets of items with a value for each and both of them are asked to maximize their total score after negotiation. Chawla et al. (2021c) propose CaSiNo, a dataset on campsite scenarios involving campsite neighbors negotiating for additional food, water, and firewood packages. Both parties have different priorities over different items.

(p6.3) Negotiation for Job Interview Another commonly encountered negotiation scenario is job offer negotiation with recruiters. Yamaguchi et al. (2021a) fill this gap and propose the JobInterview dataset. JobInterview includes recruiter-applicant interactions over salary, day off, position, company, and workplace. The participants are shown negotiators' preferences and the corresponding issues and options and are given feedback in the middle of the negotiation.","[[], ['b6', 'b1'], ['b34', 'b10', 'b41'], ['b53']]","[[], ['b6', 'b1'], ['b34', 'b10', 'b41'], ['b53']]",6,"1. In integrative negotiations, there is normally more than one issue available to be negotiated.
2. To achieve optimal negotiation goals, the involved players should make trade-offs for multiple issues.
3. Multi-player Strategy Games The strategy video games provide ideal platforms for people to verbally communicate with other players to accomplish their missions and goals.
4. Asher et al. (2016) propose the STAC benchmark, which is the player dialogue in the game of Catan.
5. In this game, players need to gather resources, including wood, wheat, sheep, and more, with each other to purchase settlements, roads and cities.
6. As each player only has access to their own resources, they have to communicate with each other.
7. To investigate the linguistic strategies used in this situation, STAC also includes an SDRT-styled discourse structure.
8. Boritchev and Amblard (2022) also collect a DinG dataset from French-speaking players in this game.
9. The participants are instructed to focus on the game, rather than talk about themselves.
10. As a result, the collected dialogues can better reflect the negotiation strategy used in the game process.Negotiation for Item Assignment
11. The item assignment scenarios involve a fixed set of items as well as a predefined priority for each player in the dialogue.
12. As the players only have access to their own priority, they need to negotiate with each other to exchange the items they prefer.
13. Nouri and Traum (2014) propose InitiativeTalking, occurring between the owners of two restaurants.
14. They discuss how to distribute the fruits (i.e., apples, bananas, and strawberries) and try to reach an agreement.
15. Lewis et al. (2017) propose DealorNoDeal, a similar two-party negotiation dialogue benchmark where both participants are only shown their own sets of items with a value for each and both of them are asked to maximize their total score after negotiation.
16. Chawla et al. (2021c) propose CaSiNo, a dataset on campsite scenarios involving campsite neighbors negotiating for additional food, water, and firewood packages.
17. Both parties have different priorities over different items.Negotiation for Job Interview Another commonly encountered negotiation scenario is job offer negotiation with recruiters.
18. Yamaguchi et al. (2021a) fill this gap and propose the JobInterview dataset.
19. JobInterview includes recruiter-applicant interactions over salary, day off, position, company, and workplace.
20. The participants are shown negotiators' preferences and the corresponding issues and options and are given feedback in the middle of the negotiation."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,Computer Science,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,s18,Mind Modeling,['p18.0'],"[""Mind modeling in negotiation dialogue systems encompasses several tasks, such as mind preference estimation and opponent response prediction. Mind preference estimation helps the agent infer the intention of the opponents and guess how their own utterances would affect the opponent's mental preference. Nazari et al. (2015) propose a heuristic frequency-based method to estimate the negotiator's preference. Langlet and Clavel (2018) consider a rule-based system incorporating linguistic features to identify user's preference. A critical challenge for mind modeling in negotiation is that it usually requires complete dialogues, so it is difficult to predict those preferences precisely for partial dialogue. To make it applicable for those partial dialogues, which is widespread in real-world applications, Chawla et al. (2022) formulated mind preference estimation as a ranking task and proposed a transformer-based model that can be trained directly on partial dialogue. In terms of opponent response prediction, He et al. (2018) firstly propose to decouple the modeling of the strategy of generation containing a parser to map utterances with dialogue acts and a dialogue manager to predict the skeleton of dialogue acts. Yang et al. (2021) further improve the negotiation system with a first-order model based on the theory of Mind (Frith and Frith, 2005) , which allows the agents to compute an expected value for each mental state. They provided two variance variants of ToM-based dialogue agents: explicit and implicit, which can fit both pipeline and end-to-end systems.""]","Mind modeling in negotiation dialogue systems encompasses several tasks, such as mind preference estimation and opponent response prediction. Mind preference estimation helps the agent infer the intention of the opponents and guess how their own utterances would affect the opponent's mental preference. Nazari et al. (2015) propose a heuristic frequency-based method to estimate the negotiator's preference. Langlet and Clavel (2018) consider a rule-based system incorporating linguistic features to identify user's preference. A critical challenge for mind modeling in negotiation is that it usually requires complete dialogues, so it is difficult to predict those preferences precisely for partial dialogue. To make it applicable for those partial dialogues, which is widespread in real-world applications, Chawla et al. (2022) formulated mind preference estimation as a ranking task and proposed a transformer-based model that can be trained directly on partial dialogue. In terms of opponent response prediction, He et al. (2018) firstly propose to decouple the modeling of the strategy of generation containing a parser to map utterances with dialogue acts and a dialogue manager to predict the skeleton of dialogue acts. Yang et al. (2021) further improve the negotiation system with a first-order model based on the theory of Mind (Frith and Frith, 2005) , which allows the agents to compute an expected value for each mental state. They provided two variance variants of ToM-based dialogue agents: explicit and implicit, which can fit both pipeline and end-to-end systems.","(p18.0) Mind modeling in negotiation dialogue systems encompasses several tasks, such as mind preference estimation and opponent response prediction. Mind preference estimation helps the agent infer the intention of the opponents and guess how their own utterances would affect the opponent's mental preference. Nazari et al. (2015) propose a heuristic frequency-based method to estimate the negotiator's preference. Langlet and Clavel (2018) consider a rule-based system incorporating linguistic features to identify user's preference. A critical challenge for mind modeling in negotiation is that it usually requires complete dialogues, so it is difficult to predict those preferences precisely for partial dialogue. To make it applicable for those partial dialogues, which is widespread in real-world applications, Chawla et al. (2022) formulated mind preference estimation as a ranking task and proposed a transformer-based model that can be trained directly on partial dialogue. In terms of opponent response prediction, He et al. (2018) firstly propose to decouple the modeling of the strategy of generation containing a parser to map utterances with dialogue acts and a dialogue manager to predict the skeleton of dialogue acts. Yang et al. (2021) further improve the negotiation system with a first-order model based on the theory of Mind (Frith and Frith, 2005) , which allows the agents to compute an expected value for each mental state. They provided two variance variants of ToM-based dialogue agents: explicit and implicit, which can fit both pipeline and end-to-end systems.","[['b55', 'b20', 'b30', 'b40', 'b24', 'b9']]","[['b55', 'b20', 'b30', 'b40', 'b24', 'b9']]",6,"1. Mind modeling in negotiation dialogue systems encompasses several tasks, such as mind preference estimation and opponent response prediction.
2. Mind preference estimation helps the agent infer the intention of the opponents and guess how their own utterances would affect the opponent's mental preference.
3. Nazari et al. (2015) propose a heuristic frequency-based method to estimate the negotiator's preference.
4. Langlet and Clavel (2018) consider a rule-based system incorporating linguistic features to identify user's preference.
5. A critical challenge for mind modeling in negotiation is that it usually requires complete dialogues, so it is difficult to predict those preferences precisely for partial dialogue.
6. To make it applicable for those partial dialogues, which is widespread in real-world applications, Chawla et al. (2022) formulated mind preference estimation as a ranking task and proposed a transformer-based model that can be trained directly on partial dialogue.
7. In terms of opponent response prediction, He et al. (2018) firstly propose to decouple the modeling of the strategy of generation containing a parser to map utterances with dialogue acts and a dialogue manager to predict the skeleton of dialogue acts.
8. Yang et al. (2021) further improve the negotiation system with a first-order model based on the theory of Mind (Frith and Frith, 2005) , which allows the agents to compute an expected value for each mental state.
9. They provided two variance variants of ToM-based dialogue agents: explicit and implicit, which can fit both pipeline and end-to-end systems."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,Computer Science,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,s14,Integrative Strategy,"['p14.0', 'p14.1']","['Integrative strategy (known as win-win) modeling aims to achieve mutual gain among participants. For instance, Zhao et al. (2019) proposes to model the discourse-level strategy using a latent action reinforcement learning (LaRL) framework. LaRL can model strategy transition within a latent space. However, due to the lack of explicit strategy labels, LaRL can only analysis strategies in implicit space.', ""To resolve the problem, Chawla et al. (2021c) define a series of explicit strategies such as Elicit-Preference, Coordination and Empathy. While Elicit-Preference is a strategy attempting to discover the preference of the opponent, Coordination promotes mutual benefits by explicit offer or implicit suggestion. In order to capture user's preference, Chawla et al. (2022) utilize those strategies using a hierarchical neural model. Besides, Yamaguchi et al. (2021b) present another collaborative strategy set to negotiate workload and salaries during the interview, which goal is to reach an agreement between employer and employee. It assists humans in becoming better negotiators during this process, e.g., communicating politely, addressing concerns, and providing side offers.""]","Integrative strategy (known as win-win) modeling aims to achieve mutual gain among participants. For instance, Zhao et al. (2019) proposes to model the discourse-level strategy using a latent action reinforcement learning (LaRL) framework. LaRL can model strategy transition within a latent space. However, due to the lack of explicit strategy labels, LaRL can only analysis strategies in implicit space.

To resolve the problem, Chawla et al. (2021c) define a series of explicit strategies such as Elicit-Preference, Coordination and Empathy. While Elicit-Preference is a strategy attempting to discover the preference of the opponent, Coordination promotes mutual benefits by explicit offer or implicit suggestion. In order to capture user's preference, Chawla et al. (2022) utilize those strategies using a hierarchical neural model. Besides, Yamaguchi et al. (2021b) present another collaborative strategy set to negotiate workload and salaries during the interview, which goal is to reach an agreement between employer and employee. It assists humans in becoming better negotiators during this process, e.g., communicating politely, addressing concerns, and providing side offers.","(p14.0) Integrative strategy (known as win-win) modeling aims to achieve mutual gain among participants. For instance, Zhao et al. (2019) proposes to model the discourse-level strategy using a latent action reinforcement learning (LaRL) framework. LaRL can model strategy transition within a latent space. However, due to the lack of explicit strategy labels, LaRL can only analysis strategies in implicit space.

(p14.1) To resolve the problem, Chawla et al. (2021c) define a series of explicit strategies such as Elicit-Preference, Coordination and Empathy. While Elicit-Preference is a strategy attempting to discover the preference of the opponent, Coordination promotes mutual benefits by explicit offer or implicit suggestion. In order to capture user's preference, Chawla et al. (2022) utilize those strategies using a hierarchical neural model. Besides, Yamaguchi et al. (2021b) present another collaborative strategy set to negotiate workload and salaries during the interview, which goal is to reach an agreement between employer and employee. It assists humans in becoming better negotiators during this process, e.g., communicating politely, addressing concerns, and providing side offers.","[['b57'], ['b10', 'b9', 'b54']]","[['b57'], ['b10', 'b9', 'b54']]",4,"1. Integrative strategy (known as win-win) modeling aims to achieve mutual gain among participants.
2. For instance, Zhao et al. (2019) proposes to model the discourse-level strategy using a latent action reinforcement learning (LaRL) framework.
3. LaRL can model strategy transition within a latent space.
4. However, due to the lack of explicit strategy labels, LaRL can only analysis strategies in implicit space.
5. To resolve the problem, Chawla et al. (2021c) define a series of explicit strategies such as Elicit-Preference, Coordination and Empathy.
6. While Elicit-Preference is a strategy attempting to discover the preference of the opponent, Coordination promotes mutual benefits by explicit offer or implicit suggestion.
7. In order to capture user's preference, Chawla et al. (2022) utilize those strategies using a hierarchical neural model.
8. Besides, Yamaguchi et al. (2021b) present another collaborative strategy set to negotiate workload and salaries during the interview, which goal is to reach an agreement between employer and employee.
9. It assists humans in becoming better negotiators during this process, e.g., communicating politely, addressing concerns, and providing side offers."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,Computer Science,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,s9,Goal-based Metrics,"['p9.0', 'p9.1']","[""Goal-oriented metrics mainly consider the agent's proximity to the goal from the perspective of strategy modeling, task fulfillment and sentence realization. Success Rate (SR) is the most widely used, which measures how frequently an agent completes the task within their goals. A similar metric Prediction Accuracy (PA) is to evaluate the agent's strategy predictions or the outcome of negotiations, such as macro or average F1 score (Wang et al., 2019;Dutt et al., 2020;Chawla et al., 2021c). For those scenario-related tasks, Yamaguchi et al."", '(2021a) present a task where the model is required to label the human-human negotiation outcomes as either a success or a breakdown, including area under the curve (ROC-AUC), confusion matrix (CM), and average precision (AP). Kornilova et al. (2022) propose a model-based evaluation based on Item Response Theory to analyze the effectiveness of persuasion on the audience. In terms of language realization for negotiation dialogue, Hiraoka et al. (2015) employ a predefined naturalness metric (a bigram overlap between the system responses and the ground-truth responses) as part of the reward to evaluate policies in cooperative persuasive dialogues. Other classical metrics for evaluating the quality of response are also used, i.e., perplexity (PPL), BLEU-2, ROUGE-L, and BOW Embedding-based Extrema matching score (Lewis et al., 2017).']","Goal-oriented metrics mainly consider the agent's proximity to the goal from the perspective of strategy modeling, task fulfillment and sentence realization. Success Rate (SR) is the most widely used, which measures how frequently an agent completes the task within their goals. A similar metric Prediction Accuracy (PA) is to evaluate the agent's strategy predictions or the outcome of negotiations, such as macro or average F1 score (Wang et al., 2019;Dutt et al., 2020;Chawla et al., 2021c). For those scenario-related tasks, Yamaguchi et al.

(2021a) present a task where the model is required to label the human-human negotiation outcomes as either a success or a breakdown, including area under the curve (ROC-AUC), confusion matrix (CM), and average precision (AP). Kornilova et al. (2022) propose a model-based evaluation based on Item Response Theory to analyze the effectiveness of persuasion on the audience. In terms of language realization for negotiation dialogue, Hiraoka et al. (2015) employ a predefined naturalness metric (a bigram overlap between the system responses and the ground-truth responses) as part of the reward to evaluate policies in cooperative persuasive dialogues. Other classical metrics for evaluating the quality of response are also used, i.e., perplexity (PPL), BLEU-2, ROUGE-L, and BOW Embedding-based Extrema matching score (Lewis et al., 2017).","(p9.0) Goal-oriented metrics mainly consider the agent's proximity to the goal from the perspective of strategy modeling, task fulfillment and sentence realization. Success Rate (SR) is the most widely used, which measures how frequently an agent completes the task within their goals. A similar metric Prediction Accuracy (PA) is to evaluate the agent's strategy predictions or the outcome of negotiations, such as macro or average F1 score (Wang et al., 2019;Dutt et al., 2020;Chawla et al., 2021c). For those scenario-related tasks, Yamaguchi et al.

(p9.1) (2021a) present a task where the model is required to label the human-human negotiation outcomes as either a success or a breakdown, including area under the curve (ROC-AUC), confusion matrix (CM), and average precision (AP). Kornilova et al. (2022) propose a model-based evaluation based on Item Response Theory to analyze the effectiveness of persuasion on the audience. In terms of language realization for negotiation dialogue, Hiraoka et al. (2015) employ a predefined naturalness metric (a bigram overlap between the system responses and the ground-truth responses) as part of the reward to evaluate policies in cooperative persuasive dialogues. Other classical metrics for evaluating the quality of response are also used, i.e., perplexity (PPL), BLEU-2, ROUGE-L, and BOW Embedding-based Extrema matching score (Lewis et al., 2017).","[['b13', 'b10', 'b52'], ['b28', 'b34', 'b25']]","[['b13', 'b10', 'b52'], ['b28', 'b34', 'b25']]",6,"1. Goal-oriented metrics mainly consider the agent's proximity to the goal from the perspective of strategy modeling, task fulfillment and sentence realization.
2. Success Rate (SR) is the most widely used, which measures how frequently an agent completes the task within their goals.
3. A similar metric Prediction Accuracy (PA) is to evaluate the agent's strategy predictions or the outcome of negotiations, such as macro or average F1 score (Wang et al., 2019;Dutt et al., 2020;Chawla et al., 2021c).
4. For those scenario-related tasks, Yamaguchi et al.(2021a) present a task where the model is required to label the human-human negotiation outcomes as either a success or a breakdown, including area under the curve (ROC-AUC), confusion matrix (CM), and average precision (AP).
5. Kornilova et al. (2022) propose a model-based evaluation based on Item Response Theory to analyze the effectiveness of persuasion on the audience.
6. In terms of language realization for negotiation dialogue, Hiraoka et al. (2015) employ a predefined naturalness metric (a bigram overlap between the system responses and the ground-truth responses) as part of the reward to evaluate policies in cooperative persuasive dialogues.
7. Other classical metrics for evaluating the quality of response are also used, i.e., perplexity (PPL), BLEU-2, ROUGE-L, and BOW Embedding-based Extrema matching score (Lewis et al., 2017)."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,Computer Science,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,s1,John F. Kennedy,"['p1.0', 'p1.1', 'p1.2', 'p1.3']","['Negotiation is one of the crucial abilities in human communication that involves two or more individuals discussing goals and tactics to resolve conflicts, achieve mutual benefit, or find mutually acceptable solutions (Fershtman, 1990;Bazerman and Neale, 1993;Lewicki et al., 2011). It is a common aspect of human interaction, occurring whenever people communicate in order to manage conflict or reach a compromise. Scientifically, one of the long-term goals of dialogue research is to empower intelligent agents with such ability. Agent effectively negotiating with a human in natural language could have significant benefits in many scenarios, from bargaining prices in everyday trade-in (He et al., 2018) to high-stakes political or legal situations (Basave and He, 2016). Negotiation dialogue systems (Lewandowska, 1982;Lambert and Carberry, 1992;Chawla et al., 2021c) is an emerging research field that aims to build intelligent conversational agents that can automatically negotiate with a human in natural languages, e.g., CICERO 1 from Meta AI. Agents negotiate with human through multi-turn interaction using logically reasoning (Sycara and Dai, 2010) over goals (Zhang et al., 2020), strategies (Zhou et al., 2020) and psychology factors (Yang et al., 2021). As illustrated in Figure 1, negotiation dialogue agents interact with the human through multiturn cycles. A successful negotiation process involves efficient information exchange, strategic discussion toward their goals, and a closing section.', 'Despite the significant amount of research that has been conducted on the task, there is a lack of a systematic review of the topic. In this work, we aim to fill this gap by reviewing contemporary work in the emerging field of negotiation dialogue systems, covering aspects such as benchmarks, evaluation, methodology, and future directions. In recent years, various benchmarks have been proposed for negotiation dialogue systems, ranging from bargaining (Lewis et al., 2017) and game scenarios (Asher et al., 2016) to job interviews  and items exchanging (Chawla et al., 2021c). Our survey will provide an overview of these benchmarks and discuss how they have been used to evaluate the performance of negotiation dialogue systems.', ""Modeling the negotiation process for conversational agents also imposes challenges. Firstly, these agents must be able to reason about and employ various strategies in different situations. In addition to strategy modeling, it is also necessary to model the personalities (e.g., mind, emotion, and behaviors) of the negotiators. Thirdly, an effective policy learning method is essential for the successful use of language. To address these challenges, we can categorize existing solutions into three areas: (1) Personality modeling helps us understand negotiator's preferences, (2) Strategy modeling enables agents to make reasonable decisions based on gathered information, and (3) Policy learning methods utilize information effectively to maximize results."", 'In summary, our contributions are three-fold: (1) To the best of our knowledge, we systematically categorize current negotiation dialogue benchmarks from the perspective of distributive and integrative, with each category based on different goal types of negotiation dialogue tasks. (2) We categorize typical evaluation methods and current solutions into an appropriate taxonomy. (3) We pointed out the current limitation and promising research directions in the future.']","Negotiation is one of the crucial abilities in human communication that involves two or more individuals discussing goals and tactics to resolve conflicts, achieve mutual benefit, or find mutually acceptable solutions (Fershtman, 1990;Bazerman and Neale, 1993;Lewicki et al., 2011). It is a common aspect of human interaction, occurring whenever people communicate in order to manage conflict or reach a compromise. Scientifically, one of the long-term goals of dialogue research is to empower intelligent agents with such ability. Agent effectively negotiating with a human in natural language could have significant benefits in many scenarios, from bargaining prices in everyday trade-in (He et al., 2018) to high-stakes political or legal situations (Basave and He, 2016). Negotiation dialogue systems (Lewandowska, 1982;Lambert and Carberry, 1992;Chawla et al., 2021c) is an emerging research field that aims to build intelligent conversational agents that can automatically negotiate with a human in natural languages, e.g., CICERO 1 from Meta AI. Agents negotiate with human through multi-turn interaction using logically reasoning (Sycara and Dai, 2010) over goals (Zhang et al., 2020), strategies (Zhou et al., 2020) and psychology factors (Yang et al., 2021). As illustrated in Figure 1, negotiation dialogue agents interact with the human through multiturn cycles. A successful negotiation process involves efficient information exchange, strategic discussion toward their goals, and a closing section.

Despite the significant amount of research that has been conducted on the task, there is a lack of a systematic review of the topic. In this work, we aim to fill this gap by reviewing contemporary work in the emerging field of negotiation dialogue systems, covering aspects such as benchmarks, evaluation, methodology, and future directions. In recent years, various benchmarks have been proposed for negotiation dialogue systems, ranging from bargaining (Lewis et al., 2017) and game scenarios (Asher et al., 2016) to job interviews  and items exchanging (Chawla et al., 2021c). Our survey will provide an overview of these benchmarks and discuss how they have been used to evaluate the performance of negotiation dialogue systems.

Modeling the negotiation process for conversational agents also imposes challenges. Firstly, these agents must be able to reason about and employ various strategies in different situations. In addition to strategy modeling, it is also necessary to model the personalities (e.g., mind, emotion, and behaviors) of the negotiators. Thirdly, an effective policy learning method is essential for the successful use of language. To address these challenges, we can categorize existing solutions into three areas: (1) Personality modeling helps us understand negotiator's preferences, (2) Strategy modeling enables agents to make reasonable decisions based on gathered information, and (3) Policy learning methods utilize information effectively to maximize results.

In summary, our contributions are three-fold: (1) To the best of our knowledge, we systematically categorize current negotiation dialogue benchmarks from the perspective of distributive and integrative, with each category based on different goal types of negotiation dialogue tasks. (2) We categorize typical evaluation methods and current solutions into an appropriate taxonomy. (3) We pointed out the current limitation and promising research directions in the future.","(p1.0) Negotiation is one of the crucial abilities in human communication that involves two or more individuals discussing goals and tactics to resolve conflicts, achieve mutual benefit, or find mutually acceptable solutions (Fershtman, 1990;Bazerman and Neale, 1993;Lewicki et al., 2011). It is a common aspect of human interaction, occurring whenever people communicate in order to manage conflict or reach a compromise. Scientifically, one of the long-term goals of dialogue research is to empower intelligent agents with such ability. Agent effectively negotiating with a human in natural language could have significant benefits in many scenarios, from bargaining prices in everyday trade-in (He et al., 2018) to high-stakes political or legal situations (Basave and He, 2016). Negotiation dialogue systems (Lewandowska, 1982;Lambert and Carberry, 1992;Chawla et al., 2021c) is an emerging research field that aims to build intelligent conversational agents that can automatically negotiate with a human in natural languages, e.g., CICERO 1 from Meta AI. Agents negotiate with human through multi-turn interaction using logically reasoning (Sycara and Dai, 2010) over goals (Zhang et al., 2020), strategies (Zhou et al., 2020) and psychology factors (Yang et al., 2021). As illustrated in Figure 1, negotiation dialogue agents interact with the human through multiturn cycles. A successful negotiation process involves efficient information exchange, strategic discussion toward their goals, and a closing section.

(p1.1) Despite the significant amount of research that has been conducted on the task, there is a lack of a systematic review of the topic. In this work, we aim to fill this gap by reviewing contemporary work in the emerging field of negotiation dialogue systems, covering aspects such as benchmarks, evaluation, methodology, and future directions. In recent years, various benchmarks have been proposed for negotiation dialogue systems, ranging from bargaining (Lewis et al., 2017) and game scenarios (Asher et al., 2016) to job interviews  and items exchanging (Chawla et al., 2021c). Our survey will provide an overview of these benchmarks and discuss how they have been used to evaluate the performance of negotiation dialogue systems.

(p1.2) Modeling the negotiation process for conversational agents also imposes challenges. Firstly, these agents must be able to reason about and employ various strategies in different situations. In addition to strategy modeling, it is also necessary to model the personalities (e.g., mind, emotion, and behaviors) of the negotiators. Thirdly, an effective policy learning method is essential for the successful use of language. To address these challenges, we can categorize existing solutions into three areas: (1) Personality modeling helps us understand negotiator's preferences, (2) Strategy modeling enables agents to make reasonable decisions based on gathered information, and (3) Policy learning methods utilize information effectively to maximize results.

(p1.3) In summary, our contributions are three-fold: (1) To the best of our knowledge, we systematically categorize current negotiation dialogue benchmarks from the perspective of distributive and integrative, with each category based on different goal types of negotiation dialogue tasks. (2) We categorize typical evaluation methods and current solutions into an appropriate taxonomy. (3) We pointed out the current limitation and promising research directions in the future.","[['b55', 'b56', 'b33', 'b32', 'b4', 'b29', 'b47', 'b10', 'b17', 'b24', 'b59', 'b2'], ['b34', 'b10', 'b1'], [], []]","[['b55', 'b56', 'b33', 'b32', 'b4', 'b29', 'b47', 'b10', 'b17', 'b24', 'b59', 'b2'], ['b34', 'b10', 'b1'], [], []]",15,"1. Negotiation is one of the crucial abilities in human communication that involves two or more individuals discussing goals and tactics to resolve conflicts, achieve mutual benefit, or find mutually acceptable solutions (Fershtman, 1990;Bazerman and Neale, 1993;Lewicki et al., 2011).
2. It is a common aspect of human interaction, occurring whenever people communicate in order to manage conflict or reach a compromise.
3. Scientifically, one of the long-term goals of dialogue research is to empower intelligent agents with such ability.
4. Agent effectively negotiating with a human in natural language could have significant benefits in many scenarios, from bargaining prices in everyday trade-in (He et al., 2018) to high-stakes political or legal situations (Basave and He, 2016).
5. Negotiation dialogue systems (Lewandowska, 1982;Lambert and Carberry, 1992;Chawla et al., 2021c) is an emerging research field that aims to build intelligent conversational agents that can automatically negotiate with a human in natural languages, e.g., CICERO 1 from Meta AI.
6. Agents negotiate with human through multi-turn interaction using logically reasoning (Sycara and Dai, 2010) over goals (Zhang et al., 2020), strategies (Zhou et al., 2020) and psychology factors (Yang et al., 2021).
7. As illustrated in Figure 1, negotiation dialogue agents interact with the human through multiturn cycles.
8. A successful negotiation process involves efficient information exchange, strategic discussion toward their goals, and a closing section.
9. Despite the significant amount of research that has been conducted on the task, there is a lack of a systematic review of the topic.
10. In this work, we aim to fill this gap by reviewing contemporary work in the emerging field of negotiation dialogue systems, covering aspects such as benchmarks, evaluation, methodology, and future directions.
11. In recent years, various benchmarks have been proposed for negotiation dialogue systems, ranging from bargaining (Lewis et al., 2017) and game scenarios (Asher et al., 2016) to job interviews  and items exchanging (Chawla et al., 2021c).
12. Our survey will provide an overview of these benchmarks and discuss how they have been used to evaluate the performance of negotiation dialogue systems.
13. Modeling the negotiation process for conversational agents also imposes challenges.
14. Firstly, these agents must be able to reason about and employ various strategies in different situations.
15. In addition to strategy modeling, it is also necessary to model the personalities (e.g., mind, emotion, and behaviors) of the negotiators.
16. Thirdly, an effective policy learning method is essential for the successful use of language.
17. To address these challenges, we can categorize existing solutions into three areas: (1) Personality modeling helps us understand negotiator's preferences, (2) Strategy modeling enables agents to make reasonable decisions based on gathered information, and (3) Policy learning methods utilize information effectively to maximize results.
18. In summary, our contributions are three-fold: (1) To the best of our knowledge, we systematically categorize current negotiation dialogue benchmarks from the perspective of distributive and integrative, with each category based on different goal types of negotiation dialogue tasks.
19. (2) We categorize typical evaluation methods and current solutions into an appropriate taxonomy.
20. (3) We pointed out the current limitation and promising research directions in the future."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,Computer Science,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,s7,Distributive Negotiation Datasets,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4']","['Distributive negotiation is about the discussion over a fixed amount of value (i.e., slicing up the pie). In such negotiation, the involved people normally talk about a single issue (e.g., item price) and therefore, there are hardly trade-offs between multiple issues in such negotiation.', 'Persuasion For Donation Persuasion, convincing others to take specific actions, is a necessary required skill for negotiation dialogue (Sycara, 1990;Sierra et al., 1997). Wang et al. (2019) focus on persuasion and propose PersuasionforGood, a twoparty persuasion conversations about charity donations. In the data annotation process, the persuaders are provided some persuasion tips and example sentences, while the persuaders are only told that this conversation is about charity. The annotators are required to complete at least ten utterances in a dialogue and are encouraged to reach an agreement at the end of the conversations. Dutt et al. (2020) further extend PersuasionforGood by adding the utterance-level annotations that change the positive and/or the negative face of the participants in a conversation. A face act can either raise or attack the positive face or negative face of either the speaker or the listener in the conversation.', 'Negotiation For Product Price Negotiations over product prices can be observed on a daily basis. He et al. (2018) propose CraigslistBargain, a negotiation benchmark based on a realistic item price bargaining scenario. In CraigslistBargain, two agents, a buyer and a seller, are required to negotiate the price of a given item. The listing price is available to both sides, but the buyer has a private price as the target. Then two agents chat freely to decide the final price. The conversation is completed when both agents agree with the price or one of the agents quits.  propose Ne-goCoach benchmark on similar scenarios, but with an additional negotiation coach who monitors messages between the two annotators and recommends tactics in real-time to the seller to get a better deal.', 'User Privacy Protection Privacy protection of negotiators has become more and more vital. Participant (e.g., attackers and defenders) goals are also conflicting. Li et al. (2020b) propose Anti-Scam benchmark which focuses on online customer service. In Anti-Scam, users try to defend themselves by identifying whether their components are attackers who try to steal sensitive personal information.', 'Anti-Scam provides an opportunity to study human elicitation strategies in this scenario.']","Distributive negotiation is about the discussion over a fixed amount of value (i.e., slicing up the pie). In such negotiation, the involved people normally talk about a single issue (e.g., item price) and therefore, there are hardly trade-offs between multiple issues in such negotiation.

Persuasion For Donation Persuasion, convincing others to take specific actions, is a necessary required skill for negotiation dialogue (Sycara, 1990;Sierra et al., 1997). Wang et al. (2019) focus on persuasion and propose PersuasionforGood, a twoparty persuasion conversations about charity donations. In the data annotation process, the persuaders are provided some persuasion tips and example sentences, while the persuaders are only told that this conversation is about charity. The annotators are required to complete at least ten utterances in a dialogue and are encouraged to reach an agreement at the end of the conversations. Dutt et al. (2020) further extend PersuasionforGood by adding the utterance-level annotations that change the positive and/or the negative face of the participants in a conversation. A face act can either raise or attack the positive face or negative face of either the speaker or the listener in the conversation.

Negotiation For Product Price Negotiations over product prices can be observed on a daily basis. He et al. (2018) propose CraigslistBargain, a negotiation benchmark based on a realistic item price bargaining scenario. In CraigslistBargain, two agents, a buyer and a seller, are required to negotiate the price of a given item. The listing price is available to both sides, but the buyer has a private price as the target. Then two agents chat freely to decide the final price. The conversation is completed when both agents agree with the price or one of the agents quits.  propose Ne-goCoach benchmark on similar scenarios, but with an additional negotiation coach who monitors messages between the two annotators and recommends tactics in real-time to the seller to get a better deal.

User Privacy Protection Privacy protection of negotiators has become more and more vital. Participant (e.g., attackers and defenders) goals are also conflicting. Li et al. (2020b) propose Anti-Scam benchmark which focuses on online customer service. In Anti-Scam, users try to defend themselves by identifying whether their components are attackers who try to steal sensitive personal information.

Anti-Scam provides an opportunity to study human elicitation strategies in this scenario.","(p7.0) Distributive negotiation is about the discussion over a fixed amount of value (i.e., slicing up the pie). In such negotiation, the involved people normally talk about a single issue (e.g., item price) and therefore, there are hardly trade-offs between multiple issues in such negotiation.

(p7.1) Persuasion For Donation Persuasion, convincing others to take specific actions, is a necessary required skill for negotiation dialogue (Sycara, 1990;Sierra et al., 1997). Wang et al. (2019) focus on persuasion and propose PersuasionforGood, a twoparty persuasion conversations about charity donations. In the data annotation process, the persuaders are provided some persuasion tips and example sentences, while the persuaders are only told that this conversation is about charity. The annotators are required to complete at least ten utterances in a dialogue and are encouraged to reach an agreement at the end of the conversations. Dutt et al. (2020) further extend PersuasionforGood by adding the utterance-level annotations that change the positive and/or the negative face of the participants in a conversation. A face act can either raise or attack the positive face or negative face of either the speaker or the listener in the conversation.

(p7.2) Negotiation For Product Price Negotiations over product prices can be observed on a daily basis. He et al. (2018) propose CraigslistBargain, a negotiation benchmark based on a realistic item price bargaining scenario. In CraigslistBargain, two agents, a buyer and a seller, are required to negotiate the price of a given item. The listing price is available to both sides, but the buyer has a private price as the target. Then two agents chat freely to decide the final price. The conversation is completed when both agents agree with the price or one of the agents quits.  propose Ne-goCoach benchmark on similar scenarios, but with an additional negotiation coach who monitors messages between the two annotators and recommends tactics in real-time to the seller to get a better deal.

(p7.3) User Privacy Protection Privacy protection of negotiators has become more and more vital. Participant (e.g., attackers and defenders) goals are also conflicting. Li et al. (2020b) propose Anti-Scam benchmark which focuses on online customer service. In Anti-Scam, users try to defend themselves by identifying whether their components are attackers who try to steal sensitive personal information.

(p7.4) Anti-Scam provides an opportunity to study human elicitation strategies in this scenario.","[[], ['b13', 'b52', 'b48', 'b46'], ['b24'], ['b37'], []]","[[], ['b13', 'b52', 'b48', 'b46'], ['b24'], ['b37'], []]",6,"1. Distributive negotiation is about the discussion over a fixed amount of value (i.e., slicing up the pie).
2. In such negotiation, the involved people normally talk about a single issue (e.g., item price) and therefore, there are hardly trade-offs between multiple issues in such negotiation.
3. Persuasion For Donation Persuasion, convincing others to take specific actions, is a necessary required skill for negotiation dialogue (Sycara, 1990;Sierra et al., 1997).
4. Wang et al. (2019) focus on persuasion and propose PersuasionforGood, a twoparty persuasion conversations about charity donations.
5. In the data annotation process, the persuaders are provided some persuasion tips and example sentences, while the persuaders are only told that this conversation is about charity.
6. The annotators are required to complete at least ten utterances in a dialogue and are encouraged to reach an agreement at the end of the conversations.
7. Dutt et al. (2020) further extend PersuasionforGood by adding the utterance-level annotations that change the positive and/or the negative face of the participants in a conversation.
8. A face act can either raise or attack the positive face or negative face of either the speaker or the listener in the conversation.
9. Negotiation For Product Price Negotiations over product prices can be observed on a daily basis.
10. He et al. (2018) propose CraigslistBargain, a negotiation benchmark based on a realistic item price bargaining scenario.
11. In CraigslistBargain, two agents, a buyer and a seller, are required to negotiate the price of a given item.
12. The listing price is available to both sides, but the buyer has a private price as the target.
13. Then two agents chat freely to decide the final price.
14. The conversation is completed when both agents agree with the price or one of the agents quits.
15. propose Ne-goCoach benchmark on similar scenarios, but with an additional negotiation coach who monitors messages between the two annotators and recommends tactics in real-time to the seller to get a better deal.
16. User Privacy Protection Privacy protection of negotiators has become more and more vital.
17. Participant (e.g., attackers and defenders) goals are also conflicting.
18. Li et al. (2020b) propose Anti-Scam benchmark which focuses on online customer service.
19. In Anti-Scam, users try to defend themselves by identifying whether their components are attackers who try to steal sensitive personal information.
20. Anti-Scam provides an opportunity to study human elicitation strategies in this scenario."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,Computer Science,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,s20,Behavior Modeling,['p20.0'],"[""Behavior modeling refers to detecting and predicting opponents' behaviors during the negotiation process. For example, fine-grained dialogue act labels are provided in the Craigslist dataset (He et al., 2018), to help track the behaviors of buyers and sellers. Zhang et al. (2020) propose an opposite behavior modeling framework to estimate opposite action using DQN-based policy learning. Chawla et al. (2021b) explore early prediction between negotiators for the outcomes. Tran et al. (2022) leverage dialogue acts to identify optimal strategies to persuade humans for donation.""]","Behavior modeling refers to detecting and predicting opponents' behaviors during the negotiation process. For example, fine-grained dialogue act labels are provided in the Craigslist dataset (He et al., 2018), to help track the behaviors of buyers and sellers. Zhang et al. (2020) propose an opposite behavior modeling framework to estimate opposite action using DQN-based policy learning. Chawla et al. (2021b) explore early prediction between negotiators for the outcomes. Tran et al. (2022) leverage dialogue acts to identify optimal strategies to persuade humans for donation.","(p20.0) Behavior modeling refers to detecting and predicting opponents' behaviors during the negotiation process. For example, fine-grained dialogue act labels are provided in the Craigslist dataset (He et al., 2018), to help track the behaviors of buyers and sellers. Zhang et al. (2020) propose an opposite behavior modeling framework to estimate opposite action using DQN-based policy learning. Chawla et al. (2021b) explore early prediction between negotiators for the outcomes. Tran et al. (2022) leverage dialogue acts to identify optimal strategies to persuade humans for donation.","[['b24', 'b49', 'b56', 'b8']]","[['b24', 'b49', 'b56', 'b8']]",4,"1. Behavior modeling refers to detecting and predicting opponents' behaviors during the negotiation process.
2. For example, fine-grained dialogue act labels are provided in the Craigslist dataset (He et al., 2018), to help track the behaviors of buyers and sellers.
3. Zhang et al. (2020) propose an opposite behavior modeling framework to estimate opposite action using DQN-based policy learning.
4. Chawla et al. (2021b) explore early prediction between negotiators for the outcomes.
5. Tran et al. (2022) leverage dialogue acts to identify optimal strategies to persuade humans for donation."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,Computer Science,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,s15,Distributive Strategy,['p15.0'],"[""Distributive strategy (known as win-loss) modeling focuses on achieving one's own goals and maximizing unilateral interests more than mutual benefits. Distributive strategy can be used when you insist on your position or resist the opponent's deal . For example, Dutt et al. (2021a) investigate four resisting categories, namely contesting, empowerment, biased processing, and avoidance (Fransen et al., 2015). Each individual category contains fine-grain strategic behaviors. For example, contesting refers to attacking the message source, and empowerment implies reinforcing personal preference to contradict a claim (Attitude Bolstering) or attempting to arouse guilt in the opponent (Self Pity). Besides, Wang et al. (2019) design a set of persuasion strategies to persuade others to donate to charity. It contains 10 different strategies containing logical appeal, emotional appeal, source-related inquiry and etc. Li et al. (2020a) explore the role structure to enhance the strategy modeling. Dutt et al. (2020) further enhances the role modeling with facing act, which helps utilize strategy between asymmetric roles.""]","Distributive strategy (known as win-loss) modeling focuses on achieving one's own goals and maximizing unilateral interests more than mutual benefits. Distributive strategy can be used when you insist on your position or resist the opponent's deal . For example, Dutt et al. (2021a) investigate four resisting categories, namely contesting, empowerment, biased processing, and avoidance (Fransen et al., 2015). Each individual category contains fine-grain strategic behaviors. For example, contesting refers to attacking the message source, and empowerment implies reinforcing personal preference to contradict a claim (Attitude Bolstering) or attempting to arouse guilt in the opponent (Self Pity). Besides, Wang et al. (2019) design a set of persuasion strategies to persuade others to donate to charity. It contains 10 different strategies containing logical appeal, emotional appeal, source-related inquiry and etc. Li et al. (2020a) explore the role structure to enhance the strategy modeling. Dutt et al. (2020) further enhances the role modeling with facing act, which helps utilize strategy between asymmetric roles.","(p15.0) Distributive strategy (known as win-loss) modeling focuses on achieving one's own goals and maximizing unilateral interests more than mutual benefits. Distributive strategy can be used when you insist on your position or resist the opponent's deal . For example, Dutt et al. (2021a) investigate four resisting categories, namely contesting, empowerment, biased processing, and avoidance (Fransen et al., 2015). Each individual category contains fine-grain strategic behaviors. For example, contesting refers to attacking the message source, and empowerment implies reinforcing personal preference to contradict a claim (Attitude Bolstering) or attempting to arouse guilt in the opponent (Self Pity). Besides, Wang et al. (2019) design a set of persuasion strategies to persuade others to donate to charity. It contains 10 different strategies containing logical appeal, emotional appeal, source-related inquiry and etc. Li et al. (2020a) explore the role structure to enhance the strategy modeling. Dutt et al. (2020) further enhances the role modeling with facing act, which helps utilize strategy between asymmetric roles.","[['b14', 'b19', 'b52', 'b13', 'b35']]","[['b14', 'b19', 'b52', 'b13', 'b35']]",5,"1. Distributive strategy (known as win-loss) modeling focuses on achieving one's own goals and maximizing unilateral interests more than mutual benefits.
2. Distributive strategy can be used when you insist on your position or resist the opponent's deal .
3. For example, Dutt et al. (2021a) investigate four resisting categories, namely contesting, empowerment, biased processing, and avoidance (Fransen et al., 2015).
4. Each individual category contains fine-grain strategic behaviors.
5. For example, contesting refers to attacking the message source, and empowerment implies reinforcing personal preference to contradict a claim (Attitude Bolstering) or attempting to arouse guilt in the opponent (Self Pity).
6. Besides, Wang et al. (2019) design a set of persuasion strategies to persuade others to donate to charity.
7. It contains 10 different strategies containing logical appeal, emotional appeal, source-related inquiry and etc.
8. Li et al. (2020a) explore the role structure to enhance the strategy modeling.
9. Dutt et al. (2020) further enhances the role modeling with facing act, which helps utilize strategy between asymmetric roles."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s53,Analysis of Deep Learning Methods,"['p53.0', 'p53.1']","['Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but', 'Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but']","Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but

Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but","(p53.0) Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but

(p53.1) Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but","[[None, 'b14', 'b33'], [None, 'b14', 'b33']]","[[None, 'b14', 'b33'], [None, 'b14', 'b33']]",6,"1. Is the current representation of numeracy sufficient?
2. The standard practice for deep learning techniques is to treat numbers in the same way as words.
3. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token.
4. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens.
5. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022).
6. Two numbers on the same or close number line could have surface forms with no shared common tokens.
7. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"".
8. This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text.
9. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems.
10. Table 5 provides examples of where language models tend to struggle with large numbers.
11. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers.
12. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, butIs the current representation of numeracy sufficient?
13. The standard practice for deep learning techniques is to treat numbers in the same way as words.
14. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token.
15. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens.
16. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022).
17. Two numbers on the same or close number line could have surface forms with no shared common tokens.
18. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"".
19. This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text.
20. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems.
21. Table 5 provides examples of where language models tend to struggle with large numbers.
22. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers.
23. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but"
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s51,High-quality Reasoning Chains,"['p51.0', 'p51.1', 'p51.2', 'p51.3', 'p51.4', 'p51.5', 'p51.6', 'p51.7']","['Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).', 'Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.', 'Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.', '6 Discussion and Findings', 'Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).', 'Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.', 'Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.', '6 Discussion and Findings']","Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

6 Discussion and Findings

Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

6 Discussion and Findings","(p51.0) Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

(p51.1) Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

(p51.2) Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

(p51.3) 6 Discussion and Findings

(p51.4) Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

(p51.5) Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

(p51.6) Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

(p51.7) 6 Discussion and Findings","[[None, 'b1', 'b36', 'b25'], [None], [None, 'b25'], [], [None, 'b1', 'b36', 'b25'], [None], [None, 'b25'], []]","[[None, 'b1', 'b36', 'b25'], [None], [None, 'b25'], [], [None, 'b1', 'b36', 'b25'], [None], [None, 'b25'], []]",14,"1. Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt.
2. However, manually creating reasoning chains has two disadvantages.
3. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks.
4. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer.
5. To address this limitation, recent studies mainly fo-
6. Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex
7. (175B) Clustering Language Auto-generated -Complexity-
8. CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT
9. (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks.
10. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.
11. Similarly , an alternative solution that uses large language models to express the reasoning process as a program.
12. The computation is then relegated to an external computer, which executes the generated programs to derive the answer.
13. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a).
14. Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer.
15. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.
16. 6 Discussion and FindingsEarly chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt.
17. However, manually creating reasoning chains has two disadvantages.
18. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks.
19. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer.
20. To address this limitation, recent studies mainly fo-
21. Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex
22. (175B) Clustering Language Auto-generated -Complexity-
23. CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT
24. (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks.
25. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.
26. Similarly , an alternative solution that uses large language models to express the reasoning process as a program.
27. The computation is then relegated to an external computer, which executes the generated programs to derive the answer.
28. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a).
29. Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer.
30. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.
31. 6 Discussion and Findings"
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s50,In-context Example Selection,"['p50.0', 'p50.1']","['Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. ', 'Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. ']","Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. 

Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. ","(p50.0) Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. 

(p50.1) Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. ","[[None, 'b25', 'b36', 'b3'], [None, 'b25', 'b36', 'b3']]","[[None, 'b25', 'b36', 'b3'], [None, 'b25', 'b36', 'b3']]",8,"1. Early chain-of-thought work randomly or heuristically selects in-context examples.
2. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a).
3. Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature.
4. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023).
5. For example, Rubin et al. Early chain-of-thought work randomly or heuristically selects in-context examples.
6. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a).
7. Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature.
8. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023).
9. For example, Rubin et al."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s49,In-context Learning for Mathematical Reasoning,"['p49.0', 'p49.1', 'p49.2', 'p49.3', 'p49.4', 'p49.5']","['Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).', 'Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.', 'Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let\'s think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.', 'Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).', 'Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.', 'Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let\'s think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.']","Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).

Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.

Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.

Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).

Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.

Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.","(p49.0) Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).

(p49.1) Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.

(p49.2) Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.

(p49.3) Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).

(p49.4) Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.

(p49.5) Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.","[[None], ['b1'], [None], [None], ['b1'], [None]]","[[None], ['b1'], [None], [None], ['b1'], [None]]",6,"1. Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020).
2. In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020).
3. ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022).
4. An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list.
5. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output.
6. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question.
7. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls.
8. He buys 2 more cans of tennis balls.
9. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now?
10. Answer: Roger started with 5 balls.
11. 2 cans of 3 tennis balls each are 6 tennis balls.
12. 5 + 6 = 11. The answer is 11. Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!""
13. prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting.
14. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.
15. Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020).
16. In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020).
17. ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022).
18. An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list.
19. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output.
20. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question.
21. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls.
22. He buys 2 more cans of tennis balls.
23. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now?
24. Answer: Roger started with 5 balls.
25. 2 cans of 3 tennis balls each are 6 tennis balls.
26. 5 + 6 = 11. The answer is 11. Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!""
27. prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting.
28. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s54,Problems,"['p54.0', 'p54.1', 'p54.2', 'p54.3', 'p54.4', 'p54.5', 'p54.6', 'p54.7', 'p54.8', 'p54.9', 'p54.10', 'p54.11', 'p54.12', 'p54.13', 'p54.14', 'p54.15', 'p54.16', 'p54.17']","['GPT-3 (text-davinci-002)', 'John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.', 'John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?', 'Mary has 5 balls.', 'John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.', 'John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.', 'John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.', 'John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.', 'Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work', 'GPT-3 (text-davinci-002)', 'John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.', 'John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?', 'Mary has 5 balls.', 'John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.', 'John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.', 'John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.', 'John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.', 'Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work']","GPT-3 (text-davinci-002)

John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.

John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?

Mary has 5 balls.

John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.

John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.

John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.

John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.

Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work

GPT-3 (text-davinci-002)

John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.

John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?

Mary has 5 balls.

John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.

John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.

John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.

John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.

Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work","(p54.0) GPT-3 (text-davinci-002)

(p54.1) John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.

(p54.2) John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?

(p54.3) Mary has 5 balls.

(p54.4) John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.

(p54.5) John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.

(p54.6) John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.

(p54.7) John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.

(p54.8) Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work

(p54.9) GPT-3 (text-davinci-002)

(p54.10) John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.

(p54.11) John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?

(p54.12) Mary has 5 balls.

(p54.13) John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.

(p54.14) John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.

(p54.15) John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.

(p54.16) John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.

(p54.17) Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work","[[], [], [], [], [], [], [], [], [None, 'b5'], [], [], [], [], [], [], [], [], [None, 'b5']]","[[], [], [], [], [], [], [], [], [None, 'b5'], [], [], [], [], [], [], [], [], [None, 'b5']]",4,"1. GPT-3 (text-davinci-002)John had 8 balls and he gave 3 to Mary.
2. How many balls does John have now?
3. John has 5 balls. John had 3 apples.
4. John had 8 balls and he gave 3 to Mary.
5. How many balls does Mary have now?
6. Mary has 5 balls. John had 8 balls and he gave 3 to Mary.
7. Who has more balls now? John has more balls.
8. John had 8 balls and he gave 3 to Mary.
9. Does John have more balls now? No, John has 5 balls now.
10. John had 8 balls and he gave 4 to Mary.
11. Does John have more balls now? No, John has 4 balls now.
12. John had 8 balls and he gave 4 to Mary.
13. Who has more balls now? John has more balls.
14. this remains an open problem. Are deep learning methods consistent for mathematical reasoning?
15. Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks.
16. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels.
17. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task.
18. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems.
19. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b).
20. The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets.
21. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade.
22. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged.
23. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models.
24. 7 Future WorkGPT-3 (text-davinci-002)John had 8 balls and he gave 3 to Mary.
25. How many balls does John have now?
26. John has 5 balls. John had 3 apples.
27. John had 8 balls and he gave 3 to Mary.
28. How many balls does Mary have now?
29. Mary has 5 balls. John had 8 balls and he gave 3 to Mary.
30. Who has more balls now? John has more balls.
31. John had 8 balls and he gave 3 to Mary.
32. Does John have more balls now? No, John has 5 balls now.
33. John had 8 balls and he gave 4 to Mary.
34. Does John have more balls now? No, John has 4 balls now.
35. John had 8 balls and he gave 4 to Mary.
36. Who has more balls now? John has more balls.
37. this remains an open problem. Are deep learning methods consistent for mathematical reasoning?
38. Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks.
39. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels.
40. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task.
41. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems.
42. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b).
43. The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets.
44. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade.
45. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged.
46. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models.
47. 7 Future Work"
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s55,Generalization and Robustness,"['p55.0', 'p55.1', 'p55.2', 'p55.3']","['Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.', 'Another aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.', 'Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.', 'Another aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.']","Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.

Another aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.

Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.

Another aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.","(p55.0) Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.

(p55.1) Another aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.

(p55.2) Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.

(p55.3) Another aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.","[[None], [None], [None], [None]]","[[None], [None], [None], [None]]",4,"1. Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks.
2. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)).
3. One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.
4. Another aspect of generalization relates to the role of memorization.
5. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution?
6. Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022).
7. On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection.
8. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.
9. Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks.
10. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)).
11. One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.
12. Another aspect of generalization relates to the role of memorization.
13. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution?
14. Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022).
15. On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection.
16. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s57,Learning from Feedback,"['p57.0', 'p57.1']","[""Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020)."", ""Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).""]","Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).

Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).","(p57.0) Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).

(p57.1) Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).","[['b31', None, 'b18', 'b17'], ['b31', None, 'b18', 'b17']]","[['b31', None, 'b18', 'b17'], ['b31', None, 'b18', 'b17']]",8,"1. Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback.
2. Such a process makes the continual improvement of models' output quality and safety possible.
3. An example is us-ing reinforcement learning from human feedback
4. (RLHF) (Ouyang et al., 2022) to align language models with instructions.
5. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a).
6. In the context of mathematical reasoning, feedback does not necessarily come from humans directly.
7. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).
8. Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback.
9. Such a process makes the continual improvement of models' output quality and safety possible.
10. An example is us-ing reinforcement learning from human feedback
11. (RLHF) (Ouyang et al., 2022) to align language models with instructions.
12. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a).
13. In the context of mathematical reasoning, feedback does not necessarily come from humans directly.
14. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020)."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s58,Multi-modal Mathematical Reasoning,"['p58.0', 'p58.1']","['In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.', 'In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.']","In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.

In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.","(p58.0) In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.

(p58.1) In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.","[[None, 'b38'], [None, 'b38']]","[[None, 'b38'], [None, 'b38']]",4,"1. In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a).
2. One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images.
3. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems.
4. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.
5. In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a).
6. One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images.
7. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems.
8. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s29,A.5 Other Quantitative Problems,"['p29.0', 'p29.1']","[""Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  "", ""Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  ""]","Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  

Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  ","(p29.0) Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  

(p29.1) Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  ","[[None, 'b23', 'b32'], [None, 'b23', 'b32']]","[[None, 'b23', 'b32'], [None, 'b23', 'b32']]",6,"1. Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets.
2. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios.
3. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area.
4. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way.
5. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams.
6. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge.
7. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework.
8. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming.
9. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format.
10. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.
11. Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets.
12. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios.
13. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area.
14. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way.
15. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams.
16. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge.
17. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework.
18. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming.
19. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format.
20. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s3,Seq2Seq-based Networks for Math,"['p3.0', 'p3.1']","['Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).', 'Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).']","Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).

Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).","(p3.0) Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).

(p3.1) Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).","[['b0', None, 'b27', 'b21'], ['b0', None, 'b27', 'b21']]","[['b0', None, 'b27', 'b21'], ['b0', None, 'b27', 'b21']]",8,"1. Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019).
2. A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task.
3. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof).
4. Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU.
5. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019).
6. A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task.
7. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof).
8. Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU.
9. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019)."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s66,A.3 Geometry Problem Solving,"['p66.0', 'p66.1', 'p66.2', 'p66.3']","['Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.', 'Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.', 'Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.', 'Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.']","Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.

Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.

Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.

Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.","(p66.0) Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.

(p66.1) Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.

(p66.2) Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.

(p66.3) Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.","[['b22', None, 'b6'], [None], ['b22', None, 'b6'], [None]]","[['b22', None, 'b6'], [None], ['b22', None, 'b6'], [None]]",8,"1. Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years.
2. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram.
3. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable.
4. GPS is a challenging task for deep learning methods due to the complex skills it requires.
5. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.
6. Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017).
7. However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods.
8. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs.
9. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.
10. Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years.
11. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram.
12. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable.
13. GPS is a challenging task for deep learning methods due to the complex skills it requires.
14. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.
15. Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017).
16. However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods.
17. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs.
18. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s65,A.2 Theorem Proving,"['p65.0', 'p65.1', 'p65.2', 'p65.3', 'p65.4', 'p65.5', 'p65.6', 'p65.7', 'p65.8', 'p65.9']","['Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP\'s programming language, then simplified by generating ""proof steps"" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.', ""Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions."", 'Other resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).', 'Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.', 'An emerging area of research aims to combine elements of informal and formal theorem proving. ', 'Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP\'s programming language, then simplified by generating ""proof steps"" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.', ""Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions."", 'Other resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).', 'Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.', 'An emerging area of research aims to combine elements of informal and formal theorem proving. ']","Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP's programming language, then simplified by generating ""proof steps"" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.

Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.

Other resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).

Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.

An emerging area of research aims to combine elements of informal and formal theorem proving. 

Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP's programming language, then simplified by generating ""proof steps"" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.

Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.

Other resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).

Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.

An emerging area of research aims to combine elements of informal and formal theorem proving. ","(p65.0) Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP's programming language, then simplified by generating ""proof steps"" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.

(p65.1) Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.

(p65.2) Other resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).

(p65.3) Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.

(p65.4) An emerging area of research aims to combine elements of informal and formal theorem proving. 

(p65.5) Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP's programming language, then simplified by generating ""proof steps"" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.

(p65.6) Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.

(p65.7) Other resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).

(p65.8) Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.

(p65.9) An emerging area of research aims to combine elements of informal and formal theorem proving. ","[[], ['b21', None, 'b40'], [None, 'b15'], ['b3', 'b2'], [], [], ['b21', None, 'b40'], [None, 'b15'], ['b3', 'b2'], []]","[[], ['b21', None, 'b40'], [None, 'b15'], ['b3', 'b2'], [], [], ['b21', None, 'b40'], [None, 'b15'], ['b3', 'b2'], []]",14,"1. Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP
2. , the theorem is stated in the ITP's programming language, then simplified by generating ""proof steps"" until it is reduced to known facts.
3. The result is a sequence of steps that constitutes a verified proof.
4. Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries.
5. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP.
6. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs.
7. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL.
8. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.
9. Other resources provide proxy environments or tasks.
10. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization.
11. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition.
12. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans.
13. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a).
14. Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks.
15. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.
16. An emerging area of research aims to combine elements of informal and formal theorem proving.
17. Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP
18. , the theorem is stated in the ITP's programming language, then simplified by generating ""proof steps"" until it is reduced to known facts.
19. The result is a sequence of steps that constitutes a verified proof.
20. Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries.
21. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP.
22. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs.
23. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL.
24. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.
25. Other resources provide proxy environments or tasks.
26. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization.
27. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition.
28. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans.
29. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a).
30. Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks.
31. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.
32. An emerging area of research aims to combine elements of informal and formal theorem proving."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s48,Task-specific Fine-tuning for Math,"['p48.0', 'p48.1']","['Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).', 'Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).']","Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).

Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).","(p48.0) Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).

(p48.1) Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).","[[None, 'b38', 'b3'], [None, 'b38', 'b3']]","[[None, 'b38', 'b3'], [None, 'b38', 'b3']]",6,"1. Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task.
2. This is also a common practice when there is not enough data for training the large models from scratch.
3. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a).
4. Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task.
5. This is also a common practice when there is not enough data for training the large models from scratch.
6. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a).
7. Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b)."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s47,Self-Supervised Learning for Math,"['p47.0', 'p47.1']","['Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.', 'Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.']","Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.

Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.","(p47.0) Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.

(p47.1) Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.","[[None, 'b18', 'b17'], [None, 'b18', 'b17']]","[[None, 'b18', 'b17'], [None, 'b18', 'b17']]",6,"1. Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data.
2. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning.
3. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020).
4. A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy.
5. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"".
6. A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.
7. (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.
8. guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens.
9. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b).
10. There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models.
11. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties.
12. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.
13. Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data.
14. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning.
15. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020).
16. A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy.
17. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"".
18. A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.
19. (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.
20. guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens.
21. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b).
22. There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models.
23. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties.
24. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s46,Pre-trained Language Models for Mathematical Reasoning,"['p46.0', 'p46.1']","['Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.', 'Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.']","Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.

Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.","(p46.0) Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.

(p46.1) Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.","[[None, 'b38'], [None, 'b38']]","[[None, 'b38'], [None, 'b38']]",4,"1. Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks.
2. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks.
3. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022).
4. However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning.
5. First, pre-trained language models are not specifically trained on mathematical data.
6. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks.
7. There is also less mathematical or scientific data available for large-scale pre-training compared to text data.
8. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks.
9. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b).
10. To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.
11. Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks.
12. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks.
13. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022).
14. However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning.
15. First, pre-trained language models are not specifically trained on mathematical data.
16. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks.
17. There is also less mathematical or scientific data available for large-scale pre-training compared to text data.
18. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks.
19. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b).
20. To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s45,Other Neural Networks for Math,"['p45.0', 'p45.1', 'p45.2', 'p45.3']","['Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.', 'Other deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.', 'Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.', 'Other deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.']","Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.

Other deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.

Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.

Other deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.","(p45.0) Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.

(p45.1) Other deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.

(p45.2) Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.

(p45.3) Other deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.","[[None], [None], [None], [None]]","[[None], [None], [None], [None]]",4,"1. Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks.
2. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017).
3. For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection.
4. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b).
5. In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM.
6. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.
7. Other deep neural network structures can also be used in mathematical reasoning.
8. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning.
9. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data.
10. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019).
11. Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.
12. Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks.
13. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017).
14. For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection.
15. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b).
16. In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM.
17. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.
18. Other deep neural network structures can also be used in mathematical reasoning.
19. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning.
20. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data.
21. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019).
22. Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s44,Attention-based Networks for Math,"['p44.0', 'p44.1']","['The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).', 'The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).']","The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).

The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).","(p44.0) The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).

(p44.1) The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).","[['b20', 'b21', 'b8', None, 'b9'], ['b20', 'b21', 'b8', None, 'b9']]","[['b20', 'b21', 'b8', None, 'b9'], ['b20', 'b21', 'b8', None, 'b9']]",10,"1. The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing.
2. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts.
3. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention.
4. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019).
5. Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).
6. The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing.
7. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts.
8. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention.
9. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019).
10. Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020)."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s43,Graph-based Networks for Math,"['p43.0', 'p43.1']","[""Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a)."", ""Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).""]","Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).

Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).","(p43.0) Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).

(p43.1) Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).","[['b26', 'b29', None, 'b10', 'b19', 'b9'], ['b26', 'b29', None, 'b10', 'b19', 'b9']]","[['b26', 'b29', None, 'b10', 'b19', 'b9'], ['b26', 'b29', None, 'b10', 'b19', 'b9']]",12,"1. Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features.
2. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions.
3. However, Seq2Seq methods do not explicitly this important information.
4. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions.
5. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021).
6. For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST.
7. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables.
8. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features.
9. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions.
10. However, Seq2Seq methods do not explicitly this important information.
11. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions.
12. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021).
13. For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST.
14. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables.
15. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a)."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s42,Seq2Seq-based Networks for Math,"['p42.0', 'p42.1']","['Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).', 'Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).']","Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).

Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).","(p42.0) Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).

(p42.1) Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).","[['b0', None, 'b27', 'b21'], ['b0', None, 'b27', 'b21']]","[['b0', None, 'b27', 'b21'], ['b0', None, 'b27', 'b21']]",8,"1. Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019).
2. A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task.
3. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof).
4. Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU.
5. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019).
6. A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task.
7. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof).
8. Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU.
9. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019)."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s40,Mathematical Reasoning Tasks,"['p40.0', 'p40.1', 'p40.2', 'p40.3', 'p40.4', 'p40.5', 'p40.6', 'p40.7']","['In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?', 'Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5', 'Answer: (B) 6.5 Figure 2: An example of geometry problems.', 'Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?"" over the content of paragraphs.', 'In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?', 'Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5', 'Answer: (B) 6.5 Figure 2: An example of geometry problems.', 'Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?"" over the content of paragraphs.']","In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?

Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5

Answer: (B) 6.5 Figure 2: An example of geometry problems.

Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?"" over the content of paragraphs.

In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?

Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5

Answer: (B) 6.5 Figure 2: An example of geometry problems.

Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?"" over the content of paragraphs.","(p40.0) In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?

(p40.1) Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5

(p40.2) Answer: (B) 6.5 Figure 2: An example of geometry problems.

(p40.3) Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?"" over the content of paragraphs.

(p40.4) In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?

(p40.5) Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5

(p40.6) Answer: (B) 6.5 Figure 2: An example of geometry problems.

(p40.7) Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?"" over the content of paragraphs.","[[None], [None, 'b6'], [], [], [None], [None, 'b6'], [], []]","[[None], [None, 'b6'], [], [], [None], [None, 'b6'], [], []]",6,"1. In this section, we briefly introduce different tasks for mathematical reasoning.
2. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving.
3. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964).
4. An example of a MWP is shown in Table 1.
5. A question involves four basic arithmetic operations with single or multiple operation steps.
6. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples.
7. How many apples do they have in total?
8. Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills.
9. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963).
10. The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ).
11. Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations.
12. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986).
13. As shown in Figure 2, a geometry problem consists of a textual description and a diagram.
14. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable.
15. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5Answer: (B) 6.5 Figure 2: An example of geometry problems.
16. Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA).
17. For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?""
18. over the content of paragraphs.
19. In this section, we briefly introduce different tasks for mathematical reasoning.
20. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving.
21. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964).
22. An example of a MWP is shown in Table 1.
23. A question involves four basic arithmetic operations with single or multiple operation steps.
24. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples.
25. How many apples do they have in total?
26. Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills.
27. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963).
28. The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ).
29. Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations.
30. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986).
31. As shown in Figure 2, a geometry problem consists of a textual description and a diagram.
32. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable.
33. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5Answer: (B) 6.5 Figure 2: An example of geometry problems.
34. Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA).
35. For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?""
36. over the content of paragraphs."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s28,A.4 Math Question Answering,"['p28.0', 'p28.1', 'p28.2', 'p28.3']","['Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning. In this work, we refer to these tasks as math question answering (MathQA). A large number of datasets have been presented recently. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b). To address this issue, new benchmarks are proposed from various aspects. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus. The dataset allows for measuring the algebraic generalization ability of a model. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.', 'Some work incorporates tabular contexts in the question inputs. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023). NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.', 'Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning. In this work, we refer to these tasks as math question answering (MathQA). A large number of datasets have been presented recently. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b). To address this issue, new benchmarks are proposed from various aspects. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus. The dataset allows for measuring the algebraic generalization ability of a model. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.', 'Some work incorporates tabular contexts in the question inputs. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023). NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.']","Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning. In this work, we refer to these tasks as math question answering (MathQA). A large number of datasets have been presented recently. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b). To address this issue, new benchmarks are proposed from various aspects. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus. The dataset allows for measuring the algebraic generalization ability of a model. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.

Some work incorporates tabular contexts in the question inputs. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023). NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.

Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning. In this work, we refer to these tasks as math question answering (MathQA). A large number of datasets have been presented recently. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b). To address this issue, new benchmarks are proposed from various aspects. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus. The dataset allows for measuring the algebraic generalization ability of a model. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.

Some work incorporates tabular contexts in the question inputs. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023). NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.","(p28.0) Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning. In this work, we refer to these tasks as math question answering (MathQA). A large number of datasets have been presented recently. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b). To address this issue, new benchmarks are proposed from various aspects. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus. The dataset allows for measuring the algebraic generalization ability of a model. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.

(p28.1) Some work incorporates tabular contexts in the question inputs. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023). NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.

(p28.2) Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning. In this work, we refer to these tasks as math question answering (MathQA). A large number of datasets have been presented recently. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b). To address this issue, new benchmarks are proposed from various aspects. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus. The dataset allows for measuring the algebraic generalization ability of a model. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.

(p28.3) Some work incorporates tabular contexts in the question inputs. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023). NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.","[[None], [None, 'b38', 'b43'], [None], [None, 'b38', 'b43']]","[[None], [None, 'b38', 'b43'], [None], [None, 'b38', 'b43']]",8,"1. Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks.
2. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning.
3. In this work, we refer to these tasks as math question answering (MathQA).
4. A large number of datasets have been presented recently.
5. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b).
6. To address this issue, new benchmarks are proposed from various aspects.
7. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus.
8. The dataset allows for measuring the algebraic generalization ability of a model.
9. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.
10. Some work incorporates tabular contexts in the question inputs.
11. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer.
12. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023).
13. NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks.
14. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.
15. Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks.
16. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning.
17. In this work, we refer to these tasks as math question answering (MathQA).
18. A large number of datasets have been presented recently.
19. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b).
20. To address this issue, new benchmarks are proposed from various aspects.
21. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus.
22. The dataset allows for measuring the algebraic generalization ability of a model.
23. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.
24. Some work incorporates tabular contexts in the question inputs.
25. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer.
26. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023).
27. NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks.
28. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s27,A.3 Geometry Problem Solving,"['p27.0', 'p27.1', 'p27.2', 'p27.3']","['Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.', 'Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.', 'Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.', 'Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.']","Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.

Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.

Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.

Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.","(p27.0) Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.

(p27.1) Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.

(p27.2) Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.

(p27.3) Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.","[['b22', None, 'b6'], [None], ['b22', None, 'b6'], [None]]","[['b22', None, 'b6'], [None], ['b22', None, 'b6'], [None]]",8,"1. Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years.
2. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram.
3. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable.
4. GPS is a challenging task for deep learning methods due to the complex skills it requires.
5. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.
6. Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017).
7. However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods.
8. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs.
9. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.
10. Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years.
11. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram.
12. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable.
13. GPS is a challenging task for deep learning methods due to the complex skills it requires.
14. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.
15. Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017).
16. However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods.
17. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs.
18. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s26,A.2 Theorem Proving,"['p26.0', 'p26.1', 'p26.2', 'p26.3', 'p26.4', 'p26.5', 'p26.6', 'p26.7', 'p26.8', 'p26.9']","['Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP\'s programming language, then simplified by generating ""proof steps"" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.', ""Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions."", 'Other resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).', 'Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.', 'An emerging area of research aims to combine elements of informal and formal theorem proving. ', 'Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP\'s programming language, then simplified by generating ""proof steps"" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.', ""Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions."", 'Other resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).', 'Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.', 'An emerging area of research aims to combine elements of informal and formal theorem proving. ']","Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP's programming language, then simplified by generating ""proof steps"" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.

Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.

Other resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).

Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.

An emerging area of research aims to combine elements of informal and formal theorem proving. 

Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP's programming language, then simplified by generating ""proof steps"" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.

Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.

Other resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).

Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.

An emerging area of research aims to combine elements of informal and formal theorem proving. ","(p26.0) Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP's programming language, then simplified by generating ""proof steps"" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.

(p26.1) Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.

(p26.2) Other resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).

(p26.3) Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.

(p26.4) An emerging area of research aims to combine elements of informal and formal theorem proving. 

(p26.5) Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP's programming language, then simplified by generating ""proof steps"" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.

(p26.6) Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.

(p26.7) Other resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).

(p26.8) Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.

(p26.9) An emerging area of research aims to combine elements of informal and formal theorem proving. ","[[], ['b21', None, 'b40'], [None, 'b15'], ['b3', 'b2'], [], [], ['b21', None, 'b40'], [None, 'b15'], ['b3', 'b2'], []]","[[], ['b21', None, 'b40'], [None, 'b15'], ['b3', 'b2'], [], [], ['b21', None, 'b40'], [None, 'b15'], ['b3', 'b2'], []]",14,"1. Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP
2. , the theorem is stated in the ITP's programming language, then simplified by generating ""proof steps"" until it is reduced to known facts.
3. The result is a sequence of steps that constitutes a verified proof.
4. Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries.
5. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP.
6. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs.
7. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL.
8. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.
9. Other resources provide proxy environments or tasks.
10. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization.
11. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition.
12. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans.
13. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a).
14. Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks.
15. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.
16. An emerging area of research aims to combine elements of informal and formal theorem proving.
17. Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP
18. , the theorem is stated in the ITP's programming language, then simplified by generating ""proof steps"" until it is reduced to known facts.
19. The result is a sequence of steps that constitutes a verified proof.
20. Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries.
21. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP.
22. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs.
23. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL.
24. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.
25. Other resources provide proxy environments or tasks.
26. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization.
27. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition.
28. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in ""standard"" mathematics (e.g., in L A T E X), and are checked for correctness by humans.
29. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a).
30. Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks.
31. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.
32. An emerging area of research aims to combine elements of informal and formal theorem proving."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s1,Mathematical Reasoning Tasks,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5', 'p1.6', 'p1.7']","['In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?', 'Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5', 'Answer: (B) 6.5 Figure 2: An example of geometry problems.', 'Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?"" over the content of paragraphs.', 'In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?', 'Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5', 'Answer: (B) 6.5 Figure 2: An example of geometry problems.', 'Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?"" over the content of paragraphs.']","In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?

Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5

Answer: (B) 6.5 Figure 2: An example of geometry problems.

Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?"" over the content of paragraphs.

In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?

Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5

Answer: (B) 6.5 Figure 2: An example of geometry problems.

Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?"" over the content of paragraphs.","(p1.0) In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?

(p1.1) Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5

(p1.2) Answer: (B) 6.5 Figure 2: An example of geometry problems.

(p1.3) Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?"" over the content of paragraphs.

(p1.4) In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?

(p1.5) Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5

(p1.6) Answer: (B) 6.5 Figure 2: An example of geometry problems.

(p1.7) Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?"" over the content of paragraphs.","[[None], [None, 'b6'], [], [], [None], [None, 'b6'], [], []]","[[None], [None, 'b6'], [], [], [None], [None, 'b6'], [], []]",6,"1. In this section, we briefly introduce different tasks for mathematical reasoning.
2. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving.
3. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964).
4. An example of a MWP is shown in Table 1.
5. A question involves four basic arithmetic operations with single or multiple operation steps.
6. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples.
7. How many apples do they have in total?
8. Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills.
9. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963).
10. The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ).
11. Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations.
12. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986).
13. As shown in Figure 2, a geometry problem consists of a textual description and a diagram.
14. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable.
15. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5Answer: (B) 6.5 Figure 2: An example of geometry problems.
16. Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA).
17. For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?""
18. over the content of paragraphs.
19. In this section, we briefly introduce different tasks for mathematical reasoning.
20. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving.
21. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964).
22. An example of a MWP is shown in Table 1.
23. A question involves four basic arithmetic operations with single or multiple operation steps.
24. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples.
25. How many apples do they have in total?
26. Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills.
27. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963).
28. The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ).
29. Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations.
30. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986).
31. As shown in Figure 2, a geometry problem consists of a textual description and a diagram.
32. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable.
33. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5Answer: (B) 6.5 Figure 2: An example of geometry problems.
34. Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA).
35. For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as ""Which kicker kicked the most field goals?""
36. over the content of paragraphs."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s25,A.1 Math Word Problem Solving,"['p25.0', 'p25.1', 'p25.2', 'p25.3', 'p25.4', 'p25.5']","['Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.', 'Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.', 'Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.', 'Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.', 'Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.', 'Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.']","Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.

Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.

Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.

Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.

Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.

Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.","(p25.0) Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.

(p25.1) Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.

(p25.2) Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.

(p25.3) Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.

(p25.4) Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.

(p25.5) Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.","[[None], ['b37', None], [None], [None], ['b37', None], [None]]","[[None], ['b37', None], [None], [None], ['b37', None], [None]]",8,"1. Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964).
2. A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities.
3. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question.
4. A typical example is shown in Table 1.
5. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps.
6. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.
7. Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021).
8. Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015).
9. Some recently curated datasets aim to increase problem diversity and difficulty levels.
10. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available.
11. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve.
12. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations.
13. More recently built datasets involve modalities beyond text.
14. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.
15. Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1).
16. To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead.
17. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b).
18. Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.
19. Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964).
20. A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities.
21. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question.
22. A typical example is shown in Table 1.
23. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps.
24. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.
25. Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021).
26. Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015).
27. Some recently curated datasets aim to increase problem diversity and difficulty levels.
28. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available.
29. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve.
30. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations.
31. More recently built datasets involve modalities beyond text.
32. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.
33. Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1).
34. To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead.
35. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b).
36. Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s18,Learning from Feedback,"['p18.0', 'p18.1']","[""Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020)."", ""Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).""]","Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).

Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).","(p18.0) Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).

(p18.1) Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).","[['b31', None, 'b18', 'b17'], ['b31', None, 'b18', 'b17']]","[['b31', None, 'b18', 'b17'], ['b31', None, 'b18', 'b17']]",8,"1. Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback.
2. Such a process makes the continual improvement of models' output quality and safety possible.
3. An example is us-ing reinforcement learning from human feedback
4. (RLHF) (Ouyang et al., 2022) to align language models with instructions.
5. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a).
6. In the context of mathematical reasoning, feedback does not necessarily come from humans directly.
7. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).
8. Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback.
9. Such a process makes the continual improvement of models' output quality and safety possible.
10. An example is us-ing reinforcement learning from human feedback
11. (RLHF) (Ouyang et al., 2022) to align language models with instructions.
12. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a).
13. In the context of mathematical reasoning, feedback does not necessarily come from humans directly.
14. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020)."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s16,Generalization and Robustness,"['p16.0', 'p16.1', 'p16.2', 'p16.3']","['Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.', 'Another aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.', 'Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.', 'Another aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.']","Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.

Another aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.

Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.

Another aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.","(p16.0) Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.

(p16.1) Another aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.

(p16.2) Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.

(p16.3) Another aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.","[[None], [None], [None], [None]]","[[None], [None], [None], [None]]",4,"1. Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks.
2. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)).
3. One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.
4. Another aspect of generalization relates to the role of memorization.
5. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution?
6. Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022).
7. On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection.
8. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.
9. Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks.
10. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)).
11. One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.
12. Another aspect of generalization relates to the role of memorization.
13. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution?
14. Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022).
15. On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection.
16. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s15,Problems,"['p15.0', 'p15.1', 'p15.2', 'p15.3', 'p15.4', 'p15.5', 'p15.6', 'p15.7', 'p15.8', 'p15.9', 'p15.10', 'p15.11', 'p15.12', 'p15.13', 'p15.14', 'p15.15', 'p15.16', 'p15.17']","['GPT-3 (text-davinci-002)', 'John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.', 'John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?', 'Mary has 5 balls.', 'John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.', 'John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.', 'John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.', 'John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.', 'Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work', 'GPT-3 (text-davinci-002)', 'John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.', 'John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?', 'Mary has 5 balls.', 'John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.', 'John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.', 'John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.', 'John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.', 'Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work']","GPT-3 (text-davinci-002)

John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.

John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?

Mary has 5 balls.

John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.

John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.

John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.

John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.

Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work

GPT-3 (text-davinci-002)

John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.

John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?

Mary has 5 balls.

John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.

John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.

John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.

John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.

Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work","(p15.0) GPT-3 (text-davinci-002)

(p15.1) John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.

(p15.2) John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?

(p15.3) Mary has 5 balls.

(p15.4) John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.

(p15.5) John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.

(p15.6) John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.

(p15.7) John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.

(p15.8) Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work

(p15.9) GPT-3 (text-davinci-002)

(p15.10) John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.

(p15.11) John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?

(p15.12) Mary has 5 balls.

(p15.13) John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.

(p15.14) John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.

(p15.15) John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.

(p15.16) John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.

(p15.17) Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work","[[], [], [], [], [], [], [], [], [None, 'b5'], [], [], [], [], [], [], [], [], [None, 'b5']]","[[], [], [], [], [], [], [], [], [None, 'b5'], [], [], [], [], [], [], [], [], [None, 'b5']]",4,"1. GPT-3 (text-davinci-002)John had 8 balls and he gave 3 to Mary.
2. How many balls does John have now?
3. John has 5 balls. John had 3 apples.
4. John had 8 balls and he gave 3 to Mary.
5. How many balls does Mary have now?
6. Mary has 5 balls. John had 8 balls and he gave 3 to Mary.
7. Who has more balls now? John has more balls.
8. John had 8 balls and he gave 3 to Mary.
9. Does John have more balls now? No, John has 5 balls now.
10. John had 8 balls and he gave 4 to Mary.
11. Does John have more balls now? No, John has 4 balls now.
12. John had 8 balls and he gave 4 to Mary.
13. Who has more balls now? John has more balls.
14. this remains an open problem. Are deep learning methods consistent for mathematical reasoning?
15. Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks.
16. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels.
17. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task.
18. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems.
19. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b).
20. The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets.
21. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade.
22. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged.
23. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models.
24. 7 Future WorkGPT-3 (text-davinci-002)John had 8 balls and he gave 3 to Mary.
25. How many balls does John have now?
26. John has 5 balls. John had 3 apples.
27. John had 8 balls and he gave 3 to Mary.
28. How many balls does Mary have now?
29. Mary has 5 balls. John had 8 balls and he gave 3 to Mary.
30. Who has more balls now? John has more balls.
31. John had 8 balls and he gave 3 to Mary.
32. Does John have more balls now? No, John has 5 balls now.
33. John had 8 balls and he gave 4 to Mary.
34. Does John have more balls now? No, John has 4 balls now.
35. John had 8 balls and he gave 4 to Mary.
36. Who has more balls now? John has more balls.
37. this remains an open problem. Are deep learning methods consistent for mathematical reasoning?
38. Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks.
39. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels.
40. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task.
41. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems.
42. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b).
43. The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets.
44. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade.
45. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged.
46. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models.
47. 7 Future Work"
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s14,Analysis of Deep Learning Methods,"['p14.0', 'p14.1']","['Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but', 'Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but']","Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but

Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but","(p14.0) Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but

(p14.1) Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but","[[None, 'b14', 'b33'], [None, 'b14', 'b33']]","[[None, 'b14', 'b33'], [None, 'b14', 'b33']]",6,"1. Is the current representation of numeracy sufficient?
2. The standard practice for deep learning techniques is to treat numbers in the same way as words.
3. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token.
4. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens.
5. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022).
6. Two numbers on the same or close number line could have surface forms with no shared common tokens.
7. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"".
8. This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text.
9. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems.
10. Table 5 provides examples of where language models tend to struggle with large numbers.
11. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers.
12. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, butIs the current representation of numeracy sufficient?
13. The standard practice for deep learning techniques is to treat numbers in the same way as words.
14. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token.
15. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens.
16. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022).
17. Two numbers on the same or close number line could have surface forms with no shared common tokens.
18. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"".
19. This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text.
20. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems.
21. Table 5 provides examples of where language models tend to struggle with large numbers.
22. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers.
23. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but"
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s12,High-quality Reasoning Chains,"['p12.0', 'p12.1', 'p12.2', 'p12.3', 'p12.4', 'p12.5', 'p12.6', 'p12.7']","['Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).', 'Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.', 'Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.', '6 Discussion and Findings', 'Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).', 'Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.', 'Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.', '6 Discussion and Findings']","Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

6 Discussion and Findings

Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

6 Discussion and Findings","(p12.0) Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

(p12.1) Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

(p12.2) Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

(p12.3) 6 Discussion and Findings

(p12.4) Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

(p12.5) Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

(p12.6) Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

(p12.7) 6 Discussion and Findings","[[None, 'b1', 'b36', 'b25'], [None], [None, 'b25'], [], [None, 'b1', 'b36', 'b25'], [None], [None, 'b25'], []]","[[None, 'b1', 'b36', 'b25'], [None], [None, 'b25'], [], [None, 'b1', 'b36', 'b25'], [None], [None, 'b25'], []]",14,"1. Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt.
2. However, manually creating reasoning chains has two disadvantages.
3. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks.
4. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer.
5. To address this limitation, recent studies mainly fo-
6. Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex
7. (175B) Clustering Language Auto-generated -Complexity-
8. CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT
9. (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks.
10. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.
11. Similarly , an alternative solution that uses large language models to express the reasoning process as a program.
12. The computation is then relegated to an external computer, which executes the generated programs to derive the answer.
13. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a).
14. Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer.
15. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.
16. 6 Discussion and FindingsEarly chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt.
17. However, manually creating reasoning chains has two disadvantages.
18. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks.
19. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer.
20. To address this limitation, recent studies mainly fo-
21. Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex
22. (175B) Clustering Language Auto-generated -Complexity-
23. CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT
24. (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks.
25. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.
26. Similarly , an alternative solution that uses large language models to express the reasoning process as a program.
27. The computation is then relegated to an external computer, which executes the generated programs to derive the answer.
28. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a).
29. Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer.
30. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.
31. 6 Discussion and Findings"
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s11,In-context Example Selection,"['p11.0', 'p11.1']","['Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. ', 'Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. ']","Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. 

Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. ","(p11.0) Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. 

(p11.1) Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. ","[[None, 'b25', 'b36', 'b3'], [None, 'b25', 'b36', 'b3']]","[[None, 'b25', 'b36', 'b3'], [None, 'b25', 'b36', 'b3']]",8,"1. Early chain-of-thought work randomly or heuristically selects in-context examples.
2. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a).
3. Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature.
4. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023).
5. For example, Rubin et al. Early chain-of-thought work randomly or heuristically selects in-context examples.
6. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a).
7. Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature.
8. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023).
9. For example, Rubin et al."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s10,In-context Learning for Mathematical Reasoning,"['p10.0', 'p10.1', 'p10.2', 'p10.3', 'p10.4', 'p10.5']","['Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).', 'Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.', 'Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let\'s think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.', 'Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).', 'Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.', 'Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let\'s think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.']","Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).

Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.

Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.

Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).

Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.

Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.","(p10.0) Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).

(p10.1) Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.

(p10.2) Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.

(p10.3) Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).

(p10.4) Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.

(p10.5) Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.","[[None], ['b1'], [None], [None], ['b1'], [None]]","[[None], ['b1'], [None], [None], ['b1'], [None]]",6,"1. Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020).
2. In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020).
3. ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022).
4. An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list.
5. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output.
6. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question.
7. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls.
8. He buys 2 more cans of tennis balls.
9. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now?
10. Answer: Roger started with 5 balls.
11. 2 cans of 3 tennis balls each are 6 tennis balls.
12. 5 + 6 = 11. The answer is 11. Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!""
13. prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting.
14. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.
15. Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020).
16. In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020).
17. ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022).
18. An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list.
19. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output.
20. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question.
21. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls.
22. He buys 2 more cans of tennis balls.
23. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now?
24. Answer: Roger started with 5 balls.
25. 2 cans of 3 tennis balls each are 6 tennis balls.
26. 5 + 6 = 11. The answer is 11. Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!""
27. prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting.
28. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s9,Task-specific Fine-tuning for Math,"['p9.0', 'p9.1']","['Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).', 'Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).']","Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).

Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).","(p9.0) Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).

(p9.1) Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).","[[None, 'b38', 'b3'], [None, 'b38', 'b3']]","[[None, 'b38', 'b3'], [None, 'b38', 'b3']]",6,"1. Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task.
2. This is also a common practice when there is not enough data for training the large models from scratch.
3. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a).
4. Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task.
5. This is also a common practice when there is not enough data for training the large models from scratch.
6. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a).
7. Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b)."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s8,Self-Supervised Learning for Math,"['p8.0', 'p8.1']","['Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.', 'Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.']","Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.

Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.","(p8.0) Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.

(p8.1) Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.","[[None, 'b18', 'b17'], [None, 'b18', 'b17']]","[[None, 'b18', 'b17'], [None, 'b18', 'b17']]",6,"1. Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data.
2. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning.
3. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020).
4. A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy.
5. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"".
6. A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.
7. (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.
8. guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens.
9. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b).
10. There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models.
11. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties.
12. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.
13. Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data.
14. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning.
15. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020).
16. A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy.
17. The study also mentions an interesting thresholding effect: ""all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters"".
18. A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.
19. (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.
20. guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens.
21. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b).
22. There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models.
23. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties.
24. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s7,Pre-trained Language Models for Mathematical Reasoning,"['p7.0', 'p7.1']","['Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.', 'Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.']","Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.

Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.","(p7.0) Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.

(p7.1) Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.","[[None, 'b38'], [None, 'b38']]","[[None, 'b38'], [None, 'b38']]",4,"1. Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks.
2. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks.
3. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022).
4. However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning.
5. First, pre-trained language models are not specifically trained on mathematical data.
6. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks.
7. There is also less mathematical or scientific data available for large-scale pre-training compared to text data.
8. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks.
9. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b).
10. To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.
11. Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks.
12. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks.
13. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022).
14. However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning.
15. First, pre-trained language models are not specifically trained on mathematical data.
16. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks.
17. There is also less mathematical or scientific data available for large-scale pre-training compared to text data.
18. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks.
19. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b).
20. To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s6,Other Neural Networks for Math,"['p6.0', 'p6.1', 'p6.2', 'p6.3']","['Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.', 'Other deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.', 'Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.', 'Other deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.']","Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.

Other deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.

Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.

Other deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.","(p6.0) Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.

(p6.1) Other deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.

(p6.2) Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.

(p6.3) Other deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.","[[None], [None], [None], [None]]","[[None], [None], [None], [None]]",4,"1. Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks.
2. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017).
3. For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection.
4. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b).
5. In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM.
6. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.
7. Other deep neural network structures can also be used in mathematical reasoning.
8. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning.
9. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data.
10. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019).
11. Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.
12. Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks.
13. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017).
14. For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection.
15. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b).
16. In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM.
17. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.
18. Other deep neural network structures can also be used in mathematical reasoning.
19. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning.
20. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data.
21. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019).
22. Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s5,Attention-based Networks for Math,"['p5.0', 'p5.1']","['The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).', 'The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).']","The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).

The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).","(p5.0) The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).

(p5.1) The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).","[['b20', 'b21', 'b8', None, 'b9'], ['b20', 'b21', 'b8', None, 'b9']]","[['b20', 'b21', 'b8', None, 'b9'], ['b20', 'b21', 'b8', None, 'b9']]",10,"1. The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing.
2. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts.
3. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention.
4. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019).
5. Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).
6. The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing.
7. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts.
8. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention.
9. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019).
10. Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020)."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s4,Graph-based Networks for Math,"['p4.0', 'p4.1']","[""Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a)."", ""Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).""]","Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).

Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).","(p4.0) Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).

(p4.1) Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).","[['b26', 'b29', None, 'b10', 'b19', 'b9'], ['b26', 'b29', None, 'b10', 'b19', 'b9']]","[['b26', 'b29', None, 'b10', 'b19', 'b9'], ['b26', 'b29', None, 'b10', 'b19', 'b9']]",12,"1. Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features.
2. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions.
3. However, Seq2Seq methods do not explicitly this important information.
4. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions.
5. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021).
6. For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST.
7. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables.
8. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features.
9. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions.
10. However, Seq2Seq methods do not explicitly this important information.
11. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions.
12. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021).
13. For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST.
14. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables.
15. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a)."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s19,Multi-modal Mathematical Reasoning,"['p19.0', 'p19.1']","['In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.', 'In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.']","In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.

In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.","(p19.0) In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.

(p19.1) In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.","[[None, 'b38'], [None, 'b38']]","[[None, 'b38'], [None, 'b38']]",4,"1. In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a).
2. One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images.
3. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems.
4. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.
5. In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a).
6. One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images.
7. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems.
8. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s64,A.1 Math Word Problem Solving,"['p64.0', 'p64.1', 'p64.2', 'p64.3', 'p64.4', 'p64.5']","['Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.', 'Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.', 'Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.', 'Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.', 'Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.', 'Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.']","Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.

Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.

Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.

Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.

Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.

Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.","(p64.0) Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.

(p64.1) Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.

(p64.2) Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.

(p64.3) Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.

(p64.4) Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.

(p64.5) Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.","[[None], ['b37', None], [None], [None], ['b37', None], [None]]","[[None], ['b37', None], [None], [None], ['b37', None], [None]]",8,"1. Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964).
2. A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities.
3. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question.
4. A typical example is shown in Table 1.
5. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps.
6. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.
7. Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021).
8. Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015).
9. Some recently curated datasets aim to increase problem diversity and difficulty levels.
10. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available.
11. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve.
12. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations.
13. More recently built datasets involve modalities beyond text.
14. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.
15. Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1).
16. To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead.
17. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b).
18. Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.
19. Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964).
20. A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities.
21. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question.
22. A typical example is shown in Table 1.
23. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps.
24. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.
25. Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021).
26. Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015).
27. Some recently curated datasets aim to increase problem diversity and difficulty levels.
28. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available.
29. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve.
30. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations.
31. More recently built datasets involve modalities beyond text.
32. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.
33. Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1).
34. To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead.
35. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b).
36. Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s68,A.5 Other Quantitative Problems,"['p68.0', 'p68.1']","[""Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  "", ""Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  ""]","Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  

Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  ","(p68.0) Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  

(p68.1) Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  ","[[None, 'b23', 'b32'], [None, 'b23', 'b32']]","[[None, 'b23', 'b32'], [None, 'b23', 'b32']]",6,"1. Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets.
2. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios.
3. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area.
4. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way.
5. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams.
6. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge.
7. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework.
8. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming.
9. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format.
10. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.
11. Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets.
12. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios.
13. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area.
14. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way.
15. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams.
16. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge.
17. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework.
18. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming.
19. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format.
20. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True."
254877175,A Survey of Deep Learning for Mathematical Reasoning,"Mathematics, Computer Science",https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,s67,A.4 Math Question Answering,"['p67.0', 'p67.1', 'p67.2', 'p67.3']","['Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning. In this work, we refer to these tasks as math question answering (MathQA). A large number of datasets have been presented recently. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b). To address this issue, new benchmarks are proposed from various aspects. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus. The dataset allows for measuring the algebraic generalization ability of a model. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.', 'Some work incorporates tabular contexts in the question inputs. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023). NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.', 'Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning. In this work, we refer to these tasks as math question answering (MathQA). A large number of datasets have been presented recently. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b). To address this issue, new benchmarks are proposed from various aspects. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus. The dataset allows for measuring the algebraic generalization ability of a model. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.', 'Some work incorporates tabular contexts in the question inputs. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023). NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.']","Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning. In this work, we refer to these tasks as math question answering (MathQA). A large number of datasets have been presented recently. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b). To address this issue, new benchmarks are proposed from various aspects. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus. The dataset allows for measuring the algebraic generalization ability of a model. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.

Some work incorporates tabular contexts in the question inputs. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023). NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.

Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning. In this work, we refer to these tasks as math question answering (MathQA). A large number of datasets have been presented recently. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b). To address this issue, new benchmarks are proposed from various aspects. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus. The dataset allows for measuring the algebraic generalization ability of a model. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.

Some work incorporates tabular contexts in the question inputs. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023). NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.","(p67.0) Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning. In this work, we refer to these tasks as math question answering (MathQA). A large number of datasets have been presented recently. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b). To address this issue, new benchmarks are proposed from various aspects. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus. The dataset allows for measuring the algebraic generalization ability of a model. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.

(p67.1) Some work incorporates tabular contexts in the question inputs. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023). NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.

(p67.2) Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning. In this work, we refer to these tasks as math question answering (MathQA). A large number of datasets have been presented recently. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b). To address this issue, new benchmarks are proposed from various aspects. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus. The dataset allows for measuring the algebraic generalization ability of a model. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.

(p67.3) Some work incorporates tabular contexts in the question inputs. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023). NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.","[[None], [None, 'b38', 'b43'], [None], [None, 'b38', 'b43']]","[[None], [None, 'b38', 'b43'], [None], [None, 'b38', 'b43']]",8,"1. Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks.
2. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning.
3. In this work, we refer to these tasks as math question answering (MathQA).
4. A large number of datasets have been presented recently.
5. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b).
6. To address this issue, new benchmarks are proposed from various aspects.
7. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus.
8. The dataset allows for measuring the algebraic generalization ability of a model.
9. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.
10. Some work incorporates tabular contexts in the question inputs.
11. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer.
12. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023).
13. NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks.
14. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.
15. Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks.
16. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning.
17. In this work, we refer to these tasks as math question answering (MathQA).
18. A large number of datasets have been presented recently.
19. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve ""satisfactory"" performance (Hendrycks et al., 2021b;Mishra et al., 2022b).
20. To address this issue, new benchmarks are proposed from various aspects.
21. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus.
22. The dataset allows for measuring the algebraic generalization ability of a model.
23. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.
24. Some work incorporates tabular contexts in the question inputs.
25. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer.
26. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023).
27. NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks.
28. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements."
254877753,Towards Reasoning in Large Language Models: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,s1,What is Reasoning?,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5', 'p1.6', 'p1.7', 'p1.8', 'p1.9']","['Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018). Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information. Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things. To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:', 'Deductive reasoning. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true. For example:', '• Premise: All mammals have kidneys. • Premise: All whales are mammals. • Conclusion: All whales have kidneys.', 'Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain. For example:', '• Observation: Every time we see a creature with wings, it is a bird. • Observation: We see a creature with wings. • Conclusion: The creature is likely to be a bird.', 'Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain. For example:', '• Observation: The car cannot start and there is a puddle of liquid under the engine. • Conclusion: The most likely explanation is that the car has a leak in the radiator.', 'Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.', 'Formal Reasoning vs Informal Reasoning. Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic. Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life. Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable. We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.', 'Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails. In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia). Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia). In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.']","Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018). Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information. Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things. To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:

Deductive reasoning. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true. For example:

• Premise: All mammals have kidneys. • Premise: All whales are mammals. • Conclusion: All whales have kidneys.

Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain. For example:

• Observation: Every time we see a creature with wings, it is a bird. • Observation: We see a creature with wings. • Conclusion: The creature is likely to be a bird.

Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain. For example:

• Observation: The car cannot start and there is a puddle of liquid under the engine. • Conclusion: The most likely explanation is that the car has a leak in the radiator.

Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.

Formal Reasoning vs Informal Reasoning. Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic. Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life. Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable. We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.

Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails. In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia). Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia). In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.","(p1.0) Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018). Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information. Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things. To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:

(p1.1) Deductive reasoning. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true. For example:

(p1.2) • Premise: All mammals have kidneys. • Premise: All whales are mammals. • Conclusion: All whales have kidneys.

(p1.3) Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain. For example:

(p1.4) • Observation: Every time we see a creature with wings, it is a bird. • Observation: We see a creature with wings. • Conclusion: The creature is likely to be a bird.

(p1.5) Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain. For example:

(p1.6) • Observation: The car cannot start and there is a puddle of liquid under the engine. • Conclusion: The most likely explanation is that the car has a leak in the radiator.

(p1.7) Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.

(p1.8) Formal Reasoning vs Informal Reasoning. Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic. Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life. Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable. We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.

(p1.9) Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails. In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia). Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia). In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.","[[None, 'b34', 'b75', 'b76'], [], [], [], [], [], [], [], [None], [None, 'b18', 'b80']]","[[None, 'b34', 'b75', 'b76'], [], [], [], [], [], [], [], [None], [None, 'b18', 'b80']]",8,"1. Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018).
2. Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information.
3. Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things.
4. To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:Deductive reasoning.
5. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises.
6. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true.
7. For example:• Premise: All mammals have kidneys.
8. • Premise: All whales are mammals.
9. • Conclusion: All whales have kidneys.
10. Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence.
11. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain.
12. For example:• Observation: Every time we see a creature with wings, it is a bird.
13. • Observation: We see a creature with wings.
14. • Conclusion: The creature is likely to be a bird.
15. Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations.
16. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain.
17. For example:• Observation: The car cannot start and there is a puddle of liquid under the engine.
18. • Conclusion: The most likely explanation is that the car has a leak in the radiator.
19. Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.
20. Formal Reasoning vs Informal Reasoning.
21. Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic.
22. Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life.
23. Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable.
24. We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.
25. Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails.
26. In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia).
27. Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia).
28. In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true."
254877753,Towards Reasoning in Large Language Models: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,s15,Findings and Implications,"['p15.0', 'p15.1', 'p15.2', 'p15.3', 'p15.4']","['In this section, we summarize the important findings and implications of studies on reasoning in large language models.', 'Reasoning seems an emergent ability of LLMs. Wei et al. (2022a,b);  show that reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters). This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks. However, the reason for this emergent ability is not yet fully understood. We refer the reader to Wei et al. (2022a);Fu et al. (2022a) for some potential explanations.', 'Chain of thought elicits ""reasoning"" of LLMs. The use of chain-of-thought (CoT) prompts (Wei et al., 2022b) has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of Wei et al. (2022a,b); . Additionally, Saparov and He (2022) ( §4.2) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual. However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs. Moreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement. In addition to these benefits, the use of CoT prompts has been shown to improve the out-ofdistribution robustness of LLMs (Wei et al., 2022b;Zhou et al., 2022a;Anil et al., 2022, inter alia), an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms.', 'LLMs show human-like content effects on reasoning. According to Dasgupta et al. (2022), LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature. For example, the models\' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions. These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well. This provides some evidence that language models may ""reason"" in a way that is similar to human reasoning.', 'LLMs are still unskilled at complex reasoning. Although LLMs seem to possess impressive reasoning capabilities with the techniques described in §3, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as Valmeekam et al. (2022);Han et al. (2022a); Ruis et al. (2022). For instance, Valmeekam et al. (2022) find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022) struggle to perform effectively. These findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard.']","In this section, we summarize the important findings and implications of studies on reasoning in large language models.

Reasoning seems an emergent ability of LLMs. Wei et al. (2022a,b);  show that reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters). This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks. However, the reason for this emergent ability is not yet fully understood. We refer the reader to Wei et al. (2022a);Fu et al. (2022a) for some potential explanations.

Chain of thought elicits ""reasoning"" of LLMs. The use of chain-of-thought (CoT) prompts (Wei et al., 2022b) has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of Wei et al. (2022a,b); . Additionally, Saparov and He (2022) ( §4.2) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual. However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs. Moreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement. In addition to these benefits, the use of CoT prompts has been shown to improve the out-ofdistribution robustness of LLMs (Wei et al., 2022b;Zhou et al., 2022a;Anil et al., 2022, inter alia), an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms.

LLMs show human-like content effects on reasoning. According to Dasgupta et al. (2022), LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature. For example, the models' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions. These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well. This provides some evidence that language models may ""reason"" in a way that is similar to human reasoning.

LLMs are still unskilled at complex reasoning. Although LLMs seem to possess impressive reasoning capabilities with the techniques described in §3, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as Valmeekam et al. (2022);Han et al. (2022a); Ruis et al. (2022). For instance, Valmeekam et al. (2022) find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022) struggle to perform effectively. These findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard.","(p15.0) In this section, we summarize the important findings and implications of studies on reasoning in large language models.

(p15.1) Reasoning seems an emergent ability of LLMs. Wei et al. (2022a,b);  show that reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters). This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks. However, the reason for this emergent ability is not yet fully understood. We refer the reader to Wei et al. (2022a);Fu et al. (2022a) for some potential explanations.

(p15.2) Chain of thought elicits ""reasoning"" of LLMs. The use of chain-of-thought (CoT) prompts (Wei et al., 2022b) has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of Wei et al. (2022a,b); . Additionally, Saparov and He (2022) ( §4.2) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual. However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs. Moreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement. In addition to these benefits, the use of CoT prompts has been shown to improve the out-ofdistribution robustness of LLMs (Wei et al., 2022b;Zhou et al., 2022a;Anil et al., 2022, inter alia), an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms.

(p15.3) LLMs show human-like content effects on reasoning. According to Dasgupta et al. (2022), LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature. For example, the models' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions. These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well. This provides some evidence that language models may ""reason"" in a way that is similar to human reasoning.

(p15.4) LLMs are still unskilled at complex reasoning. Although LLMs seem to possess impressive reasoning capabilities with the techniques described in §3, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as Valmeekam et al. (2022);Han et al. (2022a); Ruis et al. (2022). For instance, Valmeekam et al. (2022) find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022) struggle to perform effectively. These findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard.","[[], ['b77', None], ['b78', 'b77', None], ['b18'], [None, 'b71', 'b58']]","[[], ['b77', None], ['b78', 'b77', None], ['b18'], [None, 'b71', 'b58']]",9,"1. In this section, we summarize the important findings and implications of studies on reasoning in large language models.
2. Reasoning seems an emergent ability of LLMs.
3. Wei et al. (2022a,b);  show that reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters).
4. This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks.
5. However, the reason for this emergent ability is not yet fully understood.
6. We refer the reader to Wei et al. (2022a);Fu et al. (2022a) for some potential explanations.
7. Chain of thought elicits ""reasoning"" of LLMs.
8. The use of chain-of-thought (CoT) prompts (Wei et al., 2022b) has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of Wei et al. (2022a,b); .
9. Additionally, Saparov and He (2022) ( §4.2) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual.
10. However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs.
11. Moreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement.
12. In addition to these benefits, the use of CoT prompts has been shown to improve the out-ofdistribution robustness of LLMs (Wei et al., 2022b;Zhou et al., 2022a;Anil et al., 2022, inter alia), an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms.
13. LLMs show human-like content effects on reasoning.
14. According to Dasgupta et al. (2022), LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature.
15. For example, the models' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions.
16. These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well.
17. This provides some evidence that language models may ""reason"" in a way that is similar to human reasoning.
18. LLMs are still unskilled at complex reasoning.
19. Although LLMs seem to possess impressive reasoning capabilities with the techniques described in §3, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as Valmeekam et al. (2022);Han et al. (2022a); Ruis et al. (2022).
20. For instance, Valmeekam et al. (2022) find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022) struggle to perform effectively.
21. These findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard."
254877753,Towards Reasoning in Large Language Models: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,s13,End Task Performance,"['p13.0', 'p13.1']","['One way to measure reasoning abilities of LLMs is to report their performance, e.g., accuracy, on end tasks that require reasoning. We list some common benchmarks as follows.', ""Arithmetic Reasoning. Arithmetic reasoning is the ability to understand and apply mathematical concepts and principles in order to solve problems involving arithmetic operations. This involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems. Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015). It is worth mentioning that Anil et al. (2022)  Others. In practice, there are many benchmarks that can be used to evaluate reasoning abilities of LLMs (indirectly), as long as the downstream task involves reasoning. BIG-bench (Srivastava et al., 2022), for example, includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement. Other benchmarks, such as SCAN (Lake and Baroni, 2018) and the one proposed by Anil et al. (2022), focus on evaluating generalization ability. LLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA (Pasupat and Liang, 2015), FetaQA (Nan et al., 2022), as suggested by Chen (2022). In addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as CommonGen (Lin et al., 2020;Liu et al., 2022a) and Open Relation Modeling (Huang et al., 2022b,d).""]","One way to measure reasoning abilities of LLMs is to report their performance, e.g., accuracy, on end tasks that require reasoning. We list some common benchmarks as follows.

Arithmetic Reasoning. Arithmetic reasoning is the ability to understand and apply mathematical concepts and principles in order to solve problems involving arithmetic operations. This involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems. Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015). It is worth mentioning that Anil et al. (2022)  Others. In practice, there are many benchmarks that can be used to evaluate reasoning abilities of LLMs (indirectly), as long as the downstream task involves reasoning. BIG-bench (Srivastava et al., 2022), for example, includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement. Other benchmarks, such as SCAN (Lake and Baroni, 2018) and the one proposed by Anil et al. (2022), focus on evaluating generalization ability. LLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA (Pasupat and Liang, 2015), FetaQA (Nan et al., 2022), as suggested by Chen (2022). In addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as CommonGen (Lin et al., 2020;Liu et al., 2022a) and Open Relation Modeling (Huang et al., 2022b,d).","(p13.0) One way to measure reasoning abilities of LLMs is to report their performance, e.g., accuracy, on end tasks that require reasoning. We list some common benchmarks as follows.

(p13.1) Arithmetic Reasoning. Arithmetic reasoning is the ability to understand and apply mathematical concepts and principles in order to solve problems involving arithmetic operations. This involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems. Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015). It is worth mentioning that Anil et al. (2022)  Others. In practice, there are many benchmarks that can be used to evaluate reasoning abilities of LLMs (indirectly), as long as the downstream task involves reasoning. BIG-bench (Srivastava et al., 2022), for example, includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement. Other benchmarks, such as SCAN (Lake and Baroni, 2018) and the one proposed by Anil et al. (2022), focus on evaluating generalization ability. LLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA (Pasupat and Liang, 2015), FetaQA (Nan et al., 2022), as suggested by Chen (2022). In addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as CommonGen (Lin et al., 2020;Liu et al., 2022a) and Open Relation Modeling (Huang et al., 2022b,d).","[[], ['b57', 'b0', 'b26', None, 'b25', 'b24', 'b44', 'b1', 'b43']]","[[], ['b57', 'b0', 'b26', None, 'b25', 'b24', 'b44', 'b1', 'b43']]",9,"1. One way to measure reasoning abilities of LLMs is to report their performance, e.g., accuracy, on end tasks that require reasoning.
2. We list some common benchmarks as follows.
3. Arithmetic Reasoning. Arithmetic reasoning is the ability to understand and apply mathematical concepts and principles in order to solve problems involving arithmetic operations.
4. This involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems.
5. Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015).
6. It is worth mentioning that Anil et al. (2022)  Others.
7. In practice, there are many benchmarks that can be used to evaluate reasoning abilities of LLMs (indirectly), as long as the downstream task involves reasoning.
8. BIG-bench (Srivastava et al., 2022), for example, includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement.
9. Other benchmarks, such as SCAN (Lake and Baroni, 2018) and the one proposed by Anil et al. (2022), focus on evaluating generalization ability.
10. LLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA (Pasupat and Liang, 2015), FetaQA (Nan et al., 2022), as suggested by Chen (2022).
11. In addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as CommonGen (Lin et al., 2020;Liu et al., 2022a) and Open Relation Modeling (Huang et al., 2022b,d)."
254877753,Towards Reasoning in Large Language Models: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,s7,Problem Decomposition,"['p7.0', 'p7.1']","['Chain-of-thought prompting, while effective for eliciting reasoning in LLMs, can struggle with complex tasks, e.g., tasks that require compositional generalization (Lake and Baroni, 2018;Keysers et al., 2020). To solve a complex problem, it is helpful to first break it down into smaller, more manageable subproblems. By solving each of these subproblems, we can effectively solve the complex problem. This technique is called problem decom-position or divide and conquer (Talmor and Berant, 2018;Min et al., 2019;Perez et al., 2020).', 'Based on this idea, Zhou et al. (2022a) propose least-to-most prompting, which consists of two steps: decomposing the complex problem into subproblems and solving these subproblems in a specific order, with each subproblem being facilitated by the answers obtained from previously solved subproblems. As follow-up work, Drozdov et al. (2022) introduce dynamic least-to-most prompting, which is designed to solve more realistic semantic parsing problems by decomposing the problems with prompting-based syntactic parsing and dynamically selecting exemplars based on the decomposition. In addition, Khot et al. (2022) design decomposed prompting, which breaks down a complex problem into subproblems that can be handled by a shared library of prompting-based LLMs, each specialized in a particular subproblem. Furthermore, Dua et al. (2022) develop successive prompting, which iteratively decomposes a complex problem into a simple problem, with the next subproblem prediction having access to the answers to the previous subproblems. While the above methods decompose or solve compositional questions with multiple forward passes, Press et al. (2022) suggest decomposing and solving the input question in one forward pass using CoT prompting. Overall, these techniques show promise for helping LLMs to solve complex tasks by decomposing the problem into more manageable subproblems.']","Chain-of-thought prompting, while effective for eliciting reasoning in LLMs, can struggle with complex tasks, e.g., tasks that require compositional generalization (Lake and Baroni, 2018;Keysers et al., 2020). To solve a complex problem, it is helpful to first break it down into smaller, more manageable subproblems. By solving each of these subproblems, we can effectively solve the complex problem. This technique is called problem decom-position or divide and conquer (Talmor and Berant, 2018;Min et al., 2019;Perez et al., 2020).

Based on this idea, Zhou et al. (2022a) propose least-to-most prompting, which consists of two steps: decomposing the complex problem into subproblems and solving these subproblems in a specific order, with each subproblem being facilitated by the answers obtained from previously solved subproblems. As follow-up work, Drozdov et al. (2022) introduce dynamic least-to-most prompting, which is designed to solve more realistic semantic parsing problems by decomposing the problems with prompting-based syntactic parsing and dynamically selecting exemplars based on the decomposition. In addition, Khot et al. (2022) design decomposed prompting, which breaks down a complex problem into subproblems that can be handled by a shared library of prompting-based LLMs, each specialized in a particular subproblem. Furthermore, Dua et al. (2022) develop successive prompting, which iteratively decomposes a complex problem into a simple problem, with the next subproblem prediction having access to the answers to the previous subproblems. While the above methods decompose or solve compositional questions with multiple forward passes, Press et al. (2022) suggest decomposing and solving the input question in one forward pass using CoT prompting. Overall, these techniques show promise for helping LLMs to solve complex tasks by decomposing the problem into more manageable subproblems.","(p7.0) Chain-of-thought prompting, while effective for eliciting reasoning in LLMs, can struggle with complex tasks, e.g., tasks that require compositional generalization (Lake and Baroni, 2018;Keysers et al., 2020). To solve a complex problem, it is helpful to first break it down into smaller, more manageable subproblems. By solving each of these subproblems, we can effectively solve the complex problem. This technique is called problem decom-position or divide and conquer (Talmor and Berant, 2018;Min et al., 2019;Perez et al., 2020).

(p7.1) Based on this idea, Zhou et al. (2022a) propose least-to-most prompting, which consists of two steps: decomposing the complex problem into subproblems and solving these subproblems in a specific order, with each subproblem being facilitated by the answers obtained from previously solved subproblems. As follow-up work, Drozdov et al. (2022) introduce dynamic least-to-most prompting, which is designed to solve more realistic semantic parsing problems by decomposing the problems with prompting-based syntactic parsing and dynamically selecting exemplars based on the decomposition. In addition, Khot et al. (2022) design decomposed prompting, which breaks down a complex problem into subproblems that can be handled by a shared library of prompting-based LLMs, each specialized in a particular subproblem. Furthermore, Dua et al. (2022) develop successive prompting, which iteratively decomposes a complex problem into a simple problem, with the next subproblem prediction having access to the answers to the previous subproblems. While the above methods decompose or solve compositional questions with multiple forward passes, Press et al. (2022) suggest decomposing and solving the input question in one forward pass using CoT prompting. Overall, these techniques show promise for helping LLMs to solve complex tasks by decomposing the problem into more manageable subproblems.","[['b36', 'b14', 'b45', 'b17', 'b67'], ['b77', None, 'b48']]","[['b36', 'b14', 'b45', 'b17', 'b67'], ['b77', None, 'b48']]",8,"1. Chain-of-thought prompting, while effective for eliciting reasoning in LLMs, can struggle with complex tasks, e.g., tasks that require compositional generalization (Lake and Baroni, 2018;Keysers et al., 2020).
2. To solve a complex problem, it is helpful to first break it down into smaller, more manageable subproblems.
3. By solving each of these subproblems, we can effectively solve the complex problem.
4. This technique is called problem decom-position or divide and conquer (Talmor and Berant, 2018;Min et al., 2019;Perez et al., 2020).
5. Based on this idea, Zhou et al. (2022a) propose least-to-most prompting, which consists of two steps: decomposing the complex problem into subproblems and solving these subproblems in a specific order, with each subproblem being facilitated by the answers obtained from previously solved subproblems.
6. As follow-up work, Drozdov et al. (2022) introduce dynamic least-to-most prompting, which is designed to solve more realistic semantic parsing problems by decomposing the problems with prompting-based syntactic parsing and dynamically selecting exemplars based on the decomposition.
7. In addition, Khot et al. (2022) design decomposed prompting, which breaks down a complex problem into subproblems that can be handled by a shared library of prompting-based LLMs, each specialized in a particular subproblem.
8. Furthermore, Dua et al. (2022) develop successive prompting, which iteratively decomposes a complex problem into a simple problem, with the next subproblem prediction having access to the answers to the previous subproblems.
9. While the above methods decompose or solve compositional questions with multiple forward passes, Press et al. (2022) suggest decomposing and solving the input question in one forward pass using CoT prompting.
10. Overall, these techniques show promise for helping LLMs to solve complex tasks by decomposing the problem into more manageable subproblems."
254877753,Towards Reasoning in Large Language Models: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,s19,Limitations,"['p19.0', 'p19.1']","['In this paper, we provide an overview of the current state of knowledge on reasoning in large language models. Reasoning is a broad concept that encompasses various forms, making it impractical to summarize all related work in a single paper. Therefore, we focus on deductive reasoning, as it is the most commonly studied in the literature. Other forms of reasoning such as inductive reasoning Misra et al., 2022, inter alia) and abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia) may not be discussed in depth.', 'Additionally, given the rapid evolution and significance of reasoning within large language models, it is crucial to note that new contributions may have emerged in the field concurrent with the writing of this paper. An additional resource to consider is a parallel survey by Qiao et al. (2022), which emphasizes reasoning via language model prompting. Our coverage may not extend to papers released during or after 2023 such as evaluation on Chat-GPT (Bang et al., 2023;Zheng et al., 2023). As such, we recommend readers to check the papers that cite this survey for a more comprehensive and updated understanding of this field.']","In this paper, we provide an overview of the current state of knowledge on reasoning in large language models. Reasoning is a broad concept that encompasses various forms, making it impractical to summarize all related work in a single paper. Therefore, we focus on deductive reasoning, as it is the most commonly studied in the literature. Other forms of reasoning such as inductive reasoning Misra et al., 2022, inter alia) and abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia) may not be discussed in depth.

Additionally, given the rapid evolution and significance of reasoning within large language models, it is crucial to note that new contributions may have emerged in the field concurrent with the writing of this paper. An additional resource to consider is a parallel survey by Qiao et al. (2022), which emphasizes reasoning via language model prompting. Our coverage may not extend to papers released during or after 2023 such as evaluation on Chat-GPT (Bang et al., 2023;Zheng et al., 2023). As such, we recommend readers to check the papers that cite this survey for a more comprehensive and updated understanding of this field.","(p19.0) In this paper, we provide an overview of the current state of knowledge on reasoning in large language models. Reasoning is a broad concept that encompasses various forms, making it impractical to summarize all related work in a single paper. Therefore, we focus on deductive reasoning, as it is the most commonly studied in the literature. Other forms of reasoning such as inductive reasoning Misra et al., 2022, inter alia) and abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia) may not be discussed in depth.

(p19.1) Additionally, given the rapid evolution and significance of reasoning within large language models, it is crucial to note that new contributions may have emerged in the field concurrent with the writing of this paper. An additional resource to consider is a parallel survey by Qiao et al. (2022), which emphasizes reasoning via language model prompting. Our coverage may not extend to papers released during or after 2023 such as evaluation on Chat-GPT (Bang et al., 2023;Zheng et al., 2023). As such, we recommend readers to check the papers that cite this survey for a more comprehensive and updated understanding of this field.","[[None, 'b18', 'b80', 'b12'], ['b87', 'b2', 'b50']]","[[None, 'b18', 'b80', 'b12'], ['b87', 'b2', 'b50']]",7,"1. In this paper, we provide an overview of the current state of knowledge on reasoning in large language models.
2. Reasoning is a broad concept that encompasses various forms, making it impractical to summarize all related work in a single paper.
3. Therefore, we focus on deductive reasoning, as it is the most commonly studied in the literature.
4. Other forms of reasoning such as inductive reasoning Misra et al., 2022, inter alia) and abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia) may not be discussed in depth.
5. Additionally, given the rapid evolution and significance of reasoning within large language models, it is crucial to note that new contributions may have emerged in the field concurrent with the writing of this paper.
6. An additional resource to consider is a parallel survey by Qiao et al. (2022), which emphasizes reasoning via language model prompting.
7. Our coverage may not extend to papers released during or after 2023 such as evaluation on Chat-GPT (Bang et al., 2023;Zheng et al., 2023).
8. As such, we recommend readers to check the papers that cite this survey for a more comprehensive and updated understanding of this field."
254877753,Towards Reasoning in Large Language Models: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,s5,Chain of Thought and Its Variants,"['p5.0', 'p5.1', 'p5.2', 'p5.3']","['To encourage LLMs to engage in reasoning rather than simply providing answers directly, we may guide LLMs to generate ""reasoning"" explicitly. One approach for doing this is chain-of-thought prompting, proposed by Wei et al. (2022b). This approach involves providing a few examples of ""chain of thought"" (CoT), which are intermediate natural language reasoning steps, in the prompt to LLMs ( Figure 2). Specifically, in CoT prompting, ⟨input, output⟩ demonstrations are replaced with ⟨input, chain of thought, output⟩ triples, e.g., ""[input] Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? [chain of thought] Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. [output] The answer is 11."" In this way, given a target question, the model learns to generate explicit ratio- Input* Figure 2: An illustration of Chain-of-Thought Prompting and Rationale Engineering, where asterisk (*) denotes the target problem to be solved.', ""nale before producing the final answer. Experimental results show that this simple idea can improve LLMs' few-shot performance on arithmetic, symbolic, and commonsense reasoning tasks, sometimes to a striking degree."", 'There are several variants of chain-of-thought prompting that have been proposed in the literature, in a different form or to solve a specific problem.', 'Different Form: Kojima et al. (2022) introduce Zero-shot-CoT, in which LLMs are simply prompted with the phrase ""Let\'s think step by step"" after the input, in order to elicit reasoning without the need for few-shot demonstrations. Madaan et al. Specific Problem/Setting: Before chain of thought, Nye et al. (2022) also try to use intermediate computations, named ""scratchpads"", to improve language models\' reasoning performance in both finetuning and few-shot regimes, with a particular focus on programs. Shi et al. (2022) attempt to solve multilingual reasoning tasks with CoT in the native language, CoT in English (regardless of the problem language), and CoT in English (with the problem translated to English). Chen (2022) apply CoT to table-based reasoning, finding that LLMs can achieve strong performance on table tasks with only one exemplar. Prystawski et al. (2022) demonstrate that CoT can improve LLMs\' performance on paraphrase selection for metaphors. Lu et al. (2022) apply chain of thought to solve multimodal science questions.']","To encourage LLMs to engage in reasoning rather than simply providing answers directly, we may guide LLMs to generate ""reasoning"" explicitly. One approach for doing this is chain-of-thought prompting, proposed by Wei et al. (2022b). This approach involves providing a few examples of ""chain of thought"" (CoT), which are intermediate natural language reasoning steps, in the prompt to LLMs ( Figure 2). Specifically, in CoT prompting, ⟨input, output⟩ demonstrations are replaced with ⟨input, chain of thought, output⟩ triples, e.g., ""[input] Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? [chain of thought] Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. [output] The answer is 11."" In this way, given a target question, the model learns to generate explicit ratio- Input* Figure 2: An illustration of Chain-of-Thought Prompting and Rationale Engineering, where asterisk (*) denotes the target problem to be solved.

nale before producing the final answer. Experimental results show that this simple idea can improve LLMs' few-shot performance on arithmetic, symbolic, and commonsense reasoning tasks, sometimes to a striking degree.

There are several variants of chain-of-thought prompting that have been proposed in the literature, in a different form or to solve a specific problem.

Different Form: Kojima et al. (2022) introduce Zero-shot-CoT, in which LLMs are simply prompted with the phrase ""Let's think step by step"" after the input, in order to elicit reasoning without the need for few-shot demonstrations. Madaan et al. Specific Problem/Setting: Before chain of thought, Nye et al. (2022) also try to use intermediate computations, named ""scratchpads"", to improve language models' reasoning performance in both finetuning and few-shot regimes, with a particular focus on programs. Shi et al. (2022) attempt to solve multilingual reasoning tasks with CoT in the native language, CoT in English (regardless of the problem language), and CoT in English (with the problem translated to English). Chen (2022) apply CoT to table-based reasoning, finding that LLMs can achieve strong performance on table tasks with only one exemplar. Prystawski et al. (2022) demonstrate that CoT can improve LLMs' performance on paraphrase selection for metaphors. Lu et al. (2022) apply chain of thought to solve multimodal science questions.","(p5.0) To encourage LLMs to engage in reasoning rather than simply providing answers directly, we may guide LLMs to generate ""reasoning"" explicitly. One approach for doing this is chain-of-thought prompting, proposed by Wei et al. (2022b). This approach involves providing a few examples of ""chain of thought"" (CoT), which are intermediate natural language reasoning steps, in the prompt to LLMs ( Figure 2). Specifically, in CoT prompting, ⟨input, output⟩ demonstrations are replaced with ⟨input, chain of thought, output⟩ triples, e.g., ""[input] Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? [chain of thought] Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. [output] The answer is 11."" In this way, given a target question, the model learns to generate explicit ratio- Input* Figure 2: An illustration of Chain-of-Thought Prompting and Rationale Engineering, where asterisk (*) denotes the target problem to be solved.

(p5.1) nale before producing the final answer. Experimental results show that this simple idea can improve LLMs' few-shot performance on arithmetic, symbolic, and commonsense reasoning tasks, sometimes to a striking degree.

(p5.2) There are several variants of chain-of-thought prompting that have been proposed in the literature, in a different form or to solve a specific problem.

(p5.3) Different Form: Kojima et al. (2022) introduce Zero-shot-CoT, in which LLMs are simply prompted with the phrase ""Let's think step by step"" after the input, in order to elicit reasoning without the need for few-shot demonstrations. Madaan et al. Specific Problem/Setting: Before chain of thought, Nye et al. (2022) also try to use intermediate computations, named ""scratchpads"", to improve language models' reasoning performance in both finetuning and few-shot regimes, with a particular focus on programs. Shi et al. (2022) attempt to solve multilingual reasoning tasks with CoT in the native language, CoT in English (regardless of the problem language), and CoT in English (with the problem translated to English). Chen (2022) apply CoT to table-based reasoning, finding that LLMs can achieve strong performance on table tasks with only one exemplar. Prystawski et al. (2022) demonstrate that CoT can improve LLMs' performance on paraphrase selection for metaphors. Lu et al. (2022) apply chain of thought to solve multimodal science questions.","[['b78'], [], [], ['b49', 'b40', 'b62', 'b16', 'b29']]","[['b78'], [], [], ['b49', 'b40', 'b62', 'b16', 'b29']]",6,"1. To encourage LLMs to engage in reasoning rather than simply providing answers directly, we may guide LLMs to generate ""reasoning"" explicitly.
2. One approach for doing this is chain-of-thought prompting, proposed by Wei et al. (2022b).
3. This approach involves providing a few examples of ""chain of thought"" (CoT), which are intermediate natural language reasoning steps, in the prompt to LLMs ( Figure 2).
4. Specifically, in CoT prompting, ⟨input, output⟩ demonstrations are replaced with ⟨input, chain of thought, output⟩ triples, e.g., ""[input] Roger has 5 tennis balls.
5. He buys 2 more cans of tennis balls.
6. Each can has 3 tennis balls. How many tennis balls does he have now?
7. [chain of thought] Roger started with 5 balls.
8. 2 cans of 3 tennis balls each is 6 tennis balls.
9. 5 + 6 = 11. [output] The answer is 11.""
10. In this way, given a target question, the model learns to generate explicit ratio-
11. Input* Figure 2: An illustration of Chain-of-Thought Prompting and Rationale Engineering, where asterisk (*) denotes the target problem to be solved.nale before producing the final answer.
12. Experimental results show that this simple idea can improve LLMs' few-shot performance on arithmetic, symbolic, and commonsense reasoning tasks, sometimes to a striking degree.
13. There are several variants of chain-of-thought prompting that have been proposed in the literature, in a different form or to solve a specific problem.
14. Different Form: Kojima et al. (2022) introduce Zero-shot-CoT, in which LLMs are simply prompted with the phrase ""Let's think step by step"" after the input, in order to elicit reasoning without the need for few-shot demonstrations.
15. Madaan et al. Specific Problem/Setting: Before chain of thought, Nye et al. (2022) also try to use intermediate computations, named ""scratchpads"", to improve language models' reasoning performance in both finetuning and few-shot regimes, with a particular focus on programs.
16. Shi et al. (2022) attempt to solve multilingual reasoning tasks with CoT in the native language, CoT in English (regardless of the problem language), and CoT in English (with the problem translated to English).
17. Chen (2022) apply CoT to table-based reasoning, finding that LLMs can achieve strong performance on table tasks with only one exemplar.
18. Prystawski et al. (2022) demonstrate that CoT can improve LLMs' performance on paraphrase selection for metaphors.
19. Lu et al. (2022) apply chain of thought to solve multimodal science questions."
254877753,Towards Reasoning in Large Language Models: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,s3,Fully Supervised Finetuning,"['p3.0', 'p3.1', 'p3.2']","['Before discussing reasoning in large language models, it is worth mentioning there is research working on eliciting/improving reasoning in small language models through fully supervised finetuning on specific datasets. For example, Rajani et al.', '(2019) finetune a pretrained GPT model (Radford et al., 2018) to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks (Talmor et al., 2019). Talmor et al. (2020) train RoBERTa (Liu et al., 2019) to perform reasoning/inference based on both implicit pre-trained knowledge and explicit free-text statements. Hendrycks et al. (2021) finetune pretrained 2 It is important to note that the term ""reasoning"" in this paper does not necessarily imply that LLMs are truly capable of reasoning or that they are able to reason in the same way that humans do. We will discuss this issue in more detail in §6. language models to solve competition mathematics problems by generating full step-by-step solutions, though the accuracy is relatively low. Nye et al. (2022) train language models to do multi-step reasoning for program synthesis/execution by generating ""scratchpads"", i.e., intermediate computations, before producing the final answers. We refer the reader to Helwe et al. (2021); Bhargava and Ng (2022)\'s survey for more studies in this line.', 'There are two major limitations of fully supervised finetuning. First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create. Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions.']","Before discussing reasoning in large language models, it is worth mentioning there is research working on eliciting/improving reasoning in small language models through fully supervised finetuning on specific datasets. For example, Rajani et al.

(2019) finetune a pretrained GPT model (Radford et al., 2018) to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks (Talmor et al., 2019). Talmor et al. (2020) train RoBERTa (Liu et al., 2019) to perform reasoning/inference based on both implicit pre-trained knowledge and explicit free-text statements. Hendrycks et al. (2021) finetune pretrained 2 It is important to note that the term ""reasoning"" in this paper does not necessarily imply that LLMs are truly capable of reasoning or that they are able to reason in the same way that humans do. We will discuss this issue in more detail in §6. language models to solve competition mathematics problems by generating full step-by-step solutions, though the accuracy is relatively low. Nye et al. (2022) train language models to do multi-step reasoning for program synthesis/execution by generating ""scratchpads"", i.e., intermediate computations, before producing the final answers. We refer the reader to Helwe et al. (2021); Bhargava and Ng (2022)'s survey for more studies in this line.

There are two major limitations of fully supervised finetuning. First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create. Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions.","(p3.0) Before discussing reasoning in large language models, it is worth mentioning there is research working on eliciting/improving reasoning in small language models through fully supervised finetuning on specific datasets. For example, Rajani et al.

(p3.1) (2019) finetune a pretrained GPT model (Radford et al., 2018) to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks (Talmor et al., 2019). Talmor et al. (2020) train RoBERTa (Liu et al., 2019) to perform reasoning/inference based on both implicit pre-trained knowledge and explicit free-text statements. Hendrycks et al. (2021) finetune pretrained 2 It is important to note that the term ""reasoning"" in this paper does not necessarily imply that LLMs are truly capable of reasoning or that they are able to reason in the same way that humans do. We will discuss this issue in more detail in §6. language models to solve competition mathematics problems by generating full step-by-step solutions, though the accuracy is relatively low. Nye et al. (2022) train language models to do multi-step reasoning for program synthesis/execution by generating ""scratchpads"", i.e., intermediate computations, before producing the final answers. We refer the reader to Helwe et al. (2021); Bhargava and Ng (2022)'s survey for more studies in this line.

(p3.2) There are two major limitations of fully supervised finetuning. First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create. Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions.","[[], ['b28', 'b69', 'b40', 'b68', 'b51'], []]","[[], ['b28', 'b69', 'b40', 'b68', 'b51'], []]",5,"1. Before discussing reasoning in large language models, it is worth mentioning there is research working on eliciting/improving reasoning in small language models through fully supervised finetuning on specific datasets.
2. For example, Rajani et al.(2019) finetune a pretrained GPT model (Radford et al., 2018) to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks (Talmor et al., 2019).
3. Talmor et al. (2020) train RoBERTa (Liu et al., 2019) to perform reasoning/inference based on both implicit pre-trained knowledge and explicit free-text statements.
4. Hendrycks et al. (2021) finetune pretrained 2
5. It is important to note that the term ""reasoning"" in this paper does not necessarily imply that LLMs are truly capable of reasoning or that they are able to reason in the same way that humans do.
6. We will discuss this issue in more detail in §6.
7. language models to solve competition mathematics problems by generating full step-by-step solutions, though the accuracy is relatively low.
8. Nye et al. (2022) train language models to do multi-step reasoning for program synthesis/execution by generating ""scratchpads"", i.e., intermediate computations, before producing the final answers.
9. We refer the reader to Helwe et al. (2021); Bhargava and Ng (2022)'s survey for more studies in this line.
10. There are two major limitations of fully supervised finetuning.
11. First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create.
12. Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions."
254877753,Towards Reasoning in Large Language Models: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,s2,Towards Reasoning in Large Language Models,['p2.0'],"['Reasoning, particularly multi-step reasoning, is often seen as a weakness in language models and other NLP models (Bommasani et al., 2021;Rae et al., 2021;Valmeekam et al., 2022). Recent research has suggested that reasoning ability may emerge in language models at a certain scale, such as models with over 100 billion parameters (Wei et al., 2022a,b;Cobbe et al., 2021). In this paper, we follow Wei et al. (2022a) in considering reasoning as an ability that is rarely present in smallscale models like GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2019), and therefore focus on techniques applicable to improving or eliciting ""reasoning"" 2 in LLMs such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022).']","Reasoning, particularly multi-step reasoning, is often seen as a weakness in language models and other NLP models (Bommasani et al., 2021;Rae et al., 2021;Valmeekam et al., 2022). Recent research has suggested that reasoning ability may emerge in language models at a certain scale, such as models with over 100 billion parameters (Wei et al., 2022a,b;Cobbe et al., 2021). In this paper, we follow Wei et al. (2022a) in considering reasoning as an ability that is rarely present in smallscale models like GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2019), and therefore focus on techniques applicable to improving or eliciting ""reasoning"" 2 in LLMs such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022).","(p2.0) Reasoning, particularly multi-step reasoning, is often seen as a weakness in language models and other NLP models (Bommasani et al., 2021;Rae et al., 2021;Valmeekam et al., 2022). Recent research has suggested that reasoning ability may emerge in language models at a certain scale, such as models with over 100 billion parameters (Wei et al., 2022a,b;Cobbe et al., 2021). In this paper, we follow Wei et al. (2022a) in considering reasoning as an ability that is rarely present in smallscale models like GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2019), and therefore focus on techniques applicable to improving or eliciting ""reasoning"" 2 in LLMs such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022).","[['b53', 'b77', None, 'b52', 'b71']]","[['b53', 'b77', None, 'b52', 'b71']]",5,"1. Reasoning, particularly multi-step reasoning, is often seen as a weakness in language models and other NLP models (Bommasani et al., 2021;Rae et al., 2021;Valmeekam et al., 2022).
2. Recent research has suggested that reasoning ability may emerge in language models at a certain scale, such as models with over 100 billion parameters (Wei et al., 2022a,b;Cobbe et al., 2021).
3. In this paper, we follow Wei et al. (2022a) in considering reasoning as an ability that is rarely present in smallscale models like GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2019), and therefore focus on techniques applicable to improving or eliciting ""reasoning"" 2 in LLMs such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022)."
254877753,Towards Reasoning in Large Language Models: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,s6,Rationale Engineering,"['p6.0', 'p6.1', 'p6.2', 'p6.3']","['The original version of chain-of-thought prompting, proposed by Wei et al. (2022b), relies on manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation. Rationale engineering aims to more effectively elicit or utilize reasoning in LLMs. This can be achieved through rationale refinement, which involves creating more effective examples of reasoning steps, or through rationale exploration and rationale verification, which involve exploring and verifying the rationales produced by LLMs. A summary of raltionale engineering is illustrated in Figure 2.', 'Rationale refinement. The choice of exemplars can significantly affect the few-shot performance of LLMs, as demonstrated in research such as , which also appears in chain-of-thought prompting. Rationale refinement aims to create and refine rationale examples that are better able to elicit reasoning in LLMs. Fu et al. (2022b) propose complexity-based prompting to create rationales with more reasoning steps. Their experiments show that the performance of LLMs improves with the increased rationale complexity. Similarly,  propose algorithmic prompting, which suggests that providing more thorough examples of solutions can help improve reasoning performance on some simple math calculations.  design Auto-CoT to automatically construct exemplars by partitioning questions from a given dataset into clusters and then using Zero-Shot-CoT (Kojima et al., 2022) to generate the rationale for a representative question from each cluster. The analysis shows that making exemplars diverse is important in prompting LLMs to produce better rationales.', 'Rationale exploration. In addition to providing better exemplars, we can allow LLMs to fully explore various ways of reasoning to improve their performance on reasoning tasks, named rationale exploration. Based on the idea that complex problems often admit multiple ways of thinking that can lead to their unique correct answer, Wang et al. (2022c) present a decoding strategy called selfconsistency to improve upon the traditional greedy decoding used in chain-of-thought prompting. This strategy involves sampling a diverse set of rationales, rather than just the greedy one, and selecting the most consistent answer by marginalizing out the sampled rationales. The idea is also used in Fu et al. (2022b) to vote over the top complex rationales. To further improve performance,  suggest providing different demonstrations for each question by sampling exemplars from an exemplar base, in order to increase the diversity of the sampled rationales.', 'Rationale verification. Ensuring that the rationales produced by LLMs are valid is critical, as incorrect rationales can lead to incorrect final predictions (Ye and Durrett, 2022). To address this issue, the process of rationale verification aims to verify whether the rationales produced by LLMs lead to the correct final answers. Cobbe et al. (2021) propose augmenting LLMs with a trained verifier that assigns a score to each rationale and solution generated by the LLM, selecting the highest-ranked solution as the final answer when solving math word problems.  also use this technique to guide rationale selection, in conjunction with the process of rationale exploration. Different from the above methods that train an external verifier to verify the rationales, Weng et al. (2022) suggest using LLMs themselves as the verifiers.']","The original version of chain-of-thought prompting, proposed by Wei et al. (2022b), relies on manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation. Rationale engineering aims to more effectively elicit or utilize reasoning in LLMs. This can be achieved through rationale refinement, which involves creating more effective examples of reasoning steps, or through rationale exploration and rationale verification, which involve exploring and verifying the rationales produced by LLMs. A summary of raltionale engineering is illustrated in Figure 2.

Rationale refinement. The choice of exemplars can significantly affect the few-shot performance of LLMs, as demonstrated in research such as , which also appears in chain-of-thought prompting. Rationale refinement aims to create and refine rationale examples that are better able to elicit reasoning in LLMs. Fu et al. (2022b) propose complexity-based prompting to create rationales with more reasoning steps. Their experiments show that the performance of LLMs improves with the increased rationale complexity. Similarly,  propose algorithmic prompting, which suggests that providing more thorough examples of solutions can help improve reasoning performance on some simple math calculations.  design Auto-CoT to automatically construct exemplars by partitioning questions from a given dataset into clusters and then using Zero-Shot-CoT (Kojima et al., 2022) to generate the rationale for a representative question from each cluster. The analysis shows that making exemplars diverse is important in prompting LLMs to produce better rationales.

Rationale exploration. In addition to providing better exemplars, we can allow LLMs to fully explore various ways of reasoning to improve their performance on reasoning tasks, named rationale exploration. Based on the idea that complex problems often admit multiple ways of thinking that can lead to their unique correct answer, Wang et al. (2022c) present a decoding strategy called selfconsistency to improve upon the traditional greedy decoding used in chain-of-thought prompting. This strategy involves sampling a diverse set of rationales, rather than just the greedy one, and selecting the most consistent answer by marginalizing out the sampled rationales. The idea is also used in Fu et al. (2022b) to vote over the top complex rationales. To further improve performance,  suggest providing different demonstrations for each question by sampling exemplars from an exemplar base, in order to increase the diversity of the sampled rationales.

Rationale verification. Ensuring that the rationales produced by LLMs are valid is critical, as incorrect rationales can lead to incorrect final predictions (Ye and Durrett, 2022). To address this issue, the process of rationale verification aims to verify whether the rationales produced by LLMs lead to the correct final answers. Cobbe et al. (2021) propose augmenting LLMs with a trained verifier that assigns a score to each rationale and solution generated by the LLM, selecting the highest-ranked solution as the final answer when solving math word problems.  also use this technique to guide rationale selection, in conjunction with the process of rationale exploration. Different from the above methods that train an external verifier to verify the rationales, Weng et al. (2022) suggest using LLMs themselves as the verifiers.","(p6.0) The original version of chain-of-thought prompting, proposed by Wei et al. (2022b), relies on manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation. Rationale engineering aims to more effectively elicit or utilize reasoning in LLMs. This can be achieved through rationale refinement, which involves creating more effective examples of reasoning steps, or through rationale exploration and rationale verification, which involve exploring and verifying the rationales produced by LLMs. A summary of raltionale engineering is illustrated in Figure 2.

(p6.1) Rationale refinement. The choice of exemplars can significantly affect the few-shot performance of LLMs, as demonstrated in research such as , which also appears in chain-of-thought prompting. Rationale refinement aims to create and refine rationale examples that are better able to elicit reasoning in LLMs. Fu et al. (2022b) propose complexity-based prompting to create rationales with more reasoning steps. Their experiments show that the performance of LLMs improves with the increased rationale complexity. Similarly,  propose algorithmic prompting, which suggests that providing more thorough examples of solutions can help improve reasoning performance on some simple math calculations.  design Auto-CoT to automatically construct exemplars by partitioning questions from a given dataset into clusters and then using Zero-Shot-CoT (Kojima et al., 2022) to generate the rationale for a representative question from each cluster. The analysis shows that making exemplars diverse is important in prompting LLMs to produce better rationales.

(p6.2) Rationale exploration. In addition to providing better exemplars, we can allow LLMs to fully explore various ways of reasoning to improve their performance on reasoning tasks, named rationale exploration. Based on the idea that complex problems often admit multiple ways of thinking that can lead to their unique correct answer, Wang et al. (2022c) present a decoding strategy called selfconsistency to improve upon the traditional greedy decoding used in chain-of-thought prompting. This strategy involves sampling a diverse set of rationales, rather than just the greedy one, and selecting the most consistent answer by marginalizing out the sampled rationales. The idea is also used in Fu et al. (2022b) to vote over the top complex rationales. To further improve performance,  suggest providing different demonstrations for each question by sampling exemplars from an exemplar base, in order to increase the diversity of the sampled rationales.

(p6.3) Rationale verification. Ensuring that the rationales produced by LLMs are valid is critical, as incorrect rationales can lead to incorrect final predictions (Ye and Durrett, 2022). To address this issue, the process of rationale verification aims to verify whether the rationales produced by LLMs lead to the correct final answers. Cobbe et al. (2021) propose augmenting LLMs with a trained verifier that assigns a score to each rationale and solution generated by the LLM, selecting the highest-ranked solution as the final answer when solving math word problems.  also use this technique to guide rationale selection, in conjunction with the process of rationale exploration. Different from the above methods that train an external verifier to verify the rationales, Weng et al. (2022) suggest using LLMs themselves as the verifiers.","[['b78'], ['b23', 'b16'], ['b23', 'b74'], ['b82', 'b79']]","[['b78'], ['b23', 'b16'], ['b23', 'b74'], ['b82', 'b79']]",7,"1. The original version of chain-of-thought prompting, proposed by Wei et al. (2022b), relies on manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation.
2. Rationale engineering aims to more effectively elicit or utilize reasoning in LLMs.
3. This can be achieved through rationale refinement, which involves creating more effective examples of reasoning steps, or through rationale exploration and rationale verification, which involve exploring and verifying the rationales produced by LLMs.
4. A summary of raltionale engineering is illustrated in Figure 2.Rationale refinement.
5. The choice of exemplars can significantly affect the few-shot performance of LLMs, as demonstrated in research such as , which also appears in chain-of-thought prompting.
6. Rationale refinement aims to create and refine rationale examples that are better able to elicit reasoning in LLMs.
7. Fu et al. (2022b) propose complexity-based prompting to create rationales with more reasoning steps.
8. Their experiments show that the performance of LLMs improves with the increased rationale complexity.
9. Similarly,  propose algorithmic prompting, which suggests that providing more thorough examples of solutions can help improve reasoning performance on some simple math calculations.
10. design Auto-CoT to automatically construct exemplars by partitioning questions from a given dataset into clusters and then using Zero-Shot-CoT (Kojima et al., 2022) to generate the rationale for a representative question from each cluster.
11. The analysis shows that making exemplars diverse is important in prompting LLMs to produce better rationales.
12. Rationale exploration. In addition to providing better exemplars, we can allow LLMs to fully explore various ways of reasoning to improve their performance on reasoning tasks, named rationale exploration.
13. Based on the idea that complex problems often admit multiple ways of thinking that can lead to their unique correct answer, Wang et al. (2022c) present a decoding strategy called selfconsistency to improve upon the traditional greedy decoding used in chain-of-thought prompting.
14. This strategy involves sampling a diverse set of rationales, rather than just the greedy one, and selecting the most consistent answer by marginalizing out the sampled rationales.
15. The idea is also used in Fu et al. (2022b) to vote over the top complex rationales.
16. To further improve performance,  suggest providing different demonstrations for each question by sampling exemplars from an exemplar base, in order to increase the diversity of the sampled rationales.
17. Rationale verification. Ensuring that the rationales produced by LLMs are valid is critical, as incorrect rationales can lead to incorrect final predictions (Ye and Durrett, 2022).
18. To address this issue, the process of rationale verification aims to verify whether the rationales produced by LLMs lead to the correct final answers.
19. Cobbe et al. (2021) propose augmenting LLMs with a trained verifier that assigns a score to each rationale and solution generated by the LLM, selecting the highest-ranked solution as the final answer when solving math word problems.
20. also use this technique to guide rationale selection, in conjunction with the process of rationale exploration.
21. Different from the above methods that train an external verifier to verify the rationales, Weng et al. (2022) suggest using LLMs themselves as the verifiers."
256231532,Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,"Psychology, Computer Science",https://www.semanticscholar.org/paper/a1275e92f4830e5bbd53bc1b1fa44a9a2f024f88,s9,Task Evaluation:,"['p9.0', 'p9.1', 'p9.2']","[""Another key limitation of existing work is the lack of a comprehensive evaluation. Prior work majorly focused on objective metrics which only provides a limited view of the model performance. A comprehensive evaluation is challenging since it must consider partner perception along with objective outcomes. Building user simulators could potentially alleviate this problem (Li et al., 2016;Jain et al., 2018;. Most existing simulators are developed for task-oriented systems which follow a certain agenda. Future research should study how to use partner modeling to build social influence user simulators for more efficient and accurate task evaluation (He et al., 2018;Yang et al., 2020). For instance, one could potentially design different user personalities and simulate the change in user's beliefs, opinions, and attitudes accordingly (Yang et al., 2021)."", 'Multimodal systems: Being a core function of human communication, social influence occurs not just through text, but through all possible modalities. Schulman and Bickmore (2009) showed that embodied agents achieve better persuasion results than text-only agents. Other studies have recognized the importance of emotion in social influence tasks (Asai et al., 2020;Chawla et al., 2021a). Nguyen et al. (2021) proposed a speech dataset in debates and study the influence of spoken tactics on persuasiveness across genders. Given these findings, we encourage interdisciplinary efforts in the future to explore the developement of multimodal social influence agents.', ""Knowledge-enriched systems: Social influence tasks often involve constantly-changing world knowledge such as organization facts and news. Often, the system's internal state (e.g., the change of task setting from one set of products to a different set) needs to be updated. Retraining the entire system is costly to maintain after the initial development. Recent work has proposed to augment the dialogue system with internet-search ability to generate more factual and updated responses in open-domain dialogues (Komeili et al., 2021). Future efforts in this direction will benefit social influence dialogue systems as well.""]","Another key limitation of existing work is the lack of a comprehensive evaluation. Prior work majorly focused on objective metrics which only provides a limited view of the model performance. A comprehensive evaluation is challenging since it must consider partner perception along with objective outcomes. Building user simulators could potentially alleviate this problem (Li et al., 2016;Jain et al., 2018;. Most existing simulators are developed for task-oriented systems which follow a certain agenda. Future research should study how to use partner modeling to build social influence user simulators for more efficient and accurate task evaluation (He et al., 2018;Yang et al., 2020). For instance, one could potentially design different user personalities and simulate the change in user's beliefs, opinions, and attitudes accordingly (Yang et al., 2021).

Multimodal systems: Being a core function of human communication, social influence occurs not just through text, but through all possible modalities. Schulman and Bickmore (2009) showed that embodied agents achieve better persuasion results than text-only agents. Other studies have recognized the importance of emotion in social influence tasks (Asai et al., 2020;Chawla et al., 2021a). Nguyen et al. (2021) proposed a speech dataset in debates and study the influence of spoken tactics on persuasiveness across genders. Given these findings, we encourage interdisciplinary efforts in the future to explore the developement of multimodal social influence agents.

Knowledge-enriched systems: Social influence tasks often involve constantly-changing world knowledge such as organization facts and news. Often, the system's internal state (e.g., the change of task setting from one set of products to a different set) needs to be updated. Retraining the entire system is costly to maintain after the initial development. Recent work has proposed to augment the dialogue system with internet-search ability to generate more factual and updated responses in open-domain dialogues (Komeili et al., 2021). Future efforts in this direction will benefit social influence dialogue systems as well.","(p9.0) Another key limitation of existing work is the lack of a comprehensive evaluation. Prior work majorly focused on objective metrics which only provides a limited view of the model performance. A comprehensive evaluation is challenging since it must consider partner perception along with objective outcomes. Building user simulators could potentially alleviate this problem (Li et al., 2016;Jain et al., 2018;. Most existing simulators are developed for task-oriented systems which follow a certain agenda. Future research should study how to use partner modeling to build social influence user simulators for more efficient and accurate task evaluation (He et al., 2018;Yang et al., 2020). For instance, one could potentially design different user personalities and simulate the change in user's beliefs, opinions, and attitudes accordingly (Yang et al., 2021).

(p9.1) Multimodal systems: Being a core function of human communication, social influence occurs not just through text, but through all possible modalities. Schulman and Bickmore (2009) showed that embodied agents achieve better persuasion results than text-only agents. Other studies have recognized the importance of emotion in social influence tasks (Asai et al., 2020;Chawla et al., 2021a). Nguyen et al. (2021) proposed a speech dataset in debates and study the influence of spoken tactics on persuasiveness across genders. Given these findings, we encourage interdisciplinary efforts in the future to explore the developement of multimodal social influence agents.

(p9.2) Knowledge-enriched systems: Social influence tasks often involve constantly-changing world knowledge such as organization facts and news. Often, the system's internal state (e.g., the change of task setting from one set of products to a different set) needs to be updated. Retraining the entire system is costly to maintain after the initial development. Recent work has proposed to augment the dialogue system with internet-search ability to generate more factual and updated responses in open-domain dialogues (Komeili et al., 2021). Future efforts in this direction will benefit social influence dialogue systems as well.","[['b55', 'b56', 'b8', 'b22', 'b5'], [None, 'b28', 'b41'], [None]]","[['b55', 'b56', 'b8', 'b22', 'b5'], [None, 'b28', 'b41'], [None]]",9,"1. Another key limitation of existing work is the lack of a comprehensive evaluation.
2. Prior work majorly focused on objective metrics which only provides a limited view of the model performance.
3. A comprehensive evaluation is challenging since it must consider partner perception along with objective outcomes.
4. Building user simulators could potentially alleviate this problem (Li et al., 2016;Jain et al., 2018;. Most existing simulators are developed for task-oriented systems which follow a certain agenda. Future research should study how to use partner modeling to build social influence user simulators for more efficient and accurate task evaluation (He et al., 2018;Yang et al., 2020).
5. For instance, one could potentially design different user personalities and simulate the change in user's beliefs, opinions, and attitudes accordingly (Yang et al., 2021).
6. Multimodal systems: Being a core function of human communication, social influence occurs not just through text, but through all possible modalities.
7. Schulman and Bickmore (2009) showed that embodied agents achieve better persuasion results than text-only agents.
8. Other studies have recognized the importance of emotion in social influence tasks (Asai et al., 2020;Chawla et al., 2021a).
9. Nguyen et al. (2021) proposed a speech dataset in debates and study the influence of spoken tactics on persuasiveness across genders.
10. Given these findings, we encourage interdisciplinary efforts in the future to explore the developement of multimodal social influence agents.
11. Knowledge-enriched systems: Social influence tasks often involve constantly-changing world knowledge such as organization facts and news.
12. Often, the system's internal state (e.g., the change of task setting from one set of products to a different set) needs to be updated.
13. Retraining the entire system is costly to maintain after the initial development.
14. Recent work has proposed to augment the dialogue system with internet-search ability to generate more factual and updated responses in open-domain dialogues (Komeili et al., 2021).
15. Future efforts in this direction will benefit social influence dialogue systems as well."
256231532,Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,"Psychology, Computer Science",https://www.semanticscholar.org/paper/a1275e92f4830e5bbd53bc1b1fa44a9a2f024f88,s3,Methodological Progress,"['p3.0', 'p3.1', 'p3.2']","['Having summarized the datasets that capture social influence, we now discuss the modeling approaches developed for social influence dialogue systems. Most domains have seen efforts in analyzing human dialogue behaviors and their impact on task outcomes. Examples include analyzing deception in games (Peskov et al., 2020), the impact of persuasive strategies and dialogue acts on charity donations ( In addition, researchers have targeted various domain-specific subtasks that can be crucial for the eventual development of dialogue systems in this space. This involves research in lie detection methods (Yeh and Ku, 2021;Yu et al., 2015), discourse parsing (Shi and Huang, 2019;Ouyang et al., 2021), strategy prediction (Chawla et al., 2021b;, breakdown detection (Yamaguchi et al., 2021), outcome prediction (Sinha and Dasgupta, 2021;Chawla et al., 2020;Dutt et al., 2020), and argument mining (Dutta et al., 2022).', 'Research that directly targets the development of dialogue systems in this space is still nascent. Among other challenges like limited cross-cultural diversity and relatively smaller dataset size, social influence dialogue settings pose a unique challenge: an average human often exhibits sub-optimal strategic behaviors in social influence tasks (Wunderle, 2007;Babcock and Laschever, 2009). This means that standard seq2seq approaches trained on these collected datasets using supervised learning are fundamentally insufficient for developing effective dialogue systems with influence capabilities. Hence, prior work has put a special attention to the system strategy, employing different ways to model the strategy and language together.', 'We design a taxonomy of methods developed for social influence tasks, assisting readers to comprehend the progress and reflect on future directions. We organize them based on the system strategy, language generation, partner model, architecture, learning process, and the use of pretrained language models. We present annotations for all the surveyed methods in Table 2 and discuss the common categories in brief below.']","Having summarized the datasets that capture social influence, we now discuss the modeling approaches developed for social influence dialogue systems. Most domains have seen efforts in analyzing human dialogue behaviors and their impact on task outcomes. Examples include analyzing deception in games (Peskov et al., 2020), the impact of persuasive strategies and dialogue acts on charity donations ( In addition, researchers have targeted various domain-specific subtasks that can be crucial for the eventual development of dialogue systems in this space. This involves research in lie detection methods (Yeh and Ku, 2021;Yu et al., 2015), discourse parsing (Shi and Huang, 2019;Ouyang et al., 2021), strategy prediction (Chawla et al., 2021b;, breakdown detection (Yamaguchi et al., 2021), outcome prediction (Sinha and Dasgupta, 2021;Chawla et al., 2020;Dutt et al., 2020), and argument mining (Dutta et al., 2022).

Research that directly targets the development of dialogue systems in this space is still nascent. Among other challenges like limited cross-cultural diversity and relatively smaller dataset size, social influence dialogue settings pose a unique challenge: an average human often exhibits sub-optimal strategic behaviors in social influence tasks (Wunderle, 2007;Babcock and Laschever, 2009). This means that standard seq2seq approaches trained on these collected datasets using supervised learning are fundamentally insufficient for developing effective dialogue systems with influence capabilities. Hence, prior work has put a special attention to the system strategy, employing different ways to model the strategy and language together.

We design a taxonomy of methods developed for social influence tasks, assisting readers to comprehend the progress and reflect on future directions. We organize them based on the system strategy, language generation, partner model, architecture, learning process, and the use of pretrained language models. We present annotations for all the surveyed methods in Table 2 and discuss the common categories in brief below.","(p3.0) Having summarized the datasets that capture social influence, we now discuss the modeling approaches developed for social influence dialogue systems. Most domains have seen efforts in analyzing human dialogue behaviors and their impact on task outcomes. Examples include analyzing deception in games (Peskov et al., 2020), the impact of persuasive strategies and dialogue acts on charity donations ( In addition, researchers have targeted various domain-specific subtasks that can be crucial for the eventual development of dialogue systems in this space. This involves research in lie detection methods (Yeh and Ku, 2021;Yu et al., 2015), discourse parsing (Shi and Huang, 2019;Ouyang et al., 2021), strategy prediction (Chawla et al., 2021b;, breakdown detection (Yamaguchi et al., 2021), outcome prediction (Sinha and Dasgupta, 2021;Chawla et al., 2020;Dutt et al., 2020), and argument mining (Dutta et al., 2022).

(p3.1) Research that directly targets the development of dialogue systems in this space is still nascent. Among other challenges like limited cross-cultural diversity and relatively smaller dataset size, social influence dialogue settings pose a unique challenge: an average human often exhibits sub-optimal strategic behaviors in social influence tasks (Wunderle, 2007;Babcock and Laschever, 2009). This means that standard seq2seq approaches trained on these collected datasets using supervised learning are fundamentally insufficient for developing effective dialogue systems with influence capabilities. Hence, prior work has put a special attention to the system strategy, employing different ways to model the strategy and language together.

(p3.2) We design a taxonomy of methods developed for social influence tasks, assisting readers to comprehend the progress and reflect on future directions. We organize them based on the system strategy, language generation, partner model, architecture, learning process, and the use of pretrained language models. We present annotations for all the surveyed methods in Table 2 and discuss the common categories in brief below.","[['b58', 'b32', 'b33', 'b44', 'b54', None, 'b59', 'b43'], ['b53', None], []]","[['b58', 'b32', 'b33', 'b44', 'b54', None, 'b59', 'b43'], ['b53', None], []]",10,"1. Having summarized the datasets that capture social influence, we now discuss the modeling approaches developed for social influence dialogue systems.
2. Most domains have seen efforts in analyzing human dialogue behaviors and their impact on task outcomes.
3. Examples include analyzing deception in games (Peskov et al., 2020), the impact of persuasive strategies and dialogue acts on charity donations ( In addition, researchers have targeted various domain-specific subtasks that can be crucial for the eventual development of dialogue systems in this space. This involves research in lie detection methods (Yeh and Ku, 2021;Yu et al., 2015), discourse parsing (Shi and Huang, 2019;Ouyang et al., 2021), strategy prediction (Chawla et al., 2021b;, breakdown detection (Yamaguchi et al., 2021), outcome prediction (Sinha and Dasgupta, 2021;Chawla et al., 2020;Dutt et al., 2020), and argument mining (Dutta et al., 2022).
4. Research that directly targets the development of dialogue systems in this space is still nascent.
5. Among other challenges like limited cross-cultural diversity and relatively smaller dataset size, social influence dialogue settings pose a unique challenge: an average human often exhibits sub-optimal strategic behaviors in social influence tasks (Wunderle, 2007;Babcock and Laschever, 2009).
6. This means that standard seq2seq approaches trained on these collected datasets using supervised learning are fundamentally insufficient for developing effective dialogue systems with influence capabilities.
7. Hence, prior work has put a special attention to the system strategy, employing different ways to model the strategy and language together.
8. We design a taxonomy of methods developed for social influence tasks, assisting readers to comprehend the progress and reflect on future directions.
9. We organize them based on the system strategy, language generation, partner model, architecture, learning process, and the use of pretrained language models.
10. We present annotations for all the surveyed methods in Table 2 and discuss the common categories in brief below."
256231532,Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,"Psychology, Computer Science",https://www.semanticscholar.org/paper/a1275e92f4830e5bbd53bc1b1fa44a9a2f024f88,s4,Strategy Representation,['p4.0'],"['Implicit: The most obvious way to represent the system strategy is implicitly, without any intended decoupling between system strategy and response realization. This corresponds to the usual sequenceto-sequence framework that has been a standard baseline for the methods developed in this space. An important example is the work by Lewis et al. (2017), who were one of the first works to train endto-end dialogue models that exhibit social influence. The authors employed a neural network based on GRUs, one for encoding the negotiation context, one to encode the dialogue utterances, and two recurrent units to generate the output agreement in a bidirectional manner. Latent vectors: Yarats and Lewis (2018) explored latent vectors to decouple utterance semantics from its linguistic aspects. Their hierarchical approach first constructs a latent vector from the input message, which is then used for response generation and planning. These latent vectors are trained to maximize the likelihood of future dialogue messages and actions, which enables the decoupling between semantics and realization. Dialogue Acts (DAs): Dialogue Acts, such as greeting, offer propose, agreement, or disagreement, are effective at capturing a high-level structure of the dialogue flow in social influence settings, reducing the model strategy to first predicting the dialogue act for the next response. The use of DAs makes it convenient to apply reinforcement learning approaches (Zhang et al., 2020b;Yang et al., 2021), while also aiding in developing a modular dialogue system design (He et al., 2018). Semantic Strategies: The structural properties ex-  ']","Implicit: The most obvious way to represent the system strategy is implicitly, without any intended decoupling between system strategy and response realization. This corresponds to the usual sequenceto-sequence framework that has been a standard baseline for the methods developed in this space. An important example is the work by Lewis et al. (2017), who were one of the first works to train endto-end dialogue models that exhibit social influence. The authors employed a neural network based on GRUs, one for encoding the negotiation context, one to encode the dialogue utterances, and two recurrent units to generate the output agreement in a bidirectional manner. Latent vectors: Yarats and Lewis (2018) explored latent vectors to decouple utterance semantics from its linguistic aspects. Their hierarchical approach first constructs a latent vector from the input message, which is then used for response generation and planning. These latent vectors are trained to maximize the likelihood of future dialogue messages and actions, which enables the decoupling between semantics and realization. Dialogue Acts (DAs): Dialogue Acts, such as greeting, offer propose, agreement, or disagreement, are effective at capturing a high-level structure of the dialogue flow in social influence settings, reducing the model strategy to first predicting the dialogue act for the next response. The use of DAs makes it convenient to apply reinforcement learning approaches (Zhang et al., 2020b;Yang et al., 2021), while also aiding in developing a modular dialogue system design (He et al., 2018). Semantic Strategies: The structural properties ex-  ","(p4.0) Implicit: The most obvious way to represent the system strategy is implicitly, without any intended decoupling between system strategy and response realization. This corresponds to the usual sequenceto-sequence framework that has been a standard baseline for the methods developed in this space. An important example is the work by Lewis et al. (2017), who were one of the first works to train endto-end dialogue models that exhibit social influence. The authors employed a neural network based on GRUs, one for encoding the negotiation context, one to encode the dialogue utterances, and two recurrent units to generate the output agreement in a bidirectional manner. Latent vectors: Yarats and Lewis (2018) explored latent vectors to decouple utterance semantics from its linguistic aspects. Their hierarchical approach first constructs a latent vector from the input message, which is then used for response generation and planning. These latent vectors are trained to maximize the likelihood of future dialogue messages and actions, which enables the decoupling between semantics and realization. Dialogue Acts (DAs): Dialogue Acts, such as greeting, offer propose, agreement, or disagreement, are effective at capturing a high-level structure of the dialogue flow in social influence settings, reducing the model strategy to first predicting the dialogue act for the next response. The use of DAs makes it convenient to apply reinforcement learning approaches (Zhang et al., 2020b;Yang et al., 2021), while also aiding in developing a modular dialogue system design (He et al., 2018). Semantic Strategies: The structural properties ex-  ","[['b18', 'b56', 'b5', 'b62']]","[['b18', 'b56', 'b5', 'b62']]",4,"1. Implicit: The most obvious way to represent the system strategy is implicitly, without any intended decoupling between system strategy and response realization.
2. This corresponds to the usual sequenceto-sequence framework that has been a standard baseline for the methods developed in this space.
3. An important example is the work by Lewis et al. (2017), who were one of the first works to train endto-end dialogue models that exhibit social influence.
4. The authors employed a neural network based on GRUs, one for encoding the negotiation context, one to encode the dialogue utterances, and two recurrent units to generate the output agreement in a bidirectional manner.
5. Latent vectors: Yarats and Lewis (2018) explored latent vectors to decouple utterance semantics from its linguistic aspects.
6. Their hierarchical approach first constructs a latent vector from the input message, which is then used for response generation and planning.
7. These latent vectors are trained to maximize the likelihood of future dialogue messages and actions, which enables the decoupling between semantics and realization.
8. Dialogue Acts (DAs): Dialogue Acts, such as greeting, offer propose, agreement, or disagreement, are effective at capturing a high-level structure of the dialogue flow in social influence settings, reducing the model strategy to first predicting the dialogue act for the next response.
9. The use of DAs makes it convenient to apply reinforcement learning approaches (Zhang et al., 2020b;Yang et al., 2021), while also aiding in developing a modular dialogue system design (He et al., 2018).
10. Semantic Strategies: The structural properties ex-"
256231532,Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,"Psychology, Computer Science",https://www.semanticscholar.org/paper/a1275e92f4830e5bbd53bc1b1fa44a9a2f024f88,s5,Language Generation,"['p5.0', 'p5.1']","['An important aspect of the system design is an effective way to realize the language, that is, to generate the next response so that it portrays the desired strategic behaviors. Borrowing from task-oriented and open-domain research, existing dialogue models for social influence use a variety of methods to generate the final system response.', 'Templates and retrieval methods: Predefined templates and response retrieval from the training data simplify the generation pipeline, improving controllability and modularity. He et al. (2018) used templates in their generator which are later filled by retrieving similar responses from the data. This allowed the authors to explore supervised and reinforcement learning at the level of DAs for the influence strategy of the system. Conditional Generation: Text generation methods result in more diverse responses, but negatively impact the controllability and interpretability. Prior work relies on autoregressive text generation conditioned on the dialogue history, non-conversational context, and additional annotations. These are either encoder-decoder networks (Lewis et al., 2017;Li et al., 2020;Joshi et al., 2020) or use a decoderonly design (Li et al., 2020). A useful future direction is to combine generation with retrieval for knowledge-grounded settings like argumentation. Similar methods have been explored for other NLP tasks like open-domain question answering and question generation (Lewis et al., 2020).']","An important aspect of the system design is an effective way to realize the language, that is, to generate the next response so that it portrays the desired strategic behaviors. Borrowing from task-oriented and open-domain research, existing dialogue models for social influence use a variety of methods to generate the final system response.

Templates and retrieval methods: Predefined templates and response retrieval from the training data simplify the generation pipeline, improving controllability and modularity. He et al. (2018) used templates in their generator which are later filled by retrieving similar responses from the data. This allowed the authors to explore supervised and reinforcement learning at the level of DAs for the influence strategy of the system. Conditional Generation: Text generation methods result in more diverse responses, but negatively impact the controllability and interpretability. Prior work relies on autoregressive text generation conditioned on the dialogue history, non-conversational context, and additional annotations. These are either encoder-decoder networks (Lewis et al., 2017;Li et al., 2020;Joshi et al., 2020) or use a decoderonly design (Li et al., 2020). A useful future direction is to combine generation with retrieval for knowledge-grounded settings like argumentation. Similar methods have been explored for other NLP tasks like open-domain question answering and question generation (Lewis et al., 2020).","(p5.0) An important aspect of the system design is an effective way to realize the language, that is, to generate the next response so that it portrays the desired strategic behaviors. Borrowing from task-oriented and open-domain research, existing dialogue models for social influence use a variety of methods to generate the final system response.

(p5.1) Templates and retrieval methods: Predefined templates and response retrieval from the training data simplify the generation pipeline, improving controllability and modularity. He et al. (2018) used templates in their generator which are later filled by retrieving similar responses from the data. This allowed the authors to explore supervised and reinforcement learning at the level of DAs for the influence strategy of the system. Conditional Generation: Text generation methods result in more diverse responses, but negatively impact the controllability and interpretability. Prior work relies on autoregressive text generation conditioned on the dialogue history, non-conversational context, and additional annotations. These are either encoder-decoder networks (Lewis et al., 2017;Li et al., 2020;Joshi et al., 2020) or use a decoderonly design (Li et al., 2020). A useful future direction is to combine generation with retrieval for knowledge-grounded settings like argumentation. Similar methods have been explored for other NLP tasks like open-domain question answering and question generation (Lewis et al., 2020).","[[], ['b18', 'b12', 'b19', 'b23', 'b5']]","[[], ['b18', 'b12', 'b19', 'b23', 'b5']]",5,"1. An important aspect of the system design is an effective way to realize the language, that is, to generate the next response so that it portrays the desired strategic behaviors.
2. Borrowing from task-oriented and open-domain research, existing dialogue models for social influence use a variety of methods to generate the final system response.Templates and retrieval methods: Predefined templates and response retrieval from the training data simplify the generation pipeline, improving controllability and modularity.
3. He et al. (2018) used templates in their generator which are later filled by retrieving similar responses from the data.
4. This allowed the authors to explore supervised and reinforcement learning at the level of DAs for the influence strategy of the system.
5. Conditional Generation: Text generation methods result in more diverse responses, but negatively impact the controllability and interpretability.
6. Prior work relies on autoregressive text generation conditioned on the dialogue history, non-conversational context, and additional annotations.
7. These are either encoder-decoder networks (Lewis et al., 2017;Li et al., 2020;Joshi et al., 2020) or use a decoderonly design (Li et al., 2020).
8. A useful future direction is to combine generation with retrieval for knowledge-grounded settings like argumentation.
9. Similar methods have been explored for other NLP tasks like open-domain question answering and question generation (Lewis et al., 2020)."
256231532,Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,"Psychology, Computer Science",https://www.semanticscholar.org/paper/a1275e92f4830e5bbd53bc1b1fa44a9a2f024f88,s7,Training,"['p7.0', 'p7.1']","['Architecture Choices: One crucial aspect is the architecture design: End-to-end (Lewis et al., 2017;Radford et al., 2019) vs Modular (He et al., 2018). While end-to-end methods improve the diversity and need less manual effort, a modularized design enhances controllability and explainability. Perhaps, this is why modular methods are popular in large-scale models (Hadfi et al., 2021). Improving the control of desired variables such as topics, strategy, or emotion in the end-to-end methods is an open area of research and is yet to be explored for social influence dialogue systems. The performance was later improved by Joshi et al. (2020), who replaced FSTs with Graph Neural Networks to better model the interdependencies. Others have relied on RL to explicitly optimize the model on task-specific objective outcomes. While SL trains the model to mimic the average human behavior, RL techniques, such as those based on REINFORCE (Williams, 1992), allow the system to explore its own strategies in the wild while being guided by one or more overall reward metrics. Lewis et al. (2017) used RL in negotiations, with the final points scored in the agreed deal as the reward. More recent work employed RL to incorporate simplistic partner models into the decisionmaking process of the dialogue system, showing improvements in negotiation tasks (Zhang et al., 2020b;Yang et al., 2021).', 'Multi-tasking and Pretraining: Limited efforts have also explored multi-tasking and pretrained language models for social influence dialogue systems, which provide promising ways to deal with the challenge of insufficient training data. Liu (2021) trained a sequence-to-sequence transformer on a mix of Cornell Movie Dialogue corpus (Danescu-Niculescu-Mizil and Lee, 2011) and psychotherapy data. Li et al. (2020) fine-tuned the GPT model (Radford et al., 2018), while employing multi-tasking to incorporate intents and slots for both the human and the system. Wu et al. (2021) recently introduced ARDM which uses GPT2 (Radford et al., 2019) to separately encode the utterances of the human and the dialogue system, reducing the reliance on additional annotations.']","Architecture Choices: One crucial aspect is the architecture design: End-to-end (Lewis et al., 2017;Radford et al., 2019) vs Modular (He et al., 2018). While end-to-end methods improve the diversity and need less manual effort, a modularized design enhances controllability and explainability. Perhaps, this is why modular methods are popular in large-scale models (Hadfi et al., 2021). Improving the control of desired variables such as topics, strategy, or emotion in the end-to-end methods is an open area of research and is yet to be explored for social influence dialogue systems. The performance was later improved by Joshi et al. (2020), who replaced FSTs with Graph Neural Networks to better model the interdependencies. Others have relied on RL to explicitly optimize the model on task-specific objective outcomes. While SL trains the model to mimic the average human behavior, RL techniques, such as those based on REINFORCE (Williams, 1992), allow the system to explore its own strategies in the wild while being guided by one or more overall reward metrics. Lewis et al. (2017) used RL in negotiations, with the final points scored in the agreed deal as the reward. More recent work employed RL to incorporate simplistic partner models into the decisionmaking process of the dialogue system, showing improvements in negotiation tasks (Zhang et al., 2020b;Yang et al., 2021).

Multi-tasking and Pretraining: Limited efforts have also explored multi-tasking and pretrained language models for social influence dialogue systems, which provide promising ways to deal with the challenge of insufficient training data. Liu (2021) trained a sequence-to-sequence transformer on a mix of Cornell Movie Dialogue corpus (Danescu-Niculescu-Mizil and Lee, 2011) and psychotherapy data. Li et al. (2020) fine-tuned the GPT model (Radford et al., 2018), while employing multi-tasking to incorporate intents and slots for both the human and the system. Wu et al. (2021) recently introduced ARDM which uses GPT2 (Radford et al., 2019) to separately encode the utterances of the human and the dialogue system, reducing the reliance on additional annotations.","(p7.0) Architecture Choices: One crucial aspect is the architecture design: End-to-end (Lewis et al., 2017;Radford et al., 2019) vs Modular (He et al., 2018). While end-to-end methods improve the diversity and need less manual effort, a modularized design enhances controllability and explainability. Perhaps, this is why modular methods are popular in large-scale models (Hadfi et al., 2021). Improving the control of desired variables such as topics, strategy, or emotion in the end-to-end methods is an open area of research and is yet to be explored for social influence dialogue systems. The performance was later improved by Joshi et al. (2020), who replaced FSTs with Graph Neural Networks to better model the interdependencies. Others have relied on RL to explicitly optimize the model on task-specific objective outcomes. While SL trains the model to mimic the average human behavior, RL techniques, such as those based on REINFORCE (Williams, 1992), allow the system to explore its own strategies in the wild while being guided by one or more overall reward metrics. Lewis et al. (2017) used RL in negotiations, with the final points scored in the agreed deal as the reward. More recent work employed RL to incorporate simplistic partner models into the decisionmaking process of the dialogue system, showing improvements in negotiation tasks (Zhang et al., 2020b;Yang et al., 2021).

(p7.1) Multi-tasking and Pretraining: Limited efforts have also explored multi-tasking and pretrained language models for social influence dialogue systems, which provide promising ways to deal with the challenge of insufficient training data. Liu (2021) trained a sequence-to-sequence transformer on a mix of Cornell Movie Dialogue corpus (Danescu-Niculescu-Mizil and Lee, 2011) and psychotherapy data. Li et al. (2020) fine-tuned the GPT model (Radford et al., 2018), while employing multi-tasking to incorporate intents and slots for both the human and the system. Wu et al. (2021) recently introduced ARDM which uses GPT2 (Radford et al., 2019) to separately encode the utterances of the human and the dialogue system, reducing the reliance on additional annotations.","[['b18', 'b56', 'b36', 'b62', 'b4', 'b12', 'b5'], ['b52', 'b23', 'b26', 'b35']]","[['b18', 'b56', 'b36', 'b62', 'b4', 'b12', 'b5'], ['b52', 'b23', 'b26', 'b35']]",11,"1. Architecture Choices: One crucial aspect is the architecture design: End-to-end (Lewis et al., 2017;Radford et al., 2019) vs Modular (He et al., 2018).
2. While end-to-end methods improve the diversity and need less manual effort, a modularized design enhances controllability and explainability.
3. Perhaps, this is why modular methods are popular in large-scale models (Hadfi et al., 2021).
4. Improving the control of desired variables such as topics, strategy, or emotion in the end-to-end methods is an open area of research and is yet to be explored for social influence dialogue systems.
5. The performance was later improved by Joshi et al. (2020), who replaced FSTs with Graph Neural Networks to better model the interdependencies.
6. Others have relied on RL to explicitly optimize the model on task-specific objective outcomes.
7. While SL trains the model to mimic the average human behavior, RL techniques, such as those based on REINFORCE (Williams, 1992), allow the system to explore its own strategies in the wild while being guided by one or more overall reward metrics.
8. Lewis et al. (2017) used RL in negotiations, with the final points scored in the agreed deal as the reward.
9. More recent work employed RL to incorporate simplistic partner models into the decisionmaking process of the dialogue system, showing improvements in negotiation tasks (Zhang et al., 2020b;Yang et al., 2021).
10. Multi-tasking and Pretraining: Limited efforts have also explored multi-tasking and pretrained language models for social influence dialogue systems, which provide promising ways to deal with the challenge of insufficient training data.
11. Liu (2021) trained a sequence-to-sequence transformer on a mix of Cornell Movie Dialogue corpus (Danescu-Niculescu-Mizil and Lee, 2011) and psychotherapy data.
12. Li et al. (2020) fine-tuned the GPT model (Radford et al., 2018), while employing multi-tasking to incorporate intents and slots for both the human and the system.
13. Wu et al. (2021) recently introduced ARDM which uses GPT2 (Radford et al., 2019) to separately encode the utterances of the human and the dialogue system, reducing the reliance on additional annotations."
256231532,Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,"Psychology, Computer Science",https://www.semanticscholar.org/paper/a1275e92f4830e5bbd53bc1b1fa44a9a2f024f88,s8,Discussion and Recommendations,"['p8.0', 'p8.1', 'p8.2', 'p8.3']","['Past few years have seen an exciting progress in social influence dialogue systems. However, building sophisticated and practically useful systems remains a challenging endeavor. Several limitations still exist that must be addressed. To guide future work, we now discuss the key challenges and provide our recommendations. Need for unifying the efforts: One challenge in this space has been the lack of large-scale datasets for model training. Social influence tasks are complex for crowdsourcing workers to understand and to participate in. Hence, prior work used extensive instructions and tutorials, making the study expensive and time consuming Chawla et al., 2021b). To address this, we recommend the researchers to aim for a more unified view of the efforts in social influence. First, this would encourage researchers to adopt the best practices from other social influence scenarios. For instance, most datasets miss out on user attributes like demographics and personality, which are crucial in social influence scenarios (Stuhlmacher and Walters, 1999;Bogaert et al., 2008). Most datasets also ignore the partner perception after the interaction is over. This can result in misleading conclusions about the model performance, where the models perform well objectively, but hurt the relationship with their partners, and thus, negatively impacting practical utility (Aydogan et al., 2020).', 'Secondly, a holistic outlook will promote transfer learning and domain adaptation. Our taxonomy for datasets (Table 1) governs the way systems must be modeled and trained. Task structure is crucial to understand whether the model can learn from the utterances of all parties or just one. Further, understanding the context definition guides how it must be encoded. Hence, one interesting future direction is joint training on datasets with similar structure and context definition. Finally, progress in task-oriented and opendomain systems can inspire more unified modeling for social influence tasks involving multiple skills in the same interaction (e.g. a combination of negotiation and persuasion tactics as common in realistic scenarios). Roller et al. (2020) blend various open-domain tasks to address multiple challenges together (e.g., persona-based, knowledge-enriched, etc.). Hosseini-Asl et al. (2020) concatenate structured and unstructured data in task-oriented dialogues, and unify task-oriented dialogue system building to be a single sequence generation problem. Future work should explore similar unified approaches for social influence settings as well, especially since these tasks follow a common conceptual foundation (Figure 1), with similar evaluation and theoretical principles (Cialdini, 2009).', 'To encourage this unified view, we encapsulate our insights from this survey effort in a theoretical framework, which is presented in Appendix C. The framework covers key components for designing a social influence dialogue task, including system attributes, target audience, underlying modeling techniques, and evaluation mechanisms. Theory integration: Most modeling efforts are based on crowdsourced datasets. Since crowdsourcing workers may not exhibit optimal strategies, supervised training on these datasets is fundamentally insufficient to build an effective system for applications like pedagogy (teaching social skills to students). Unfortunately, this holds regardless of how system strategy and partner model are designed. Further, using RL to optimize on objective rewards is also not expected to be enough to reliably learn complex influence capabilities, especially when the reward is restrictive.', ""To address this, we recommend to tap into the vast amount of research effort in social sciences and psychology on building theories for social influence (Cameron, 2009;Giles, 2016;Lewicki et al., 2016;Cialdini and Goldstein, 2004). Instead of solely relying on the collected data, future work should consider leveraging fundamentals from this research to guide the dialogue policy. Previous works have studied resistance to social influence (Knowles and Linn, 2004;Dal Cin et al., 2004;Petty and Cacioppo, 1977;Ahluwalia, 2000). Rucker et al. (2004) found that people resist persuasion differently depending on their beliefs, suggest-ing personalizing the social influence process. One can also employ the politeness theory (Brown and Levinson, 1978) and model the participants' face acts to better understand users in social influence contexts (Dutt et al., 2020).""]","Past few years have seen an exciting progress in social influence dialogue systems. However, building sophisticated and practically useful systems remains a challenging endeavor. Several limitations still exist that must be addressed. To guide future work, we now discuss the key challenges and provide our recommendations. Need for unifying the efforts: One challenge in this space has been the lack of large-scale datasets for model training. Social influence tasks are complex for crowdsourcing workers to understand and to participate in. Hence, prior work used extensive instructions and tutorials, making the study expensive and time consuming Chawla et al., 2021b). To address this, we recommend the researchers to aim for a more unified view of the efforts in social influence. First, this would encourage researchers to adopt the best practices from other social influence scenarios. For instance, most datasets miss out on user attributes like demographics and personality, which are crucial in social influence scenarios (Stuhlmacher and Walters, 1999;Bogaert et al., 2008). Most datasets also ignore the partner perception after the interaction is over. This can result in misleading conclusions about the model performance, where the models perform well objectively, but hurt the relationship with their partners, and thus, negatively impacting practical utility (Aydogan et al., 2020).

Secondly, a holistic outlook will promote transfer learning and domain adaptation. Our taxonomy for datasets (Table 1) governs the way systems must be modeled and trained. Task structure is crucial to understand whether the model can learn from the utterances of all parties or just one. Further, understanding the context definition guides how it must be encoded. Hence, one interesting future direction is joint training on datasets with similar structure and context definition. Finally, progress in task-oriented and opendomain systems can inspire more unified modeling for social influence tasks involving multiple skills in the same interaction (e.g. a combination of negotiation and persuasion tactics as common in realistic scenarios). Roller et al. (2020) blend various open-domain tasks to address multiple challenges together (e.g., persona-based, knowledge-enriched, etc.). Hosseini-Asl et al. (2020) concatenate structured and unstructured data in task-oriented dialogues, and unify task-oriented dialogue system building to be a single sequence generation problem. Future work should explore similar unified approaches for social influence settings as well, especially since these tasks follow a common conceptual foundation (Figure 1), with similar evaluation and theoretical principles (Cialdini, 2009).

To encourage this unified view, we encapsulate our insights from this survey effort in a theoretical framework, which is presented in Appendix C. The framework covers key components for designing a social influence dialogue task, including system attributes, target audience, underlying modeling techniques, and evaluation mechanisms. Theory integration: Most modeling efforts are based on crowdsourced datasets. Since crowdsourcing workers may not exhibit optimal strategies, supervised training on these datasets is fundamentally insufficient to build an effective system for applications like pedagogy (teaching social skills to students). Unfortunately, this holds regardless of how system strategy and partner model are designed. Further, using RL to optimize on objective rewards is also not expected to be enough to reliably learn complex influence capabilities, especially when the reward is restrictive.

To address this, we recommend to tap into the vast amount of research effort in social sciences and psychology on building theories for social influence (Cameron, 2009;Giles, 2016;Lewicki et al., 2016;Cialdini and Goldstein, 2004). Instead of solely relying on the collected data, future work should consider leveraging fundamentals from this research to guide the dialogue policy. Previous works have studied resistance to social influence (Knowles and Linn, 2004;Dal Cin et al., 2004;Petty and Cacioppo, 1977;Ahluwalia, 2000). Rucker et al. (2004) found that people resist persuasion differently depending on their beliefs, suggest-ing personalizing the social influence process. One can also employ the politeness theory (Brown and Levinson, 1978) and model the participants' face acts to better understand users in social influence contexts (Dutt et al., 2020).","(p8.0) Past few years have seen an exciting progress in social influence dialogue systems. However, building sophisticated and practically useful systems remains a challenging endeavor. Several limitations still exist that must be addressed. To guide future work, we now discuss the key challenges and provide our recommendations. Need for unifying the efforts: One challenge in this space has been the lack of large-scale datasets for model training. Social influence tasks are complex for crowdsourcing workers to understand and to participate in. Hence, prior work used extensive instructions and tutorials, making the study expensive and time consuming Chawla et al., 2021b). To address this, we recommend the researchers to aim for a more unified view of the efforts in social influence. First, this would encourage researchers to adopt the best practices from other social influence scenarios. For instance, most datasets miss out on user attributes like demographics and personality, which are crucial in social influence scenarios (Stuhlmacher and Walters, 1999;Bogaert et al., 2008). Most datasets also ignore the partner perception after the interaction is over. This can result in misleading conclusions about the model performance, where the models perform well objectively, but hurt the relationship with their partners, and thus, negatively impacting practical utility (Aydogan et al., 2020).

(p8.1) Secondly, a holistic outlook will promote transfer learning and domain adaptation. Our taxonomy for datasets (Table 1) governs the way systems must be modeled and trained. Task structure is crucial to understand whether the model can learn from the utterances of all parties or just one. Further, understanding the context definition guides how it must be encoded. Hence, one interesting future direction is joint training on datasets with similar structure and context definition. Finally, progress in task-oriented and opendomain systems can inspire more unified modeling for social influence tasks involving multiple skills in the same interaction (e.g. a combination of negotiation and persuasion tactics as common in realistic scenarios). Roller et al. (2020) blend various open-domain tasks to address multiple challenges together (e.g., persona-based, knowledge-enriched, etc.). Hosseini-Asl et al. (2020) concatenate structured and unstructured data in task-oriented dialogues, and unify task-oriented dialogue system building to be a single sequence generation problem. Future work should explore similar unified approaches for social influence settings as well, especially since these tasks follow a common conceptual foundation (Figure 1), with similar evaluation and theoretical principles (Cialdini, 2009).

(p8.2) To encourage this unified view, we encapsulate our insights from this survey effort in a theoretical framework, which is presented in Appendix C. The framework covers key components for designing a social influence dialogue task, including system attributes, target audience, underlying modeling techniques, and evaluation mechanisms. Theory integration: Most modeling efforts are based on crowdsourced datasets. Since crowdsourcing workers may not exhibit optimal strategies, supervised training on these datasets is fundamentally insufficient to build an effective system for applications like pedagogy (teaching social skills to students). Unfortunately, this holds regardless of how system strategy and partner model are designed. Further, using RL to optimize on objective rewards is also not expected to be enough to reliably learn complex influence capabilities, especially when the reward is restrictive.

(p8.3) To address this, we recommend to tap into the vast amount of research effort in social sciences and psychology on building theories for social influence (Cameron, 2009;Giles, 2016;Lewicki et al., 2016;Cialdini and Goldstein, 2004). Instead of solely relying on the collected data, future work should consider leveraging fundamentals from this research to guide the dialogue policy. Previous works have studied resistance to social influence (Knowles and Linn, 2004;Dal Cin et al., 2004;Petty and Cacioppo, 1977;Ahluwalia, 2000). Rucker et al. (2004) found that people resist persuasion differently depending on their beliefs, suggest-ing personalizing the social influence process. One can also employ the politeness theory (Brown and Levinson, 1978) and model the participants' face acts to better understand users in social influence contexts (Dutt et al., 2020).","[['b45', None], [None, 'b39'], [], ['b34', 'b40', 'b14', None, 'b17', 'b2']]","[['b45', None], [None, 'b39'], [], ['b34', 'b40', 'b14', None, 'b17', 'b2']]",10,"1. Past few years have seen an exciting progress in social influence dialogue systems.
2. However, building sophisticated and practically useful systems remains a challenging endeavor.
3. Several limitations still exist that must be addressed.
4. To guide future work, we now discuss the key challenges and provide our recommendations.
5. Need for unifying the efforts: One challenge in this space has been the lack of large-scale datasets for model training.
6. Social influence tasks are complex for crowdsourcing workers to understand and to participate in.
7. Hence, prior work used extensive instructions and tutorials, making the study expensive and time consuming Chawla et al., 2021b).
8. To address this, we recommend the researchers to aim for a more unified view of the efforts in social influence.
9. First, this would encourage researchers to adopt the best practices from other social influence scenarios.
10. For instance, most datasets miss out on user attributes like demographics and personality, which are crucial in social influence scenarios (Stuhlmacher and Walters, 1999;Bogaert et al., 2008).
11. Most datasets also ignore the partner perception after the interaction is over.
12. This can result in misleading conclusions about the model performance, where the models perform well objectively, but hurt the relationship with their partners, and thus, negatively impacting practical utility (Aydogan et al., 2020).
13. Secondly, a holistic outlook will promote transfer learning and domain adaptation.
14. Our taxonomy for datasets (Table 1) governs the way systems must be modeled and trained.
15. Task structure is crucial to understand whether the model can learn from the utterances of all parties or just one.
16. Further, understanding the context definition guides how it must be encoded.
17. Hence, one interesting future direction is joint training on datasets with similar structure and context definition.
18. Finally, progress in task-oriented and opendomain systems can inspire more unified modeling for social influence tasks involving multiple skills in the same interaction (e.g. a combination of negotiation and persuasion tactics as common in realistic scenarios).
19. Roller et al. (2020) blend various open-domain tasks to address multiple challenges together (e.g., persona-based, knowledge-enriched, etc.).
20. Hosseini-Asl et al. (2020) concatenate structured and unstructured data in task-oriented dialogues, and unify task-oriented dialogue system building to be a single sequence generation problem.
21. Future work should explore similar unified approaches for social influence settings as well, especially since these tasks follow a common conceptual foundation (Figure 1), with similar evaluation and theoretical principles (Cialdini, 2009).
22. To encourage this unified view, we encapsulate our insights from this survey effort in a theoretical framework, which is presented in Appendix C. The framework covers key components for designing a social influence dialogue task, including system attributes, target audience, underlying modeling techniques, and evaluation mechanisms.
23. Theory integration: Most modeling efforts are based on crowdsourced datasets.
24. Since crowdsourcing workers may not exhibit optimal strategies, supervised training on these datasets is fundamentally insufficient to build an effective system for applications like pedagogy (teaching social skills to students).
25. Unfortunately, this holds regardless of how system strategy and partner model are designed.
26. Further, using RL to optimize on objective rewards is also not expected to be enough to reliably learn complex influence capabilities, especially when the reward is restrictive.
27. To address this, we recommend to tap into the vast amount of research effort in social sciences and psychology on building theories for social influence (Cameron, 2009;Giles, 2016;Lewicki et al., 2016;Cialdini and Goldstein, 2004).
28. Instead of solely relying on the collected data, future work should consider leveraging fundamentals from this research to guide the dialogue policy.
29. Previous works have studied resistance to social influence (Knowles and Linn, 2004;Dal Cin et al., 2004;Petty and Cacioppo, 1977;Ahluwalia, 2000).
30. Rucker et al. (2004) found that people resist persuasion differently depending on their beliefs, suggest-ing personalizing the social influence process.
31. One can also employ the politeness theory (Brown and Levinson, 1978) and model the participants' face acts to better understand users in social influence contexts (Dutt et al., 2020)."
256231532,Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,"Psychology, Computer Science",https://www.semanticscholar.org/paper/a1275e92f4830e5bbd53bc1b1fa44a9a2f024f88,s11,Broader Impact and Ethical Considerations,"['p11.0', 'p11.1', 'p11.2', 'p11.3', 'p11.4']","[""Social influence is ubiquitous in everyday life. Research on how we use influence in all aspects of our lives spans a number of fields, including social psychology, communication, consumer behavior, behavioral change, and behavioral economics. This research has led to crucial findings about the strategies of social influence and how they impact our decision-making. Over the past few decades, research has accumulated and demonstrated the effectiveness of using various strategies across contexts and domains. Prominent examples include core principles of social influence by Cialdini from social psychology: reciprocity, commitment and consistency, social proof, liking and attractiveness, authority, and scarcity (Cialdini, 2009). Further, communication strategies used in persuasion and general social influence contexts include credibility appeals, two-sided argumentation, emotional tactics, and appeals to social norms, among others (Cameron, 2009;O'keefe, 2015). First, the well-studied principles in social influence research can guide the development of effective dialogue systems with influence capabilities. In fact, many of the strategies found in the datasets developed for social influence tasks (Section 3) directly map to the principles laid out by Cialdini, for instance, credibility and emotional appeal in PersuasionForGood dataset  and reciprocity observed in CaSiNo negotiation dataset (Chawla et al., 2021b). Second, research in social influence dialogue systems provides novel datasets on human-human and humanmachine communication, and therefore, holds a great potential to advance theories of human cogni-tion and influence processes (Gratch et al., 2015). The datasets and subsequent analyses can further contribute new theoretical insights to social influence research."", 'Although dialogue systems have already been used in a number of applications involving chatbots and AI assistants, advancements in social influence dialogue systems can help to bridge the gap between our existing task definitions and a number of other real-world applications. For instance, realistic customer support interactions often involve active behaviors from both the support agent and the user where the agent uses social cues for improved customer satisfaction and retention, while the user attempts to address their queries. These settings naturally involve aspects of social influence, unlike traditional task-oriented definitions where the dialogue system plays a passive role to assist the human users. As discussed earlier, social influence dialogue systems can positively help to advance other areas as well. In therapy domain, these systems can assist in various psychological treatments such as by increasing the willingness to disclose (Lucas et al., 2014). In pedagogy, they can help to make social skills training more accessible (Johnson et al., 2019).', 'While we think about these applications, it is crucial to also lay out proper ethical guidelines to avoid any misuse of these systems. Primary concerns are around the use of deception (e.g. in Diplomacy and other negotiation tasks), emotional appeals (e.g. in persuasion), and behavior change (e.g. in conversational recommendations).', 'To mitigate possible misuse scenarios or unintended harms, we now lay out a few ethical guidelines which also apply to dialogue research in general. First, rigorous attempts must be made to ensure that the data collection, design processes, and evaluations, strictly abide by the guidelines and regulations laid out by the relevant Institutional Review Board (IRB). Second, the research team needs to develop a thorough plan to monitor and understand the behaviors of the developed systems before deployment. This includes identifying the goals of the dialogue system, identifying potential toxic language use, and any discriminatory behaviors. Third, investment into improved data collection practices, along with explainable and controllable dialogue systems can help identify these issues early on and allow manipulation to avoid them. Fourth, we argue that transparency is the key.', 'All stakeholders must be made aware of the goals and design objectives of the system, along with any known misbehaviors or potential risks. The users must also be informed of any data collected during the deployment phase. Lastly, we believe that continuous monitoring of dialogue systems is necessary to ensure that the system performs consistently and does not diverge to unexpected conditions that may incur offensive or discriminative actions. We hope that our work promotes a more systematic study of social influence dialogue systems, which in turn will help to tackle the ethical concerns in a more principled way.']","Social influence is ubiquitous in everyday life. Research on how we use influence in all aspects of our lives spans a number of fields, including social psychology, communication, consumer behavior, behavioral change, and behavioral economics. This research has led to crucial findings about the strategies of social influence and how they impact our decision-making. Over the past few decades, research has accumulated and demonstrated the effectiveness of using various strategies across contexts and domains. Prominent examples include core principles of social influence by Cialdini from social psychology: reciprocity, commitment and consistency, social proof, liking and attractiveness, authority, and scarcity (Cialdini, 2009). Further, communication strategies used in persuasion and general social influence contexts include credibility appeals, two-sided argumentation, emotional tactics, and appeals to social norms, among others (Cameron, 2009;O'keefe, 2015). First, the well-studied principles in social influence research can guide the development of effective dialogue systems with influence capabilities. In fact, many of the strategies found in the datasets developed for social influence tasks (Section 3) directly map to the principles laid out by Cialdini, for instance, credibility and emotional appeal in PersuasionForGood dataset  and reciprocity observed in CaSiNo negotiation dataset (Chawla et al., 2021b). Second, research in social influence dialogue systems provides novel datasets on human-human and humanmachine communication, and therefore, holds a great potential to advance theories of human cogni-tion and influence processes (Gratch et al., 2015). The datasets and subsequent analyses can further contribute new theoretical insights to social influence research.

Although dialogue systems have already been used in a number of applications involving chatbots and AI assistants, advancements in social influence dialogue systems can help to bridge the gap between our existing task definitions and a number of other real-world applications. For instance, realistic customer support interactions often involve active behaviors from both the support agent and the user where the agent uses social cues for improved customer satisfaction and retention, while the user attempts to address their queries. These settings naturally involve aspects of social influence, unlike traditional task-oriented definitions where the dialogue system plays a passive role to assist the human users. As discussed earlier, social influence dialogue systems can positively help to advance other areas as well. In therapy domain, these systems can assist in various psychological treatments such as by increasing the willingness to disclose (Lucas et al., 2014). In pedagogy, they can help to make social skills training more accessible (Johnson et al., 2019).

While we think about these applications, it is crucial to also lay out proper ethical guidelines to avoid any misuse of these systems. Primary concerns are around the use of deception (e.g. in Diplomacy and other negotiation tasks), emotional appeals (e.g. in persuasion), and behavior change (e.g. in conversational recommendations).

To mitigate possible misuse scenarios or unintended harms, we now lay out a few ethical guidelines which also apply to dialogue research in general. First, rigorous attempts must be made to ensure that the data collection, design processes, and evaluations, strictly abide by the guidelines and regulations laid out by the relevant Institutional Review Board (IRB). Second, the research team needs to develop a thorough plan to monitor and understand the behaviors of the developed systems before deployment. This includes identifying the goals of the dialogue system, identifying potential toxic language use, and any discriminatory behaviors. Third, investment into improved data collection practices, along with explainable and controllable dialogue systems can help identify these issues early on and allow manipulation to avoid them. Fourth, we argue that transparency is the key.

All stakeholders must be made aware of the goals and design objectives of the system, along with any known misbehaviors or potential risks. The users must also be informed of any data collected during the deployment phase. Lastly, we believe that continuous monitoring of dialogue systems is necessary to ensure that the system performs consistently and does not diverge to unexpected conditions that may incur offensive or discriminative actions. We hope that our work promotes a more systematic study of social influence dialogue systems, which in turn will help to tackle the ethical concerns in a more principled way.","(p11.0) Social influence is ubiquitous in everyday life. Research on how we use influence in all aspects of our lives spans a number of fields, including social psychology, communication, consumer behavior, behavioral change, and behavioral economics. This research has led to crucial findings about the strategies of social influence and how they impact our decision-making. Over the past few decades, research has accumulated and demonstrated the effectiveness of using various strategies across contexts and domains. Prominent examples include core principles of social influence by Cialdini from social psychology: reciprocity, commitment and consistency, social proof, liking and attractiveness, authority, and scarcity (Cialdini, 2009). Further, communication strategies used in persuasion and general social influence contexts include credibility appeals, two-sided argumentation, emotional tactics, and appeals to social norms, among others (Cameron, 2009;O'keefe, 2015). First, the well-studied principles in social influence research can guide the development of effective dialogue systems with influence capabilities. In fact, many of the strategies found in the datasets developed for social influence tasks (Section 3) directly map to the principles laid out by Cialdini, for instance, credibility and emotional appeal in PersuasionForGood dataset  and reciprocity observed in CaSiNo negotiation dataset (Chawla et al., 2021b). Second, research in social influence dialogue systems provides novel datasets on human-human and humanmachine communication, and therefore, holds a great potential to advance theories of human cogni-tion and influence processes (Gratch et al., 2015). The datasets and subsequent analyses can further contribute new theoretical insights to social influence research.

(p11.1) Although dialogue systems have already been used in a number of applications involving chatbots and AI assistants, advancements in social influence dialogue systems can help to bridge the gap between our existing task definitions and a number of other real-world applications. For instance, realistic customer support interactions often involve active behaviors from both the support agent and the user where the agent uses social cues for improved customer satisfaction and retention, while the user attempts to address their queries. These settings naturally involve aspects of social influence, unlike traditional task-oriented definitions where the dialogue system plays a passive role to assist the human users. As discussed earlier, social influence dialogue systems can positively help to advance other areas as well. In therapy domain, these systems can assist in various psychological treatments such as by increasing the willingness to disclose (Lucas et al., 2014). In pedagogy, they can help to make social skills training more accessible (Johnson et al., 2019).

(p11.2) While we think about these applications, it is crucial to also lay out proper ethical guidelines to avoid any misuse of these systems. Primary concerns are around the use of deception (e.g. in Diplomacy and other negotiation tasks), emotional appeals (e.g. in persuasion), and behavior change (e.g. in conversational recommendations).

(p11.3) To mitigate possible misuse scenarios or unintended harms, we now lay out a few ethical guidelines which also apply to dialogue research in general. First, rigorous attempts must be made to ensure that the data collection, design processes, and evaluations, strictly abide by the guidelines and regulations laid out by the relevant Institutional Review Board (IRB). Second, the research team needs to develop a thorough plan to monitor and understand the behaviors of the developed systems before deployment. This includes identifying the goals of the dialogue system, identifying potential toxic language use, and any discriminatory behaviors. Third, investment into improved data collection practices, along with explainable and controllable dialogue systems can help identify these issues early on and allow manipulation to avoid them. Fourth, we argue that transparency is the key.

(p11.4) All stakeholders must be made aware of the goals and design objectives of the system, along with any known misbehaviors or potential risks. The users must also be informed of any data collected during the deployment phase. Lastly, we believe that continuous monitoring of dialogue systems is necessary to ensure that the system performs consistently and does not diverge to unexpected conditions that may incur offensive or discriminative actions. We hope that our work promotes a more systematic study of social influence dialogue systems, which in turn will help to tackle the ethical concerns in a more principled way.","[['b31', None, 'b3'], ['b11', 'b27'], [], [], []]","[['b31', None, 'b3'], ['b11', 'b27'], [], [], []]",5,"1. Social influence is ubiquitous in everyday life.
2. Research on how we use influence in all aspects of our lives spans a number of fields, including social psychology, communication, consumer behavior, behavioral change, and behavioral economics.
3. This research has led to crucial findings about the strategies of social influence and how they impact our decision-making.
4. Over the past few decades, research has accumulated and demonstrated the effectiveness of using various strategies across contexts and domains.
5. Prominent examples include core principles of social influence by Cialdini from social psychology: reciprocity, commitment and consistency, social proof, liking and attractiveness, authority, and scarcity (Cialdini, 2009).
6. Further, communication strategies used in persuasion and general social influence contexts include credibility appeals, two-sided argumentation, emotional tactics, and appeals to social norms, among others (Cameron, 2009;O'keefe, 2015).
7. First, the well-studied principles in social influence research can guide the development of effective dialogue systems with influence capabilities.
8. In fact, many of the strategies found in the datasets developed for social influence tasks (Section 3) directly map to the principles laid out by Cialdini, for instance, credibility and emotional appeal in PersuasionForGood dataset  and reciprocity observed in CaSiNo negotiation dataset (Chawla et al., 2021b).
9. Second, research in social influence dialogue systems provides novel datasets on human-human and humanmachine communication, and therefore, holds a great potential to advance theories of human cogni-tion and influence processes (Gratch et al., 2015).
10. The datasets and subsequent analyses can further contribute new theoretical insights to social influence research.
11. Although dialogue systems have already been used in a number of applications involving chatbots and AI assistants, advancements in social influence dialogue systems can help to bridge the gap between our existing task definitions and a number of other real-world applications.
12. For instance, realistic customer support interactions often involve active behaviors from both the support agent and the user where the agent uses social cues for improved customer satisfaction and retention, while the user attempts to address their queries.
13. These settings naturally involve aspects of social influence, unlike traditional task-oriented definitions where the dialogue system plays a passive role to assist the human users.
14. As discussed earlier, social influence dialogue systems can positively help to advance other areas as well.
15. In therapy domain, these systems can assist in various psychological treatments such as by increasing the willingness to disclose (Lucas et al., 2014).
16. In pedagogy, they can help to make social skills training more accessible (Johnson et al., 2019).
17. While we think about these applications, it is crucial to also lay out proper ethical guidelines to avoid any misuse of these systems.
18. Primary concerns are around the use of deception (e.g. in Diplomacy and other negotiation tasks), emotional appeals (e.g. in persuasion), and behavior change (e.g. in conversational recommendations).
19. To mitigate possible misuse scenarios or unintended harms, we now lay out a few ethical guidelines which also apply to dialogue research in general.
20. First, rigorous attempts must be made to ensure that the data collection, design processes, and evaluations, strictly abide by the guidelines and regulations laid out by the relevant Institutional Review Board (IRB).
21. Second, the research team needs to develop a thorough plan to monitor and understand the behaviors of the developed systems before deployment.
22. This includes identifying the goals of the dialogue system, identifying potential toxic language use, and any discriminatory behaviors.
23. Third, investment into improved data collection practices, along with explainable and controllable dialogue systems can help identify these issues early on and allow manipulation to avoid them.
24. Fourth, we argue that transparency is the key.
25. All stakeholders must be made aware of the goals and design objectives of the system, along with any known misbehaviors or potential risks.
26. The users must also be informed of any data collected during the deployment phase.
27. Lastly, we believe that continuous monitoring of dialogue systems is necessary to ensure that the system performs consistently and does not diverge to unexpected conditions that may incur offensive or discriminative actions.
28. We hope that our work promotes a more systematic study of social influence dialogue systems, which in turn will help to tackle the ethical concerns in a more principled way."
256231532,Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,"Psychology, Computer Science",https://www.semanticscholar.org/paper/a1275e92f4830e5bbd53bc1b1fa44a9a2f024f88,s1,Social Influence Dialogue Systems,"['p1.0', 'p1.1']","['""Social influence is a fact of everyday life"" (Gass, 2015). It is the change in thoughts, feelings, attitudes, or behaviors resulting from interaction with an individual or a group (Rashotte, 2007). Influence is measured by quantifiable proxies of the observed change, like the interest to indulge in physical exercise before or after the interaction with a system, or the final deal in a negotiation as opposed to one person taking it all. Social influence dialogue systems act interactively and influence their partners in decision-making and behavioral contexts (Zhang et al., 2020a;Lee et al., 2020). This calls for an active role by the system, distinguishing them from other well-studied scenarios, such as purely task-oriented, where systems passively assist their partners to complete tasks, and opendomain, that target social companionship. Key social influence tasks include persuasion , aiming to change users\' attitudes or behaviors, and negotiation, aiming to change the users\' perspective to achieve a common ground (Lewis et al., 2017). Conceptual overview: Figure 1 distinguishes between the kinds of conversational content in social influence interactions. The task-oriented content focuses on influencing for a domain-specific goal, like persuading for donation, bargaining with tradeoffs, or encouraging healthier habits. These interactions may also contain social content, such as small talk, empathy, or self-disclosure. The task-oriented content provides a context for social interactions. Depending on the task, social content is optional, but if present, can in turn build rapport and enhance user-system relationship for improved task outcomes (Liao et al., 2021). Connections with task-oriented and opendomain systems: Similar to a task-oriented or an open-domain scenario, social influence dialogue can also be seen as a sequential decision making process with the goal of maximizing the expected reward Gao et al., 2018). Our proposed category is not meant to be disjoint from these traditional categories. However, it still uniquely brings together the tasks that capture social influence, which is fundamentally absent from how we primarily define dialogue tasks in the community. Defining a new category that captures social influence dialogue would foster a dedicated effort towards this important aspect of real-world conversations.', ""Task-oriented scenarios focus on collaborative information exchange for a common goal of task completion. In social influence tasks, the goals of the system and the user can be different and even conflicting, leading to collaborative or noncollaborative interactions. Further, the goals can go beyond the current task (e.g. multiple therapy interactions, repeated negotiations), leading to social interactions for long-term relationships. If a scenario involves the system's goal to influence its partner, we consider it under social influence in this paper. For instance, He et al. (2018) studied buyerseller price negotiations. The task of the buyer is to negotiate for a reasonable price (arguably making it task-oriented), but achieving it requires social influence skills of engaging in trade-offs and building a rapport with the seller so as to reach an agreement. Measures of Success: The above discussion indicates that a comprehensive evaluation of social influence systems must draw from both task-oriented and open-domain dialogue research. Since there exist surveys that discuss the evaluation in these settings (Deriu et al., 2021;Li et al., 2021), we don't cover them here in detail. However, we define three essential axes for evaluation: 1) Linguistic Performance, or the system's linguistic sophistication based on automatic (e.g. perplexity, BLEU) and human (e.g. fluency, consistency, coherency) evaluation. 2) Influence Outcome, or the ability to influence defined by objective goals like the negotiated price or weight loss after therapy. 3) Partner Perception, or the subjective evaluation of the user, for instance, the user's satisfaction, likeness towards the system, and interest in interacting again. In a buyer-seller negotiation, if the seller hates the buyer in the end, no matter how favorable the deal is for the buyer, one might argue that this is still a failed negotiation for the buyer. Hence, we encourage future work to take all three dimensions into account collectively.""]","""Social influence is a fact of everyday life"" (Gass, 2015). It is the change in thoughts, feelings, attitudes, or behaviors resulting from interaction with an individual or a group (Rashotte, 2007). Influence is measured by quantifiable proxies of the observed change, like the interest to indulge in physical exercise before or after the interaction with a system, or the final deal in a negotiation as opposed to one person taking it all. Social influence dialogue systems act interactively and influence their partners in decision-making and behavioral contexts (Zhang et al., 2020a;Lee et al., 2020). This calls for an active role by the system, distinguishing them from other well-studied scenarios, such as purely task-oriented, where systems passively assist their partners to complete tasks, and opendomain, that target social companionship. Key social influence tasks include persuasion , aiming to change users' attitudes or behaviors, and negotiation, aiming to change the users' perspective to achieve a common ground (Lewis et al., 2017). Conceptual overview: Figure 1 distinguishes between the kinds of conversational content in social influence interactions. The task-oriented content focuses on influencing for a domain-specific goal, like persuading for donation, bargaining with tradeoffs, or encouraging healthier habits. These interactions may also contain social content, such as small talk, empathy, or self-disclosure. The task-oriented content provides a context for social interactions. Depending on the task, social content is optional, but if present, can in turn build rapport and enhance user-system relationship for improved task outcomes (Liao et al., 2021). Connections with task-oriented and opendomain systems: Similar to a task-oriented or an open-domain scenario, social influence dialogue can also be seen as a sequential decision making process with the goal of maximizing the expected reward Gao et al., 2018). Our proposed category is not meant to be disjoint from these traditional categories. However, it still uniquely brings together the tasks that capture social influence, which is fundamentally absent from how we primarily define dialogue tasks in the community. Defining a new category that captures social influence dialogue would foster a dedicated effort towards this important aspect of real-world conversations.

Task-oriented scenarios focus on collaborative information exchange for a common goal of task completion. In social influence tasks, the goals of the system and the user can be different and even conflicting, leading to collaborative or noncollaborative interactions. Further, the goals can go beyond the current task (e.g. multiple therapy interactions, repeated negotiations), leading to social interactions for long-term relationships. If a scenario involves the system's goal to influence its partner, we consider it under social influence in this paper. For instance, He et al. (2018) studied buyerseller price negotiations. The task of the buyer is to negotiate for a reasonable price (arguably making it task-oriented), but achieving it requires social influence skills of engaging in trade-offs and building a rapport with the seller so as to reach an agreement. Measures of Success: The above discussion indicates that a comprehensive evaluation of social influence systems must draw from both task-oriented and open-domain dialogue research. Since there exist surveys that discuss the evaluation in these settings (Deriu et al., 2021;Li et al., 2021), we don't cover them here in detail. However, we define three essential axes for evaluation: 1) Linguistic Performance, or the system's linguistic sophistication based on automatic (e.g. perplexity, BLEU) and human (e.g. fluency, consistency, coherency) evaluation. 2) Influence Outcome, or the ability to influence defined by objective goals like the negotiated price or weight loss after therapy. 3) Partner Perception, or the subjective evaluation of the user, for instance, the user's satisfaction, likeness towards the system, and interest in interacting again. In a buyer-seller negotiation, if the seller hates the buyer in the end, no matter how favorable the deal is for the buyer, one might argue that this is still a failed negotiation for the buyer. Hence, we encourage future work to take all three dimensions into account collectively.","(p1.0) ""Social influence is a fact of everyday life"" (Gass, 2015). It is the change in thoughts, feelings, attitudes, or behaviors resulting from interaction with an individual or a group (Rashotte, 2007). Influence is measured by quantifiable proxies of the observed change, like the interest to indulge in physical exercise before or after the interaction with a system, or the final deal in a negotiation as opposed to one person taking it all. Social influence dialogue systems act interactively and influence their partners in decision-making and behavioral contexts (Zhang et al., 2020a;Lee et al., 2020). This calls for an active role by the system, distinguishing them from other well-studied scenarios, such as purely task-oriented, where systems passively assist their partners to complete tasks, and opendomain, that target social companionship. Key social influence tasks include persuasion , aiming to change users' attitudes or behaviors, and negotiation, aiming to change the users' perspective to achieve a common ground (Lewis et al., 2017). Conceptual overview: Figure 1 distinguishes between the kinds of conversational content in social influence interactions. The task-oriented content focuses on influencing for a domain-specific goal, like persuading for donation, bargaining with tradeoffs, or encouraging healthier habits. These interactions may also contain social content, such as small talk, empathy, or self-disclosure. The task-oriented content provides a context for social interactions. Depending on the task, social content is optional, but if present, can in turn build rapport and enhance user-system relationship for improved task outcomes (Liao et al., 2021). Connections with task-oriented and opendomain systems: Similar to a task-oriented or an open-domain scenario, social influence dialogue can also be seen as a sequential decision making process with the goal of maximizing the expected reward Gao et al., 2018). Our proposed category is not meant to be disjoint from these traditional categories. However, it still uniquely brings together the tasks that capture social influence, which is fundamentally absent from how we primarily define dialogue tasks in the community. Defining a new category that captures social influence dialogue would foster a dedicated effort towards this important aspect of real-world conversations.

(p1.1) Task-oriented scenarios focus on collaborative information exchange for a common goal of task completion. In social influence tasks, the goals of the system and the user can be different and even conflicting, leading to collaborative or noncollaborative interactions. Further, the goals can go beyond the current task (e.g. multiple therapy interactions, repeated negotiations), leading to social interactions for long-term relationships. If a scenario involves the system's goal to influence its partner, we consider it under social influence in this paper. For instance, He et al. (2018) studied buyerseller price negotiations. The task of the buyer is to negotiate for a reasonable price (arguably making it task-oriented), but achieving it requires social influence skills of engaging in trade-offs and building a rapport with the seller so as to reach an agreement. Measures of Success: The above discussion indicates that a comprehensive evaluation of social influence systems must draw from both task-oriented and open-domain dialogue research. Since there exist surveys that discuss the evaluation in these settings (Deriu et al., 2021;Li et al., 2021), we don't cover them here in detail. However, we define three essential axes for evaluation: 1) Linguistic Performance, or the system's linguistic sophistication based on automatic (e.g. perplexity, BLEU) and human (e.g. fluency, consistency, coherency) evaluation. 2) Influence Outcome, or the ability to influence defined by objective goals like the negotiated price or weight loss after therapy. 3) Partner Perception, or the subjective evaluation of the user, for instance, the user's satisfaction, likeness towards the system, and interest in interacting again. In a buyer-seller negotiation, if the seller hates the buyer in the end, no matter how favorable the deal is for the buyer, one might argue that this is still a failed negotiation for the buyer. Hence, we encourage future work to take all three dimensions into account collectively.","[['b18', 'b60', 'b0', 'b16', 'b38', 'b25', 'b1'], [None, 'b5', 'b52']]","[['b18', 'b60', 'b0', 'b16', 'b38', 'b25', 'b1'], [None, 'b5', 'b52']]",10,"1. ""Social influence is a fact of everyday life"" (Gass, 2015).
2. It is the change in thoughts, feelings, attitudes, or behaviors resulting from interaction with an individual or a group (Rashotte, 2007).
3. Influence is measured by quantifiable proxies of the observed change, like the interest to indulge in physical exercise before or after the interaction with a system, or the final deal in a negotiation as opposed to one person taking it all.
4. Social influence dialogue systems act interactively and influence their partners in decision-making and behavioral contexts (Zhang et al., 2020a;Lee et al., 2020).
5. This calls for an active role by the system, distinguishing them from other well-studied scenarios, such as purely task-oriented, where systems passively assist their partners to complete tasks, and opendomain, that target social companionship.
6. Key social influence tasks include persuasion , aiming to change users' attitudes or behaviors, and negotiation, aiming to change the users' perspective to achieve a common ground (Lewis et al., 2017).
7. Conceptual overview: Figure 1 distinguishes between the kinds of conversational content in social influence interactions.
8. The task-oriented content focuses on influencing for a domain-specific goal, like persuading for donation, bargaining with tradeoffs, or encouraging healthier habits.
9. These interactions may also contain social content, such as small talk, empathy, or self-disclosure.
10. The task-oriented content provides a context for social interactions.
11. Depending on the task, social content is optional, but if present, can in turn build rapport and enhance user-system relationship for improved task outcomes (Liao et al., 2021).
12. Connections with task-oriented and opendomain systems: Similar to a task-oriented or an open-domain scenario, social influence dialogue can also be seen as a sequential decision making process with the goal of maximizing the expected reward Gao et al., 2018).
13. Our proposed category is not meant to be disjoint from these traditional categories.
14. However, it still uniquely brings together the tasks that capture social influence, which is fundamentally absent from how we primarily define dialogue tasks in the community.
15. Defining a new category that captures social influence dialogue would foster a dedicated effort towards this important aspect of real-world conversations.
16. Task-oriented scenarios focus on collaborative information exchange for a common goal of task completion.
17. In social influence tasks, the goals of the system and the user can be different and even conflicting, leading to collaborative or noncollaborative interactions.
18. Further, the goals can go beyond the current task (e.g. multiple therapy interactions, repeated negotiations), leading to social interactions for long-term relationships.
19. If a scenario involves the system's goal to influence its partner, we consider it under social influence in this paper.
20. For instance, He et al. (2018) studied buyerseller price negotiations.
21. The task of the buyer is to negotiate for a reasonable price (arguably making it task-oriented), but achieving it requires social influence skills of engaging in trade-offs and building a rapport with the seller so as to reach an agreement.
22. Measures of Success: The above discussion indicates that a comprehensive evaluation of social influence systems must draw from both task-oriented and open-domain dialogue research.
23. Since there exist surveys that discuss the evaluation in these settings (Deriu et al., 2021;Li et al., 2021), we don't cover them here in detail.
24. However, we define three essential axes for evaluation: 1) Linguistic Performance, or the system's linguistic sophistication based on automatic (e.g. perplexity, BLEU) and human (e.g. fluency, consistency, coherency) evaluation.
25. 2) Influence Outcome, or the ability to influence defined by objective goals like the negotiated price or weight loss after therapy.
26. 3) Partner Perception, or the subjective evaluation of the user, for instance, the user's satisfaction, likeness towards the system, and interest in interacting again.
27. In a buyer-seller negotiation, if the seller hates the buyer in the end, no matter how favorable the deal is for the buyer, one might argue that this is still a failed negotiation for the buyer.
28. Hence, we encourage future work to take all three dimensions into account collectively."
256231532,Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,"Psychology, Computer Science",https://www.semanticscholar.org/paper/a1275e92f4830e5bbd53bc1b1fa44a9a2f024f88,s2,Social Influence Across Diverse Application Areas,"['p2.0', 'p2.1', 'p2.2', 'p2.3', 'p2.4', 'p2.5', 'p2.6', 'p2.7']","['We now illustrate social influence across numerous domains and application areas. In total, we curated 22 datasets from prior work that capture social influence in various forms, spanning 12 publication venues, 4 languages, and 7 application domains (see Appendix A for details on the compilation process). In general, the datasets capture the following information about an interaction: the non-conversational context for the participants (e.g. negotiation preferences or other role-specific information), the conversation between them, and outcome assessment. Optionally, some datasets also gather participant demographics and personality traits, utterance-level annotations, and subjective evaluations via post-surveys.', 'To understand the structural similarities and differences between these datasets, we design a taxonomy with two primary dimensions: Task Structure (Symmetric vs Asymmetric), and Context Definition (Global vs Local). Task Structure captures whether the participant roles are defined in a sym-metric or an asymmetric manner. For instance, a typical multi-issue negotiation is symmetric, in the sense that both parties have their own preferences and goals based on which they actively try to reach a favorable agreement (Lewis et al., 2017). On the other hand, a counseling session between a therapist and a patient is asymmetric, where the therapist attempts to emotionally support the patient by employing social influence skills (Althoff et al., 2016). Context Definition relates to whether the input context before each interaction is defined globally or locally. For instance, the PersuasionFor-Good dataset globally defines the context of persuasion for charity donation, which is kept the same throughout . On the contrary, in a typical debate, although the rules are defined globally, the conversation topic and arguments are local and can vary for each conversation (Durmus and Cardie, 2019). We present this categorization in Table 1. We further categorize the datasets according to their Domain, Source, and the # of parties. We provide key statistics and the available metadata in Appendix B. We now briefly discuss the datasets in each domain.', 'Games: Strategy games involve social influence dynamics of trust and deception. Diplomacy captures deception in long-lasting relationships, where players forge and break alliances to dominate Europe (Peskov et al., 2020). Catan revolves around the trade of resources for acquiring roads, settlements, and cities (Asher et al., 2016;Boritchev and Amblard, 2021). The players have access to only a subset of resources that they would need, which encourages strategic influence and trade.', ""Multi-Issue Bargaining Tasks (MIBT): MIBT is a tractable closed-domain abstraction of a typical negotiation (Fershtman, 1990). It is based on a fixed set of issues each with a predefined priority for each player, which essentially governs the goals of the players. If the priorities of the players align, this leads to competitive negotiations, where each party attempts to convince their partner with tradeoffs and persuasive arguments. If they don't, this allows cooperative interactions where the negotiators try to find optimal divisions that benefit everyone. DealOrNoDeal (Lewis et al., 2017) involves negotiations over three issues: books, balls, and hats. Other datasets define a more grounded scenario, such as symmetric CaSiNo (Chawla et al., 2021b) negotiations between two campsite neighbors and asymmetric JobInterview (Yamaguchi et al., 2021)   negotiations between recruiters and applicants. Social Good: Social influence is critical for social good applications. The tactics must be personalized using knowledge that is both relevant and appealing. PersuasionForGood  involves asymmetric interactions led by a persuader who attempts to convince the other participant for charity donation by employing a variety of tactics. For instance, Logical Appeal uses reason and evidence to support the argument, while Emotional Appeal elicits specific emotions. E-commerce: These tasks are typically asymmetric. A buyer influences the seller towards a reasonable price, while the seller tries to maximize their own profit. An effective system must combine price-related reasoning with language realization. CraigslistBargain (He et al., 2018) involves openended price negotiations with rich influence strategies like embellishments, side offers, emotional appeals, and using world knowledge. Another example is customer support interactions in AntiScam dataset (Li et al., 2020), where users defend themselves against attackers who try to steal sensitive personal information with convincing arguments."", ""Therapy & Support: Effective therapy using social influence aids in the treatment of mental disorders, and substance use disorders, along with changing undesirable behaviors like unhealthy diets. A counselor needs to be adaptive, personalized, should understand the core issues, and should facilitate a change in patient's perspective (Althoff et al., 2016). In SMS counseling, Althoff et al. (2016) found that linguistic influence like pushing the conversation in the desired direction is associated with perspective change. Similar scenarios were captured in other datasets as well (Demasi et al., 2019;Liang et al., 2021). Tanana et al. (2016) collected the Motivational Interviewing dataset where the goal is to elicit and explore the patient's own motivations for behavior change. EmpatheticDialogues (Rashkin et al., 2019) captured empathetic support interactions, which has been associated with rapport and better task outcomes (Kim et al., 2004;Norfolk et al., 2007;Fraser et al., 2018)."", 'Argumentation: In addition to factuality and social proof, a convincing argument must also consider the intensity, valence, authoritativeness, and framing (Chaiken, 1987;Althoff et al., 2014). Tan et al. (2016) released the ChangeMyView logs from Reddit, involving discussions on numerous controversial topics. Other datasets include Debate Dot Org (DDO) debates on diverse topics (Durmus and Cardie, 2019), congressional proceedings (Thomas et al., 2006), and court hearings (Fornaciari and Poesio, 2012;D.-N.-M. et al., 2012;Ji et al., 2020). Conversational Recommendation: Everyday scenarios naturally hold potential for influence via recommendations, for instance, a movie fan per-suading their friends to watch a movie that they adore.  and Dodge et al. (2016) collected movie recommendation datasets. Instead of guiding the conversation towards a specific movie, the goal is simply to provide recommendations based on facts and personal experiences. Nevertheless, they still provide interesting examples of scenarios that can involve social influence.', 'Miscellaneous: The Target-Guided dataset (Tang et al., 2019) was constructed from the PersonaChat corpus (Zhang et al., 2018).', 'Instead of being openended, the Target-Guided scenario defines a concrete goal of naturally guiding the conversation to a designated target subject, thereby, making it a social influence setting.']","We now illustrate social influence across numerous domains and application areas. In total, we curated 22 datasets from prior work that capture social influence in various forms, spanning 12 publication venues, 4 languages, and 7 application domains (see Appendix A for details on the compilation process). In general, the datasets capture the following information about an interaction: the non-conversational context for the participants (e.g. negotiation preferences or other role-specific information), the conversation between them, and outcome assessment. Optionally, some datasets also gather participant demographics and personality traits, utterance-level annotations, and subjective evaluations via post-surveys.

To understand the structural similarities and differences between these datasets, we design a taxonomy with two primary dimensions: Task Structure (Symmetric vs Asymmetric), and Context Definition (Global vs Local). Task Structure captures whether the participant roles are defined in a sym-metric or an asymmetric manner. For instance, a typical multi-issue negotiation is symmetric, in the sense that both parties have their own preferences and goals based on which they actively try to reach a favorable agreement (Lewis et al., 2017). On the other hand, a counseling session between a therapist and a patient is asymmetric, where the therapist attempts to emotionally support the patient by employing social influence skills (Althoff et al., 2016). Context Definition relates to whether the input context before each interaction is defined globally or locally. For instance, the PersuasionFor-Good dataset globally defines the context of persuasion for charity donation, which is kept the same throughout . On the contrary, in a typical debate, although the rules are defined globally, the conversation topic and arguments are local and can vary for each conversation (Durmus and Cardie, 2019). We present this categorization in Table 1. We further categorize the datasets according to their Domain, Source, and the # of parties. We provide key statistics and the available metadata in Appendix B. We now briefly discuss the datasets in each domain.

Games: Strategy games involve social influence dynamics of trust and deception. Diplomacy captures deception in long-lasting relationships, where players forge and break alliances to dominate Europe (Peskov et al., 2020). Catan revolves around the trade of resources for acquiring roads, settlements, and cities (Asher et al., 2016;Boritchev and Amblard, 2021). The players have access to only a subset of resources that they would need, which encourages strategic influence and trade.

Multi-Issue Bargaining Tasks (MIBT): MIBT is a tractable closed-domain abstraction of a typical negotiation (Fershtman, 1990). It is based on a fixed set of issues each with a predefined priority for each player, which essentially governs the goals of the players. If the priorities of the players align, this leads to competitive negotiations, where each party attempts to convince their partner with tradeoffs and persuasive arguments. If they don't, this allows cooperative interactions where the negotiators try to find optimal divisions that benefit everyone. DealOrNoDeal (Lewis et al., 2017) involves negotiations over three issues: books, balls, and hats. Other datasets define a more grounded scenario, such as symmetric CaSiNo (Chawla et al., 2021b) negotiations between two campsite neighbors and asymmetric JobInterview (Yamaguchi et al., 2021)   negotiations between recruiters and applicants. Social Good: Social influence is critical for social good applications. The tactics must be personalized using knowledge that is both relevant and appealing. PersuasionForGood  involves asymmetric interactions led by a persuader who attempts to convince the other participant for charity donation by employing a variety of tactics. For instance, Logical Appeal uses reason and evidence to support the argument, while Emotional Appeal elicits specific emotions. E-commerce: These tasks are typically asymmetric. A buyer influences the seller towards a reasonable price, while the seller tries to maximize their own profit. An effective system must combine price-related reasoning with language realization. CraigslistBargain (He et al., 2018) involves openended price negotiations with rich influence strategies like embellishments, side offers, emotional appeals, and using world knowledge. Another example is customer support interactions in AntiScam dataset (Li et al., 2020), where users defend themselves against attackers who try to steal sensitive personal information with convincing arguments.

Therapy & Support: Effective therapy using social influence aids in the treatment of mental disorders, and substance use disorders, along with changing undesirable behaviors like unhealthy diets. A counselor needs to be adaptive, personalized, should understand the core issues, and should facilitate a change in patient's perspective (Althoff et al., 2016). In SMS counseling, Althoff et al. (2016) found that linguistic influence like pushing the conversation in the desired direction is associated with perspective change. Similar scenarios were captured in other datasets as well (Demasi et al., 2019;Liang et al., 2021). Tanana et al. (2016) collected the Motivational Interviewing dataset where the goal is to elicit and explore the patient's own motivations for behavior change. EmpatheticDialogues (Rashkin et al., 2019) captured empathetic support interactions, which has been associated with rapport and better task outcomes (Kim et al., 2004;Norfolk et al., 2007;Fraser et al., 2018).

Argumentation: In addition to factuality and social proof, a convincing argument must also consider the intensity, valence, authoritativeness, and framing (Chaiken, 1987;Althoff et al., 2014). Tan et al. (2016) released the ChangeMyView logs from Reddit, involving discussions on numerous controversial topics. Other datasets include Debate Dot Org (DDO) debates on diverse topics (Durmus and Cardie, 2019), congressional proceedings (Thomas et al., 2006), and court hearings (Fornaciari and Poesio, 2012;D.-N.-M. et al., 2012;Ji et al., 2020). Conversational Recommendation: Everyday scenarios naturally hold potential for influence via recommendations, for instance, a movie fan per-suading their friends to watch a movie that they adore.  and Dodge et al. (2016) collected movie recommendation datasets. Instead of guiding the conversation towards a specific movie, the goal is simply to provide recommendations based on facts and personal experiences. Nevertheless, they still provide interesting examples of scenarios that can involve social influence.

Miscellaneous: The Target-Guided dataset (Tang et al., 2019) was constructed from the PersonaChat corpus (Zhang et al., 2018).

Instead of being openended, the Target-Guided scenario defines a concrete goal of naturally guiding the conversation to a designated target subject, thereby, making it a social influence setting.","(p2.0) We now illustrate social influence across numerous domains and application areas. In total, we curated 22 datasets from prior work that capture social influence in various forms, spanning 12 publication venues, 4 languages, and 7 application domains (see Appendix A for details on the compilation process). In general, the datasets capture the following information about an interaction: the non-conversational context for the participants (e.g. negotiation preferences or other role-specific information), the conversation between them, and outcome assessment. Optionally, some datasets also gather participant demographics and personality traits, utterance-level annotations, and subjective evaluations via post-surveys.

(p2.1) To understand the structural similarities and differences between these datasets, we design a taxonomy with two primary dimensions: Task Structure (Symmetric vs Asymmetric), and Context Definition (Global vs Local). Task Structure captures whether the participant roles are defined in a sym-metric or an asymmetric manner. For instance, a typical multi-issue negotiation is symmetric, in the sense that both parties have their own preferences and goals based on which they actively try to reach a favorable agreement (Lewis et al., 2017). On the other hand, a counseling session between a therapist and a patient is asymmetric, where the therapist attempts to emotionally support the patient by employing social influence skills (Althoff et al., 2016). Context Definition relates to whether the input context before each interaction is defined globally or locally. For instance, the PersuasionFor-Good dataset globally defines the context of persuasion for charity donation, which is kept the same throughout . On the contrary, in a typical debate, although the rules are defined globally, the conversation topic and arguments are local and can vary for each conversation (Durmus and Cardie, 2019). We present this categorization in Table 1. We further categorize the datasets according to their Domain, Source, and the # of parties. We provide key statistics and the available metadata in Appendix B. We now briefly discuss the datasets in each domain.

(p2.2) Games: Strategy games involve social influence dynamics of trust and deception. Diplomacy captures deception in long-lasting relationships, where players forge and break alliances to dominate Europe (Peskov et al., 2020). Catan revolves around the trade of resources for acquiring roads, settlements, and cities (Asher et al., 2016;Boritchev and Amblard, 2021). The players have access to only a subset of resources that they would need, which encourages strategic influence and trade.

(p2.3) Multi-Issue Bargaining Tasks (MIBT): MIBT is a tractable closed-domain abstraction of a typical negotiation (Fershtman, 1990). It is based on a fixed set of issues each with a predefined priority for each player, which essentially governs the goals of the players. If the priorities of the players align, this leads to competitive negotiations, where each party attempts to convince their partner with tradeoffs and persuasive arguments. If they don't, this allows cooperative interactions where the negotiators try to find optimal divisions that benefit everyone. DealOrNoDeal (Lewis et al., 2017) involves negotiations over three issues: books, balls, and hats. Other datasets define a more grounded scenario, such as symmetric CaSiNo (Chawla et al., 2021b) negotiations between two campsite neighbors and asymmetric JobInterview (Yamaguchi et al., 2021)   negotiations between recruiters and applicants. Social Good: Social influence is critical for social good applications. The tactics must be personalized using knowledge that is both relevant and appealing. PersuasionForGood  involves asymmetric interactions led by a persuader who attempts to convince the other participant for charity donation by employing a variety of tactics. For instance, Logical Appeal uses reason and evidence to support the argument, while Emotional Appeal elicits specific emotions. E-commerce: These tasks are typically asymmetric. A buyer influences the seller towards a reasonable price, while the seller tries to maximize their own profit. An effective system must combine price-related reasoning with language realization. CraigslistBargain (He et al., 2018) involves openended price negotiations with rich influence strategies like embellishments, side offers, emotional appeals, and using world knowledge. Another example is customer support interactions in AntiScam dataset (Li et al., 2020), where users defend themselves against attackers who try to steal sensitive personal information with convincing arguments.

(p2.4) Therapy & Support: Effective therapy using social influence aids in the treatment of mental disorders, and substance use disorders, along with changing undesirable behaviors like unhealthy diets. A counselor needs to be adaptive, personalized, should understand the core issues, and should facilitate a change in patient's perspective (Althoff et al., 2016). In SMS counseling, Althoff et al. (2016) found that linguistic influence like pushing the conversation in the desired direction is associated with perspective change. Similar scenarios were captured in other datasets as well (Demasi et al., 2019;Liang et al., 2021). Tanana et al. (2016) collected the Motivational Interviewing dataset where the goal is to elicit and explore the patient's own motivations for behavior change. EmpatheticDialogues (Rashkin et al., 2019) captured empathetic support interactions, which has been associated with rapport and better task outcomes (Kim et al., 2004;Norfolk et al., 2007;Fraser et al., 2018).

(p2.5) Argumentation: In addition to factuality and social proof, a convincing argument must also consider the intensity, valence, authoritativeness, and framing (Chaiken, 1987;Althoff et al., 2014). Tan et al. (2016) released the ChangeMyView logs from Reddit, involving discussions on numerous controversial topics. Other datasets include Debate Dot Org (DDO) debates on diverse topics (Durmus and Cardie, 2019), congressional proceedings (Thomas et al., 2006), and court hearings (Fornaciari and Poesio, 2012;D.-N.-M. et al., 2012;Ji et al., 2020). Conversational Recommendation: Everyday scenarios naturally hold potential for influence via recommendations, for instance, a movie fan per-suading their friends to watch a movie that they adore.  and Dodge et al. (2016) collected movie recommendation datasets. Instead of guiding the conversation towards a specific movie, the goal is simply to provide recommendations based on facts and personal experiences. Nevertheless, they still provide interesting examples of scenarios that can involve social influence.

(p2.6) Miscellaneous: The Target-Guided dataset (Tang et al., 2019) was constructed from the PersonaChat corpus (Zhang et al., 2018).

(p2.7) Instead of being openended, the Target-Guided scenario defines a concrete goal of naturally guiding the conversation to a designated target subject, thereby, making it a social influence setting.","[[], [None, 'b18'], [None, 'b33'], ['b18', 'b54', None, 'b23', 'b5'], ['b30', None, 'b47', 'b24', 'b13'], [None, 'b10', 'b49', 'b46'], ['b61', 'b48'], []]","[[], [None, 'b18'], [None, 'b33'], ['b18', 'b54', None, 'b23', 'b5'], ['b30', None, 'b47', 'b24', 'b13'], [None, 'b10', 'b49', 'b46'], ['b61', 'b48'], []]",20,"1. We now illustrate social influence across numerous domains and application areas.
2. In total, we curated 22 datasets from prior work that capture social influence in various forms, spanning 12 publication venues, 4 languages, and 7 application domains (see Appendix A for details on the compilation process).
3. In general, the datasets capture the following information about an interaction: the non-conversational context for the participants (e.g. negotiation preferences or other role-specific information), the conversation between them, and outcome assessment.
4. Optionally, some datasets also gather participant demographics and personality traits, utterance-level annotations, and subjective evaluations via post-surveys.
5. To understand the structural similarities and differences between these datasets, we design a taxonomy with two primary dimensions: Task Structure (Symmetric vs Asymmetric), and Context Definition (Global vs Local).
6. Task Structure captures whether the participant roles are defined in a sym-metric or an asymmetric manner.
7. For instance, a typical multi-issue negotiation is symmetric, in the sense that both parties have their own preferences and goals based on which they actively try to reach a favorable agreement (Lewis et al., 2017).
8. On the other hand, a counseling session between a therapist and a patient is asymmetric, where the therapist attempts to emotionally support the patient by employing social influence skills (Althoff et al., 2016).
9. Context Definition relates to whether the input context before each interaction is defined globally or locally.
10. For instance, the PersuasionFor-Good dataset globally defines the context of persuasion for charity donation, which is kept the same throughout .
11. On the contrary, in a typical debate, although the rules are defined globally, the conversation topic and arguments are local and can vary for each conversation (Durmus and Cardie, 2019).
12. We present this categorization in Table 1.
13. We further categorize the datasets according to their Domain, Source, and the # of parties.
14. We provide key statistics and the available metadata in
15. Appendix B. We now briefly discuss the datasets in each domain.
16. Games: Strategy games involve social influence dynamics of trust and deception.
17. Diplomacy captures deception in long-lasting relationships, where players forge and break alliances to dominate Europe (Peskov et al., 2020).
18. Catan revolves around the trade of resources for acquiring roads, settlements, and cities (Asher et al., 2016;Boritchev and Amblard, 2021).
19. The players have access to only a subset of resources that they would need, which encourages strategic influence and trade.
20. Multi-Issue Bargaining Tasks (MIBT): MIBT is a tractable closed-domain abstraction of a typical negotiation (Fershtman, 1990).
21. It is based on a fixed set of issues each with a predefined priority for each player, which essentially governs the goals of the players.
22. If the priorities of the players align, this leads to competitive negotiations, where each party attempts to convince their partner with tradeoffs and persuasive arguments.
23. If they don't, this allows cooperative interactions where the negotiators try to find optimal divisions that benefit everyone.
24. DealOrNoDeal (Lewis et al., 2017) involves negotiations over three issues: books, balls, and hats.
25. Other datasets define a more grounded scenario, such as symmetric CaSiNo (Chawla et al., 2021b) negotiations between two campsite neighbors and asymmetric JobInterview (Yamaguchi et al., 2021)   negotiations between recruiters and applicants.
26. Social Good: Social influence is critical for social good applications.
27. The tactics must be personalized using knowledge that is both relevant and appealing.
28. PersuasionForGood  involves asymmetric interactions led by a persuader who attempts to convince the other participant for charity donation by employing a variety of tactics.
29. For instance, Logical Appeal uses reason and evidence to support the argument, while Emotional Appeal elicits specific emotions.
30. E-commerce: These tasks are typically asymmetric.
31. A buyer influences the seller towards a reasonable price, while the seller tries to maximize their own profit.
32. An effective system must combine price-related reasoning with language realization.
33. CraigslistBargain (He et al., 2018) involves openended price negotiations with rich influence strategies like embellishments, side offers, emotional appeals, and using world knowledge.
34. Another example is customer support interactions in AntiScam dataset (Li et al., 2020), where users defend themselves against attackers who try to steal sensitive personal information with convincing arguments.
35. Therapy & Support: Effective therapy using social influence aids in the treatment of mental disorders, and substance use disorders, along with changing undesirable behaviors like unhealthy diets.
36. A counselor needs to be adaptive, personalized, should understand the core issues, and should facilitate a change in patient's perspective (Althoff et al., 2016).
37. In SMS counseling, Althoff et al. (2016) found that linguistic influence like pushing the conversation in the desired direction is associated with perspective change.
38. Similar scenarios were captured in other datasets as well (Demasi et al., 2019;Liang et al., 2021).
39. Tanana et al. (2016) collected the Motivational Interviewing dataset where the goal is to elicit and explore the patient's own motivations for behavior change.
40. EmpatheticDialogues (Rashkin et al., 2019) captured empathetic support interactions, which has been associated with rapport and better task outcomes (Kim et al., 2004;Norfolk et al., 2007;Fraser et al., 2018).
41. Argumentation: In addition to factuality and social proof, a convincing argument must also consider the intensity, valence, authoritativeness, and framing (Chaiken, 1987;Althoff et al., 2014).
42. Tan et al. (2016) released the ChangeMyView logs from Reddit, involving discussions on numerous controversial topics.
43. Other datasets include Debate Dot Org (DDO) debates on diverse topics (Durmus and Cardie, 2019), congressional proceedings (Thomas et al., 2006), and court hearings (Fornaciari and Poesio, 2012;D.-N.-M. et al., 2012;Ji et al., 2020).
44. Conversational Recommendation: Everyday scenarios naturally hold potential for influence via recommendations, for instance, a movie fan per-suading their friends to watch a movie that they adore.  and Dodge et al. (2016) collected movie recommendation datasets.
45. Instead of guiding the conversation towards a specific movie, the goal is simply to provide recommendations based on facts and personal experiences.
46. Nevertheless, they still provide interesting examples of scenarios that can involve social influence.
47. Miscellaneous: The Target-Guided dataset (Tang et al., 2019) was constructed from the PersonaChat corpus (Zhang et al., 2018).
48. Instead of being openended, the Target-Guided scenario defines a concrete goal of naturally guiding the conversation to a designated target subject, thereby, making it a social influence setting."
256662721,"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems","Computer Science, Linguistics",https://www.semanticscholar.org/paper/c589a3420ba335a05c248f525ea3c6e90215e42b,s14,Evaluation metrics,"['p14.0', 'p14.1', 'p14.2']","['As an essential aspect of recommendation design, evaluation can provide insights on recommendation quality from multiple dimensions. Apart from well-known metrics such as RMSE, MAP, AUC, MAE, Recall, Precision, MRR, NDCG, F1-score and HitRate in offline mode, some works define Group AUC  or User Group AUC  to evaluate the utility of group recommendations. JIANG et al. (2022) and  conducted A/B testing to evaluate performance with online users using Conversion rate or CTR.', ""The integration of generative modules such as GPT and T5 into existing recommender systems offers additional possibilities for recommender systems, such as generating free-form textual explanations for recommendation results or simulating more realistic real-life dialogue scenarios during conversational recommendations to enhance users' experience. In such cases, BLEU and ROUGE are commonly adopted to automatically evaluate the relevance of generated text based on lexicon overlap. Besides, Perplexity (PPL), Distinct-n, and Unique Sentence Ratio (USR) are also widely used metrics to measure fluency, diversity, and informativeness of generated texts. Other evaluation metrics are leveraged with respect to special requests in LMRSs. For instance, Xie et al. (2023) adopted Entailment Ratio and MAUVE to measure if the generated explanations are factually correct and how close the generated contents are to the ground truth corpus, respectively. Geng et al. (2022a) adopted Feature Diversity (DIV) and CLIPScore (CS) to measure the generated explanations and text-image alignment. Besides, to assess the system's capability to provide item recommendations during conversations, Wang et al. (2022a) computed the Item Ratio within the final generated responses. They evaluated the recommendation performance in an end-to-end manner to prevent the inappropriate insertion of recommended items into dialogues."", 'Human evaluation complements objective evaluation, as automatic metrics may not match sub-jective feedback from users. Liu et al. (2023a) pointed out that human subjective and automatic objective evaluation measurements may yield opposite results, which underscores the limitations of existing automatic metrics for evaluating generated explanations and dialogues in LMRSs. Figure  3 displays usage frequency statistics for different evaluation metrics in their respective tasks.']","As an essential aspect of recommendation design, evaluation can provide insights on recommendation quality from multiple dimensions. Apart from well-known metrics such as RMSE, MAP, AUC, MAE, Recall, Precision, MRR, NDCG, F1-score and HitRate in offline mode, some works define Group AUC  or User Group AUC  to evaluate the utility of group recommendations. JIANG et al. (2022) and  conducted A/B testing to evaluate performance with online users using Conversion rate or CTR.

The integration of generative modules such as GPT and T5 into existing recommender systems offers additional possibilities for recommender systems, such as generating free-form textual explanations for recommendation results or simulating more realistic real-life dialogue scenarios during conversational recommendations to enhance users' experience. In such cases, BLEU and ROUGE are commonly adopted to automatically evaluate the relevance of generated text based on lexicon overlap. Besides, Perplexity (PPL), Distinct-n, and Unique Sentence Ratio (USR) are also widely used metrics to measure fluency, diversity, and informativeness of generated texts. Other evaluation metrics are leveraged with respect to special requests in LMRSs. For instance, Xie et al. (2023) adopted Entailment Ratio and MAUVE to measure if the generated explanations are factually correct and how close the generated contents are to the ground truth corpus, respectively. Geng et al. (2022a) adopted Feature Diversity (DIV) and CLIPScore (CS) to measure the generated explanations and text-image alignment. Besides, to assess the system's capability to provide item recommendations during conversations, Wang et al. (2022a) computed the Item Ratio within the final generated responses. They evaluated the recommendation performance in an end-to-end manner to prevent the inappropriate insertion of recommended items into dialogues.

Human evaluation complements objective evaluation, as automatic metrics may not match sub-jective feedback from users. Liu et al. (2023a) pointed out that human subjective and automatic objective evaluation measurements may yield opposite results, which underscores the limitations of existing automatic metrics for evaluating generated explanations and dialogues in LMRSs. Figure  3 displays usage frequency statistics for different evaluation metrics in their respective tasks.","(p14.0) As an essential aspect of recommendation design, evaluation can provide insights on recommendation quality from multiple dimensions. Apart from well-known metrics such as RMSE, MAP, AUC, MAE, Recall, Precision, MRR, NDCG, F1-score and HitRate in offline mode, some works define Group AUC  or User Group AUC  to evaluate the utility of group recommendations. JIANG et al. (2022) and  conducted A/B testing to evaluate performance with online users using Conversion rate or CTR.

(p14.1) The integration of generative modules such as GPT and T5 into existing recommender systems offers additional possibilities for recommender systems, such as generating free-form textual explanations for recommendation results or simulating more realistic real-life dialogue scenarios during conversational recommendations to enhance users' experience. In such cases, BLEU and ROUGE are commonly adopted to automatically evaluate the relevance of generated text based on lexicon overlap. Besides, Perplexity (PPL), Distinct-n, and Unique Sentence Ratio (USR) are also widely used metrics to measure fluency, diversity, and informativeness of generated texts. Other evaluation metrics are leveraged with respect to special requests in LMRSs. For instance, Xie et al. (2023) adopted Entailment Ratio and MAUVE to measure if the generated explanations are factually correct and how close the generated contents are to the ground truth corpus, respectively. Geng et al. (2022a) adopted Feature Diversity (DIV) and CLIPScore (CS) to measure the generated explanations and text-image alignment. Besides, to assess the system's capability to provide item recommendations during conversations, Wang et al. (2022a) computed the Item Ratio within the final generated responses. They evaluated the recommendation performance in an end-to-end manner to prevent the inappropriate insertion of recommended items into dialogues.

(p14.2) Human evaluation complements objective evaluation, as automatic metrics may not match sub-jective feedback from users. Liu et al. (2023a) pointed out that human subjective and automatic objective evaluation measurements may yield opposite results, which underscores the limitations of existing automatic metrics for evaluating generated explanations and dialogues in LMRSs. Figure  3 displays usage frequency statistics for different evaluation metrics in their respective tasks.","[[], ['b45', 'b7', 'b54'], ['b22']]","[[], ['b45', 'b7', 'b54'], ['b22']]",4,"1. As an essential aspect of recommendation design, evaluation can provide insights on recommendation quality from multiple dimensions.
2. Apart from well-known metrics such as RMSE, MAP, AUC, MAE, Recall, Precision, MRR, NDCG, F1-score and HitRate in offline mode, some works define Group AUC  or User Group AUC  to evaluate the utility of group recommendations.
3. JIANG et al. (2022) and  conducted A/B testing to evaluate performance with online users using Conversion rate or CTR.
4. The integration of generative modules such as GPT and T5 into existing recommender systems offers additional possibilities for recommender systems, such as generating free-form textual explanations for recommendation results or simulating more realistic real-life dialogue scenarios during conversational recommendations to enhance users' experience.
5. In such cases, BLEU and ROUGE are commonly adopted to automatically evaluate the relevance of generated text based on lexicon overlap.
6. Besides, Perplexity (PPL), Distinct-n, and Unique Sentence Ratio (USR) are also widely used metrics to measure fluency, diversity, and informativeness of generated texts.
7. Other evaluation metrics are leveraged with respect to special requests in LMRSs.
8. For instance, Xie et al. (2023) adopted Entailment Ratio and MAUVE to measure if the generated explanations are factually correct and how close the generated contents are to the ground truth corpus, respectively.
9. Geng et al. (2022a) adopted Feature Diversity (DIV) and CLIPScore (CS) to measure the generated explanations and text-image alignment.
10. Besides, to assess the system's capability to provide item recommendations during conversations, Wang et al. (2022a) computed the Item Ratio within the final generated responses.
11. They evaluated the recommendation performance in an end-to-end manner to prevent the inappropriate insertion of recommended items into dialogues.
12. Human evaluation complements objective evaluation, as automatic metrics may not match sub-jective feedback from users.
13. Liu et al. (2023a) pointed out that human subjective and automatic objective evaluation measurements may yield opposite results, which underscores the limitations of existing automatic metrics for evaluating generated explanations and dialogues in LMRSs.
14. Figure  3 displays usage frequency statistics for different evaluation metrics in their respective tasks."
256662721,"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems","Computer Science, Linguistics",https://www.semanticscholar.org/paper/c589a3420ba335a05c248f525ea3c6e90215e42b,s11,Adaptive objectives to recommendation,"['p11.0', 'p11.1', 'p11.2', 'p11.3']","['Numerous pre-training or fine-tuning objectives draw inspiration from LM objectives and have been effectively applied to specific downstream tasks based on the input data types and recommendation goals. In sequential recommendations, there is a common interest in modelling an ordered input sequence in an auto-regressive manner from left to right.', ""Analogous to text sentences,  and Xiao et al. (2022) treated the user's clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction. However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly. Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training. Autoregressive learning tasks can also be adapted to other types of data. Geng et al. (2022b) modeled a series of paths sampled from a knowledge graph in an auto-regressive manner for recommendation by generating the end node from the pretrained model. Zhao (2022) proposed pre-training the Rearrange Sequence Prediction task to learn the sequence-level information of the user's entire interaction history by predicting whether the user interaction history had been rearranged, which is similar to Permuted Language Modelling (PerLM) (Yang et al., 2019)."", 'MLM, also known as Cloze Prediction, can be adapted to learn graph representations for different recommendation purposes.  proposed pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives. Specifically, MNP was performed by randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node. Similarly, MEP was used to recover the masked edge of two adjacent nodes based on the surrounding context. Apart from that, MLM can also be adapted to multi-modal data called Masked Multimodal Modelling (MMM) (Wu et al., 2022a). MMM was performed by predicting the semantics of masked news and news image regions given the unmasked inputs and indicating whether a news image and news content segment correspond to each other for news recommendation purposes.', 'The NSP/SOP can be adapted for CTR prediction as Next K Behaviors Prediction (NBP). NBP was proposed to learn user representations in the pre-training stage by inferring whether a candidate behavior is the next i-th behavior of the target user based on their past N behaviors. NBP can also capture the relatedness between past and multiple future behaviors.']","Numerous pre-training or fine-tuning objectives draw inspiration from LM objectives and have been effectively applied to specific downstream tasks based on the input data types and recommendation goals. In sequential recommendations, there is a common interest in modelling an ordered input sequence in an auto-regressive manner from left to right.

Analogous to text sentences,  and Xiao et al. (2022) treated the user's clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction. However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly. Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training. Autoregressive learning tasks can also be adapted to other types of data. Geng et al. (2022b) modeled a series of paths sampled from a knowledge graph in an auto-regressive manner for recommendation by generating the end node from the pretrained model. Zhao (2022) proposed pre-training the Rearrange Sequence Prediction task to learn the sequence-level information of the user's entire interaction history by predicting whether the user interaction history had been rearranged, which is similar to Permuted Language Modelling (PerLM) (Yang et al., 2019).

MLM, also known as Cloze Prediction, can be adapted to learn graph representations for different recommendation purposes.  proposed pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives. Specifically, MNP was performed by randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node. Similarly, MEP was used to recover the masked edge of two adjacent nodes based on the surrounding context. Apart from that, MLM can also be adapted to multi-modal data called Masked Multimodal Modelling (MMM) (Wu et al., 2022a). MMM was performed by predicting the semantics of masked news and news image regions given the unmasked inputs and indicating whether a news image and news content segment correspond to each other for news recommendation purposes.

The NSP/SOP can be adapted for CTR prediction as Next K Behaviors Prediction (NBP). NBP was proposed to learn user representations in the pre-training stage by inferring whether a candidate behavior is the next i-th behavior of the target user based on their past N behaviors. NBP can also capture the relatedness between past and multiple future behaviors.","(p11.0) Numerous pre-training or fine-tuning objectives draw inspiration from LM objectives and have been effectively applied to specific downstream tasks based on the input data types and recommendation goals. In sequential recommendations, there is a common interest in modelling an ordered input sequence in an auto-regressive manner from left to right.

(p11.1) Analogous to text sentences,  and Xiao et al. (2022) treated the user's clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction. However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly. Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training. Autoregressive learning tasks can also be adapted to other types of data. Geng et al. (2022b) modeled a series of paths sampled from a knowledge graph in an auto-regressive manner for recommendation by generating the end node from the pretrained model. Zhao (2022) proposed pre-training the Rearrange Sequence Prediction task to learn the sequence-level information of the user's entire interaction history by predicting whether the user interaction history had been rearranged, which is similar to Permuted Language Modelling (PerLM) (Yang et al., 2019).

(p11.2) MLM, also known as Cloze Prediction, can be adapted to learn graph representations for different recommendation purposes.  proposed pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives. Specifically, MNP was performed by randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node. Similarly, MEP was used to recover the masked edge of two adjacent nodes based on the surrounding context. Apart from that, MLM can also be adapted to multi-modal data called Masked Multimodal Modelling (MMM) (Wu et al., 2022a). MMM was performed by predicting the semantics of masked news and news image regions given the unmasked inputs and indicating whether a news image and news content segment correspond to each other for news recommendation purposes.

(p11.3) The NSP/SOP can be adapted for CTR prediction as Next K Behaviors Prediction (NBP). NBP was proposed to learn user representations in the pre-training stage by inferring whether a candidate behavior is the next i-th behavior of the target user based on their past N behaviors. NBP can also capture the relatedness between past and multiple future behaviors.","[[], ['b53', 'b8', 'b62', 'b63', None, 'b70'], ['b50'], []]","[[], ['b53', 'b8', 'b62', 'b63', None, 'b70'], ['b50'], []]",7,"1. Numerous pre-training or fine-tuning objectives draw inspiration from LM objectives and have been effectively applied to specific downstream tasks based on the input data types and recommendation goals.
2. In sequential recommendations, there is a common interest in modelling an ordered input sequence in an auto-regressive manner from left to right.
3. Analogous to text sentences,  and Xiao et al. (2022) treated the user's clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction.
4. However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly.
5. Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training.
6. Autoregressive learning tasks can also be adapted to other types of data.
7. Geng et al. (2022b) modeled a series of paths sampled from a knowledge graph in an auto-regressive manner for recommendation by generating the end node from the pretrained model.
8. Zhao (2022) proposed pre-training the Rearrange Sequence Prediction task to learn the sequence-level information of the user's entire interaction history by predicting whether the user interaction history had been rearranged, which is similar to Permuted Language Modelling (PerLM) (Yang et al., 2019).MLM, also known as Cloze Prediction, can be adapted to learn graph representations for different recommendation purposes.
9. proposed pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives.
10. Specifically, MNP was performed by randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node.
11. Similarly, MEP was used to recover the masked edge of two adjacent nodes based on the surrounding context.
12. Apart from that, MLM can also be adapted to multi-modal data called Masked Multimodal Modelling (MMM) (Wu et al., 2022a).
13. MMM was performed by predicting the semantics of masked news and news image regions given the unmasked inputs and indicating whether a news image and news content segment correspond to each other for news recommendation purposes.
14. The NSP/SOP can be adapted for CTR prediction as Next K Behaviors Prediction (NBP).
15. NBP was proposed to learn user representations in the pre-training stage by inferring whether a candidate behavior is the next i-th behavior of the target user based on their past N behaviors.
16. NBP can also capture the relatedness between past and multiple future behaviors."
256662721,"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems","Computer Science, Linguistics",https://www.semanticscholar.org/paper/c589a3420ba335a05c248f525ea3c6e90215e42b,s6,Rating Prediction,['p6.0'],"['Tuning-free Prompting (Gao et al., 2023) Textual data (Zhang et al., 2021b;Sileo et al., 2022;Penha and Hauff, 2020;Xie et al., 2023;Gao et al., 2023); Sequential data (Yuan et al., 2020a;Zhao, 2022); Graph (Liu et al., 2023c; Multi-modal data  Explainable RS Fine-tuning Holistic Model (Xie et al., 2023) Sequential RS Pre-training w/o Fine-tuning (Yuan et al., 2020a), Fine-tuning Holistic Model (Zhao, 2022) Conversational RS Fine-tuning Holistic Model (Penha and Hauff, 2020), Tuningfree Prompting (Gao et al., 2023) ( Yelp Link']","Tuning-free Prompting (Gao et al., 2023) Textual data (Zhang et al., 2021b;Sileo et al., 2022;Penha and Hauff, 2020;Xie et al., 2023;Gao et al., 2023); Sequential data (Yuan et al., 2020a;Zhao, 2022); Graph (Liu et al., 2023c; Multi-modal data  Explainable RS Fine-tuning Holistic Model (Xie et al., 2023) Sequential RS Pre-training w/o Fine-tuning (Yuan et al., 2020a), Fine-tuning Holistic Model (Zhao, 2022) Conversational RS Fine-tuning Holistic Model (Penha and Hauff, 2020), Tuningfree Prompting (Gao et al., 2023) ( Yelp Link","(p6.0) Tuning-free Prompting (Gao et al., 2023) Textual data (Zhang et al., 2021b;Sileo et al., 2022;Penha and Hauff, 2020;Xie et al., 2023;Gao et al., 2023); Sequential data (Yuan et al., 2020a;Zhao, 2022); Graph (Liu et al., 2023c; Multi-modal data  Explainable RS Fine-tuning Holistic Model (Xie et al., 2023) Sequential RS Pre-training w/o Fine-tuning (Yuan et al., 2020a), Fine-tuning Holistic Model (Zhao, 2022) Conversational RS Fine-tuning Holistic Model (Penha and Hauff, 2020), Tuningfree Prompting (Gao et al., 2023) ( Yelp Link","[['b33', 'b41', 'b62', 'b54', 'b6', 'b25', 'b67', 'b70']]","[['b33', 'b41', 'b62', 'b54', 'b6', 'b25', 'b67', 'b70']]",8,"1. Tuning-free Prompting (Gao et al., 2023)
2. Textual data (Zhang et al., 2021b;Sileo et al., 2022;Penha and Hauff, 2020;Xie et al., 2023;Gao et al., 2023); Sequential data (Yuan et al., 2020a;Zhao, 2022); Graph (Liu et al., 2023c; Multi-modal data  Explainable RS Fine-tuning Holistic Model (Xie et al., 2023) Sequential RS Pre-training w/o Fine-tuning (Yuan et al., 2020a), Fine-tuning Holistic Model (Zhao, 2022) Conversational RS Fine-tuning Holistic Model (Penha and Hauff, 2020), Tuningfree Prompting (Gao et al., 2023) ( Yelp Link"
256662721,"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems","Computer Science, Linguistics",https://www.semanticscholar.org/paper/c589a3420ba335a05c248f525ea3c6e90215e42b,s5,Language modelling objectives to recommendation,"['p5.0', 'p5.1', 'p5.2', 'p5.3', 'p5.4', 'p5.5', 'p5.6']","['The expensive manual efforts required for annotated datasets have led many language learning objectives to adopt self-supervised labels, converting them to classic probabilistic density estimation problems. Among language modelling objectives, autoregressive, reconstruction, and auxiliary are three categories commonly used (Liu et al., 2023b). Here, we only introduce several language modelling objectives used for RSs.', 'Partial/ Auto-regressive Modelling (P/AM) Given a text sequence X 1:T = [x 1 , x 2 , · · · x T ], the training objective of AM can be summarized as a joint negative log-likelihood of each variable given all previous variables:', 'Modern LMRS typically utilize popular pretrained left-to-right LMs such as GPT-2 (Hada and Shevade, 2021) and DialoGPT (Wang et al., 2022a,c) as the backbone for explainable and conversational recommendations, respectively, to avoid the laborious task of pre-training from scratch. While auto-regressive objectives can effectively model context dependency, the modelling context can only be accessed from one direction, primarily left-to-right. To address this limitation, PAM is introduced, which extends AM by enabling the factorization step to be a span. For each input X, one factorization order M is sampled. One popular PTM that includes PAM as an objective is UniLMv2 (Bao et al., 2020). The pretrained UniLMv2 model can be utilized to initialize the news embedding model for news recommendation (Yu et al., 2022). Besides directly leveraging PTMs trained on textual inputs, some researchers apply this objective to train inputs with sequential patterns, such as graphs (Geng et al., 2022b) and user-item interactions . These patterns serve as either scoring functions to select suitable paths from the start node/user to the end node/item or detectors to explore novel user-item pairs. Masked Language Modelling (MLM) Taking a sequence of textual sentences as input, MLM first masks a token or multi-tokens with a special token such as [M ASK]. Then the model is trained to predict the masked tokens taking the rest of the tokens as context. The objective is as follows:', 'where M (X) and X M (X) represent the masked tokens in the input sequence X and the rest of the tokens in X respectively. A typical example of MLM training strategy can be found on BERT, which is leveraged as backbone in (Zhang et al., 2021a) to capture user-news matching signals for news recommendation. Concurrently, some research works propose multiple enhanced versions of MLM. RoBERTa  ', 'where x and y represent two segments from the input corpus, and c = 1 if x and y are consecutive, otherwise c = 0. The NSP objective involves reasoning about the relationships between pairs of sentences and can be utilized for better representation learning of textual items such as news articles, item descriptions, and conversational data for recommendation purposes. Moreover, it can be employed to model the intimate relationships between two components. Malkiel et al. (2020) used the NSP to capture the relationship between the title and description of an item for next-item prediction. Furthermore, models pre-trained with NSP (such as BERT) can be leveraged for probing the learned knowledge with prompts, which are then infused in the fine-tuning stage to improve model training on adversarial data for conversational recommendation (Penha and Hauff, 2020). Sentence Order Prediction (SOP) as a variation of the NSP takes two consecutive segments from the same document as positive examples, which are then swapped in order as negative examples. SOP has been used to learn the inner coherence of title, description, and code for tag recommendation on StackOverflow (He et al., 2022).', 'Nevertheless, some researchers have questioned the necessity and effectiveness of the NSP and SOP for downstream tasks (He et al., 2022), which highlights the need for further investigation in recommendation scenarios. Replaced Token Detection(RTD) It is used to predict whether a token is replaced given its surrounding context:', 'MovieLens Link']","The expensive manual efforts required for annotated datasets have led many language learning objectives to adopt self-supervised labels, converting them to classic probabilistic density estimation problems. Among language modelling objectives, autoregressive, reconstruction, and auxiliary are three categories commonly used (Liu et al., 2023b). Here, we only introduce several language modelling objectives used for RSs.

Partial/ Auto-regressive Modelling (P/AM) Given a text sequence X 1:T = [x 1 , x 2 , · · · x T ], the training objective of AM can be summarized as a joint negative log-likelihood of each variable given all previous variables:

Modern LMRS typically utilize popular pretrained left-to-right LMs such as GPT-2 (Hada and Shevade, 2021) and DialoGPT (Wang et al., 2022a,c) as the backbone for explainable and conversational recommendations, respectively, to avoid the laborious task of pre-training from scratch. While auto-regressive objectives can effectively model context dependency, the modelling context can only be accessed from one direction, primarily left-to-right. To address this limitation, PAM is introduced, which extends AM by enabling the factorization step to be a span. For each input X, one factorization order M is sampled. One popular PTM that includes PAM as an objective is UniLMv2 (Bao et al., 2020). The pretrained UniLMv2 model can be utilized to initialize the news embedding model for news recommendation (Yu et al., 2022). Besides directly leveraging PTMs trained on textual inputs, some researchers apply this objective to train inputs with sequential patterns, such as graphs (Geng et al., 2022b) and user-item interactions . These patterns serve as either scoring functions to select suitable paths from the start node/user to the end node/item or detectors to explore novel user-item pairs. Masked Language Modelling (MLM) Taking a sequence of textual sentences as input, MLM first masks a token or multi-tokens with a special token such as [M ASK]. Then the model is trained to predict the masked tokens taking the rest of the tokens as context. The objective is as follows:

where M (X) and X M (X) represent the masked tokens in the input sequence X and the rest of the tokens in X respectively. A typical example of MLM training strategy can be found on BERT, which is leveraged as backbone in (Zhang et al., 2021a) to capture user-news matching signals for news recommendation. Concurrently, some research works propose multiple enhanced versions of MLM. RoBERTa  

where x and y represent two segments from the input corpus, and c = 1 if x and y are consecutive, otherwise c = 0. The NSP objective involves reasoning about the relationships between pairs of sentences and can be utilized for better representation learning of textual items such as news articles, item descriptions, and conversational data for recommendation purposes. Moreover, it can be employed to model the intimate relationships between two components. Malkiel et al. (2020) used the NSP to capture the relationship between the title and description of an item for next-item prediction. Furthermore, models pre-trained with NSP (such as BERT) can be leveraged for probing the learned knowledge with prompts, which are then infused in the fine-tuning stage to improve model training on adversarial data for conversational recommendation (Penha and Hauff, 2020). Sentence Order Prediction (SOP) as a variation of the NSP takes two consecutive segments from the same document as positive examples, which are then swapped in order as negative examples. SOP has been used to learn the inner coherence of title, description, and code for tag recommendation on StackOverflow (He et al., 2022).

Nevertheless, some researchers have questioned the necessity and effectiveness of the NSP and SOP for downstream tasks (He et al., 2022), which highlights the need for further investigation in recommendation scenarios. Replaced Token Detection(RTD) It is used to predict whether a token is replaced given its surrounding context:

MovieLens Link","(p5.0) The expensive manual efforts required for annotated datasets have led many language learning objectives to adopt self-supervised labels, converting them to classic probabilistic density estimation problems. Among language modelling objectives, autoregressive, reconstruction, and auxiliary are three categories commonly used (Liu et al., 2023b). Here, we only introduce several language modelling objectives used for RSs.

(p5.1) Partial/ Auto-regressive Modelling (P/AM) Given a text sequence X 1:T = [x 1 , x 2 , · · · x T ], the training objective of AM can be summarized as a joint negative log-likelihood of each variable given all previous variables:

(p5.2) Modern LMRS typically utilize popular pretrained left-to-right LMs such as GPT-2 (Hada and Shevade, 2021) and DialoGPT (Wang et al., 2022a,c) as the backbone for explainable and conversational recommendations, respectively, to avoid the laborious task of pre-training from scratch. While auto-regressive objectives can effectively model context dependency, the modelling context can only be accessed from one direction, primarily left-to-right. To address this limitation, PAM is introduced, which extends AM by enabling the factorization step to be a span. For each input X, one factorization order M is sampled. One popular PTM that includes PAM as an objective is UniLMv2 (Bao et al., 2020). The pretrained UniLMv2 model can be utilized to initialize the news embedding model for news recommendation (Yu et al., 2022). Besides directly leveraging PTMs trained on textual inputs, some researchers apply this objective to train inputs with sequential patterns, such as graphs (Geng et al., 2022b) and user-item interactions . These patterns serve as either scoring functions to select suitable paths from the start node/user to the end node/item or detectors to explore novel user-item pairs. Masked Language Modelling (MLM) Taking a sequence of textual sentences as input, MLM first masks a token or multi-tokens with a special token such as [M ASK]. Then the model is trained to predict the masked tokens taking the rest of the tokens as context. The objective is as follows:

(p5.3) where M (X) and X M (X) represent the masked tokens in the input sequence X and the rest of the tokens in X respectively. A typical example of MLM training strategy can be found on BERT, which is leveraged as backbone in (Zhang et al., 2021a) to capture user-news matching signals for news recommendation. Concurrently, some research works propose multiple enhanced versions of MLM. RoBERTa  

(p5.4) where x and y represent two segments from the input corpus, and c = 1 if x and y are consecutive, otherwise c = 0. The NSP objective involves reasoning about the relationships between pairs of sentences and can be utilized for better representation learning of textual items such as news articles, item descriptions, and conversational data for recommendation purposes. Moreover, it can be employed to model the intimate relationships between two components. Malkiel et al. (2020) used the NSP to capture the relationship between the title and description of an item for next-item prediction. Furthermore, models pre-trained with NSP (such as BERT) can be leveraged for probing the learned knowledge with prompts, which are then infused in the fine-tuning stage to improve model training on adversarial data for conversational recommendation (Penha and Hauff, 2020). Sentence Order Prediction (SOP) as a variation of the NSP takes two consecutive segments from the same document as positive examples, which are then swapped in order as negative examples. SOP has been used to learn the inner coherence of title, description, and code for tag recommendation on StackOverflow (He et al., 2022).

(p5.5) Nevertheless, some researchers have questioned the necessity and effectiveness of the NSP and SOP for downstream tasks (He et al., 2022), which highlights the need for further investigation in recommendation scenarios. Replaced Token Detection(RTD) It is used to predict whether a token is replaced given its surrounding context:

(p5.6) MovieLens Link","[['b23'], [], ['b61', 'b0', None, 'b8'], ['b65'], ['b31', 'b13', 'b33'], ['b13'], []]","[['b23'], [], ['b61', 'b0', None, 'b8'], ['b65'], ['b31', 'b13', 'b33'], ['b13'], []]",10,"1. The expensive manual efforts required for annotated datasets have led many language learning objectives to adopt self-supervised labels, converting them to classic probabilistic density estimation problems.
2. Among language modelling objectives, autoregressive, reconstruction, and auxiliary are three categories commonly used (Liu et al., 2023b).
3. Here, we only introduce several language modelling objectives used for RSs.
4. Partial/ Auto-regressive Modelling (P/AM)
5. Given a text sequence X 1:T = [x 1 , x 2 , · · · x T ], the training objective of AM can be summarized as a joint negative log-likelihood of each variable given all previous variables:Modern LMRS typically utilize popular pretrained left-to-right LMs such as GPT-2 (Hada and Shevade, 2021) and DialoGPT (Wang et al., 2022a,c) as the backbone for explainable and conversational recommendations, respectively, to avoid the laborious task of pre-training from scratch.
6. While auto-regressive objectives can effectively model context dependency, the modelling context can only be accessed from one direction, primarily left-to-right.
7. To address this limitation, PAM is introduced, which extends AM by enabling the factorization step to be a span.
8. For each input X, one factorization order M is sampled.
9. One popular PTM that includes PAM as an objective is UniLMv2 (Bao et al., 2020).
10. The pretrained UniLMv2 model can be utilized to initialize the news embedding model for news recommendation (Yu et al., 2022).
11. Besides directly leveraging PTMs trained on textual inputs, some researchers apply this objective to train inputs with sequential patterns, such as graphs (Geng et al., 2022b) and user-item interactions .
12. These patterns serve as either scoring functions to select suitable paths from the start node/user to the end node/item or detectors to explore novel user-item pairs.
13. Masked Language Modelling (MLM)
14. Taking a sequence of textual sentences as input, MLM first masks a token or multi-tokens with a special token such as [M ASK].
15. Then the model is trained to predict the masked tokens taking the rest of the tokens as context.
16. The objective is as follows:where M (X) and X M (X) represent the masked tokens in the input sequence X and the rest of the tokens in X respectively.
17. A typical example of MLM training strategy can be found on BERT, which is leveraged as backbone in (Zhang et al., 2021a) to capture user-news matching signals for news recommendation.
18. Concurrently, some research works propose multiple enhanced versions of MLM.
19. RoBERTa  where x and y represent two segments from the input corpus, and c = 1 if x and y are consecutive, otherwise c
20. = 0. The NSP objective involves reasoning about the relationships between pairs of sentences and can be utilized for better representation learning of textual items such as news articles, item descriptions, and conversational data for recommendation purposes.
21. Moreover, it can be employed to model the intimate relationships between two components.
22. Malkiel et al. (2020) used the NSP to capture the relationship between the title and description of an item for next-item prediction.
23. Furthermore, models pre-trained with NSP (such as BERT) can be leveraged for probing the learned knowledge with prompts, which are then infused in the fine-tuning stage to improve model training on adversarial data for conversational recommendation (Penha and Hauff, 2020).
24. Sentence Order Prediction (SOP) as a variation of the NSP takes two consecutive segments from the same document as positive examples, which are then swapped in order as negative examples.
25. SOP has been used to learn the inner coherence of title, description, and code for tag recommendation on StackOverflow (He et al., 2022).
26. Nevertheless, some researchers have questioned the necessity and effectiveness of the NSP and SOP for downstream tasks (He et al., 2022), which highlights the need for further investigation in recommendation scenarios.
27. Replaced Token Detection(RTD) It is used to predict whether a token is replaced given its surrounding context:MovieLens Link"
256662721,"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems","Computer Science, Linguistics",https://www.semanticscholar.org/paper/c589a3420ba335a05c248f525ea3c6e90215e42b,s3,Prompting paradigm for RSs,"['p3.0', 'p3.1', 'p3.2']","['Instead of adapting PLMs to different downstream recommendation tasks by designing specific objective functions, a rising trend in recent years is to use the ""pre-train, prompt, and inference"" paradigm to reformulate downstream recommendations through hard/soft prompts. In this paradigm, fine-tuning can be avoided, and the pretrained model itself can be directly employed to predict item ratings, generate top-k item ranking lists, make conversations, recommend similar libraries for programmers while coding, or even output subtasks related to recommendation targets such as explanations (Li et al., 2023b). Prompt learning breaks through the problem of data constraints and bridges the gap of objective forms between pre-training and fine-tuning. Fixed-PTM prompt tuning Prompt-tuning only requires tuning a small set of parameters for the prompts and labels, which is especially efficient for few-shot recommendation tasks. Despite the promising results achieved through constructing prompt information without significantly changing the structure and parameters of PTMs, it also calls for the necessity of choosing the most appropriate prompt template and verbalizer, which can greatly impact recommendation performance. Prompt tuning can be both in the form of discrete textual templates (Penha and Hauff, 2020), which are more human-readable, and soft continuous vectors (Wang et al., 2022c;Wu et al., 2022b). For instance, Penha and Hauff (2020) manually designed several prompt templates to test the performance of movie/book recommendations on a pre-trained BERT model with a similarity measure. Wu et al. (2022b) proposed a personalized prompt generator tuned to generate a soft prompt as a prefix before the user behaviour sequence for sequential recommendation. Fixed-prompt PTM tuning Fixed-prompt PTM tuning tunes the parameters of PTMs similarly to the ""pre-train, fine-tune"" strategy but additionally uses prompts with fixed parameters to steer the recommendation task. One natural way is to use artificially designed discrete prompt to specify recommendation items. For instance, Zhang et al.', '(2021b) designed a prompt ""A user watched item A, item B, and item C. Now the user may want to watch () "" to reformulate the recommendation as a multi-token cloze task during fine-tuning of the LM-based PTM. The prompts can also be one or several tokens/words to seamlessly shift/lead the conversations from various tasks. Deng et al. (2023)  token as a prompt to indicate the start of the recommendation process and to summarize the dialogue context for the conversational recommendation.', 'Tuning-free prompting This training strategy can be referred to as zero-shot recommendations, which directly generate recommendations or/and related subtasks without changing the parameters of the PTMs but based only on the input prompts. Zero-shot recommendation has been shown to be effective in dealing with new users/items in one domain or cross-domain settings (Sileo et al., 2022;Geng et al., 2022c), compared to state-ofthe-art baselines. Specifically, Geng et al. (2022c) learned multiple tasks, such as sequential recommendation, rating prediction, explanation generation, review summarization and direct recommendation, in a unified way with the same Negative Log-likelihood (NLL) training objectives during pre-training. At the inference stage, a series of carefully designed discrete textual template prompts were taken as input, including prompts for recommending items in the new domain (not appearing in the pre-training phase), and the trained model outputs the preferable results without a fine-tuning stage. The reason for the effectiveness of zero-shot recommendation is that the training data and pre-training tasks are able to distil rich knowledge of semantics and correlations from diverse modalities into user and item tokens, which can comprehend user preference behaviours w.r.t. item characteristics (Geng et al., 2022c). Building upon this research, Geng et al. (2023) extended their efforts to train an adapter for diverse multimodal assignments, including sequential recommendations, direct recommendations, and the generation of explanations. In particular, they utilized the pre-trained CLIP component to convert images into image tokens. These tokens were added to the textual tokens of an item to create a personalized multimodal soft prompt. This com-bined prompt was then used as input to fine-tune the adapter in an autoregressive manner. Prompt+PTM tuning In this setting, the parameters include two parts: prompt-relevant parameters and model parameters. The tuning phase involves optimizing all parameters for specific recommendation tasks. Prompt+PTM tuning differs from the ""pre-train, fine-tune the holistic model"" strategy by providing additional prompts that can provide additional bootstrapping at the start of model training. For example, Li et al. (2023b) proposed a continuous prompt learning approach by first fixing the PTM, tuning the prompt to bridge the gap between the continuous prompts and the loaded PTM, and then fine-tuning both the prompt and PTM, resulting in a higher BLUE score in empirical results. They combined both discrete prompts (three user/item feature keywords, such as gym, breakfast, and Wi-Fi) and soft prompts (user/item embeddings) to generate recommendation explanations. Case studies showed improvements in the readability and fluency of generated explanations using the proposed prompts. Note that the Prompt+PTM tuning stage does not necessarily mean the fine-tuning stage but can be any possible stage for tuning parameters from both sides for specific data input. Xin et al. (2022) adapted a reinforcement learning framework as a Prompt+PTM tuning strategy by learning rewardstate pairs as soft prompt encodings w.r.t. observed actions during training. At the inference stage, the trained prompt generator can directly generate soft prompt embeddings for the recommendation model to generate actions (items).']","Instead of adapting PLMs to different downstream recommendation tasks by designing specific objective functions, a rising trend in recent years is to use the ""pre-train, prompt, and inference"" paradigm to reformulate downstream recommendations through hard/soft prompts. In this paradigm, fine-tuning can be avoided, and the pretrained model itself can be directly employed to predict item ratings, generate top-k item ranking lists, make conversations, recommend similar libraries for programmers while coding, or even output subtasks related to recommendation targets such as explanations (Li et al., 2023b). Prompt learning breaks through the problem of data constraints and bridges the gap of objective forms between pre-training and fine-tuning. Fixed-PTM prompt tuning Prompt-tuning only requires tuning a small set of parameters for the prompts and labels, which is especially efficient for few-shot recommendation tasks. Despite the promising results achieved through constructing prompt information without significantly changing the structure and parameters of PTMs, it also calls for the necessity of choosing the most appropriate prompt template and verbalizer, which can greatly impact recommendation performance. Prompt tuning can be both in the form of discrete textual templates (Penha and Hauff, 2020), which are more human-readable, and soft continuous vectors (Wang et al., 2022c;Wu et al., 2022b). For instance, Penha and Hauff (2020) manually designed several prompt templates to test the performance of movie/book recommendations on a pre-trained BERT model with a similarity measure. Wu et al. (2022b) proposed a personalized prompt generator tuned to generate a soft prompt as a prefix before the user behaviour sequence for sequential recommendation. Fixed-prompt PTM tuning Fixed-prompt PTM tuning tunes the parameters of PTMs similarly to the ""pre-train, fine-tune"" strategy but additionally uses prompts with fixed parameters to steer the recommendation task. One natural way is to use artificially designed discrete prompt to specify recommendation items. For instance, Zhang et al.

(2021b) designed a prompt ""A user watched item A, item B, and item C. Now the user may want to watch () "" to reformulate the recommendation as a multi-token cloze task during fine-tuning of the LM-based PTM. The prompts can also be one or several tokens/words to seamlessly shift/lead the conversations from various tasks. Deng et al. (2023)  token as a prompt to indicate the start of the recommendation process and to summarize the dialogue context for the conversational recommendation.

Tuning-free prompting This training strategy can be referred to as zero-shot recommendations, which directly generate recommendations or/and related subtasks without changing the parameters of the PTMs but based only on the input prompts. Zero-shot recommendation has been shown to be effective in dealing with new users/items in one domain or cross-domain settings (Sileo et al., 2022;Geng et al., 2022c), compared to state-ofthe-art baselines. Specifically, Geng et al. (2022c) learned multiple tasks, such as sequential recommendation, rating prediction, explanation generation, review summarization and direct recommendation, in a unified way with the same Negative Log-likelihood (NLL) training objectives during pre-training. At the inference stage, a series of carefully designed discrete textual template prompts were taken as input, including prompts for recommending items in the new domain (not appearing in the pre-training phase), and the trained model outputs the preferable results without a fine-tuning stage. The reason for the effectiveness of zero-shot recommendation is that the training data and pre-training tasks are able to distil rich knowledge of semantics and correlations from diverse modalities into user and item tokens, which can comprehend user preference behaviours w.r.t. item characteristics (Geng et al., 2022c). Building upon this research, Geng et al. (2023) extended their efforts to train an adapter for diverse multimodal assignments, including sequential recommendations, direct recommendations, and the generation of explanations. In particular, they utilized the pre-trained CLIP component to convert images into image tokens. These tokens were added to the textual tokens of an item to create a personalized multimodal soft prompt. This com-bined prompt was then used as input to fine-tune the adapter in an autoregressive manner. Prompt+PTM tuning In this setting, the parameters include two parts: prompt-relevant parameters and model parameters. The tuning phase involves optimizing all parameters for specific recommendation tasks. Prompt+PTM tuning differs from the ""pre-train, fine-tune the holistic model"" strategy by providing additional prompts that can provide additional bootstrapping at the start of model training. For example, Li et al. (2023b) proposed a continuous prompt learning approach by first fixing the PTM, tuning the prompt to bridge the gap between the continuous prompts and the loaded PTM, and then fine-tuning both the prompt and PTM, resulting in a higher BLUE score in empirical results. They combined both discrete prompts (three user/item feature keywords, such as gym, breakfast, and Wi-Fi) and soft prompts (user/item embeddings) to generate recommendation explanations. Case studies showed improvements in the readability and fluency of generated explanations using the proposed prompts. Note that the Prompt+PTM tuning stage does not necessarily mean the fine-tuning stage but can be any possible stage for tuning parameters from both sides for specific data input. Xin et al. (2022) adapted a reinforcement learning framework as a Prompt+PTM tuning strategy by learning rewardstate pairs as soft prompt encodings w.r.t. observed actions during training. At the inference stage, the trained prompt generator can directly generate soft prompt embeddings for the recommendation model to generate actions (items).","(p3.0) Instead of adapting PLMs to different downstream recommendation tasks by designing specific objective functions, a rising trend in recent years is to use the ""pre-train, prompt, and inference"" paradigm to reformulate downstream recommendations through hard/soft prompts. In this paradigm, fine-tuning can be avoided, and the pretrained model itself can be directly employed to predict item ratings, generate top-k item ranking lists, make conversations, recommend similar libraries for programmers while coding, or even output subtasks related to recommendation targets such as explanations (Li et al., 2023b). Prompt learning breaks through the problem of data constraints and bridges the gap of objective forms between pre-training and fine-tuning. Fixed-PTM prompt tuning Prompt-tuning only requires tuning a small set of parameters for the prompts and labels, which is especially efficient for few-shot recommendation tasks. Despite the promising results achieved through constructing prompt information without significantly changing the structure and parameters of PTMs, it also calls for the necessity of choosing the most appropriate prompt template and verbalizer, which can greatly impact recommendation performance. Prompt tuning can be both in the form of discrete textual templates (Penha and Hauff, 2020), which are more human-readable, and soft continuous vectors (Wang et al., 2022c;Wu et al., 2022b). For instance, Penha and Hauff (2020) manually designed several prompt templates to test the performance of movie/book recommendations on a pre-trained BERT model with a similarity measure. Wu et al. (2022b) proposed a personalized prompt generator tuned to generate a soft prompt as a prefix before the user behaviour sequence for sequential recommendation. Fixed-prompt PTM tuning Fixed-prompt PTM tuning tunes the parameters of PTMs similarly to the ""pre-train, fine-tune"" strategy but additionally uses prompts with fixed parameters to steer the recommendation task. One natural way is to use artificially designed discrete prompt to specify recommendation items. For instance, Zhang et al.

(p3.1) (2021b) designed a prompt ""A user watched item A, item B, and item C. Now the user may want to watch () "" to reformulate the recommendation as a multi-token cloze task during fine-tuning of the LM-based PTM. The prompts can also be one or several tokens/words to seamlessly shift/lead the conversations from various tasks. Deng et al. (2023)  token as a prompt to indicate the start of the recommendation process and to summarize the dialogue context for the conversational recommendation.

(p3.2) Tuning-free prompting This training strategy can be referred to as zero-shot recommendations, which directly generate recommendations or/and related subtasks without changing the parameters of the PTMs but based only on the input prompts. Zero-shot recommendation has been shown to be effective in dealing with new users/items in one domain or cross-domain settings (Sileo et al., 2022;Geng et al., 2022c), compared to state-ofthe-art baselines. Specifically, Geng et al. (2022c) learned multiple tasks, such as sequential recommendation, rating prediction, explanation generation, review summarization and direct recommendation, in a unified way with the same Negative Log-likelihood (NLL) training objectives during pre-training. At the inference stage, a series of carefully designed discrete textual template prompts were taken as input, including prompts for recommending items in the new domain (not appearing in the pre-training phase), and the trained model outputs the preferable results without a fine-tuning stage. The reason for the effectiveness of zero-shot recommendation is that the training data and pre-training tasks are able to distil rich knowledge of semantics and correlations from diverse modalities into user and item tokens, which can comprehend user preference behaviours w.r.t. item characteristics (Geng et al., 2022c). Building upon this research, Geng et al. (2023) extended their efforts to train an adapter for diverse multimodal assignments, including sequential recommendations, direct recommendations, and the generation of explanations. In particular, they utilized the pre-trained CLIP component to convert images into image tokens. These tokens were added to the textual tokens of an item to create a personalized multimodal soft prompt. This com-bined prompt was then used as input to fine-tune the adapter in an autoregressive manner. Prompt+PTM tuning In this setting, the parameters include two parts: prompt-relevant parameters and model parameters. The tuning phase involves optimizing all parameters for specific recommendation tasks. Prompt+PTM tuning differs from the ""pre-train, fine-tune the holistic model"" strategy by providing additional prompts that can provide additional bootstrapping at the start of model training. For example, Li et al. (2023b) proposed a continuous prompt learning approach by first fixing the PTM, tuning the prompt to bridge the gap between the continuous prompts and the loaded PTM, and then fine-tuning both the prompt and PTM, resulting in a higher BLUE score in empirical results. They combined both discrete prompts (three user/item feature keywords, such as gym, breakfast, and Wi-Fi) and soft prompts (user/item embeddings) to generate recommendation explanations. Case studies showed improvements in the readability and fluency of generated explanations using the proposed prompts. Note that the Prompt+PTM tuning stage does not necessarily mean the fine-tuning stage but can be any possible stage for tuning parameters from both sides for specific data input. Xin et al. (2022) adapted a reinforcement learning framework as a Prompt+PTM tuning strategy by learning rewardstate pairs as soft prompt encodings w.r.t. observed actions during training. At the inference stage, the trained prompt generator can directly generate soft prompt embeddings for the recommendation model to generate actions (items).","[['b20', None, 'b48', 'b33'], ['b3'], ['b20', 'b15', 'b41', None, 'b9']]","[['b20', None, 'b48', 'b33'], ['b3'], ['b20', 'b15', 'b41', None, 'b9']]",10,"1. Instead of adapting PLMs to different downstream recommendation tasks by designing specific objective functions, a rising trend in recent years is to use the ""pre-train, prompt, and inference"" paradigm to reformulate downstream recommendations through hard/soft prompts.
2. In this paradigm, fine-tuning can be avoided, and the pretrained model itself can be directly employed to predict item ratings, generate top-k item ranking lists, make conversations, recommend similar libraries for programmers while coding, or even output subtasks related to recommendation targets such as explanations (Li et al., 2023b).
3. Prompt learning breaks through the problem of data constraints and bridges the gap of objective forms between pre-training and fine-tuning.
4. Fixed-PTM prompt tuning Prompt-tuning only requires tuning a small set of parameters for the prompts and labels, which is especially efficient for few-shot recommendation tasks.
5. Despite the promising results achieved through constructing prompt information without significantly changing the structure and parameters of PTMs, it also calls for the necessity of choosing the most appropriate prompt template and verbalizer, which can greatly impact recommendation performance.
6. Prompt tuning can be both in the form of discrete textual templates (Penha and Hauff, 2020), which are more human-readable, and soft continuous vectors (Wang et al., 2022c;Wu et al., 2022b).
7. For instance, Penha and Hauff (2020) manually designed several prompt templates to test the performance of movie/book recommendations on a pre-trained BERT model with a similarity measure.
8. Wu et al. (2022b) proposed a personalized prompt generator tuned to generate a soft prompt as a prefix before the user behaviour sequence for sequential recommendation.
9. Fixed-prompt PTM tuning Fixed-prompt PTM tuning tunes the parameters of PTMs similarly to the ""pre-train, fine-tune"" strategy but additionally uses prompts with fixed parameters to steer the recommendation task.
10. One natural way is to use artificially designed discrete prompt to specify recommendation items.
11. For instance, Zhang et al.(2021b) designed a prompt ""A user watched item A, item B, and item C. Now the user may want to watch () "" to reformulate the recommendation as a multi-token cloze task during fine-tuning of the LM-based PTM.
12. The prompts can also be one or several tokens/words to seamlessly shift/lead the conversations from various tasks.
13. Deng et al. (2023)  token as a prompt to indicate the start of the recommendation process and to summarize the dialogue context for the conversational recommendation.
14. Tuning-free prompting This training strategy can be referred to as zero-shot recommendations, which directly generate recommendations or/and related subtasks without changing the parameters of the PTMs but based only on the input prompts.
15. Zero-shot recommendation has been shown to be effective in dealing with new users/items in one domain or cross-domain settings (Sileo et al., 2022;Geng et al., 2022c), compared to state-ofthe-art baselines.
16. Specifically, Geng et al. (2022c) learned multiple tasks, such as sequential recommendation, rating prediction, explanation generation, review summarization and direct recommendation, in a unified way with the same Negative Log-likelihood (NLL) training objectives during pre-training.
17. At the inference stage, a series of carefully designed discrete textual template prompts were taken as input, including prompts for recommending items in the new domain (not appearing in the pre-training phase), and the trained model outputs the preferable results without a fine-tuning stage.
18. The reason for the effectiveness of zero-shot recommendation is that the training data and pre-training tasks are able to distil rich knowledge of semantics and correlations from diverse modalities into user and item tokens, which can comprehend user preference behaviours w.r.t. item characteristics (Geng et al., 2022c).
19. Building upon this research, Geng et al. (2023) extended their efforts to train an adapter for diverse multimodal assignments, including sequential recommendations, direct recommendations, and the generation of explanations.
20. In particular, they utilized the pre-trained CLIP component to convert images into image tokens.
21. These tokens were added to the textual tokens of an item to create a personalized multimodal soft prompt.
22. This com-bined prompt was then used as input to fine-tune the adapter in an autoregressive manner.
23. Prompt+PTM tuning In this setting, the parameters include two parts: prompt-relevant parameters and model parameters.
24. The tuning phase involves optimizing all parameters for specific recommendation tasks.
25. Prompt+PTM tuning differs from the ""pre-train, fine-tune the holistic model"" strategy by providing additional prompts that can provide additional bootstrapping at the start of model training.
26. For example, Li et al. (2023b) proposed a continuous prompt learning approach by first fixing the PTM, tuning the prompt to bridge the gap between the continuous prompts and the loaded PTM, and then fine-tuning both the prompt and PTM, resulting in a higher BLUE score in empirical results.
27. They combined both discrete prompts (three user/item feature keywords, such as gym, breakfast, and Wi-Fi) and soft prompts (user/item embeddings) to generate recommendation explanations.
28. Case studies showed improvements in the readability and fluency of generated explanations using the proposed prompts.
29. Note that the Prompt+PTM tuning stage does not necessarily mean the fine-tuning stage but can be any possible stage for tuning parameters from both sides for specific data input.
30. Xin et al. (2022) adapted a reinforcement learning framework as a Prompt+PTM tuning strategy by learning rewardstate pairs as soft prompt encodings w.r.t.
31. observed actions during training.
32. At the inference stage, the trained prompt generator can directly generate soft prompt embeddings for the recommendation model to generate actions (items)."
256662721,"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems","Computer Science, Linguistics",https://www.semanticscholar.org/paper/c589a3420ba335a05c248f525ea3c6e90215e42b,s2,Training Strategies of LMRS,"['p2.0', 'p2.1', 'p2.2', 'p2.3', 'p2.4', 'p2.5']","['Given the significant impact that PLMs have had on NLP tasks in the pre-train and fine-tune paradigm, there has been a surge recently in adapting such paradigms to multiple recommendation tasks. As illustrated in Figure 1, there are mainly two classes regarding different training paradigms: pre-train, fine-tune paradigm and prompt learning paradigm. Each class is further classified into subclasses regarding different training efforts on different parts of the recommendation model. This section will go through various training strategies w.r.t. specific recommendation purposes. Figure  2(a) presents the statistics of recent publications of LMRSs grouped by different training strategies and the total number of published research works each year. Figure 2(b) shows the taxonomy and some corresponding representative LMRSs.', '4.1 Pre-train, fine-tune paradigm for RS', 'The ""pre-train, fine-tune"" paradigm attracts increasing attention from researchers in the recommendation field due to several advantages: 1) Pre-training provides a better model initialization, which usually leads to better generalization on different downstream recommendation tasks, improves recommendation performance from various perspectives, and speeds up convergence on the fine-tuning stage; 2) Pre-training on huge source corpus can learn universal knowledge which can be beneficial for the downstream recommenders; 3) Pre-training can be regarded as a kind of regularization to avoid overfitting on lowresource, and small datasets (Erhan et al., 2010).', 'Pre-train This training strategy can be seen as traditional end-to-end training with domain input. Differently, we only focus on research works adapting LM-based learning objectives into the training phase. Many typical LM-based RSs fall into this category, such as BERT4Rec , which models sequential user behaviour with a bidirectional self-attention network through Cloze task, and Transformers4Rec (de Souza Pereira Moreira et al., 2021) which adopts a haggingface transformer-based architecture as the base model for next-item prediction and explores four different LM tasks, namely Causal LM, MLM, Permutation LM, and Replacement Token Detection during training. These two models laid the foundation for LM-based recommender systems and have become popular baselines for their successors.', 'Pre-train, fine-tune holistic model Under this category, the model is pre-trained and fine-tuned with different data sources, and the fine-tuning process will go through adjusting the whole model parameters. The learning objectives can also vary between the pre-training and fine-tuning stages. Pre-training and fine-tuning with different domains of data sources, also called cross-domain recommendation, can refer to the works of Kang et al. (2021) and Qiu et al. (2021). Kang et al. (2021) pre-trained a GPT model using segmented source API code and fine-tuned it with API code snippets from another library for cross-library recommendation. Wang et al. (2022a) fine-tuned the pre-trained DialoGPT model on domain-specific datasets for conversational recommendation together with an R-GCN model to inject knowledge from DBpedia to enhance recommendation perfor- mance. Xiao et al. (2022) fine-tuned the PTM to learn news embedding together with a user embedding part in an auto-regressive manner for news recommendation. They also explored different fine-tuning strategies like tuning part of the PTM and tuning the last layer of the PTM but empirically found fine-tuning the whole model resulted in better performance, which gives us an insight into balancing the recommendation accuracy and training efficiency.', 'Pre-train, fine-tune partial model Since finetuning the whole model is usually time-consuming and less flexible, many LMRSs choose to fine-tune partial parameters of the model to achieve a balance between training overhead and recommendation performance (Hou et al., 2022;Yu et al., 2022;Wu et al., 2022a). For instance, to deal with the domain bias problem that BERT induces a non-smooth anisotropic semantic space for general texts resulting in a large language gap for texts from different domains of items, Hou et al. (2022) applied a linear transformation layer to transform BERT representations of items from different domains followed by an adaptive combination strategy to derive a universal item representation. Meanwhile, considering the seesaw phenomenon that learning from multiple domainspecific behavioural patterns can be a conflict, they proposed sequence-item and sequence-sequence contrastive tasks for multi-task learning during the pre-training stage. They found only fine-tuning a small proportion of model parameters could quickly adapt the model to unseen domains with cold-start or new items. Pre-train, fine-tune extra part of the model With the increase in the depth of PTMs, the representation captured by them makes the downstream recommendation easier. Apart from the aforementioned two fine-tuning strategies, some works leverage a task-specific layer on top of the PTMs for recommendation tasks. Fine-tuning only goes through such extra parts of the PTMs by optimizing the parameters of the task-specific layer. Shang et al. (2019) pre-trained a GPT and a BERT model to learn patient visit embeddings, which were then used as input to fine-tune the extra prediction layer for medication recommendation. Another approach is to use the PTM to initialize a new model with a similar architecture in the finetuning stage, and the fine-tuned model is used for recommendations. In , a bidirectional Transformer-based model was first pretrained on four different self-supervised learning objectives (associated attribute prediction, masked item prediction, masked attribute prediction and segment prediction) to learn item embeddings. Then, the learned model parameters were adopted to initialize a unidirectional Transformer-based model for fine-tuning with pairwise rank loss for recommendation. In (McKee et al., 2023), the authors leveraged the pre-trained BLOOM-176B to generate natural languages descriptions of music given a set of music tags. Subsequently, two distinct pre-trained models, namely CLIP and the D2T pipeline, were employed to initialize textual, video, and audio representations of the provided music content. Following this, a transformerbased architecture model was fine-tuned for multimodal music recommendation.']","Given the significant impact that PLMs have had on NLP tasks in the pre-train and fine-tune paradigm, there has been a surge recently in adapting such paradigms to multiple recommendation tasks. As illustrated in Figure 1, there are mainly two classes regarding different training paradigms: pre-train, fine-tune paradigm and prompt learning paradigm. Each class is further classified into subclasses regarding different training efforts on different parts of the recommendation model. This section will go through various training strategies w.r.t. specific recommendation purposes. Figure  2(a) presents the statistics of recent publications of LMRSs grouped by different training strategies and the total number of published research works each year. Figure 2(b) shows the taxonomy and some corresponding representative LMRSs.

4.1 Pre-train, fine-tune paradigm for RS

The ""pre-train, fine-tune"" paradigm attracts increasing attention from researchers in the recommendation field due to several advantages: 1) Pre-training provides a better model initialization, which usually leads to better generalization on different downstream recommendation tasks, improves recommendation performance from various perspectives, and speeds up convergence on the fine-tuning stage; 2) Pre-training on huge source corpus can learn universal knowledge which can be beneficial for the downstream recommenders; 3) Pre-training can be regarded as a kind of regularization to avoid overfitting on lowresource, and small datasets (Erhan et al., 2010).

Pre-train This training strategy can be seen as traditional end-to-end training with domain input. Differently, we only focus on research works adapting LM-based learning objectives into the training phase. Many typical LM-based RSs fall into this category, such as BERT4Rec , which models sequential user behaviour with a bidirectional self-attention network through Cloze task, and Transformers4Rec (de Souza Pereira Moreira et al., 2021) which adopts a haggingface transformer-based architecture as the base model for next-item prediction and explores four different LM tasks, namely Causal LM, MLM, Permutation LM, and Replacement Token Detection during training. These two models laid the foundation for LM-based recommender systems and have become popular baselines for their successors.

Pre-train, fine-tune holistic model Under this category, the model is pre-trained and fine-tuned with different data sources, and the fine-tuning process will go through adjusting the whole model parameters. The learning objectives can also vary between the pre-training and fine-tuning stages. Pre-training and fine-tuning with different domains of data sources, also called cross-domain recommendation, can refer to the works of Kang et al. (2021) and Qiu et al. (2021). Kang et al. (2021) pre-trained a GPT model using segmented source API code and fine-tuned it with API code snippets from another library for cross-library recommendation. Wang et al. (2022a) fine-tuned the pre-trained DialoGPT model on domain-specific datasets for conversational recommendation together with an R-GCN model to inject knowledge from DBpedia to enhance recommendation perfor- mance. Xiao et al. (2022) fine-tuned the PTM to learn news embedding together with a user embedding part in an auto-regressive manner for news recommendation. They also explored different fine-tuning strategies like tuning part of the PTM and tuning the last layer of the PTM but empirically found fine-tuning the whole model resulted in better performance, which gives us an insight into balancing the recommendation accuracy and training efficiency.

Pre-train, fine-tune partial model Since finetuning the whole model is usually time-consuming and less flexible, many LMRSs choose to fine-tune partial parameters of the model to achieve a balance between training overhead and recommendation performance (Hou et al., 2022;Yu et al., 2022;Wu et al., 2022a). For instance, to deal with the domain bias problem that BERT induces a non-smooth anisotropic semantic space for general texts resulting in a large language gap for texts from different domains of items, Hou et al. (2022) applied a linear transformation layer to transform BERT representations of items from different domains followed by an adaptive combination strategy to derive a universal item representation. Meanwhile, considering the seesaw phenomenon that learning from multiple domainspecific behavioural patterns can be a conflict, they proposed sequence-item and sequence-sequence contrastive tasks for multi-task learning during the pre-training stage. They found only fine-tuning a small proportion of model parameters could quickly adapt the model to unseen domains with cold-start or new items. Pre-train, fine-tune extra part of the model With the increase in the depth of PTMs, the representation captured by them makes the downstream recommendation easier. Apart from the aforementioned two fine-tuning strategies, some works leverage a task-specific layer on top of the PTMs for recommendation tasks. Fine-tuning only goes through such extra parts of the PTMs by optimizing the parameters of the task-specific layer. Shang et al. (2019) pre-trained a GPT and a BERT model to learn patient visit embeddings, which were then used as input to fine-tune the extra prediction layer for medication recommendation. Another approach is to use the PTM to initialize a new model with a similar architecture in the finetuning stage, and the fine-tuned model is used for recommendations. In , a bidirectional Transformer-based model was first pretrained on four different self-supervised learning objectives (associated attribute prediction, masked item prediction, masked attribute prediction and segment prediction) to learn item embeddings. Then, the learned model parameters were adopted to initialize a unidirectional Transformer-based model for fine-tuning with pairwise rank loss for recommendation. In (McKee et al., 2023), the authors leveraged the pre-trained BLOOM-176B to generate natural languages descriptions of music given a set of music tags. Subsequently, two distinct pre-trained models, namely CLIP and the D2T pipeline, were employed to initialize textual, video, and audio representations of the provided music content. Following this, a transformerbased architecture model was fine-tuned for multimodal music recommendation.","(p2.0) Given the significant impact that PLMs have had on NLP tasks in the pre-train and fine-tune paradigm, there has been a surge recently in adapting such paradigms to multiple recommendation tasks. As illustrated in Figure 1, there are mainly two classes regarding different training paradigms: pre-train, fine-tune paradigm and prompt learning paradigm. Each class is further classified into subclasses regarding different training efforts on different parts of the recommendation model. This section will go through various training strategies w.r.t. specific recommendation purposes. Figure  2(a) presents the statistics of recent publications of LMRSs grouped by different training strategies and the total number of published research works each year. Figure 2(b) shows the taxonomy and some corresponding representative LMRSs.

(p2.1) 4.1 Pre-train, fine-tune paradigm for RS

(p2.2) The ""pre-train, fine-tune"" paradigm attracts increasing attention from researchers in the recommendation field due to several advantages: 1) Pre-training provides a better model initialization, which usually leads to better generalization on different downstream recommendation tasks, improves recommendation performance from various perspectives, and speeds up convergence on the fine-tuning stage; 2) Pre-training on huge source corpus can learn universal knowledge which can be beneficial for the downstream recommenders; 3) Pre-training can be regarded as a kind of regularization to avoid overfitting on lowresource, and small datasets (Erhan et al., 2010).

(p2.3) Pre-train This training strategy can be seen as traditional end-to-end training with domain input. Differently, we only focus on research works adapting LM-based learning objectives into the training phase. Many typical LM-based RSs fall into this category, such as BERT4Rec , which models sequential user behaviour with a bidirectional self-attention network through Cloze task, and Transformers4Rec (de Souza Pereira Moreira et al., 2021) which adopts a haggingface transformer-based architecture as the base model for next-item prediction and explores four different LM tasks, namely Causal LM, MLM, Permutation LM, and Replacement Token Detection during training. These two models laid the foundation for LM-based recommender systems and have become popular baselines for their successors.

(p2.4) Pre-train, fine-tune holistic model Under this category, the model is pre-trained and fine-tuned with different data sources, and the fine-tuning process will go through adjusting the whole model parameters. The learning objectives can also vary between the pre-training and fine-tuning stages. Pre-training and fine-tuning with different domains of data sources, also called cross-domain recommendation, can refer to the works of Kang et al. (2021) and Qiu et al. (2021). Kang et al. (2021) pre-trained a GPT model using segmented source API code and fine-tuned it with API code snippets from another library for cross-library recommendation. Wang et al. (2022a) fine-tuned the pre-trained DialoGPT model on domain-specific datasets for conversational recommendation together with an R-GCN model to inject knowledge from DBpedia to enhance recommendation perfor- mance. Xiao et al. (2022) fine-tuned the PTM to learn news embedding together with a user embedding part in an auto-regressive manner for news recommendation. They also explored different fine-tuning strategies like tuning part of the PTM and tuning the last layer of the PTM but empirically found fine-tuning the whole model resulted in better performance, which gives us an insight into balancing the recommendation accuracy and training efficiency.

(p2.5) Pre-train, fine-tune partial model Since finetuning the whole model is usually time-consuming and less flexible, many LMRSs choose to fine-tune partial parameters of the model to achieve a balance between training overhead and recommendation performance (Hou et al., 2022;Yu et al., 2022;Wu et al., 2022a). For instance, to deal with the domain bias problem that BERT induces a non-smooth anisotropic semantic space for general texts resulting in a large language gap for texts from different domains of items, Hou et al. (2022) applied a linear transformation layer to transform BERT representations of items from different domains followed by an adaptive combination strategy to derive a universal item representation. Meanwhile, considering the seesaw phenomenon that learning from multiple domainspecific behavioural patterns can be a conflict, they proposed sequence-item and sequence-sequence contrastive tasks for multi-task learning during the pre-training stage. They found only fine-tuning a small proportion of model parameters could quickly adapt the model to unseen domains with cold-start or new items. Pre-train, fine-tune extra part of the model With the increase in the depth of PTMs, the representation captured by them makes the downstream recommendation easier. Apart from the aforementioned two fine-tuning strategies, some works leverage a task-specific layer on top of the PTMs for recommendation tasks. Fine-tuning only goes through such extra parts of the PTMs by optimizing the parameters of the task-specific layer. Shang et al. (2019) pre-trained a GPT and a BERT model to learn patient visit embeddings, which were then used as input to fine-tune the extra prediction layer for medication recommendation. Another approach is to use the PTM to initialize a new model with a similar architecture in the finetuning stage, and the fine-tuned model is used for recommendations. In , a bidirectional Transformer-based model was first pretrained on four different self-supervised learning objectives (associated attribute prediction, masked item prediction, masked attribute prediction and segment prediction) to learn item embeddings. Then, the learned model parameters were adopted to initialize a unidirectional Transformer-based model for fine-tuning with pairwise rank loss for recommendation. In (McKee et al., 2023), the authors leveraged the pre-trained BLOOM-176B to generate natural languages descriptions of music given a set of music tags. Subsequently, two distinct pre-trained models, namely CLIP and the D2T pipeline, were employed to initialize textual, video, and audio representations of the provided music content. Following this, a transformerbased architecture model was fine-tuned for multimodal music recommendation.","[[], [], ['b5'], [], ['b45', 'b53', 'b36', 'b17'], ['b50', 'b39', 'b32', 'b15', 'b61']]","[[], [], ['b5'], [], ['b45', 'b53', 'b36', 'b17'], ['b50', 'b39', 'b32', 'b15', 'b61']]",10,"1. Given the significant impact that PLMs have had on NLP tasks in the pre-train and fine-tune paradigm, there has been a surge recently in adapting such paradigms to multiple recommendation tasks.
2. As illustrated in Figure 1, there are mainly two classes regarding different training paradigms: pre-train, fine-tune paradigm and prompt learning paradigm.
3. Each class is further classified into subclasses regarding different training efforts on different parts of the recommendation model.
4. This section will go through various training strategies w.r.t. specific recommendation purposes.
5. Figure  2(a) presents the statistics of recent publications of LMRSs grouped by different training strategies and the total number of published research works each year.
6. Figure 2(b) shows the taxonomy and some corresponding representative LMRSs.
7. 4.1 Pre-train, fine-tune paradigm for RSThe ""pre-train, fine-tune"" paradigm attracts increasing attention from researchers in the recommendation field due to several advantages: 1) Pre-training provides a better model initialization, which usually leads to better generalization on different downstream recommendation tasks, improves recommendation performance from various perspectives, and speeds up convergence on the fine-tuning stage; 2) Pre-training on huge source corpus can learn universal knowledge which can be beneficial for the downstream recommenders; 3) Pre-training can be regarded as a kind of regularization to avoid overfitting on lowresource, and small datasets (Erhan et al., 2010).
8. Pre-train This training strategy can be seen as traditional end-to-end training with domain input.
9. Differently, we only focus on research works adapting LM-based learning objectives into the training phase.
10. Many typical LM-based RSs fall into this category, such as BERT4Rec , which models sequential user behaviour with a bidirectional self-attention network through Cloze task, and Transformers4Rec (de Souza Pereira Moreira et al., 2021) which adopts a haggingface transformer-based architecture as the base model for next-item prediction and explores four different LM tasks, namely Causal LM, MLM, Permutation LM, and Replacement Token Detection during training.
11. These two models laid the foundation for LM-based recommender systems and have become popular baselines for their successors.
12. Pre-train, fine-tune holistic model Under this category
13. , the model is pre-trained and fine-tuned with different data sources, and the fine-tuning process will go through adjusting the whole model parameters.
14. The learning objectives can also vary between the pre-training and fine-tuning stages.
15. Pre-training and fine-tuning with different domains of data sources, also called cross-domain recommendation, can refer to the works of Kang et al. (2021) and Qiu et al. (2021).
16. Kang et al. (2021) pre-trained a GPT model using segmented source API code and fine-tuned it with API code snippets from another library for cross-library recommendation.
17. Wang et al. (2022a) fine-tuned the pre-trained DialoGPT model on domain-specific datasets for conversational recommendation together with an R-GCN model to inject knowledge from DBpedia to enhance recommendation perfor- mance.
18. Xiao et al. (2022) fine-tuned the PTM to learn news embedding together with a user embedding part in an auto-regressive manner for news recommendation.
19. They also explored different fine-tuning strategies like tuning part of the PTM and tuning the last layer of the PTM but empirically found fine-tuning the whole model resulted in better performance, which gives us an insight into balancing the recommendation accuracy and training efficiency.
20. Pre-train, fine-tune partial model Since finetuning the whole model is usually time-consuming and less flexible, many LMRSs choose to fine-tune partial parameters of the model to achieve a balance between training overhead and recommendation performance (Hou et al., 2022;Yu et al., 2022;Wu et al., 2022a).
21. For instance, to deal with the domain bias problem that BERT induces a non-smooth anisotropic semantic space for general texts resulting in a large language gap for texts from different domains of items, Hou et al. (2022) applied a linear transformation layer to transform BERT representations of items from different domains followed by an adaptive combination strategy to derive a universal item representation.
22. Meanwhile, considering the seesaw phenomenon that learning from multiple domainspecific behavioural patterns can be a conflict, they proposed sequence-item and sequence-sequence contrastive tasks for multi-task learning during the pre-training stage.
23. They found only fine-tuning a small proportion of model parameters could quickly adapt the model to unseen domains with cold-start or new items.
24. Pre-train, fine-tune extra part of the model With the increase in the depth of PTMs, the representation captured by them makes the downstream recommendation easier.
25. Apart from the aforementioned two fine-tuning strategies, some works leverage a task-specific layer on top of the PTMs for recommendation tasks.
26. Fine-tuning only goes through such extra parts of the PTMs by optimizing the parameters of the task-specific layer.
27. Shang et al. (2019) pre-trained a GPT and a BERT model to learn patient visit embeddings, which were then used as input to fine-tune the extra prediction layer for medication recommendation.
28. Another approach is to use the PTM to initialize a new model with a similar architecture in the finetuning stage, and the fine-tuned model is used for recommendations.
29. In , a bidirectional Transformer-based model was first pretrained on four different self-supervised learning objectives (associated attribute prediction, masked item prediction, masked attribute prediction and segment prediction) to learn item embeddings.
30. Then, the learned model parameters were adopted to initialize a unidirectional Transformer-based model for fine-tuning with pairwise rank loss for recommendation.
31. In (McKee et al., 2023), the authors leveraged the pre-trained BLOOM-176B to generate natural languages descriptions of music given a set of music tags.
32. Subsequently, two distinct pre-trained models, namely CLIP and the D2T pipeline, were employed to initialize textual, video, and audio representations of the provided music content.
33. Following this, a transformerbased architecture model was fine-tuned for multimodal music recommendation."
256662721,"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems","Computer Science, Linguistics",https://www.semanticscholar.org/paper/c589a3420ba335a05c248f525ea3c6e90215e42b,s15,Discussion on evaluation across datasets,['p15.0'],"[""In this section, we compare the results obtained from various models using commonly used datasets. Specifically, based on the reported results in the paper, we measured the improvement achieved by different models compared to a shared baseline and evaluated them using the same metrics on the same dataset. The comparisons are presented in Table 3∼6. Most improvements are highlighted in bold, and N@k denotes NDCG@k, H@k denotes HitRate@k. It's important to recognize that a comprehensive and precise assessment cannot be achieved without a carefully designed platform and thoughtful settings for conducting the experiments. Various factors, such as diverse training platforms, parameter settings, and data split strategy, can lead to fluctuations in the results. Hence, it is essential to consider the analysis solely for reference purposes. From the tables, we can observe that: First, among the four conversational recommender systems assessed using the ReDial dataset, fixed prompt PTM tuning paradigm Yang et al. (2022a) demonstrate the most significant improvements compared to the shared baselines. Second, on the Amazon dataset, zero-shot and few-shot learning of Chat-GPT underperformed the supervised recommendation baselines (Liu et al., 2023a). This could be due to language models' strength in capturing language patterns rather than effectively collaborating to suggest similar items based on user preferences (Zhang et al., 2021b). Besides, Liu et al. (2023a) pointed out that the position of candidate items in the item pool can also affect the direct recommendation performance. Another promptingbased model, P5, showed the most improvements for both Amazon and Yelp datasets (Geng et al., 2022c), which verifies the need for more guidance when using large pre-trained language models for recommendations. Finally, for news recommendation on the MIND dataset, Xiao et al. (2022) introduced a model-agnostic fine-tuning framework     with cache management, which can accelerate the model training process and yield the most improvements over the baselines.""]","In this section, we compare the results obtained from various models using commonly used datasets. Specifically, based on the reported results in the paper, we measured the improvement achieved by different models compared to a shared baseline and evaluated them using the same metrics on the same dataset. The comparisons are presented in Table 3∼6. Most improvements are highlighted in bold, and N@k denotes NDCG@k, H@k denotes HitRate@k. It's important to recognize that a comprehensive and precise assessment cannot be achieved without a carefully designed platform and thoughtful settings for conducting the experiments. Various factors, such as diverse training platforms, parameter settings, and data split strategy, can lead to fluctuations in the results. Hence, it is essential to consider the analysis solely for reference purposes. From the tables, we can observe that: First, among the four conversational recommender systems assessed using the ReDial dataset, fixed prompt PTM tuning paradigm Yang et al. (2022a) demonstrate the most significant improvements compared to the shared baselines. Second, on the Amazon dataset, zero-shot and few-shot learning of Chat-GPT underperformed the supervised recommendation baselines (Liu et al., 2023a). This could be due to language models' strength in capturing language patterns rather than effectively collaborating to suggest similar items based on user preferences (Zhang et al., 2021b). Besides, Liu et al. (2023a) pointed out that the position of candidate items in the item pool can also affect the direct recommendation performance. Another promptingbased model, P5, showed the most improvements for both Amazon and Yelp datasets (Geng et al., 2022c), which verifies the need for more guidance when using large pre-trained language models for recommendations. Finally, for news recommendation on the MIND dataset, Xiao et al. (2022) introduced a model-agnostic fine-tuning framework     with cache management, which can accelerate the model training process and yield the most improvements over the baselines.","(p15.0) In this section, we compare the results obtained from various models using commonly used datasets. Specifically, based on the reported results in the paper, we measured the improvement achieved by different models compared to a shared baseline and evaluated them using the same metrics on the same dataset. The comparisons are presented in Table 3∼6. Most improvements are highlighted in bold, and N@k denotes NDCG@k, H@k denotes HitRate@k. It's important to recognize that a comprehensive and precise assessment cannot be achieved without a carefully designed platform and thoughtful settings for conducting the experiments. Various factors, such as diverse training platforms, parameter settings, and data split strategy, can lead to fluctuations in the results. Hence, it is essential to consider the analysis solely for reference purposes. From the tables, we can observe that: First, among the four conversational recommender systems assessed using the ReDial dataset, fixed prompt PTM tuning paradigm Yang et al. (2022a) demonstrate the most significant improvements compared to the shared baselines. Second, on the Amazon dataset, zero-shot and few-shot learning of Chat-GPT underperformed the supervised recommendation baselines (Liu et al., 2023a). This could be due to language models' strength in capturing language patterns rather than effectively collaborating to suggest similar items based on user preferences (Zhang et al., 2021b). Besides, Liu et al. (2023a) pointed out that the position of candidate items in the item pool can also affect the direct recommendation performance. Another promptingbased model, P5, showed the most improvements for both Amazon and Yelp datasets (Geng et al., 2022c), which verifies the need for more guidance when using large pre-trained language models for recommendations. Finally, for news recommendation on the MIND dataset, Xiao et al. (2022) introduced a model-agnostic fine-tuning framework     with cache management, which can accelerate the model training process and yield the most improvements over the baselines.","[['b53', 'b56', 'b67', 'b22', 'b9']]","[['b53', 'b56', 'b67', 'b22', 'b9']]",5,"1. In this section, we compare the results obtained from various models using commonly used datasets.
2. Specifically, based on the reported results in the paper, we measured the improvement achieved by different models compared to a shared baseline and evaluated them using the same metrics on the same dataset.
3. The comparisons are presented in Table 3∼6.
4. Most improvements are highlighted in bold, and N@k denotes NDCG@k, H@k denotes HitRate@k.
5. It's important to recognize that a comprehensive and precise assessment cannot be achieved without a carefully designed platform and thoughtful settings for conducting the experiments.
6. Various factors, such as diverse training platforms, parameter settings, and data split strategy, can lead to fluctuations in the results.
7. Hence, it is essential to consider the analysis solely for reference purposes.
8. From the tables, we can observe that: First, among the four conversational recommender systems assessed using the ReDial dataset, fixed prompt PTM tuning paradigm Yang et al. (2022a) demonstrate the most significant improvements compared to the shared baselines.
9. Second, on the Amazon dataset, zero-shot and few-shot learning of Chat-GPT underperformed the supervised recommendation baselines (Liu et al., 2023a).
10. This could be due to language models' strength in capturing language patterns rather than effectively collaborating to suggest similar items based on user preferences (Zhang et al., 2021b).
11. Besides, Liu et al. (2023a) pointed out that the position of candidate items in the item pool can also affect the direct recommendation performance.
12. Another promptingbased model, P5, showed the most improvements for both Amazon and Yelp datasets (Geng et al., 2022c), which verifies the need for more guidance when using large pre-trained language models for recommendations.
13. Finally, for news recommendation on the MIND dataset, Xiao et al. (2022) introduced a model-agnostic fine-tuning framework     with cache management, which can accelerate the model training process and yield the most improvements over the baselines."
256662721,"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems","Computer Science, Linguistics",https://www.semanticscholar.org/paper/c589a3420ba335a05c248f525ea3c6e90215e42b,s7,Rating Prediction,['p7.0'],"['Fine-tuning Holistic Model (Xie et al., 2023), Fine-tuning External Part (Hada and Shevade, 2021;Geng et al., 2022a), Tuning-free Prompting (Geng et al., 2022c) Textual data (Hada and Shevade, 2021;Qiu et al., 2021;Li et al., 2023b;Geng et al., 2022c;Xie et al., 2023); Sequential data (Geng et al., 2022c;Sankar et al., 2021); Graph ; Multi-modal data (Geng et al., 2022a) Cross-domain RS Fine-tuning Holistic Model (Qiu et al., 2021) Explainable RS Fine-tuning Holistic Model (Xie et al., 2023), Fine-tuning External Part (Geng et al., 2022a), Fixed-PTM Prompt Tuning (Li et al., 2023b), Tuning-free Prompting (Geng et al., 2022c) ']","Fine-tuning Holistic Model (Xie et al., 2023), Fine-tuning External Part (Hada and Shevade, 2021;Geng et al., 2022a), Tuning-free Prompting (Geng et al., 2022c) Textual data (Hada and Shevade, 2021;Qiu et al., 2021;Li et al., 2023b;Geng et al., 2022c;Xie et al., 2023); Sequential data (Geng et al., 2022c;Sankar et al., 2021); Graph ; Multi-modal data (Geng et al., 2022a) Cross-domain RS Fine-tuning Holistic Model (Qiu et al., 2021) Explainable RS Fine-tuning Holistic Model (Xie et al., 2023), Fine-tuning External Part (Geng et al., 2022a), Fixed-PTM Prompt Tuning (Li et al., 2023b), Tuning-free Prompting (Geng et al., 2022c) ","(p7.0) Fine-tuning Holistic Model (Xie et al., 2023), Fine-tuning External Part (Hada and Shevade, 2021;Geng et al., 2022a), Tuning-free Prompting (Geng et al., 2022c) Textual data (Hada and Shevade, 2021;Qiu et al., 2021;Li et al., 2023b;Geng et al., 2022c;Xie et al., 2023); Sequential data (Geng et al., 2022c;Sankar et al., 2021); Graph ; Multi-modal data (Geng et al., 2022a) Cross-domain RS Fine-tuning Holistic Model (Qiu et al., 2021) Explainable RS Fine-tuning Holistic Model (Xie et al., 2023), Fine-tuning External Part (Geng et al., 2022a), Fixed-PTM Prompt Tuning (Li et al., 2023b), Tuning-free Prompting (Geng et al., 2022c) ","[['b20', 'b37', 'b36', 'b54', 'b7', 'b12', 'b9']]","[['b20', 'b37', 'b36', 'b54', 'b7', 'b12', 'b9']]",7,"1. Fine-tuning Holistic Model (Xie et al., 2023), Fine-tuning External Part (Hada and Shevade, 2021;Geng et al., 2022a), Tuning-free Prompting (Geng et al., 2022c) Textual data (Hada and Shevade, 2021;Qiu et al., 2021;Li et al., 2023b;Geng et al., 2022c;Xie et al., 2023); Sequential data (Geng et al., 2022c;Sankar et al., 2021); Graph ; Multi-modal data (Geng et al., 2022a)
2. Cross-domain RS Fine-tuning Holistic Model (Qiu et al., 2021)
3. Explainable RS Fine-tuning Holistic Model (Xie et al., 2023), Fine-tuning External Part (Geng et al., 2022a), Fixed-PTM Prompt Tuning (Li et al., 2023b), Tuning-free Prompting (Geng et al., 2022c)"
256662721,"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems","Computer Science, Linguistics",https://www.semanticscholar.org/paper/c589a3420ba335a05c248f525ea3c6e90215e42b,s10,MIND Link Top-N RS,['p10.0'],"['Fine-tuning Holistic Model (Xiao et al., 2022), Fine-tuning Partial Mode (Yu et al., 2022), Fine-tuning External Part (Yu et al., 2022), Fixed-prompt PTM Tuning  Textual data (Xiao et al., 2022;Yu et al., 2022;; Sequential data (Xiao et al., 2022;Yu et al., 2022) ReDial Link Conversational RS Fine-tuning Holistic Model , Fixed-PTM Prompt Tuning (Wang et al., 2022c), Fixed-prompt PTM Tuning (Yang et al., 2022a) Textual data (Wang et al., 2022c;Yang et al., 2022a;; Graph (Li et (Hou et al., 2022) Textual + Sequential data (Hou et al., 2022)  where y t = 1(x t = x t ), andX is corrupted from the input sequence X. de Souza Pereira Moreira et al. (2021) trained a Transformer-based model with RTD objective for session-based recommendations, which achieved the best performance among MLM and AM objectives. This is probably because RTD takes the whole user-item interaction sequence as input and model the context from the bidirectional way.']","Fine-tuning Holistic Model (Xiao et al., 2022), Fine-tuning Partial Mode (Yu et al., 2022), Fine-tuning External Part (Yu et al., 2022), Fixed-prompt PTM Tuning  Textual data (Xiao et al., 2022;Yu et al., 2022;; Sequential data (Xiao et al., 2022;Yu et al., 2022) ReDial Link Conversational RS Fine-tuning Holistic Model , Fixed-PTM Prompt Tuning (Wang et al., 2022c), Fixed-prompt PTM Tuning (Yang et al., 2022a) Textual data (Wang et al., 2022c;Yang et al., 2022a;; Graph (Li et (Hou et al., 2022) Textual + Sequential data (Hou et al., 2022)  where y t = 1(x t = x t ), andX is corrupted from the input sequence X. de Souza Pereira Moreira et al. (2021) trained a Transformer-based model with RTD objective for session-based recommendations, which achieved the best performance among MLM and AM objectives. This is probably because RTD takes the whole user-item interaction sequence as input and model the context from the bidirectional way.","(p10.0) Fine-tuning Holistic Model (Xiao et al., 2022), Fine-tuning Partial Mode (Yu et al., 2022), Fine-tuning External Part (Yu et al., 2022), Fixed-prompt PTM Tuning  Textual data (Xiao et al., 2022;Yu et al., 2022;; Sequential data (Xiao et al., 2022;Yu et al., 2022) ReDial Link Conversational RS Fine-tuning Holistic Model , Fixed-PTM Prompt Tuning (Wang et al., 2022c), Fixed-prompt PTM Tuning (Yang et al., 2022a) Textual data (Wang et al., 2022c;Yang et al., 2022a;; Graph (Li et (Hou et al., 2022) Textual + Sequential data (Hou et al., 2022)  where y t = 1(x t = x t ), andX is corrupted from the input sequence X. de Souza Pereira Moreira et al. (2021) trained a Transformer-based model with RTD objective for session-based recommendations, which achieved the best performance among MLM and AM objectives. This is probably because RTD takes the whole user-item interaction sequence as input and model the context from the bidirectional way.","[['b53', 'b56', 'b48', 'b15', None, 'b61']]","[['b53', 'b56', 'b48', 'b15', None, 'b61']]",6,"1. Fine-tuning Holistic Model (Xiao et al., 2022), Fine-tuning Partial Mode (Yu et al., 2022), Fine-tuning External Part (Yu et al., 2022), Fixed-prompt PTM Tuning  Textual data (Xiao et al., 2022;Yu et al., 2022;; Sequential data (Xiao et al., 2022;Yu et al., 2022) ReDial Link Conversational RS Fine-tuning Holistic Model , Fixed-PTM Prompt Tuning (Wang et al., 2022c), Fixed-prompt PTM Tuning (Yang et al., 2022a) Textual data (Wang et al., 2022c;Yang et al., 2022a;; Graph (Li et (Hou et al., 2022) Textual + Sequential data (Hou et al., 2022)  where y t = 1(x t = x t ), andX is corrupted from the input sequence X. de Souza Pereira Moreira et al. (2021) trained a Transformer-based model with RTD objective for session-based recommendations, which achieved the best performance among MLM and AM objectives.
2. This is probably because RTD takes the whole user-item interaction sequence as input and model the context from the bidirectional way."
258378191,A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,Computer Science,https://www.semanticscholar.org/paper/0d94850737bcf0df409ac7f2a6610f6ec69a6809,s4,Keyphrase Extraction Dataset,"['p4.0', 'p4.1']","['Since the first shared task on KE (Turney, 1999), many shared tasks and benchmark datasets for KE have been created. Specifically, OpenKP (Xiong et al., 2019), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, Se-mEval2017 (Augenstein et al., 2017), and KP20k (Meng et al., 2017) were created from scientific articles in English.', 'Compared with other datasets, KP20k contains a large amount of annotation data, so it is often used as the dataset to train the neural-based KE models recently. Meanwhile, in recent papers (Sun et al., 2020a;Song et al., 2021), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, and SemEval2017 (Augenstein et al., 2017)  datasets are often used as the zero-shot test sets to verify the robustness of the KE models trained by the KP20k dataset. Furthermore, KE tasks have also been organized on newswire articles in English, e.g., DUC2001 (Wan and Xiao, 2008b). Table 1 summarizes the statistics of several commonly used benchmark datasets.']","Since the first shared task on KE (Turney, 1999), many shared tasks and benchmark datasets for KE have been created. Specifically, OpenKP (Xiong et al., 2019), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, Se-mEval2017 (Augenstein et al., 2017), and KP20k (Meng et al., 2017) were created from scientific articles in English.

Compared with other datasets, KP20k contains a large amount of annotation data, so it is often used as the dataset to train the neural-based KE models recently. Meanwhile, in recent papers (Sun et al., 2020a;Song et al., 2021), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, and SemEval2017 (Augenstein et al., 2017)  datasets are often used as the zero-shot test sets to verify the robustness of the KE models trained by the KP20k dataset. Furthermore, KE tasks have also been organized on newswire articles in English, e.g., DUC2001 (Wan and Xiao, 2008b). Table 1 summarizes the statistics of several commonly used benchmark datasets.","(p4.0) Since the first shared task on KE (Turney, 1999), many shared tasks and benchmark datasets for KE have been created. Specifically, OpenKP (Xiong et al., 2019), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, Se-mEval2017 (Augenstein et al., 2017), and KP20k (Meng et al., 2017) were created from scientific articles in English.

(p4.1) Compared with other datasets, KP20k contains a large amount of annotation data, so it is often used as the dataset to train the neural-based KE models recently. Meanwhile, in recent papers (Sun et al., 2020a;Song et al., 2021), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, and SemEval2017 (Augenstein et al., 2017)  datasets are often used as the zero-shot test sets to verify the robustness of the KE models trained by the KP20k dataset. Furthermore, KE tasks have also been organized on newswire articles in English, e.g., DUC2001 (Wan and Xiao, 2008b). Table 1 summarizes the statistics of several commonly used benchmark datasets.","[['b66', 'b29', None, 'b38', 'b22', 'b2', 'b43'], ['b61', 'b29', None, 'b52', 'b22', 'b51', 'b2', 'b43']]","[['b66', 'b29', None, 'b38', 'b22', 'b2', 'b43'], ['b61', 'b29', None, 'b52', 'b22', 'b51', 'b2', 'b43']]",15,"1. Since the first shared task on KE (Turney, 1999), many shared tasks and benchmark datasets for KE have been created.
2. Specifically, OpenKP (Xiong et al., 2019), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, Se-mEval2017 (Augenstein et al., 2017), and KP20k (Meng et al., 2017) were created from scientific articles in English.
3. Compared with other datasets, KP20k contains a large amount of annotation data, so it is often used as the dataset to train the neural-based KE models recently.
4. Meanwhile, in recent papers (Sun et al., 2020a;Song et al., 2021), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, and SemEval2017 (Augenstein et al., 2017)  datasets are often used as the zero-shot test sets to verify the robustness of the KE models trained by the KP20k dataset.
5. Furthermore, KE tasks have also been organized on newswire articles in English, e.g., DUC2001 (Wan and Xiao, 2008b).
6. Table 1 summarizes the statistics of several commonly used benchmark datasets."
258378191,A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,Computer Science,https://www.semanticscholar.org/paper/0d94850737bcf0df409ac7f2a6610f6ec69a6809,s9,One-Stage Supervised Keyphrase Extraction Models,['p9.0'],"['A major limitation of the above two-stage supervised approaches is classifying the labels of each candidate phrase independently while ignoring the dependencies that could potentially exist between candidates. Therefore, recent studies (Gollapalli et al., 2017;Basaldella et al., 2018;Wang et al., 2018;Alzaidy et al., 2019;Sun et al., 2019;Mu et al., 2020;Sahrawat et al., 2020) formulated keyphrase extraction as sequence labeling and showed that using linear-chain Conditional Random Fields improved the performance over baseline models for this task. Then, Mu et al. (2020) proposes SKE-BASE-CLS and -RANK, which directly extracts span-based phrase representations from all the document tokens via pre-trained language models and further learn to capture the interaction between them and their corresponding document to get better ranking results. Furthermore, this kind of model can extract overlapped keyphrases (Mu et al., 2020).']","A major limitation of the above two-stage supervised approaches is classifying the labels of each candidate phrase independently while ignoring the dependencies that could potentially exist between candidates. Therefore, recent studies (Gollapalli et al., 2017;Basaldella et al., 2018;Wang et al., 2018;Alzaidy et al., 2019;Sun et al., 2019;Mu et al., 2020;Sahrawat et al., 2020) formulated keyphrase extraction as sequence labeling and showed that using linear-chain Conditional Random Fields improved the performance over baseline models for this task. Then, Mu et al. (2020) proposes SKE-BASE-CLS and -RANK, which directly extracts span-based phrase representations from all the document tokens via pre-trained language models and further learn to capture the interaction between them and their corresponding document to get better ranking results. Furthermore, this kind of model can extract overlapped keyphrases (Mu et al., 2020).","(p9.0) A major limitation of the above two-stage supervised approaches is classifying the labels of each candidate phrase independently while ignoring the dependencies that could potentially exist between candidates. Therefore, recent studies (Gollapalli et al., 2017;Basaldella et al., 2018;Wang et al., 2018;Alzaidy et al., 2019;Sun et al., 2019;Mu et al., 2020;Sahrawat et al., 2020) formulated keyphrase extraction as sequence labeling and showed that using linear-chain Conditional Random Fields improved the performance over baseline models for this task. Then, Mu et al. (2020) proposes SKE-BASE-CLS and -RANK, which directly extracts span-based phrase representations from all the document tokens via pre-trained language models and further learn to capture the interaction between them and their corresponding document to get better ranking results. Furthermore, this kind of model can extract overlapped keyphrases (Mu et al., 2020).","[['b3', 'b18', 'b54', 'b63', None, 'b47', 'b1']]","[['b3', 'b18', 'b54', 'b63', None, 'b47', 'b1']]",7,"1. A major limitation of the above two-stage supervised approaches is classifying the labels of each candidate phrase independently while ignoring the dependencies that could potentially exist between candidates.
2. Therefore, recent studies (Gollapalli et al., 2017;Basaldella et al., 2018;Wang et al., 2018;Alzaidy et al., 2019;Sun et al., 2019;Mu et al., 2020;Sahrawat et al., 2020) formulated keyphrase extraction as sequence labeling and showed that using linear-chain Conditional Random Fields improved the performance over baseline models for this task.
3. Then, Mu et al. (2020) proposes SKE-BASE-CLS and -RANK, which directly extracts span-based phrase representations from all the document tokens via pre-trained language models and further learn to capture the interaction between them and their corresponding document to get better ranking results.
4. Furthermore, this kind of model can extract overlapped keyphrases (Mu et al., 2020)."
258378191,A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,Computer Science,https://www.semanticscholar.org/paper/0d94850737bcf0df409ac7f2a6610f6ec69a6809,s8,Two-Stage Supervised Keyphrase Extraction Models,['p8.0'],"['Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context. JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.  ']","Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context. JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.  ","(p8.0) Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context. JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.  ","[['b49', 'b66', 'b0', 'b46', None, 'b52', 'b13', 'b51']]","[['b49', 'b66', 'b0', 'b46', None, 'b52', 'b13', 'b51']]",8,"1. Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously.
2. Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations.
3. BLING-KPE achieves significant improvement over previous models.
4. To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction.
5. Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents.
6. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.
7. JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.
8. To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.
9. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases."
258378191,A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,Computer Science,https://www.semanticscholar.org/paper/0d94850737bcf0df409ac7f2a6610f6ec69a6809,s10,Discussion,"['p10.0', 'p10.1', 'p10.2', 'p10.3', 'p10.4']","[""In this section, we report the results of the recent unsupervised and supervised keyphrase extraction baselines, which all adopt pre-trained language models as the backbone, as shown in Table 2 and  Table 3. Specifically, Table 2 presents the results of the traditional unsupervised methods and the unsupervised embedding-based keyphrase extraction baselines discussed in Section 5.1 on the DUC2001 (Wan and Xiao, 2008b), Inspec (Hulth, 2003), Se-mEval2010 (Kim et al., 2010, and SemEval2017 (Augenstein et al., 2017) datasets. Embeddingbased two-stage models without PLMs indicate that the models do not use pre-trained language models as the backbone to obtain representations. Table 3 shows the results of all the different categories of the supervised keyphrase extraction systems discussed in Section 5.2 and Section 5.3 on the KP20k (Meng et al., 2017) and OpenKP (Xiong et al., 2019) datasets. Our first finding from the survey is those twostage embedding-based systems with static embeddings outperform two-stage traditional methods, despite the latter's access to different valuable features (e.g., word frequency, position, linguistic properties, topic, length, the relationship between words, external knowledge-based information, etc.). This further demonstrates the necessity of studying embedding-based methods."", 'Our second finding is those embedding-based systems with PLMs outperform embedding-based approaches with static embeddings in most cases.  Table 3: Results of different categories of supervised keyphrase extraction models on two benchmark keyphrase datasets. F1 scores on the top 1, 3, 5, and 10 keyphrases are reported. † indicates the results are reported by their corresponding paper (Sun et al., 2020a), and ‡ denotes that these results are re-evaluated by ourselves via the code which is provided by its corresponding paper (Sun et al., 2020a). The best results are highlighted in bold. The results of baseline models are those presented in the original papers or better results published in other papers recently.', 'However, not all embedding-based systems with PLMs are superior to embedding-based systems with static embeddings. The former generally outperforms the latter when adopting the same importance estimation strategy, but the estimation strategy can significantly affect the results of keyphrase extraction. To sum up, effectively using pre-trained embeddings to estimate the importance score of each candidate is a critical part of improving the performance of keyphrase extraction. Furthermore, there is still interesting progress to be made by leveraging a self-supervised learning strategy to optimize embedding-based systems. MDERank uses a simple yet effective contrastive learning strategy to optimize embedding-based systems, achieving better performance.', 'Our third finding is that the embedding-based methods have slight improvement on long document datasets (e.g., SemEval2010), and all unsupervised methods have poor effects on long document datasets. This demonstrates that keyphrase extraction from long documents is still a challeng-ing problem.', 'Our final finding is that two-stage supervised keyphrase extraction methods are superior to onestage supervised keyphrase extraction methods, as illustrated in Table 3. In addition, the two-stage method has higher scalability and adaptability than the one-stage method, such as handling long and extremely long documents.']","In this section, we report the results of the recent unsupervised and supervised keyphrase extraction baselines, which all adopt pre-trained language models as the backbone, as shown in Table 2 and  Table 3. Specifically, Table 2 presents the results of the traditional unsupervised methods and the unsupervised embedding-based keyphrase extraction baselines discussed in Section 5.1 on the DUC2001 (Wan and Xiao, 2008b), Inspec (Hulth, 2003), Se-mEval2010 (Kim et al., 2010, and SemEval2017 (Augenstein et al., 2017) datasets. Embeddingbased two-stage models without PLMs indicate that the models do not use pre-trained language models as the backbone to obtain representations. Table 3 shows the results of all the different categories of the supervised keyphrase extraction systems discussed in Section 5.2 and Section 5.3 on the KP20k (Meng et al., 2017) and OpenKP (Xiong et al., 2019) datasets. Our first finding from the survey is those twostage embedding-based systems with static embeddings outperform two-stage traditional methods, despite the latter's access to different valuable features (e.g., word frequency, position, linguistic properties, topic, length, the relationship between words, external knowledge-based information, etc.). This further demonstrates the necessity of studying embedding-based methods.

Our second finding is those embedding-based systems with PLMs outperform embedding-based approaches with static embeddings in most cases.  Table 3: Results of different categories of supervised keyphrase extraction models on two benchmark keyphrase datasets. F1 scores on the top 1, 3, 5, and 10 keyphrases are reported. † indicates the results are reported by their corresponding paper (Sun et al., 2020a), and ‡ denotes that these results are re-evaluated by ourselves via the code which is provided by its corresponding paper (Sun et al., 2020a). The best results are highlighted in bold. The results of baseline models are those presented in the original papers or better results published in other papers recently.

However, not all embedding-based systems with PLMs are superior to embedding-based systems with static embeddings. The former generally outperforms the latter when adopting the same importance estimation strategy, but the estimation strategy can significantly affect the results of keyphrase extraction. To sum up, effectively using pre-trained embeddings to estimate the importance score of each candidate is a critical part of improving the performance of keyphrase extraction. Furthermore, there is still interesting progress to be made by leveraging a self-supervised learning strategy to optimize embedding-based systems. MDERank uses a simple yet effective contrastive learning strategy to optimize embedding-based systems, achieving better performance.

Our third finding is that the embedding-based methods have slight improvement on long document datasets (e.g., SemEval2010), and all unsupervised methods have poor effects on long document datasets. This demonstrates that keyphrase extraction from long documents is still a challeng-ing problem.

Our final finding is that two-stage supervised keyphrase extraction methods are superior to onestage supervised keyphrase extraction methods, as illustrated in Table 3. In addition, the two-stage method has higher scalability and adaptability than the one-stage method, such as handling long and extremely long documents.","(p10.0) In this section, we report the results of the recent unsupervised and supervised keyphrase extraction baselines, which all adopt pre-trained language models as the backbone, as shown in Table 2 and  Table 3. Specifically, Table 2 presents the results of the traditional unsupervised methods and the unsupervised embedding-based keyphrase extraction baselines discussed in Section 5.1 on the DUC2001 (Wan and Xiao, 2008b), Inspec (Hulth, 2003), Se-mEval2010 (Kim et al., 2010, and SemEval2017 (Augenstein et al., 2017) datasets. Embeddingbased two-stage models without PLMs indicate that the models do not use pre-trained language models as the backbone to obtain representations. Table 3 shows the results of all the different categories of the supervised keyphrase extraction systems discussed in Section 5.2 and Section 5.3 on the KP20k (Meng et al., 2017) and OpenKP (Xiong et al., 2019) datasets. Our first finding from the survey is those twostage embedding-based systems with static embeddings outperform two-stage traditional methods, despite the latter's access to different valuable features (e.g., word frequency, position, linguistic properties, topic, length, the relationship between words, external knowledge-based information, etc.). This further demonstrates the necessity of studying embedding-based methods.

(p10.1) Our second finding is those embedding-based systems with PLMs outperform embedding-based approaches with static embeddings in most cases.  Table 3: Results of different categories of supervised keyphrase extraction models on two benchmark keyphrase datasets. F1 scores on the top 1, 3, 5, and 10 keyphrases are reported. † indicates the results are reported by their corresponding paper (Sun et al., 2020a), and ‡ denotes that these results are re-evaluated by ourselves via the code which is provided by its corresponding paper (Sun et al., 2020a). The best results are highlighted in bold. The results of baseline models are those presented in the original papers or better results published in other papers recently.

(p10.2) However, not all embedding-based systems with PLMs are superior to embedding-based systems with static embeddings. The former generally outperforms the latter when adopting the same importance estimation strategy, but the estimation strategy can significantly affect the results of keyphrase extraction. To sum up, effectively using pre-trained embeddings to estimate the importance score of each candidate is a critical part of improving the performance of keyphrase extraction. Furthermore, there is still interesting progress to be made by leveraging a self-supervised learning strategy to optimize embedding-based systems. MDERank uses a simple yet effective contrastive learning strategy to optimize embedding-based systems, achieving better performance.

(p10.3) Our third finding is that the embedding-based methods have slight improvement on long document datasets (e.g., SemEval2010), and all unsupervised methods have poor effects on long document datasets. This demonstrates that keyphrase extraction from long documents is still a challeng-ing problem.

(p10.4) Our final finding is that two-stage supervised keyphrase extraction methods are superior to onestage supervised keyphrase extraction methods, as illustrated in Table 3. In addition, the two-stage method has higher scalability and adaptability than the one-stage method, such as handling long and extremely long documents.","[['b22', 'b66', None, 'b38', 'b61', 'b2'], ['b52'], [], [], []]","[['b22', 'b66', None, 'b38', 'b61', 'b2'], ['b52'], [], [], []]",7,"1. In this section, we report the results of the recent unsupervised and supervised keyphrase extraction baselines, which all adopt pre-trained language models as the backbone, as shown in Table 2 and  Table 3.
2. Specifically, Table 2 presents the results of the traditional unsupervised methods and the unsupervised embedding-based keyphrase extraction baselines discussed in Section 5.1 on the DUC2001 (Wan and Xiao, 2008b), Inspec (Hulth, 2003), Se-mEval2010 (Kim et al., 2010, and SemEval2017 (Augenstein et al., 2017) datasets.
3. Embeddingbased two-stage models without PLMs indicate that the models do not use pre-trained language models as the backbone to obtain representations.
4. Table 3 shows the results of all the different categories of the supervised keyphrase extraction systems discussed in Section 5.2 and Section 5.3 on the KP20k (Meng et al., 2017) and OpenKP (Xiong et al., 2019) datasets.
5. Our first finding from the survey is those twostage embedding-based systems with static embeddings outperform two-stage traditional methods, despite the latter's access to different valuable features (e.g., word frequency, position, linguistic properties, topic, length, the relationship between words, external knowledge-based information, etc.).
6. This further demonstrates the necessity of studying embedding-based methods.
7. Our second finding is those embedding-based systems with PLMs outperform embedding-based approaches with static embeddings in most cases.
8. Table 3: Results of different categories of supervised keyphrase extraction models on two benchmark keyphrase datasets.
9. F1 scores on the top 1, 3, 5, and 10 keyphrases are reported.
10. † indicates the results are reported by their corresponding paper (Sun et al., 2020a), and ‡ denotes that these results are re-evaluated by ourselves via the code which is provided by its corresponding paper (Sun et al., 2020a).
11. The best results are highlighted in bold.
12. The results of baseline models are those presented in the original papers or better results published in other papers recently.
13. However, not all embedding-based systems with PLMs are superior to embedding-based systems with static embeddings.
14. The former generally outperforms the latter when adopting the same importance estimation strategy, but the estimation strategy can significantly affect the results of keyphrase extraction.
15. To sum up, effectively using pre-trained embeddings to estimate the importance score of each candidate is a critical part of improving the performance of keyphrase extraction.
16. Furthermore, there is still interesting progress to be made by leveraging a self-supervised learning strategy to optimize embedding-based systems.
17. MDERank uses a simple yet effective contrastive learning strategy to optimize embedding-based systems, achieving better performance.
18. Our third finding is that the embedding-based methods have slight improvement on long document datasets (e.g., SemEval2010), and all unsupervised methods have poor effects on long document datasets.
19. This demonstrates that keyphrase extraction from long documents is still a challeng-ing problem.
20. Our final finding is that two-stage supervised keyphrase extraction methods are superior to onestage supervised keyphrase extraction methods, as illustrated in Table 3.
21. In addition, the two-stage method has higher scalability and adaptability than the one-stage method, such as handling long and extremely long documents."
258378191,A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,Computer Science,https://www.semanticscholar.org/paper/0d94850737bcf0df409ac7f2a6610f6ec69a6809,s7,Two-Stage Unsupervised Keyphrase Extraction Models,"['p7.0', 'p7.1', 'p7.2']","['As noted before, unsupervised keyphrase extraction systems generally extract a set of phrases from the source document as candidates by using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum (Hasan and Ng, 2014). The main steps of the commonly used candidate keyphrases extraction methods for the recent unsupervised keyphrase extraction models are as follows, (1) tokenizing the document and tagging the document with partof-speech (POS) tags via the StanfordCoreNLP Tools 3 ; (2) extracting candidate phrases based on part-of-speech tags by the regular expression via the python package NLTK 4 . Furthermore, different pruning heuristics have been designed for pruning candidates that are unlikely to be keyphrases to obtain a better candidate set (Huang et al., 2006;Kumar and Srinathan, 2008;El-Beltagy and Rafea, 2009;Newman et al., 2012;You et al., 2009). After obtaining candidates, keyphrases are determined by estimating the importance of each candidate through various strategies. Here, to facilitate the introduction, we divide the methods of importance estimation into two categories, namely, traditional methods and embedding-based methods. Traditional unsupervised keyphrase extraction systems can be mainly divided into statistics-based (Jones, 2004;Campos et al., 2018b), topic-based (Liu et al., 2009;Jardine and Teufel, 2014), and graph-based (Mihalcea and Tarau, 2004;Wan and Xiao, 2008b;Bougouin et al., 2013;Florescu and Caragea, 2017b) methods. Generally, these models primarily use different features of documents (e.g., word frequency, position, linguistic properties, topic, length, the relationship between words, external knowledge-based information, etc.) to estimate the importance of each candidate phrase and discriminate whether a candidate phrase is a keyphrase (Hasan and Ng, 2014; Papagiannopoulou and Tsoumakas, 2019).', 'However, these traditional unsupervised models estimate the importance scores of candidate phrases based on the surface-level features, ignoring the high-level features (e.g., syntactic and semantic information) of natural languages, which leads to extract wrong keyphrases. Therefore, recent studies focus on embedding-based models (Wang et al., 2015;Mahata et al., 2018a;Papagiannopoulou and Tsoumakas, 2018;Sahrawat et al., 2020;Kulkarni et al., 2022;Song et al., 2022b), which leverage pretrained embeddings (containing high-level features) to obtain phrase and document embeddings and calculate the importance scores of candidate phrases for extracting keyphrases. Wang et al. (2015) is the first work to explore utilizing word embedding and frequency to generate weighted edges between words, then using the weighted PageRank algorithm to compute and rank candidate scores. Key2vec (Mahata et al., 2018a) proposes an effective way of processing text documents for training multi-word phrase embeddings that are used for topic representations of scientific articles and ranking of keyphrases extracted from them using the topic-weighted PageRank algorithm. Mahata et al. (2018b) uses a combination of theme-weighted personalized PageRank algorithm and neural phrase embeddings for extracting and ranking keyphrases. EmbedRank (Bennani-Smires et al., 2018) ranks candidate phrases by measuring the semantic similarity between each candidate phrase and document embeddings.', 'With the development of pre-trained language models (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERta ), SIFRank 5 (Sun et al., 2020b) improves candidate phrase and document embeddings from EmbedRank with the pre-trained language model ELMo (Peters et al., 2018) and achieves better performance. JointGL 6 (Liang et al., 2021) integrates boundary-aware phrase centrality (the semantic similarities are calculated between all candidate phrases for identifying which candidate is better) and phrase-document relevance (the semantic similarities are calculated between candidate phrases and their corresponding document) from both local and global views, then used both jointly to determine the importance of each candidate. Attention-Rank 7 (Ding and Luo, 2021) adopts a pre-trained language model to calculate the self-attention of a candidate within the context of a sentence, and the cross-attention between a candidate and sentences within the source document to evaluate the local and global importance of each candidate. MDERank 8  proposes to rank candidates using the similarity between the BERT embeddings of the source document and the masked document. Totally, these models achieve state-ofthe-art performance in the unsupervised keyphrase extraction task, benefiting from the development of representation learning.']","As noted before, unsupervised keyphrase extraction systems generally extract a set of phrases from the source document as candidates by using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum (Hasan and Ng, 2014). The main steps of the commonly used candidate keyphrases extraction methods for the recent unsupervised keyphrase extraction models are as follows, (1) tokenizing the document and tagging the document with partof-speech (POS) tags via the StanfordCoreNLP Tools 3 ; (2) extracting candidate phrases based on part-of-speech tags by the regular expression via the python package NLTK 4 . Furthermore, different pruning heuristics have been designed for pruning candidates that are unlikely to be keyphrases to obtain a better candidate set (Huang et al., 2006;Kumar and Srinathan, 2008;El-Beltagy and Rafea, 2009;Newman et al., 2012;You et al., 2009). After obtaining candidates, keyphrases are determined by estimating the importance of each candidate through various strategies. Here, to facilitate the introduction, we divide the methods of importance estimation into two categories, namely, traditional methods and embedding-based methods. Traditional unsupervised keyphrase extraction systems can be mainly divided into statistics-based (Jones, 2004;Campos et al., 2018b), topic-based (Liu et al., 2009;Jardine and Teufel, 2014), and graph-based (Mihalcea and Tarau, 2004;Wan and Xiao, 2008b;Bougouin et al., 2013;Florescu and Caragea, 2017b) methods. Generally, these models primarily use different features of documents (e.g., word frequency, position, linguistic properties, topic, length, the relationship between words, external knowledge-based information, etc.) to estimate the importance of each candidate phrase and discriminate whether a candidate phrase is a keyphrase (Hasan and Ng, 2014; Papagiannopoulou and Tsoumakas, 2019).

However, these traditional unsupervised models estimate the importance scores of candidate phrases based on the surface-level features, ignoring the high-level features (e.g., syntactic and semantic information) of natural languages, which leads to extract wrong keyphrases. Therefore, recent studies focus on embedding-based models (Wang et al., 2015;Mahata et al., 2018a;Papagiannopoulou and Tsoumakas, 2018;Sahrawat et al., 2020;Kulkarni et al., 2022;Song et al., 2022b), which leverage pretrained embeddings (containing high-level features) to obtain phrase and document embeddings and calculate the importance scores of candidate phrases for extracting keyphrases. Wang et al. (2015) is the first work to explore utilizing word embedding and frequency to generate weighted edges between words, then using the weighted PageRank algorithm to compute and rank candidate scores. Key2vec (Mahata et al., 2018a) proposes an effective way of processing text documents for training multi-word phrase embeddings that are used for topic representations of scientific articles and ranking of keyphrases extracted from them using the topic-weighted PageRank algorithm. Mahata et al. (2018b) uses a combination of theme-weighted personalized PageRank algorithm and neural phrase embeddings for extracting and ranking keyphrases. EmbedRank (Bennani-Smires et al., 2018) ranks candidate phrases by measuring the semantic similarity between each candidate phrase and document embeddings.

With the development of pre-trained language models (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERta ), SIFRank 5 (Sun et al., 2020b) improves candidate phrase and document embeddings from EmbedRank with the pre-trained language model ELMo (Peters et al., 2018) and achieves better performance. JointGL 6 (Liang et al., 2021) integrates boundary-aware phrase centrality (the semantic similarities are calculated between all candidate phrases for identifying which candidate is better) and phrase-document relevance (the semantic similarities are calculated between candidate phrases and their corresponding document) from both local and global views, then used both jointly to determine the importance of each candidate. Attention-Rank 7 (Ding and Luo, 2021) adopts a pre-trained language model to calculate the self-attention of a candidate within the context of a sentence, and the cross-attention between a candidate and sentences within the source document to evaluate the local and global importance of each candidate. MDERank 8  proposes to rank candidates using the similarity between the BERT embeddings of the source document and the masked document. Totally, these models achieve state-ofthe-art performance in the unsupervised keyphrase extraction task, benefiting from the development of representation learning.","(p7.0) As noted before, unsupervised keyphrase extraction systems generally extract a set of phrases from the source document as candidates by using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum (Hasan and Ng, 2014). The main steps of the commonly used candidate keyphrases extraction methods for the recent unsupervised keyphrase extraction models are as follows, (1) tokenizing the document and tagging the document with partof-speech (POS) tags via the StanfordCoreNLP Tools 3 ; (2) extracting candidate phrases based on part-of-speech tags by the regular expression via the python package NLTK 4 . Furthermore, different pruning heuristics have been designed for pruning candidates that are unlikely to be keyphrases to obtain a better candidate set (Huang et al., 2006;Kumar and Srinathan, 2008;El-Beltagy and Rafea, 2009;Newman et al., 2012;You et al., 2009). After obtaining candidates, keyphrases are determined by estimating the importance of each candidate through various strategies. Here, to facilitate the introduction, we divide the methods of importance estimation into two categories, namely, traditional methods and embedding-based methods. Traditional unsupervised keyphrase extraction systems can be mainly divided into statistics-based (Jones, 2004;Campos et al., 2018b), topic-based (Liu et al., 2009;Jardine and Teufel, 2014), and graph-based (Mihalcea and Tarau, 2004;Wan and Xiao, 2008b;Bougouin et al., 2013;Florescu and Caragea, 2017b) methods. Generally, these models primarily use different features of documents (e.g., word frequency, position, linguistic properties, topic, length, the relationship between words, external knowledge-based information, etc.) to estimate the importance of each candidate phrase and discriminate whether a candidate phrase is a keyphrase (Hasan and Ng, 2014; Papagiannopoulou and Tsoumakas, 2019).

(p7.1) However, these traditional unsupervised models estimate the importance scores of candidate phrases based on the surface-level features, ignoring the high-level features (e.g., syntactic and semantic information) of natural languages, which leads to extract wrong keyphrases. Therefore, recent studies focus on embedding-based models (Wang et al., 2015;Mahata et al., 2018a;Papagiannopoulou and Tsoumakas, 2018;Sahrawat et al., 2020;Kulkarni et al., 2022;Song et al., 2022b), which leverage pretrained embeddings (containing high-level features) to obtain phrase and document embeddings and calculate the importance scores of candidate phrases for extracting keyphrases. Wang et al. (2015) is the first work to explore utilizing word embedding and frequency to generate weighted edges between words, then using the weighted PageRank algorithm to compute and rank candidate scores. Key2vec (Mahata et al., 2018a) proposes an effective way of processing text documents for training multi-word phrase embeddings that are used for topic representations of scientific articles and ranking of keyphrases extracted from them using the topic-weighted PageRank algorithm. Mahata et al. (2018b) uses a combination of theme-weighted personalized PageRank algorithm and neural phrase embeddings for extracting and ranking keyphrases. EmbedRank (Bennani-Smires et al., 2018) ranks candidate phrases by measuring the semantic similarity between each candidate phrase and document embeddings.

(p7.2) With the development of pre-trained language models (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERta ), SIFRank 5 (Sun et al., 2020b) improves candidate phrase and document embeddings from EmbedRank with the pre-trained language model ELMo (Peters et al., 2018) and achieves better performance. JointGL 6 (Liang et al., 2021) integrates boundary-aware phrase centrality (the semantic similarities are calculated between all candidate phrases for identifying which candidate is better) and phrase-document relevance (the semantic similarities are calculated between candidate phrases and their corresponding document) from both local and global views, then used both jointly to determine the importance of each candidate. Attention-Rank 7 (Ding and Luo, 2021) adopts a pre-trained language model to calculate the self-attention of a candidate within the context of a sentence, and the cross-attention between a candidate and sentences within the source document to evaluate the local and global importance of each candidate. MDERank 8  proposes to rank candidates using the similarity between the BERT embeddings of the source document and the masked document. Totally, these models achieve state-ofthe-art performance in the unsupervised keyphrase extraction task, benefiting from the development of representation learning.","[['b21', 'b41', 'b31', 'b15', 'b8', 'b39', 'b61', 'b17', 'b27', 'b35', 'b67', 'b24', 'b5'], ['b37', 'b30', 'b50', 'b36', 'b62', 'b47', 'b44'], ['b53', 'b13', 'b32', 'b46']]","[['b21', 'b41', 'b31', 'b15', 'b8', 'b39', 'b61', 'b17', 'b27', 'b35', 'b67', 'b24', 'b5'], ['b37', 'b30', 'b50', 'b36', 'b62', 'b47', 'b44'], ['b53', 'b13', 'b32', 'b46']]",24,"1. As noted before, unsupervised keyphrase extraction systems generally extract a set of phrases from the source document as candidates by using heuristic rules.
2. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum (Hasan and Ng, 2014).
3. The main steps of the commonly used candidate keyphrases extraction methods for the recent unsupervised keyphrase extraction models are as follows, (1) tokenizing the document and tagging the document with partof-speech (POS) tags via the StanfordCoreNLP Tools 3 ; (2) extracting candidate phrases based on part-of-speech tags by the regular expression via the python package NLTK 4 .
4. Furthermore, different pruning heuristics have been designed for pruning candidates that are unlikely to be keyphrases to obtain a better candidate set (Huang et al., 2006;Kumar and Srinathan, 2008;El-Beltagy and Rafea, 2009;Newman et al., 2012;You et al., 2009).
5. After obtaining candidates, keyphrases are determined by estimating the importance of each candidate through various strategies.
6. Here, to facilitate the introduction, we divide the methods of importance estimation into two categories, namely, traditional methods and embedding-based methods.
7. Traditional unsupervised keyphrase extraction systems can be mainly divided into statistics-based (Jones, 2004;Campos et al., 2018b), topic-based (Liu et al., 2009;Jardine and Teufel, 2014), and graph-based (Mihalcea and Tarau, 2004;Wan and Xiao, 2008b;Bougouin et al., 2013;Florescu and Caragea, 2017b) methods.
8. Generally, these models primarily use different features of documents (e.g., word frequency, position, linguistic properties, topic, length, the relationship between words, external knowledge-based information, etc.) to estimate the importance of each candidate phrase and discriminate whether a candidate phrase is a keyphrase (Hasan and Ng, 2014; Papagiannopoulou and Tsoumakas, 2019).
9. However, these traditional unsupervised models estimate the importance scores of candidate phrases based on the surface-level features, ignoring the high-level features (e.g., syntactic and semantic information) of natural languages, which leads to extract wrong keyphrases.
10. Therefore, recent studies focus on embedding-based models (Wang et al., 2015;Mahata et al., 2018a;Papagiannopoulou and Tsoumakas, 2018;Sahrawat et al., 2020;Kulkarni et al., 2022;Song et al., 2022b), which leverage pretrained embeddings (containing high-level features) to obtain phrase and document embeddings and calculate the importance scores of candidate phrases for extracting keyphrases.
11. Wang et al. (2015) is the first work to explore utilizing word embedding and frequency to generate weighted edges between words, then using the weighted PageRank algorithm to compute and rank candidate scores.
12. Key2vec (Mahata et al., 2018a) proposes an effective way of processing text documents for training multi-word phrase embeddings that are used for topic representations of scientific articles and ranking of keyphrases extracted from them using the topic-weighted PageRank algorithm.
13. Mahata et al. (2018b) uses a combination of theme-weighted personalized PageRank algorithm and neural phrase embeddings for extracting and ranking keyphrases.
14. EmbedRank (Bennani-Smires et al., 2018) ranks candidate phrases by measuring the semantic similarity between each candidate phrase and document embeddings.
15. With the development of pre-trained language models (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERta ), SIFRank 5 (Sun et al., 2020b) improves candidate phrase and document embeddings from EmbedRank with the pre-trained language model ELMo (Peters et al., 2018) and achieves better performance.
16. JointGL 6 (Liang et al., 2021) integrates boundary-aware phrase centrality (the semantic similarities are calculated between all candidate phrases for identifying which candidate is better) and phrase-document relevance (the semantic similarities are calculated between candidate phrases and their corresponding document) from both local and global views, then used both jointly to determine the importance of each candidate.
17. Attention-Rank 7 (Ding and Luo, 2021) adopts a pre-trained language model to calculate the self-attention of a candidate within the context of a sentence, and the cross-attention between a candidate and sentences within the source document to evaluate the local and global importance of each candidate.
18. MDERank 8  proposes to rank candidates using the similarity between the BERT embeddings of the source document and the masked document.
19. Totally, these models achieve state-ofthe-art performance in the unsupervised keyphrase extraction task, benefiting from the development of representation learning."
258378191,A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,Computer Science,https://www.semanticscholar.org/paper/0d94850737bcf0df409ac7f2a6610f6ec69a6809,s3,Pre-trained Language Models,['p3.0'],"['Recently, pre-trained language models have advanced the state-of-the-art in many NLP tasks ranging from textual similarity to text summarization (Zhang et al., 2019;Liu and Lapata, 2019;Zhong et al., 2020) and named entity recognition (Zhou et al., 2021). State-of-the-art pre-trained models include LSTM-based language models (e.g., ELMo (Peters et al., 2018)) and Transformer-based language models (e.g., BERT 2 (Devlin et al., 2019) and RoBERTa ). Specifically, the transformer-based models learn bidirectional representations for words based on a masked language model and sentence adjacency training objective (Devlin et al., 2019). Simply using contextualized embeddings obtained from the transformerbased pre-trained language models in place of traditional embeddings has resulted in state-of-the-art performance on a range of NLP tasks. Therefore, pre-trained language models have been employed as encoders for obtaining word-, sentence-, and document-level representations to assist the downstream tasks.']","Recently, pre-trained language models have advanced the state-of-the-art in many NLP tasks ranging from textual similarity to text summarization (Zhang et al., 2019;Liu and Lapata, 2019;Zhong et al., 2020) and named entity recognition (Zhou et al., 2021). State-of-the-art pre-trained models include LSTM-based language models (e.g., ELMo (Peters et al., 2018)) and Transformer-based language models (e.g., BERT 2 (Devlin et al., 2019) and RoBERTa ). Specifically, the transformer-based models learn bidirectional representations for words based on a masked language model and sentence adjacency training objective (Devlin et al., 2019). Simply using contextualized embeddings obtained from the transformerbased pre-trained language models in place of traditional embeddings has resulted in state-of-the-art performance on a range of NLP tasks. Therefore, pre-trained language models have been employed as encoders for obtaining word-, sentence-, and document-level representations to assist the downstream tasks.","(p3.0) Recently, pre-trained language models have advanced the state-of-the-art in many NLP tasks ranging from textual similarity to text summarization (Zhang et al., 2019;Liu and Lapata, 2019;Zhong et al., 2020) and named entity recognition (Zhou et al., 2021). State-of-the-art pre-trained models include LSTM-based language models (e.g., ELMo (Peters et al., 2018)) and Transformer-based language models (e.g., BERT 2 (Devlin et al., 2019) and RoBERTa ). Specifically, the transformer-based models learn bidirectional representations for words based on a masked language model and sentence adjacency training objective (Devlin et al., 2019). Simply using contextualized embeddings obtained from the transformerbased pre-trained language models in place of traditional embeddings has resulted in state-of-the-art performance on a range of NLP tasks. Therefore, pre-trained language models have been employed as encoders for obtaining word-, sentence-, and document-level representations to assist the downstream tasks.","[['b73', 'b33', 'b72', 'b46', 'b13', 'b70']]","[['b73', 'b33', 'b72', 'b46', 'b13', 'b70']]",6,"1. Recently, pre-trained language models have advanced the state-of-the-art in many NLP tasks ranging from textual similarity to text summarization (Zhang et al., 2019;Liu and Lapata, 2019;Zhong et al., 2020) and named entity recognition (Zhou et al., 2021).
2. State-of-the-art pre-trained models include LSTM-based language models (e.g., ELMo (Peters et al., 2018)) and Transformer-based language models (e.g., BERT 2 (Devlin et al., 2019) and RoBERTa ).
3. Specifically, the transformer-based models learn bidirectional representations for words based on a masked language model and sentence adjacency training objective (Devlin et al., 2019).
4. Simply using contextualized embeddings obtained from the transformerbased pre-trained language models in place of traditional embeddings has resulted in state-of-the-art performance on a range of NLP tasks.
5. Therefore, pre-trained language models have been employed as encoders for obtaining word-, sentence-, and document-level representations to assist the downstream tasks."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Computer Science,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,s6,Discussion,['p6.0'],"['If the heuristics or QG are properly designed, NR models trained from their supervision can even match the fully-supervised performance Ren et al., 2022). The biggest challenge is the difficulty to pick the most suitable heuristics or QG when we face a new domain. A general solution is to automatically select good pseudo pairs with reinforcement learning (RL) when minimal target-domain annotations are available , so as avoiding the need to manually fixing the WS signals, but this would bring significant computational overhead. In practice hyperlink-based approaches often perform the best among the heuristics as they have additional reference information to leverage, which makes them most similar to the actual relevance annotations. However, hyperlink information is not available in most domains and thereby limits its use cases . QG-based WS signals are often preferred over heuristics-based ones as they can produce naturally-sound questions themselves without relying on the chance to find good pseudo questions in the documents. Nonetheless, obtaining a high-performing QG can also be non-trivial. One big challenge comes from the one-to-many mapping relations between questions and documents. Under this situation, standard supervised learning tends to produce safe questions with less diversity and high lexical overlap with the document. For example, Shinoda et al. (2021) found that QG reinforces the model bias towards high lexical overlap. We will need more sophisticated training techniques such as latent-variable models (Shen and Su, 2018; and reinforcement learning (Yuan et al., 2017;Zhang and Bansal, 2019;Shen et al., 2019a) to alleviate the model bias towards safe questions.']","If the heuristics or QG are properly designed, NR models trained from their supervision can even match the fully-supervised performance Ren et al., 2022). The biggest challenge is the difficulty to pick the most suitable heuristics or QG when we face a new domain. A general solution is to automatically select good pseudo pairs with reinforcement learning (RL) when minimal target-domain annotations are available , so as avoiding the need to manually fixing the WS signals, but this would bring significant computational overhead. In practice hyperlink-based approaches often perform the best among the heuristics as they have additional reference information to leverage, which makes them most similar to the actual relevance annotations. However, hyperlink information is not available in most domains and thereby limits its use cases . QG-based WS signals are often preferred over heuristics-based ones as they can produce naturally-sound questions themselves without relying on the chance to find good pseudo questions in the documents. Nonetheless, obtaining a high-performing QG can also be non-trivial. One big challenge comes from the one-to-many mapping relations between questions and documents. Under this situation, standard supervised learning tends to produce safe questions with less diversity and high lexical overlap with the document. For example, Shinoda et al. (2021) found that QG reinforces the model bias towards high lexical overlap. We will need more sophisticated training techniques such as latent-variable models (Shen and Su, 2018; and reinforcement learning (Yuan et al., 2017;Zhang and Bansal, 2019;Shen et al., 2019a) to alleviate the model bias towards safe questions.","(p6.0) If the heuristics or QG are properly designed, NR models trained from their supervision can even match the fully-supervised performance Ren et al., 2022). The biggest challenge is the difficulty to pick the most suitable heuristics or QG when we face a new domain. A general solution is to automatically select good pseudo pairs with reinforcement learning (RL) when minimal target-domain annotations are available , so as avoiding the need to manually fixing the WS signals, but this would bring significant computational overhead. In practice hyperlink-based approaches often perform the best among the heuristics as they have additional reference information to leverage, which makes them most similar to the actual relevance annotations. However, hyperlink information is not available in most domains and thereby limits its use cases . QG-based WS signals are often preferred over heuristics-based ones as they can produce naturally-sound questions themselves without relying on the chance to find good pseudo questions in the documents. Nonetheless, obtaining a high-performing QG can also be non-trivial. One big challenge comes from the one-to-many mapping relations between questions and documents. Under this situation, standard supervised learning tends to produce safe questions with less diversity and high lexical overlap with the document. For example, Shinoda et al. (2021) found that QG reinforces the model bias towards high lexical overlap. We will need more sophisticated training techniques such as latent-variable models (Shen and Su, 2018; and reinforcement learning (Yuan et al., 2017;Zhang and Bansal, 2019;Shen et al., 2019a) to alleviate the model bias towards safe questions.","[['b77', 'b49', 'b69', 'b46', 'b45', None]]","[['b77', 'b49', 'b69', 'b46', 'b45', None]]",6,"1. If the heuristics or QG are properly designed, NR models trained from their supervision can even match the fully-supervised performance Ren et al., 2022).
2. The biggest challenge is the difficulty to pick the most suitable heuristics or QG when we face a new domain.
3. A general solution is to automatically select good pseudo pairs with reinforcement learning (RL) when minimal target-domain annotations are available , so as avoiding the need to manually fixing the WS signals, but this would bring significant computational overhead.
4. In practice hyperlink-based approaches often perform the best among the heuristics as they have additional reference information to leverage, which makes them most similar to the actual relevance annotations.
5. However, hyperlink information is not available in most domains and thereby limits its use cases .
6. QG-based WS signals are often preferred over heuristics-based ones as they can produce naturally-sound questions themselves without relying on the chance to find good pseudo questions in the documents.
7. Nonetheless, obtaining a high-performing QG can also be non-trivial.
8. One big challenge comes from the one-to-many mapping relations between questions and documents.
9. Under this situation, standard supervised learning tends to produce safe questions with less diversity and high lexical overlap with the document.
10. For example, Shinoda et al. (2021) found that QG reinforces the model bias towards high lexical overlap.
11. We will need more sophisticated training techniques such as latent-variable models (Shen and Su, 2018; and reinforcement learning (Yuan et al., 2017;Zhang and Bansal, 2019;Shen et al., 2019a) to alleviate the model bias towards safe questions."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Computer Science,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,s7,Resource: Documents + Questions,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4']","['This section includes WS signals that require additional access to a question set Q. In practice, annotating question-document relations usually requires domain experts to read long documents and careful sampling strategies to ensure enough positive samples, while unlabeled questions are much easier to obtain either through real user-generated content or simulated traffic. Therefore, it is common to have a predominance of unlabeled questions. The crucial point is to establish the missing relevance labels. Suppose a WS method can provide the missing label WS(q, d) for a question-document pair (q, d), then we can use it to supervise the NR model by:', 'where L is the loss function that encourages similarity between R(q, d) and WS(q, d).', 'There are three popular types models that can provide such WS signal here: (1) sparse retriever, (2) pre-trained language model and (3) supervised teacher model.', 'Sparse Retriever (SR) Recent research finds that NR and SR models are complementary. NR models are better at semantic matching while SRs are better at capturing exact match and handling long documents . SRs are also more robust across domains (Thakur et al., 2021;Chen et al., 2022). This motivates the use of unsupervised sparse retrievers like BM25 as WS signals. Pre-trained Language Model (PLM) As PLMs already encode significant linguistic knowledge, there have also been attempts at using promptbased PLMs to provide WS signals for question-document relations (Smith et al., 2022;Zeng et al., 2022). Similar as in question generation, we can use prompts like ""Please write a question based on this passage"", concatenate the document and question, then use the probability assigned by the PLM to auto-label question-document pairs. To maximize the chances of finding positive document, normally we first obtain a set of candidate documents by BM25, then apply PLM to auto-label the candidate set (Sachan et al., 2022). This can further exploit the latent knowledge inside PLMs that has been honed through pre-training, so it often shows better performance compared with weak supervision only using BM25 Singh Sachan et al., 2022).', 'Supervised Teacher Model A very common choice is using a supervised teacher model to provide WS signals. The teacher model is ""supervised"" because it is explicitly fine-tuned on annotated question-document pairs. When in-domain annotations are not sufficient, we can leverage outof-domain (OOD) annotations, if available, to train the teacher model. The teacher model usually employs a more powerful architecture such as with more complex interactions or larger sizes. It may not be directly applicable in downstream tasks due to the latency constraints, but can be useful in providing WS signals for training the NR model. For example, previous research has shown that models with larger sizes or late/cross-interaction structures generalize much better on OOD data Ni et al., 2021;Rosa et al., 2022;Muennighoff, 2022;. After training a teacher model on OOD annotations, applying it to provide WS signals through targetdomain question and document collections can significantly improve the in-domain performance of the NR model (Hofstätter et al., 2021;Lin et al., 2021b;Lu et al., 2022). Kim et al. (2022) further show that we can even use the same architecture and capacity to obtain a good teacher model. They expand the question with centroids of word embeddings from top retrieved passages (using BM25), and then use the expanded query for self knowledge distillation. Similar ideas of reusing the same architecture to provide WS signals have also been explored by Yu et al. (2021a); Kulshreshtha et al. (2021); Zhuang and Zuccon (2022).']","This section includes WS signals that require additional access to a question set Q. In practice, annotating question-document relations usually requires domain experts to read long documents and careful sampling strategies to ensure enough positive samples, while unlabeled questions are much easier to obtain either through real user-generated content or simulated traffic. Therefore, it is common to have a predominance of unlabeled questions. The crucial point is to establish the missing relevance labels. Suppose a WS method can provide the missing label WS(q, d) for a question-document pair (q, d), then we can use it to supervise the NR model by:

where L is the loss function that encourages similarity between R(q, d) and WS(q, d).

There are three popular types models that can provide such WS signal here: (1) sparse retriever, (2) pre-trained language model and (3) supervised teacher model.

Sparse Retriever (SR) Recent research finds that NR and SR models are complementary. NR models are better at semantic matching while SRs are better at capturing exact match and handling long documents . SRs are also more robust across domains (Thakur et al., 2021;Chen et al., 2022). This motivates the use of unsupervised sparse retrievers like BM25 as WS signals. Pre-trained Language Model (PLM) As PLMs already encode significant linguistic knowledge, there have also been attempts at using promptbased PLMs to provide WS signals for question-document relations (Smith et al., 2022;Zeng et al., 2022). Similar as in question generation, we can use prompts like ""Please write a question based on this passage"", concatenate the document and question, then use the probability assigned by the PLM to auto-label question-document pairs. To maximize the chances of finding positive document, normally we first obtain a set of candidate documents by BM25, then apply PLM to auto-label the candidate set (Sachan et al., 2022). This can further exploit the latent knowledge inside PLMs that has been honed through pre-training, so it often shows better performance compared with weak supervision only using BM25 Singh Sachan et al., 2022).

Supervised Teacher Model A very common choice is using a supervised teacher model to provide WS signals. The teacher model is ""supervised"" because it is explicitly fine-tuned on annotated question-document pairs. When in-domain annotations are not sufficient, we can leverage outof-domain (OOD) annotations, if available, to train the teacher model. The teacher model usually employs a more powerful architecture such as with more complex interactions or larger sizes. It may not be directly applicable in downstream tasks due to the latency constraints, but can be useful in providing WS signals for training the NR model. For example, previous research has shown that models with larger sizes or late/cross-interaction structures generalize much better on OOD data Ni et al., 2021;Rosa et al., 2022;Muennighoff, 2022;. After training a teacher model on OOD annotations, applying it to provide WS signals through targetdomain question and document collections can significantly improve the in-domain performance of the NR model (Hofstätter et al., 2021;Lin et al., 2021b;Lu et al., 2022). Kim et al. (2022) further show that we can even use the same architecture and capacity to obtain a good teacher model. They expand the question with centroids of word embeddings from top retrieved passages (using BM25), and then use the expanded query for self knowledge distillation. Similar ideas of reusing the same architecture to provide WS signals have also been explored by Yu et al. (2021a); Kulshreshtha et al. (2021); Zhuang and Zuccon (2022).","(p7.0) This section includes WS signals that require additional access to a question set Q. In practice, annotating question-document relations usually requires domain experts to read long documents and careful sampling strategies to ensure enough positive samples, while unlabeled questions are much easier to obtain either through real user-generated content or simulated traffic. Therefore, it is common to have a predominance of unlabeled questions. The crucial point is to establish the missing relevance labels. Suppose a WS method can provide the missing label WS(q, d) for a question-document pair (q, d), then we can use it to supervise the NR model by:

(p7.1) where L is the loss function that encourages similarity between R(q, d) and WS(q, d).

(p7.2) There are three popular types models that can provide such WS signal here: (1) sparse retriever, (2) pre-trained language model and (3) supervised teacher model.

(p7.3) Sparse Retriever (SR) Recent research finds that NR and SR models are complementary. NR models are better at semantic matching while SRs are better at capturing exact match and handling long documents . SRs are also more robust across domains (Thakur et al., 2021;Chen et al., 2022). This motivates the use of unsupervised sparse retrievers like BM25 as WS signals. Pre-trained Language Model (PLM) As PLMs already encode significant linguistic knowledge, there have also been attempts at using promptbased PLMs to provide WS signals for question-document relations (Smith et al., 2022;Zeng et al., 2022). Similar as in question generation, we can use prompts like ""Please write a question based on this passage"", concatenate the document and question, then use the probability assigned by the PLM to auto-label question-document pairs. To maximize the chances of finding positive document, normally we first obtain a set of candidate documents by BM25, then apply PLM to auto-label the candidate set (Sachan et al., 2022). This can further exploit the latent knowledge inside PLMs that has been honed through pre-training, so it often shows better performance compared with weak supervision only using BM25 Singh Sachan et al., 2022).

(p7.4) Supervised Teacher Model A very common choice is using a supervised teacher model to provide WS signals. The teacher model is ""supervised"" because it is explicitly fine-tuned on annotated question-document pairs. When in-domain annotations are not sufficient, we can leverage outof-domain (OOD) annotations, if available, to train the teacher model. The teacher model usually employs a more powerful architecture such as with more complex interactions or larger sizes. It may not be directly applicable in downstream tasks due to the latency constraints, but can be useful in providing WS signals for training the NR model. For example, previous research has shown that models with larger sizes or late/cross-interaction structures generalize much better on OOD data Ni et al., 2021;Rosa et al., 2022;Muennighoff, 2022;. After training a teacher model on OOD annotations, applying it to provide WS signals through targetdomain question and document collections can significantly improve the in-domain performance of the NR model (Hofstätter et al., 2021;Lin et al., 2021b;Lu et al., 2022). Kim et al. (2022) further show that we can even use the same architecture and capacity to obtain a good teacher model. They expand the question with centroids of word embeddings from top retrieved passages (using BM25), and then use the expanded query for self knowledge distillation. Similar ideas of reusing the same architecture to provide WS signals have also been explored by Yu et al. (2021a); Kulshreshtha et al. (2021); Zhuang and Zuccon (2022).","[[], [], [], ['b53', 'b57', 'b73', 'b40', None], ['b20', 'b67', 'b4', 'b84', None, 'b7', 'b22']]","[[], [], [], ['b53', 'b57', 'b73', 'b40', None], ['b20', 'b67', 'b4', 'b84', None, 'b7', 'b22']]",12,"1. This section includes WS signals that require additional access to a question set Q. In practice, annotating question-document relations usually requires domain experts to read long documents and careful sampling strategies to ensure enough positive samples, while unlabeled questions are much easier to obtain either through real user-generated content or simulated traffic.
2. Therefore, it is common to have a predominance of unlabeled questions.
3. The crucial point is to establish the missing relevance labels.
4. Suppose a WS method can provide the missing label WS(q, d) for a question-document pair (q, d), then we can use it to supervise the NR model by:where L is the loss function that encourages similarity between R(q, d) and WS(q, d).
5. There are three popular types models that can provide such WS signal here: (1) sparse retriever, (2) pre-trained language model and (3) supervised teacher model.
6. Sparse Retriever (SR) Recent research finds that NR and SR models are complementary.
7. NR models are better at semantic matching while SRs are better at capturing exact match and handling long documents .
8. SRs are also more robust across domains (Thakur et al., 2021;Chen et al., 2022).
9. This motivates the use of unsupervised sparse retrievers like BM25 as WS signals.
10. Pre-trained Language Model (PLM)
11. As PLMs already encode significant linguistic knowledge, there have also been attempts at using promptbased PLMs to provide WS signals for question-document relations (Smith et al., 2022;Zeng et al., 2022).
12. Similar as in question generation, we can use prompts like ""Please write a question based on this passage"", concatenate the document and question, then use the probability assigned by the PLM to auto-label question-document pairs.
13. To maximize the chances of finding positive document, normally we first obtain a set of candidate documents by BM25, then apply PLM to auto-label the candidate set (Sachan et al., 2022).
14. This can further exploit the latent knowledge inside PLMs that has been honed through pre-training, so it often shows better performance compared with weak supervision only using BM25 Singh Sachan et al., 2022).
15. Supervised Teacher Model A very common choice is using a supervised teacher model to provide WS signals.
16. The teacher model is ""supervised"" because it is explicitly fine-tuned on annotated question-document pairs.
17. When in-domain annotations are not sufficient, we can leverage outof-domain (OOD) annotations, if available, to train the teacher model.
18. The teacher model usually employs a more powerful architecture such as with more complex interactions or larger sizes.
19. It may not be directly applicable in downstream tasks due to the latency constraints, but can be useful in providing WS signals for training the NR model.
20. For example, previous research has shown that models with larger sizes or late/cross-interaction structures generalize much better on OOD data Ni et al., 2021;Rosa et al., 2022;Muennighoff, 2022;.
21. After training a teacher model on OOD annotations, applying it to provide WS signals through targetdomain question and document collections can significantly improve the in-domain performance of the NR model (Hofstätter et al., 2021;Lin et al., 2021b;Lu et al., 2022).
22. Kim et al. (2022) further show that we can even use the same architecture and capacity to obtain a good teacher model.
23. They expand the question with centroids of word embeddings from top retrieved passages (using BM25), and then use the expanded query for self knowledge distillation.
24. Similar ideas of reusing the same architecture to provide WS signals have also been explored by Yu et al. (2021a); Kulshreshtha et al. (2021); Zhuang and Zuccon (2022)."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Computer Science,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,s8,Discussion,['p8.0'],"['The three WS signals listed above work directly on actual questions instead of pseudo pairs as in §3 so that the NR model can adapt better to the target-domain question distribution. The bottleneck is the quality of the WS signals. SRs and PLMs are unsupervised, which could be more robust when we face a completely different domain (Dai et al., 2022). Otherwise, if we already have certain amounts relevance annotations from the target or similar domains, usually using a supervised teacher model is preferred. Nevertheless, these WS signals inevitably contain noise, and can harm the downstream performance if the noise is significant. There are two main strategies to reduce the noise effects: (1) Apply less strict margin-based loss such as the hinge loss (Dehghani et al., 2017;Xu et al., 2019) and MarginMSE loss (Hofstätter et al., 2020;, then models have fewer chances of overfitting to the exact labels, and (2) Apply noise-resistant training methods such as confidence-based filtering (Mukherjee and Awadallah, 2020;Yu et al., 2021b) and metalearning-based refinement (Ren et al., 2018;Zhu et al., 2022). Another potential issue is that the amount of training data in this section relies on the amount of questions we have. Unlike the document set which we can obtain for free, the question set takes time to collect and are often orders of magnitudes smaller. If no sufficient questions are available, we can use synthetic questions from question generation, then apply same WS signals in this section, which has been shown to perform on par with using real questions in certain domains Thakur et al., 2022).']","The three WS signals listed above work directly on actual questions instead of pseudo pairs as in §3 so that the NR model can adapt better to the target-domain question distribution. The bottleneck is the quality of the WS signals. SRs and PLMs are unsupervised, which could be more robust when we face a completely different domain (Dai et al., 2022). Otherwise, if we already have certain amounts relevance annotations from the target or similar domains, usually using a supervised teacher model is preferred. Nevertheless, these WS signals inevitably contain noise, and can harm the downstream performance if the noise is significant. There are two main strategies to reduce the noise effects: (1) Apply less strict margin-based loss such as the hinge loss (Dehghani et al., 2017;Xu et al., 2019) and MarginMSE loss (Hofstätter et al., 2020;, then models have fewer chances of overfitting to the exact labels, and (2) Apply noise-resistant training methods such as confidence-based filtering (Mukherjee and Awadallah, 2020;Yu et al., 2021b) and metalearning-based refinement (Ren et al., 2018;Zhu et al., 2022). Another potential issue is that the amount of training data in this section relies on the amount of questions we have. Unlike the document set which we can obtain for free, the question set takes time to collect and are often orders of magnitudes smaller. If no sufficient questions are available, we can use synthetic questions from question generation, then apply same WS signals in this section, which has been shown to perform on par with using real questions in certain domains Thakur et al., 2022).","(p8.0) The three WS signals listed above work directly on actual questions instead of pseudo pairs as in §3 so that the NR model can adapt better to the target-domain question distribution. The bottleneck is the quality of the WS signals. SRs and PLMs are unsupervised, which could be more robust when we face a completely different domain (Dai et al., 2022). Otherwise, if we already have certain amounts relevance annotations from the target or similar domains, usually using a supervised teacher model is preferred. Nevertheless, these WS signals inevitably contain noise, and can harm the downstream performance if the noise is significant. There are two main strategies to reduce the noise effects: (1) Apply less strict margin-based loss such as the hinge loss (Dehghani et al., 2017;Xu et al., 2019) and MarginMSE loss (Hofstätter et al., 2020;, then models have fewer chances of overfitting to the exact labels, and (2) Apply noise-resistant training methods such as confidence-based filtering (Mukherjee and Awadallah, 2020;Yu et al., 2021b) and metalearning-based refinement (Ren et al., 2018;Zhu et al., 2022). Another potential issue is that the amount of training data in this section relies on the amount of questions we have. Unlike the document set which we can obtain for free, the question set takes time to collect and are often orders of magnitudes smaller. If no sufficient questions are available, we can use synthetic questions from question generation, then apply same WS signals in this section, which has been shown to perform on par with using real questions in certain domains Thakur et al., 2022).","[['b21', 'b65', 'b56', 'b33', None, 'b68']]","[['b21', 'b65', 'b56', 'b33', None, 'b68']]",6,"1. The three WS signals listed above work directly on actual questions instead of pseudo pairs as in §3 so that the NR model can adapt better to the target-domain question distribution.
2. The bottleneck is the quality of the WS signals.
3. SRs and PLMs are unsupervised, which could be more robust when we face a completely different domain (Dai et al., 2022).
4. Otherwise, if we already have certain amounts relevance annotations from the target or similar domains, usually using a supervised teacher model is preferred.
5. Nevertheless, these WS signals inevitably contain noise, and can harm the downstream performance if the noise is significant.
6. There are two main strategies to reduce the noise effects: (1)
7. Apply less strict margin-based loss such as the hinge loss (Dehghani et al., 2017;Xu et al., 2019) and MarginMSE loss (Hofstätter et al., 2020;, then models have fewer chances of overfitting to the exact labels, and (2)
8. Apply noise-resistant training methods such as confidence-based filtering (Mukherjee and Awadallah, 2020;Yu et al., 2021b) and metalearning-based refinement (Ren et al., 2018;Zhu et al., 2022).
9. Another potential issue is that the amount of training data in this section relies on the amount of questions we have.
10. Unlike the document set which we can obtain for free, the question set takes time to collect and are often orders of magnitudes smaller.
11. If no sufficient questions are available, we can use synthetic questions from question generation, then apply same WS signals in this section, which has been shown to perform on par with using real questions in certain domains Thakur et al., 2022)."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Computer Science,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,s10,Answer as a Document,"['p10.0', 'p10.1', 'p10.2']","['As a straightforward way to leverage QA pairs, this method directly treats QA pairs as positive samples and does not distinguish between documents and answers (Lai et al., 2018). These QA pairs can provide direct WS signals to train the NR model:', 'where (q, a + ) ∈ Q × A are question-answer pairs in the target domain, a − 1∼n are sampled n negative answers and L is the standard ranking loss.', 'Though simple, this has been a common practice to ""warm up"" the NR model when no sufficient relevance annotations are available. For largesized models, this can be crucial to fully leverage the model capacity since we often have orders of magnitude more QA pairs than relevance annotations (Ni et al., 2021;Oguz et al., 2021). However, the style, structure and format differ between the document and the answer. The answer is a direct response to the question, and so it is easier to predict due to its strong semantic correlation with the question. Whereas the document can be implicit and may contain fewer obvious clues that can imply an answer; deep text understanding is required to predict the relevance between questions and documents Shen et al., 2022b). Therefore, this approach may be insufficient to reach satisfying results as a standalone method.']","As a straightforward way to leverage QA pairs, this method directly treats QA pairs as positive samples and does not distinguish between documents and answers (Lai et al., 2018). These QA pairs can provide direct WS signals to train the NR model:

where (q, a + ) ∈ Q × A are question-answer pairs in the target domain, a − 1∼n are sampled n negative answers and L is the standard ranking loss.

Though simple, this has been a common practice to ""warm up"" the NR model when no sufficient relevance annotations are available. For largesized models, this can be crucial to fully leverage the model capacity since we often have orders of magnitude more QA pairs than relevance annotations (Ni et al., 2021;Oguz et al., 2021). However, the style, structure and format differ between the document and the answer. The answer is a direct response to the question, and so it is easier to predict due to its strong semantic correlation with the question. Whereas the document can be implicit and may contain fewer obvious clues that can imply an answer; deep text understanding is required to predict the relevance between questions and documents Shen et al., 2022b). Therefore, this approach may be insufficient to reach satisfying results as a standalone method.","(p10.0) As a straightforward way to leverage QA pairs, this method directly treats QA pairs as positive samples and does not distinguish between documents and answers (Lai et al., 2018). These QA pairs can provide direct WS signals to train the NR model:

(p10.1) where (q, a + ) ∈ Q × A are question-answer pairs in the target domain, a − 1∼n are sampled n negative answers and L is the standard ranking loss.

(p10.2) Though simple, this has been a common practice to ""warm up"" the NR model when no sufficient relevance annotations are available. For largesized models, this can be crucial to fully leverage the model capacity since we often have orders of magnitude more QA pairs than relevance annotations (Ni et al., 2021;Oguz et al., 2021). However, the style, structure and format differ between the document and the answer. The answer is a direct response to the question, and so it is easier to predict due to its strong semantic correlation with the question. Whereas the document can be implicit and may contain fewer obvious clues that can imply an answer; deep text understanding is required to predict the relevance between questions and documents Shen et al., 2022b). Therefore, this approach may be insufficient to reach satisfying results as a standalone method.","[[None], [], ['b22', 'b44', 'b25']]","[[None], [], ['b22', 'b44', 'b25']]",4,"1. As a straightforward way to leverage QA pairs, this method directly treats QA pairs as positive samples and does not distinguish between documents and answers (Lai et al., 2018).
2. These QA pairs can provide direct WS signals to train the NR model:where (q, a + ) ∈ Q ×
3. A are question-answer pairs in the target domain, a − 1∼n are sampled n negative answers and L is the standard ranking loss.
4. Though simple, this has been a common practice to ""warm up"" the NR model when no sufficient relevance annotations are available.
5. For largesized models, this can be crucial to fully leverage the model capacity since we often have orders of magnitude more QA pairs than relevance annotations (Ni et al., 2021;Oguz et al., 2021).
6. However, the style, structure and format differ between the document and the answer.
7. The answer is a direct response to the question, and so it is easier to predict due to its strong semantic correlation with the question.
8. Whereas the document can be implicit and may contain fewer obvious clues that can imply an answer; deep text understanding is required to predict the relevance between questions and documents Shen et al., 2022b).
9. Therefore, this approach may be insufficient to reach satisfying results as a standalone method."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Computer Science,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,s12,Latent-Variable Model,"['p12.0', 'p12.1', 'p12.2', 'p12.3']","['We can still train the NR model on questiondocument pairs as in Answer-Document Mapping. However, instead of relying on a heuristic-based mapping function, we can treat this mapping as a ""latent variable"" within a probabilistic generative process (Lee et al., 2019;Shen, 2022). By this means, the NR model R gets WS signals from the QA reader G by maximizing the marginal likelihood:', 'where Z indicates all possible document combinations. Directly optimizing over Eq 6 is infeasible as it requires enumerating over all documents. A closed-form solution does not exist due to the deep neural network parameterization of R and G. The following section explains popular optimization options. An overview can be seen in Table 4.', 'Top-k approximation A popular approach is to assume a categorical distribution for R(Z|q); that is, to assume for each question only a single document is selected and the answer is generated from that one document. Eq 6 can be approximated by enumerating over only the top-k documents, assuming the remaining documents having negligibly small contributions to the likelihood:', 'This has been a popular choice in end-to-end training of text generation models (Lee et al., 2019;Shen et al., 2019b;Guu et al., 2020;Lewis et al., 2020;Shuster et al., 2021;Ferguson et al., 2022). Despite its simplicity, the top-k approximation has two main drawbacks. (1) The approximation is performed on the top-k documents obtained from the NR model. If the NR model is very weak at the beginning of training, these top-k documents can be a bad approximation to the real joint likelihood and the model might struggle to converge. (2) The assumption that document follow a categorical distribution might be problematic especially if the answer requires evidence from multiple documents (Wang and Pan, 2022).']","We can still train the NR model on questiondocument pairs as in Answer-Document Mapping. However, instead of relying on a heuristic-based mapping function, we can treat this mapping as a ""latent variable"" within a probabilistic generative process (Lee et al., 2019;Shen, 2022). By this means, the NR model R gets WS signals from the QA reader G by maximizing the marginal likelihood:

where Z indicates all possible document combinations. Directly optimizing over Eq 6 is infeasible as it requires enumerating over all documents. A closed-form solution does not exist due to the deep neural network parameterization of R and G. The following section explains popular optimization options. An overview can be seen in Table 4.

Top-k approximation A popular approach is to assume a categorical distribution for R(Z|q); that is, to assume for each question only a single document is selected and the answer is generated from that one document. Eq 6 can be approximated by enumerating over only the top-k documents, assuming the remaining documents having negligibly small contributions to the likelihood:

This has been a popular choice in end-to-end training of text generation models (Lee et al., 2019;Shen et al., 2019b;Guu et al., 2020;Lewis et al., 2020;Shuster et al., 2021;Ferguson et al., 2022). Despite its simplicity, the top-k approximation has two main drawbacks. (1) The approximation is performed on the top-k documents obtained from the NR model. If the NR model is very weak at the beginning of training, these top-k documents can be a bad approximation to the real joint likelihood and the model might struggle to converge. (2) The assumption that document follow a categorical distribution might be problematic especially if the answer requires evidence from multiple documents (Wang and Pan, 2022).","(p12.0) We can still train the NR model on questiondocument pairs as in Answer-Document Mapping. However, instead of relying on a heuristic-based mapping function, we can treat this mapping as a ""latent variable"" within a probabilistic generative process (Lee et al., 2019;Shen, 2022). By this means, the NR model R gets WS signals from the QA reader G by maximizing the marginal likelihood:

(p12.1) where Z indicates all possible document combinations. Directly optimizing over Eq 6 is infeasible as it requires enumerating over all documents. A closed-form solution does not exist due to the deep neural network parameterization of R and G. The following section explains popular optimization options. An overview can be seen in Table 4.

(p12.2) Top-k approximation A popular approach is to assume a categorical distribution for R(Z|q); that is, to assume for each question only a single document is selected and the answer is generated from that one document. Eq 6 can be approximated by enumerating over only the top-k documents, assuming the remaining documents having negligibly small contributions to the likelihood:

(p12.3) This has been a popular choice in end-to-end training of text generation models (Lee et al., 2019;Shen et al., 2019b;Guu et al., 2020;Lewis et al., 2020;Shuster et al., 2021;Ferguson et al., 2022). Despite its simplicity, the top-k approximation has two main drawbacks. (1) The approximation is performed on the top-k documents obtained from the NR model. If the NR model is very weak at the beginning of training, these top-k documents can be a bad approximation to the real joint likelihood and the model might struggle to converge. (2) The assumption that document follow a categorical distribution might be problematic especially if the answer requires evidence from multiple documents (Wang and Pan, 2022).","[[None, 'b42'], [], [], [None, 'b48', 'b50', 'b62']]","[[None, 'b42'], [], [], [None, 'b48', 'b50', 'b62']]",6,"1. We can still train the NR model on questiondocument pairs as in Answer-Document Mapping.
2. However, instead of relying on a heuristic-based mapping function, we can treat this mapping as a ""latent variable"" within a probabilistic generative process (Lee et al., 2019;Shen, 2022).
3. By this means, the NR model R gets WS signals from the QA reader G by maximizing the marginal likelihood:where Z indicates all possible document combinations.
4. Directly optimizing over Eq 6 is infeasible as it requires enumerating over all documents.
5. A closed-form solution does not exist due to the deep neural network parameterization of R and G.
6. The following section explains popular optimization options.
7. An overview can be seen in Table 4.Top-k approximation A popular approach is to assume a categorical distribution for R(Z|q); that is, to assume for each question only a single document is selected and the answer is generated from that one document.
8. Eq 6 can be approximated by enumerating over only the top-k documents, assuming the remaining documents having negligibly small contributions to the likelihood:This has been a popular choice in end-to-end training of text generation models (Lee et al., 2019;Shen et al., 2019b;Guu et al., 2020;Lewis et al., 2020;Shuster et al., 2021;Ferguson et al., 2022).
9. Despite its simplicity, the top-k approximation has two main drawbacks.
10. (1) The approximation is performed on the top-k documents obtained from the NR model.
11. If the NR model is very weak at the beginning of training, these top-k documents can be a bad approximation to the real joint likelihood and the model might struggle to converge.
12. (2) The assumption that document follow a categorical distribution might be problematic especially if the answer requires evidence from multiple documents (Wang and Pan, 2022)."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Computer Science,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,s13,Expectation-Maximization (EM) algorithm,"['p13.0', 'p13.1', 'p13.2', 'p13.3', 'p13.4', 'p13.5']","['To address the second drawback of the top-k approximation approach, we can assume a multinomial distribution for R(Z|q) so that an answer can be generated from multiple documents. The cost of this relaxation is the increased difficulty of optimization. Approximating the joint likelihood from top-k samples becomes infeasible due to the combinatorial distribution of document. Singh et al. (2021) propose optimizating it with the EM algorithm under an independent assumption about the posterior distribution of R(z|q):', 'where SG means stop-gradient (gradients are not backpropagated through G). As can be seen, the training signal for the NR model is essentially the same as in the Top-k Approximation case, except that the reader is trained by conditioning on all top-k documents to generate the answer. Singh et al. (2021) also find that Eq 7 is quite robust with respect to parameter initialization. Similarly,  apply the hard-EM algorithm to train the NR model, which only treats documents with the highest likelihood estimated by the reader as positive. Izacard et al. (2022) further experiment with using the leave-one-out perplexity from the reader to supervise the ranker.', 'Learning from attention Another way to optimize the NR model in Eq 6 is to leverage attention scores from the reader G. The assumption is that when training G to generate the answer, its attention score is a good approximation of question-document relevance. The training objective is:', 'where G is trained to generate the right answer based on the question and the top-k document, same as in the EM algorithm. A z is the attention score of G on the document z. L is the loss function to encourage the similarity between distributions of the attention scores and retrieving scores.', 'Izacard and Grave (2021) propose a training process that optimizes R and G iteratively. R is trained to minimize KL divergence between relevance and attention scores. (Lee et al., 2021) jointly optimize R and G and apply a stop-gradient operation on G when updating R. Sachan et al. (2021) use retriever scores to bias attention scores on the contrary. These can be considered as first-order Taylor series approximations of Eq. 6 by replacing R(Z|q) with attention scores (Deng et al., 2018).', 'Discussion Training with latent-variable models can perform close to fully supervised models under certain scenarios Sachan et al., 2021). The main challenge is the training difficulty. In practice, we can often initialize the NR model using the answer as document or answer-document mapping to make the training more stable. If not enough QA pairs are available, we can use heuristics like masked salient entities (Guu et al., 2020) to form pseudo pairs, then apply the same WS techniques in this section. Combining supervision signals from various various optimization techniques such as learning from attention and EM algorithm can also be beneficial (Izacard et al., 2022). If the independence assumption made by Eq 7 does not hold, we need to resort to more complex optimization algorithms. A potential direction is to apply a Dirichlet prior over R(z|q t ), which is a conjugate distribution to the multinomial distribution (Minka, 2000), with the result that the sampled document are not independent individuals but a combination set. Eq 6 can then be estimated by rejection sampling (Deng et al., 2018) or a Laplace approximation (Srivastava and Sutton, 2017) so as to avoid the independence assumption about the posterior distribution. Nonetheless, this will further increase the training complexity, which is already a key bottleneck for training the NR model.']","To address the second drawback of the top-k approximation approach, we can assume a multinomial distribution for R(Z|q) so that an answer can be generated from multiple documents. The cost of this relaxation is the increased difficulty of optimization. Approximating the joint likelihood from top-k samples becomes infeasible due to the combinatorial distribution of document. Singh et al. (2021) propose optimizating it with the EM algorithm under an independent assumption about the posterior distribution of R(z|q):

where SG means stop-gradient (gradients are not backpropagated through G). As can be seen, the training signal for the NR model is essentially the same as in the Top-k Approximation case, except that the reader is trained by conditioning on all top-k documents to generate the answer. Singh et al. (2021) also find that Eq 7 is quite robust with respect to parameter initialization. Similarly,  apply the hard-EM algorithm to train the NR model, which only treats documents with the highest likelihood estimated by the reader as positive. Izacard et al. (2022) further experiment with using the leave-one-out perplexity from the reader to supervise the ranker.

Learning from attention Another way to optimize the NR model in Eq 6 is to leverage attention scores from the reader G. The assumption is that when training G to generate the answer, its attention score is a good approximation of question-document relevance. The training objective is:

where G is trained to generate the right answer based on the question and the top-k document, same as in the EM algorithm. A z is the attention score of G on the document z. L is the loss function to encourage the similarity between distributions of the attention scores and retrieving scores.

Izacard and Grave (2021) propose a training process that optimizes R and G iteratively. R is trained to minimize KL divergence between relevance and attention scores. (Lee et al., 2021) jointly optimize R and G and apply a stop-gradient operation on G when updating R. Sachan et al. (2021) use retriever scores to bias attention scores on the contrary. These can be considered as first-order Taylor series approximations of Eq. 6 by replacing R(Z|q) with attention scores (Deng et al., 2018).

Discussion Training with latent-variable models can perform close to fully supervised models under certain scenarios Sachan et al., 2021). The main challenge is the training difficulty. In practice, we can often initialize the NR model using the answer as document or answer-document mapping to make the training more stable. If not enough QA pairs are available, we can use heuristics like masked salient entities (Guu et al., 2020) to form pseudo pairs, then apply the same WS techniques in this section. Combining supervision signals from various various optimization techniques such as learning from attention and EM algorithm can also be beneficial (Izacard et al., 2022). If the independence assumption made by Eq 7 does not hold, we need to resort to more complex optimization algorithms. A potential direction is to apply a Dirichlet prior over R(z|q t ), which is a conjugate distribution to the multinomial distribution (Minka, 2000), with the result that the sampled document are not independent individuals but a combination set. Eq 6 can then be estimated by rejection sampling (Deng et al., 2018) or a Laplace approximation (Srivastava and Sutton, 2017) so as to avoid the independence assumption about the posterior distribution. Nonetheless, this will further increase the training complexity, which is already a key bottleneck for training the NR model.","(p13.0) To address the second drawback of the top-k approximation approach, we can assume a multinomial distribution for R(Z|q) so that an answer can be generated from multiple documents. The cost of this relaxation is the increased difficulty of optimization. Approximating the joint likelihood from top-k samples becomes infeasible due to the combinatorial distribution of document. Singh et al. (2021) propose optimizating it with the EM algorithm under an independent assumption about the posterior distribution of R(z|q):

(p13.1) where SG means stop-gradient (gradients are not backpropagated through G). As can be seen, the training signal for the NR model is essentially the same as in the Top-k Approximation case, except that the reader is trained by conditioning on all top-k documents to generate the answer. Singh et al. (2021) also find that Eq 7 is quite robust with respect to parameter initialization. Similarly,  apply the hard-EM algorithm to train the NR model, which only treats documents with the highest likelihood estimated by the reader as positive. Izacard et al. (2022) further experiment with using the leave-one-out perplexity from the reader to supervise the ranker.

(p13.2) Learning from attention Another way to optimize the NR model in Eq 6 is to leverage attention scores from the reader G. The assumption is that when training G to generate the answer, its attention score is a good approximation of question-document relevance. The training objective is:

(p13.3) where G is trained to generate the right answer based on the question and the top-k document, same as in the EM algorithm. A z is the attention score of G on the document z. L is the loss function to encourage the similarity between distributions of the attention scores and retrieving scores.

(p13.4) Izacard and Grave (2021) propose a training process that optimizes R and G iteratively. R is trained to minimize KL divergence between relevance and attention scores. (Lee et al., 2021) jointly optimize R and G and apply a stop-gradient operation on G when updating R. Sachan et al. (2021) use retriever scores to bias attention scores on the contrary. These can be considered as first-order Taylor series approximations of Eq. 6 by replacing R(Z|q) with attention scores (Deng et al., 2018).

(p13.5) Discussion Training with latent-variable models can perform close to fully supervised models under certain scenarios Sachan et al., 2021). The main challenge is the training difficulty. In practice, we can often initialize the NR model using the answer as document or answer-document mapping to make the training more stable. If not enough QA pairs are available, we can use heuristics like masked salient entities (Guu et al., 2020) to form pseudo pairs, then apply the same WS techniques in this section. Combining supervision signals from various various optimization techniques such as learning from attention and EM algorithm can also be beneficial (Izacard et al., 2022). If the independence assumption made by Eq 7 does not hold, we need to resort to more complex optimization algorithms. A potential direction is to apply a Dirichlet prior over R(z|q t ), which is a conjugate distribution to the multinomial distribution (Minka, 2000), with the result that the sampled document are not independent individuals but a combination set. Eq 6 can then be estimated by rejection sampling (Deng et al., 2018) or a Laplace approximation (Srivastava and Sutton, 2017) so as to avoid the independence assumption about the posterior distribution. Nonetheless, this will further increase the training complexity, which is already a key bottleneck for training the NR model.","[['b51'], [None, 'b51'], [], [], [None, 'b39'], [None, 'b17', 'b54', 'b39']]","[['b51'], [None, 'b51'], [], [], [None, 'b39'], [None, 'b17', 'b54', 'b39']]",9,"1. To address the second drawback of the top-k approximation approach, we can assume a multinomial distribution for R(Z|q) so that an answer can be generated from multiple documents.
2. The cost of this relaxation is the increased difficulty of optimization.
3. Approximating the joint likelihood from top-k samples becomes infeasible due to the combinatorial distribution of document.
4. Singh et al. (2021) propose optimizating it with the EM algorithm under an independent assumption about the posterior distribution of R(z|q):where SG means stop-gradient (gradients are not backpropagated through G).
5. As can be seen, the training signal for the NR model is essentially the same as in the Top-k Approximation case, except that the reader is trained by conditioning on all top-k documents to generate the answer.
6. Singh et al. (2021) also find that Eq 7 is quite robust with respect to parameter initialization.
7. Similarly,  apply the hard-EM algorithm to train the NR model, which only treats documents with the highest likelihood estimated by the reader as positive.
8. Izacard et al. (2022) further experiment with using the leave-one-out perplexity from the reader to supervise the ranker.
9. Learning from attention Another way to optimize the NR model in Eq 6 is to leverage attention scores from the reader G. The assumption is that when training G to generate the answer, its attention score is a good approximation of question-document relevance.
10. The training objective is:where G is trained to generate the right answer based on the question and the top-k document, same as in the EM algorithm.
11. A z is the attention score of G on the document z. L is the loss function to encourage the similarity between distributions of the attention scores and retrieving scores.
12. Izacard and Grave (2021) propose a training process that optimizes R and G iteratively.
13. R is trained to minimize KL divergence between relevance and attention scores.
14. (Lee et al., 2021) jointly optimize R and G and apply a stop-gradient operation on G when updating R. Sachan et al. (2021) use retriever scores to bias attention scores on the contrary.
15. These can be considered as first-order Taylor series approximations of Eq. 6 by replacing R(Z|q) with attention scores (Deng et al., 2018).Discussion Training with latent-variable models can perform close to fully supervised models under certain scenarios Sachan et al., 2021).
16. The main challenge is the training difficulty.
17. In practice, we can often initialize the NR model using the answer as document or answer-document mapping to make the training more stable.
18. If not enough QA pairs are available, we can use heuristics like masked salient entities (Guu et al., 2020) to form pseudo pairs, then apply the same WS techniques in this section.
19. Combining supervision signals from various various optimization techniques such as learning from attention and EM algorithm can also be beneficial (Izacard et al., 2022).
20. If the independence assumption made by Eq 7 does not hold, we need to resort to more complex optimization algorithms.
21. A potential direction is to apply a Dirichlet prior over R(z|q t ), which is a conjugate distribution to the multinomial distribution (Minka, 2000), with the result that the sampled document are not independent individuals but a combination set.
22. Eq 6 can then be estimated by rejection sampling (Deng et al., 2018) or a Laplace approximation (Srivastava and Sutton, 2017) so as to avoid the independence assumption about the posterior distribution.
23. Nonetheless, this will further increase the training complexity, which is already a key bottleneck for training the NR model."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Computer Science,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,s5,Choices of Filter,['p5.0'],"['Filtering is a crucial part of QG since a significant portion of generated questions could be of low quality and would provide misleading signals when used to train the NR model (Alberti et al., 2019). A typical choice is filtering based on round-trip consistency (Alberti et al., 2019;Dong et al., 2019), where a pre-trained QA system is applied to produce an answer based on the generated question. A question is kept only when the produced answer is consistent with the answer from which the question is generated. We can also relax this strict consistency requirement and manually adjust an acceptance threshold based on the probability from the pre-trained QA system (Zhang and Bansal, 2019;Lewis et al., 2021), LM score from the generator itself (Shakeri et al., 2020;Liang et al., 2020), or an entailment score from a model trained on question-context-answer pairs . Influence functions (Cook and Weisberg, 1982) can be used to estimate the effect on the validation loss of including a synthetic example (Yang et al., 2020), but this does not achieve satisfying performances on QA tasks (Bartolo et al., 2021). Bartolo et al. (2021) propose filtering questions based on ensemble consistency, where an ensemble of QA models are trained with different random seeds and only questions agreed by most QA models are selected. When minimal target-domain annotation is available, we can also learn to reweight pseudo samples based on the validation loss , or use RL to select samples that lead to validation performance gains (value estimation) (Yue et al., 2022b).']","Filtering is a crucial part of QG since a significant portion of generated questions could be of low quality and would provide misleading signals when used to train the NR model (Alberti et al., 2019). A typical choice is filtering based on round-trip consistency (Alberti et al., 2019;Dong et al., 2019), where a pre-trained QA system is applied to produce an answer based on the generated question. A question is kept only when the produced answer is consistent with the answer from which the question is generated. We can also relax this strict consistency requirement and manually adjust an acceptance threshold based on the probability from the pre-trained QA system (Zhang and Bansal, 2019;Lewis et al., 2021), LM score from the generator itself (Shakeri et al., 2020;Liang et al., 2020), or an entailment score from a model trained on question-context-answer pairs . Influence functions (Cook and Weisberg, 1982) can be used to estimate the effect on the validation loss of including a synthetic example (Yang et al., 2020), but this does not achieve satisfying performances on QA tasks (Bartolo et al., 2021). Bartolo et al. (2021) propose filtering questions based on ensemble consistency, where an ensemble of QA models are trained with different random seeds and only questions agreed by most QA models are selected. When minimal target-domain annotation is available, we can also learn to reweight pseudo samples based on the validation loss , or use RL to select samples that lead to validation performance gains (value estimation) (Yue et al., 2022b).","(p5.0) Filtering is a crucial part of QG since a significant portion of generated questions could be of low quality and would provide misleading signals when used to train the NR model (Alberti et al., 2019). A typical choice is filtering based on round-trip consistency (Alberti et al., 2019;Dong et al., 2019), where a pre-trained QA system is applied to produce an answer based on the generated question. A question is kept only when the produced answer is consistent with the answer from which the question is generated. We can also relax this strict consistency requirement and manually adjust an acceptance threshold based on the probability from the pre-trained QA system (Zhang and Bansal, 2019;Lewis et al., 2021), LM score from the generator itself (Shakeri et al., 2020;Liang et al., 2020), or an entailment score from a model trained on question-context-answer pairs . Influence functions (Cook and Weisberg, 1982) can be used to estimate the effect on the validation loss of including a synthetic example (Yang et al., 2020), but this does not achieve satisfying performances on QA tasks (Bartolo et al., 2021). Bartolo et al. (2021) propose filtering questions based on ensemble consistency, where an ensemble of QA models are trained with different random seeds and only questions agreed by most QA models are selected. When minimal target-domain annotation is available, we can also learn to reweight pseudo samples based on the validation loss , or use RL to select samples that lead to validation performance gains (value estimation) (Yue et al., 2022b).","[['b77', 'b66', None, 'b25', 'b1', 'b71']]","[['b77', 'b66', None, 'b25', 'b1', 'b71']]",6,"1. Filtering is a crucial part of QG since a significant portion of generated questions could be of low quality and would provide misleading signals when used to train the NR model (Alberti et al., 2019).
2. A typical choice is filtering based on round-trip consistency (Alberti et al., 2019;Dong et al., 2019), where a pre-trained QA system is applied to produce an answer based on the generated question.
3. A question is kept only when the produced answer is consistent with the answer from which the question is generated.
4. We can also relax this strict consistency requirement and manually adjust an acceptance threshold based on the probability from the pre-trained QA system (Zhang and Bansal, 2019;Lewis et al., 2021), LM score from the generator itself (Shakeri et al., 2020;Liang et al., 2020), or an entailment score from a model trained on question-context-answer pairs .
5. Influence functions (Cook and Weisberg, 1982) can be used to estimate the effect on the validation loss of including a synthetic example (Yang et al., 2020), but this does not achieve satisfying performances on QA tasks (Bartolo et al., 2021).
6. Bartolo et al. (2021) propose filtering questions based on ensemble consistency, where an ensemble of QA models are trained with different random seeds and only questions agreed by most QA models are selected.
7. When minimal target-domain annotation is available, we can also learn to reweight pseudo samples based on the validation loss , or use RL to select samples that lead to validation performance gains (value estimation)
8. (Yue et al., 2022b)."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Computer Science,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,s3,Self Contrastive Learning,"['p3.0', 'p3.1', 'p3.2', 'p3.3', 'p3.4', 'p3.5', 'p3.6']","['Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q ′ , d ′+/− ) from D, then uses them to supervise training of a NR model. The objective is:', 'where L is the ranking loss as in Eq 1. Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q ′ , d ′+ ). There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based. An overview is in Table 2.', 'Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair. The intuition is that perturbed text should still be relevant to the original text. Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).', 'Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other. The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document. They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness. Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.', 'Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021). For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus. A term from it is treated as the answer and replaced with a special token. Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents. Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.', 'Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant . For example, Chang et al.', '(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic. A passage from another page containing hyperlinks to p is treated as a positive document. Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question. A passage from its hyperlinked document that contains the same entity word is treated as a positive sample. Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.  ']","Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q ′ , d ′+/− ) from D, then uses them to supervise training of a NR model. The objective is:

where L is the ranking loss as in Eq 1. Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q ′ , d ′+ ). There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based. An overview is in Table 2.

Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair. The intuition is that perturbed text should still be relevant to the original text. Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).

Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other. The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document. They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness. Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.

Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021). For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus. A term from it is treated as the answer and replaced with a special token. Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents. Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.

Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant . For example, Chang et al.

(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic. A passage from another page containing hyperlinks to p is treated as a positive document. Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question. A passage from its hyperlinked document that contains the same entity word is treated as a positive sample. Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.  ","(p3.0) Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q ′ , d ′+/− ) from D, then uses them to supervise training of a NR model. The objective is:

(p3.1) where L is the ranking loss as in Eq 1. Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q ′ , d ′+ ). There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based. An overview is in Table 2.

(p3.2) Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair. The intuition is that perturbed text should still be relevant to the original text. Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).

(p3.3) Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other. The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document. They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness. Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.

(p3.4) Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021). For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus. A term from it is treated as the answer and replaced with a special token. Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents. Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.

(p3.5) Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant . For example, Chang et al.

(p3.6) (2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic. A passage from another page containing hyperlinks to p is treated as a positive document. Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question. A passage from its hyperlinked document that contains the same entity word is treated as a positive sample. Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.  ","[[], [], ['b10', 'b16', 'b83'], [None, 'b64', 'b74'], ['b31', 'b30'], [], ['b70', 'b80']]","[[], [], ['b10', 'b16', 'b83'], [None, 'b64', 'b74'], ['b31', 'b30'], [], ['b70', 'b80']]",10,"1. Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q ′ , d ′+/− ) from D, then uses them to supervise training of a NR model.
2. The objective is:where L is the ranking loss as in Eq 1.
3. Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q ′ , d ′+ ).
4. There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based.
5. An overview is in Table 2.Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair.
6. The intuition is that perturbed text should still be relevant to the original text.
7. Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).
8. Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other.
9. The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document.
10. They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness.
11. Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021).
12. For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus.
13. A term from it is treated as the answer and replaced with a special token.
14. Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents.
15. Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.
16. Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant .
17. For example, Chang et al.(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic.
18. A passage from another page containing hyperlinks to p is treated as a positive document.
19. Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question.
20. A passage from its hyperlinked document that contains the same entity word is treated as a positive sample.
21. Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Computer Science,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,s15,Limitations,['p15.0'],"['This survey covers introductions and related work of major WS algorithms used for neural ranking. Due to the space limit, most methods included in this paper are brief. Readers might not have a good understand on all the introduced methods. Interested readers can refer to existing surveys about general knowledge in QA (Zeng et al., 2020;Zhu et al., 2021a;Roy and Anand, 2021;Rogers et al., 2021;Pandya and Bhatt, 2021). Furthermore, we did not provide points to existing ODQA datasets and the performance of recent models. The conclusions in this survey also come from summaries of previous works. The lack of datasets including various resources needed for different WS algorithms prevents a comprehensive, fair comparison across algorithms. We hope future research can work on the creation of more datasets with various availabilitis of resources in different domains to enable this comparison. Lastly, we aim to create a big picture from the technology level, so we did not strictly limit our references only to the application of ODQA. The connection to specific ODQA applications might be loose, readers would need to extract useful information for the specific use cases. ']","This survey covers introductions and related work of major WS algorithms used for neural ranking. Due to the space limit, most methods included in this paper are brief. Readers might not have a good understand on all the introduced methods. Interested readers can refer to existing surveys about general knowledge in QA (Zeng et al., 2020;Zhu et al., 2021a;Roy and Anand, 2021;Rogers et al., 2021;Pandya and Bhatt, 2021). Furthermore, we did not provide points to existing ODQA datasets and the performance of recent models. The conclusions in this survey also come from summaries of previous works. The lack of datasets including various resources needed for different WS algorithms prevents a comprehensive, fair comparison across algorithms. We hope future research can work on the creation of more datasets with various availabilitis of resources in different domains to enable this comparison. Lastly, we aim to create a big picture from the technology level, so we did not strictly limit our references only to the application of ODQA. The connection to specific ODQA applications might be loose, readers would need to extract useful information for the specific use cases. ","(p15.0) This survey covers introductions and related work of major WS algorithms used for neural ranking. Due to the space limit, most methods included in this paper are brief. Readers might not have a good understand on all the introduced methods. Interested readers can refer to existing surveys about general knowledge in QA (Zeng et al., 2020;Zhu et al., 2021a;Roy and Anand, 2021;Rogers et al., 2021;Pandya and Bhatt, 2021). Furthermore, we did not provide points to existing ODQA datasets and the performance of recent models. The conclusions in this survey also come from summaries of previous works. The lack of datasets including various resources needed for different WS algorithms prevents a comprehensive, fair comparison across algorithms. We hope future research can work on the creation of more datasets with various availabilitis of resources in different domains to enable this comparison. Lastly, we aim to create a big picture from the technology level, so we did not strictly limit our references only to the application of ODQA. The connection to specific ODQA applications might be loose, readers would need to extract useful information for the specific use cases. ","[['b36', 'b72', None, 'b38', 'b27']]","[['b36', 'b72', None, 'b38', 'b27']]",5,"1. This survey covers introductions and related work of major WS algorithms used for neural ranking.
2. Due to the space limit, most methods included in this paper are brief.
3. Readers might not have a good understand on all the introduced methods.
4. Interested readers can refer to existing surveys about general knowledge in QA (Zeng et al., 2020;Zhu et al., 2021a;Roy and Anand, 2021;Rogers et al., 2021;Pandya and Bhatt, 2021).
5. Furthermore, we did not provide points to existing ODQA datasets and the performance of recent models.
6. The conclusions in this survey also come from summaries of previous works.
7. The lack of datasets including various resources needed for different WS algorithms prevents a comprehensive, fair comparison across algorithms.
8. We hope future research can work on the creation of more datasets with various availabilitis of resources in different domains to enable this comparison.
9. Lastly, we aim to create a big picture from the technology level, so we did not strictly limit our references only to the application of ODQA.
10. The connection to specific ODQA applications might be loose, readers would need to extract useful information for the specific use cases."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Computer Science,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,s4,Question Generation,"['p4.0', 'p4.1', 'p4.2', 'p4.3', 'p4.4']","['Self contrastive learning relies on sentences already present in D. Question generation leverage a question generator (QG) to generate new questions not found in D, which can then be used to provide WS signals for the NR model. It often employs a filter F il to filter poorly generated questions. The training objective is:', 'where the expectation is with respect to documents d + and d − 1∼n drawn from D, the q are generated from d + , F il(q, d + ) = 0 requires that these questions not be discarded by F il, and L is the standard ranking loss. There are various ways of designing the question generator and filter. We will cover the popular choices in the following section. An overview can be seen in Table 3.', 'Choices of Input A variety of information can be provided as input for the QG. The most straightforward approach is answer-agnostic which provides only the document (Du and Cardie, 2017;Kumar et al., 2019). In this way, the model can choose to attend to different spans of the document as potential answers and so generate different, corresponding questions. A more common method is answer-aware where an answer span is first extracted from a document, then the QG generates a question based on both the document and answer (Alberti et al., 2019;Shakeri et al., 2020). Finer-grained information can also be provided such as the question type (""what/how/..."") (Cao and Wang, 2021;Gao et al., 2022) as well as additional clues (such as document context to disambiguate the question) . Adding more information reduces the entropy of the question and makes it easier for the model to learn, but also increases the possibility of error propagation (Zhang and Bansal, 2019). In practice, well-defined filters should be applied to remove low-quality questions.', 'Choices of Question Generator There are three popular choices for the question generator. (1) Rule-based methods (Pandey and Rajeswari, 2013;Rakangor and Ghodasara, 2015) rely on handcrafted templates and features. These are timeconsuming to design, domain-specific, and can only cover certain forms of questions.', '(2) Promptbased methods relying on pre-trained language models (PLMs). Documents can be presented to a PLM, with an appended prompt such as ""Please write a question based on this passage"" so that the PLM can continue the generation to produce a question (Bonifacio et al., 2022;Sachan et al., 2022;Dai et al., 2022). (3) Fine-tuned generators that are trained on annotated question-document pairs. When in-domain annotations are not enough, we can leverage out-of-domain (OOD) annotations, if any, to fine-tune the QG. The first two QGs require no training data, but their quality is often inadequate. In practice, we should only consider them when there is a complete lack of high-quality supervised data for fine-tuning the QG. When targetdomain questions are available, we can also apply semi-supervised techniques such as back-training to adapt the QG to the target domain (Zhao et al., 2019;Kulshreshtha et al., 2021;Shen et al., 2022a).']","Self contrastive learning relies on sentences already present in D. Question generation leverage a question generator (QG) to generate new questions not found in D, which can then be used to provide WS signals for the NR model. It often employs a filter F il to filter poorly generated questions. The training objective is:

where the expectation is with respect to documents d + and d − 1∼n drawn from D, the q are generated from d + , F il(q, d + ) = 0 requires that these questions not be discarded by F il, and L is the standard ranking loss. There are various ways of designing the question generator and filter. We will cover the popular choices in the following section. An overview can be seen in Table 3.

Choices of Input A variety of information can be provided as input for the QG. The most straightforward approach is answer-agnostic which provides only the document (Du and Cardie, 2017;Kumar et al., 2019). In this way, the model can choose to attend to different spans of the document as potential answers and so generate different, corresponding questions. A more common method is answer-aware where an answer span is first extracted from a document, then the QG generates a question based on both the document and answer (Alberti et al., 2019;Shakeri et al., 2020). Finer-grained information can also be provided such as the question type (""what/how/..."") (Cao and Wang, 2021;Gao et al., 2022) as well as additional clues (such as document context to disambiguate the question) . Adding more information reduces the entropy of the question and makes it easier for the model to learn, but also increases the possibility of error propagation (Zhang and Bansal, 2019). In practice, well-defined filters should be applied to remove low-quality questions.

Choices of Question Generator There are three popular choices for the question generator. (1) Rule-based methods (Pandey and Rajeswari, 2013;Rakangor and Ghodasara, 2015) rely on handcrafted templates and features. These are timeconsuming to design, domain-specific, and can only cover certain forms of questions.

(2) Promptbased methods relying on pre-trained language models (PLMs). Documents can be presented to a PLM, with an appended prompt such as ""Please write a question based on this passage"" so that the PLM can continue the generation to produce a question (Bonifacio et al., 2022;Sachan et al., 2022;Dai et al., 2022). (3) Fine-tuned generators that are trained on annotated question-document pairs. When in-domain annotations are not enough, we can leverage out-of-domain (OOD) annotations, if any, to fine-tune the QG. The first two QGs require no training data, but their quality is often inadequate. In practice, we should only consider them when there is a complete lack of high-quality supervised data for fine-tuning the QG. When targetdomain questions are available, we can also apply semi-supervised techniques such as back-training to adapt the QG to the target domain (Zhao et al., 2019;Kulshreshtha et al., 2021;Shen et al., 2022a).","(p4.0) Self contrastive learning relies on sentences already present in D. Question generation leverage a question generator (QG) to generate new questions not found in D, which can then be used to provide WS signals for the NR model. It often employs a filter F il to filter poorly generated questions. The training objective is:

(p4.1) where the expectation is with respect to documents d + and d − 1∼n drawn from D, the q are generated from d + , F il(q, d + ) = 0 requires that these questions not be discarded by F il, and L is the standard ranking loss. There are various ways of designing the question generator and filter. We will cover the popular choices in the following section. An overview can be seen in Table 3.

(p4.2) Choices of Input A variety of information can be provided as input for the QG. The most straightforward approach is answer-agnostic which provides only the document (Du and Cardie, 2017;Kumar et al., 2019). In this way, the model can choose to attend to different spans of the document as potential answers and so generate different, corresponding questions. A more common method is answer-aware where an answer span is first extracted from a document, then the QG generates a question based on both the document and answer (Alberti et al., 2019;Shakeri et al., 2020). Finer-grained information can also be provided such as the question type (""what/how/..."") (Cao and Wang, 2021;Gao et al., 2022) as well as additional clues (such as document context to disambiguate the question) . Adding more information reduces the entropy of the question and makes it easier for the model to learn, but also increases the possibility of error propagation (Zhang and Bansal, 2019). In practice, well-defined filters should be applied to remove low-quality questions.

(p4.3) Choices of Question Generator There are three popular choices for the question generator. (1) Rule-based methods (Pandey and Rajeswari, 2013;Rakangor and Ghodasara, 2015) rely on handcrafted templates and features. These are timeconsuming to design, domain-specific, and can only cover certain forms of questions.

(p4.4) (2) Promptbased methods relying on pre-trained language models (PLMs). Documents can be presented to a PLM, with an appended prompt such as ""Please write a question based on this passage"" so that the PLM can continue the generation to produce a question (Bonifacio et al., 2022;Sachan et al., 2022;Dai et al., 2022). (3) Fine-tuned generators that are trained on annotated question-document pairs. When in-domain annotations are not enough, we can leverage out-of-domain (OOD) annotations, if any, to fine-tune the QG. The first two QGs require no training data, but their quality is often inadequate. In practice, we should only consider them when there is a complete lack of high-quality supervised data for fine-tuning the QG. When targetdomain questions are available, we can also apply semi-supervised techniques such as back-training to adapt the QG to the target domain (Zhao et al., 2019;Kulshreshtha et al., 2021;Shen et al., 2022a).","[[], [], ['b77', None, 'b75', 'b1'], ['b29', 'b26'], [None, 'b79', 'b40', 'b43']]","[[], [], ['b77', None, 'b75', 'b1'], ['b29', 'b26'], [None, 'b79', 'b40', 'b43']]",10,"1. Self contrastive learning relies on sentences already present in D. Question generation leverage a question generator (QG) to generate new questions not found in D, which can then be used to provide WS signals for the NR model.
2. It often employs a filter F il to filter poorly generated questions.
3. The training objective is:where the expectation is with respect to documents d + and d
4. − 1∼n drawn from D, the q are generated from d + , F il(q, d + ) = 0 requires that these questions not be discarded by F il, and L is the standard ranking loss.
5. There are various ways of designing the question generator and filter.
6. We will cover the popular choices in the following section.
7. An overview can be seen in Table 3.
8. Choices of Input A variety of information can be provided as input for the QG.
9. The most straightforward approach is answer-agnostic which provides only the document (Du and Cardie, 2017;Kumar et al., 2019).
10. In this way, the model can choose to attend to different spans of the document as potential answers and so generate different, corresponding questions.
11. A more common method is answer-aware where an answer span is first extracted from a document, then the QG generates a question based on both the document and answer (Alberti et al., 2019;Shakeri et al., 2020).
12. Finer-grained information can also be provided such as the question type (""what/how/..."") (Cao and Wang, 2021;Gao et al., 2022) as well as additional clues (such as document context to disambiguate the question) .
13. Adding more information reduces the entropy of the question and makes it easier for the model to learn, but also increases the possibility of error propagation (Zhang and Bansal, 2019).
14. In practice, well-defined filters should be applied to remove low-quality questions.
15. Choices of Question Generator There are three popular choices for the question generator.
16. (1) Rule-based methods (Pandey and Rajeswari, 2013;Rakangor and Ghodasara, 2015) rely on handcrafted templates and features.
17. These are timeconsuming to design, domain-specific, and can only cover certain forms of questions.
18. (2) Promptbased methods relying on pre-trained language models (PLMs).
19. Documents can be presented to a PLM, with an appended prompt such as ""Please write a question based on this passage"" so that the PLM can continue the generation to produce a question (Bonifacio et al., 2022;Sachan et al., 2022;Dai et al., 2022).
20. (3) Fine-tuned generators that are trained on annotated question-document pairs.
21. When in-domain annotations are not enough, we can leverage out-of-domain (OOD) annotations, if any, to fine-tune the QG.
22. The first two QGs require no training data, but their quality is often inadequate.
23. In practice, we should only consider them when there is a complete lack of high-quality supervised data for fine-tuning the QG.
24. When targetdomain questions are available, we can also apply semi-supervised techniques such as back-training to adapt the QG to the target domain (Zhao et al., 2019;Kulshreshtha et al., 2021;Shen et al., 2022a)."
258426970,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef,s14,Decoding with Feedback Models,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4']","['As mentioned, feedback models have the advantage that they can be queried cheaply for feedback once trained. Perhaps for this reason, most approaches that leverage feedback models by sampling a large number of candidate generations, and reranking them according to the feedback model:', 'whereĥ ϕ is a trained (numerical) feedback model and C is a set of S candidate generations given specific regularization terms, such as the KL terms in PPO (Schulman et al., 2017). by the model (for example, by sampling from its distribution multiple times).', 'In machine translation, Fernandes et al. (2022) andFreitag et al. (2022a) build upon recent advances in automatic quality estimation and evaluation via feedback model training to improve generation. Their framework comprises a candidate generation stage followed by a ranking stage, in which the candidates are scored using quality metrics trained to regress on human assessments (reward models) (Rei et al., 2020a,b) via N -best list reranking or minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2002). The highest-scoring candidate is then chosen as the final translation. Li et al. (2022) collected a dataset of both numerical and natural language feedback for responses from a QA system, and finetuned a pretrained model to predict both kinds of feedback, using the predicted scores from this feedback model to re-rank the predictions from the model. Gao et al. (2022) also used this approach to study the scaling properties of feedback models and the problem of ""overoptimization"" (see below).', 'Additionally, there are several works combining MT and APE systems at decoding time, in which the output of an MT system is further improved by an APE system (Bhattacharyya et al., 2022).', 'Feedback Model Overoptimization One problem that arises when optimizing a system with a feedback model is that this model is only an imperfect proxy for the ground truth human feedback, therefore, ""overoptimizing"" for them can lead to systems that receive good feedback from the model, but not humans. This problem is known as the overoptimization problem, and is the main reason for the regularization term in Equation 11 Gao et al. (2022) studies the overoptimization problem in preference models, by both optimizing against it with reinforcement learning (training) and reranking outputs with it (decoding). They found that both using preference models during training or decoding led to similar levels of overoptimization, and that the scale of the generation model helps little with this problem.']","As mentioned, feedback models have the advantage that they can be queried cheaply for feedback once trained. Perhaps for this reason, most approaches that leverage feedback models by sampling a large number of candidate generations, and reranking them according to the feedback model:

whereĥ ϕ is a trained (numerical) feedback model and C is a set of S candidate generations given specific regularization terms, such as the KL terms in PPO (Schulman et al., 2017). by the model (for example, by sampling from its distribution multiple times).

In machine translation, Fernandes et al. (2022) andFreitag et al. (2022a) build upon recent advances in automatic quality estimation and evaluation via feedback model training to improve generation. Their framework comprises a candidate generation stage followed by a ranking stage, in which the candidates are scored using quality metrics trained to regress on human assessments (reward models) (Rei et al., 2020a,b) via N -best list reranking or minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2002). The highest-scoring candidate is then chosen as the final translation. Li et al. (2022) collected a dataset of both numerical and natural language feedback for responses from a QA system, and finetuned a pretrained model to predict both kinds of feedback, using the predicted scores from this feedback model to re-rank the predictions from the model. Gao et al. (2022) also used this approach to study the scaling properties of feedback models and the problem of ""overoptimization"" (see below).

Additionally, there are several works combining MT and APE systems at decoding time, in which the output of an MT system is further improved by an APE system (Bhattacharyya et al., 2022).

Feedback Model Overoptimization One problem that arises when optimizing a system with a feedback model is that this model is only an imperfect proxy for the ground truth human feedback, therefore, ""overoptimizing"" for them can lead to systems that receive good feedback from the model, but not humans. This problem is known as the overoptimization problem, and is the main reason for the regularization term in Equation 11 Gao et al. (2022) studies the overoptimization problem in preference models, by both optimizing against it with reinforcement learning (training) and reranking outputs with it (decoding). They found that both using preference models during training or decoding led to similar levels of overoptimization, and that the scale of the generation model helps little with this problem.","(p14.0) As mentioned, feedback models have the advantage that they can be queried cheaply for feedback once trained. Perhaps for this reason, most approaches that leverage feedback models by sampling a large number of candidate generations, and reranking them according to the feedback model:

(p14.1) whereĥ ϕ is a trained (numerical) feedback model and C is a set of S candidate generations given specific regularization terms, such as the KL terms in PPO (Schulman et al., 2017). by the model (for example, by sampling from its distribution multiple times).

(p14.2) In machine translation, Fernandes et al. (2022) andFreitag et al. (2022a) build upon recent advances in automatic quality estimation and evaluation via feedback model training to improve generation. Their framework comprises a candidate generation stage followed by a ranking stage, in which the candidates are scored using quality metrics trained to regress on human assessments (reward models) (Rei et al., 2020a,b) via N -best list reranking or minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2002). The highest-scoring candidate is then chosen as the final translation. Li et al. (2022) collected a dataset of both numerical and natural language feedback for responses from a QA system, and finetuned a pretrained model to predict both kinds of feedback, using the predicted scores from this feedback model to re-rank the predictions from the model. Gao et al. (2022) also used this approach to study the scaling properties of feedback models and the problem of ""overoptimization"" (see below).

(p14.3) Additionally, there are several works combining MT and APE systems at decoding time, in which the output of an MT system is further improved by an APE system (Bhattacharyya et al., 2022).

(p14.4) Feedback Model Overoptimization One problem that arises when optimizing a system with a feedback model is that this model is only an imperfect proxy for the ground truth human feedback, therefore, ""overoptimizing"" for them can lead to systems that receive good feedback from the model, but not humans. This problem is known as the overoptimization problem, and is the main reason for the regularization term in Equation 11 Gao et al. (2022) studies the overoptimization problem in preference models, by both optimizing against it with reinforcement learning (training) and reranking outputs with it (decoding). They found that both using preference models during training or decoding led to similar levels of overoptimization, and that the scale of the generation model helps little with this problem.","[[], ['b75'], [None, 'b30', 'b25'], [None], []]","[[], ['b75'], [None, 'b30', 'b25'], [None], []]",5,"1. As mentioned, feedback models have the advantage that they can be queried cheaply for feedback once trained.
2. Perhaps for this reason, most approaches that leverage feedback models by sampling a large number of candidate generations, and reranking them according to the feedback model:whereĥ ϕ is a trained (numerical) feedback model and C is a set of S candidate generations given specific regularization terms, such as the KL terms in PPO (Schulman et al., 2017).
3. by the model (for example, by sampling from its distribution multiple times).
4. In machine translation, Fernandes et al. (2022) andFreitag et al. (2022a) build upon recent advances in automatic quality estimation and evaluation via feedback model training to improve generation.
5. Their framework comprises a candidate generation stage followed by a ranking stage, in which the candidates are scored using quality metrics trained to regress on human assessments (reward models) (Rei et al., 2020a,b) via N -best list reranking or minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2002).
6. The highest-scoring candidate is then chosen as the final translation.
7. Li et al. (2022) collected a dataset of both numerical and natural language feedback for responses from a QA system, and finetuned a pretrained model to predict both kinds of feedback, using the predicted scores from this feedback model to re-rank the predictions from the model.
8. Gao et al. (2022) also used this approach to study the scaling properties of feedback models and the problem of ""overoptimization"" (see below).
9. Additionally, there are several works combining MT and APE systems at decoding time, in which the output of an MT system is further improved by an APE system (Bhattacharyya et al., 2022).
10. Feedback Model Overoptimization One problem that arises when optimizing a system with a feedback model is that this model is only an imperfect proxy for the ground truth human feedback, therefore, ""overoptimizing"" for them can lead to systems that receive good feedback from the model, but not humans.
11. This problem is known as the overoptimization problem, and is the main reason for the regularization term in Equation 11 Gao et al. (2022) studies the overoptimization problem in preference models, by both optimizing against it with reinforcement learning (training) and reranking outputs with it (decoding).
12. They found that both using preference models during training or decoding led to similar levels of overoptimization, and that the scale of the generation model helps little with this problem."
258426970,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef,s15,Collecting and Using Human Feedback,"['p15.0', 'p15.1']","['Collecting human feedback can be rather expensive and may present i ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethical considerations in the use and collection of human feedback.', 'In future, richer types of feedback may be collected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007;Amershi et al., 2014a;Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022;Gehrmann et al., 2022b).']","Collecting human feedback can be rather expensive and may present i ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethical considerations in the use and collection of human feedback.

In future, richer types of feedback may be collected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007;Amershi et al., 2014a;Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022;Gehrmann et al., 2022b).","(p15.0) Collecting human feedback can be rather expensive and may present i ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethical considerations in the use and collection of human feedback.

(p15.1) In future, richer types of feedback may be collected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007;Amershi et al., 2014a;Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022;Gehrmann et al., 2022b).","[[], ['b88', 'b1', 'b2', None]]","[[], ['b88', 'b1', 'b2', None]]",4,"1. Collecting human feedback can be rather expensive and may present i ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully.
2. We present an introduction to existing datasets and their collection methods, along with considerations for experimenters creating preference datasets for their own use cases.
3. Additionally, we discuss ethical considerations in the use and collection of human feedback.
4. In future, richer types of feedback may be collected and we may find ways to make use of this signal.
5. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007;Amershi et al., 2014a;Ghai et al., 2021).
6. Furthermore, variability between human annotators has also not been fully explored (Plank, 2022;Gehrmann et al., 2022b)."
258426970,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef,s16,Considerations in Data Collection,['p16.0'],"['There are multiple facets to consider when collecting human feedback data for a generation task; a non-exhaustive list of axes along which data collection can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008;Sheng et al., 2008;Clark et al., 2021;Gillick and Liu, 2010;Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020;Bai et al., 2022a;Freitag et al., 2021). 3. Collection method: Data can be gathered explicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018;Freitag et al., 2021). 4. Collection platform: Common platforms include Amazon Mechanical Turk, Upwork, and Scale AI. 5. Annotator demographics: Different groups may have varying opinions on quality generations; demographics may be collected during data collection. There is generally a trade-off between the effort needed to create the datasets and the reliability of judgments collected. For higher-stakes applications in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment.']","There are multiple facets to consider when collecting human feedback data for a generation task; a non-exhaustive list of axes along which data collection can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008;Sheng et al., 2008;Clark et al., 2021;Gillick and Liu, 2010;Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020;Bai et al., 2022a;Freitag et al., 2021). 3. Collection method: Data can be gathered explicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018;Freitag et al., 2021). 4. Collection platform: Common platforms include Amazon Mechanical Turk, Upwork, and Scale AI. 5. Annotator demographics: Different groups may have varying opinions on quality generations; demographics may be collected during data collection. There is generally a trade-off between the effort needed to create the datasets and the reliability of judgments collected. For higher-stakes applications in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment.","(p16.0) There are multiple facets to consider when collecting human feedback data for a generation task; a non-exhaustive list of axes along which data collection can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008;Sheng et al., 2008;Clark et al., 2021;Gillick and Liu, 2010;Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020;Bai et al., 2022a;Freitag et al., 2021). 3. Collection method: Data can be gathered explicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018;Freitag et al., 2021). 4. Collection platform: Common platforms include Amazon Mechanical Turk, Upwork, and Scale AI. 5. Annotator demographics: Different groups may have varying opinions on quality generations; demographics may be collected during data collection. There is generally a trade-off between the effort needed to create the datasets and the reliability of judgments collected. For higher-stakes applications in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment.","[['b79', 'b4', 'b84', None, 'b38', 'b87', 'b24']]","[['b79', 'b4', 'b84', None, 'b38', 'b87', 'b24']]",7,"1. There are multiple facets to consider when collecting human feedback data for a generation task; a non-exhaustive list of axes along which data collection can vary is presented below.
2. 1. Annotator expertise: Depending on task and training (Snow et al., 2008;Sheng et al., 2008;Clark et al., 2021;Gillick and Liu, 2010;Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models.
3. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020;Bai et al., 2022a;Freitag et al., 2021).
4. 3. Collection method: Data can be gathered explicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018;Freitag et al., 2021).
5. 4. Collection platform: Common platforms include Amazon Mechanical Turk, Upwork, and Scale AI.
6. 5. Annotator demographics: Different groups may have varying opinions on quality generations; demographics may be collected during data collection.
7. There is generally a trade-off between the effort needed to create the datasets and the reliability of judgments collected.
8. For higher-stakes applications in specific domains, it may be worth the effort to consult expert annotators in an extended partnership.
9. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment."
258426970,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef,s18,Subjectivity and variance in judgment,"['p18.0', 'p18.1']","[""Considering K annotators with feedback functions h i K i=1 , judgments are given on data D = d 1 , ..., d N . Inter-rater reliability metrics, such as Cohen's Kappa, Fleiss' Kappa, or Krippendorff's alpha, can assess annotator agreement (Hayes and Krippendorff, 2007;Fleiss, 1971;Cohen, 1960). Low reliability may result from unclear tasks or evaluation criteria (Gehrmann et al., 2022b;Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022;Nie et al., 2020;Gordon et al., 2022)."", 'Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021;Zerva et al., 2022). Clear annotation guidelines and including rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019).']","Considering K annotators with feedback functions h i K i=1 , judgments are given on data D = d 1 , ..., d N . Inter-rater reliability metrics, such as Cohen's Kappa, Fleiss' Kappa, or Krippendorff's alpha, can assess annotator agreement (Hayes and Krippendorff, 2007;Fleiss, 1971;Cohen, 1960). Low reliability may result from unclear tasks or evaluation criteria (Gehrmann et al., 2022b;Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022;Nie et al., 2020;Gordon et al., 2022).

Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021;Zerva et al., 2022). Clear annotation guidelines and including rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019).","(p18.0) Considering K annotators with feedback functions h i K i=1 , judgments are given on data D = d 1 , ..., d N . Inter-rater reliability metrics, such as Cohen's Kappa, Fleiss' Kappa, or Krippendorff's alpha, can assess annotator agreement (Hayes and Krippendorff, 2007;Fleiss, 1971;Cohen, 1960). Low reliability may result from unclear tasks or evaluation criteria (Gehrmann et al., 2022b;Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022;Nie et al., 2020;Gordon et al., 2022).

(p18.1) Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021;Zerva et al., 2022). Clear annotation guidelines and including rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019).","[['b8', 'b93', None, 'b12', 'b1', 'b43'], ['b112', 'b3', 'b6', 'b108']]","[['b8', 'b93', None, 'b12', 'b1', 'b43'], ['b112', 'b3', 'b6', 'b108']]",10,"1. Considering K annotators with feedback functions
2. h i K i=1 , judgments are given on data D = d 1 , ..., d N .
3. Inter-rater reliability metrics, such as Cohen's Kappa, Fleiss' Kappa, or Krippendorff's alpha, can assess annotator agreement (Hayes and Krippendorff, 2007;Fleiss, 1971;Cohen, 1960).
4. Low reliability may result from unclear tasks or evaluation criteria (Gehrmann et al., 2022b;Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022;Nie et al., 2020;Gordon et al., 2022).
5. Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021;Zerva et al., 2022).
6. Clear annotation guidelines and including rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019)."
258426970,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef,s19,Bias in judgment,"['p19.0', 'p19.1', 'p19.2']","['Even if all K annotators agree on a particular judgment for a certain data point, they may all be mistaken. There are well-known biases in human reasoning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if annotators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of   Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information.', 'systematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annotators are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021). When asked to generate text, anchoring bias can cause people to write in a different manner than usual (Jakesch et al., 2023;Lehmann et al., 2022), which may influence what types of suggestions or corrections they give. Mitigation strategies include asking people to rank several diverse outputs and being explicit about the dimensions people are asked to evaluate.', 'Positivity bias: When giving feedback to learners in traditional RL environments, users tend to give much more positive feedback than negative feedback, which may lead the agent to avoid the goal they are actually trying to reach in these scenarios (Amershi et al., 2014b;Knox and Stone, 2013;Thomaz and Breazeal, 2008).']","Even if all K annotators agree on a particular judgment for a certain data point, they may all be mistaken. There are well-known biases in human reasoning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if annotators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of   Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information.

systematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annotators are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021). When asked to generate text, anchoring bias can cause people to write in a different manner than usual (Jakesch et al., 2023;Lehmann et al., 2022), which may influence what types of suggestions or corrections they give. Mitigation strategies include asking people to rank several diverse outputs and being explicit about the dimensions people are asked to evaluate.

Positivity bias: When giving feedback to learners in traditional RL environments, users tend to give much more positive feedback than negative feedback, which may lead the agent to avoid the goal they are actually trying to reach in these scenarios (Amershi et al., 2014b;Knox and Stone, 2013;Thomaz and Breazeal, 2008).","(p19.0) Even if all K annotators agree on a particular judgment for a certain data point, they may all be mistaken. There are well-known biases in human reasoning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if annotators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of   Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information.

(p19.1) systematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annotators are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021). When asked to generate text, anchoring bias can cause people to write in a different manner than usual (Jakesch et al., 2023;Lehmann et al., 2022), which may influence what types of suggestions or corrections they give. Mitigation strategies include asking people to rank several diverse outputs and being explicit about the dimensions people are asked to evaluate.

(p19.2) Positivity bias: When giving feedback to learners in traditional RL environments, users tend to give much more positive feedback than negative feedback, which may lead the agent to avoid the goal they are actually trying to reach in these scenarios (Amershi et al., 2014b;Knox and Stone, 2013;Thomaz and Breazeal, 2008).","[[], [None, 'b27'], ['b22', None, 'b92']]","[[], [None, 'b27'], ['b22', None, 'b92']]",5,"1. Even if all K annotators agree on a particular judgment for a certain data point, they may all be mistaken.
2. There are well-known biases in human reasoning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account.
3. Furthermore, even if annotators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of   Table 1 for definitions related to feedback types.
4. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT.
5. N/A means we could not find information.
6. systematic bias away from the originally intended task (Parmar et al., 2023).
7. Anchoring/Confirmation bias: When annotators are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021).
8. When asked to generate text, anchoring bias can cause people to write in a different manner than usual (Jakesch et al., 2023;Lehmann et al., 2022), which may influence what types of suggestions or corrections they give.
9. Mitigation strategies include asking people to rank several diverse outputs and being explicit about the dimensions people are asked to evaluate.Positivity bias: When giving feedback to learners in traditional RL environments, users tend to give much more positive feedback than negative feedback, which may lead the agent to avoid the goal they are actually trying to reach in these scenarios (Amershi et al., 2014b;Knox and Stone, 2013;Thomaz and Breazeal, 2008)."
258426970,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef,s21,AI Feedback,"['p21.0', 'p21.1', 'p21.2']","[""Feedback models have been crucial in advancing generation techniques by effectively leveraging feedback. However, they are heavily reliant on human input: for example, Gao et al. (2022) found that across various preference model sizes, utilizing fewer than 1,000 comparisons resulted in only minor improvements, with outcomes approximating chance. Moreover, employing static feedback can create consistency and accuracy challenges, as the integration of feedback leads to changes in the model's output distribution. AI-generated feedback, an emerging research area, focuses on harnessing the large language model's own abilities to evaluate and improve its output, enhancing the model without constant human intervention. Two primary approaches have emerged in this domain:"", 'Self AI Feedback The first approach involves using the same model to provide feedback and improve its output. In this scenario, the model engages in a continuous self-improvement process, learning from its evaluations and refining its capabilities accordingly. Examples of this approach include prompting models to generate harmful responses and revising them for harmlessness (Bai et al., 2022b), or employing rule-based reward models for RLHF fine-tuning (OpenAI, 2023a). Techniques such as iterative output revision through few-shot prompting (Peng et al., 2023;Shinn et al., 2023;Paul et al., 2023;Madaan et al., 2023; have been explored using LLMs like GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023a). Notably, these techniques demonstrate potential when applied to LLMs trained to adhere to human instructions and align outputs with human preferences. This suggests that incorporating human feedback during training equips AI models to comprehend task requirements better, align outputs with directives, and function as dependable feedback mechanisms, thereby minimizing human intervention. Intriguingly, the capacity to offer valuable AI feedback may depend on the model being trained with human feedback.', ""External AI Feedback: The second approach employs a separate model to provide feedback on the model's outputs which is being improved. In this setting, the task model is often paired with a separately trained feedback model (Yasunaga and Liang, 2020;Madaan et al., 2021;Welleck et al., 2022;Bai et al., 2022b;Akyürek et al., 2023). An advantage of this approach is that the feedback model does not need to be a large, general-purpose model like GPT-4. Thus, training smaller feedback models becomes an attractive alternative when a large amount of feedback is available.""]","Feedback models have been crucial in advancing generation techniques by effectively leveraging feedback. However, they are heavily reliant on human input: for example, Gao et al. (2022) found that across various preference model sizes, utilizing fewer than 1,000 comparisons resulted in only minor improvements, with outcomes approximating chance. Moreover, employing static feedback can create consistency and accuracy challenges, as the integration of feedback leads to changes in the model's output distribution. AI-generated feedback, an emerging research area, focuses on harnessing the large language model's own abilities to evaluate and improve its output, enhancing the model without constant human intervention. Two primary approaches have emerged in this domain:

Self AI Feedback The first approach involves using the same model to provide feedback and improve its output. In this scenario, the model engages in a continuous self-improvement process, learning from its evaluations and refining its capabilities accordingly. Examples of this approach include prompting models to generate harmful responses and revising them for harmlessness (Bai et al., 2022b), or employing rule-based reward models for RLHF fine-tuning (OpenAI, 2023a). Techniques such as iterative output revision through few-shot prompting (Peng et al., 2023;Shinn et al., 2023;Paul et al., 2023;Madaan et al., 2023; have been explored using LLMs like GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023a). Notably, these techniques demonstrate potential when applied to LLMs trained to adhere to human instructions and align outputs with human preferences. This suggests that incorporating human feedback during training equips AI models to comprehend task requirements better, align outputs with directives, and function as dependable feedback mechanisms, thereby minimizing human intervention. Intriguingly, the capacity to offer valuable AI feedback may depend on the model being trained with human feedback.

External AI Feedback: The second approach employs a separate model to provide feedback on the model's outputs which is being improved. In this setting, the task model is often paired with a separately trained feedback model (Yasunaga and Liang, 2020;Madaan et al., 2021;Welleck et al., 2022;Bai et al., 2022b;Akyürek et al., 2023). An advantage of this approach is that the feedback model does not need to be a large, general-purpose model like GPT-4. Thus, training smaller feedback models becomes an attractive alternative when a large amount of feedback is available.","(p21.0) Feedback models have been crucial in advancing generation techniques by effectively leveraging feedback. However, they are heavily reliant on human input: for example, Gao et al. (2022) found that across various preference model sizes, utilizing fewer than 1,000 comparisons resulted in only minor improvements, with outcomes approximating chance. Moreover, employing static feedback can create consistency and accuracy challenges, as the integration of feedback leads to changes in the model's output distribution. AI-generated feedback, an emerging research area, focuses on harnessing the large language model's own abilities to evaluate and improve its output, enhancing the model without constant human intervention. Two primary approaches have emerged in this domain:

(p21.1) Self AI Feedback The first approach involves using the same model to provide feedback and improve its output. In this scenario, the model engages in a continuous self-improvement process, learning from its evaluations and refining its capabilities accordingly. Examples of this approach include prompting models to generate harmful responses and revising them for harmlessness (Bai et al., 2022b), or employing rule-based reward models for RLHF fine-tuning (OpenAI, 2023a). Techniques such as iterative output revision through few-shot prompting (Peng et al., 2023;Shinn et al., 2023;Paul et al., 2023;Madaan et al., 2023; have been explored using LLMs like GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023a). Notably, these techniques demonstrate potential when applied to LLMs trained to adhere to human instructions and align outputs with human preferences. This suggests that incorporating human feedback during training equips AI models to comprehend task requirements better, align outputs with directives, and function as dependable feedback mechanisms, thereby minimizing human intervention. Intriguingly, the capacity to offer valuable AI feedback may depend on the model being trained with human feedback.

(p21.2) External AI Feedback: The second approach employs a separate model to provide feedback on the model's outputs which is being improved. In this setting, the task model is often paired with a separately trained feedback model (Yasunaga and Liang, 2020;Madaan et al., 2021;Welleck et al., 2022;Bai et al., 2022b;Akyürek et al., 2023). An advantage of this approach is that the feedback model does not need to be a large, general-purpose model like GPT-4. Thus, training smaller feedback models becomes an attractive alternative when a large amount of feedback is available.","[[None], ['b37', 'b50', 'b81', None, 'b52'], ['b110', None, 'b38', 'b100']]","[[None], ['b37', 'b50', 'b81', None, 'b52'], ['b110', None, 'b38', 'b100']]",10,"1. Feedback models have been crucial in advancing generation techniques by effectively leveraging feedback.
2. However, they are heavily reliant on human input: for example, Gao et al. (2022) found that across various preference model sizes, utilizing fewer than 1,000 comparisons resulted in only minor improvements, with outcomes approximating chance.
3. Moreover, employing static feedback can create consistency and accuracy challenges, as the integration of feedback leads to changes in the model's output distribution.
4. AI-generated feedback, an emerging research area, focuses on harnessing the large language model's own abilities to evaluate and improve its output, enhancing the model without constant human intervention.
5. Two primary approaches have emerged in this domain:Self AI Feedback
6. The first approach involves using the same model to provide feedback and improve its output.
7. In this scenario, the model engages in a continuous self-improvement process, learning from its evaluations and refining its capabilities accordingly.
8. Examples of this approach include prompting models to generate harmful responses and revising them for harmlessness (Bai et al., 2022b), or employing rule-based reward models for RLHF fine-tuning (OpenAI, 2023a).
9. Techniques such as iterative output revision through few-shot prompting (Peng et al., 2023;Shinn et al., 2023;Paul et al., 2023;Madaan et al., 2023; have been explored using LLMs like GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023a).
10. Notably, these techniques demonstrate potential when applied to LLMs trained to adhere to human instructions and align outputs with human preferences.
11. This suggests that incorporating human feedback during training equips AI models to comprehend task requirements better, align outputs with directives, and function as dependable feedback mechanisms, thereby minimizing human intervention.
12. Intriguingly, the capacity to offer valuable AI feedback may depend on the model being trained with human feedback.
13. External AI Feedback: The second approach employs a separate model to provide feedback on the model's outputs which is being improved.
14. In this setting, the task model is often paired with a separately trained feedback model (Yasunaga and Liang, 2020;Madaan et al., 2021;Welleck et al., 2022;Bai et al., 2022b;Akyürek et al., 2023).
15. An advantage of this approach is that the feedback model does not need to be a large, general-purpose model like GPT-4.
16. Thus, training smaller feedback models becomes an attractive alternative when a large amount of feedback is available."
258426970,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef,s13,Optimizing for Feedback Models,"['p13.0', 'p13.1', 'p13.2', 'p13.3', 'p13.4', 'p13.5']","['Similarly to optimizing for human feedback, one possible way to use the feedback model is to optimize model parameters with respect to the feedback it gives. If the feedback model outputs numerical feedback (ĥ ϕ : X × Y → R) we can define an optimization problem similar to Equation 2. However, due to the limitations of feedback models as imperfect proxies, typically a regularization term R is introduced to avoid ""overfitting"" to the feedback model (Ziegler et al., 2019) (more on this at the end of this section):', '(11) Due to the similarities between both optimization problems, approaches to tackle Equation 11 can be divided into two of the three categories in §4.2: joint-feedback modeling and reinforcement learning. Recall that while in §4.2 we discuss approaches for directly optimizing for human feedback, while this section is focused on cases where a model of human feedback is used instead.', 'Unlike when using human feedback directly, most works attempt to optimize for feedback models using reinforcement learning. Gao et al. (2018);Böhm et al. (2019) use the (numerical) feedback collected in other works to train reward and preference models, and use reinforcement learning to optimize against these models, showing that humans preferred their summarization model to other supervised and RL-trained baselines. Ziegler et al. (2019) proposed a similar approach, but trained preference models using feedback collected on the model being improved, and introduced a KL regularization term', 'to avoid the optimized model deviating too much from the original (supervised) model with parameters θ SL 4 . Stiennon et al. (2020) extended this work, by scaling both the summarization and preference models, showing that their model was highly preferred by humans, and generalized better than supervised baselines. Ouyang et al. (2022) also used reinforcement learning with preference models to improve the ability of LLMs to follow instructions, but combined the RL objective with the original pretraining objective to avoid performance regressions in public NLP benchmarks. Other works have also used reinforcement learning with preference models in a similar manner (Askell et al., 2021;Bai et al., 2022a;Wu et al., 2021;Nguyen et al., 2022). Underlying all these methods is that generally the model is first trained with imitation-learning on human demonstrations, which improves performance compared to using reinforcement learning directly on the pretrained policy. Glaese et al. (2022) compared doing feedbackbased imitation learning with human feedback ( §4.1) with doing reinforcement learning with a feedback model, finding that the latter led to a better preference rate and lower rule violation rate.', 'The joint-feedback modeling with feedback models was explored by , who study pre-training an LLMs with a loss similar to Equation 6, based on feedback from a preference model trained on ranking-based feedback for toxicity. They showed that this leads to models producing less toxic generations, when compared to pretraining a model with vanilla MLE.', 'In an approach outside these main categories, Peyrard and Gurevych (2018) use a scoring function learned from human judgments as a fitness function for a genetic algorithm to generate summaries of input texts.']","Similarly to optimizing for human feedback, one possible way to use the feedback model is to optimize model parameters with respect to the feedback it gives. If the feedback model outputs numerical feedback (ĥ ϕ : X × Y → R) we can define an optimization problem similar to Equation 2. However, due to the limitations of feedback models as imperfect proxies, typically a regularization term R is introduced to avoid ""overfitting"" to the feedback model (Ziegler et al., 2019) (more on this at the end of this section):

(11) Due to the similarities between both optimization problems, approaches to tackle Equation 11 can be divided into two of the three categories in §4.2: joint-feedback modeling and reinforcement learning. Recall that while in §4.2 we discuss approaches for directly optimizing for human feedback, while this section is focused on cases where a model of human feedback is used instead.

Unlike when using human feedback directly, most works attempt to optimize for feedback models using reinforcement learning. Gao et al. (2018);Böhm et al. (2019) use the (numerical) feedback collected in other works to train reward and preference models, and use reinforcement learning to optimize against these models, showing that humans preferred their summarization model to other supervised and RL-trained baselines. Ziegler et al. (2019) proposed a similar approach, but trained preference models using feedback collected on the model being improved, and introduced a KL regularization term

to avoid the optimized model deviating too much from the original (supervised) model with parameters θ SL 4 . Stiennon et al. (2020) extended this work, by scaling both the summarization and preference models, showing that their model was highly preferred by humans, and generalized better than supervised baselines. Ouyang et al. (2022) also used reinforcement learning with preference models to improve the ability of LLMs to follow instructions, but combined the RL objective with the original pretraining objective to avoid performance regressions in public NLP benchmarks. Other works have also used reinforcement learning with preference models in a similar manner (Askell et al., 2021;Bai et al., 2022a;Wu et al., 2021;Nguyen et al., 2022). Underlying all these methods is that generally the model is first trained with imitation-learning on human demonstrations, which improves performance compared to using reinforcement learning directly on the pretrained policy. Glaese et al. (2022) compared doing feedbackbased imitation learning with human feedback ( §4.1) with doing reinforcement learning with a feedback model, finding that the latter led to a better preference rate and lower rule violation rate.

The joint-feedback modeling with feedback models was explored by , who study pre-training an LLMs with a loss similar to Equation 6, based on feedback from a preference model trained on ranking-based feedback for toxicity. They showed that this leads to models producing less toxic generations, when compared to pretraining a model with vanilla MLE.

In an approach outside these main categories, Peyrard and Gurevych (2018) use a scoring function learned from human judgments as a fitness function for a genetic algorithm to generate summaries of input texts.","(p13.0) Similarly to optimizing for human feedback, one possible way to use the feedback model is to optimize model parameters with respect to the feedback it gives. If the feedback model outputs numerical feedback (ĥ ϕ : X × Y → R) we can define an optimization problem similar to Equation 2. However, due to the limitations of feedback models as imperfect proxies, typically a regularization term R is introduced to avoid ""overfitting"" to the feedback model (Ziegler et al., 2019) (more on this at the end of this section):

(p13.1) (11) Due to the similarities between both optimization problems, approaches to tackle Equation 11 can be divided into two of the three categories in §4.2: joint-feedback modeling and reinforcement learning. Recall that while in §4.2 we discuss approaches for directly optimizing for human feedback, while this section is focused on cases where a model of human feedback is used instead.

(p13.2) Unlike when using human feedback directly, most works attempt to optimize for feedback models using reinforcement learning. Gao et al. (2018);Böhm et al. (2019) use the (numerical) feedback collected in other works to train reward and preference models, and use reinforcement learning to optimize against these models, showing that humans preferred their summarization model to other supervised and RL-trained baselines. Ziegler et al. (2019) proposed a similar approach, but trained preference models using feedback collected on the model being improved, and introduced a KL regularization term

(p13.3) to avoid the optimized model deviating too much from the original (supervised) model with parameters θ SL 4 . Stiennon et al. (2020) extended this work, by scaling both the summarization and preference models, showing that their model was highly preferred by humans, and generalized better than supervised baselines. Ouyang et al. (2022) also used reinforcement learning with preference models to improve the ability of LLMs to follow instructions, but combined the RL objective with the original pretraining objective to avoid performance regressions in public NLP benchmarks. Other works have also used reinforcement learning with preference models in a similar manner (Askell et al., 2021;Bai et al., 2022a;Wu et al., 2021;Nguyen et al., 2022). Underlying all these methods is that generally the model is first trained with imitation-learning on human demonstrations, which improves performance compared to using reinforcement learning directly on the pretrained policy. Glaese et al. (2022) compared doing feedbackbased imitation learning with human feedback ( §4.1) with doing reinforcement learning with a feedback model, finding that the latter led to a better preference rate and lower rule violation rate.

(p13.4) The joint-feedback modeling with feedback models was explored by , who study pre-training an LLMs with a loss similar to Equation 6, based on feedback from a preference model trained on ranking-based feedback for toxicity. They showed that this leads to models producing less toxic generations, when compared to pretraining a model with vanilla MLE.

(p13.5) In an approach outside these main categories, Peyrard and Gurevych (2018) use a scoring function learned from human judgments as a fitness function for a genetic algorithm to generate summaries of input texts.","[[], [], [None], ['b69', None, 'b87', 'b42', 'b5'], [], []]","[[], [], [None], ['b69', None, 'b87', 'b42', 'b5'], [], []]",6,"1. Similarly to optimizing for human feedback, one possible way to use the feedback model is to optimize model parameters with respect to the feedback it gives.
2. If the feedback model outputs numerical feedback (ĥ ϕ : X × Y → R) we can define an optimization problem similar to Equation 2.
3. However, due to the limitations of feedback models as imperfect proxies, typically a regularization term R is introduced to avoid ""overfitting"" to the feedback model (Ziegler et al., 2019)
4. (more on this at the end of this section):
5. (11) Due to the similarities between both optimization problems, approaches to tackle Equation 11 can be divided into two of the three categories in §4.2: joint-feedback modeling and reinforcement learning.
6. Recall that while in §4.2 we discuss approaches for directly optimizing for human feedback, while this section is focused on cases where a model of human feedback is used instead.
7. Unlike when using human feedback directly, most works attempt to optimize for feedback models using reinforcement learning.
8. Gao et al. (2018);Böhm et al. (2019) use the (numerical) feedback collected in other works to train reward and preference models, and use reinforcement learning to optimize against these models, showing that humans preferred their summarization model to other supervised and RL-trained baselines.
9. Ziegler et al. (2019) proposed a similar approach, but trained preference models using feedback collected on the model being improved, and introduced a KL regularization termto avoid the optimized model deviating too much from the original (supervised) model with parameters θ SL 4 .
10. Stiennon et al. (2020) extended this work, by scaling both the summarization and preference models, showing that their model was highly preferred by humans, and generalized better than supervised baselines.
11. Ouyang et al. (2022) also used reinforcement learning with preference models to improve the ability of LLMs to follow instructions, but combined the RL objective with the original pretraining objective to avoid performance regressions in public NLP benchmarks.
12. Other works have also used reinforcement learning with preference models in a similar manner (Askell et al., 2021;Bai et al., 2022a;Wu et al., 2021;Nguyen et al., 2022).
13. Underlying all these methods is that generally the model is first trained with imitation-learning on human demonstrations, which improves performance compared to using reinforcement learning directly on the pretrained policy.
14. Glaese et al. (2022) compared doing feedbackbased imitation learning with human feedback ( §4.1) with doing reinforcement learning with a feedback model, finding that the latter led to a better preference rate and lower rule violation rate.
15. The joint-feedback modeling with feedback models was explored by , who study pre-training an LLMs with a loss similar to Equation 6, based on feedback from a preference model trained on ranking-based feedback for toxicity.
16. They showed that this leads to models producing less toxic generations, when compared to pretraining a model with vanilla MLE.
17. In an approach outside these main categories, Peyrard and Gurevych (2018) use a scoring function learned from human judgments as a fitness function for a genetic algorithm to generate summaries of input texts."
258426970,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef,s5,Format,"['p5.0', 'p5.1', 'p5.2', 'p5.3', 'p5.4', 'p5.5']","['An important decision to make when we want to improve language generation systems through human Example input and output for three tasks (machine translation, summarization, and instruction following) and possible different (example) feedback that can be given.', 'feedback is in what format to collect this feedback in. The choice of format has implications on the expressivity of the feedback, the ease of its collection, and how we can use it to improve systems. In particular, the complexity of the feedback format is an important factor: simpler formats are often easier to collect and use as part of the training/decoding process, but contain less information than more ""complex"" formats, and might not be able to capture important information for improving the system. The choice of format also has implications in the difficulty for humans to give feedback, its consistency/agreement, and the level of rationality of said feedback (Ghosal et al., 2023). Types of feedback are summarized in Table 1 with examples.', 'Numerical Numerical feedback, which takes an input and output and returns a single score (X × Y → N ⊆ R), is one of the simplest feedback formats to collect and use. Kreutzer et al. (2018) studied using categorical feedback, in the form of 5 possible ""stars"" that can be assigned to a translation, which are then averaged to produce a score (N = [1, 5]) and used to improve the model.  and Shi et al. (2021) used even simpler feedback, by asking humans to choose if a given response is good or not (N = {0, 1}). Numerical feedback has also been extensively used for evaluation, albeit not with the explicit goal of improving generation. For example, direct assessments (Graham et al., 2013) in machine translation ask humans to rate translations on a continuous scale, and some works have attempted to use this feedback data to train feedback models (Sellam et al., 2020;Rei et al., 2020a) and improve generation (Freitag et al., 2022a;Fernandes et al., 2022).', 'Although easy to leverage, numerical feedback suffers from some limitations: depending on the complexity of the generation task, reducing feedback to a single score might generally be a hard and ill-defined task for humans, leading to a costly collection process and problems of subjectivity and variance (see §6.2.1). Furthermore, such feedback might not be suited to distinguish between outputs of similar quality.', ""Ranking-based An alternative to asking humans to assign a single score to a given input-output pair is asking them to rank multiple possible alternative outputs h : X × Y 1 × · · · × Y n → S n where S n represents the set of all permutations/rankings of n elements (optionally allowing ties). This has been used extensively in evaluation (Chaganty et al., 2018). Compared to numerical feedback, this format tends to be easier to collect, and, potentially, for this reason, ranking-based feedback tends to be collected to improve model behavior rather than just for evaluation (since the former tends to require more feedback data). Ziegler et al. (2019) and Stiennon et al. (2020) asked humans to rank alternative summaries of the system they are trying to improve. Similarly, Ouyang et al. (2022) collected rankings of alternative responses to an instruction given to the model. They utilized these rankings to enhance the model's instruction-following capabilities. Subsequent research has also employed ranking-based feedback for the same task (Askell et al., 2021;Bai et al., 2022a,b). Natural Language Both numerical and rankingbased feedback lack the ability to capture detailed information about problems with the output, which can be crucial for improving generation systems. Instead of asking humans to rank or score outputs, we can instead ask for natural language feedback. In such cases, the feedback typically provides more detailed information, either highlighting the shortcomings of the current output or suggesting specific actions for improvement. For example, Li et al. (2017) asked humans to give natural language feedback to a dialogue question answering model, including positive or negative feedback, but also possibly providing the correct answer to the model or hinting about it.  and  gather natural language feedback on errors present in model-generated graphs and the model's interpretation of a given instruction. Scheurer et al. (2022Scheurer et al. ( , 2023 improve summarization capabilities of language models by asking humans to provide natural language feedback of summaries of the model. Li et al. (2022) collect natural language feedback (in addition to numerical feedback) for responses from a Question Answering (QA) system."", ""Others Besides these feedback types, other (potentially domain-specific) types of feedback can be used to improve model behavior. Commonly humans are asked to provide multi-aspect feedback (X × Y → R d or F d more generally), scoring an output or ranking multiple outputs with respect to multiple dimensions (Böhm et al., 2019;Glaese et al., 2022;Madaan et al., 2023;Nguyen et al., 2022). Post-editions ask humans to provide corrections to the output in the form of small edits (e.g., replace X by Y), and post-edition data has been used to directly improve models (Denkowski et al., 2014) or train automatic post edition systems that correct model mistakes (Pal et al., 2016;Mehta and Goldwasser, 2019;Madaan et al., 2021;Talmor et al., 2020;Elgohary et al., 2021). There are also other feedback types that haven't been fully leveraged to improve generation: e.g., Multidimensional Quality Metrics (MQM) (Lommel et al., 2014b), the standard for evaluating translation quality, asks professional translators to identify errors spans in a translation, alongside severity and type of error.""]","An important decision to make when we want to improve language generation systems through human Example input and output for three tasks (machine translation, summarization, and instruction following) and possible different (example) feedback that can be given.

feedback is in what format to collect this feedback in. The choice of format has implications on the expressivity of the feedback, the ease of its collection, and how we can use it to improve systems. In particular, the complexity of the feedback format is an important factor: simpler formats are often easier to collect and use as part of the training/decoding process, but contain less information than more ""complex"" formats, and might not be able to capture important information for improving the system. The choice of format also has implications in the difficulty for humans to give feedback, its consistency/agreement, and the level of rationality of said feedback (Ghosal et al., 2023). Types of feedback are summarized in Table 1 with examples.

Numerical Numerical feedback, which takes an input and output and returns a single score (X × Y → N ⊆ R), is one of the simplest feedback formats to collect and use. Kreutzer et al. (2018) studied using categorical feedback, in the form of 5 possible ""stars"" that can be assigned to a translation, which are then averaged to produce a score (N = [1, 5]) and used to improve the model.  and Shi et al. (2021) used even simpler feedback, by asking humans to choose if a given response is good or not (N = {0, 1}). Numerical feedback has also been extensively used for evaluation, albeit not with the explicit goal of improving generation. For example, direct assessments (Graham et al., 2013) in machine translation ask humans to rate translations on a continuous scale, and some works have attempted to use this feedback data to train feedback models (Sellam et al., 2020;Rei et al., 2020a) and improve generation (Freitag et al., 2022a;Fernandes et al., 2022).

Although easy to leverage, numerical feedback suffers from some limitations: depending on the complexity of the generation task, reducing feedback to a single score might generally be a hard and ill-defined task for humans, leading to a costly collection process and problems of subjectivity and variance (see §6.2.1). Furthermore, such feedback might not be suited to distinguish between outputs of similar quality.

Ranking-based An alternative to asking humans to assign a single score to a given input-output pair is asking them to rank multiple possible alternative outputs h : X × Y 1 × · · · × Y n → S n where S n represents the set of all permutations/rankings of n elements (optionally allowing ties). This has been used extensively in evaluation (Chaganty et al., 2018). Compared to numerical feedback, this format tends to be easier to collect, and, potentially, for this reason, ranking-based feedback tends to be collected to improve model behavior rather than just for evaluation (since the former tends to require more feedback data). Ziegler et al. (2019) and Stiennon et al. (2020) asked humans to rank alternative summaries of the system they are trying to improve. Similarly, Ouyang et al. (2022) collected rankings of alternative responses to an instruction given to the model. They utilized these rankings to enhance the model's instruction-following capabilities. Subsequent research has also employed ranking-based feedback for the same task (Askell et al., 2021;Bai et al., 2022a,b). Natural Language Both numerical and rankingbased feedback lack the ability to capture detailed information about problems with the output, which can be crucial for improving generation systems. Instead of asking humans to rank or score outputs, we can instead ask for natural language feedback. In such cases, the feedback typically provides more detailed information, either highlighting the shortcomings of the current output or suggesting specific actions for improvement. For example, Li et al. (2017) asked humans to give natural language feedback to a dialogue question answering model, including positive or negative feedback, but also possibly providing the correct answer to the model or hinting about it.  and  gather natural language feedback on errors present in model-generated graphs and the model's interpretation of a given instruction. Scheurer et al. (2022Scheurer et al. ( , 2023 improve summarization capabilities of language models by asking humans to provide natural language feedback of summaries of the model. Li et al. (2022) collect natural language feedback (in addition to numerical feedback) for responses from a Question Answering (QA) system.

Others Besides these feedback types, other (potentially domain-specific) types of feedback can be used to improve model behavior. Commonly humans are asked to provide multi-aspect feedback (X × Y → R d or F d more generally), scoring an output or ranking multiple outputs with respect to multiple dimensions (Böhm et al., 2019;Glaese et al., 2022;Madaan et al., 2023;Nguyen et al., 2022). Post-editions ask humans to provide corrections to the output in the form of small edits (e.g., replace X by Y), and post-edition data has been used to directly improve models (Denkowski et al., 2014) or train automatic post edition systems that correct model mistakes (Pal et al., 2016;Mehta and Goldwasser, 2019;Madaan et al., 2021;Talmor et al., 2020;Elgohary et al., 2021). There are also other feedback types that haven't been fully leveraged to improve generation: e.g., Multidimensional Quality Metrics (MQM) (Lommel et al., 2014b), the standard for evaluating translation quality, asks professional translators to identify errors spans in a translation, alongside severity and type of error.","(p5.0) An important decision to make when we want to improve language generation systems through human Example input and output for three tasks (machine translation, summarization, and instruction following) and possible different (example) feedback that can be given.

(p5.1) feedback is in what format to collect this feedback in. The choice of format has implications on the expressivity of the feedback, the ease of its collection, and how we can use it to improve systems. In particular, the complexity of the feedback format is an important factor: simpler formats are often easier to collect and use as part of the training/decoding process, but contain less information than more ""complex"" formats, and might not be able to capture important information for improving the system. The choice of format also has implications in the difficulty for humans to give feedback, its consistency/agreement, and the level of rationality of said feedback (Ghosal et al., 2023). Types of feedback are summarized in Table 1 with examples.

(p5.2) Numerical Numerical feedback, which takes an input and output and returns a single score (X × Y → N ⊆ R), is one of the simplest feedback formats to collect and use. Kreutzer et al. (2018) studied using categorical feedback, in the form of 5 possible ""stars"" that can be assigned to a translation, which are then averaged to produce a score (N = [1, 5]) and used to improve the model.  and Shi et al. (2021) used even simpler feedback, by asking humans to choose if a given response is good or not (N = {0, 1}). Numerical feedback has also been extensively used for evaluation, albeit not with the explicit goal of improving generation. For example, direct assessments (Graham et al., 2013) in machine translation ask humans to rate translations on a continuous scale, and some works have attempted to use this feedback data to train feedback models (Sellam et al., 2020;Rei et al., 2020a) and improve generation (Freitag et al., 2022a;Fernandes et al., 2022).

(p5.3) Although easy to leverage, numerical feedback suffers from some limitations: depending on the complexity of the generation task, reducing feedback to a single score might generally be a hard and ill-defined task for humans, leading to a costly collection process and problems of subjectivity and variance (see §6.2.1). Furthermore, such feedback might not be suited to distinguish between outputs of similar quality.

(p5.4) Ranking-based An alternative to asking humans to assign a single score to a given input-output pair is asking them to rank multiple possible alternative outputs h : X × Y 1 × · · · × Y n → S n where S n represents the set of all permutations/rankings of n elements (optionally allowing ties). This has been used extensively in evaluation (Chaganty et al., 2018). Compared to numerical feedback, this format tends to be easier to collect, and, potentially, for this reason, ranking-based feedback tends to be collected to improve model behavior rather than just for evaluation (since the former tends to require more feedback data). Ziegler et al. (2019) and Stiennon et al. (2020) asked humans to rank alternative summaries of the system they are trying to improve. Similarly, Ouyang et al. (2022) collected rankings of alternative responses to an instruction given to the model. They utilized these rankings to enhance the model's instruction-following capabilities. Subsequent research has also employed ranking-based feedback for the same task (Askell et al., 2021;Bai et al., 2022a,b). Natural Language Both numerical and rankingbased feedback lack the ability to capture detailed information about problems with the output, which can be crucial for improving generation systems. Instead of asking humans to rank or score outputs, we can instead ask for natural language feedback. In such cases, the feedback typically provides more detailed information, either highlighting the shortcomings of the current output or suggesting specific actions for improvement. For example, Li et al. (2017) asked humans to give natural language feedback to a dialogue question answering model, including positive or negative feedback, but also possibly providing the correct answer to the model or hinting about it.  and  gather natural language feedback on errors present in model-generated graphs and the model's interpretation of a given instruction. Scheurer et al. (2022Scheurer et al. ( , 2023 improve summarization capabilities of language models by asking humans to provide natural language feedback of summaries of the model. Li et al. (2022) collect natural language feedback (in addition to numerical feedback) for responses from a Question Answering (QA) system.

(p5.5) Others Besides these feedback types, other (potentially domain-specific) types of feedback can be used to improve model behavior. Commonly humans are asked to provide multi-aspect feedback (X × Y → R d or F d more generally), scoring an output or ranking multiple outputs with respect to multiple dimensions (Böhm et al., 2019;Glaese et al., 2022;Madaan et al., 2023;Nguyen et al., 2022). Post-editions ask humans to provide corrections to the output in the form of small edits (e.g., replace X by Y), and post-edition data has been used to directly improve models (Denkowski et al., 2014) or train automatic post edition systems that correct model mistakes (Pal et al., 2016;Mehta and Goldwasser, 2019;Madaan et al., 2021;Talmor et al., 2020;Elgohary et al., 2021). There are also other feedback types that haven't been fully leveraged to improve generation: e.g., Multidimensional Quality Metrics (MQM) (Lommel et al., 2014b), the standard for evaluating translation quality, asks professional translators to identify errors spans in a translation, alongside severity and type of error.","[[], ['b3'], ['b60', 'b80', 'b76', None, 'b24', 'b9'], [], ['b30', 'b72', 'b29', None, 'b87', 'b71'], ['b37', 'b40', 'b90', None, 'b38', 'b35', 'b5', 'b42']]","[[], ['b3'], ['b60', 'b80', 'b76', None, 'b24', 'b9'], [], ['b30', 'b72', 'b29', None, 'b87', 'b71'], ['b37', 'b40', 'b90', None, 'b38', 'b35', 'b5', 'b42']]",21,"1. An important decision to make when we want to improve language generation systems through human Example input and output for three tasks (machine translation, summarization, and instruction following) and possible different (example) feedback that can be given.feedback is in what format to collect this feedback in.
2. The choice of format has implications on the expressivity of the feedback, the ease of its collection, and how we can use it to improve systems.
3. In particular, the complexity of the feedback format is an important factor: simpler formats are often easier to collect and use as part of the training/decoding process, but contain less information than more ""complex"" formats, and might not be able to capture important information for improving the system.
4. The choice of format also has implications in the difficulty for humans to give feedback, its consistency/agreement, and the level of rationality of said feedback (Ghosal et al., 2023).
5. Types of feedback are summarized in Table 1 with examples.
6. Numerical Numerical feedback, which takes an input and output and returns a single score (X × Y → N ⊆ R), is one of the simplest feedback formats to collect and use.
7. Kreutzer et al. (2018) studied using categorical feedback, in the form of 5 possible ""stars"" that can be assigned to a translation, which are then averaged to produce a score (N = [1, 5]) and used to improve the model.
8. and Shi et al. (2021) used even simpler feedback, by asking humans to choose if a given response is good or not (N = {0, 1}).
9. Numerical feedback has also been extensively used for evaluation, albeit not with the explicit goal of improving generation.
10. For example, direct assessments (Graham et al., 2013) in machine translation ask humans to rate translations on a continuous scale, and some works have attempted to use this feedback data to train feedback models (Sellam et al., 2020;Rei et al., 2020a) and improve generation (Freitag et al., 2022a;Fernandes et al., 2022).
11. Although easy to leverage, numerical feedback suffers from some limitations: depending on the complexity of the generation task, reducing feedback to a single score might generally be a hard and ill-defined task for humans, leading to a costly collection process and problems of subjectivity and variance (see §6.2.1).
12. Furthermore, such feedback might not be suited to distinguish between outputs of similar quality.
13. Ranking-based An alternative to asking humans to assign a single score to a given input-output pair is asking them to rank multiple possible alternative outputs h : X ×
14. Y 1 × · · · × Y n → S n where S n represents the set of all permutations/rankings of n elements (optionally allowing ties).
15. This has been used extensively in evaluation (Chaganty et al., 2018).
16. Compared to numerical feedback, this format tends to be easier to collect, and, potentially, for this reason, ranking-based feedback tends to be collected to improve model behavior rather than just for evaluation (since the former tends to require more feedback data).
17. Ziegler et al. (2019) and Stiennon et al. (2020) asked humans to rank alternative summaries of the system they are trying to improve.
18. Similarly, Ouyang et al. (2022) collected rankings of alternative responses to an instruction given to the model.
19. They utilized these rankings to enhance the model's instruction-following capabilities.
20. Subsequent research has also employed ranking-based feedback for the same task (Askell et al., 2021;Bai et al., 2022a,b).
21. Natural Language Both numerical and rankingbased feedback lack the ability to capture detailed information about problems with the output, which can be crucial for improving generation systems.
22. Instead of asking humans to rank or score outputs, we can instead ask for natural language feedback.
23. In such cases, the feedback typically provides more detailed information, either highlighting the shortcomings of the current output or suggesting specific actions for improvement.
24. For example, Li et al. (2017) asked humans to give natural language feedback to a dialogue question answering model, including positive or negative feedback, but also possibly providing the correct answer to the model or hinting about it.  and  gather natural language feedback on errors present in model-generated graphs and the model's interpretation of a given instruction.
25. Scheurer et al. (2022Scheurer et al. ( , 2023 improve summarization capabilities of language models by asking humans to provide natural language feedback of summaries of the model.
26. Li et al. (2022) collect natural language feedback (in addition to numerical feedback) for responses from a Question Answering (QA) system.
27. Others Besides these feedback types, other (potentially domain-specific) types of feedback can be used to improve model behavior.
28. Commonly humans are asked to provide multi-aspect feedback (X × Y → R d or F d more generally), scoring an output or ranking multiple outputs with respect to multiple dimensions (Böhm et al., 2019;Glaese et al., 2022;Madaan et al., 2023;Nguyen et al., 2022).
29. Post-editions ask humans to provide corrections to the output in the form of small edits (e.g., replace X by Y), and post-edition data has been used to directly improve models (Denkowski et al., 2014) or train automatic post edition systems that correct model mistakes (Pal et al., 2016;Mehta and Goldwasser, 2019;Madaan et al., 2021;Talmor et al., 2020;Elgohary et al., 2021).
30. There are also other feedback types that haven't been fully leveraged to improve generation: e.g., Multidimensional Quality Metrics (MQM) (Lommel et al., 2014b), the standard for evaluating translation quality, asks professional translators to identify errors spans in a translation, alongside severity and type of error."
258426970,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef,s6,Objective,"['p6.0', 'p6.1', 'p6.2', 'p6.3']","[""The purpose of collecting feedback is to align the model's behavior with some (often ill-defined) goal behavior: we might want our summarization model to generate summaries that contain all core information, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate businesscritical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014;Amodei et al., 2016;Bommasani et al., 2021). In addition, Kenton et al. (2021b) discuss some behavioral issues in language agents (natural language generation models) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective."", 'Bai et al. (2022a) explicitly divided the problem of ""aligning"" a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness factors (such as not producing toxic text or providing information that could lead to harm). 2', 'Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a necessary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018;Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream applications. Similarly, in summarization, most works leverage feedback related to aspects such as relevance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instructions (Ouyang et al., 2022): the task of instructionfollowing can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021).', 'Harmlessness Another important alignment objective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (besides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their system, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could increase harmlessness without reducing helpfulness.']","The purpose of collecting feedback is to align the model's behavior with some (often ill-defined) goal behavior: we might want our summarization model to generate summaries that contain all core information, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate businesscritical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014;Amodei et al., 2016;Bommasani et al., 2021). In addition, Kenton et al. (2021b) discuss some behavioral issues in language agents (natural language generation models) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective.

Bai et al. (2022a) explicitly divided the problem of ""aligning"" a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness factors (such as not producing toxic text or providing information that could lead to harm). 2

Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a necessary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018;Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream applications. Similarly, in summarization, most works leverage feedback related to aspects such as relevance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instructions (Ouyang et al., 2022): the task of instructionfollowing can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021).

Harmlessness Another important alignment objective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (besides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their system, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could increase harmlessness without reducing helpfulness.","(p6.0) The purpose of collecting feedback is to align the model's behavior with some (often ill-defined) goal behavior: we might want our summarization model to generate summaries that contain all core information, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate businesscritical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014;Amodei et al., 2016;Bommasani et al., 2021). In addition, Kenton et al. (2021b) discuss some behavioral issues in language agents (natural language generation models) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective.

(p6.1) Bai et al. (2022a) explicitly divided the problem of ""aligning"" a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness factors (such as not producing toxic text or providing information that could lead to harm). 2

(p6.2) Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a necessary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018;Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream applications. Similarly, in summarization, most works leverage feedback related to aspects such as relevance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instructions (Ouyang et al., 2022): the task of instructionfollowing can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021).

(p6.3) Harmlessness Another important alignment objective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (besides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their system, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could increase harmlessness without reducing helpfulness.","[[None, 'b28'], [], ['b24', None, 'b69'], [None, 'b69', 'b5']]","[[None, 'b28'], [], ['b24', None, 'b69'], [None, 'b69', 'b5']]",8,"1. The purpose of collecting feedback is to align the model's behavior with some (often ill-defined) goal behavior: we might want our summarization model to generate summaries that contain all core information, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate businesscritical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses.
2. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014;Amodei et al., 2016;Bommasani et al., 2021).
3. In addition, Kenton et al. (2021b) discuss some behavioral issues in language agents (natural language generation models) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective.
4. Bai et al. (2022a) explicitly divided the problem of ""aligning"" a language model into improving its helpfulness and increasing its harmlessness.
5. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness factors (such as not producing toxic text or providing information that could lead to harm). 2Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a necessary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella.
6. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018;Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream applications.
7. Similarly, in summarization, most works leverage feedback related to aspects such as relevance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary).
8. One particularly well-studied feedback objective is the ability to follow instructions (Ouyang et al., 2022): the task of instructionfollowing can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021).
9. Harmlessness Another important alignment objective is harmlessness: we want our models not to produce certain types of output or violate certain norms.
10. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (besides the overall ability to follow instructions).
11. Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both.
12. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model.
13. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their system, by defining a set of rules and asking humans if the outputs violate these rules.
14. Bai et al. (2022b) showed that feedback produced by LLMs could increase harmlessness without reducing helpfulness."
258426970,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef,s8,Optimizing for Human Feedback,"['p8.0', 'p8.1', 'p8.2', 'p8.3', 'p8.4', 'p8.5', 'p8.6', 'p8.7', 'p8.8', 'p8.9']","['Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be ""optimizable"", i.e., possibly formulated as an optimization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f ∈ R), we can create the following optimization problem:', '( 2) Where D is the distribution of possible inputs. Various techniques have been suggested to optimize the model parameters, θ, using the collected human feedback. These can be divided into three main categories based on the training mechanisms, which we will call feedback-based imitation learning, joint-feedback modeling, and reinforcement learning (RL).', 'The feedback-based imitation learning approach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled generations together with the corresponding inputs, D + . This can be achieved by minimizing the loss:', ""An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model's answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine translation model on a set of positively-labeled translations, and Glaese et al. (2022) performed supervised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), according to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human utterances as targets to fine-tune the model. Scheurer et al. (2022Scheurer et al. ( , 2023 leverage the fact that LLMs can follow instructions and start by collecting natural language human feedback about the model generations, which often describes what an improved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corresponding feedback. The highest similarity refinements for each generation are then used to finetune the LLM. OpenAI's text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disregard the generations which do not receive positive feedback, which may contain useful information to optimize the model. On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural language). Having D as the dataset of inputs x, generations y, and human feedback f collected, this can be achieved by minimizing the following loss of the form"", 'Over all examples in D. These equation can be factorized as', '. Some works simply train the model to predict the feedback given to each generation (Weston, 2016, forward prediction), disregarding the second term of the factorization. One example of this approach is the work of Li et al. (2017), in which the authors asked humans to give natural language feedback (e.g., positive/negative feedback, providing the correct answer to the model, or giving a hint about the correct answer) to a dialogue question answering model. Then, after having collected the feedback, the model is trained to predict it. Hancock et al. (2019) proposed having an auxiliary model predicting the satisfaction of the human speaking with the model. Then, if the satisfaction score is lower than a pre-defined threshold, the model will ask the human for feedback. The model then leverages the natural language feedback humans give by learning to predict it. Yuan et al. (2023); Rafailov et al. (2023) showed that having summarization models predict the rankings of different summaries helps the model generate better summaries, and might even outperform more complicated approaches using feedback models ( §5).', ""Other works train the model to predict the generations and the corresponding human feedback. Xu et al. (2022) proposed using the DIRECTOR model introduced by Arora et al. (2022) to leverage human feedback. As this model has a unified decoderclassifier architecture, Xu et al. (2022) proposed using positively-labeled examples to train its language modeling head (similarly to feedback-based imitation learning) and using both the positive and negatively-labeled examples to train a classifier head that directs the model away from generating undesirable sequences. Thoppilan et al. (2022a) follow this approach to enforce the model's quality and safety. First, they collect dialogues between crowd-workers and the proposed language model LaMDA, which are annotated with feedback provided by the crowd-workers. This feedback states each response's quality (sensible, specific, and interesting) or safety. Then, LaMDA is fine-tuned to predict the high-quality responses and the rewards given to every response regarding its quality attributes and safety. At inference time, LaMDA is also used to filter out candidate responses for which its safety prediction is below a threshold."", 'Finally, this can also be achieved by training the model to predict generation and conditioning on the feedback. This corresponds to minimizing the following loss: Liu et al. (2023) proposed prompt-based finetuning, where they create prompts containing previous generations rated by humans, in the order of preference. They also suggest inserting languagebased feedback (e.g., ""... is a worse answer than ..."") to the prompt, between the generations. Then, the model is fine-tuned to maximize the likelihood of generating the most preferred answer.', ""Finally, reinforcement learning (RL) offers a more versatile approach, allowing for direct optimization of a model's parameters based on human feedback, regardless of the feedback's differentiability. A common RL algorithm used in this context is the REINFORCE algorithm (Williams, 1992), which updates the policy parameters using the following gradient:"", '(7) Here, D represents the set of inputs x, and p θ is the policy. This flexibility enables RL to handle various types of feedback and better align the generated output with human preferences. For instance, Kreutzer et al. (2018) proposed using task-based implicit feedback from user queries as a reward signal to train a machine translation model using a word-level variant of minimum risk training (Shen et al., 2016), while Jaques et al. (2019) used implicit human reactions in chat to improve open-domain dialog systems through off-policy Q-learning (Watkins and Dayan, 1992). Given that collecting human feedback can be expensive and time-consuming, learning is done offline from logged data, which is typically more favorable than on-policy settings that need feedback on the fly. Later in §5.2.1, we discuss several works that attempt to optimize feedback models using RL instead of directly optimizing human feedback. In conjuction, these aproaches are commonly known as Reinforcement Learning from Human Feedback (RLHF).']","Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be ""optimizable"", i.e., possibly formulated as an optimization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f ∈ R), we can create the following optimization problem:

( 2) Where D is the distribution of possible inputs. Various techniques have been suggested to optimize the model parameters, θ, using the collected human feedback. These can be divided into three main categories based on the training mechanisms, which we will call feedback-based imitation learning, joint-feedback modeling, and reinforcement learning (RL).

The feedback-based imitation learning approach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled generations together with the corresponding inputs, D + . This can be achieved by minimizing the loss:

An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model's answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine translation model on a set of positively-labeled translations, and Glaese et al. (2022) performed supervised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), according to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human utterances as targets to fine-tune the model. Scheurer et al. (2022Scheurer et al. ( , 2023 leverage the fact that LLMs can follow instructions and start by collecting natural language human feedback about the model generations, which often describes what an improved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corresponding feedback. The highest similarity refinements for each generation are then used to finetune the LLM. OpenAI's text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disregard the generations which do not receive positive feedback, which may contain useful information to optimize the model. On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural language). Having D as the dataset of inputs x, generations y, and human feedback f collected, this can be achieved by minimizing the following loss of the form

Over all examples in D. These equation can be factorized as

. Some works simply train the model to predict the feedback given to each generation (Weston, 2016, forward prediction), disregarding the second term of the factorization. One example of this approach is the work of Li et al. (2017), in which the authors asked humans to give natural language feedback (e.g., positive/negative feedback, providing the correct answer to the model, or giving a hint about the correct answer) to a dialogue question answering model. Then, after having collected the feedback, the model is trained to predict it. Hancock et al. (2019) proposed having an auxiliary model predicting the satisfaction of the human speaking with the model. Then, if the satisfaction score is lower than a pre-defined threshold, the model will ask the human for feedback. The model then leverages the natural language feedback humans give by learning to predict it. Yuan et al. (2023); Rafailov et al. (2023) showed that having summarization models predict the rankings of different summaries helps the model generate better summaries, and might even outperform more complicated approaches using feedback models ( §5).

Other works train the model to predict the generations and the corresponding human feedback. Xu et al. (2022) proposed using the DIRECTOR model introduced by Arora et al. (2022) to leverage human feedback. As this model has a unified decoderclassifier architecture, Xu et al. (2022) proposed using positively-labeled examples to train its language modeling head (similarly to feedback-based imitation learning) and using both the positive and negatively-labeled examples to train a classifier head that directs the model away from generating undesirable sequences. Thoppilan et al. (2022a) follow this approach to enforce the model's quality and safety. First, they collect dialogues between crowd-workers and the proposed language model LaMDA, which are annotated with feedback provided by the crowd-workers. This feedback states each response's quality (sensible, specific, and interesting) or safety. Then, LaMDA is fine-tuned to predict the high-quality responses and the rewards given to every response regarding its quality attributes and safety. At inference time, LaMDA is also used to filter out candidate responses for which its safety prediction is below a threshold.

Finally, this can also be achieved by training the model to predict generation and conditioning on the feedback. This corresponds to minimizing the following loss: Liu et al. (2023) proposed prompt-based finetuning, where they create prompts containing previous generations rated by humans, in the order of preference. They also suggest inserting languagebased feedback (e.g., ""... is a worse answer than ..."") to the prompt, between the generations. Then, the model is fine-tuned to maximize the likelihood of generating the most preferred answer.

Finally, reinforcement learning (RL) offers a more versatile approach, allowing for direct optimization of a model's parameters based on human feedback, regardless of the feedback's differentiability. A common RL algorithm used in this context is the REINFORCE algorithm (Williams, 1992), which updates the policy parameters using the following gradient:

(7) Here, D represents the set of inputs x, and p θ is the policy. This flexibility enables RL to handle various types of feedback and better align the generated output with human preferences. For instance, Kreutzer et al. (2018) proposed using task-based implicit feedback from user queries as a reward signal to train a machine translation model using a word-level variant of minimum risk training (Shen et al., 2016), while Jaques et al. (2019) used implicit human reactions in chat to improve open-domain dialog systems through off-policy Q-learning (Watkins and Dayan, 1992). Given that collecting human feedback can be expensive and time-consuming, learning is done offline from logged data, which is typically more favorable than on-policy settings that need feedback on the fly. Later in §5.2.1, we discuss several works that attempt to optimize feedback models using RL instead of directly optimizing human feedback. In conjuction, these aproaches are commonly known as Reinforcement Learning from Human Feedback (RLHF).","(p8.0) Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be ""optimizable"", i.e., possibly formulated as an optimization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f ∈ R), we can create the following optimization problem:

(p8.1) ( 2) Where D is the distribution of possible inputs. Various techniques have been suggested to optimize the model parameters, θ, using the collected human feedback. These can be divided into three main categories based on the training mechanisms, which we will call feedback-based imitation learning, joint-feedback modeling, and reinforcement learning (RL).

(p8.2) The feedback-based imitation learning approach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled generations together with the corresponding inputs, D + . This can be achieved by minimizing the loss:

(p8.3) An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model's answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine translation model on a set of positively-labeled translations, and Glaese et al. (2022) performed supervised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), according to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human utterances as targets to fine-tune the model. Scheurer et al. (2022Scheurer et al. ( , 2023 leverage the fact that LLMs can follow instructions and start by collecting natural language human feedback about the model generations, which often describes what an improved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corresponding feedback. The highest similarity refinements for each generation are then used to finetune the LLM. OpenAI's text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disregard the generations which do not receive positive feedback, which may contain useful information to optimize the model. On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural language). Having D as the dataset of inputs x, generations y, and human feedback f collected, this can be achieved by minimizing the following loss of the form

(p8.4) Over all examples in D. These equation can be factorized as

(p8.5) . Some works simply train the model to predict the feedback given to each generation (Weston, 2016, forward prediction), disregarding the second term of the factorization. One example of this approach is the work of Li et al. (2017), in which the authors asked humans to give natural language feedback (e.g., positive/negative feedback, providing the correct answer to the model, or giving a hint about the correct answer) to a dialogue question answering model. Then, after having collected the feedback, the model is trained to predict it. Hancock et al. (2019) proposed having an auxiliary model predicting the satisfaction of the human speaking with the model. Then, if the satisfaction score is lower than a pre-defined threshold, the model will ask the human for feedback. The model then leverages the natural language feedback humans give by learning to predict it. Yuan et al. (2023); Rafailov et al. (2023) showed that having summarization models predict the rankings of different summaries helps the model generate better summaries, and might even outperform more complicated approaches using feedback models ( §5).

(p8.6) Other works train the model to predict the generations and the corresponding human feedback. Xu et al. (2022) proposed using the DIRECTOR model introduced by Arora et al. (2022) to leverage human feedback. As this model has a unified decoderclassifier architecture, Xu et al. (2022) proposed using positively-labeled examples to train its language modeling head (similarly to feedback-based imitation learning) and using both the positive and negatively-labeled examples to train a classifier head that directs the model away from generating undesirable sequences. Thoppilan et al. (2022a) follow this approach to enforce the model's quality and safety. First, they collect dialogues between crowd-workers and the proposed language model LaMDA, which are annotated with feedback provided by the crowd-workers. This feedback states each response's quality (sensible, specific, and interesting) or safety. Then, LaMDA is fine-tuned to predict the high-quality responses and the rewards given to every response regarding its quality attributes and safety. At inference time, LaMDA is also used to filter out candidate responses for which its safety prediction is below a threshold.

(p8.7) Finally, this can also be achieved by training the model to predict generation and conditioning on the feedback. This corresponds to minimizing the following loss: Liu et al. (2023) proposed prompt-based finetuning, where they create prompts containing previous generations rated by humans, in the order of preference. They also suggest inserting languagebased feedback (e.g., ""... is a worse answer than ..."") to the prompt, between the generations. Then, the model is fine-tuned to maximize the likelihood of generating the most preferred answer.

(p8.8) Finally, reinforcement learning (RL) offers a more versatile approach, allowing for direct optimization of a model's parameters based on human feedback, regardless of the feedback's differentiability. A common RL algorithm used in this context is the REINFORCE algorithm (Williams, 1992), which updates the policy parameters using the following gradient:

(p8.9) (7) Here, D represents the set of inputs x, and p θ is the policy. This flexibility enables RL to handle various types of feedback and better align the generated output with human preferences. For instance, Kreutzer et al. (2018) proposed using task-based implicit feedback from user queries as a reward signal to train a machine translation model using a word-level variant of minimum risk training (Shen et al., 2016), while Jaques et al. (2019) used implicit human reactions in chat to improve open-domain dialog systems through off-policy Q-learning (Watkins and Dayan, 1992). Given that collecting human feedback can be expensive and time-consuming, learning is done offline from logged data, which is typically more favorable than on-policy settings that need feedback on the fly. Later in §5.2.1, we discuss several works that attempt to optimize feedback models using RL instead of directly optimizing human feedback. In conjuction, these aproaches are commonly known as Reinforcement Learning from Human Feedback (RLHF).","[[], [], [], ['b72', 'b29', 'b71', 'b24', 'b5'], [], ['b29', 'b59', None, 'b10'], ['b94', 'b107'], ['b32'], ['b104'], ['b77', 'b24', 'b99', 'b17']]","[[], [], [], ['b72', 'b29', 'b71', 'b24', 'b5'], [], ['b29', 'b59', None, 'b10'], ['b94', 'b107'], ['b32'], ['b104'], ['b77', 'b24', 'b99', 'b17']]",17,"1. Once human feedback has been collected, one way to use it is by optimizing the model parameters directly.
2. However, this requires the feedback to be ""optimizable"", i.e., possibly formulated as an optimization problem based on which we can obtain an improved model.
3. For instance, if the feedback is a numerical score (f ∈ R), we can create the following optimization problem:( 2)
4. Where D is the distribution of possible inputs.
5. Various techniques have been suggested to optimize the model parameters, θ, using the collected human feedback.
6. These can be divided into three main categories based on the training mechanisms, which we will call feedback-based imitation learning, joint-feedback modeling, and reinforcement learning (RL).
7. The feedback-based imitation learning approach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled generations together with the corresponding inputs, D + .
8. This can be achieved by minimizing the loss:An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model's answers labeled as correct by humans.
9. Similarly, Kreutzer et al. (2018) trained a machine translation model on a set of positively-labeled translations, and Glaese et al. (2022) performed supervised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), according to humans.
10. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human utterances as targets to fine-tune the model.
11. Scheurer et al. (2022Scheurer et al. ( , 2023 leverage the fact that LLMs can follow instructions and start by collecting natural language human feedback about the model generations, which often describes what an improved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corresponding feedback. The highest similarity refinements for each generation are then used to finetune the LLM. OpenAI's text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b).
12. A downside of these approaches is that they disregard the generations which do not receive positive feedback, which may contain useful information to optimize the model.
13. On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model.
14. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural language).
15. Having D as the dataset of inputs x, generations y, and human feedback f collected, this can be achieved by minimizing the following loss of the formOver all examples in D. These equation can be factorized as.
16. Some works simply train the model to predict the feedback given to each generation (Weston, 2016, forward prediction), disregarding the second term of the factorization.
17. One example of this approach is the work of Li et al. (2017), in which the authors asked humans to give natural language feedback (e.g., positive/negative feedback, providing the correct answer to the model, or giving a hint about the correct answer) to a dialogue question answering model.
18. Then, after having collected the feedback, the model is trained to predict it.
19. Hancock et al. (2019) proposed having an auxiliary model predicting the satisfaction of the human speaking with the model.
20. Then, if the satisfaction score is lower than a pre-defined threshold, the model will ask the human for feedback.
21. The model then leverages the natural language feedback humans give by learning to predict it.
22. Yuan et al. (2023); Rafailov et al. (2023) showed that having summarization models predict the rankings of different summaries helps the model generate better summaries, and might even outperform more complicated approaches using feedback models ( §5).
23. Other works train the model to predict the generations and the corresponding human feedback.
24. Xu et al. (2022) proposed using the DIRECTOR model introduced by Arora et al. (2022) to leverage human feedback.
25. As this model has a unified decoderclassifier architecture, Xu et al. (2022) proposed using positively-labeled examples to train its language modeling head (similarly to feedback-based imitation learning) and using both the positive and negatively-labeled examples to train a classifier head that directs the model away from generating undesirable sequences.
26. Thoppilan et al. (2022a) follow this approach to enforce the model's quality and safety.
27. First, they collect dialogues between crowd-workers and the proposed language model LaMDA, which are annotated with feedback provided by the crowd-workers.
28. This feedback states each response's quality (sensible, specific, and interesting) or safety.
29. Then, LaMDA is fine-tuned to predict the high-quality responses and the rewards given to every response regarding its quality attributes and safety.
30. At inference time, LaMDA is also used to filter out candidate responses for which its safety prediction is below a threshold.
31. Finally, this can also be achieved by training the model to predict generation and conditioning on the feedback.
32. This corresponds to minimizing the following loss: Liu et al. (2023) proposed prompt-based finetuning, where they create prompts containing previous generations rated by humans, in the order of preference.
33. They also suggest inserting languagebased feedback (e.g., ""... is a worse answer than ..."") to the prompt, between the generations.
34. Then, the model is fine-tuned to maximize the likelihood of generating the most preferred answer.
35. Finally, reinforcement learning (RL) offers a more versatile approach, allowing for direct optimization of a model's parameters based on human feedback, regardless of the feedback's differentiability.
36. A common RL algorithm used in this context is the REINFORCE algorithm (Williams, 1992), which updates the policy parameters using the following gradient:
37. (7) Here, D represents the set of inputs x, and p θ is the policy.
38. This flexibility enables RL to handle various types of feedback and better align the generated output with human preferences.
39. For instance, Kreutzer et al. (2018) proposed using task-based implicit feedback from user queries as a reward signal to train a machine translation model using a word-level variant of minimum risk training (Shen et al., 2016), while Jaques et al. (2019) used implicit human reactions in chat to improve open-domain dialog systems through off-policy Q-learning (Watkins and Dayan, 1992).
40. Given that collecting human feedback can be expensive and time-consuming, learning is done offline from logged data, which is typically more favorable than on-policy settings that need feedback on the fly.
41. Later in §5.2.1, we discuss several works that attempt to optimize feedback models using RL instead of directly optimizing human feedback.
42. In conjuction, these aproaches are commonly known as Reinforcement Learning from Human Feedback (RLHF)."
258426970,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef,s9,Decoding with Human Feedback,"['p9.0', 'p9.1', 'p9.2', 'p9.3']","[""While directly optimizing model parameters provides greater control, modifying them may not always be feasible, particularly in the case of LLMs. Additionally, feedback might be unavailable during model training, limiting the scope for parameter adjustments. In such cases, leveraging human feedback during decoding plays a critical role in enhancing LLMs's performance. This type of feedback, derived from interactions between LLMs and users in practical scenarios, enables models to learn from their errors and offers opportunities for ongoing refinement without altering model parameters. In addition, the feedback functions as a guiding mechanism, allowing the model to generate more desirable outputs by leveraging its existing capabilities."", 'There are two broad categories in which human feedback is used in this setup: 1. Feedback Memory: Feedback Memory Utilization involves maintaining a repository of feedback from prior sessions. Then, when processing new inputs, the system uses relevant feedback from similar inputs in its memory to guide the model toward generating more desirable outputs based on past experiences and user preferences. While a classical concept (Riesbeck, 1981;Schank, 1983), recent work has shown the promise of such a memoryaugmented approach in both finetuning (Weston et al., 2014;Wu et al., 2018; and few-shot setups .', ""2. Iterative Output Refinement: This method employs human feedback to refine the model's output iteratively. Users can provide feedback on intermediate responses, enabling the model to adjust its output until it meets the user's satisfaction. This process allows the model to better understand user preferences and produce more suitable outcomes (Reid and Neubig, 2022;Saunders et al., 2022;Schick et al., 2022;Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs."", 'These two techniques are not mutually exclusive and can be combined to achieve even better performance, creating a more adaptive and responsive system that caters to user expectations.']","While directly optimizing model parameters provides greater control, modifying them may not always be feasible, particularly in the case of LLMs. Additionally, feedback might be unavailable during model training, limiting the scope for parameter adjustments. In such cases, leveraging human feedback during decoding plays a critical role in enhancing LLMs's performance. This type of feedback, derived from interactions between LLMs and users in practical scenarios, enables models to learn from their errors and offers opportunities for ongoing refinement without altering model parameters. In addition, the feedback functions as a guiding mechanism, allowing the model to generate more desirable outputs by leveraging its existing capabilities.

There are two broad categories in which human feedback is used in this setup: 1. Feedback Memory: Feedback Memory Utilization involves maintaining a repository of feedback from prior sessions. Then, when processing new inputs, the system uses relevant feedback from similar inputs in its memory to guide the model toward generating more desirable outputs based on past experiences and user preferences. While a classical concept (Riesbeck, 1981;Schank, 1983), recent work has shown the promise of such a memoryaugmented approach in both finetuning (Weston et al., 2014;Wu et al., 2018; and few-shot setups .

2. Iterative Output Refinement: This method employs human feedback to refine the model's output iteratively. Users can provide feedback on intermediate responses, enabling the model to adjust its output until it meets the user's satisfaction. This process allows the model to better understand user preferences and produce more suitable outcomes (Reid and Neubig, 2022;Saunders et al., 2022;Schick et al., 2022;Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs.

These two techniques are not mutually exclusive and can be combined to achieve even better performance, creating a more adaptive and responsive system that caters to user expectations.","(p9.0) While directly optimizing model parameters provides greater control, modifying them may not always be feasible, particularly in the case of LLMs. Additionally, feedback might be unavailable during model training, limiting the scope for parameter adjustments. In such cases, leveraging human feedback during decoding plays a critical role in enhancing LLMs's performance. This type of feedback, derived from interactions between LLMs and users in practical scenarios, enables models to learn from their errors and offers opportunities for ongoing refinement without altering model parameters. In addition, the feedback functions as a guiding mechanism, allowing the model to generate more desirable outputs by leveraging its existing capabilities.

(p9.1) There are two broad categories in which human feedback is used in this setup: 1. Feedback Memory: Feedback Memory Utilization involves maintaining a repository of feedback from prior sessions. Then, when processing new inputs, the system uses relevant feedback from similar inputs in its memory to guide the model toward generating more desirable outputs based on past experiences and user preferences. While a classical concept (Riesbeck, 1981;Schank, 1983), recent work has shown the promise of such a memoryaugmented approach in both finetuning (Weston et al., 2014;Wu et al., 2018; and few-shot setups .

(p9.2) 2. Iterative Output Refinement: This method employs human feedback to refine the model's output iteratively. Users can provide feedback on intermediate responses, enabling the model to adjust its output until it meets the user's satisfaction. This process allows the model to better understand user preferences and produce more suitable outcomes (Reid and Neubig, 2022;Saunders et al., 2022;Schick et al., 2022;Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs.

(p9.3) These two techniques are not mutually exclusive and can be combined to achieve even better performance, creating a more adaptive and responsive system that caters to user expectations.","[[], ['b63', None, 'b70', 'b105'], [None, 'b69', 'b49', 'b62'], []]","[[], ['b63', None, 'b70', 'b105'], [None, 'b69', 'b49', 'b62'], []]",8,"1. While directly optimizing model parameters provides greater control, modifying them may not always be feasible, particularly in the case of LLMs.
2. Additionally, feedback might be unavailable during model training, limiting the scope for parameter adjustments.
3. In such cases, leveraging human feedback during decoding plays a critical role in enhancing LLMs's performance.
4. This type of feedback, derived from interactions between LLMs and users in practical scenarios, enables models to learn from their errors and offers opportunities for ongoing refinement without altering model parameters.
5. In addition, the feedback functions as a guiding mechanism, allowing the model to generate more desirable outputs by leveraging its existing capabilities.
6. There are two broad categories in which human feedback is used in this setup: 1.
7. Feedback Memory: Feedback Memory Utilization involves maintaining a repository of feedback from prior sessions.
8. Then, when processing new inputs, the system uses relevant feedback from similar inputs in its memory to guide the model toward generating more desirable outputs based on past experiences and user preferences.
9. While a classical concept (Riesbeck, 1981;Schank, 1983), recent work has shown the promise of such a memoryaugmented approach in both finetuning (Weston et al., 2014;Wu et al., 2018; and few-shot setups .
10. 2. Iterative Output Refinement: This method employs human feedback to refine the model's output iteratively.
11. Users can provide feedback on intermediate responses, enabling the model to adjust its output until it meets the user's satisfaction.
12. This process allows the model to better understand user preferences and produce more suitable outcomes (Reid and Neubig, 2022;Saunders et al., 2022;Schick et al., 2022;Nijkamp et al., 2022).
13. Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs.
14. These two techniques are not mutually exclusive and can be combined to achieve even better performance, creating a more adaptive and responsive system that caters to user expectations."
258426970,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef,s20,Ethical considerations,"['p20.0', 'p20.1']","['Some subjectivity in annotator judgment can arise from differences across cultural or social groups. Santurkar et al. (2023) measure opinions in language model generations, demonstrating varying degrees of representation of demographic groups. Several works observe that tuning with human feedback increases the alignment of generated outputs with US liberal views on controversial topics (Perez et al. (2022b), Hartmann et al. (2023). Annotators with different demographic or political backgrounds may disagree on what qualifies as toxic content (Sap et al. (2022), Ding et al. (2022). This is particularly pronounced when annotators are asked to make ethical judgments, which may vary with cultural context and personal sensibilities (Jiang et al. (2022), Talat et al. (2022)). Steiger et al. (2021) survey moderators of toxic content, identifying harms ranging from slight discomfort to lasting psychological harm from the prolonged performance of content moderation tasks; however, the severity and frequency of toxic content examined in content moderation likely exceeds that in other types of human feedback annotation. Shmueli et al. (2021) identify toxicity classification and generation from open-ended inputs as two NLP annotation tasks that may trigger harmful responses in annotators. They further argue that this moves beyond the ""minimal risk"" requirement for Institutional Review Board exemption in the United States and encourage academic researchers using crowdworker annotation to file for this ethical review of their work.', 'Media attention has also focused on fair pay for annotators, with one TIME article 6 describing annotators paid $2 USD or less per hour to review toxic content and provide harmfulness annotations for model training. Research on crowdsourcing (Shmueli et al. (2021); Rothschild et al. (2022); Soratana et al. (2022);Toxtli et al. (2021); Hornuf and Vrankar (2022)) cautions that inadequate pay, especially for workers in lower-resourced regions, can be a form of worker exploitation.']","Some subjectivity in annotator judgment can arise from differences across cultural or social groups. Santurkar et al. (2023) measure opinions in language model generations, demonstrating varying degrees of representation of demographic groups. Several works observe that tuning with human feedback increases the alignment of generated outputs with US liberal views on controversial topics (Perez et al. (2022b), Hartmann et al. (2023). Annotators with different demographic or political backgrounds may disagree on what qualifies as toxic content (Sap et al. (2022), Ding et al. (2022). This is particularly pronounced when annotators are asked to make ethical judgments, which may vary with cultural context and personal sensibilities (Jiang et al. (2022), Talat et al. (2022)). Steiger et al. (2021) survey moderators of toxic content, identifying harms ranging from slight discomfort to lasting psychological harm from the prolonged performance of content moderation tasks; however, the severity and frequency of toxic content examined in content moderation likely exceeds that in other types of human feedback annotation. Shmueli et al. (2021) identify toxicity classification and generation from open-ended inputs as two NLP annotation tasks that may trigger harmful responses in annotators. They further argue that this moves beyond the ""minimal risk"" requirement for Institutional Review Board exemption in the United States and encourage academic researchers using crowdworker annotation to file for this ethical review of their work.

Media attention has also focused on fair pay for annotators, with one TIME article 6 describing annotators paid $2 USD or less per hour to review toxic content and provide harmfulness annotations for model training. Research on crowdsourcing (Shmueli et al. (2021); Rothschild et al. (2022); Soratana et al. (2022);Toxtli et al. (2021); Hornuf and Vrankar (2022)) cautions that inadequate pay, especially for workers in lower-resourced regions, can be a form of worker exploitation.","(p20.0) Some subjectivity in annotator judgment can arise from differences across cultural or social groups. Santurkar et al. (2023) measure opinions in language model generations, demonstrating varying degrees of representation of demographic groups. Several works observe that tuning with human feedback increases the alignment of generated outputs with US liberal views on controversial topics (Perez et al. (2022b), Hartmann et al. (2023). Annotators with different demographic or political backgrounds may disagree on what qualifies as toxic content (Sap et al. (2022), Ding et al. (2022). This is particularly pronounced when annotators are asked to make ethical judgments, which may vary with cultural context and personal sensibilities (Jiang et al. (2022), Talat et al. (2022)). Steiger et al. (2021) survey moderators of toxic content, identifying harms ranging from slight discomfort to lasting psychological harm from the prolonged performance of content moderation tasks; however, the severity and frequency of toxic content examined in content moderation likely exceeds that in other types of human feedback annotation. Shmueli et al. (2021) identify toxicity classification and generation from open-ended inputs as two NLP annotation tasks that may trigger harmful responses in annotators. They further argue that this moves beyond the ""minimal risk"" requirement for Institutional Review Board exemption in the United States and encourage academic researchers using crowdworker annotation to file for this ethical review of their work.

(p20.1) Media attention has also focused on fair pay for annotators, with one TIME article 6 describing annotators paid $2 USD or less per hour to review toxic content and provide harmfulness annotations for model training. Research on crowdsourcing (Shmueli et al. (2021); Rothschild et al. (2022); Soratana et al. (2022);Toxtli et al. (2021); Hornuf and Vrankar (2022)) cautions that inadequate pay, especially for workers in lower-resourced regions, can be a form of worker exploitation.","[['b89', 'b86', 'b82', None, 'b11', 'b68', 'b67'], ['b65', 'b85', 'b82', 'b14', None]]","[['b89', 'b86', 'b82', None, 'b11', 'b68', 'b67'], ['b65', 'b85', 'b82', 'b14', None]]",12,"1. Some subjectivity in annotator judgment can arise from differences across cultural or social groups.
2. Santurkar et al. (2023) measure opinions in language model generations, demonstrating varying degrees of representation of demographic groups.
3. Several works observe that tuning with human feedback increases the alignment of generated outputs with US liberal views on controversial topics (Perez et al. (2022b), Hartmann et al. (2023).
4. Annotators with different demographic or political backgrounds may disagree on what qualifies as toxic content (Sap et al. (2022), Ding et al. (2022).
5. This is particularly pronounced when annotators are asked to make ethical judgments, which may vary with cultural context and personal sensibilities (Jiang et al. (2022), Talat et al. (2022)).
6. Steiger et al. (2021) survey moderators of toxic content, identifying harms ranging from slight discomfort to lasting psychological harm from the prolonged performance of content moderation tasks; however, the severity and frequency of toxic content examined in content moderation likely exceeds that in other types of human feedback annotation.
7. Shmueli et al. (2021) identify toxicity classification and generation from open-ended inputs as two NLP annotation tasks that may trigger harmful responses in annotators.
8. They further argue that this moves beyond the ""minimal risk"" requirement for Institutional Review Board exemption in the United States and encourage academic researchers using crowdworker annotation to file for this ethical review of their work.
9. Media attention has also focused on fair pay for annotators, with one TIME article 6 describing annotators paid $2 USD or less per hour to review toxic content and provide harmfulness annotations for model training.
10. Research on crowdsourcing (Shmueli et al. (2021); Rothschild et al. (2022); Soratana et al. (2022);Toxtli et al. (2021); Hornuf and Vrankar (2022)) cautions that inadequate pay, especially for workers in lower-resourced regions, can be a form of worker exploitation."
258426970,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef,s11,Learning Models of Human Feedback,"['p11.0', 'p11.1', 'p11.2', 'p11.3', 'p11.4', 'p11.5']","['An alternative approach to obtaining human feedback is to develop models that can predict or ap-proximate it. Although these models may not be perfect, they offer the advantage of providing feedback at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X × Y 1 × · · · × Y n → F, we want to learn a parametric (numerical) feedback modelĥ ϕ : X × Y → R (with parameters ϕ) that ""agrees"" with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss:', 'L(ϕ) = loss ĥ ϕ (x, y 1 ), · · · , h(x, y 1:n )', 'For example, if the feedback function we are trying to model is also numerical (h : X × Y → R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(ϕ) = ĥ ϕ (x, y) − h(x, y) 2 . Importantly, while the feedback model is (generally) numerical, the human feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3ĥ ϕ (x, y n ) on ranking-based feedback, using a loss of the form L(ϕ) = log σ ĥ ϕ (x, y +1 ) −ĥ ϕ (x, y −1 )', '(10) such that sample y +1 was preferred to y −1 for the same input x: h(x, y −1 , y +1 ) = (y −1 < y +1 ). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022;Askell et al., 2021;Qin et al., 2022;Yuan et al., 2023).', 'The problem of feedback modeling has been studied extensively in the context of metric learning for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to compute similarity scores between the generated text or code snippets and their references. In MT, Sellam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged annotated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary-level metric from a set of human judgements included in older summarization datasets (e.g., TAC-2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these reward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in §5.2.', 'Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (Böhm et al. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally occurring implicit feedback, such as from user interactions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feedback, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminishing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed objectives: whether the summary has an appropriate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correcting the output of an MT system, can also be seen as feedback models (albeit non-numerical).']","An alternative approach to obtaining human feedback is to develop models that can predict or ap-proximate it. Although these models may not be perfect, they offer the advantage of providing feedback at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X × Y 1 × · · · × Y n → F, we want to learn a parametric (numerical) feedback modelĥ ϕ : X × Y → R (with parameters ϕ) that ""agrees"" with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss:

L(ϕ) = loss ĥ ϕ (x, y 1 ), · · · , h(x, y 1:n )

For example, if the feedback function we are trying to model is also numerical (h : X × Y → R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(ϕ) = ĥ ϕ (x, y) − h(x, y) 2 . Importantly, while the feedback model is (generally) numerical, the human feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3ĥ ϕ (x, y n ) on ranking-based feedback, using a loss of the form L(ϕ) = log σ ĥ ϕ (x, y +1 ) −ĥ ϕ (x, y −1 )

(10) such that sample y +1 was preferred to y −1 for the same input x: h(x, y −1 , y +1 ) = (y −1 < y +1 ). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022;Askell et al., 2021;Qin et al., 2022;Yuan et al., 2023).

The problem of feedback modeling has been studied extensively in the context of metric learning for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to compute similarity scores between the generated text or code snippets and their references. In MT, Sellam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged annotated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary-level metric from a set of human judgements included in older summarization datasets (e.g., TAC-2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these reward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in §5.2.

Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (Böhm et al. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally occurring implicit feedback, such as from user interactions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feedback, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminishing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed objectives: whether the summary has an appropriate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correcting the output of an MT system, can also be seen as feedback models (albeit non-numerical).","(p11.0) An alternative approach to obtaining human feedback is to develop models that can predict or ap-proximate it. Although these models may not be perfect, they offer the advantage of providing feedback at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X × Y 1 × · · · × Y n → F, we want to learn a parametric (numerical) feedback modelĥ ϕ : X × Y → R (with parameters ϕ) that ""agrees"" with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss:

(p11.1) L(ϕ) = loss ĥ ϕ (x, y 1 ), · · · , h(x, y 1:n )

(p11.2) For example, if the feedback function we are trying to model is also numerical (h : X × Y → R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(ϕ) = ĥ ϕ (x, y) − h(x, y) 2 . Importantly, while the feedback model is (generally) numerical, the human feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3ĥ ϕ (x, y n ) on ranking-based feedback, using a loss of the form L(ϕ) = log σ ĥ ϕ (x, y +1 ) −ĥ ϕ (x, y −1 )

(p11.3) (10) such that sample y +1 was preferred to y −1 for the same input x: h(x, y −1 , y +1 ) = (y −1 < y +1 ). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022;Askell et al., 2021;Qin et al., 2022;Yuan et al., 2023).

(p11.4) The problem of feedback modeling has been studied extensively in the context of metric learning for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to compute similarity scores between the generated text or code snippets and their references. In MT, Sellam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged annotated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary-level metric from a set of human judgements included in older summarization datasets (e.g., TAC-2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these reward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in §5.2.

(p11.5) Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (Böhm et al. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally occurring implicit feedback, such as from user interactions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feedback, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminishing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed objectives: whether the summary has an appropriate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correcting the output of an MT system, can also be seen as feedback models (albeit non-numerical).","[[], [], ['b87'], [None, 'b69', 'b58'], ['b55', 'b60', 'b113', 'b115', None], [None, 'b83', 'b42']]","[[], [], ['b87'], [None, 'b69', 'b58'], ['b55', 'b60', 'b113', 'b115', None], [None, 'b83', 'b42']]",12,"1. An alternative approach to obtaining human feedback is to develop models that can predict or ap-proximate it.
2. Although these models may not be perfect, they offer the advantage of providing feedback at a low cost after training, thereby enabling the scaling of feedback-dependent techniques.
3. More formally, given a feedback function h : X ×
4. Y 1 × · · · × Y n → F, we want to learn a parametric (numerical) feedback modelĥ ϕ : X ×
5. Y → R (with parameters ϕ) that ""agrees"" with human feedback.
6. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss:
7. L(ϕ) = loss ĥ ϕ (x, y 1 ), · · · , h(x, y 1:n )For example, if the feedback function we are trying to model is also numerical (h : X × Y → R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback
8. L(ϕ) = ĥ ϕ (x, y) − h(x, y) 2 .
9. Importantly, while the feedback model is (generally) numerical, the human feedback can be in any other format, as long as a suitable loss function can be specified.
10. Stiennon et al. (2020) train preference models 3ĥ ϕ (x, y n ) on ranking-based feedback, using a loss of the form
11. L(ϕ) = log σ ĥ ϕ (x, y +1 ) −ĥ ϕ (x, y −1 )(10) such that sample y +1 was preferred to y −1 for the same input x: h(x, y −1 , y +1 ) = (y −1 < y +1 ).
12. Variants of this loss have subsequently been used in other works (Ouyang et al., 2022;Askell et al., 2021;Qin et al., 2022;Yuan et al., 2023).
13. The problem of feedback modeling has been studied extensively in the context of metric learning for NLP.
14. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to compute similarity scores between the generated text or code snippets and their references.
15. In MT, Sellam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality.
16. For summarization, Zopf (2018) leveraged annotated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary-level metric from a set of human judgements included in older summarization datasets (e.g., TAC-2008).
17. These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b).
18. It is notable that these reward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in §5.2.
19. Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (Böhm et al. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally occurring implicit feedback, such as from user interactions on websites (e.g., Reddit, StackOverflow).
20. Though less accurate than explicitly-collected feedback, it allows feedback models to be trained on much more data.
21. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminishing returns when the number of explicit-collected feedback increases.
22. Nguyen et al. (2022) train a preference model based on rankings on three human-designed objectives: whether the summary has an appropriate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss.
23. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correcting the output of an MT system, can also be seen as feedback models (albeit non-numerical)."
258557362,Large Language Models Meet NL2Code: A Survey,Computer Science,https://www.semanticscholar.org/paper/4f939f0751e5484f54089f6a97598e39afdcb3b5,s6,Benchmarks and Metrics,"['p6.0', 'p6.1', 'p6.2', 'p6.3']","['To evaluate the NL2Code task, high-quality benchmarks and reliable metrics are fundamental and essential. In this section, we provide a brief overview of current benchmarks and metrics, as well as our observations and the open challenges.', 'We summarize 17 well-studied NL2Code benchmarks in Table 3, where we can find that each of these benchmarks has its own characteristics regarding size, language, complexity, and scenario. We observe that most benchmarks contain a limited number of instances. For example, the widely used HumanEval and MBPP have 164 and 974 instances, respectively. This is because these benchmarks are typically hand-written to ensure that LLMs have not seen them during training. In the era of large language models, it is crucial to avoid data leak-age when creating new benchmarks. Additionally, most current benchmarks have their problem descriptions in English and code solutions in Python.', 'Recently, several multi-lingual benchmarks have been proposed, such as MBXP (Athiwaratkun et al., 2022), HumanEvalX (Zheng et al., 2023), and Mul-tiPL (Cassano et al., 2022), which cover multiple programming languages, and ODEX (Wang et al., 2022c), which covers multiple natural languages. Details of multi-lingual benchmarks are listed in Appendix Table 7. Furthermore, benchmarks have been proposed for other practical scenarios, such as data science (Lai et al., 2022), public library (Zan et al., 2022b), private library (Zan et al., 2022a), multi-turn program synthesis (Nijkamp et al., 2023), and code security (Siddiq and msiddiq, 2022). For execution-based benchmarks, comprehensive test cases with complete coverage of the generated program can ensure the trustworthiness of evaluation results. As a reference, the average number of test cases for each benchmark, as well as the length statistics of the problem descriptions and solutions are also provided in Table 3.', 'Manually evaluating the generated code is impractical, which calls for the need for automatic metrics. The above mentioned benchmarks all provide test cases for execution-based evaluation, where metrics such as pass@k (Chen et al., 2021), n@k (Li et al., 2022b), test case aver-age (Hendrycks et al., 2021), and execution accuracy (Rajkumar et al., 2022) can be used. However, this approach has stringent requirements for the quality of test cases and can only evaluate executable code. For non-executable code, metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), andCodeBLEU (Ren et al., 2020) are used, while they can not precisely evaluate the correctness of the code. So far, there are many open challenges in designing metrics to evaluate various aspects of code, such as vulnerability, maintainability, clarity, execution complexity, and stability.']","To evaluate the NL2Code task, high-quality benchmarks and reliable metrics are fundamental and essential. In this section, we provide a brief overview of current benchmarks and metrics, as well as our observations and the open challenges.

We summarize 17 well-studied NL2Code benchmarks in Table 3, where we can find that each of these benchmarks has its own characteristics regarding size, language, complexity, and scenario. We observe that most benchmarks contain a limited number of instances. For example, the widely used HumanEval and MBPP have 164 and 974 instances, respectively. This is because these benchmarks are typically hand-written to ensure that LLMs have not seen them during training. In the era of large language models, it is crucial to avoid data leak-age when creating new benchmarks. Additionally, most current benchmarks have their problem descriptions in English and code solutions in Python.

Recently, several multi-lingual benchmarks have been proposed, such as MBXP (Athiwaratkun et al., 2022), HumanEvalX (Zheng et al., 2023), and Mul-tiPL (Cassano et al., 2022), which cover multiple programming languages, and ODEX (Wang et al., 2022c), which covers multiple natural languages. Details of multi-lingual benchmarks are listed in Appendix Table 7. Furthermore, benchmarks have been proposed for other practical scenarios, such as data science (Lai et al., 2022), public library (Zan et al., 2022b), private library (Zan et al., 2022a), multi-turn program synthesis (Nijkamp et al., 2023), and code security (Siddiq and msiddiq, 2022). For execution-based benchmarks, comprehensive test cases with complete coverage of the generated program can ensure the trustworthiness of evaluation results. As a reference, the average number of test cases for each benchmark, as well as the length statistics of the problem descriptions and solutions are also provided in Table 3.

Manually evaluating the generated code is impractical, which calls for the need for automatic metrics. The above mentioned benchmarks all provide test cases for execution-based evaluation, where metrics such as pass@k (Chen et al., 2021), n@k (Li et al., 2022b), test case aver-age (Hendrycks et al., 2021), and execution accuracy (Rajkumar et al., 2022) can be used. However, this approach has stringent requirements for the quality of test cases and can only evaluate executable code. For non-executable code, metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), andCodeBLEU (Ren et al., 2020) are used, while they can not precisely evaluate the correctness of the code. So far, there are many open challenges in designing metrics to evaluate various aspects of code, such as vulnerability, maintainability, clarity, execution complexity, and stability.","(p6.0) To evaluate the NL2Code task, high-quality benchmarks and reliable metrics are fundamental and essential. In this section, we provide a brief overview of current benchmarks and metrics, as well as our observations and the open challenges.

(p6.1) We summarize 17 well-studied NL2Code benchmarks in Table 3, where we can find that each of these benchmarks has its own characteristics regarding size, language, complexity, and scenario. We observe that most benchmarks contain a limited number of instances. For example, the widely used HumanEval and MBPP have 164 and 974 instances, respectively. This is because these benchmarks are typically hand-written to ensure that LLMs have not seen them during training. In the era of large language models, it is crucial to avoid data leak-age when creating new benchmarks. Additionally, most current benchmarks have their problem descriptions in English and code solutions in Python.

(p6.2) Recently, several multi-lingual benchmarks have been proposed, such as MBXP (Athiwaratkun et al., 2022), HumanEvalX (Zheng et al., 2023), and Mul-tiPL (Cassano et al., 2022), which cover multiple programming languages, and ODEX (Wang et al., 2022c), which covers multiple natural languages. Details of multi-lingual benchmarks are listed in Appendix Table 7. Furthermore, benchmarks have been proposed for other practical scenarios, such as data science (Lai et al., 2022), public library (Zan et al., 2022b), private library (Zan et al., 2022a), multi-turn program synthesis (Nijkamp et al., 2023), and code security (Siddiq and msiddiq, 2022). For execution-based benchmarks, comprehensive test cases with complete coverage of the generated program can ensure the trustworthiness of evaluation results. As a reference, the average number of test cases for each benchmark, as well as the length statistics of the problem descriptions and solutions are also provided in Table 3.

(p6.3) Manually evaluating the generated code is impractical, which calls for the need for automatic metrics. The above mentioned benchmarks all provide test cases for execution-based evaluation, where metrics such as pass@k (Chen et al., 2021), n@k (Li et al., 2022b), test case aver-age (Hendrycks et al., 2021), and execution accuracy (Rajkumar et al., 2022) can be used. However, this approach has stringent requirements for the quality of test cases and can only evaluate executable code. For non-executable code, metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), andCodeBLEU (Ren et al., 2020) are used, while they can not precisely evaluate the correctness of the code. So far, there are many open challenges in designing metrics to evaluate various aspects of code, such as vulnerability, maintainability, clarity, execution complexity, and stability.","[[], [], ['b37', None, 'b38', 'b8'], [None, 'b10', 'b15']]","[[], [], ['b37', None, 'b38', 'b8'], [None, 'b10', 'b15']]",7,"1. To evaluate the NL2Code task, high-quality benchmarks and reliable metrics are fundamental and essential.
2. In this section, we provide a brief overview of current benchmarks and metrics, as well as our observations and the open challenges.
3. We summarize 17 well-studied NL2Code benchmarks in Table 3, where we can find that each of these benchmarks has its own characteristics regarding size, language, complexity, and scenario.
4. We observe that most benchmarks contain a limited number of instances.
5. For example, the widely used HumanEval and MBPP have 164 and 974 instances, respectively.
6. This is because these benchmarks are typically hand-written to ensure that LLMs have not seen them during training.
7. In the era of large language models, it is crucial to avoid data leak-age when creating new benchmarks.
8. Additionally, most current benchmarks have their problem descriptions in English and code solutions in Python.
9. Recently, several multi-lingual benchmarks have been proposed, such as MBXP (Athiwaratkun et al., 2022), HumanEvalX (Zheng et al., 2023), and Mul-tiPL (Cassano et al., 2022), which cover multiple programming languages, and ODEX (Wang et al., 2022c), which covers multiple natural languages.
10. Details of multi-lingual benchmarks are listed in Appendix Table 7.
11. Furthermore, benchmarks have been proposed for other practical scenarios, such as data science (Lai et al., 2022), public library (Zan et al., 2022b), private library (Zan et al., 2022a), multi-turn program synthesis (Nijkamp et al., 2023), and code security (Siddiq and msiddiq, 2022).
12. For execution-based benchmarks, comprehensive test cases with complete coverage of the generated program can ensure the trustworthiness of evaluation results.
13. As a reference, the average number of test cases for each benchmark, as well as the length statistics of the problem descriptions and solutions are also provided in Table 3.
14. Manually evaluating the generated code is impractical, which calls for the need for automatic metrics.
15. The above mentioned benchmarks all provide test cases for execution-based evaluation, where metrics such as pass@k (Chen et al., 2021), n@k (Li et al., 2022b), test case aver-age (Hendrycks et al., 2021), and execution accuracy (Rajkumar et al., 2022) can be used.
16. However, this approach has stringent requirements for the quality of test cases and can only evaluate executable code.
17. For non-executable code, metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), andCodeBLEU (Ren et al., 2020) are used, while they can not precisely evaluate the correctness of the code.
18. So far, there are many open challenges in designing metrics to evaluate various aspects of code, such as vulnerability, maintainability, clarity, execution complexity, and stability."
258557362,Large Language Models Meet NL2Code: A Survey,Computer Science,https://www.semanticscholar.org/paper/4f939f0751e5484f54089f6a97598e39afdcb3b5,s1,Large Language Models for NL2Code,"['p1.0', 'p1.1', 'p1.2']","['Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code. To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2 We summarize the related surveys in Appendix A.  problem domain. Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters. There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task. Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility. For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes. One trend observed is that these large language models are consistently growing in size as the research field advances. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.', 'Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation. Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code. While they show surprisingly good performance on NL2Code, most of them are not readily accessible. At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios. For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages. Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing. Details of the website can be found in Appendix B.', ""These models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023). One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions. Other notable products include CodeGeeX 3 and CodeWhisperer 4 . A summary of 10 products can be found in Appendix Table 5. Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users. There is still room for improvement before LLMs can be fully practical and capable of coding like humans.""]","Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code. To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2 We summarize the related surveys in Appendix A.  problem domain. Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters. There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task. Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility. For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes. One trend observed is that these large language models are consistently growing in size as the research field advances. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.

Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation. Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code. While they show surprisingly good performance on NL2Code, most of them are not readily accessible. At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios. For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages. Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing. Details of the website can be found in Appendix B.

These models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023). One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions. Other notable products include CodeGeeX 3 and CodeWhisperer 4 . A summary of 10 products can be found in Appendix Table 5. Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users. There is still room for improvement before LLMs can be fully practical and capable of coding like humans.","(p1.0) Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code. To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2 We summarize the related surveys in Appendix A.  problem domain. Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters. There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task. Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility. For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes. One trend observed is that these large language models are consistently growing in size as the research field advances. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.

(p1.1) Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation. Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code. While they show surprisingly good performance on NL2Code, most of them are not readily accessible. At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios. For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages. Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing. Details of the website can be found in Appendix B.

(p1.2) These models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023). One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions. Other notable products include CodeGeeX 3 and CodeWhisperer 4 . A summary of 10 products can be found in Appendix Table 5. Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users. There is still room for improvement before LLMs can be fully practical and capable of coding like humans.","[[None, 'b25', 'b8'], [None, 'b27'], [None, 'b12', 'b6', 'b8']]","[[None, 'b25', 'b8'], [None, 'b27'], [None, 'b12', 'b6', 'b8']]",9,"1. Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code.
2. To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2
3. We summarize the related surveys in Appendix A.  problem domain.
4. Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus.
5. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens.
6. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters.
7. There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance.
8. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task.
9. Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility.
10. For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes.
11. One trend observed is that these large language models are consistently growing in size as the research field advances.
12. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.
13. Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation.
14. Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus.
15. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code.
16. While they show surprisingly good performance on NL2Code, most of them are not readily accessible.
17. At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code.
18. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios.
19. For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages.
20. Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code.
21. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing.
22. Details of the website can be found in Appendix B.These models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023).
23. One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions.
24. Other notable products include CodeGeeX 3 and CodeWhisperer 4 .
25. A summary of 10 products can be found in Appendix Table 5.
26. Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users.
27. There is still room for improvement before LLMs can be fully practical and capable of coding like humans."
258557362,Large Language Models Meet NL2Code: A Survey,Computer Science,https://www.semanticscholar.org/paper/4f939f0751e5484f54089f6a97598e39afdcb3b5,s4,Large and Premium Data,"['p4.0', 'p4.1']","['As the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases. This highlights the importance of selecting and pre-processing high-quality data. In this section, we will discuss various commonly used data sources and pre-processing strategies that are essential for training LLMs.', 'Early models were trained using manually annotated data pairs of NL and code, and the data sources include CodeSearchNet (Husain et al., 2019), CoST (Zhu et al., 2022b), and XL-CoST (Zhu et al., 2022a). However, manual annotation is labour-intensive and time-consuming. There are also models like GPT-3 (Brown et al., 2020), GPT-Neo (Black et al., 2021), and GPT-J (Wang and Komatsuzaki, 2021)  In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data. We carefully review the data pre-processing methods of five powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b), and identify several commonalities. One is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless. Additionally, specific rules are employed to filter out uncommon code files. These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate. In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.']","As the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases. This highlights the importance of selecting and pre-processing high-quality data. In this section, we will discuss various commonly used data sources and pre-processing strategies that are essential for training LLMs.

Early models were trained using manually annotated data pairs of NL and code, and the data sources include CodeSearchNet (Husain et al., 2019), CoST (Zhu et al., 2022b), and XL-CoST (Zhu et al., 2022a). However, manual annotation is labour-intensive and time-consuming. There are also models like GPT-3 (Brown et al., 2020), GPT-Neo (Black et al., 2021), and GPT-J (Wang and Komatsuzaki, 2021)  In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data. We carefully review the data pre-processing methods of five powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b), and identify several commonalities. One is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless. Additionally, specific rules are employed to filter out uncommon code files. These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate. In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.","(p4.0) As the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases. This highlights the importance of selecting and pre-processing high-quality data. In this section, we will discuss various commonly used data sources and pre-processing strategies that are essential for training LLMs.

(p4.1) Early models were trained using manually annotated data pairs of NL and code, and the data sources include CodeSearchNet (Husain et al., 2019), CoST (Zhu et al., 2022b), and XL-CoST (Zhu et al., 2022a). However, manual annotation is labour-intensive and time-consuming. There are also models like GPT-3 (Brown et al., 2020), GPT-Neo (Black et al., 2021), and GPT-J (Wang and Komatsuzaki, 2021)  In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data. We carefully review the data pre-processing methods of five powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b), and identify several commonalities. One is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless. Additionally, specific rules are employed to filter out uncommon code files. These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate. In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.","[[], ['b8', None, 'b38', 'b27', 'b42', 'b43']]","[[], ['b8', None, 'b38', 'b27', 'b42', 'b43']]",6,"1. As the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases.
2. This highlights the importance of selecting and pre-processing high-quality data.
3. In this section, we will discuss various commonly used data sources and pre-processing strategies that are essential for training LLMs.
4. Early models were trained using manually annotated data pairs of NL and code, and the data sources include CodeSearchNet (Husain et al., 2019), CoST (Zhu et al., 2022b), and XL-CoST (Zhu et al., 2022a).
5. However, manual annotation is labour-intensive and time-consuming.
6. There are also models like GPT-3 (Brown et al., 2020), GPT-Neo (Black et al., 2021), and GPT-J (Wang and Komatsuzaki, 2021)
7. In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data.
8. We carefully review the data pre-processing methods of five powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b), and identify several commonalities.
9. One is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless.
10. Additionally, specific rules are employed to filter out uncommon code files.
11. These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate.
12. In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature."
258557362,Large Language Models Meet NL2Code: A Survey,Computer Science,https://www.semanticscholar.org/paper/4f939f0751e5484f54089f6a97598e39afdcb3b5,s7,Challenges and Opportunities,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4', 'p7.5', 'p7.6']","['Our investigations have revealed that advances in LLMs for NL2Code have a considerable impact on both academia and industry. Despite this progress, there are still numerous challenges that need to be addressed, offering ample opportunities for further research and applications. In this section, we explore the challenges and opportunities in terms of the ability gap between LLMs and humans.', 'Understanding Ability The inherent flexibility of natural language allows for a variety of expressions to convey functional requirements. Humans are able to understand various descriptions at different levels of abstraction. In contrast, current LLMs tend to be sensitive to the given context, which may cause unexpected performance degradation . In addition, LLMs may struggle when faced with complex problems that have numerous conditions and requirements (Barke et al., 2022;Imai, 2022). We believe exploring the understanding abilities of LLMs is a crucial research direction. One potential solution is to break down complex problems into multiple steps, as is commonly done in reasoning tasks (Wei et al., 2022).', 'Judgement Ability Humans have the ability to determine whether they can solve a programming problem or not. While current models will always return a solution even if there is no answer to the problem, due to the fact that they are trained by unsupervised causal language modeling objective. This can cause problems in practical applications. To improve the judgment ability of LLMs, researchers have employed reinforcement learning to leverage user feedback, as seen in models like InstructGPT  and ChatGPT 8 . However, collecting high-quality feedback for code is costly and challenging. There are also ongoing studies (Chen et al., 2023;Key et al., 2022) exploring the possibility of self-validation for LLMs, which is also a promising research direction.', 'Explanation Ability It is widely acknowledged that human developers possess the ability to interpret the meaning of the code they write, which is crucial for educational purposes and software maintenance. Recent studies showed that LLMs have the potential to automatically generate code explanations. MacNeil et al. (2022a) proposed using LLMs to generate code explanations for students during their learning process, and MacNeil et al. (2022b) proposed explaining numerous aspects of a given code snippet using Copilot. Further research and explorations are necessary to fully realize the potential of LLMs in this regard.', 'Adaptive Learning Ability A fundamental difference between current large language models and humans is their ability to adapt to new and updated knowledge. Human developers possess a unique ability to quickly search and learn new materials, such as programming documentation, and adapt to changes in APIs with relative ease. However, re-training or fine-tuning LLMs requires significant effort and resources. This issue has inspired a number of recent studies, such as DocCoder  and APICoder (Zan et al., 2022a), which utilize retrieval-based methods to provide extra or updated knowledge during model inference. Despite these advancements, it remains an open challenge to endow LLMs with the powerful learning capabilities humans possess.', 'Multi-tasking Ability Large language models have been applied to a variety of code-related tasks, such as code repair (Joshi et al., 2022; Prenner and Robbes, 2021), code search (Neelakantan et al., 2022), and code review (Li et al., 2022c) as well as non-code tasks that can be formatted in a code-like manner, such as mathematics (Drori and Verma, 2021; Drori et al., 2021) and chemistry (Krenn et al., 2022;Hocky and White, 2022). However, there are differences between LLMs and human abilities in terms of multi-tasking. Humans can seamlessly switch between tasks, while LLMs may require sophisticated prompt engineering (Liu et al., 2023). Another evidence is that LLMs lack the ability to quickly master multiple programming languages (Zheng et al., 2023) as humans do. These limitations highlight areas for future research.', 'In this paper, we survey 27 existing large language models for NL2Code, and draw a thorough analysis of the underlying reasons for their success. We also provide a detailed review of benchmarks and metrics. Regarding the gap between models and humans, we present ongoing challenges and opportunities. In addition, we have developed a website to track the latest findings in this field. We hope this survey can contribute to a comprehensive overview of the field and promote its thriving evolution.']","Our investigations have revealed that advances in LLMs for NL2Code have a considerable impact on both academia and industry. Despite this progress, there are still numerous challenges that need to be addressed, offering ample opportunities for further research and applications. In this section, we explore the challenges and opportunities in terms of the ability gap between LLMs and humans.

Understanding Ability The inherent flexibility of natural language allows for a variety of expressions to convey functional requirements. Humans are able to understand various descriptions at different levels of abstraction. In contrast, current LLMs tend to be sensitive to the given context, which may cause unexpected performance degradation . In addition, LLMs may struggle when faced with complex problems that have numerous conditions and requirements (Barke et al., 2022;Imai, 2022). We believe exploring the understanding abilities of LLMs is a crucial research direction. One potential solution is to break down complex problems into multiple steps, as is commonly done in reasoning tasks (Wei et al., 2022).

Judgement Ability Humans have the ability to determine whether they can solve a programming problem or not. While current models will always return a solution even if there is no answer to the problem, due to the fact that they are trained by unsupervised causal language modeling objective. This can cause problems in practical applications. To improve the judgment ability of LLMs, researchers have employed reinforcement learning to leverage user feedback, as seen in models like InstructGPT  and ChatGPT 8 . However, collecting high-quality feedback for code is costly and challenging. There are also ongoing studies (Chen et al., 2023;Key et al., 2022) exploring the possibility of self-validation for LLMs, which is also a promising research direction.

Explanation Ability It is widely acknowledged that human developers possess the ability to interpret the meaning of the code they write, which is crucial for educational purposes and software maintenance. Recent studies showed that LLMs have the potential to automatically generate code explanations. MacNeil et al. (2022a) proposed using LLMs to generate code explanations for students during their learning process, and MacNeil et al. (2022b) proposed explaining numerous aspects of a given code snippet using Copilot. Further research and explorations are necessary to fully realize the potential of LLMs in this regard.

Adaptive Learning Ability A fundamental difference between current large language models and humans is their ability to adapt to new and updated knowledge. Human developers possess a unique ability to quickly search and learn new materials, such as programming documentation, and adapt to changes in APIs with relative ease. However, re-training or fine-tuning LLMs requires significant effort and resources. This issue has inspired a number of recent studies, such as DocCoder  and APICoder (Zan et al., 2022a), which utilize retrieval-based methods to provide extra or updated knowledge during model inference. Despite these advancements, it remains an open challenge to endow LLMs with the powerful learning capabilities humans possess.

Multi-tasking Ability Large language models have been applied to a variety of code-related tasks, such as code repair (Joshi et al., 2022; Prenner and Robbes, 2021), code search (Neelakantan et al., 2022), and code review (Li et al., 2022c) as well as non-code tasks that can be formatted in a code-like manner, such as mathematics (Drori and Verma, 2021; Drori et al., 2021) and chemistry (Krenn et al., 2022;Hocky and White, 2022). However, there are differences between LLMs and human abilities in terms of multi-tasking. Humans can seamlessly switch between tasks, while LLMs may require sophisticated prompt engineering (Liu et al., 2023). Another evidence is that LLMs lack the ability to quickly master multiple programming languages (Zheng et al., 2023) as humans do. These limitations highlight areas for future research.

In this paper, we survey 27 existing large language models for NL2Code, and draw a thorough analysis of the underlying reasons for their success. We also provide a detailed review of benchmarks and metrics. Regarding the gap between models and humans, we present ongoing challenges and opportunities. In addition, we have developed a website to track the latest findings in this field. We hope this survey can contribute to a comprehensive overview of the field and promote its thriving evolution.","(p7.0) Our investigations have revealed that advances in LLMs for NL2Code have a considerable impact on both academia and industry. Despite this progress, there are still numerous challenges that need to be addressed, offering ample opportunities for further research and applications. In this section, we explore the challenges and opportunities in terms of the ability gap between LLMs and humans.

(p7.1) Understanding Ability The inherent flexibility of natural language allows for a variety of expressions to convey functional requirements. Humans are able to understand various descriptions at different levels of abstraction. In contrast, current LLMs tend to be sensitive to the given context, which may cause unexpected performance degradation . In addition, LLMs may struggle when faced with complex problems that have numerous conditions and requirements (Barke et al., 2022;Imai, 2022). We believe exploring the understanding abilities of LLMs is a crucial research direction. One potential solution is to break down complex problems into multiple steps, as is commonly done in reasoning tasks (Wei et al., 2022).

(p7.2) Judgement Ability Humans have the ability to determine whether they can solve a programming problem or not. While current models will always return a solution even if there is no answer to the problem, due to the fact that they are trained by unsupervised causal language modeling objective. This can cause problems in practical applications. To improve the judgment ability of LLMs, researchers have employed reinforcement learning to leverage user feedback, as seen in models like InstructGPT  and ChatGPT 8 . However, collecting high-quality feedback for code is costly and challenging. There are also ongoing studies (Chen et al., 2023;Key et al., 2022) exploring the possibility of self-validation for LLMs, which is also a promising research direction.

(p7.3) Explanation Ability It is widely acknowledged that human developers possess the ability to interpret the meaning of the code they write, which is crucial for educational purposes and software maintenance. Recent studies showed that LLMs have the potential to automatically generate code explanations. MacNeil et al. (2022a) proposed using LLMs to generate code explanations for students during their learning process, and MacNeil et al. (2022b) proposed explaining numerous aspects of a given code snippet using Copilot. Further research and explorations are necessary to fully realize the potential of LLMs in this regard.

(p7.4) Adaptive Learning Ability A fundamental difference between current large language models and humans is their ability to adapt to new and updated knowledge. Human developers possess a unique ability to quickly search and learn new materials, such as programming documentation, and adapt to changes in APIs with relative ease. However, re-training or fine-tuning LLMs requires significant effort and resources. This issue has inspired a number of recent studies, such as DocCoder  and APICoder (Zan et al., 2022a), which utilize retrieval-based methods to provide extra or updated knowledge during model inference. Despite these advancements, it remains an open challenge to endow LLMs with the powerful learning capabilities humans possess.

(p7.5) Multi-tasking Ability Large language models have been applied to a variety of code-related tasks, such as code repair (Joshi et al., 2022; Prenner and Robbes, 2021), code search (Neelakantan et al., 2022), and code review (Li et al., 2022c) as well as non-code tasks that can be formatted in a code-like manner, such as mathematics (Drori and Verma, 2021; Drori et al., 2021) and chemistry (Krenn et al., 2022;Hocky and White, 2022). However, there are differences between LLMs and human abilities in terms of multi-tasking. Humans can seamlessly switch between tasks, while LLMs may require sophisticated prompt engineering (Liu et al., 2023). Another evidence is that LLMs lack the ability to quickly master multiple programming languages (Zheng et al., 2023) as humans do. These limitations highlight areas for future research.

(p7.6) In this paper, we survey 27 existing large language models for NL2Code, and draw a thorough analysis of the underlying reasons for their success. We also provide a detailed review of benchmarks and metrics. Regarding the gap between models and humans, we present ongoing challenges and opportunities. In addition, we have developed a website to track the latest findings in this field. We hope this survey can contribute to a comprehensive overview of the field and promote its thriving evolution.","[[], [None, 'b32'], [None], [None], ['b37'], [None], []]","[[], [None, 'b32'], [None], [None], ['b37'], [None], []]",6,"1. Our investigations have revealed that advances in LLMs for NL2Code have a considerable impact on both academia and industry.
2. Despite this progress, there are still numerous challenges that need to be addressed, offering ample opportunities for further research and applications.
3. In this section, we explore the challenges and opportunities in terms of the ability gap between LLMs and humans.
4. Understanding Ability The inherent flexibility of natural language allows for a variety of expressions to convey functional requirements.
5. Humans are able to understand various descriptions at different levels of abstraction.
6. In contrast, current LLMs tend to be sensitive to the given context, which may cause unexpected performance degradation .
7. In addition, LLMs may struggle when faced with complex problems that have numerous conditions and requirements (Barke et al., 2022;Imai, 2022).
8. We believe exploring the understanding abilities of LLMs is a crucial research direction.
9. One potential solution is to break down complex problems into multiple steps, as is commonly done in reasoning tasks (Wei et al., 2022).
10. Judgement Ability Humans have the ability to determine whether they can solve a programming problem or not.
11. While current models will always return a solution even if there is no answer to the problem, due to the fact that they are trained by unsupervised causal language modeling objective.
12. This can cause problems in practical applications.
13. To improve the judgment ability of LLMs, researchers have employed reinforcement learning to leverage user feedback, as seen in models like InstructGPT  and ChatGPT 8 .
14. However, collecting high-quality feedback for code is costly and challenging.
15. There are also ongoing studies (Chen et al., 2023;Key et al., 2022) exploring the possibility of self-validation for LLMs, which is also a promising research direction.
16. Explanation Ability It is widely acknowledged that human developers possess the ability to interpret the meaning of the code they write, which is crucial for educational purposes and software maintenance.
17. Recent studies showed that LLMs have the potential to automatically generate code explanations.
18. MacNeil et al. (2022a) proposed using LLMs to generate code explanations for students during their learning process, and MacNeil et al. (2022b) proposed explaining numerous aspects of a given code snippet using Copilot.
19. Further research and explorations are necessary to fully realize the potential of LLMs in this regard.
20. Adaptive Learning Ability A fundamental difference between current large language models and humans is their ability to adapt to new and updated knowledge.
21. Human developers possess a unique ability to quickly search and learn new materials, such as programming documentation, and adapt to changes in APIs with relative ease.
22. However, re-training or fine-tuning LLMs requires significant effort and resources.
23. This issue has inspired a number of recent studies, such as DocCoder  and APICoder (Zan et al., 2022a), which utilize retrieval-based methods to provide extra or updated knowledge during model inference.
24. Despite these advancements, it remains an open challenge to endow LLMs with the powerful learning capabilities humans possess.
25. Multi-tasking Ability Large language models have been applied to a variety of code-related tasks, such as code repair (Joshi et al., 2022; Prenner and Robbes, 2021), code search (Neelakantan et al., 2022), and code review (Li et al., 2022c) as well as non-code tasks that can be formatted in a code-like manner, such as mathematics (Drori and Verma, 2021; Drori et al., 2021) and chemistry (Krenn et al., 2022;Hocky and White, 2022).
26. However, there are differences between LLMs and human abilities in terms of multi-tasking.
27. Humans can seamlessly switch between tasks, while LLMs may require sophisticated prompt engineering (Liu et al., 2023).
28. Another evidence is that LLMs lack the ability to quickly master multiple programming languages (Zheng et al., 2023) as humans do.
29. These limitations highlight areas for future research.
30. In this paper, we survey 27 existing large language models for NL2Code, and draw a thorough analysis of the underlying reasons for their success.
31. We also provide a detailed review of benchmarks and metrics.
32. Regarding the gap between models and humans, we present ongoing challenges and opportunities.
33. In addition, we have developed a website to track the latest findings in this field.
34. We hope this survey can contribute to a comprehensive overview of the field and promote its thriving evolution."
258557362,Large Language Models Meet NL2Code: A Survey,Computer Science,https://www.semanticscholar.org/paper/4f939f0751e5484f54089f6a97598e39afdcb3b5,s8,Limitations,['p8.0'],"['In this paper, we thoroughly investigate the existing large language models for NL2Code, and summarize them from diverse perspectives with our own thinking. However, as this field is evolving so rapidly, there may be aspects that we have overlooked, or some new works that we have not covered. To mitigate this issue, we have created a website to track the latest progress through crowdsourcing, hoping that it will continually contribute to the development of the field. Besides, the existing LLMs possess their own characteristics in terms of model size, architecture, corpus, pre-processing, tokenizer, hyper-parameters, and training platforms. Also, some of them are currently not publicly available, such as AlphaCode (Li et al., 2022b) and PaLM-Coder (Chowdhery et al., 2022). Therefore, it is almost impractical to conduct a completely fair comparison. We tried our best to show a kind of comparison on the popular HumanEval and MBPP benchmarks, hoping that it can provide clues to the differences in performance of different LLMs. In addition, evaluating LLMs has a high cost in computational resources. We thus have made all files generated by the LLMs publicly available on https://nl2code.github.io. Previous surveys on the topic of code intelligence (Allamanis et al., 2018;Le et al., 2020;Li et al., 2022a;Xu and Zhu, 2022) and code generation (Pawade et al., 2018;Shin and Nam, 2021;Dehaerne et al., 2022) have primarily focused on early methodologies such as the use of programming templates (Syriani et al., 2018;Luhunu and Syriani, 2017), neural models based on CNN, RNN, and LSTM architectures (Allamanis et al., 2018;Sharma et al., 2021), and small-scale Transformer models that require labelled data for training (Mastropaolo et al., 2021;Shah et al., 2021). However, with the advancement of model size, Transformerbased models have demonstrated exceptional performance in NL2Code tasks and have given rise to the development of more capable code generation models. In light of this, there exists a clear need for a comprehensive survey of large language models for NL2Code tasks to bridge this gap in knowledge. This study endeavours to fulfill this need by providing a thorough analysis of the successful LLMs and a detailed review of NL2Code benchmarks and metrics. We also present the ongoing challenges and opportunities regarding the ability gap between LLMs and humans.']","In this paper, we thoroughly investigate the existing large language models for NL2Code, and summarize them from diverse perspectives with our own thinking. However, as this field is evolving so rapidly, there may be aspects that we have overlooked, or some new works that we have not covered. To mitigate this issue, we have created a website to track the latest progress through crowdsourcing, hoping that it will continually contribute to the development of the field. Besides, the existing LLMs possess their own characteristics in terms of model size, architecture, corpus, pre-processing, tokenizer, hyper-parameters, and training platforms. Also, some of them are currently not publicly available, such as AlphaCode (Li et al., 2022b) and PaLM-Coder (Chowdhery et al., 2022). Therefore, it is almost impractical to conduct a completely fair comparison. We tried our best to show a kind of comparison on the popular HumanEval and MBPP benchmarks, hoping that it can provide clues to the differences in performance of different LLMs. In addition, evaluating LLMs has a high cost in computational resources. We thus have made all files generated by the LLMs publicly available on https://nl2code.github.io. Previous surveys on the topic of code intelligence (Allamanis et al., 2018;Le et al., 2020;Li et al., 2022a;Xu and Zhu, 2022) and code generation (Pawade et al., 2018;Shin and Nam, 2021;Dehaerne et al., 2022) have primarily focused on early methodologies such as the use of programming templates (Syriani et al., 2018;Luhunu and Syriani, 2017), neural models based on CNN, RNN, and LSTM architectures (Allamanis et al., 2018;Sharma et al., 2021), and small-scale Transformer models that require labelled data for training (Mastropaolo et al., 2021;Shah et al., 2021). However, with the advancement of model size, Transformerbased models have demonstrated exceptional performance in NL2Code tasks and have given rise to the development of more capable code generation models. In light of this, there exists a clear need for a comprehensive survey of large language models for NL2Code tasks to bridge this gap in knowledge. This study endeavours to fulfill this need by providing a thorough analysis of the successful LLMs and a detailed review of NL2Code benchmarks and metrics. We also present the ongoing challenges and opportunities regarding the ability gap between LLMs and humans.","(p8.0) In this paper, we thoroughly investigate the existing large language models for NL2Code, and summarize them from diverse perspectives with our own thinking. However, as this field is evolving so rapidly, there may be aspects that we have overlooked, or some new works that we have not covered. To mitigate this issue, we have created a website to track the latest progress through crowdsourcing, hoping that it will continually contribute to the development of the field. Besides, the existing LLMs possess their own characteristics in terms of model size, architecture, corpus, pre-processing, tokenizer, hyper-parameters, and training platforms. Also, some of them are currently not publicly available, such as AlphaCode (Li et al., 2022b) and PaLM-Coder (Chowdhery et al., 2022). Therefore, it is almost impractical to conduct a completely fair comparison. We tried our best to show a kind of comparison on the popular HumanEval and MBPP benchmarks, hoping that it can provide clues to the differences in performance of different LLMs. In addition, evaluating LLMs has a high cost in computational resources. We thus have made all files generated by the LLMs publicly available on https://nl2code.github.io. Previous surveys on the topic of code intelligence (Allamanis et al., 2018;Le et al., 2020;Li et al., 2022a;Xu and Zhu, 2022) and code generation (Pawade et al., 2018;Shin and Nam, 2021;Dehaerne et al., 2022) have primarily focused on early methodologies such as the use of programming templates (Syriani et al., 2018;Luhunu and Syriani, 2017), neural models based on CNN, RNN, and LSTM architectures (Allamanis et al., 2018;Sharma et al., 2021), and small-scale Transformer models that require labelled data for training (Mastropaolo et al., 2021;Shah et al., 2021). However, with the advancement of model size, Transformerbased models have demonstrated exceptional performance in NL2Code tasks and have given rise to the development of more capable code generation models. In light of this, there exists a clear need for a comprehensive survey of large language models for NL2Code tasks to bridge this gap in knowledge. This study endeavours to fulfill this need by providing a thorough analysis of the successful LLMs and a detailed review of NL2Code benchmarks and metrics. We also present the ongoing challenges and opportunities regarding the ability gap between LLMs and humans.","[['b20', 'b28', 'b34', 'b3', 'b0', None, 'b11', 'b22']]","[['b20', 'b28', 'b34', 'b3', 'b0', None, 'b11', 'b22']]",8,"1. In this paper, we thoroughly investigate the existing large language models for NL2Code, and summarize them from diverse perspectives with our own thinking.
2. However, as this field is evolving so rapidly, there may be aspects that we have overlooked, or some new works that we have not covered.
3. To mitigate this issue, we have created a website to track the latest progress through crowdsourcing, hoping that it will continually contribute to the development of the field.
4. Besides, the existing LLMs possess their own characteristics in terms of model size, architecture, corpus, pre-processing, tokenizer, hyper-parameters, and training platforms.
5. Also, some of them are currently not publicly available, such as AlphaCode (Li et al., 2022b) and PaLM-Coder (Chowdhery et al., 2022).
6. Therefore, it is almost impractical to conduct a completely fair comparison.
7. We tried our best to show a kind of comparison on the popular HumanEval and MBPP benchmarks, hoping that it can provide clues to the differences in performance of different LLMs.
8. In addition, evaluating LLMs has a high cost in computational resources.
9. We thus have made all files generated by the LLMs publicly available on https://nl2code.github.io.
10. Previous surveys on the topic of code intelligence (Allamanis et al., 2018;Le et al., 2020;Li et al., 2022a;Xu and Zhu, 2022) and code generation (Pawade et al., 2018;Shin and Nam, 2021;Dehaerne et al., 2022) have primarily focused on early methodologies such as the use of programming templates (Syriani et al., 2018;Luhunu and Syriani, 2017), neural models based on CNN, RNN, and LSTM architectures (Allamanis et al., 2018;Sharma et al., 2021), and small-scale Transformer models that require labelled data for training (Mastropaolo et al., 2021;Shah et al., 2021).
11. However, with the advancement of model size, Transformerbased models have demonstrated exceptional performance in NL2Code tasks and have given rise to the development of more capable code generation models.
12. In light of this, there exists a clear need for a comprehensive survey of large language models for NL2Code tasks to bridge this gap in knowledge.
13. This study endeavours to fulfill this need by providing a thorough analysis of the successful LLMs and a detailed review of NL2Code benchmarks and metrics.
14. We also present the ongoing challenges and opportunities regarding the ability gap between LLMs and humans."
258740687,A Survey on Zero Pronoun Translation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/8fa265a8ca46c9c0ad35d2c5b519a86e2a092dfe,s7,Zero Pronoun Resolution,"['p7.0', 'p7.1', 'p7.2']","['The task contains three steps: ZP detection, anaphoricity determination and reference linking. Earlier works investigated rich features using traditional ML models (Zhao and Ng, 2007;Kong and Zhou, 2010;Chen andNg, 2013, 2015). Recent studies exploited neural models to achieve the better performance (Chen and Ng, 2016;Yin et al., 2018;Song et al., 2020). The CoNLL2011 and CoNLL2012 2 are commonlyused benchmarks on modeling unrestricted coreference. The corpus contains 144K coreference instances, but dropped subjects only occupy 15%.', 'Zero Pronoun Recovery Given a source sentence, this aims to insert omitted pronouns in proper positions without changing the original meaning (Yang and Xue, 2010;Yang et al., 2015Yang et al., , 2019a. It is different from ZP resolution, which identifies the antecedent of a referential pronoun (Mitkov, 2014). Previous studies regarded ZP recovery as a classification or sequence labelling problem, which only achieve 40∼60% F1 scores on closed datasets Song et al., 2020), indicating the difficulty of generating ZPs. It is worth noting that ZP recovery models can work for ZPT task in a pipeline manner: input sentences are labeled with ZPs using an external recovery system and then fed into a standard MT model (Chung and Gildea, 2010;Wang et al., 2016a).', 'Zero Pronoun Translation When pronouns are omitted in a source sentence, ZPT aims to generate ZPs in its target translation. Early studies have investigate a number of works for SMT models (Chung and Gildea, 2010;Le Nagard and Koehn, 2010;Taira et al., 2012;Xiang et al., 2013;Wang et al., 2016a). Recent years have seen a surge of interest in NMT Wang et al., 2018a), since the problem still exists in advanced NMT systems. ZPT is also related to pronoun translation, which aims to correctly translate explicit pronoun in terms of feminine and masculine. The DiscoMT 3 is a commonly-cited benchmark on pronoun translation, however, there was no standard ZPT benchmarks up until now.']","The task contains three steps: ZP detection, anaphoricity determination and reference linking. Earlier works investigated rich features using traditional ML models (Zhao and Ng, 2007;Kong and Zhou, 2010;Chen andNg, 2013, 2015). Recent studies exploited neural models to achieve the better performance (Chen and Ng, 2016;Yin et al., 2018;Song et al., 2020). The CoNLL2011 and CoNLL2012 2 are commonlyused benchmarks on modeling unrestricted coreference. The corpus contains 144K coreference instances, but dropped subjects only occupy 15%.

Zero Pronoun Recovery Given a source sentence, this aims to insert omitted pronouns in proper positions without changing the original meaning (Yang and Xue, 2010;Yang et al., 2015Yang et al., , 2019a. It is different from ZP resolution, which identifies the antecedent of a referential pronoun (Mitkov, 2014). Previous studies regarded ZP recovery as a classification or sequence labelling problem, which only achieve 40∼60% F1 scores on closed datasets Song et al., 2020), indicating the difficulty of generating ZPs. It is worth noting that ZP recovery models can work for ZPT task in a pipeline manner: input sentences are labeled with ZPs using an external recovery system and then fed into a standard MT model (Chung and Gildea, 2010;Wang et al., 2016a).

Zero Pronoun Translation When pronouns are omitted in a source sentence, ZPT aims to generate ZPs in its target translation. Early studies have investigate a number of works for SMT models (Chung and Gildea, 2010;Le Nagard and Koehn, 2010;Taira et al., 2012;Xiang et al., 2013;Wang et al., 2016a). Recent years have seen a surge of interest in NMT Wang et al., 2018a), since the problem still exists in advanced NMT systems. ZPT is also related to pronoun translation, which aims to correctly translate explicit pronoun in terms of feminine and masculine. The DiscoMT 3 is a commonly-cited benchmark on pronoun translation, however, there was no standard ZPT benchmarks up until now.","(p7.0) The task contains three steps: ZP detection, anaphoricity determination and reference linking. Earlier works investigated rich features using traditional ML models (Zhao and Ng, 2007;Kong and Zhou, 2010;Chen andNg, 2013, 2015). Recent studies exploited neural models to achieve the better performance (Chen and Ng, 2016;Yin et al., 2018;Song et al., 2020). The CoNLL2011 and CoNLL2012 2 are commonlyused benchmarks on modeling unrestricted coreference. The corpus contains 144K coreference instances, but dropped subjects only occupy 15%.

(p7.1) Zero Pronoun Recovery Given a source sentence, this aims to insert omitted pronouns in proper positions without changing the original meaning (Yang and Xue, 2010;Yang et al., 2015Yang et al., , 2019a. It is different from ZP resolution, which identifies the antecedent of a referential pronoun (Mitkov, 2014). Previous studies regarded ZP recovery as a classification or sequence labelling problem, which only achieve 40∼60% F1 scores on closed datasets Song et al., 2020), indicating the difficulty of generating ZPs. It is worth noting that ZP recovery models can work for ZPT task in a pipeline manner: input sentences are labeled with ZPs using an external recovery system and then fed into a standard MT model (Chung and Gildea, 2010;Wang et al., 2016a).

(p7.2) Zero Pronoun Translation When pronouns are omitted in a source sentence, ZPT aims to generate ZPs in its target translation. Early studies have investigate a number of works for SMT models (Chung and Gildea, 2010;Le Nagard and Koehn, 2010;Taira et al., 2012;Xiang et al., 2013;Wang et al., 2016a). Recent years have seen a surge of interest in NMT Wang et al., 2018a), since the problem still exists in advanced NMT systems. ZPT is also related to pronoun translation, which aims to correctly translate explicit pronoun in terms of feminine and masculine. The DiscoMT 3 is a commonly-cited benchmark on pronoun translation, however, there was no standard ZPT benchmarks up until now.","[['b55', None, 'b51', 'b19'], ['b49', 'b48', None, 'b47', 'b38', 'b19', 'b5'], ['b34', 'b46', None, 'b38', 'b23']]","[['b55', None, 'b51', 'b19'], ['b49', 'b48', None, 'b47', 'b38', 'b19', 'b5'], ['b34', 'b46', None, 'b38', 'b23']]",16,"1. The task contains three steps: ZP detection, anaphoricity determination and reference linking.
2. Earlier works investigated rich features using traditional ML models (Zhao and Ng, 2007;Kong and Zhou, 2010;Chen andNg, 2013, 2015).
3. Recent studies exploited neural models to achieve the better performance (Chen and Ng, 2016;Yin et al., 2018;Song et al., 2020).
4. The CoNLL2011 and CoNLL2012 2 are commonlyused benchmarks on modeling unrestricted coreference.
5. The corpus contains 144K coreference instances, but dropped subjects only occupy 15%.
6. Zero Pronoun Recovery Given a source sentence, this aims to insert omitted pronouns in proper positions without changing the original meaning
7. (Yang and Xue, 2010;Yang et al., 2015Yang et al., , 2019a. It is different from ZP resolution, which identifies the antecedent of a referential pronoun (Mitkov, 2014).
8. Previous studies regarded ZP recovery as a classification or sequence labelling problem, which only achieve 40∼60% F1 scores on closed datasets Song et al., 2020), indicating the difficulty of generating ZPs.
9. It is worth noting that ZP recovery models can work for ZPT task in a pipeline manner: input sentences are labeled with ZPs using an external recovery system and then fed into a standard MT model (Chung and Gildea, 2010;Wang et al., 2016a).
10. Zero Pronoun Translation When pronouns are omitted in a source sentence, ZPT aims to generate ZPs in its target translation.
11. Early studies have investigate a number of works for SMT models (Chung and Gildea, 2010;Le Nagard and Koehn, 2010;Taira et al., 2012;Xiang et al., 2013;Wang et al., 2016a).
12. Recent years have seen a surge of interest in NMT Wang et al., 2018a), since the problem still exists in advanced NMT systems.
13. ZPT is also related to pronoun translation, which aims to correctly translate explicit pronoun in terms of feminine and masculine.
14. The DiscoMT 3 is a commonly-cited benchmark on pronoun translation, however, there was no standard ZPT benchmarks up until now."
258740687,A Survey on Zero Pronoun Translation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/8fa265a8ca46c9c0ad35d2c5b519a86e2a092dfe,s18,Data-Level Methods Do Not Change Model,"['p18.0', 'p18.1']","['Architecture. This is more friendly to NMT. Some researchers targeted making better usage of the limited training data (Tan et al., 2019;Ohtani et al., 2019;Tan et al., 2021). They trained an external model on the ZP data to recover the ZP information in the input sequence of the MT model (Tan et al., 2019;Ohtani et al., 2019;Tan et al., 2021) or correct the errors in the translation outputs (Voita et al., 2019). Others aimed to up-sample the training data for the ZPT task (Sugiyama and Yoshinaga, 2019;Kimura et al., 2019;Ri et al., 2021). They preferred to  improve the ZPT performance via a data augmentation without modifying the MT architecture (Wang et al., 2016a;Sugiyama and Yoshinaga, 2019). Kimura et al. (2019); Ri et al. (2021) verified that the performance can be further improved by denoising the pseudo data. 4. Multitask and Multi-Lingual Learning. ZPT is a hard task to be done alone, researchers are investigating how to leverage other related NLP tasks to improve ZPT by training models to perform multiple tasks simultaneously (Wang et al., 2018a). Since ZPT is a cross-lingual problem, researchers are exploring techniques for training models that can work across multiple languages, rather than being limited to a single language (Aloraini and Poesio, 2020).', '6 Evaluation Methods']","Architecture. This is more friendly to NMT. Some researchers targeted making better usage of the limited training data (Tan et al., 2019;Ohtani et al., 2019;Tan et al., 2021). They trained an external model on the ZP data to recover the ZP information in the input sequence of the MT model (Tan et al., 2019;Ohtani et al., 2019;Tan et al., 2021) or correct the errors in the translation outputs (Voita et al., 2019). Others aimed to up-sample the training data for the ZPT task (Sugiyama and Yoshinaga, 2019;Kimura et al., 2019;Ri et al., 2021). They preferred to  improve the ZPT performance via a data augmentation without modifying the MT architecture (Wang et al., 2016a;Sugiyama and Yoshinaga, 2019). Kimura et al. (2019); Ri et al. (2021) verified that the performance can be further improved by denoising the pseudo data. 4. Multitask and Multi-Lingual Learning. ZPT is a hard task to be done alone, researchers are investigating how to leverage other related NLP tasks to improve ZPT by training models to perform multiple tasks simultaneously (Wang et al., 2018a). Since ZPT is a cross-lingual problem, researchers are exploring techniques for training models that can work across multiple languages, rather than being limited to a single language (Aloraini and Poesio, 2020).

6 Evaluation Methods","(p18.0) Architecture. This is more friendly to NMT. Some researchers targeted making better usage of the limited training data (Tan et al., 2019;Ohtani et al., 2019;Tan et al., 2021). They trained an external model on the ZP data to recover the ZP information in the input sequence of the MT model (Tan et al., 2019;Ohtani et al., 2019;Tan et al., 2021) or correct the errors in the translation outputs (Voita et al., 2019). Others aimed to up-sample the training data for the ZPT task (Sugiyama and Yoshinaga, 2019;Kimura et al., 2019;Ri et al., 2021). They preferred to  improve the ZPT performance via a data augmentation without modifying the MT architecture (Wang et al., 2016a;Sugiyama and Yoshinaga, 2019). Kimura et al. (2019); Ri et al. (2021) verified that the performance can be further improved by denoising the pseudo data. 4. Multitask and Multi-Lingual Learning. ZPT is a hard task to be done alone, researchers are investigating how to leverage other related NLP tasks to improve ZPT by training models to perform multiple tasks simultaneously (Wang et al., 2018a). Since ZPT is a cross-lingual problem, researchers are exploring techniques for training models that can work across multiple languages, rather than being limited to a single language (Aloraini and Poesio, 2020).

(p18.1) 6 Evaluation Methods","[['b34', 'b16', 'b29', 'b7', None, 'b38', 'b25', 'b22', 'b24'], []]","[['b34', 'b16', 'b29', 'b7', None, 'b38', 'b25', 'b22', 'b24'], []]",9,"1. Architecture. This is more friendly to NMT.
2. Some researchers targeted making better usage of the limited training data (Tan et al., 2019;Ohtani et al., 2019;Tan et al., 2021).
3. They trained an external model on the ZP data to recover the ZP information in the input sequence of the MT model (Tan et al., 2019;Ohtani et al., 2019;Tan et al., 2021) or correct the errors in the translation outputs (Voita et al., 2019).
4. Others aimed to up-sample the training data for the ZPT task (Sugiyama and Yoshinaga, 2019;Kimura et al., 2019;Ri et al., 2021).
5. They preferred to  improve the ZPT performance via a data augmentation without modifying the MT architecture (Wang et al., 2016a;Sugiyama and Yoshinaga, 2019).
6. Kimura et al. (2019); Ri et al. (2021) verified that the performance can be further improved by denoising the pseudo data.
7. 4. Multitask and Multi-Lingual Learning.
8. ZPT is a hard task to be done alone, researchers are investigating how to leverage other related NLP tasks to improve ZPT by training models to perform multiple tasks simultaneously (Wang et al., 2018a).
9. Since ZPT is a cross-lingual problem, researchers are exploring techniques for training models that can work across multiple languages, rather than being limited to a single language (Aloraini and Poesio, 2020).
10. 6 Evaluation Methods"
258740687,A Survey on Zero Pronoun Translation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/8fa265a8ca46c9c0ad35d2c5b519a86e2a092dfe,s14,Overview,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4', 'p14.5', 'p14.6']","['Early researchers have investigated several approaches for conventional statistical machine translation (SMT) (Le Nagard and Koehn, 2010; Xiang et al., 2013;Wang et al., 2016a). Modeling ZPs for advanced NMT models, however, has received more attention, resulting in better performance in this field (Wang et al., 2018a;Tan et al., 2021;Hwang et al., 2021). Generally prior works fall into three categories: (1) Pipeline, where input sentences are labeled with ZPs using an external ZP recovery system and then fed into a standard MT model (Chung and Gildea, 2010;Wang et al., 2016a); (2) Implicit, where ZP phenomenon is implicitly resolved by modelling document-level contexts Ri et al., 2021); (3) Endto-End, where ZP prediction and translation are jointly learned in an end-to-end manner Tan et al., 2021).', 'Pipeline The pipeline method of ZPT borrows from that in pronoun translation (Le Nagard and Koehn, 2010;Pradhan et al., 2012) due to the strong relevance between the two tasks. Chung and Gildea (2010) systematically examine the effects of empty category (EC) 9 on SMT with pattern-, CRF-and parsing-based methods. The results show that this can really improve the translation quality, even though the automatic prediction of EC is not highly accurate. Besides, Wang et al. (2016aWang et al. ( ,b, 2017b proposed to integrate neural-based ZP recovery with SMT systems, showing better performance on both ZP recovery and overall translation. When entering the era of NMT, ZP recovery is also employed as an external system. Assuming that no-pro-drop languages can benefit pro-drop ones, Ohtani et al. (2019) tagged the coreference information in the source language, and then encoded it using a graph-based encoder integrated with NMT model. Tan et al. (2019) recovered ZP in the source sentence via a BiLSTM-CRF model (Lample et al., 2016). Different from the conventional ZP recovery methods, the label is the corresponding translation of ZP around with special tokens. They then trained a NMT model on this modified data, letting the model learn the copy behaviors. Tan et al. (2021) used ZP detector to predict the ZP position and inserted a special token. Second, they used a attention-based ZP recovery model to recover the ZP word on the corresponding ZP position.', 'End-to-End Due the lack of training data on ZPT, a couple of studies pay attention to data augmentation. Sugiyama and Yoshinaga (2019) employed the back-translation on a context-aware NMT model to augment the training data. With the help of context, the pronoun in no-pronoun-drop language can be translated correctly into pronoundrop language. They also build a contrastive dataset to filter the pseudo data. Besides, Kimura et al.', '(2019) investigated the selective standards in detail to filter the pseudo data. Ri et al. (2021) deleted the personal pronoun in the sentence to augment the training data. And they trained a classifier to keep the sentences that pronouns can be recovered without any context. About model architecture, Wang et al. (2018a) first proposed a reconstruction-based approach to reconstruct the ZP-annotated source sentence from the hidden states of either encoder or decoder, or both. The central idea behind is to guide the corresponding hidden states to embed the recalled source-side ZP information and subsequently to help the NMT model generate the missing pronouns with these enhanced hidden representations. Although this model achieved significant improvements, there nonetheless exist two drawbacks: 1) there is no interaction between the two separate reconstructors, which misses the opportunity to exploit useful relations between encoder and decoder representations; and 2) testing phase needs an external ZP prediction model and it only has an accuracy of 66% in F1-score, which propagates numerous errors to the translation model. Thus, Wang et al. (2018b) further proposed to improve the reconstruction-based model by using shared reconstructor and joint learning. Furthermore, relying on external ZP models in decoding makes these approaches unwieldy in practice, due to introducing more computation cost and complexity.', 'About learning objective, contrastive learning is often used to let the output more close to golden data while far away from negative samples. Yang et al. (2019b) proposed a contrastive learning to reduce the word omitted error. To construct the negative samples, they randomly dropped the word by considering its frequency or part-of-speech tag. Hwang et al. (2021) further considered the coreference information to construct the negative sample. According to the coreference information, they took place the antecedent in context with empty, mask or random token to get the negative samples. Besides, Jwalapuram et al. (2020) served the pronoun mistranslated output as the negative samples while golden sentences as positive sample. To get the negative samples, they aligned the word between model outputs and golden references to get the sentences with mistranslated pronoun.', 'Implicit Some works consider not just the ZPT issue but rather focus on the overall discourse problem. The document-level NMT models (Wang et al., 2017a;Werlen et al., 2018;Ma et al., 2020;Lopes et al., 2020) are expected to have strong capabilities in discourse modelling such as translation consistency and ZPT. Another method is the round-trip translation, which is commonly-used in automatic post-editing (APE) (Freitag et al., 2019), quality estimation (QE) (Moon et al., 2020) to correct of detect the translation errors. Voita et al. (2019) served this idea on context-aware NMT to correct the discourse error in the output. They employed the round-trip translation on monolingual data to get the parallel corpus in the target language. They then used the corpus to train a model to repair discourse phenomenon in MT output.  proposed a fully unified ZPT model, which absolutely released the reliance on external ZP models at decoding time. Besides, they exploited to jointly learn inter-sentential con-  Table 2: A comparison of representative ZPT methods with different benchmarks. The ZPT methods are detailed in Section 5.1. The Baseline is a standard Transformer-big model while ORACLE is manually recovering ZPs in input sentences and then feeding them into the Baseline (Wu et al., 2020). As detailed in Section 4.1, TVSub (both translation and ZP training data) and BaiduKnows (ZP training data) are widely-used benchmarks in movie subtitle and Q&A forum domains, respectively. The Webnovel is our in-house testing data (no training data) in web fiction domain. As detailed in Section 6.1, BLEU is a general-purpose evaluation metric while APT is a ZP-targeted one.', 'text (Sordoni et al., 2015) to further improve ZP prediction and translation. Table 1 shows that only the TVsub is suitable for both training and testing in ZPT task, while others like LATL is too small and only suitable for testing. To facilitate fair and comprehensive comparisons of different models across different benchmarkss, we expanded the BaiduKnows by adding human translations and included in-house dataset 10 . As shown in Table 2, we re-implemented three representative ZPT methods and conducted experiments on three benchmarks, which are diverse in terms of domain, size, annotation type, and task. As the training data in three benchmarks decrease, the difficulty of modelling ZPT gradually increases.']","Early researchers have investigated several approaches for conventional statistical machine translation (SMT) (Le Nagard and Koehn, 2010; Xiang et al., 2013;Wang et al., 2016a). Modeling ZPs for advanced NMT models, however, has received more attention, resulting in better performance in this field (Wang et al., 2018a;Tan et al., 2021;Hwang et al., 2021). Generally prior works fall into three categories: (1) Pipeline, where input sentences are labeled with ZPs using an external ZP recovery system and then fed into a standard MT model (Chung and Gildea, 2010;Wang et al., 2016a); (2) Implicit, where ZP phenomenon is implicitly resolved by modelling document-level contexts Ri et al., 2021); (3) Endto-End, where ZP prediction and translation are jointly learned in an end-to-end manner Tan et al., 2021).

Pipeline The pipeline method of ZPT borrows from that in pronoun translation (Le Nagard and Koehn, 2010;Pradhan et al., 2012) due to the strong relevance between the two tasks. Chung and Gildea (2010) systematically examine the effects of empty category (EC) 9 on SMT with pattern-, CRF-and parsing-based methods. The results show that this can really improve the translation quality, even though the automatic prediction of EC is not highly accurate. Besides, Wang et al. (2016aWang et al. ( ,b, 2017b proposed to integrate neural-based ZP recovery with SMT systems, showing better performance on both ZP recovery and overall translation. When entering the era of NMT, ZP recovery is also employed as an external system. Assuming that no-pro-drop languages can benefit pro-drop ones, Ohtani et al. (2019) tagged the coreference information in the source language, and then encoded it using a graph-based encoder integrated with NMT model. Tan et al. (2019) recovered ZP in the source sentence via a BiLSTM-CRF model (Lample et al., 2016). Different from the conventional ZP recovery methods, the label is the corresponding translation of ZP around with special tokens. They then trained a NMT model on this modified data, letting the model learn the copy behaviors. Tan et al. (2021) used ZP detector to predict the ZP position and inserted a special token. Second, they used a attention-based ZP recovery model to recover the ZP word on the corresponding ZP position.

End-to-End Due the lack of training data on ZPT, a couple of studies pay attention to data augmentation. Sugiyama and Yoshinaga (2019) employed the back-translation on a context-aware NMT model to augment the training data. With the help of context, the pronoun in no-pronoun-drop language can be translated correctly into pronoundrop language. They also build a contrastive dataset to filter the pseudo data. Besides, Kimura et al.

(2019) investigated the selective standards in detail to filter the pseudo data. Ri et al. (2021) deleted the personal pronoun in the sentence to augment the training data. And they trained a classifier to keep the sentences that pronouns can be recovered without any context. About model architecture, Wang et al. (2018a) first proposed a reconstruction-based approach to reconstruct the ZP-annotated source sentence from the hidden states of either encoder or decoder, or both. The central idea behind is to guide the corresponding hidden states to embed the recalled source-side ZP information and subsequently to help the NMT model generate the missing pronouns with these enhanced hidden representations. Although this model achieved significant improvements, there nonetheless exist two drawbacks: 1) there is no interaction between the two separate reconstructors, which misses the opportunity to exploit useful relations between encoder and decoder representations; and 2) testing phase needs an external ZP prediction model and it only has an accuracy of 66% in F1-score, which propagates numerous errors to the translation model. Thus, Wang et al. (2018b) further proposed to improve the reconstruction-based model by using shared reconstructor and joint learning. Furthermore, relying on external ZP models in decoding makes these approaches unwieldy in practice, due to introducing more computation cost and complexity.

About learning objective, contrastive learning is often used to let the output more close to golden data while far away from negative samples. Yang et al. (2019b) proposed a contrastive learning to reduce the word omitted error. To construct the negative samples, they randomly dropped the word by considering its frequency or part-of-speech tag. Hwang et al. (2021) further considered the coreference information to construct the negative sample. According to the coreference information, they took place the antecedent in context with empty, mask or random token to get the negative samples. Besides, Jwalapuram et al. (2020) served the pronoun mistranslated output as the negative samples while golden sentences as positive sample. To get the negative samples, they aligned the word between model outputs and golden references to get the sentences with mistranslated pronoun.

Implicit Some works consider not just the ZPT issue but rather focus on the overall discourse problem. The document-level NMT models (Wang et al., 2017a;Werlen et al., 2018;Ma et al., 2020;Lopes et al., 2020) are expected to have strong capabilities in discourse modelling such as translation consistency and ZPT. Another method is the round-trip translation, which is commonly-used in automatic post-editing (APE) (Freitag et al., 2019), quality estimation (QE) (Moon et al., 2020) to correct of detect the translation errors. Voita et al. (2019) served this idea on context-aware NMT to correct the discourse error in the output. They employed the round-trip translation on monolingual data to get the parallel corpus in the target language. They then used the corpus to train a model to repair discourse phenomenon in MT output.  proposed a fully unified ZPT model, which absolutely released the reliance on external ZP models at decoding time. Besides, they exploited to jointly learn inter-sentential con-  Table 2: A comparison of representative ZPT methods with different benchmarks. The ZPT methods are detailed in Section 5.1. The Baseline is a standard Transformer-big model while ORACLE is manually recovering ZPs in input sentences and then feeding them into the Baseline (Wu et al., 2020). As detailed in Section 4.1, TVSub (both translation and ZP training data) and BaiduKnows (ZP training data) are widely-used benchmarks in movie subtitle and Q&A forum domains, respectively. The Webnovel is our in-house testing data (no training data) in web fiction domain. As detailed in Section 6.1, BLEU is a general-purpose evaluation metric while APT is a ZP-targeted one.

text (Sordoni et al., 2015) to further improve ZP prediction and translation. Table 1 shows that only the TVsub is suitable for both training and testing in ZPT task, while others like LATL is too small and only suitable for testing. To facilitate fair and comprehensive comparisons of different models across different benchmarkss, we expanded the BaiduKnows by adding human translations and included in-house dataset 10 . As shown in Table 2, we re-implemented three representative ZPT methods and conducted experiments on three benchmarks, which are diverse in terms of domain, size, annotation type, and task. As the training data in three benchmarks decrease, the difficulty of modelling ZPT gradually increases.","(p14.0) Early researchers have investigated several approaches for conventional statistical machine translation (SMT) (Le Nagard and Koehn, 2010; Xiang et al., 2013;Wang et al., 2016a). Modeling ZPs for advanced NMT models, however, has received more attention, resulting in better performance in this field (Wang et al., 2018a;Tan et al., 2021;Hwang et al., 2021). Generally prior works fall into three categories: (1) Pipeline, where input sentences are labeled with ZPs using an external ZP recovery system and then fed into a standard MT model (Chung and Gildea, 2010;Wang et al., 2016a); (2) Implicit, where ZP phenomenon is implicitly resolved by modelling document-level contexts Ri et al., 2021); (3) Endto-End, where ZP prediction and translation are jointly learned in an end-to-end manner Tan et al., 2021).

(p14.1) Pipeline The pipeline method of ZPT borrows from that in pronoun translation (Le Nagard and Koehn, 2010;Pradhan et al., 2012) due to the strong relevance between the two tasks. Chung and Gildea (2010) systematically examine the effects of empty category (EC) 9 on SMT with pattern-, CRF-and parsing-based methods. The results show that this can really improve the translation quality, even though the automatic prediction of EC is not highly accurate. Besides, Wang et al. (2016aWang et al. ( ,b, 2017b proposed to integrate neural-based ZP recovery with SMT systems, showing better performance on both ZP recovery and overall translation. When entering the era of NMT, ZP recovery is also employed as an external system. Assuming that no-pro-drop languages can benefit pro-drop ones, Ohtani et al. (2019) tagged the coreference information in the source language, and then encoded it using a graph-based encoder integrated with NMT model. Tan et al. (2019) recovered ZP in the source sentence via a BiLSTM-CRF model (Lample et al., 2016). Different from the conventional ZP recovery methods, the label is the corresponding translation of ZP around with special tokens. They then trained a NMT model on this modified data, letting the model learn the copy behaviors. Tan et al. (2021) used ZP detector to predict the ZP position and inserted a special token. Second, they used a attention-based ZP recovery model to recover the ZP word on the corresponding ZP position.

(p14.2) End-to-End Due the lack of training data on ZPT, a couple of studies pay attention to data augmentation. Sugiyama and Yoshinaga (2019) employed the back-translation on a context-aware NMT model to augment the training data. With the help of context, the pronoun in no-pronoun-drop language can be translated correctly into pronoundrop language. They also build a contrastive dataset to filter the pseudo data. Besides, Kimura et al.

(p14.3) (2019) investigated the selective standards in detail to filter the pseudo data. Ri et al. (2021) deleted the personal pronoun in the sentence to augment the training data. And they trained a classifier to keep the sentences that pronouns can be recovered without any context. About model architecture, Wang et al. (2018a) first proposed a reconstruction-based approach to reconstruct the ZP-annotated source sentence from the hidden states of either encoder or decoder, or both. The central idea behind is to guide the corresponding hidden states to embed the recalled source-side ZP information and subsequently to help the NMT model generate the missing pronouns with these enhanced hidden representations. Although this model achieved significant improvements, there nonetheless exist two drawbacks: 1) there is no interaction between the two separate reconstructors, which misses the opportunity to exploit useful relations between encoder and decoder representations; and 2) testing phase needs an external ZP prediction model and it only has an accuracy of 66% in F1-score, which propagates numerous errors to the translation model. Thus, Wang et al. (2018b) further proposed to improve the reconstruction-based model by using shared reconstructor and joint learning. Furthermore, relying on external ZP models in decoding makes these approaches unwieldy in practice, due to introducing more computation cost and complexity.

(p14.4) About learning objective, contrastive learning is often used to let the output more close to golden data while far away from negative samples. Yang et al. (2019b) proposed a contrastive learning to reduce the word omitted error. To construct the negative samples, they randomly dropped the word by considering its frequency or part-of-speech tag. Hwang et al. (2021) further considered the coreference information to construct the negative sample. According to the coreference information, they took place the antecedent in context with empty, mask or random token to get the negative samples. Besides, Jwalapuram et al. (2020) served the pronoun mistranslated output as the negative samples while golden sentences as positive sample. To get the negative samples, they aligned the word between model outputs and golden references to get the sentences with mistranslated pronoun.

(p14.5) Implicit Some works consider not just the ZPT issue but rather focus on the overall discourse problem. The document-level NMT models (Wang et al., 2017a;Werlen et al., 2018;Ma et al., 2020;Lopes et al., 2020) are expected to have strong capabilities in discourse modelling such as translation consistency and ZPT. Another method is the round-trip translation, which is commonly-used in automatic post-editing (APE) (Freitag et al., 2019), quality estimation (QE) (Moon et al., 2020) to correct of detect the translation errors. Voita et al. (2019) served this idea on context-aware NMT to correct the discourse error in the output. They employed the round-trip translation on monolingual data to get the parallel corpus in the target language. They then used the corpus to train a model to repair discourse phenomenon in MT output.  proposed a fully unified ZPT model, which absolutely released the reliance on external ZP models at decoding time. Besides, they exploited to jointly learn inter-sentential con-  Table 2: A comparison of representative ZPT methods with different benchmarks. The ZPT methods are detailed in Section 5.1. The Baseline is a standard Transformer-big model while ORACLE is manually recovering ZPs in input sentences and then feeding them into the Baseline (Wu et al., 2020). As detailed in Section 4.1, TVSub (both translation and ZP training data) and BaiduKnows (ZP training data) are widely-used benchmarks in movie subtitle and Q&A forum domains, respectively. The Webnovel is our in-house testing data (no training data) in web fiction domain. As detailed in Section 6.1, BLEU is a general-purpose evaluation metric while APT is a ZP-targeted one.

(p14.6) text (Sordoni et al., 2015) to further improve ZP prediction and translation. Table 1 shows that only the TVsub is suitable for both training and testing in ZPT task, while others like LATL is too small and only suitable for testing. To facilitate fair and comprehensive comparisons of different models across different benchmarkss, we expanded the BaiduKnows by adding human translations and included in-house dataset 10 . As shown in Table 2, we re-implemented three representative ZPT methods and conducted experiments on three benchmarks, which are diverse in terms of domain, size, annotation type, and task. As the training data in three benchmarks decrease, the difficulty of modelling ZPT gradually increases.","[['b34', 'b16', 'b46', None, 'b38', 'b25'], ['b7', None, 'b12', 'b38', 'b25', 'b24'], ['b22'], ['b37', 'b34', 'b16'], [None, 'b50'], ['b36', 'b4', 'b29', 'b45', 'b6', 'b44', 'b1'], ['b20']]","[['b34', 'b16', 'b46', None, 'b38', 'b25'], ['b7', None, 'b12', 'b38', 'b25', 'b24'], ['b22'], ['b37', 'b34', 'b16'], [None, 'b50'], ['b36', 'b4', 'b29', 'b45', 'b6', 'b44', 'b1'], ['b20']]",26,"1. Early researchers have investigated several approaches for conventional statistical machine translation (SMT) (Le Nagard and Koehn, 2010; Xiang et al., 2013;Wang et al., 2016a).
2. Modeling ZPs for advanced NMT models, however, has received more attention, resulting in better performance in this field (Wang et al., 2018a;Tan et al., 2021;Hwang et al., 2021).
3. Generally prior works fall into three categories: (1) Pipeline, where input sentences are labeled with ZPs using an external ZP recovery system and then fed into a standard MT model (Chung and Gildea, 2010;Wang et al., 2016a); (2) Implicit, where ZP phenomenon is implicitly resolved by modelling document-level contexts
4. Ri et al., 2021); (3) Endto-End, where ZP prediction and translation are jointly learned in an end-to-end manner Tan et al., 2021).
5. Pipeline The pipeline method of ZPT borrows from that in pronoun translation (Le Nagard and Koehn, 2010;Pradhan et al., 2012) due to the strong relevance between the two tasks.
6. Chung and Gildea (2010) systematically examine the effects of empty category (EC) 9 on SMT with pattern-, CRF-and parsing-based methods.
7. The results show that this can really improve the translation quality, even though the automatic prediction of EC is not highly accurate.
8. Besides, Wang et al. (2016aWang et al. ( ,b, 2017b proposed to integrate neural-based ZP recovery with SMT systems, showing better performance on both ZP recovery and overall translation. When entering the era of NMT, ZP recovery is also employed as an external system. Assuming that no-pro-drop languages can benefit pro-drop ones, Ohtani et al. (2019) tagged the coreference information in the source language, and then encoded it using a graph-based encoder integrated with NMT model.
9. Tan et al. (2019) recovered ZP in the source sentence via a BiLSTM-CRF model (Lample et al., 2016).
10. Different from the conventional ZP recovery methods, the label is the corresponding translation of ZP around with special tokens.
11. They then trained a NMT model on this modified data, letting the model learn the copy behaviors.
12. Tan et al. (2021) used ZP detector to predict the ZP position and inserted a special token.
13. Second, they used a attention-based ZP recovery model to recover the ZP word on the corresponding ZP position.
14. End-to-End Due the lack of training data on ZPT, a couple of studies pay attention to data augmentation.
15. Sugiyama and Yoshinaga (2019) employed the back-translation on a context-aware NMT model to augment the training data.
16. With the help of context, the pronoun in no-pronoun-drop language can be translated correctly into pronoundrop language.
17. They also build a contrastive dataset to filter the pseudo data.
18. Besides, Kimura et al.(2019) investigated the selective standards in detail to filter the pseudo data.
19. Ri et al. (2021) deleted the personal pronoun in the sentence to augment the training data.
20. And they trained a classifier to keep the sentences that pronouns can be recovered without any context.
21. About model architecture, Wang et al. (2018a) first proposed a reconstruction-based approach to reconstruct the ZP-annotated source sentence from the hidden states of either encoder or decoder, or both.
22. The central idea behind is to guide the corresponding hidden states to embed the recalled source-side ZP information and subsequently to help the NMT model generate the missing pronouns with these enhanced hidden representations.
23. Although this model achieved significant improvements, there nonetheless exist two drawbacks: 1) there is no interaction between the two separate reconstructors, which misses the opportunity to exploit useful relations between encoder and decoder representations; and 2) testing phase needs an external ZP prediction model and it only has an accuracy of 66% in F1-score, which propagates numerous errors to the translation model.
24. Thus, Wang et al. (2018b) further proposed to improve the reconstruction-based model by using shared reconstructor and joint learning.
25. Furthermore, relying on external ZP models in decoding makes these approaches unwieldy in practice, due to introducing more computation cost and complexity.
26. About learning objective, contrastive learning is often used to let the output more close to golden data while far away from negative samples.
27. Yang et al. (2019b) proposed a contrastive learning to reduce the word omitted error.
28. To construct the negative samples, they randomly dropped the word by considering its frequency or part-of-speech tag.
29. Hwang et al. (2021) further considered the coreference information to construct the negative sample.
30. According to the coreference information, they took place the antecedent in context with empty, mask or random token to get the negative samples.
31. Besides, Jwalapuram et al. (2020) served the pronoun mistranslated output as the negative samples while golden sentences as positive sample.
32. To get the negative samples, they aligned the word between model outputs and golden references to get the sentences with mistranslated pronoun.
33. Implicit Some works consider not just the ZPT issue but rather focus on the overall discourse problem.
34. The document-level NMT models (Wang et al., 2017a;Werlen et al., 2018;Ma et al., 2020;Lopes et al., 2020) are expected to have strong capabilities in discourse modelling such as translation consistency and ZPT.
35. Another method is the round-trip translation, which is commonly-used in automatic post-editing (APE) (Freitag et al., 2019), quality estimation (QE) (Moon et al., 2020) to correct of detect the translation errors.
36. Voita et al. (2019) served this idea on context-aware NMT to correct the discourse error in the output.
37. They employed the round-trip translation on monolingual data to get the parallel corpus in the target language.
38. They then used the corpus to train a model to repair discourse phenomenon in MT output.
39. proposed a fully unified ZPT model, which absolutely released the reliance on external ZP models at decoding time.
40. Besides, they exploited to jointly learn inter-sentential con-  Table 2: A comparison of representative ZPT methods with different benchmarks.
41. The ZPT methods are detailed in Section 5.1.
42. The Baseline is a standard Transformer-big model while ORACLE is manually recovering ZPs in input sentences and then feeding them into the Baseline (Wu et al., 2020).
43. As detailed in Section 4.1, TVSub (both translation and ZP training data) and BaiduKnows (ZP training data) are widely-used benchmarks in movie subtitle and Q&A forum domains, respectively.
44. The Webnovel is our in-house testing data (no training data) in web fiction domain.
45. As detailed in Section 6.1, BLEU is a general-purpose evaluation metric while APT is a ZP-targeted one.
46. text (Sordoni et al., 2015) to further improve ZP prediction and translation.
47. Table 1 shows that only the TVsub is suitable for both training and testing in ZPT task, while others like LATL is too small and only suitable for testing.
48. To facilitate fair and comprehensive comparisons of different models across different benchmarkss, we expanded the BaiduKnows by adding human translations and included in-house dataset 10 .
49. As shown in Table 2, we re-implemented three representative ZPT methods and conducted experiments on three benchmarks, which are diverse in terms of domain, size, annotation type, and task.
50. As the training data in three benchmarks decrease, the difficulty of modelling ZPT gradually increases."
258740687,A Survey on Zero Pronoun Translation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/8fa265a8ca46c9c0ad35d2c5b519a86e2a092dfe,s12,Become An Independent Research Problem.,"['p12.0', 'p12.1', 'p12.2']","['Early works extracted ZP information from closed annotations (e.g. OntoNotes and Treebanks) (Yang and Xue, 2010;Chung and Gildea, 2010), which were considered as a sub-problem of coreference or syntactic parsing. With further investigation on the problem, MT community payed more attention to it by manually or automatically constructing ZP recovery and translation datasets (e.g. BaiduKnows and TVsub) (Wang et al., 2018a;. 4. Coping with Data Scarcity. The scarcity of ZPT data remains a core issue (currently only 2.2M ∼ 0.1K sentences) due to two challenges:', '(1) it requires experts for both source ZP annotation and target translation (Wang et al., 2016c(Wang et al., , 2018a; (2) annotating the training data manually spends much time and money. Nonetheless, it is still necessary to establish testing datasets for validating/analyzing the model performance.', 'Besides, pre-trained modes are already equipped with some capabilities on discourse (Chen et al., 2019;Koto et al., 2021). This highlights the importance of formulating the downstream task in a manner that can effectively leverage the capabilities of the pre-trained models.']","Early works extracted ZP information from closed annotations (e.g. OntoNotes and Treebanks) (Yang and Xue, 2010;Chung and Gildea, 2010), which were considered as a sub-problem of coreference or syntactic parsing. With further investigation on the problem, MT community payed more attention to it by manually or automatically constructing ZP recovery and translation datasets (e.g. BaiduKnows and TVsub) (Wang et al., 2018a;. 4. Coping with Data Scarcity. The scarcity of ZPT data remains a core issue (currently only 2.2M ∼ 0.1K sentences) due to two challenges:

(1) it requires experts for both source ZP annotation and target translation (Wang et al., 2016c(Wang et al., , 2018a; (2) annotating the training data manually spends much time and money. Nonetheless, it is still necessary to establish testing datasets for validating/analyzing the model performance.

Besides, pre-trained modes are already equipped with some capabilities on discourse (Chen et al., 2019;Koto et al., 2021). This highlights the importance of formulating the downstream task in a manner that can effectively leverage the capabilities of the pre-trained models.","(p12.0) Early works extracted ZP information from closed annotations (e.g. OntoNotes and Treebanks) (Yang and Xue, 2010;Chung and Gildea, 2010), which were considered as a sub-problem of coreference or syntactic parsing. With further investigation on the problem, MT community payed more attention to it by manually or automatically constructing ZP recovery and translation datasets (e.g. BaiduKnows and TVsub) (Wang et al., 2018a;. 4. Coping with Data Scarcity. The scarcity of ZPT data remains a core issue (currently only 2.2M ∼ 0.1K sentences) due to two challenges:

(p12.1) (1) it requires experts for both source ZP annotation and target translation (Wang et al., 2016c(Wang et al., , 2018a; (2) annotating the training data manually spends much time and money. Nonetheless, it is still necessary to establish testing datasets for validating/analyzing the model performance.

(p12.2) Besides, pre-trained modes are already equipped with some capabilities on discourse (Chen et al., 2019;Koto et al., 2021). This highlights the importance of formulating the downstream task in a manner that can effectively leverage the capabilities of the pre-trained models.","[['b49', 'b34', None], ['b34', 'b42'], [None, 'b30']]","[['b49', 'b34', None], ['b34', 'b42'], [None, 'b30']]",7,"1. Early works extracted ZP information from closed annotations (e.g. OntoNotes and Treebanks) (Yang and Xue, 2010;Chung and Gildea, 2010), which were considered as a sub-problem of coreference or syntactic parsing.
2. With further investigation on the problem, MT community payed more attention to it by manually or automatically constructing ZP recovery and translation datasets (e.g. BaiduKnows and TVsub) (Wang et al., 2018a;. 4. Coping with Data Scarcity. The scarcity of ZPT data remains a core issue (currently only 2.2M ∼ 0.1K sentences) due to two challenges:
3. (1) it requires experts for both source ZP annotation and target translation (Wang et al., 2016c(Wang et al., , 2018a; (2) annotating the training data manually spends much time and money.
4. Nonetheless, it is still necessary to establish testing datasets for validating/analyzing the model performance.
5. Besides, pre-trained modes are already equipped with some capabilities on discourse (Chen et al., 2019;Koto et al., 2021).
6. This highlights the importance of formulating the downstream task in a manner that can effectively leverage the capabilities of the pre-trained models."
258740687,A Survey on Zero Pronoun Translation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/8fa265a8ca46c9c0ad35d2c5b519a86e2a092dfe,s19,Overview,"['p19.0', 'p19.1']","['There are three kinds of automatic metrics to evaluate performances of related models:', '• Accuracy of ZP Recovery: this aims to measure model performance on detecting and predicting ZPs of sentences in one pro-drop language. For instance, the micro F1-score is used to evaluating Chinese ZPR systems Song et al. (2020). 11 • General Translation Quality: there are a number of automatic evaluation metrics for measuring general performance of MT systems (Snover  Table 4: Correlation between the manual evaluation and other automatic metrics, which are applied on different ZPT benchmarks, which are same as in Table 2. et al., 2006). BLEU (Papineni et al., 2002) is the most widely-used one, which measures the precision of n-grams of the MT output compared to the reference, weighted by a brevity penalty to punish overly short translations. ME-TEOR (Banerjee and Lavie, 2005) incorporates semantic information by calculating either exact match, stem match, or synonymy match. Furthermore, COMET (Rei et al., 2020) is a neural framework for training multilingual MT evaluation models which obtains new SOTA levels of correlation with human judgements. • Pronoun-Aware Translation Quality: Previous works usually evaluate ZPT using the BLEU metric (Wang et al., 2016a(Wang et al., , 2018aRi et al., 2021), however, general-purpose metrics cannot characterize the performance of ZP translation. As shown in Table 3, the missed or incorrect pronouns may not affect BLEU scores but severely harm true performances. To fix this gap, some works proposed pronoun-targeted evaluation metrics (Werlen and Popescu-Belis, 2017; Läubli et al., 2018).']","There are three kinds of automatic metrics to evaluate performances of related models:

• Accuracy of ZP Recovery: this aims to measure model performance on detecting and predicting ZPs of sentences in one pro-drop language. For instance, the micro F1-score is used to evaluating Chinese ZPR systems Song et al. (2020). 11 • General Translation Quality: there are a number of automatic evaluation metrics for measuring general performance of MT systems (Snover  Table 4: Correlation between the manual evaluation and other automatic metrics, which are applied on different ZPT benchmarks, which are same as in Table 2. et al., 2006). BLEU (Papineni et al., 2002) is the most widely-used one, which measures the precision of n-grams of the MT output compared to the reference, weighted by a brevity penalty to punish overly short translations. ME-TEOR (Banerjee and Lavie, 2005) incorporates semantic information by calculating either exact match, stem match, or synonymy match. Furthermore, COMET (Rei et al., 2020) is a neural framework for training multilingual MT evaluation models which obtains new SOTA levels of correlation with human judgements. • Pronoun-Aware Translation Quality: Previous works usually evaluate ZPT using the BLEU metric (Wang et al., 2016a(Wang et al., , 2018aRi et al., 2021), however, general-purpose metrics cannot characterize the performance of ZP translation. As shown in Table 3, the missed or incorrect pronouns may not affect BLEU scores but severely harm true performances. To fix this gap, some works proposed pronoun-targeted evaluation metrics (Werlen and Popescu-Belis, 2017; Läubli et al., 2018).","(p19.0) There are three kinds of automatic metrics to evaluate performances of related models:

(p19.1) • Accuracy of ZP Recovery: this aims to measure model performance on detecting and predicting ZPs of sentences in one pro-drop language. For instance, the micro F1-score is used to evaluating Chinese ZPR systems Song et al. (2020). 11 • General Translation Quality: there are a number of automatic evaluation metrics for measuring general performance of MT systems (Snover  Table 4: Correlation between the manual evaluation and other automatic metrics, which are applied on different ZPT benchmarks, which are same as in Table 2. et al., 2006). BLEU (Papineni et al., 2002) is the most widely-used one, which measures the precision of n-grams of the MT output compared to the reference, weighted by a brevity penalty to punish overly short translations. ME-TEOR (Banerjee and Lavie, 2005) incorporates semantic information by calculating either exact match, stem match, or synonymy match. Furthermore, COMET (Rei et al., 2020) is a neural framework for training multilingual MT evaluation models which obtains new SOTA levels of correlation with human judgements. • Pronoun-Aware Translation Quality: Previous works usually evaluate ZPT using the BLEU metric (Wang et al., 2016a(Wang et al., , 2018aRi et al., 2021), however, general-purpose metrics cannot characterize the performance of ZP translation. As shown in Table 3, the missed or incorrect pronouns may not affect BLEU scores but severely harm true performances. To fix this gap, some works proposed pronoun-targeted evaluation metrics (Werlen and Popescu-Belis, 2017; Läubli et al., 2018).","[[], ['b34', 'b15', 'b8', 'b16', None, 'b38', 'b19']]","[[], ['b34', 'b15', 'b8', 'b16', None, 'b38', 'b19']]",7,"1. There are three kinds of automatic metrics to evaluate performances of related models:• Accuracy of ZP Recovery: this aims to measure model performance on detecting and predicting ZPs of sentences in one pro-drop language.
2. For instance, the micro F1-score is used to evaluating Chinese ZPR systems Song et al. (2020).
3. 11 • General Translation Quality: there are a number of automatic evaluation metrics for measuring general performance of MT systems (Snover  Table 4: Correlation between the manual evaluation and other automatic metrics, which are applied on different ZPT benchmarks, which are same as in Table 2. et al., 2006).
4. BLEU (Papineni et al., 2002) is the most widely-used one, which measures the precision of n-grams of the MT output compared to the reference, weighted by a brevity penalty to punish overly short translations.
5. ME-TEOR (Banerjee and Lavie, 2005) incorporates semantic information by calculating either exact match, stem match, or synonymy match.
6. Furthermore, COMET (Rei et al., 2020) is a neural framework for training multilingual MT evaluation models which obtains new SOTA levels of correlation with human judgements.
7. • Pronoun-Aware Translation Quality: Previous works usually evaluate ZPT using the BLEU metric (Wang et al., 2016a(Wang et al., , 2018aRi et al., 2021), however, general-purpose metrics cannot characterize the performance of ZP translation.
8. As shown in Table 3, the missed or incorrect pronouns may not affect BLEU scores but severely harm true performances.
9. To fix this gap, some works proposed pronoun-targeted evaluation metrics (Werlen and Popescu-Belis, 2017; Läubli et al., 2018)."
258740687,A Survey on Zero Pronoun Translation,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/8fa265a8ca46c9c0ad35d2c5b519a86e2a092dfe,s10,Overview,"['p10.0', 'p10.1']","['Modeling ZPs has so far not been extensively explored in prior research, largely due to the lack of publicly available data sets. Existing works mostly focused on human-annotated, small-scale and single-domain corpora such as OntoNotes (Pradhan et al., 2012;Aloraini and Poesio, 2020) and Treebanks (Yang and Xue, 2010;Chung and Gildea, 2010). We summarize representative corpora as:', '• OntoNotes. 5 This is annotated with structural information (e.g. syntax and predicate argument structure) and shallow semantics (e.g. word sense linked to an ontology and coreference). It comprises various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in English, Chinese, and Arabic languages. ZP sentences are extracted for ZP resolution task (Chen and Ng, 2013, 2016). • TVSub. 6 This extracts Chinese-English subtitles from television episodes. Its source-side sentences are automatically annotated with ZPs by a heuristic algorithm (Wang et al., 2016a), which was generally used to study dialogue translation and zero anaphora phenomenon (Wang et al., 2018a;Tan et al., 2021). • CTB. 7 This is a part-of-speech tagged and fully bracketed Chinese language corpus. The text are extracted from various domains including newswire, government documents, magazine articles, various broadcast news and broadcast conversation programs, web newsgroups and weblogs. Instances with empty category are extracted for ZP recovery task (Yang and Xue, 2010;Chung and Gildea, 2010). • BaiduKnows. The source-side sentences are collected from the Baidu Knows website, 8 which were annotated with ZP labels with boundary tags. It is widely-used the task of ZP recovery Song et al., 2020).  (Yang and Xue, 2010) ZH Human News 10.6K ✗ ✓ ✗ KTB (Chung and Gildea, 2010) KO Human News 5.0K ✗ ✓ ✗ BaiduKnows  ZH Human Baidu Knows 5.0K ✗ ✓ ✗ TVsub (Wang et al., 2018a) ZH, EN Auto Movie Subtitles 2.2M ✗ ✗ ✓ ZAC (Pereira, 2009) PT Human Mixed Sources 0.6K ✓ ✗ ✗ Nagoya (Zhan and Nakaiwa, 2015) JA Auto Scientific Paper 1.2K ✓ ✗ ✗ SKKU (Park et al., 2015) KO Human Dialogue 1.1K ✓ ✗ ✗ UPENN (Prasad, 2000) HI Human News 2.2K ✓ ✗ ✗ LATL (Russo et al., 2012) IT, ES Human Europarl 2.0K ✓ ✗ ✓ UCFV (Bacolini, 2017) HE Human Dialogue 0.1K ✓ ✗ ✗ Table 1: A summary of existing datasets regarding ZP. We classify them according to language (Lang.), annotation type (Anno.) and text domain. We also report the number of sentences (Size). ""Reso."", ""Reco."" and ""Trans."" indicate whether a dataset can be used for specific ZP tasks. The symbol ✓ or ✗ means ""Yes"" or ""No"".']","Modeling ZPs has so far not been extensively explored in prior research, largely due to the lack of publicly available data sets. Existing works mostly focused on human-annotated, small-scale and single-domain corpora such as OntoNotes (Pradhan et al., 2012;Aloraini and Poesio, 2020) and Treebanks (Yang and Xue, 2010;Chung and Gildea, 2010). We summarize representative corpora as:

• OntoNotes. 5 This is annotated with structural information (e.g. syntax and predicate argument structure) and shallow semantics (e.g. word sense linked to an ontology and coreference). It comprises various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in English, Chinese, and Arabic languages. ZP sentences are extracted for ZP resolution task (Chen and Ng, 2013, 2016). • TVSub. 6 This extracts Chinese-English subtitles from television episodes. Its source-side sentences are automatically annotated with ZPs by a heuristic algorithm (Wang et al., 2016a), which was generally used to study dialogue translation and zero anaphora phenomenon (Wang et al., 2018a;Tan et al., 2021). • CTB. 7 This is a part-of-speech tagged and fully bracketed Chinese language corpus. The text are extracted from various domains including newswire, government documents, magazine articles, various broadcast news and broadcast conversation programs, web newsgroups and weblogs. Instances with empty category are extracted for ZP recovery task (Yang and Xue, 2010;Chung and Gildea, 2010). • BaiduKnows. The source-side sentences are collected from the Baidu Knows website, 8 which were annotated with ZP labels with boundary tags. It is widely-used the task of ZP recovery Song et al., 2020).  (Yang and Xue, 2010) ZH Human News 10.6K ✗ ✓ ✗ KTB (Chung and Gildea, 2010) KO Human News 5.0K ✗ ✓ ✗ BaiduKnows  ZH Human Baidu Knows 5.0K ✗ ✓ ✗ TVsub (Wang et al., 2018a) ZH, EN Auto Movie Subtitles 2.2M ✗ ✗ ✓ ZAC (Pereira, 2009) PT Human Mixed Sources 0.6K ✓ ✗ ✗ Nagoya (Zhan and Nakaiwa, 2015) JA Auto Scientific Paper 1.2K ✓ ✗ ✗ SKKU (Park et al., 2015) KO Human Dialogue 1.1K ✓ ✗ ✗ UPENN (Prasad, 2000) HI Human News 2.2K ✓ ✗ ✗ LATL (Russo et al., 2012) IT, ES Human Europarl 2.0K ✓ ✗ ✓ UCFV (Bacolini, 2017) HE Human Dialogue 0.1K ✓ ✗ ✗ Table 1: A summary of existing datasets regarding ZP. We classify them according to language (Lang.), annotation type (Anno.) and text domain. We also report the number of sentences (Size). ""Reso."", ""Reco."" and ""Trans."" indicate whether a dataset can be used for specific ZP tasks. The symbol ✓ or ✗ means ""Yes"" or ""No"".","(p10.0) Modeling ZPs has so far not been extensively explored in prior research, largely due to the lack of publicly available data sets. Existing works mostly focused on human-annotated, small-scale and single-domain corpora such as OntoNotes (Pradhan et al., 2012;Aloraini and Poesio, 2020) and Treebanks (Yang and Xue, 2010;Chung and Gildea, 2010). We summarize representative corpora as:

(p10.1) • OntoNotes. 5 This is annotated with structural information (e.g. syntax and predicate argument structure) and shallow semantics (e.g. word sense linked to an ontology and coreference). It comprises various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in English, Chinese, and Arabic languages. ZP sentences are extracted for ZP resolution task (Chen and Ng, 2013, 2016). • TVSub. 6 This extracts Chinese-English subtitles from television episodes. Its source-side sentences are automatically annotated with ZPs by a heuristic algorithm (Wang et al., 2016a), which was generally used to study dialogue translation and zero anaphora phenomenon (Wang et al., 2018a;Tan et al., 2021). • CTB. 7 This is a part-of-speech tagged and fully bracketed Chinese language corpus. The text are extracted from various domains including newswire, government documents, magazine articles, various broadcast news and broadcast conversation programs, web newsgroups and weblogs. Instances with empty category are extracted for ZP recovery task (Yang and Xue, 2010;Chung and Gildea, 2010). • BaiduKnows. The source-side sentences are collected from the Baidu Knows website, 8 which were annotated with ZP labels with boundary tags. It is widely-used the task of ZP recovery Song et al., 2020).  (Yang and Xue, 2010) ZH Human News 10.6K ✗ ✓ ✗ KTB (Chung and Gildea, 2010) KO Human News 5.0K ✗ ✓ ✗ BaiduKnows  ZH Human Baidu Knows 5.0K ✗ ✓ ✗ TVsub (Wang et al., 2018a) ZH, EN Auto Movie Subtitles 2.2M ✗ ✗ ✓ ZAC (Pereira, 2009) PT Human Mixed Sources 0.6K ✓ ✗ ✗ Nagoya (Zhan and Nakaiwa, 2015) JA Auto Scientific Paper 1.2K ✓ ✗ ✗ SKKU (Park et al., 2015) KO Human Dialogue 1.1K ✓ ✗ ✗ UPENN (Prasad, 2000) HI Human News 2.2K ✓ ✗ ✗ LATL (Russo et al., 2012) IT, ES Human Europarl 2.0K ✓ ✗ ✓ UCFV (Bacolini, 2017) HE Human Dialogue 0.1K ✓ ✗ ✗ Table 1: A summary of existing datasets regarding ZP. We classify them according to language (Lang.), annotation type (Anno.) and text domain. We also report the number of sentences (Size). ""Reso."", ""Reco."" and ""Trans."" indicate whether a dataset can be used for specific ZP tasks. The symbol ✓ or ✗ means ""Yes"" or ""No"".","[[None, 'b12', 'b49'], ['b53', 'b49', 'b34', None, 'b38', 'b11', 'b25', 'b17', 'b19', 'b13', 'b9']]","[[None, 'b12', 'b49'], ['b53', 'b49', 'b34', None, 'b38', 'b11', 'b25', 'b17', 'b19', 'b13', 'b9']]",14,"1. Modeling ZPs has so far not been extensively explored in prior research, largely due to the lack of publicly available data sets.
2. Existing works mostly focused on human-annotated, small-scale and single-domain corpora such as OntoNotes (Pradhan et al., 2012;Aloraini and Poesio, 2020) and Treebanks (Yang and Xue, 2010;Chung and Gildea, 2010).
3. We summarize representative corpora as:• OntoNotes.
4. 5 This is annotated with structural information (e.g. syntax and predicate argument structure) and shallow semantics (e.g. word sense linked to an ontology and coreference).
5. It comprises various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in English, Chinese, and Arabic languages.
6. ZP sentences are extracted for ZP resolution task (Chen and Ng, 2013, 2016). • TVSub.
7. 6 This extracts Chinese-English subtitles from television episodes.
8. Its source-side sentences are automatically annotated with ZPs by a heuristic algorithm (Wang et al., 2016a), which was generally used to study dialogue translation and zero anaphora phenomenon (Wang et al., 2018a;Tan et al., 2021). • CTB. 7
9. This is a part-of-speech tagged and fully bracketed Chinese language corpus.
10. The text are extracted from various domains including newswire, government documents, magazine articles, various broadcast news and broadcast conversation programs, web newsgroups and weblogs.
11. Instances with empty category are extracted for ZP recovery task (Yang and Xue, 2010;Chung and Gildea, 2010).
12. • BaiduKnows. The source-side sentences are collected from the Baidu Knows website, 8 which were annotated with ZP labels with boundary tags.
13. It is widely-used the task of ZP recovery Song et al., 2020).
14. (Yang and Xue, 2010) ZH Human News 10.6K ✗ ✓ ✗ KTB (Chung and Gildea, 2010) KO Human News 5.0K ✗ ✓ ✗ BaiduKnows  ZH Human Baidu Knows 5.0K ✗ ✓ ✗ TVsub (Wang et al., 2018a) ZH, EN Auto Movie Subtitles 2.2M ✗ ✗ ✓ ZAC (Pereira, 2009) PT Human Mixed Sources 0.6K ✓ ✗ ✗ Nagoya (Zhan and Nakaiwa, 2015) JA Auto Scientific Paper 1.2K ✓ ✗ ✗ SKKU (Park et al., 2015) KO Human Dialogue 1.1K ✓ ✗ ✗ UPENN (Prasad, 2000) HI
15. Human News 2.2K ✓ ✗ ✗ LATL (Russo et al., 2012) IT, ES Human Europarl 2.0K ✓ ✗ ✓ UCFV (Bacolini, 2017) HE Human Dialogue 0.1K ✓ ✗ ✗ Table 1: A summary of existing datasets regarding ZP.
16. We classify them according to language (Lang.), annotation type (Anno.) and text domain.
17. We also report the number of sentences (Size).
18. ""Reso."", ""Reco."" and ""Trans."" indicate whether a dataset can be used for specific ZP tasks.
19. The symbol ✓ or ✗ means ""Yes"" or ""No""."
258832362,Beyond Words: A Comprehensive Survey of Sentence Representations,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/8579ad4a8e835cada64c0eae142a00205ce857b5,s14,Alternative Methods,"['p14.0', 'p14.1']","['Researchers have explored various other methods for obtaining positive samples for unsupervised sentence representations. One approach involves using other spans of text that either overlap with the given span or come from the same document (Giorgi et al., 2021). Another strategy is to use related entities as positives instead of text spans (Nishikawa et al., 2022). This approach is particularly useful for cross-lingual sentence representation learning, as information about the same entity is often available in different languages.', 'Other approaches use large language models that are capable of performing various tasks from observing few examples, even for tasks that they have never encountered before. Generally, larger models with more parameters tend to perform better (Brown et al., 2020). Moreover, these models can be directed to perform tasks by providing them with specific instructions-a technique known as ""prompting"". This prompting paradigm has become a popular approach in natural language processing. Researchers have used prompts to obtain better sentence representations, as demonstrated in studies such as , which employs the ""[X] means [MASK]"" prompt to extract sentence representations from the representation of the ""[MASK]"" token in a sentence. Another study by (Zeng et al., 2022) combines prompt-derived sentence representations with contrastive learning to improve the quality of the representations.']","Researchers have explored various other methods for obtaining positive samples for unsupervised sentence representations. One approach involves using other spans of text that either overlap with the given span or come from the same document (Giorgi et al., 2021). Another strategy is to use related entities as positives instead of text spans (Nishikawa et al., 2022). This approach is particularly useful for cross-lingual sentence representation learning, as information about the same entity is often available in different languages.

Other approaches use large language models that are capable of performing various tasks from observing few examples, even for tasks that they have never encountered before. Generally, larger models with more parameters tend to perform better (Brown et al., 2020). Moreover, these models can be directed to perform tasks by providing them with specific instructions-a technique known as ""prompting"". This prompting paradigm has become a popular approach in natural language processing. Researchers have used prompts to obtain better sentence representations, as demonstrated in studies such as , which employs the ""[X] means [MASK]"" prompt to extract sentence representations from the representation of the ""[MASK]"" token in a sentence. Another study by (Zeng et al., 2022) combines prompt-derived sentence representations with contrastive learning to improve the quality of the representations.","(p14.0) Researchers have explored various other methods for obtaining positive samples for unsupervised sentence representations. One approach involves using other spans of text that either overlap with the given span or come from the same document (Giorgi et al., 2021). Another strategy is to use related entities as positives instead of text spans (Nishikawa et al., 2022). This approach is particularly useful for cross-lingual sentence representation learning, as information about the same entity is often available in different languages.

(p14.1) Other approaches use large language models that are capable of performing various tasks from observing few examples, even for tasks that they have never encountered before. Generally, larger models with more parameters tend to perform better (Brown et al., 2020). Moreover, these models can be directed to perform tasks by providing them with specific instructions-a technique known as ""prompting"". This prompting paradigm has become a popular approach in natural language processing. Researchers have used prompts to obtain better sentence representations, as demonstrated in studies such as , which employs the ""[X] means [MASK]"" prompt to extract sentence representations from the representation of the ""[MASK]"" token in a sentence. Another study by (Zeng et al., 2022) combines prompt-derived sentence representations with contrastive learning to improve the quality of the representations.","[[None, 'b19'], [None, 'b38']]","[[None, 'b19'], [None, 'b38']]",4,"1. Researchers have explored various other methods for obtaining positive samples for unsupervised sentence representations.
2. One approach involves using other spans of text that either overlap with the given span or come from the same document (Giorgi et al., 2021).
3. Another strategy is to use related entities as positives instead of text spans (Nishikawa et al., 2022).
4. This approach is particularly useful for cross-lingual sentence representation learning, as information about the same entity is often available in different languages.
5. Other approaches use large language models that are capable of performing various tasks from observing few examples, even for tasks that they have never encountered before.
6. Generally, larger models with more parameters tend to perform better (Brown et al., 2020).
7. Moreover, these models can be directed to perform tasks by providing them with specific instructions-a technique known as ""prompting"".
8. This prompting paradigm has become a popular approach in natural language processing.
9. Researchers have used prompts to obtain better sentence representations, as demonstrated in studies such as , which employs the ""[X] means [MASK]"" prompt to extract sentence representations from the representation of the ""[MASK]"" token in a sentence.
10. Another study by (Zeng et al., 2022) combines prompt-derived sentence representations with contrastive learning to improve the quality of the representations."
258832362,Beyond Words: A Comprehensive Survey of Sentence Representations,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/8579ad4a8e835cada64c0eae142a00205ce857b5,s12,Model Level,"['p12.0', 'p12.1', 'p12.2']","['Another approach to generating positive examples is by leveraging the distinctive characteristics of the backbone model utilized in contrastive learning. These characteristics might be the architectural choices or using representation from certain components of the model.', 'Dropout is a regularization technique used in deep learning to prevent overfitting of a model. During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times. These different representations can be used as positive examples for sentence representations. Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy. Several other works have also incorporated this technique and improved upon it: promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d).', 'Specific components of language models can be trained to generate semantically similar representations. One example is the use of prefix modules (Li and Liang, 2021), which are small, trainable modules added to a pretrained language model. In a recent study, Wang and Lu (2022) trained prefix modules on NLI data before using them to generate positive examples for unsupervised sentence representations. a']","Another approach to generating positive examples is by leveraging the distinctive characteristics of the backbone model utilized in contrastive learning. These characteristics might be the architectural choices or using representation from certain components of the model.

Dropout is a regularization technique used in deep learning to prevent overfitting of a model. During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times. These different representations can be used as positive examples for sentence representations. Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy. Several other works have also incorporated this technique and improved upon it: promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d).

Specific components of language models can be trained to generate semantically similar representations. One example is the use of prefix modules (Li and Liang, 2021), which are small, trainable modules added to a pretrained language model. In a recent study, Wang and Lu (2022) trained prefix modules on NLI data before using them to generate positive examples for unsupervised sentence representations. a","(p12.0) Another approach to generating positive examples is by leveraging the distinctive characteristics of the backbone model utilized in contrastive learning. These characteristics might be the architectural choices or using representation from certain components of the model.

(p12.1) Dropout is a regularization technique used in deep learning to prevent overfitting of a model. During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times. These different representations can be used as positive examples for sentence representations. Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy. Several other works have also incorporated this technique and improved upon it: promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d).

(p12.2) Specific components of language models can be trained to generate semantically similar representations. One example is the use of prefix modules (Li and Liang, 2021), which are small, trainable modules added to a pretrained language model. In a recent study, Wang and Lu (2022) trained prefix modules on NLI data before using them to generate positive examples for unsupervised sentence representations. a","[[], ['b11', None], ['b13', 'b30']]","[[], ['b11', None], ['b13', 'b30']]",4,"1. Another approach to generating positive examples is by leveraging the distinctive characteristics of the backbone model utilized in contrastive learning.
2. These characteristics might be the architectural choices or using representation from certain components of the model.
3. Dropout is a regularization technique used in deep learning to prevent overfitting of a model.
4. During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times.
5. These different representations can be used as positive examples for sentence representations.
6. Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy.
7. Several other works have also incorporated this technique and improved upon it: promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d).
8. Specific components of language models can be trained to generate semantically similar representations.
9. One example is the use of prefix modules (Li and Liang, 2021), which are small, trainable modules added to a pretrained language model.
10. In a recent study, Wang and Lu (2022) trained prefix modules on NLI data before using them to generate positive examples for unsupervised sentence representations.
11. a"
258832362,Beyond Words: A Comprehensive Survey of Sentence Representations,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/8579ad4a8e835cada64c0eae142a00205ce857b5,s15,Alternative Loss and Objectives,"['p15.0', 'p15.1', 'p15.2', 'p15.3']","['In § 2, we discuss Contrastive loss, which is widely used in machine learning. However, this loss suffers from several limitations: for instance it only considers binary relationships between instances, lacks a mechanism to incorporate ""hard negatives"". To overcome these drawbacks, researchers have explored supplementary losses that can be used in conjunction with the Contrastive loss. Moreover, they have proposed alterations to the loss function that make it more effective for learning, as well as alternative loss functions for contrastive learning of sentence representations. This section provides an overview of these approaches.', 'To improve the learning of sentence representations using the Contrastive loss, researchers have introduced several supplementary losses. These include: (1) the hinge loss proposed by Jiang et al. (2022b), which enhances discrimination between positive and negative pairs; (2) losses suggested by Wu et al. (2022b) for reconstructing the original sentence from its representation to better capture sentence semantics; (3) a loss developed by Chuang et al. (2022) to identify masked words and improve sensitivity to meaningless semantic transformations; and (4) a loss proposed by Chen et al. (2022a) to minimize redundant information from transformations by minimizing information entropy.', 'Besides such supplementary losses, some studies have suggested modifications to the original contrastive loss for better effectiveness. For instance, Wu et al. (2022c) have proposed an additional term that incorporates random noise from a Gaussian distribution as negative instances. In the same vein, Zhang et al. (2022d) have introduced two lossesthe angular loss and margin-based triplet loss-to better address the intricacies of similarity between pairs of examples.', 'However, recent research has moved away from using the contrastive loss and has employed different loss functions to learn sentence representations. For instance, Zhang et al. (2020) maximize the mutual information between a local and a global representation of a sentence using .  identify an alternative sub-manifold within the sentence representation space that considers the geometric structure of sentences to compute semantic similarity.']","In § 2, we discuss Contrastive loss, which is widely used in machine learning. However, this loss suffers from several limitations: for instance it only considers binary relationships between instances, lacks a mechanism to incorporate ""hard negatives"". To overcome these drawbacks, researchers have explored supplementary losses that can be used in conjunction with the Contrastive loss. Moreover, they have proposed alterations to the loss function that make it more effective for learning, as well as alternative loss functions for contrastive learning of sentence representations. This section provides an overview of these approaches.

To improve the learning of sentence representations using the Contrastive loss, researchers have introduced several supplementary losses. These include: (1) the hinge loss proposed by Jiang et al. (2022b), which enhances discrimination between positive and negative pairs; (2) losses suggested by Wu et al. (2022b) for reconstructing the original sentence from its representation to better capture sentence semantics; (3) a loss developed by Chuang et al. (2022) to identify masked words and improve sensitivity to meaningless semantic transformations; and (4) a loss proposed by Chen et al. (2022a) to minimize redundant information from transformations by minimizing information entropy.

Besides such supplementary losses, some studies have suggested modifications to the original contrastive loss for better effectiveness. For instance, Wu et al. (2022c) have proposed an additional term that incorporates random noise from a Gaussian distribution as negative instances. In the same vein, Zhang et al. (2022d) have introduced two lossesthe angular loss and margin-based triplet loss-to better address the intricacies of similarity between pairs of examples.

However, recent research has moved away from using the contrastive loss and has employed different loss functions to learn sentence representations. For instance, Zhang et al. (2020) maximize the mutual information between a local and a global representation of a sentence using .  identify an alternative sub-manifold within the sentence representation space that considers the geometric structure of sentences to compute semantic similarity.","(p15.0) In § 2, we discuss Contrastive loss, which is widely used in machine learning. However, this loss suffers from several limitations: for instance it only considers binary relationships between instances, lacks a mechanism to incorporate ""hard negatives"". To overcome these drawbacks, researchers have explored supplementary losses that can be used in conjunction with the Contrastive loss. Moreover, they have proposed alterations to the loss function that make it more effective for learning, as well as alternative loss functions for contrastive learning of sentence representations. This section provides an overview of these approaches.

(p15.1) To improve the learning of sentence representations using the Contrastive loss, researchers have introduced several supplementary losses. These include: (1) the hinge loss proposed by Jiang et al. (2022b), which enhances discrimination between positive and negative pairs; (2) losses suggested by Wu et al. (2022b) for reconstructing the original sentence from its representation to better capture sentence semantics; (3) a loss developed by Chuang et al. (2022) to identify masked words and improve sensitivity to meaningless semantic transformations; and (4) a loss proposed by Chen et al. (2022a) to minimize redundant information from transformations by minimizing information entropy.

(p15.2) Besides such supplementary losses, some studies have suggested modifications to the original contrastive loss for better effectiveness. For instance, Wu et al. (2022c) have proposed an additional term that incorporates random noise from a Gaussian distribution as negative instances. In the same vein, Zhang et al. (2022d) have introduced two lossesthe angular loss and margin-based triplet loss-to better address the intricacies of similarity between pairs of examples.

(p15.3) However, recent research has moved away from using the contrastive loss and has employed different loss functions to learn sentence representations. For instance, Zhang et al. (2020) maximize the mutual information between a local and a global representation of a sentence using .  identify an alternative sub-manifold within the sentence representation space that considers the geometric structure of sentences to compute semantic similarity.","[[], ['b7', None, 'b35'], ['b44', 'b36'], ['b42']]","[[], ['b7', None, 'b35'], ['b44', 'b36'], ['b42']]",6,"1. In § 2, we discuss Contrastive loss, which is widely used in machine learning.
2. However, this loss suffers from several limitations: for instance it only considers binary relationships between instances, lacks a mechanism to incorporate ""hard negatives"".
3. To overcome these drawbacks, researchers have explored supplementary losses that can be used in conjunction with the Contrastive loss.
4. Moreover, they have proposed alterations to the loss function that make it more effective for learning, as well as alternative loss functions for contrastive learning of sentence representations.
5. This section provides an overview of these approaches.
6. To improve the learning of sentence representations using the Contrastive loss, researchers have introduced several supplementary losses.
7. These include: (1) the hinge loss proposed by Jiang et al. (2022b), which enhances discrimination between positive and negative pairs; (2) losses suggested by Wu et al. (2022b) for reconstructing the original sentence from its representation to better capture sentence semantics;
8. (3) a loss developed by Chuang et al. (2022) to identify masked words and improve sensitivity to meaningless semantic transformations; and (4) a loss proposed by Chen et al. (2022a) to minimize redundant information from transformations by minimizing information entropy.
9. Besides such supplementary losses, some studies have suggested modifications to the original contrastive loss for better effectiveness.
10. For instance, Wu et al. (2022c) have proposed an additional term that incorporates random noise from a Gaussian distribution as negative instances.
11. In the same vein, Zhang et al. (2022d) have introduced two lossesthe angular loss and margin-based triplet loss-to better address the intricacies of similarity between pairs of examples.
12. However, recent research has moved away from using the contrastive loss and has employed different loss functions to learn sentence representations.
13. For instance, Zhang et al. (2020) maximize the mutual information between a local and a global representation of a sentence using .
14. identify an alternative sub-manifold within the sentence representation space that considers the geometric structure of sentences to compute semantic similarity."
258832362,Beyond Words: A Comprehensive Survey of Sentence Representations,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/8579ad4a8e835cada64c0eae142a00205ce857b5,s16,Better Negative Sampling,"['p16.0', 'p16.1']","[""The efficacy of contrastive learning hinges on the quality of negative samples used during training. While most methods prioritize selecting positive samples that bear similarity to the query text, it's equally crucial to include hard negatives that are dissimilar to the query text and pose a challenge for the model to classify. Failure to do so leads to a gradual diminution of the loss gradients, impeding the learning of useful representations (Zhang et al., 2022c). Additionally, using an adequate number of negative samples is also imperative for effective learning (Cao et al., 2022)."", 'Given the importance of incorporating hard negatives, several innovative strategies have emerged. For instance, researchers have found that mixednegatives-a combination of representations of a positive and a randomly chosen negative-serve as an excellent hard negative representation (Zhang et al., 2022c). Similarly, Zhou et al. (2022) have leveraged noise from a uniform Gaussian distribution to foster uniformity in the learned representation space-a metric to assess learned sentence representation. To further refine their approach, they also implemented techniques to identify and penalize false negative instances, where similarity scores exceed a predetermined threshold with the positive instance.']","The efficacy of contrastive learning hinges on the quality of negative samples used during training. While most methods prioritize selecting positive samples that bear similarity to the query text, it's equally crucial to include hard negatives that are dissimilar to the query text and pose a challenge for the model to classify. Failure to do so leads to a gradual diminution of the loss gradients, impeding the learning of useful representations (Zhang et al., 2022c). Additionally, using an adequate number of negative samples is also imperative for effective learning (Cao et al., 2022).

Given the importance of incorporating hard negatives, several innovative strategies have emerged. For instance, researchers have found that mixednegatives-a combination of representations of a positive and a randomly chosen negative-serve as an excellent hard negative representation (Zhang et al., 2022c). Similarly, Zhou et al. (2022) have leveraged noise from a uniform Gaussian distribution to foster uniformity in the learned representation space-a metric to assess learned sentence representation. To further refine their approach, they also implemented techniques to identify and penalize false negative instances, where similarity scores exceed a predetermined threshold with the positive instance.","(p16.0) The efficacy of contrastive learning hinges on the quality of negative samples used during training. While most methods prioritize selecting positive samples that bear similarity to the query text, it's equally crucial to include hard negatives that are dissimilar to the query text and pose a challenge for the model to classify. Failure to do so leads to a gradual diminution of the loss gradients, impeding the learning of useful representations (Zhang et al., 2022c). Additionally, using an adequate number of negative samples is also imperative for effective learning (Cao et al., 2022).

(p16.1) Given the importance of incorporating hard negatives, several innovative strategies have emerged. For instance, researchers have found that mixednegatives-a combination of representations of a positive and a randomly chosen negative-serve as an excellent hard negative representation (Zhang et al., 2022c). Similarly, Zhou et al. (2022) have leveraged noise from a uniform Gaussian distribution to foster uniformity in the learned representation space-a metric to assess learned sentence representation. To further refine their approach, they also implemented techniques to identify and penalize false negative instances, where similarity scores exceed a predetermined threshold with the positive instance.","[['b38', 'b43'], ['b45', 'b43']]","[['b38', 'b43'], ['b45', 'b43']]",4,"1. The efficacy of contrastive learning hinges on the quality of negative samples used during training.
2. While most methods prioritize selecting positive samples that bear similarity to the query text, it's equally crucial to include hard negatives that are dissimilar to the query text and pose a challenge for the model to classify.
3. Failure to do so leads to a gradual diminution of the loss gradients, impeding the learning of useful representations (Zhang et al., 2022c).
4. Additionally, using an adequate number of negative samples is also imperative for effective learning (Cao et al., 2022).
5. Given the importance of incorporating hard negatives, several innovative strategies have emerged.
6. For instance, researchers have found that mixednegatives-a combination of representations of a positive and a randomly chosen negative-serve as an excellent hard negative representation (Zhang et al., 2022c).
7. Similarly, Zhou et al. (2022) have leveraged noise from a uniform Gaussian distribution to foster uniformity in the learned representation space-a metric to assess learned sentence representation.
8. To further refine their approach, they also implemented techniques to identify and penalize false negative instances, where similarity scores exceed a predetermined threshold with the positive instance."
258832362,Beyond Words: A Comprehensive Survey of Sentence Representations,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/8579ad4a8e835cada64c0eae142a00205ce857b5,s19,Computer Vision Inspired,"['p19.0', 'p19.1', 'p19.2']","['The effectiveness of computer vision techniques in NLP is often debated. Some researchers are interested in exploring their transferability to NLP. For example, momentum encoder , a queue-based approach, has been proposed to improve the stability of training during contrastive learning. This approach involves using a queue of representations from previous batches as negatives for the current batch, effectively decoupling the batch size from the learning process. Several researchers have incorporated the momentum encoder technique for learning sentence representations, reporting improved performance (Cao et al., 2022;Wu et al., 2022a,d;Tan et al., 2022).', 'Bootstrap Your Own Latent (BYOL) (Grill et al., 2020 Figure 2) and (e) AVERAGE shows the average STS score.', 'network to predict a set of ""target"" representations of an input data point, given another ""online"" representation of the same data point. The network is optimized using a contrastive loss function that incentivizes the online representation to be similar to the target representation. One advantage of BYOL is that BYOL completely removes the requirement of negative samples and instead uses the augmented versions of the same data point as positive samples. This approach has been successfully applied to natural language processing by Zhang et al. (2021) for learning sentence representations.']","The effectiveness of computer vision techniques in NLP is often debated. Some researchers are interested in exploring their transferability to NLP. For example, momentum encoder , a queue-based approach, has been proposed to improve the stability of training during contrastive learning. This approach involves using a queue of representations from previous batches as negatives for the current batch, effectively decoupling the batch size from the learning process. Several researchers have incorporated the momentum encoder technique for learning sentence representations, reporting improved performance (Cao et al., 2022;Wu et al., 2022a,d;Tan et al., 2022).

Bootstrap Your Own Latent (BYOL) (Grill et al., 2020 Figure 2) and (e) AVERAGE shows the average STS score.

network to predict a set of ""target"" representations of an input data point, given another ""online"" representation of the same data point. The network is optimized using a contrastive loss function that incentivizes the online representation to be similar to the target representation. One advantage of BYOL is that BYOL completely removes the requirement of negative samples and instead uses the augmented versions of the same data point as positive samples. This approach has been successfully applied to natural language processing by Zhang et al. (2021) for learning sentence representations.","(p19.0) The effectiveness of computer vision techniques in NLP is often debated. Some researchers are interested in exploring their transferability to NLP. For example, momentum encoder , a queue-based approach, has been proposed to improve the stability of training during contrastive learning. This approach involves using a queue of representations from previous batches as negatives for the current batch, effectively decoupling the batch size from the learning process. Several researchers have incorporated the momentum encoder technique for learning sentence representations, reporting improved performance (Cao et al., 2022;Wu et al., 2022a,d;Tan et al., 2022).

(p19.1) Bootstrap Your Own Latent (BYOL) (Grill et al., 2020 Figure 2) and (e) AVERAGE shows the average STS score.

(p19.2) network to predict a set of ""target"" representations of an input data point, given another ""online"" representation of the same data point. The network is optimized using a contrastive loss function that incentivizes the online representation to be similar to the target representation. One advantage of BYOL is that BYOL completely removes the requirement of negative samples and instead uses the augmented versions of the same data point as positive samples. This approach has been successfully applied to natural language processing by Zhang et al. (2021) for learning sentence representations.","[[None, 'b38', 'b25'], ['b2'], ['b41']]","[[None, 'b38', 'b25'], ['b2'], ['b41']]",5,"1. The effectiveness of computer vision techniques in NLP is often debated.
2. Some researchers are interested in exploring their transferability to NLP.
3. For example, momentum encoder , a queue-based approach, has been proposed to improve the stability of training during contrastive learning.
4. This approach involves using a queue of representations from previous batches as negatives for the current batch, effectively decoupling the batch size from the learning process.
5. Several researchers have incorporated the momentum encoder technique for learning sentence representations, reporting improved performance (Cao et al., 2022;Wu et al., 2022a,d;Tan et al., 2022).
6. Bootstrap Your Own Latent (BYOL) (Grill et al., 2020 Figure 2) and (e) AVERAGE shows the average STS score.
7. network to predict a set of ""target"" representations of an input data point, given another ""online"" representation of the same data point.
8. The network is optimized using a contrastive loss function that incentivizes the online representation to be similar to the target representation.
9. One advantage of BYOL is that BYOL completely removes the requirement of negative samples and instead uses the augmented versions of the same data point as positive samples.
10. This approach has been successfully applied to natural language processing by Zhang et al. (2021) for learning sentence representations."
258832362,Beyond Words: A Comprehensive Survey of Sentence Representations,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/8579ad4a8e835cada64c0eae142a00205ce857b5,s20,Challenges for Sentence Representations,"['p20.0', 'p20.1']","['Practical Applications and the rise of Tools: Sentence representations are commonly employed for sentence retrieval in practical applications, as evidenced by the increasing number of benchmarks (Thakur et al., 2021b) However, their utility extends beyond retrieval, as demonstrated by recent work (Schuster et al., 2022) that leverages sentence representations to make practical enhancements for processing long documents. Specifically, the authors propose novel methods for identifying documents that share a similar stance on a topic, as well as for isolating documents that diverge from the consensus.', 'With the increasing use of sentence representations in practical applications such as retrieval, there is a growing need for efficient storage and indexing solutions that enable fast retrieval. These solutions are commonly referred to as vector databases and include popular options such as Pinecone, Milvus, and Faiss, among others 3 . Furthermore, these vector databases can be integrated with other frameworks that facilitate the development of applications using large language models. Langchain and ChatGPT Retrieval Plugin are examples of such framewor Adapting to different domains: Research has shown that sentence representations learned in one domain may not accurately capture the semantic meaning of sentences in another domain (Jiang et al., 2022b;Thakur et al., 2021a). For instance, using sentence representations from one domain for text retrieval in another domain often results in poor performance. Although some solutions have been proposed in the literature, such as generating queries using a pretrained T5 model on a paragraph from the target domain or using a pretrained crossencoder to label the query and paragraph, or using a denoising objective (Wang et al., 2021). Training models that work well across domains remains a challenging task Cross-lingual sentence representations : Creating sentence representations that can be used across languages, especially those with limited annotated data, poses a significant challenge. New solutions for cross-lingual retrieval are being developed and deployed for real-world use cases. 4 Many scholarly works (Nishikawa et al., 2022) have addressed cross-lingual sentence representation learning in recent times. Given that there are thousands of languages spoken worldwide, developing models that can operate across languages is increasingly important.']","Practical Applications and the rise of Tools: Sentence representations are commonly employed for sentence retrieval in practical applications, as evidenced by the increasing number of benchmarks (Thakur et al., 2021b) However, their utility extends beyond retrieval, as demonstrated by recent work (Schuster et al., 2022) that leverages sentence representations to make practical enhancements for processing long documents. Specifically, the authors propose novel methods for identifying documents that share a similar stance on a topic, as well as for isolating documents that diverge from the consensus.

With the increasing use of sentence representations in practical applications such as retrieval, there is a growing need for efficient storage and indexing solutions that enable fast retrieval. These solutions are commonly referred to as vector databases and include popular options such as Pinecone, Milvus, and Faiss, among others 3 . Furthermore, these vector databases can be integrated with other frameworks that facilitate the development of applications using large language models. Langchain and ChatGPT Retrieval Plugin are examples of such framewor Adapting to different domains: Research has shown that sentence representations learned in one domain may not accurately capture the semantic meaning of sentences in another domain (Jiang et al., 2022b;Thakur et al., 2021a). For instance, using sentence representations from one domain for text retrieval in another domain often results in poor performance. Although some solutions have been proposed in the literature, such as generating queries using a pretrained T5 model on a paragraph from the target domain or using a pretrained crossencoder to label the query and paragraph, or using a denoising objective (Wang et al., 2021). Training models that work well across domains remains a challenging task Cross-lingual sentence representations : Creating sentence representations that can be used across languages, especially those with limited annotated data, poses a significant challenge. New solutions for cross-lingual retrieval are being developed and deployed for real-world use cases. 4 Many scholarly works (Nishikawa et al., 2022) have addressed cross-lingual sentence representation learning in recent times. Given that there are thousands of languages spoken worldwide, developing models that can operate across languages is increasingly important.","(p20.0) Practical Applications and the rise of Tools: Sentence representations are commonly employed for sentence retrieval in practical applications, as evidenced by the increasing number of benchmarks (Thakur et al., 2021b) However, their utility extends beyond retrieval, as demonstrated by recent work (Schuster et al., 2022) that leverages sentence representations to make practical enhancements for processing long documents. Specifically, the authors propose novel methods for identifying documents that share a similar stance on a topic, as well as for isolating documents that diverge from the consensus.

(p20.1) With the increasing use of sentence representations in practical applications such as retrieval, there is a growing need for efficient storage and indexing solutions that enable fast retrieval. These solutions are commonly referred to as vector databases and include popular options such as Pinecone, Milvus, and Faiss, among others 3 . Furthermore, these vector databases can be integrated with other frameworks that facilitate the development of applications using large language models. Langchain and ChatGPT Retrieval Plugin are examples of such framewor Adapting to different domains: Research has shown that sentence representations learned in one domain may not accurately capture the semantic meaning of sentences in another domain (Jiang et al., 2022b;Thakur et al., 2021a). For instance, using sentence representations from one domain for text retrieval in another domain often results in poor performance. Although some solutions have been proposed in the literature, such as generating queries using a pretrained T5 model on a paragraph from the target domain or using a pretrained crossencoder to label the query and paragraph, or using a denoising objective (Wang et al., 2021). Training models that work well across domains remains a challenging task Cross-lingual sentence representations : Creating sentence representations that can be used across languages, especially those with limited annotated data, poses a significant challenge. New solutions for cross-lingual retrieval are being developed and deployed for real-world use cases. 4 Many scholarly works (Nishikawa et al., 2022) have addressed cross-lingual sentence representation learning in recent times. Given that there are thousands of languages spoken worldwide, developing models that can operate across languages is increasingly important.","[[None], ['b29', 'b7', 'b26', 'b19']]","[[None], ['b29', 'b7', 'b26', 'b19']]",5,"1. Practical Applications and the rise of Tools: Sentence representations are commonly employed for sentence retrieval in practical applications, as evidenced by the increasing number of benchmarks (Thakur et al., 2021b)
2. However, their utility extends beyond retrieval, as demonstrated by recent work (Schuster et al., 2022) that leverages sentence representations to make practical enhancements for processing long documents.
3. Specifically, the authors propose novel methods for identifying documents that share a similar stance on a topic, as well as for isolating documents that diverge from the consensus.
4. With the increasing use of sentence representations in practical applications such as retrieval, there is a growing need for efficient storage and indexing solutions that enable fast retrieval.
5. These solutions are commonly referred to as vector databases and include popular options such as Pinecone, Milvus, and Faiss, among others 3 .
6. Furthermore, these vector databases can be integrated with other frameworks that facilitate the development of applications using large language models.
7. Langchain and ChatGPT Retrieval Plugin are examples of such framewor Adapting to different domains: Research has shown that sentence representations learned in one domain may not accurately capture the semantic meaning of sentences in another domain (Jiang et al., 2022b;Thakur et al., 2021a).
8. For instance, using sentence representations from one domain for text retrieval in another domain often results in poor performance.
9. Although some solutions have been proposed in the literature, such as generating queries using a pretrained T5 model on a paragraph from the target domain or using a pretrained crossencoder to label the query and paragraph, or using a denoising objective (Wang et al., 2021).
10. Training models that work well across domains remains a challenging task Cross-lingual sentence representations : Creating sentence representations that can be used across languages, especially those with limited annotated data, poses a significant challenge.
11. New solutions for cross-lingual retrieval are being developed and deployed for real-world use cases.
12. 4 Many scholarly works (Nishikawa et al., 2022) have addressed cross-lingual sentence representation learning in recent times.
13. Given that there are thousands of languages spoken worldwide, developing models that can operate across languages is increasingly important."
258832362,Beyond Words: A Comprehensive Survey of Sentence Representations,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/8579ad4a8e835cada64c0eae142a00205ce857b5,s7,Natural Language Inference,"['p7.0', 'p7.1', 'p7.2']","['Natural Language Inference (NLI) is the process of determining the logical relationship between a premise (an assumed true sentence) and a hypothesis (a possibly true sentence). The objective of NLI is to determine whether the hypothesis can be logically inferred from the premise (entailment), contradicts the premise (contradiction), or is neutral with respect to it (Dagan et al., 2013). NLI serves as a proxy for evaluating natural language understanding. According to Conneau et al. (2017), learning sentence representations using NLI data can be effectively transferred to other NLP tasks, demonstrating the generality of this approach.', ""In § 2.3, we discussed Siamese-BERT networks as presented in Reimers and Gurevych (2019). There are two noteworthy components to this model. First, processing inputs individually without promoting interaction between words; second, using an encoder like BERT that is not generative as its backbone model. The first component is computationally efficient but has been found to result in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019). This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings. In order to solve this, Cheng (2021) incorporate word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks. Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015): using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT."", 'Meanwhile, generative models have been pretrained on huge amounts of text data, and can perform a myriad of tasks. Ni et al. (2022) examined the use of generative models as backbone for extracting sentence embeddings. They ex-plore three methods using pre-trained T5 encoderdecoder models: using the representation of the first token of the encoder, using the representation of the first generated token of the decoder, or using the mean of the representations from the encoder. This is one of the first studies that shows the utility of generative models for obtaining sentence representations.']","Natural Language Inference (NLI) is the process of determining the logical relationship between a premise (an assumed true sentence) and a hypothesis (a possibly true sentence). The objective of NLI is to determine whether the hypothesis can be logically inferred from the premise (entailment), contradicts the premise (contradiction), or is neutral with respect to it (Dagan et al., 2013). NLI serves as a proxy for evaluating natural language understanding. According to Conneau et al. (2017), learning sentence representations using NLI data can be effectively transferred to other NLP tasks, demonstrating the generality of this approach.

In § 2.3, we discussed Siamese-BERT networks as presented in Reimers and Gurevych (2019). There are two noteworthy components to this model. First, processing inputs individually without promoting interaction between words; second, using an encoder like BERT that is not generative as its backbone model. The first component is computationally efficient but has been found to result in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019). This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings. In order to solve this, Cheng (2021) incorporate word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks. Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015): using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT.

Meanwhile, generative models have been pretrained on huge amounts of text data, and can perform a myriad of tasks. Ni et al. (2022) examined the use of generative models as backbone for extracting sentence embeddings. They ex-plore three methods using pre-trained T5 encoderdecoder models: using the representation of the first token of the encoder, using the representation of the first generated token of the decoder, or using the mean of the representations from the encoder. This is one of the first studies that shows the utility of generative models for obtaining sentence representations.","(p7.0) Natural Language Inference (NLI) is the process of determining the logical relationship between a premise (an assumed true sentence) and a hypothesis (a possibly true sentence). The objective of NLI is to determine whether the hypothesis can be logically inferred from the premise (entailment), contradicts the premise (contradiction), or is neutral with respect to it (Dagan et al., 2013). NLI serves as a proxy for evaluating natural language understanding. According to Conneau et al. (2017), learning sentence representations using NLI data can be effectively transferred to other NLP tasks, demonstrating the generality of this approach.

(p7.1) In § 2.3, we discussed Siamese-BERT networks as presented in Reimers and Gurevych (2019). There are two noteworthy components to this model. First, processing inputs individually without promoting interaction between words; second, using an encoder like BERT that is not generative as its backbone model. The first component is computationally efficient but has been found to result in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019). This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings. In order to solve this, Cheng (2021) incorporate word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks. Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015): using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT.

(p7.2) Meanwhile, generative models have been pretrained on huge amounts of text data, and can perform a myriad of tasks. Ni et al. (2022) examined the use of generative models as backbone for extracting sentence embeddings. They ex-plore three methods using pre-trained T5 encoderdecoder models: using the representation of the first token of the encoder, using the representation of the first generated token of the decoder, or using the mean of the representations from the encoder. This is one of the first studies that shows the utility of generative models for obtaining sentence representations.","[[None], ['b22', 'b4'], ['b17']]","[[None], ['b22', 'b4'], ['b17']]",4,"1. Natural Language Inference (NLI) is the process of determining the logical relationship between a premise (an assumed true sentence) and a hypothesis (a possibly true sentence).
2. The objective of NLI is to determine whether the hypothesis can be logically inferred from the premise (entailment), contradicts the premise (contradiction), or is neutral with respect to it (Dagan et al., 2013).
3. NLI serves as a proxy for evaluating natural language understanding.
4. According to Conneau et al. (2017), learning sentence representations using NLI data can be effectively transferred to other NLP tasks, demonstrating the generality of this approach.
5. In § 2.3, we discussed Siamese-BERT networks as presented in Reimers and Gurevych (2019).
6. There are two noteworthy components to this model.
7. First, processing inputs individually without promoting interaction between words; second, using an encoder like BERT that is not generative as its backbone model.
8. The first component is computationally efficient but has been found to result in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019).
9. This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings.
10. In order to solve this, Cheng (2021) incorporate word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks.
11. Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015): using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT.
12. Meanwhile, generative models have been pretrained on huge amounts of text data, and can perform a myriad of tasks.
13. Ni et al. (2022) examined the use of generative models as backbone for extracting sentence embeddings.
14. They ex-plore three methods using pre-trained T5 encoderdecoder models: using the representation of the first token of the encoder, using the representation of the first generated token of the decoder, or using the mean of the representations from the encoder.
15. This is one of the first studies that shows the utility of generative models for obtaining sentence representations."
259108815,Mapping Brains with Language Models: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/a92c82231c0ea7027ab20b25fe5f82565047aad7,s1,Datasets,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5', 'p1.6', 'p1.7', 'p1.8', 'p1.9', 'p1.10', 'p1.11', 'p1.12', 'p1.13', 'p1.14', 'p1.15', 'p1.16']","['To infer a mapping between language models and brains, researchers rely on datasets in which brain activity is recorded in response to linguistic stimuli. In some studies, the stimuli are single words (Mitchell et al., 2008;Anderson et al., 2017) or sentences displayed on a screen (Pereira et al., 2018). In others, participants read longer stories (Wehbe et al., 2014a;Bhattasali et al., 2020;Nastase et al., 2021) or listened to speech or podcasts (Huth et al., 2016;Antonello et al., 2021). Table 1 lists publicly available datasets that have been used in the context of mapping language models to and from recordings of brain response. Differences between the datasets -the number of participants, the equipment, the experimental setup, pre-processing steps, and probabilistic corrections -should lead us to expect some variation in what researchers have concluded (Hollenstein et al., 2020).', '3 How to predict brain activity?', 'In this section, we survey work in which neural responses are predicted from linguistic representations. Such work typically aims to shed light on how language functions in the brain. One of the earliest studies exploring the mapping between brain and language representations is by Mitchell et al. (2008), who trained a linear regression model on a set of word representations extracted from 60 nouns using 115 semantic features based on cooccurrence statistics, to predict the corresponding fMRI representations of the same nouns. They use pair-wise matching accuracy evaluation, extracting two words w and w ′ for evaluation, and showed that the predicted fMRI for a word w was closer to the real fMRI image for w than to the real fMRI image for w ′ , at above-chance levels. Mitchell et al. (2008) also report percentile rank results, ranking predicted fMRI images by similarity with the real image of w. We discuss how the metrics relate in §6.', 'The dataset of Mitchell et al. (2008)is also used by Murphy et al. (2012), who extract linguistic features from part-of-speech taggers, stemmers, and dependency parsers, showing that dependency parsers are the most successful in predicting brain activity. They also use leave-2-out pair-matching as their performance metric.', ""Later on, Wehbe et al. (2014a) moved on to predicting brain activation patterns for entire sentences rather than for isolated words. They recorded fMRI neural response measurements while participants read a chapter from Harry Potter and the Sorcerer's Stone, then extracted a set of 195 features for each word (ranging from semantic, syntactic properties to visual and discourse-level features) to train a comprehensive generative model that would then predict the time series of the fMRI activity observed when the participants read that passage. Leave-2-out pair-matching accuracy is used for evaluation. Huth et al. (2016), in contrast, use fMRI recordings of participants listening to spoken narrative stories, representing each word in the corpus as a 985-dimensional vector encoding semantic information driven by co-occurrence statistics. They train per-voxel linear regression models and evaluate their predicted per-word fMRI images by their per-voxel Pearson correlation with the real fMRI images, showing that 3-4 dimensions explained a significant amount of variance in the FMRI data. Wehbe et al. (2014b) are among the first to use neural language models, using recurrent models to compute contextualized embeddings, hidden state vectors of previous words, and word probabilities.  of participants reading Harry Potter, obtained in a follow-up study to Wehbe et al. (2014a). From the three sets of representations, they then train linear regression models to predict the MEG vectors corresponding to each word, and the regression models are then evaluated by computing pair-matching accuracy."", 'Similarly, Søgaard (2016) evaluates static word embeddings on the data from Wehbe et al. (2014a), learning linear transformation from word embeddings into an fMRI vector space. The predictions are evaluated through mean squared error (MSE).', 'Jain and Huth (2018) evaluate recurrent language models against the fMRI dataset from Huth et al. (2016). Their findings show that contextual language model representations align significantly better (to brain response) compared to static word embedding models. Their evaluation metric is the total sum of explained variance 1 Following this, Schwartz et al. (2019) use attention-based transformer language models for brain mapping. They finetune BERT (Devlin et al., 2019)to predict neural response measurements from the Harry Potter dataset, showing that the fine-tuned models have representations that encode more brain-activity-relevant language information than the non-finetuned models. They rely on pair-matching accuracy as their performance metric.', 'As in Søgaard (2016), Zhang et al. (2020) map static word embeddings into the vector space of the neural response measurements (fMRI). They introduce a new dataset of such measurements from subjects listening to natural stories. They rely on explained variance as their performance metric.', 'Toneva and Wehbe (2019) evaluate word and sequence embeddings from 4 recurrent and attention-based transformer language models, using the Harry Potter fMRI dataset. They evaluate models across layers, context lengths, and attention types, using pairwise matching accuracy as their performance metric. In a later study, Toneva et al. (2022a) induce compositional semantic representations of ""supra-word meaning"" which they then use to predict neural responses across regions of interest, evaluating their models using Pearson correlation.', 'Also using the Harry Potter data,  evaluate five models, one static and four contextualized, relying on a variant of representational similarity analysis (Kriegeskorte et al., 2008). The results suggest that models provide representations of local contexts that are well-aligned to neural measurements. However, as information from further away context is integrated by the models, representations become less aligned to neural measurements.', 'In a large-scale study, Schrimpf et al. (2021) examine the relationships between 43 diverse stateof-the-art neural network models (including embedding models, recurrent models, and transformers) across three datasets (two fMRI, one electrocardiography). They rely on a metric they term Brain Score which involves normalising the Pearson correlation by a noise ceiling. Their results show that transformer-based models perform better than recurrent or static models, and larger models perform better than smaller ones.', 'Similarly, in , the Schoffelen et al. (2019) fMRI and MEG datasets are used to compare a variety of transformer architectures. They study how architectural details, training settings, and the linguistic performance of these models independently account for the generation of brain correspondent representations. The results suggest that the better language models are at predicting words from context, the better their activations linearly map onto those of the brain. Antonello et al. (2021) evaluate three static and five attention-based transformer models, in combination with four fine-tuning tasks and two machine translation models. They train linear regression models to evaluate their word-level representations against a new fMRI dataset from participants listening to podcast stories. They find a low-dimensional structure in language representations that can predict brain responses. In a similar setting, Antonello and Huth (2022) examine why some features fit the brain data better arguing that the reason is that they capture various linguistic phenomena.', 'Reddy and Wehbe (2021) evaluate syntactic features in conjunction with BERT representations, finding that syntax explains additional variance in brain activity in various parts of the language system, even while controlling for complexity metrics that capture processing load.', ""In a series of studies Caucheteux et al. (2021Caucheteux et al. ( , 2022b investigate GPT2's activations in predicting brain signals using the Nastase et al. (2021) dataset. Their evaluation metric is Brain Score (Schrimpf et al., 2018). To determine which factors affect the brain encoding Pasquiou et al. (2022) examine the impact of test loss, training corpus, model architecture, and fine-tuning in various models using the Li et al. (2022) dataset. They evaluate model performance using Pearson Correlation."", 'Oota et al. (2022a) study the impact of context size in language models on how they align with neural response measurements. They use the Nastase et al. (2021) dataset and evaluate recurrent and attention-based transformer architectures. In a later study, Oota et al. (2022b) use the Pereira et al. (2018) dataset and evaluate BERTbase models (fine-tuned for various NLP tasks). They showed that neural response predictions from ridge regression with BERT-base models fine-tuned for coreference resolution, NER, and shallow syntactic parsing explained more variance for Pereira et al. (2018) response measurements. On the other hand, tasks such as paraphrase generation, summarization, and natural language inference led to better encoding performance for the Nastase et al. (2021) data (audio). Using the same dataset, in Oota et al. (2022c) it is shown that the presence of surface, syntactic, and semantic linguistic information is crucial for the alignment across all layers of the language model. They use pairwise matching accuracy and/or Pearson correlation as their performance metrics in these studies.', 'Aw and Toneva (2023) extract feature representations from four attention-based transformer models. They evaluate the impact of fine-tuning on the BookSum dataset (Kryscinski et al., 2021). All models are used to predict brain activity on the Harry Potter data. Pairwise matching accuracy and Pearson correlation are their performance metrics. Merlin and Toneva (2022) focus more narrowly on variants of GPT-2, showing that improvements in alignment with brain recordings are probably not because of the next-word prediction task or wordlevel semantics, but due to multi-word semantics. Their reported metric is Pearson correlation.', 'Intermediate summary The above studies differ in many respects. Several metrics are used: pairwise-matching accuracy, 2 Pearson correlation (or Brain Score), mean squared error, and representational similarity analysis. Even studies that report the same performance metrics are not directly comparable because they often report on results on different datasets and use slightly different protocols, e.g., Murphy et al. (2012) and Wehbe et al. (2014b). Beinborn et al. (2023) compare various encoding experiments and receive very diverse results for different evaluation metrics. The diversity of metrics and data renders a direct comparison difficult. To remedy this, we consider how the metrics compare in §6.']","To infer a mapping between language models and brains, researchers rely on datasets in which brain activity is recorded in response to linguistic stimuli. In some studies, the stimuli are single words (Mitchell et al., 2008;Anderson et al., 2017) or sentences displayed on a screen (Pereira et al., 2018). In others, participants read longer stories (Wehbe et al., 2014a;Bhattasali et al., 2020;Nastase et al., 2021) or listened to speech or podcasts (Huth et al., 2016;Antonello et al., 2021). Table 1 lists publicly available datasets that have been used in the context of mapping language models to and from recordings of brain response. Differences between the datasets -the number of participants, the equipment, the experimental setup, pre-processing steps, and probabilistic corrections -should lead us to expect some variation in what researchers have concluded (Hollenstein et al., 2020).

3 How to predict brain activity?

In this section, we survey work in which neural responses are predicted from linguistic representations. Such work typically aims to shed light on how language functions in the brain. One of the earliest studies exploring the mapping between brain and language representations is by Mitchell et al. (2008), who trained a linear regression model on a set of word representations extracted from 60 nouns using 115 semantic features based on cooccurrence statistics, to predict the corresponding fMRI representations of the same nouns. They use pair-wise matching accuracy evaluation, extracting two words w and w ′ for evaluation, and showed that the predicted fMRI for a word w was closer to the real fMRI image for w than to the real fMRI image for w ′ , at above-chance levels. Mitchell et al. (2008) also report percentile rank results, ranking predicted fMRI images by similarity with the real image of w. We discuss how the metrics relate in §6.

The dataset of Mitchell et al. (2008)is also used by Murphy et al. (2012), who extract linguistic features from part-of-speech taggers, stemmers, and dependency parsers, showing that dependency parsers are the most successful in predicting brain activity. They also use leave-2-out pair-matching as their performance metric.

Later on, Wehbe et al. (2014a) moved on to predicting brain activation patterns for entire sentences rather than for isolated words. They recorded fMRI neural response measurements while participants read a chapter from Harry Potter and the Sorcerer's Stone, then extracted a set of 195 features for each word (ranging from semantic, syntactic properties to visual and discourse-level features) to train a comprehensive generative model that would then predict the time series of the fMRI activity observed when the participants read that passage. Leave-2-out pair-matching accuracy is used for evaluation. Huth et al. (2016), in contrast, use fMRI recordings of participants listening to spoken narrative stories, representing each word in the corpus as a 985-dimensional vector encoding semantic information driven by co-occurrence statistics. They train per-voxel linear regression models and evaluate their predicted per-word fMRI images by their per-voxel Pearson correlation with the real fMRI images, showing that 3-4 dimensions explained a significant amount of variance in the FMRI data. Wehbe et al. (2014b) are among the first to use neural language models, using recurrent models to compute contextualized embeddings, hidden state vectors of previous words, and word probabilities.  of participants reading Harry Potter, obtained in a follow-up study to Wehbe et al. (2014a). From the three sets of representations, they then train linear regression models to predict the MEG vectors corresponding to each word, and the regression models are then evaluated by computing pair-matching accuracy.

Similarly, Søgaard (2016) evaluates static word embeddings on the data from Wehbe et al. (2014a), learning linear transformation from word embeddings into an fMRI vector space. The predictions are evaluated through mean squared error (MSE).

Jain and Huth (2018) evaluate recurrent language models against the fMRI dataset from Huth et al. (2016). Their findings show that contextual language model representations align significantly better (to brain response) compared to static word embedding models. Their evaluation metric is the total sum of explained variance 1 Following this, Schwartz et al. (2019) use attention-based transformer language models for brain mapping. They finetune BERT (Devlin et al., 2019)to predict neural response measurements from the Harry Potter dataset, showing that the fine-tuned models have representations that encode more brain-activity-relevant language information than the non-finetuned models. They rely on pair-matching accuracy as their performance metric.

As in Søgaard (2016), Zhang et al. (2020) map static word embeddings into the vector space of the neural response measurements (fMRI). They introduce a new dataset of such measurements from subjects listening to natural stories. They rely on explained variance as their performance metric.

Toneva and Wehbe (2019) evaluate word and sequence embeddings from 4 recurrent and attention-based transformer language models, using the Harry Potter fMRI dataset. They evaluate models across layers, context lengths, and attention types, using pairwise matching accuracy as their performance metric. In a later study, Toneva et al. (2022a) induce compositional semantic representations of ""supra-word meaning"" which they then use to predict neural responses across regions of interest, evaluating their models using Pearson correlation.

Also using the Harry Potter data,  evaluate five models, one static and four contextualized, relying on a variant of representational similarity analysis (Kriegeskorte et al., 2008). The results suggest that models provide representations of local contexts that are well-aligned to neural measurements. However, as information from further away context is integrated by the models, representations become less aligned to neural measurements.

In a large-scale study, Schrimpf et al. (2021) examine the relationships between 43 diverse stateof-the-art neural network models (including embedding models, recurrent models, and transformers) across three datasets (two fMRI, one electrocardiography). They rely on a metric they term Brain Score which involves normalising the Pearson correlation by a noise ceiling. Their results show that transformer-based models perform better than recurrent or static models, and larger models perform better than smaller ones.

Similarly, in , the Schoffelen et al. (2019) fMRI and MEG datasets are used to compare a variety of transformer architectures. They study how architectural details, training settings, and the linguistic performance of these models independently account for the generation of brain correspondent representations. The results suggest that the better language models are at predicting words from context, the better their activations linearly map onto those of the brain. Antonello et al. (2021) evaluate three static and five attention-based transformer models, in combination with four fine-tuning tasks and two machine translation models. They train linear regression models to evaluate their word-level representations against a new fMRI dataset from participants listening to podcast stories. They find a low-dimensional structure in language representations that can predict brain responses. In a similar setting, Antonello and Huth (2022) examine why some features fit the brain data better arguing that the reason is that they capture various linguistic phenomena.

Reddy and Wehbe (2021) evaluate syntactic features in conjunction with BERT representations, finding that syntax explains additional variance in brain activity in various parts of the language system, even while controlling for complexity metrics that capture processing load.

In a series of studies Caucheteux et al. (2021Caucheteux et al. ( , 2022b investigate GPT2's activations in predicting brain signals using the Nastase et al. (2021) dataset. Their evaluation metric is Brain Score (Schrimpf et al., 2018). To determine which factors affect the brain encoding Pasquiou et al. (2022) examine the impact of test loss, training corpus, model architecture, and fine-tuning in various models using the Li et al. (2022) dataset. They evaluate model performance using Pearson Correlation.

Oota et al. (2022a) study the impact of context size in language models on how they align with neural response measurements. They use the Nastase et al. (2021) dataset and evaluate recurrent and attention-based transformer architectures. In a later study, Oota et al. (2022b) use the Pereira et al. (2018) dataset and evaluate BERTbase models (fine-tuned for various NLP tasks). They showed that neural response predictions from ridge regression with BERT-base models fine-tuned for coreference resolution, NER, and shallow syntactic parsing explained more variance for Pereira et al. (2018) response measurements. On the other hand, tasks such as paraphrase generation, summarization, and natural language inference led to better encoding performance for the Nastase et al. (2021) data (audio). Using the same dataset, in Oota et al. (2022c) it is shown that the presence of surface, syntactic, and semantic linguistic information is crucial for the alignment across all layers of the language model. They use pairwise matching accuracy and/or Pearson correlation as their performance metrics in these studies.

Aw and Toneva (2023) extract feature representations from four attention-based transformer models. They evaluate the impact of fine-tuning on the BookSum dataset (Kryscinski et al., 2021). All models are used to predict brain activity on the Harry Potter data. Pairwise matching accuracy and Pearson correlation are their performance metrics. Merlin and Toneva (2022) focus more narrowly on variants of GPT-2, showing that improvements in alignment with brain recordings are probably not because of the next-word prediction task or wordlevel semantics, but due to multi-word semantics. Their reported metric is Pearson correlation.

Intermediate summary The above studies differ in many respects. Several metrics are used: pairwise-matching accuracy, 2 Pearson correlation (or Brain Score), mean squared error, and representational similarity analysis. Even studies that report the same performance metrics are not directly comparable because they often report on results on different datasets and use slightly different protocols, e.g., Murphy et al. (2012) and Wehbe et al. (2014b). Beinborn et al. (2023) compare various encoding experiments and receive very diverse results for different evaluation metrics. The diversity of metrics and data renders a direct comparison difficult. To remedy this, we consider how the metrics compare in §6.","(p1.0) To infer a mapping between language models and brains, researchers rely on datasets in which brain activity is recorded in response to linguistic stimuli. In some studies, the stimuli are single words (Mitchell et al., 2008;Anderson et al., 2017) or sentences displayed on a screen (Pereira et al., 2018). In others, participants read longer stories (Wehbe et al., 2014a;Bhattasali et al., 2020;Nastase et al., 2021) or listened to speech or podcasts (Huth et al., 2016;Antonello et al., 2021). Table 1 lists publicly available datasets that have been used in the context of mapping language models to and from recordings of brain response. Differences between the datasets -the number of participants, the equipment, the experimental setup, pre-processing steps, and probabilistic corrections -should lead us to expect some variation in what researchers have concluded (Hollenstein et al., 2020).

(p1.1) 3 How to predict brain activity?

(p1.2) In this section, we survey work in which neural responses are predicted from linguistic representations. Such work typically aims to shed light on how language functions in the brain. One of the earliest studies exploring the mapping between brain and language representations is by Mitchell et al. (2008), who trained a linear regression model on a set of word representations extracted from 60 nouns using 115 semantic features based on cooccurrence statistics, to predict the corresponding fMRI representations of the same nouns. They use pair-wise matching accuracy evaluation, extracting two words w and w ′ for evaluation, and showed that the predicted fMRI for a word w was closer to the real fMRI image for w than to the real fMRI image for w ′ , at above-chance levels. Mitchell et al. (2008) also report percentile rank results, ranking predicted fMRI images by similarity with the real image of w. We discuss how the metrics relate in §6.

(p1.3) The dataset of Mitchell et al. (2008)is also used by Murphy et al. (2012), who extract linguistic features from part-of-speech taggers, stemmers, and dependency parsers, showing that dependency parsers are the most successful in predicting brain activity. They also use leave-2-out pair-matching as their performance metric.

(p1.4) Later on, Wehbe et al. (2014a) moved on to predicting brain activation patterns for entire sentences rather than for isolated words. They recorded fMRI neural response measurements while participants read a chapter from Harry Potter and the Sorcerer's Stone, then extracted a set of 195 features for each word (ranging from semantic, syntactic properties to visual and discourse-level features) to train a comprehensive generative model that would then predict the time series of the fMRI activity observed when the participants read that passage. Leave-2-out pair-matching accuracy is used for evaluation. Huth et al. (2016), in contrast, use fMRI recordings of participants listening to spoken narrative stories, representing each word in the corpus as a 985-dimensional vector encoding semantic information driven by co-occurrence statistics. They train per-voxel linear regression models and evaluate their predicted per-word fMRI images by their per-voxel Pearson correlation with the real fMRI images, showing that 3-4 dimensions explained a significant amount of variance in the FMRI data. Wehbe et al. (2014b) are among the first to use neural language models, using recurrent models to compute contextualized embeddings, hidden state vectors of previous words, and word probabilities.  of participants reading Harry Potter, obtained in a follow-up study to Wehbe et al. (2014a). From the three sets of representations, they then train linear regression models to predict the MEG vectors corresponding to each word, and the regression models are then evaluated by computing pair-matching accuracy.

(p1.5) Similarly, Søgaard (2016) evaluates static word embeddings on the data from Wehbe et al. (2014a), learning linear transformation from word embeddings into an fMRI vector space. The predictions are evaluated through mean squared error (MSE).

(p1.6) Jain and Huth (2018) evaluate recurrent language models against the fMRI dataset from Huth et al. (2016). Their findings show that contextual language model representations align significantly better (to brain response) compared to static word embedding models. Their evaluation metric is the total sum of explained variance 1 Following this, Schwartz et al. (2019) use attention-based transformer language models for brain mapping. They finetune BERT (Devlin et al., 2019)to predict neural response measurements from the Harry Potter dataset, showing that the fine-tuned models have representations that encode more brain-activity-relevant language information than the non-finetuned models. They rely on pair-matching accuracy as their performance metric.

(p1.7) As in Søgaard (2016), Zhang et al. (2020) map static word embeddings into the vector space of the neural response measurements (fMRI). They introduce a new dataset of such measurements from subjects listening to natural stories. They rely on explained variance as their performance metric.

(p1.8) Toneva and Wehbe (2019) evaluate word and sequence embeddings from 4 recurrent and attention-based transformer language models, using the Harry Potter fMRI dataset. They evaluate models across layers, context lengths, and attention types, using pairwise matching accuracy as their performance metric. In a later study, Toneva et al. (2022a) induce compositional semantic representations of ""supra-word meaning"" which they then use to predict neural responses across regions of interest, evaluating their models using Pearson correlation.

(p1.9) Also using the Harry Potter data,  evaluate five models, one static and four contextualized, relying on a variant of representational similarity analysis (Kriegeskorte et al., 2008). The results suggest that models provide representations of local contexts that are well-aligned to neural measurements. However, as information from further away context is integrated by the models, representations become less aligned to neural measurements.

(p1.10) In a large-scale study, Schrimpf et al. (2021) examine the relationships between 43 diverse stateof-the-art neural network models (including embedding models, recurrent models, and transformers) across three datasets (two fMRI, one electrocardiography). They rely on a metric they term Brain Score which involves normalising the Pearson correlation by a noise ceiling. Their results show that transformer-based models perform better than recurrent or static models, and larger models perform better than smaller ones.

(p1.11) Similarly, in , the Schoffelen et al. (2019) fMRI and MEG datasets are used to compare a variety of transformer architectures. They study how architectural details, training settings, and the linguistic performance of these models independently account for the generation of brain correspondent representations. The results suggest that the better language models are at predicting words from context, the better their activations linearly map onto those of the brain. Antonello et al. (2021) evaluate three static and five attention-based transformer models, in combination with four fine-tuning tasks and two machine translation models. They train linear regression models to evaluate their word-level representations against a new fMRI dataset from participants listening to podcast stories. They find a low-dimensional structure in language representations that can predict brain responses. In a similar setting, Antonello and Huth (2022) examine why some features fit the brain data better arguing that the reason is that they capture various linguistic phenomena.

(p1.12) Reddy and Wehbe (2021) evaluate syntactic features in conjunction with BERT representations, finding that syntax explains additional variance in brain activity in various parts of the language system, even while controlling for complexity metrics that capture processing load.

(p1.13) In a series of studies Caucheteux et al. (2021Caucheteux et al. ( , 2022b investigate GPT2's activations in predicting brain signals using the Nastase et al. (2021) dataset. Their evaluation metric is Brain Score (Schrimpf et al., 2018). To determine which factors affect the brain encoding Pasquiou et al. (2022) examine the impact of test loss, training corpus, model architecture, and fine-tuning in various models using the Li et al. (2022) dataset. They evaluate model performance using Pearson Correlation.

(p1.14) Oota et al. (2022a) study the impact of context size in language models on how they align with neural response measurements. They use the Nastase et al. (2021) dataset and evaluate recurrent and attention-based transformer architectures. In a later study, Oota et al. (2022b) use the Pereira et al. (2018) dataset and evaluate BERTbase models (fine-tuned for various NLP tasks). They showed that neural response predictions from ridge regression with BERT-base models fine-tuned for coreference resolution, NER, and shallow syntactic parsing explained more variance for Pereira et al. (2018) response measurements. On the other hand, tasks such as paraphrase generation, summarization, and natural language inference led to better encoding performance for the Nastase et al. (2021) data (audio). Using the same dataset, in Oota et al. (2022c) it is shown that the presence of surface, syntactic, and semantic linguistic information is crucial for the alignment across all layers of the language model. They use pairwise matching accuracy and/or Pearson correlation as their performance metrics in these studies.

(p1.15) Aw and Toneva (2023) extract feature representations from four attention-based transformer models. They evaluate the impact of fine-tuning on the BookSum dataset (Kryscinski et al., 2021). All models are used to predict brain activity on the Harry Potter data. Pairwise matching accuracy and Pearson correlation are their performance metrics. Merlin and Toneva (2022) focus more narrowly on variants of GPT-2, showing that improvements in alignment with brain recordings are probably not because of the next-word prediction task or wordlevel semantics, but due to multi-word semantics. Their reported metric is Pearson correlation.

(p1.16) Intermediate summary The above studies differ in many respects. Several metrics are used: pairwise-matching accuracy, 2 Pearson correlation (or Brain Score), mean squared error, and representational similarity analysis. Even studies that report the same performance metrics are not directly comparable because they often report on results on different datasets and use slightly different protocols, e.g., Murphy et al. (2012) and Wehbe et al. (2014b). Beinborn et al. (2023) compare various encoding experiments and receive very diverse results for different evaluation metrics. The diversity of metrics and data renders a direct comparison difficult. To remedy this, we consider how the metrics compare in §6.","[['b3', 'b22', 'b40', 'b50', 'b10', 'b25', 'b27', 'b67', 'b5'], [], ['b40'], ['b40', 'b41'], ['b67', 'b27', 'b68'], ['b67', 'b59'], [None, 'b17'], ['b69'], ['b63'], ['b31'], ['b56'], [None, 'b5'], [], ['b57', 'b48', 'b14', 'b12', 'b22', 'b23'], ['b22', 'b50', 'b45'], ['b34', 'b32'], ['b68', 'b8', 'b41']]","[['b3', 'b22', 'b40', 'b50', 'b10', 'b25', 'b27', 'b67', 'b5'], [], ['b40'], ['b40', 'b41'], ['b67', 'b27', 'b68'], ['b67', 'b59'], [None, 'b17'], ['b69'], ['b63'], ['b31'], ['b56'], [None, 'b5'], [], ['b57', 'b48', 'b14', 'b12', 'b22', 'b23'], ['b22', 'b50', 'b45'], ['b34', 'b32'], ['b68', 'b8', 'b41']]",39,"1. To infer a mapping between language models and brains, researchers rely on datasets in which brain activity is recorded in response to linguistic stimuli.
2. In some studies, the stimuli are single words (Mitchell et al., 2008;Anderson et al., 2017) or sentences displayed on a screen (Pereira et al., 2018).
3. In others, participants read longer stories (Wehbe et al., 2014a;Bhattasali et al., 2020;Nastase et al., 2021) or listened to speech or podcasts (Huth et al., 2016;Antonello et al., 2021).
4. Table 1 lists publicly available datasets that have been used in the context of mapping language models to and from recordings of brain response.
5. Differences between the datasets -the number of participants, the equipment, the experimental setup, pre-processing steps, and probabilistic corrections -should lead us to expect some variation in what researchers have concluded (Hollenstein et al., 2020).
6. 3 How to predict brain activity?
7. In this section, we survey work in which neural responses are predicted from linguistic representations.
8. Such work typically aims to shed light on how language functions in the brain.
9. One of the earliest studies exploring the mapping between brain and language representations is by Mitchell et al. (2008), who trained a linear regression model on a set of word representations extracted from 60 nouns using 115 semantic features based on cooccurrence statistics, to predict the corresponding fMRI representations of the same nouns.
10. They use pair-wise matching accuracy evaluation, extracting two words w and w ′ for evaluation, and showed that the predicted fMRI for a word w was closer to the real fMRI image for w than to the real fMRI image for w ′ , at above-chance levels.
11. Mitchell et al. (2008) also report percentile rank results, ranking predicted fMRI images by similarity with the real image of w. We discuss how the metrics relate in §6.
12. The dataset of Mitchell et al. (2008)is also used by Murphy et al. (2012), who extract linguistic features from part-of-speech taggers, stemmers, and dependency parsers, showing that dependency parsers are the most successful in predicting brain activity.
13. They also use leave-2-out pair-matching as their performance metric.Later on, Wehbe et al. (2014a) moved on to predicting brain activation patterns for entire sentences rather than for isolated words.
14. They recorded fMRI neural response measurements while participants read a chapter from Harry Potter and the Sorcerer's Stone, then extracted a set of 195 features for each word (ranging from semantic, syntactic properties to visual and discourse-level features) to train a comprehensive generative model that would then predict the time series of the fMRI activity observed when the participants read that passage.
15. Leave-2-out pair-matching accuracy is used for evaluation.
16. Huth et al. (2016), in contrast, use fMRI recordings of participants listening to spoken narrative stories, representing each word in the corpus as a 985-dimensional vector encoding semantic information driven by co-occurrence statistics.
17. They train per-voxel linear regression models and evaluate their predicted per-word fMRI images by their per-voxel Pearson correlation with the real fMRI images, showing that 3-4 dimensions explained a significant amount of variance in the FMRI data.
18. Wehbe et al. (2014b) are among the first to use neural language models, using recurrent models to compute contextualized embeddings, hidden state vectors of previous words, and word probabilities.
19. of participants reading Harry Potter, obtained in a follow-up study to Wehbe et al. (2014a).
20. From the three sets of representations, they then train linear regression models to predict the MEG vectors corresponding to each word, and the regression models are then evaluated by computing pair-matching accuracy.
21. Similarly, Søgaard (2016) evaluates static word embeddings on the data from Wehbe et al. (2014a), learning linear transformation from word embeddings into an fMRI vector space.
22. The predictions are evaluated through mean squared error (MSE).
23. Jain and Huth (2018) evaluate recurrent language models against the fMRI dataset from Huth et al. (2016).
24. Their findings show that contextual language model representations align significantly better (to brain response) compared to static word embedding models.
25. Their evaluation metric is the total sum of explained variance 1
26. Following this, Schwartz et al. (2019) use attention-based transformer language models for brain mapping.
27. They finetune BERT (Devlin et al., 2019)to predict neural response measurements from the Harry Potter dataset, showing that the fine-tuned models have representations that encode more brain-activity-relevant language information than the non-finetuned models.
28. They rely on pair-matching accuracy as their performance metric.
29. As in Søgaard (2016), Zhang et al. (2020) map static word embeddings into the vector space of the neural response measurements (fMRI).
30. They introduce a new dataset of such measurements from subjects listening to natural stories.
31. They rely on explained variance as their performance metric.
32. Toneva and Wehbe (2019) evaluate word and sequence embeddings from 4 recurrent and attention-based transformer language models, using the Harry Potter fMRI dataset.
33. They evaluate models across layers, context lengths, and attention types, using pairwise matching accuracy as their performance metric.
34. In a later study, Toneva et al. (2022a) induce compositional semantic representations of ""supra-word meaning"" which they then use to predict neural responses across regions of interest, evaluating their models using Pearson correlation.
35. Also using the Harry Potter data,  evaluate five models, one static and four contextualized, relying on a variant of representational similarity analysis (Kriegeskorte et al., 2008).
36. The results suggest that models provide representations of local contexts that are well-aligned to neural measurements.
37. However, as information from further away context is integrated by the models, representations become less aligned to neural measurements.
38. In a large-scale study, Schrimpf et al. (2021) examine the relationships between 43 diverse stateof-the-art neural network models (including embedding models, recurrent models, and transformers) across three datasets (two fMRI, one electrocardiography).
39. They rely on a metric they term Brain Score which involves normalising the Pearson correlation by a noise ceiling.
40. Their results show that transformer-based models perform better than recurrent or static models, and larger models perform better than smaller ones.
41. Similarly, in , the Schoffelen et al. (2019) fMRI and MEG datasets are used to compare a variety of transformer architectures.
42. They study how architectural details, training settings, and the linguistic performance of these models independently account for the generation of brain correspondent representations.
43. The results suggest that the better language models are at predicting words from context, the better their activations linearly map onto those of the brain.
44. Antonello et al. (2021) evaluate three static and five attention-based transformer models, in combination with four fine-tuning tasks and two machine translation models.
45. They train linear regression models to evaluate their word-level representations against a new fMRI dataset from participants listening to podcast stories.
46. They find a low-dimensional structure in language representations that can predict brain responses.
47. In a similar setting, Antonello and Huth (2022) examine why some features fit the brain data better arguing that the reason is that they capture various linguistic phenomena.
48. Reddy and Wehbe (2021) evaluate syntactic features in conjunction with BERT representations, finding that syntax explains additional variance in brain activity in various parts of the language system, even while controlling for complexity metrics that capture processing load.
49. In a series of studies Caucheteux et al. (2021Caucheteux et al. ( , 2022b investigate GPT2's activations in predicting brain signals using the Nastase et al. (2021) dataset.
50. Their evaluation metric is Brain Score (Schrimpf et al., 2018).
51. To determine which factors affect the brain encoding Pasquiou et al. (2022) examine the impact of test loss, training corpus, model architecture, and fine-tuning in various models using the Li et al. (2022) dataset.
52. They evaluate model performance using Pearson Correlation.
53. Oota et al. (2022a) study the impact of context size in language models on how they align with neural response measurements.
54. They use the Nastase et al. (2021) dataset and evaluate recurrent and attention-based transformer architectures.
55. In a later study, Oota et al. (2022b) use the Pereira et al. (2018) dataset and evaluate BERTbase models (fine-tuned for various NLP tasks).
56. They showed that neural response predictions from ridge regression with BERT-base models fine-tuned for coreference resolution, NER, and shallow syntactic parsing explained more variance for Pereira et al. (2018) response measurements.
57. On the other hand, tasks such as paraphrase generation, summarization, and natural language inference led to better encoding performance for the Nastase et al. (2021) data (audio).
58. Using the same dataset, in Oota et al. (2022c) it is shown that the presence of surface, syntactic, and semantic linguistic information is crucial for the alignment across all layers of the language model.
59. They use pairwise matching accuracy and/or Pearson correlation as their performance metrics in these studies.
60. Aw and Toneva (2023) extract feature representations from four attention-based transformer models.
61. They evaluate the impact of fine-tuning on the BookSum dataset (Kryscinski et al., 2021).
62. All models are used to predict brain activity on the Harry Potter data.
63. Pairwise matching accuracy and Pearson correlation are their performance metrics.
64. Merlin and Toneva (2022) focus more narrowly on variants of GPT-2, showing that improvements in alignment with brain recordings are probably not because of the next-word prediction task or wordlevel semantics, but due to multi-word semantics.
65. Their reported metric is Pearson correlation.
66. Intermediate summary The above studies differ in many respects.
67. Several metrics are used: pairwise-matching accuracy, 2 Pearson correlation (or Brain Score), mean squared error, and representational similarity analysis.
68. Even studies that report the same performance metrics are not directly comparable because they often report on results on different datasets and use slightly different protocols, e.g., Murphy et al. (2012) and Wehbe et al. (2014b).
69. Beinborn et al. (2023) compare various encoding experiments and receive very diverse results for different evaluation metrics.
70. The diversity of metrics and data renders a direct comparison difficult.
71. To remedy this, we consider how the metrics compare in §6."
259108815,Mapping Brains with Language Models: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/a92c82231c0ea7027ab20b25fe5f82565047aad7,s2,How to predict linguistic stimuli?,"['p2.0', 'p2.1', 'p2.2', 'p2.3', 'p2.4']","['Decoding models work in the other direction and aim to predict linguistic features of the stimuli from recordings of brain response. Pereira et al. (2018) introduce a decoder that predicts stimuli representation of semantic features given fMRI data.They introduce a novel dataset of neural responses aligned with annotation of concrete and abstract semantic categories (such as pleasure, ignorance, cooking etc.). They evaluate static word embeddings by applying ridge regression to predict per-word fMRI vectors. A separate regression model is trained per dimension, allowing for dimension-wise regularization. The model is evaluated in terms of pairwise matching accuracy, but also in terms of percentile rank, adapted to the decoding scenario.', 'Gauthier and Levy (2019) also train linear regression models which map from the response measurements in Pereira et al. (2018), but to representations of the same sentences produced by the BERT language model finetuned on different natural language understanding tasks. The regression models are evaluated using two metrics: mean squared error and average percentile rank. Their results show that fine-tuning with different NLU objectives leads to worse alignment and that, somewhat surprisingly, the only objective which does lead to better alignment is a scrambled language modeling task where the model is trained to predict scrambled sentences.', 'Minnema and Herbelot (2019) re-examine the work of Pereira et al. (2018) using various metrics (pairwise matching accuracy, percentile rank, cosine distance, R 2 , RSA), comparing decoder models (ridge regression, perceptron, and convolutional neural networks). 3 They show that positive results are only obtained using pairwise matching accuracy. Abdou et al. (2021) investigate whether aligning language models with brain recordings can be improved by biasing their attention with annotations from syntactic or semantic formalisms. They fine-tune the BERT models using several syntactosemantic formalisms and evaluate their alignment with brain activity measurements from the Wehbe et al. (2014a) and Pereira et al. (2018) datasets. Their results -obtained using Pearson correlation as performance metric -are positive for two in three formalisms. Zou et al. (2022) propose a new evaluation method for decoding, a so-called cross-modal cloze task. They generate the data for the task from the neural response measures in Mitchell et al. (2008) and Wehbe et al. (2014a). The task itself amounts to a cloze task in which the context is prefixed by the fMRI image of the masked word. They evaluate models using precision@k. Note how this task is considerably easier than linearly mapping from language model representations into fMRI images, and precision@k results therefore cannot be compared to those obtained in other settings. Their best precision@1 scores are around 0.3, but only marginally (0.03) better than a unimodal LM.', 'Finally, Pascual et al. (2022) try a more realistic setup by predicting language from fMRI scans of subjects not included in the training. They use the (Pereira et al., 2018) dataset and evaluate the regression models based on pairwise accuracy and precision@k (or top-k accuracy). They propose evaluating with direct classification as a more demanding setup to evaluate and understand current brain decoding models.', 'Intermediate summary Decoding studies also differ in many respects. Several metrics are used: pairwise-matching accuracy, Pearson correlation, percentile rank, cosine distance, precision@k, and representational similarity analysis; and several datasets are used. Gauthier and Ivanova (2018) criticize the evaluation techniques of decoding studies and suggest adopting task and mechanism explicit models. It is of particular interest to our study that both Minnema and Herbelot (2019) only report positive results for pairwise matching accuracy compared to other metrics. This suggests pairwise matching accuracy is a less conservative metric (and maybe less reliable).']","Decoding models work in the other direction and aim to predict linguistic features of the stimuli from recordings of brain response. Pereira et al. (2018) introduce a decoder that predicts stimuli representation of semantic features given fMRI data.They introduce a novel dataset of neural responses aligned with annotation of concrete and abstract semantic categories (such as pleasure, ignorance, cooking etc.). They evaluate static word embeddings by applying ridge regression to predict per-word fMRI vectors. A separate regression model is trained per dimension, allowing for dimension-wise regularization. The model is evaluated in terms of pairwise matching accuracy, but also in terms of percentile rank, adapted to the decoding scenario.

Gauthier and Levy (2019) also train linear regression models which map from the response measurements in Pereira et al. (2018), but to representations of the same sentences produced by the BERT language model finetuned on different natural language understanding tasks. The regression models are evaluated using two metrics: mean squared error and average percentile rank. Their results show that fine-tuning with different NLU objectives leads to worse alignment and that, somewhat surprisingly, the only objective which does lead to better alignment is a scrambled language modeling task where the model is trained to predict scrambled sentences.

Minnema and Herbelot (2019) re-examine the work of Pereira et al. (2018) using various metrics (pairwise matching accuracy, percentile rank, cosine distance, R 2 , RSA), comparing decoder models (ridge regression, perceptron, and convolutional neural networks). 3 They show that positive results are only obtained using pairwise matching accuracy. Abdou et al. (2021) investigate whether aligning language models with brain recordings can be improved by biasing their attention with annotations from syntactic or semantic formalisms. They fine-tune the BERT models using several syntactosemantic formalisms and evaluate their alignment with brain activity measurements from the Wehbe et al. (2014a) and Pereira et al. (2018) datasets. Their results -obtained using Pearson correlation as performance metric -are positive for two in three formalisms. Zou et al. (2022) propose a new evaluation method for decoding, a so-called cross-modal cloze task. They generate the data for the task from the neural response measures in Mitchell et al. (2008) and Wehbe et al. (2014a). The task itself amounts to a cloze task in which the context is prefixed by the fMRI image of the masked word. They evaluate models using precision@k. Note how this task is considerably easier than linearly mapping from language model representations into fMRI images, and precision@k results therefore cannot be compared to those obtained in other settings. Their best precision@1 scores are around 0.3, but only marginally (0.03) better than a unimodal LM.

Finally, Pascual et al. (2022) try a more realistic setup by predicting language from fMRI scans of subjects not included in the training. They use the (Pereira et al., 2018) dataset and evaluate the regression models based on pairwise accuracy and precision@k (or top-k accuracy). They propose evaluating with direct classification as a more demanding setup to evaluate and understand current brain decoding models.

Intermediate summary Decoding studies also differ in many respects. Several metrics are used: pairwise-matching accuracy, Pearson correlation, percentile rank, cosine distance, precision@k, and representational similarity analysis; and several datasets are used. Gauthier and Ivanova (2018) criticize the evaluation techniques of decoding studies and suggest adopting task and mechanism explicit models. It is of particular interest to our study that both Minnema and Herbelot (2019) only report positive results for pairwise matching accuracy compared to other metrics. This suggests pairwise matching accuracy is a less conservative metric (and maybe less reliable).","(p2.0) Decoding models work in the other direction and aim to predict linguistic features of the stimuli from recordings of brain response. Pereira et al. (2018) introduce a decoder that predicts stimuli representation of semantic features given fMRI data.They introduce a novel dataset of neural responses aligned with annotation of concrete and abstract semantic categories (such as pleasure, ignorance, cooking etc.). They evaluate static word embeddings by applying ridge regression to predict per-word fMRI vectors. A separate regression model is trained per dimension, allowing for dimension-wise regularization. The model is evaluated in terms of pairwise matching accuracy, but also in terms of percentile rank, adapted to the decoding scenario.

(p2.1) Gauthier and Levy (2019) also train linear regression models which map from the response measurements in Pereira et al. (2018), but to representations of the same sentences produced by the BERT language model finetuned on different natural language understanding tasks. The regression models are evaluated using two metrics: mean squared error and average percentile rank. Their results show that fine-tuning with different NLU objectives leads to worse alignment and that, somewhat surprisingly, the only objective which does lead to better alignment is a scrambled language modeling task where the model is trained to predict scrambled sentences.

(p2.2) Minnema and Herbelot (2019) re-examine the work of Pereira et al. (2018) using various metrics (pairwise matching accuracy, percentile rank, cosine distance, R 2 , RSA), comparing decoder models (ridge regression, perceptron, and convolutional neural networks). 3 They show that positive results are only obtained using pairwise matching accuracy. Abdou et al. (2021) investigate whether aligning language models with brain recordings can be improved by biasing their attention with annotations from syntactic or semantic formalisms. They fine-tune the BERT models using several syntactosemantic formalisms and evaluate their alignment with brain activity measurements from the Wehbe et al. (2014a) and Pereira et al. (2018) datasets. Their results -obtained using Pearson correlation as performance metric -are positive for two in three formalisms. Zou et al. (2022) propose a new evaluation method for decoding, a so-called cross-modal cloze task. They generate the data for the task from the neural response measures in Mitchell et al. (2008) and Wehbe et al. (2014a). The task itself amounts to a cloze task in which the context is prefixed by the fMRI image of the masked word. They evaluate models using precision@k. Note how this task is considerably easier than linearly mapping from language model representations into fMRI images, and precision@k results therefore cannot be compared to those obtained in other settings. Their best precision@1 scores are around 0.3, but only marginally (0.03) better than a unimodal LM.

(p2.3) Finally, Pascual et al. (2022) try a more realistic setup by predicting language from fMRI scans of subjects not included in the training. They use the (Pereira et al., 2018) dataset and evaluate the regression models based on pairwise accuracy and precision@k (or top-k accuracy). They propose evaluating with direct classification as a more demanding setup to evaluate and understand current brain decoding models.

(p2.4) Intermediate summary Decoding studies also differ in many respects. Several metrics are used: pairwise-matching accuracy, Pearson correlation, percentile rank, cosine distance, precision@k, and representational similarity analysis; and several datasets are used. Gauthier and Ivanova (2018) criticize the evaluation techniques of decoding studies and suggest adopting task and mechanism explicit models. It is of particular interest to our study that both Minnema and Herbelot (2019) only report positive results for pairwise matching accuracy compared to other metrics. This suggests pairwise matching accuracy is a less conservative metric (and maybe less reliable).","[['b50'], ['b50'], ['b40', 'b50', 'b0', 'b67', 'b70'], ['b47', 'b50'], ['b19']]","[['b50'], ['b50'], ['b40', 'b50', 'b0', 'b67', 'b70'], ['b47', 'b50'], ['b19']]",10,"1. Decoding models work in the other direction and aim to predict linguistic features of the stimuli from recordings of brain response.
2. Pereira et al. (2018) introduce a decoder that predicts stimuli representation of semantic features given fMRI data.
3. They introduce a novel dataset of neural responses aligned with annotation of concrete and abstract semantic categories (such as pleasure, ignorance, cooking etc.).
4. They evaluate static word embeddings by applying ridge regression to predict per-word fMRI vectors.
5. A separate regression model is trained per dimension, allowing for dimension-wise regularization.
6. The model is evaluated in terms of pairwise matching accuracy, but also in terms of percentile rank, adapted to the decoding scenario.
7. Gauthier and Levy (2019) also train linear regression models which map from the response measurements in Pereira et al. (2018), but to representations of the same sentences produced by the BERT language model finetuned on different natural language understanding tasks.
8. The regression models are evaluated using two metrics: mean squared error and average percentile rank.
9. Their results show that fine-tuning with different NLU objectives leads to worse alignment and that, somewhat surprisingly, the only objective which does lead to better alignment is a scrambled language modeling task where the model is trained to predict scrambled sentences.
10. Minnema and Herbelot (2019) re-examine the work of Pereira et al. (2018) using various metrics (pairwise matching accuracy, percentile rank, cosine distance, R 2 , RSA), comparing decoder models (ridge regression, perceptron, and convolutional neural networks). 3
11. They show that positive results are only obtained using pairwise matching accuracy.
12. Abdou et al. (2021) investigate whether aligning language models with brain recordings can be improved by biasing their attention with annotations from syntactic or semantic formalisms.
13. They fine-tune the BERT models using several syntactosemantic formalisms and evaluate their alignment with brain activity measurements from the Wehbe et al. (2014a) and Pereira et al. (2018) datasets.
14. Their results -obtained using Pearson correlation as performance metric -are positive for two in three formalisms.
15. Zou et al. (2022) propose a new evaluation method for decoding, a so-called cross-modal cloze task.
16. They generate the data for the task from the neural response measures in Mitchell et al. (2008) and Wehbe et al. (2014a).
17. The task itself amounts to a cloze task in which the context is prefixed by the fMRI image of the masked word.
18. They evaluate models using precision@k.
19. Note how this task is considerably easier than linearly mapping from language model representations into fMRI images, and precision@k results therefore cannot be compared to those obtained in other settings.
20. Their best precision@1 scores are around 0.3, but only marginally (0.03) better than a unimodal LM.
21. Finally, Pascual et al. (2022) try a more realistic setup by predicting language from fMRI scans of subjects not included in the training.
22. They use the (Pereira et al., 2018) dataset and evaluate the regression models based on pairwise accuracy and precision@k (or top-k accuracy).
23. They propose evaluating with direct classification as a more demanding setup to evaluate and understand current brain decoding models.
24. Intermediate summary Decoding studies also differ in many respects.
25. Several metrics are used: pairwise-matching accuracy, Pearson correlation, percentile rank, cosine distance, precision@k, and representational similarity analysis; and several datasets are used.
26. Gauthier and Ivanova (2018) criticize the evaluation techniques of decoding studies and suggest adopting task and mechanism explicit models.
27. It is of particular interest to our study that both Minnema and Herbelot (2019) only report positive results for pairwise matching accuracy compared to other metrics.
28. This suggests pairwise matching accuracy is a less conservative metric (and maybe less reliable)."
259108815,Mapping Brains with Language Models: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/a92c82231c0ea7027ab20b25fe5f82565047aad7,s4,Discussion,"['p4.0', 'p4.1', 'p4.2', 'p4.3', 'p4.4', 'p4.5']","['Many studies, summarized above, aim to compare language model representations with neural response measurements using linear mapping mod-els. Our main reason to focus on linear mapping models is that they quantify the degree of structural similarity (isomorphism). Overall, results suggest that structural similarities between language models and neural responses exist. Furthermore, there is good evidence that alignment has correlated positively with model quality and model size, suggesting a certain level of convergence as language models improve.', 'What drives alignment? Is alignment driven by deep processing characteristics or by shallow textual characteristics? Classical candidates for shallow ones would be word length, frequency, regularity, and part of speech. Mitchell et al. (2008), for example, only controlled for part of speech. Some authors have presented results to suggest that alignments are driven by syntactic or semantic factors (Abdou et al., 2021;Reddy and Wehbe, 2021;Caucheteux et al., 2021;Zhang et al., 2020), whereas others have claimed some similarities reflect semantic phenomena (Huth et al., 2016;Caucheteux et al., 2021). Others suggest that alignments reflect deeper similarities between model objectives and predictive processing in human brains (Schrimpf et al., 2018;Caucheteux et al., 2022a;Goldstein et al., 2021), but see Antonello and Huth (2022) for a critical discussion of such work.', ""Linguistically-transparent models that allow for a principled decomposition of a model's components into smaller linguistically meaningful units and models that move towards possible neurobiological implementations of neural computation are likely to be key for answering this question Ten Oever et al., 2022). Given the plethora of interpretability methods recently developed, however, we believe that even models which are not intrinsically interpretable can be useful toward this goal."", 'Do some models align better? Most studies observe that better and larger, contextual models align better with neural responses (Jain and Huth, 2018;. Other improvements include fine-tuning on specific tasks (Oota et al., 2022b;Aw and Toneva, 2023). Pasquiou et al. (2022) outline the impact of model training choices.', 'What metrics? The inconsistent use of performance metrics makes it hard to compare and interpret the results reported in the literature (Beinborn et al., 2023). We have shown that some metrics are perhaps too permissible to detect structural sim-ilarities between language models and neural responses. We have argued that precision@k is more conservative than most other metrics. Minnema and Herbelot (2019) have proposed using analogy scores. In the limit (given sufficient analogies), perfect analogical accuracy implies isomorphism (Garneau et al., 2021). So do perfect precision@1 and perfect RSA scores. We, therefore, propose giving priority to these performance metrics, not to conflate shallow processing characteristics with deeper, more semantic properties.', 'Meta-analysis? Proper meta-analysis is currently hindered by the use of different metrics, but we have taken steps to relate these.']","Many studies, summarized above, aim to compare language model representations with neural response measurements using linear mapping mod-els. Our main reason to focus on linear mapping models is that they quantify the degree of structural similarity (isomorphism). Overall, results suggest that structural similarities between language models and neural responses exist. Furthermore, there is good evidence that alignment has correlated positively with model quality and model size, suggesting a certain level of convergence as language models improve.

What drives alignment? Is alignment driven by deep processing characteristics or by shallow textual characteristics? Classical candidates for shallow ones would be word length, frequency, regularity, and part of speech. Mitchell et al. (2008), for example, only controlled for part of speech. Some authors have presented results to suggest that alignments are driven by syntactic or semantic factors (Abdou et al., 2021;Reddy and Wehbe, 2021;Caucheteux et al., 2021;Zhang et al., 2020), whereas others have claimed some similarities reflect semantic phenomena (Huth et al., 2016;Caucheteux et al., 2021). Others suggest that alignments reflect deeper similarities between model objectives and predictive processing in human brains (Schrimpf et al., 2018;Caucheteux et al., 2022a;Goldstein et al., 2021), but see Antonello and Huth (2022) for a critical discussion of such work.

Linguistically-transparent models that allow for a principled decomposition of a model's components into smaller linguistically meaningful units and models that move towards possible neurobiological implementations of neural computation are likely to be key for answering this question Ten Oever et al., 2022). Given the plethora of interpretability methods recently developed, however, we believe that even models which are not intrinsically interpretable can be useful toward this goal.

Do some models align better? Most studies observe that better and larger, contextual models align better with neural responses (Jain and Huth, 2018;. Other improvements include fine-tuning on specific tasks (Oota et al., 2022b;Aw and Toneva, 2023). Pasquiou et al. (2022) outline the impact of model training choices.

What metrics? The inconsistent use of performance metrics makes it hard to compare and interpret the results reported in the literature (Beinborn et al., 2023). We have shown that some metrics are perhaps too permissible to detect structural sim-ilarities between language models and neural responses. We have argued that precision@k is more conservative than most other metrics. Minnema and Herbelot (2019) have proposed using analogy scores. In the limit (given sufficient analogies), perfect analogical accuracy implies isomorphism (Garneau et al., 2021). So do perfect precision@1 and perfect RSA scores. We, therefore, propose giving priority to these performance metrics, not to conflate shallow processing characteristics with deeper, more semantic properties.

Meta-analysis? Proper meta-analysis is currently hindered by the use of different metrics, but we have taken steps to relate these.","(p4.0) Many studies, summarized above, aim to compare language model representations with neural response measurements using linear mapping mod-els. Our main reason to focus on linear mapping models is that they quantify the degree of structural similarity (isomorphism). Overall, results suggest that structural similarities between language models and neural responses exist. Furthermore, there is good evidence that alignment has correlated positively with model quality and model size, suggesting a certain level of convergence as language models improve.

(p4.1) What drives alignment? Is alignment driven by deep processing characteristics or by shallow textual characteristics? Classical candidates for shallow ones would be word length, frequency, regularity, and part of speech. Mitchell et al. (2008), for example, only controlled for part of speech. Some authors have presented results to suggest that alignments are driven by syntactic or semantic factors (Abdou et al., 2021;Reddy and Wehbe, 2021;Caucheteux et al., 2021;Zhang et al., 2020), whereas others have claimed some similarities reflect semantic phenomena (Huth et al., 2016;Caucheteux et al., 2021). Others suggest that alignments reflect deeper similarities between model objectives and predictive processing in human brains (Schrimpf et al., 2018;Caucheteux et al., 2022a;Goldstein et al., 2021), but see Antonello and Huth (2022) for a critical discussion of such work.

(p4.2) Linguistically-transparent models that allow for a principled decomposition of a model's components into smaller linguistically meaningful units and models that move towards possible neurobiological implementations of neural computation are likely to be key for answering this question Ten Oever et al., 2022). Given the plethora of interpretability methods recently developed, however, we believe that even models which are not intrinsically interpretable can be useful toward this goal.

(p4.3) Do some models align better? Most studies observe that better and larger, contextual models align better with neural responses (Jain and Huth, 2018;. Other improvements include fine-tuning on specific tasks (Oota et al., 2022b;Aw and Toneva, 2023). Pasquiou et al. (2022) outline the impact of model training choices.

(p4.4) What metrics? The inconsistent use of performance metrics makes it hard to compare and interpret the results reported in the literature (Beinborn et al., 2023). We have shown that some metrics are perhaps too permissible to detect structural sim-ilarities between language models and neural responses. We have argued that precision@k is more conservative than most other metrics. Minnema and Herbelot (2019) have proposed using analogy scores. In the limit (given sufficient analogies), perfect analogical accuracy implies isomorphism (Garneau et al., 2021). So do perfect precision@1 and perfect RSA scores. We, therefore, propose giving priority to these performance metrics, not to conflate shallow processing characteristics with deeper, more semantic properties.

(p4.5) Meta-analysis? Proper meta-analysis is currently hindered by the use of different metrics, but we have taken steps to relate these.","[[], ['b53', 'b57', 'b69', 'b40', 'b0', 'b4', 'b12', 'b27', 'b22', 'b13'], ['b62'], ['b29', 'b44', 'b48', 'b7'], ['b38', 'b18', 'b8'], []]","[[], ['b53', 'b57', 'b69', 'b40', 'b0', 'b4', 'b12', 'b27', 'b22', 'b13'], ['b62'], ['b29', 'b44', 'b48', 'b7'], ['b38', 'b18', 'b8'], []]",18,"1. Many studies, summarized above, aim to compare language model representations with neural response measurements using linear mapping mod-els.
2. Our main reason to focus on linear mapping models is that they quantify the degree of structural similarity (isomorphism).
3. Overall, results suggest that structural similarities between language models and neural responses exist.
4. Furthermore, there is good evidence that alignment has correlated positively with model quality and model size, suggesting a certain level of convergence as language models improve.
5. What drives alignment? Is alignment driven by deep processing characteristics or by shallow textual characteristics?
6. Classical candidates for shallow ones would be word length, frequency, regularity, and part of speech.
7. Mitchell et al. (2008), for example, only controlled for part of speech.
8. Some authors have presented results to suggest that alignments are driven by syntactic or semantic factors (Abdou et al., 2021;Reddy and Wehbe, 2021;Caucheteux et al., 2021;Zhang et al., 2020), whereas others have claimed some similarities reflect semantic phenomena (Huth et al., 2016;Caucheteux et al., 2021).
9. Others suggest that alignments reflect deeper similarities between model objectives and predictive processing in human brains (Schrimpf et al., 2018;Caucheteux et al., 2022a;Goldstein et al., 2021), but see Antonello and Huth (2022) for a critical discussion of such work.
10. Linguistically-transparent models that allow for a principled decomposition of a model's components into smaller linguistically meaningful units and models that move towards possible neurobiological implementations of neural computation are likely to be key for answering this question Ten Oever et al., 2022).
11. Given the plethora of interpretability methods recently developed, however, we believe that even models which are not intrinsically interpretable can be useful toward this goal.
12. Do some models align better? Most studies observe that better and larger, contextual models align better with neural responses (Jain and Huth, 2018;. Other improvements include fine-tuning on specific tasks (Oota et al., 2022b;Aw and Toneva, 2023).
13. Pasquiou et al. (2022) outline the impact of model training choices.
14. What metrics? The inconsistent use of performance metrics makes it hard to compare and interpret the results reported in the literature (Beinborn et al., 2023).
15. We have shown that some metrics are perhaps too permissible to detect structural sim-ilarities between language models and neural responses.
16. We have argued that precision@k is more conservative than most other metrics.
17. Minnema and Herbelot (2019) have proposed using analogy scores.
18. In the limit (given sufficient analogies), perfect analogical accuracy implies isomorphism (Garneau et al., 2021).
19. So do perfect precision@1 and perfect RSA scores.
20. We, therefore, propose giving priority to these performance metrics, not to conflate shallow processing characteristics with deeper, more semantic properties.
21. Meta-analysis? Proper meta-analysis is currently hindered by the use of different metrics, but we have taken steps to relate these."
259108815,Mapping Brains with Language Models: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/a92c82231c0ea7027ab20b25fe5f82565047aad7,s6,Limitations,"['p6.0', 'p6.1']","[""This work focuses on a specific view of the whole neuro-computational modeling field. We exclude specific angles of research such as non-linear models (Ruan et al., 2016;Qian et al., 2016;Bingel et al., 2016;Anderson et al., 2017;Oota et al., 2018) since we want to evaluate the accumulated evidence for structural similarity (isomorphism) between neural responses and language models. (Ivanova et al., 2022) mention several advantages of using linear mapping models, they are more interpretable and more biologically plausible. They also provide an insightful discussion on mapping model choice, emphasizing the importance of estimating models' complexity over categorizing them as purely linear or nonlinear."", 'Another limitation is that we do not include speech models (Vaidya et al., 2022;Défossez et al., 2022;Millet et al., 2022) that have been used to map brain representations mostly due to coherency and page-limit restrictions. The survey is also limited to fMRI and MEG data instead of other modalities for two many reasons: (i) fMRI and MEG are used as a combination in many studies Schrimpf et al., 2021;Toneva et al., 2022a), and (ii) they offer high spatial resolution and signal reliability (fMRI) and better temporal and spatial resolution (MEG), making them suitable for NLP (Hollenstein et al., 2020). For a survey in encoding and decoding models in cognitive electrophysiology, see Holdgraf et al. (2017).']","This work focuses on a specific view of the whole neuro-computational modeling field. We exclude specific angles of research such as non-linear models (Ruan et al., 2016;Qian et al., 2016;Bingel et al., 2016;Anderson et al., 2017;Oota et al., 2018) since we want to evaluate the accumulated evidence for structural similarity (isomorphism) between neural responses and language models. (Ivanova et al., 2022) mention several advantages of using linear mapping models, they are more interpretable and more biologically plausible. They also provide an insightful discussion on mapping model choice, emphasizing the importance of estimating models' complexity over categorizing them as purely linear or nonlinear.

Another limitation is that we do not include speech models (Vaidya et al., 2022;Défossez et al., 2022;Millet et al., 2022) that have been used to map brain representations mostly due to coherency and page-limit restrictions. The survey is also limited to fMRI and MEG data instead of other modalities for two many reasons: (i) fMRI and MEG are used as a combination in many studies Schrimpf et al., 2021;Toneva et al., 2022a), and (ii) they offer high spatial resolution and signal reliability (fMRI) and better temporal and spatial resolution (MEG), making them suitable for NLP (Hollenstein et al., 2020). For a survey in encoding and decoding models in cognitive electrophysiology, see Holdgraf et al. (2017).","(p6.0) This work focuses on a specific view of the whole neuro-computational modeling field. We exclude specific angles of research such as non-linear models (Ruan et al., 2016;Qian et al., 2016;Bingel et al., 2016;Anderson et al., 2017;Oota et al., 2018) since we want to evaluate the accumulated evidence for structural similarity (isomorphism) between neural responses and language models. (Ivanova et al., 2022) mention several advantages of using linear mapping models, they are more interpretable and more biologically plausible. They also provide an insightful discussion on mapping model choice, emphasizing the importance of estimating models' complexity over categorizing them as purely linear or nonlinear.

(p6.1) Another limitation is that we do not include speech models (Vaidya et al., 2022;Défossez et al., 2022;Millet et al., 2022) that have been used to map brain representations mostly due to coherency and page-limit restrictions. The survey is also limited to fMRI and MEG data instead of other modalities for two many reasons: (i) fMRI and MEG are used as a combination in many studies Schrimpf et al., 2021;Toneva et al., 2022a), and (ii) they offer high spatial resolution and signal reliability (fMRI) and better temporal and spatial resolution (MEG), making them suitable for NLP (Hollenstein et al., 2020). For a survey in encoding and decoding models in cognitive electrophysiology, see Holdgraf et al. (2017).","[['b3', 'b54', 'b11', None, 'b52'], ['b56', 'b66', 'b36', 'b16', 'b63', 'b25', 'b24']]","[['b3', 'b54', 'b11', None, 'b52'], ['b56', 'b66', 'b36', 'b16', 'b63', 'b25', 'b24']]",12,"1. This work focuses on a specific view of the whole neuro-computational modeling field.
2. We exclude specific angles of research such as non-linear models (Ruan et al., 2016;Qian et al., 2016;Bingel et al., 2016;Anderson et al., 2017;Oota et al., 2018) since we want to evaluate the accumulated evidence for structural similarity (isomorphism) between neural responses and language models.
3. (Ivanova et al., 2022) mention several advantages of using linear mapping models, they are more interpretable and more biologically plausible.
4. They also provide an insightful discussion on mapping model choice, emphasizing the importance of estimating models' complexity over categorizing them as purely linear or nonlinear.
5. Another limitation is that we do not include speech models (Vaidya et al., 2022;Défossez et al., 2022;Millet et al., 2022) that have been used to map brain representations mostly due to coherency and page-limit restrictions.
6. The survey is also limited to fMRI and MEG data instead of other modalities for two many reasons: (i) fMRI and MEG are used as a combination in many studies Schrimpf et al., 2021;Toneva et al., 2022a), and (ii) they offer high spatial resolution and signal reliability (fMRI) and better temporal and spatial resolution (MEG), making them suitable for NLP (Hollenstein et al., 2020).
7. For a survey in encoding and decoding models in cognitive electrophysiology, see Holdgraf et al. (2017)."
259108815,Mapping Brains with Language Models: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/a92c82231c0ea7027ab20b25fe5f82565047aad7,s3,Performance Metrics,"['p3.0', 'p3.1', 'p3.2', 'p3.3', 'p3.4', 'p3.5', 'p3.6', 'p3.7', 'p3.8', 'p3.9', 'p3.10', 'p3.11', 'p3.12', 'p3.13', 'p3.14', 'p3.15']","[""We present the evaluation metrics used in the above studies and discuss how they relate. See Table 2 for a summary of metrics and corresponding studies. Mitchell et al. (2008) introduce pairwise matching accuracy. Because of their small sample size, they use a leave-2-out cross-validation, which later work also adopted. The metric is a binary classification accuracy metric on a balanced dataset, so a random baseline converges toward 0.5. Many studies have relied on this metric, both in encoding and decoding (see Table 2). 4 Pearson correlationPearson correlation is another widely used metric in the studies surveyed above, measuring the linear relationship between variables, and providing insight into the strength and direction of their association. Huth et al. (2016), compute Pearson correlation between predicted and actual brain responses using Gaussian random vectors to test statistical significance. Resulting p-values are corrected for multiple comparisons within each subject using false discovery rate (FDR) (Benjamini and Hochberg, 1995). Others have used Bonferroni correction (Huth et al., 2016) or block-wise permutation test (Adolf et al., 2014) to evaluate the statistical significance of the correlation (Zhang et al., 2020). Some report R 2 (explained variance) instead of or in addition to correlation coefficients (Minnema and Herbelot, 2019;Reddy and Wehbe, 2021). Others have adopted a more elaborate extension of Pearson correlation, namely BrainScore (Schrimpf et al., 2018). Brain-Score is estimated on held-out test data, calculating Pearson's correlation between model predictions and neural recordings divided by the estimated ceiling and averaged across voxels and participants."", 'Percentile rank was first used for encoding (Mitchell et al., 2008), but can also be used for decoding (Pereira et al., 2018;Gauthier and Levy, 2019;Minnema and Herbelot, 2019). In encoding, the predicted brain image for w is ranked along the predicted images for a set of candidate words w ′ by their similarity to the real (ground truth) image for w. The average rank is then reported. For decoding, they rank word vectors rather than neural response images. Note the similarity metric is unspecified, but typically cosine distance is used.', 'Mean squared error, the average of the squared differences between word vectors and neural responses, was first used for encoding in Søgaard (2016) on a held-out test split. It was also used by Gauthier and Levy (2019).', 'Representational similarity analysis (RSA) was introduced in Kriegeskorte et al. (2008) as a non-parametric way to characterize structural alignment between the geometries of representations derived from disparate modalities. RSA abstracts away from activity patterns themselves and instead computes representational similarity matrices (RSMs), which characterize the information carried by a given representation method through global similarity structure. A rank correlation coefficient is computed between RSMs derived from the two spaces, providing a summary statistic indicative of the overall representational alignment between them. Being non-parametric, RSA circumvents many of the various methodological weaknesses (such as over fitting, etc.). Gauthier and Levy (2019), Minnema and Herbelot (2019), and  apply (variations of) RSA to investigate the relations between different model components, and then to study the alignment of these components with brain response.', 'Cosine similarity was used in Mitchell et al. (2008) to select between the candidate images in pairwise matching accuracy, as well as in percentile rank and RSA, but the raw cosine similarities between predicted and real images or embeddings can also be used as a metric. Minnema and Herbelot (2019) use this metric to quantify how close the predicted word vectors are to the target. Finally, Zou et al. (2022) use precision@k, a standard metric in other mapping problems, e.g., cross-lingual word embeddings (Søgaard et al., 2019).', 'Comparisons Most metrics are used to evaluate both encoding and decoding models (pairwise matching accuracy, Pearson correlation, percentile rank, MSE, RSA, cosine distance). Results for two of the most widely used metrics -pairwise matching accuracy 5 and percentile rank -tend to be around 0.7-0.8 with generally better results for more recent architectures and larger LMs. To draw conclusions across studies relying on different metrics, we need to investigate which metrics are more conservative, and how different metrics relate.', 'Pairwise matching accuracy vs. Pearson correlation It seems that pairwise matching accuracy tends to increase monotonically with Pearson correlation. Consider three sets of distances over corresponding point sets, A, B, and C. If A and B are more strongly linearly correlated than A and C, under an optimal linear mapping Ω (minimizing point-wise squared error distance), E[(a − bΩ) 2 ] > E[(a − cΩ) 2 ]. Even in this conservative setting in our synthetic experiments in Appendix A.1, the correlation between matching accuracy and percentile rank was very high,~0.9.', 'Pairwise matching accuracy vs. percentile rank Both metrics have random baseline scores of 0.5, and they will converge in the limit. If a has a percentile rank of p in a list A, it will be higher than a random member of A p percent of the time. In our experiments in Appendix A.1, the correlation converges toward 1.0, with values consistently higher than 0.8 for N = 100.', 'Pairwise matching accuracy vs. precision@k are also positively correlated. Perfect score in one entails perfect score in the other, but precision@k can of course be very small for very high values of pairwise matching accuracy (especially if the set of candidate words is big). Conversely, we can have 5 When discriminating averages over 20 images (Wehbe et al., 2014b), scores are naturally lower. saturation for high values of k, because matching accuracies higher than n−k n will mean near-perfect precision@k scores. In practice, precision@k (for low values of k) will be much more conservative, however. The correlation coefficient for N = 100 (see Appendix A.1) tends to lie around 0.7.', 'Relative strength Pairwise Matching Accuracy is a relatively permissible performance metric. To see this, consider the scenario in which all target words can be divided into two equal-sized buckets based on word length (number of characters). Say the neural responses capture nothing but this binary distinction between long and short words, but do so perfectly. Moreover, our mapping method, e.g., linear regression, learns this from training data. Now, from this alone, the pairwise matching accuracy will converge toward µ = 0.75, since our model will do perfectly (1.0) on half of the data, and exhibit random performance (0.5) on the other half. If the neural responses tracked word length (and not just the distinction between short and long words), performance would be even better. In other words, Pairwise Matching Accuracy scores around 0.7-0.8 (observed in the studies above) may only reflect very shallow processing characteristics. The fact that Minnema and Herbelot (2019) only observed good results with this metric, led them to adopt a rather critical stance, for good reasons.', 'Other metrics are clearly more conservative. For a set of n candidate words, a random mapping will induce a precision@1-score of 1 n . While hubs may inflate scores for larger values, the metric is extremely conservative for small values of k. However, only Zou et al. (2022) use this metric, and they modify the experimental protocol substantially, making the task much easier by providing additional input to a non-linear model. The small improvement from adding neural response input is interesting, but could potentially be explained by shallow processing characteristics.', 'They argue that analogy testing would provide a better evaluation protocol: one would ideally use standard metrics such as semantic relatedness judgment tasks, analogy tasks, etc.', ""[but] this is not possible due to the limited vocabulary sizes of the available brain datasets Such evaluation is possible on small scale, though, and increasingly larger fMRI datasets are becoming available (see above). Zhang et al. (2020) have identified analogical reasoning in fMRI brain activation spaces. The analogies are computed using vector offset and probe the systematicity of how semantic relations are encoded. If a model encodes the capital-of relation systematically, we can retrieve the capital of Germany by subtracting the fMRI vector for 'Paris' from the sum of our the fMRI vectors for Germany and France. This is the same kind of analogical reasoning found in language models (Mikolov et al., 2013). Garneau et al. (2021) show that the more language models satisfy analogies, the more isomorphic they are."", 'So far, it seems that, with the possible exception of Zhang et al. (2020), there is little evidence for structural similarities, beyond what could be induced by shallow processing characteristics, but what about all the studies that report strong Pearson correlations? Per-voxel correlation coefficients are low on average, but across the above studies, typically only around 4-40% of the voxels exhibit significant correlations (Huth et al., 2016;. Since these correlations have been replicated across different datasets, they are generally not disputed, but could still reflect rather shallow processing characteristics.', 'On a more positive note, several studies show that larger (and better) language models align better with neural response measurements (Schrimpf et al., 2021;. This suggests that language models in the future may align even better with such measurements, possibly reflecting properties of deep processing. Such correlations with model quality and size are positive, making the results reported above more credible.', 'Generally, the conclusions we can draw from the above studies are somewhat vague. There are two reasons for this: (i) Past studies have relied on permissible (pairwise matching accuracy) and ambiguous (Pearson correlation) performance metrics; and (ii) past studies have relied on small-sized datasets. We believe that this calls for a meta-analysis of the above studies. To provide grounds for such a meta-analysis, we have in this section taken steps to compare the metrics used in these studies. We leave it for future work to explore various ways effect sizes can be computed across these studies.']","We present the evaluation metrics used in the above studies and discuss how they relate. See Table 2 for a summary of metrics and corresponding studies. Mitchell et al. (2008) introduce pairwise matching accuracy. Because of their small sample size, they use a leave-2-out cross-validation, which later work also adopted. The metric is a binary classification accuracy metric on a balanced dataset, so a random baseline converges toward 0.5. Many studies have relied on this metric, both in encoding and decoding (see Table 2). 4 Pearson correlationPearson correlation is another widely used metric in the studies surveyed above, measuring the linear relationship between variables, and providing insight into the strength and direction of their association. Huth et al. (2016), compute Pearson correlation between predicted and actual brain responses using Gaussian random vectors to test statistical significance. Resulting p-values are corrected for multiple comparisons within each subject using false discovery rate (FDR) (Benjamini and Hochberg, 1995). Others have used Bonferroni correction (Huth et al., 2016) or block-wise permutation test (Adolf et al., 2014) to evaluate the statistical significance of the correlation (Zhang et al., 2020). Some report R 2 (explained variance) instead of or in addition to correlation coefficients (Minnema and Herbelot, 2019;Reddy and Wehbe, 2021). Others have adopted a more elaborate extension of Pearson correlation, namely BrainScore (Schrimpf et al., 2018). Brain-Score is estimated on held-out test data, calculating Pearson's correlation between model predictions and neural recordings divided by the estimated ceiling and averaged across voxels and participants.

Percentile rank was first used for encoding (Mitchell et al., 2008), but can also be used for decoding (Pereira et al., 2018;Gauthier and Levy, 2019;Minnema and Herbelot, 2019). In encoding, the predicted brain image for w is ranked along the predicted images for a set of candidate words w ′ by their similarity to the real (ground truth) image for w. The average rank is then reported. For decoding, they rank word vectors rather than neural response images. Note the similarity metric is unspecified, but typically cosine distance is used.

Mean squared error, the average of the squared differences between word vectors and neural responses, was first used for encoding in Søgaard (2016) on a held-out test split. It was also used by Gauthier and Levy (2019).

Representational similarity analysis (RSA) was introduced in Kriegeskorte et al. (2008) as a non-parametric way to characterize structural alignment between the geometries of representations derived from disparate modalities. RSA abstracts away from activity patterns themselves and instead computes representational similarity matrices (RSMs), which characterize the information carried by a given representation method through global similarity structure. A rank correlation coefficient is computed between RSMs derived from the two spaces, providing a summary statistic indicative of the overall representational alignment between them. Being non-parametric, RSA circumvents many of the various methodological weaknesses (such as over fitting, etc.). Gauthier and Levy (2019), Minnema and Herbelot (2019), and  apply (variations of) RSA to investigate the relations between different model components, and then to study the alignment of these components with brain response.

Cosine similarity was used in Mitchell et al. (2008) to select between the candidate images in pairwise matching accuracy, as well as in percentile rank and RSA, but the raw cosine similarities between predicted and real images or embeddings can also be used as a metric. Minnema and Herbelot (2019) use this metric to quantify how close the predicted word vectors are to the target. Finally, Zou et al. (2022) use precision@k, a standard metric in other mapping problems, e.g., cross-lingual word embeddings (Søgaard et al., 2019).

Comparisons Most metrics are used to evaluate both encoding and decoding models (pairwise matching accuracy, Pearson correlation, percentile rank, MSE, RSA, cosine distance). Results for two of the most widely used metrics -pairwise matching accuracy 5 and percentile rank -tend to be around 0.7-0.8 with generally better results for more recent architectures and larger LMs. To draw conclusions across studies relying on different metrics, we need to investigate which metrics are more conservative, and how different metrics relate.

Pairwise matching accuracy vs. Pearson correlation It seems that pairwise matching accuracy tends to increase monotonically with Pearson correlation. Consider three sets of distances over corresponding point sets, A, B, and C. If A and B are more strongly linearly correlated than A and C, under an optimal linear mapping Ω (minimizing point-wise squared error distance), E[(a − bΩ) 2 ] > E[(a − cΩ) 2 ]. Even in this conservative setting in our synthetic experiments in Appendix A.1, the correlation between matching accuracy and percentile rank was very high,~0.9.

Pairwise matching accuracy vs. percentile rank Both metrics have random baseline scores of 0.5, and they will converge in the limit. If a has a percentile rank of p in a list A, it will be higher than a random member of A p percent of the time. In our experiments in Appendix A.1, the correlation converges toward 1.0, with values consistently higher than 0.8 for N = 100.

Pairwise matching accuracy vs. precision@k are also positively correlated. Perfect score in one entails perfect score in the other, but precision@k can of course be very small for very high values of pairwise matching accuracy (especially if the set of candidate words is big). Conversely, we can have 5 When discriminating averages over 20 images (Wehbe et al., 2014b), scores are naturally lower. saturation for high values of k, because matching accuracies higher than n−k n will mean near-perfect precision@k scores. In practice, precision@k (for low values of k) will be much more conservative, however. The correlation coefficient for N = 100 (see Appendix A.1) tends to lie around 0.7.

Relative strength Pairwise Matching Accuracy is a relatively permissible performance metric. To see this, consider the scenario in which all target words can be divided into two equal-sized buckets based on word length (number of characters). Say the neural responses capture nothing but this binary distinction between long and short words, but do so perfectly. Moreover, our mapping method, e.g., linear regression, learns this from training data. Now, from this alone, the pairwise matching accuracy will converge toward µ = 0.75, since our model will do perfectly (1.0) on half of the data, and exhibit random performance (0.5) on the other half. If the neural responses tracked word length (and not just the distinction between short and long words), performance would be even better. In other words, Pairwise Matching Accuracy scores around 0.7-0.8 (observed in the studies above) may only reflect very shallow processing characteristics. The fact that Minnema and Herbelot (2019) only observed good results with this metric, led them to adopt a rather critical stance, for good reasons.

Other metrics are clearly more conservative. For a set of n candidate words, a random mapping will induce a precision@1-score of 1 n . While hubs may inflate scores for larger values, the metric is extremely conservative for small values of k. However, only Zou et al. (2022) use this metric, and they modify the experimental protocol substantially, making the task much easier by providing additional input to a non-linear model. The small improvement from adding neural response input is interesting, but could potentially be explained by shallow processing characteristics.

They argue that analogy testing would provide a better evaluation protocol: one would ideally use standard metrics such as semantic relatedness judgment tasks, analogy tasks, etc.

[but] this is not possible due to the limited vocabulary sizes of the available brain datasets Such evaluation is possible on small scale, though, and increasingly larger fMRI datasets are becoming available (see above). Zhang et al. (2020) have identified analogical reasoning in fMRI brain activation spaces. The analogies are computed using vector offset and probe the systematicity of how semantic relations are encoded. If a model encodes the capital-of relation systematically, we can retrieve the capital of Germany by subtracting the fMRI vector for 'Paris' from the sum of our the fMRI vectors for Germany and France. This is the same kind of analogical reasoning found in language models (Mikolov et al., 2013). Garneau et al. (2021) show that the more language models satisfy analogies, the more isomorphic they are.

So far, it seems that, with the possible exception of Zhang et al. (2020), there is little evidence for structural similarities, beyond what could be induced by shallow processing characteristics, but what about all the studies that report strong Pearson correlations? Per-voxel correlation coefficients are low on average, but across the above studies, typically only around 4-40% of the voxels exhibit significant correlations (Huth et al., 2016;. Since these correlations have been replicated across different datasets, they are generally not disputed, but could still reflect rather shallow processing characteristics.

On a more positive note, several studies show that larger (and better) language models align better with neural response measurements (Schrimpf et al., 2021;. This suggests that language models in the future may align even better with such measurements, possibly reflecting properties of deep processing. Such correlations with model quality and size are positive, making the results reported above more credible.

Generally, the conclusions we can draw from the above studies are somewhat vague. There are two reasons for this: (i) Past studies have relied on permissible (pairwise matching accuracy) and ambiguous (Pearson correlation) performance metrics; and (ii) past studies have relied on small-sized datasets. We believe that this calls for a meta-analysis of the above studies. To provide grounds for such a meta-analysis, we have in this section taken steps to compare the metrics used in these studies. We leave it for future work to explore various ways effect sizes can be computed across these studies.","(p3.0) We present the evaluation metrics used in the above studies and discuss how they relate. See Table 2 for a summary of metrics and corresponding studies. Mitchell et al. (2008) introduce pairwise matching accuracy. Because of their small sample size, they use a leave-2-out cross-validation, which later work also adopted. The metric is a binary classification accuracy metric on a balanced dataset, so a random baseline converges toward 0.5. Many studies have relied on this metric, both in encoding and decoding (see Table 2). 4 Pearson correlationPearson correlation is another widely used metric in the studies surveyed above, measuring the linear relationship between variables, and providing insight into the strength and direction of their association. Huth et al. (2016), compute Pearson correlation between predicted and actual brain responses using Gaussian random vectors to test statistical significance. Resulting p-values are corrected for multiple comparisons within each subject using false discovery rate (FDR) (Benjamini and Hochberg, 1995). Others have used Bonferroni correction (Huth et al., 2016) or block-wise permutation test (Adolf et al., 2014) to evaluate the statistical significance of the correlation (Zhang et al., 2020). Some report R 2 (explained variance) instead of or in addition to correlation coefficients (Minnema and Herbelot, 2019;Reddy and Wehbe, 2021). Others have adopted a more elaborate extension of Pearson correlation, namely BrainScore (Schrimpf et al., 2018). Brain-Score is estimated on held-out test data, calculating Pearson's correlation between model predictions and neural recordings divided by the estimated ceiling and averaged across voxels and participants.

(p3.1) Percentile rank was first used for encoding (Mitchell et al., 2008), but can also be used for decoding (Pereira et al., 2018;Gauthier and Levy, 2019;Minnema and Herbelot, 2019). In encoding, the predicted brain image for w is ranked along the predicted images for a set of candidate words w ′ by their similarity to the real (ground truth) image for w. The average rank is then reported. For decoding, they rank word vectors rather than neural response images. Note the similarity metric is unspecified, but typically cosine distance is used.

(p3.2) Mean squared error, the average of the squared differences between word vectors and neural responses, was first used for encoding in Søgaard (2016) on a held-out test split. It was also used by Gauthier and Levy (2019).

(p3.3) Representational similarity analysis (RSA) was introduced in Kriegeskorte et al. (2008) as a non-parametric way to characterize structural alignment between the geometries of representations derived from disparate modalities. RSA abstracts away from activity patterns themselves and instead computes representational similarity matrices (RSMs), which characterize the information carried by a given representation method through global similarity structure. A rank correlation coefficient is computed between RSMs derived from the two spaces, providing a summary statistic indicative of the overall representational alignment between them. Being non-parametric, RSA circumvents many of the various methodological weaknesses (such as over fitting, etc.). Gauthier and Levy (2019), Minnema and Herbelot (2019), and  apply (variations of) RSA to investigate the relations between different model components, and then to study the alignment of these components with brain response.

(p3.4) Cosine similarity was used in Mitchell et al. (2008) to select between the candidate images in pairwise matching accuracy, as well as in percentile rank and RSA, but the raw cosine similarities between predicted and real images or embeddings can also be used as a metric. Minnema and Herbelot (2019) use this metric to quantify how close the predicted word vectors are to the target. Finally, Zou et al. (2022) use precision@k, a standard metric in other mapping problems, e.g., cross-lingual word embeddings (Søgaard et al., 2019).

(p3.5) Comparisons Most metrics are used to evaluate both encoding and decoding models (pairwise matching accuracy, Pearson correlation, percentile rank, MSE, RSA, cosine distance). Results for two of the most widely used metrics -pairwise matching accuracy 5 and percentile rank -tend to be around 0.7-0.8 with generally better results for more recent architectures and larger LMs. To draw conclusions across studies relying on different metrics, we need to investigate which metrics are more conservative, and how different metrics relate.

(p3.6) Pairwise matching accuracy vs. Pearson correlation It seems that pairwise matching accuracy tends to increase monotonically with Pearson correlation. Consider three sets of distances over corresponding point sets, A, B, and C. If A and B are more strongly linearly correlated than A and C, under an optimal linear mapping Ω (minimizing point-wise squared error distance), E[(a − bΩ) 2 ] > E[(a − cΩ) 2 ]. Even in this conservative setting in our synthetic experiments in Appendix A.1, the correlation between matching accuracy and percentile rank was very high,~0.9.

(p3.7) Pairwise matching accuracy vs. percentile rank Both metrics have random baseline scores of 0.5, and they will converge in the limit. If a has a percentile rank of p in a list A, it will be higher than a random member of A p percent of the time. In our experiments in Appendix A.1, the correlation converges toward 1.0, with values consistently higher than 0.8 for N = 100.

(p3.8) Pairwise matching accuracy vs. precision@k are also positively correlated. Perfect score in one entails perfect score in the other, but precision@k can of course be very small for very high values of pairwise matching accuracy (especially if the set of candidate words is big). Conversely, we can have 5 When discriminating averages over 20 images (Wehbe et al., 2014b), scores are naturally lower. saturation for high values of k, because matching accuracies higher than n−k n will mean near-perfect precision@k scores. In practice, precision@k (for low values of k) will be much more conservative, however. The correlation coefficient for N = 100 (see Appendix A.1) tends to lie around 0.7.

(p3.9) Relative strength Pairwise Matching Accuracy is a relatively permissible performance metric. To see this, consider the scenario in which all target words can be divided into two equal-sized buckets based on word length (number of characters). Say the neural responses capture nothing but this binary distinction between long and short words, but do so perfectly. Moreover, our mapping method, e.g., linear regression, learns this from training data. Now, from this alone, the pairwise matching accuracy will converge toward µ = 0.75, since our model will do perfectly (1.0) on half of the data, and exhibit random performance (0.5) on the other half. If the neural responses tracked word length (and not just the distinction between short and long words), performance would be even better. In other words, Pairwise Matching Accuracy scores around 0.7-0.8 (observed in the studies above) may only reflect very shallow processing characteristics. The fact that Minnema and Herbelot (2019) only observed good results with this metric, led them to adopt a rather critical stance, for good reasons.

(p3.10) Other metrics are clearly more conservative. For a set of n candidate words, a random mapping will induce a precision@1-score of 1 n . While hubs may inflate scores for larger values, the metric is extremely conservative for small values of k. However, only Zou et al. (2022) use this metric, and they modify the experimental protocol substantially, making the task much easier by providing additional input to a non-linear model. The small improvement from adding neural response input is interesting, but could potentially be explained by shallow processing characteristics.

(p3.11) They argue that analogy testing would provide a better evaluation protocol: one would ideally use standard metrics such as semantic relatedness judgment tasks, analogy tasks, etc.

(p3.12) [but] this is not possible due to the limited vocabulary sizes of the available brain datasets Such evaluation is possible on small scale, though, and increasingly larger fMRI datasets are becoming available (see above). Zhang et al. (2020) have identified analogical reasoning in fMRI brain activation spaces. The analogies are computed using vector offset and probe the systematicity of how semantic relations are encoded. If a model encodes the capital-of relation systematically, we can retrieve the capital of Germany by subtracting the fMRI vector for 'Paris' from the sum of our the fMRI vectors for Germany and France. This is the same kind of analogical reasoning found in language models (Mikolov et al., 2013). Garneau et al. (2021) show that the more language models satisfy analogies, the more isomorphic they are.

(p3.13) So far, it seems that, with the possible exception of Zhang et al. (2020), there is little evidence for structural similarities, beyond what could be induced by shallow processing characteristics, but what about all the studies that report strong Pearson correlations? Per-voxel correlation coefficients are low on average, but across the above studies, typically only around 4-40% of the voxels exhibit significant correlations (Huth et al., 2016;. Since these correlations have been replicated across different datasets, they are generally not disputed, but could still reflect rather shallow processing characteristics.

(p3.14) On a more positive note, several studies show that larger (and better) language models align better with neural response measurements (Schrimpf et al., 2021;. This suggests that language models in the future may align even better with such measurements, possibly reflecting properties of deep processing. Such correlations with model quality and size are positive, making the results reported above more credible.

(p3.15) Generally, the conclusions we can draw from the above studies are somewhat vague. There are two reasons for this: (i) Past studies have relied on permissible (pairwise matching accuracy) and ambiguous (Pearson correlation) performance metrics; and (ii) past studies have relied on small-sized datasets. We believe that this calls for a meta-analysis of the above studies. To provide grounds for such a meta-analysis, we have in this section taken steps to compare the metrics used in these studies. We leave it for future work to explore various ways effect sizes can be computed across these studies.","[['b53', 'b57', 'b69', 'b40', None, 'b38', 'b27', 'b9', 'b2'], ['b20', 'b38', 'b40', 'b50'], ['b20'], ['b31', 'b20', 'b38'], ['b70', 'b38', 'b40', 'b60'], [], [], [], ['b68'], ['b38'], ['b70'], [], ['b69', 'b18', 'b35'], ['b69', 'b27'], ['b56'], []]","[['b53', 'b57', 'b69', 'b40', None, 'b38', 'b27', 'b9', 'b2'], ['b20', 'b38', 'b40', 'b50'], ['b20'], ['b31', 'b20', 'b38'], ['b70', 'b38', 'b40', 'b60'], [], [], [], ['b68'], ['b38'], ['b70'], [], ['b69', 'b18', 'b35'], ['b69', 'b27'], ['b56'], []]",30,"1. We present the evaluation metrics used in the above studies and discuss how they relate.
2. See Table 2 for a summary of metrics and corresponding studies.
3. Mitchell et al. (2008) introduce pairwise matching accuracy.
4. Because of their small sample size, they use a leave-2-out cross-validation, which later work also adopted.
5. The metric is a binary classification accuracy metric on a balanced dataset, so a random baseline converges toward 0.5.
6. Many studies have relied on this metric, both in encoding and decoding (see Table 2).
7. 4 Pearson correlationPearson correlation is another widely used metric in the studies surveyed above, measuring the linear relationship between variables, and providing insight into the strength and direction of their association.
8. Huth et al. (2016), compute Pearson correlation between predicted and actual brain responses using Gaussian random vectors to test statistical significance.
9. Resulting p-values are corrected for multiple comparisons within each subject using false discovery rate (FDR) (Benjamini and Hochberg, 1995).
10. Others have used Bonferroni correction (Huth et al., 2016) or block-wise permutation test (Adolf et al., 2014) to evaluate the statistical significance of the correlation (Zhang et al., 2020).
11. Some report R 2 (explained variance) instead of or in addition to correlation coefficients (Minnema and Herbelot, 2019;Reddy and Wehbe, 2021).
12. Others have adopted a more elaborate extension of Pearson correlation, namely BrainScore (Schrimpf et al., 2018).
13. Brain-Score is estimated on held-out test data, calculating Pearson's correlation between model predictions and neural recordings divided by the estimated ceiling and averaged across voxels and participants.
14. Percentile rank was first used for encoding (Mitchell et al., 2008), but can also be used for decoding (Pereira et al., 2018;Gauthier and Levy, 2019;Minnema and Herbelot, 2019).
15. In encoding, the predicted brain image for w is ranked along the predicted images for a set of candidate words w ′ by their similarity to the real (ground truth) image for w.
16. The average rank is then reported.
17. For decoding, they rank word vectors rather than neural response images.
18. Note the similarity metric is unspecified, but typically cosine distance is used.
19. Mean squared error, the average of the squared differences between word vectors and neural responses, was first used for encoding in Søgaard (2016) on a held-out test split.
20. It was also used by Gauthier and Levy (2019).
21. Representational similarity analysis (RSA) was introduced in Kriegeskorte et al. (2008) as a non-parametric way to characterize structural alignment between the geometries of representations derived from disparate modalities.
22. RSA abstracts away from activity patterns themselves and instead computes representational similarity matrices (RSMs), which characterize the information carried by a given representation method through global similarity structure.
23. A rank correlation coefficient is computed between RSMs derived from the two spaces, providing a summary statistic indicative of the overall representational alignment between them.
24. Being non-parametric, RSA circumvents many of the various methodological weaknesses (such as over fitting, etc.).
25. Gauthier and Levy (2019), Minnema and Herbelot (2019), and  apply (variations of) RSA to investigate the relations between different model components, and then to study the alignment of these components with brain response.
26. Cosine similarity was used in Mitchell et al. (2008) to select between the candidate images in pairwise matching accuracy, as well as in percentile rank and RSA, but the raw cosine similarities between predicted and real images or embeddings can also be used as a metric.
27. Minnema and Herbelot (2019) use this metric to quantify how close the predicted word vectors are to the target.
28. Finally, Zou et al. (2022) use precision@k, a standard metric in other mapping problems, e.g., cross-lingual word embeddings (Søgaard et al., 2019).
29. Comparisons Most metrics are used to evaluate both encoding and decoding models (pairwise matching accuracy, Pearson correlation, percentile rank, MSE, RSA, cosine distance).
30. Results for two of the most widely used metrics -pairwise matching accuracy 5 and percentile rank -tend to be around 0.7-0.8 with generally better results for more recent architectures and larger LMs.
31. To draw conclusions across studies relying on different metrics, we need to investigate which metrics are more conservative, and how different metrics relate.
32. Pairwise matching accuracy vs. Pearson correlation It seems that pairwise matching accuracy tends to increase monotonically with Pearson correlation.
33. Consider three sets of distances over corresponding point sets, A, B, and C. If A and B are more strongly linearly correlated than A and C, under an optimal linear mapping Ω (minimizing point-wise squared error distance), E[(a − bΩ) 2 ] > E[(a − cΩ) 2 ].
34. Even in this conservative setting in our synthetic experiments in Appendix A.1, the correlation between matching accuracy and percentile rank was very high,~0.9.
35. Pairwise matching accuracy vs. percentile rank Both metrics have random baseline scores of 0.5, and they will converge in the limit.
36. If a has a percentile rank of p in a list A, it will be higher than a random member of A p percent of the time.
37. In our experiments in Appendix A.1, the correlation converges toward 1.0, with values consistently higher than 0.8 for N = 100.
38. Pairwise matching accuracy vs. precision@k are also positively correlated.
39. Perfect score in one entails perfect score in the other, but precision@k can of course be very small for very high values of pairwise matching accuracy (especially if the set of candidate words is big).
40. Conversely, we can have 5 When discriminating averages over 20 images (Wehbe et al., 2014b), scores are naturally lower.
41. saturation for high values of k, because matching accuracies higher than n−k n will mean near-perfect precision@k scores.
42. In practice, precision@k (for low values of k) will be much more conservative, however.
43. The correlation coefficient for N = 100 (see Appendix A.1) tends to lie around 0.7.
44. Relative strength Pairwise Matching Accuracy is a relatively permissible performance metric.
45. To see this, consider the scenario in which all target words can be divided into two equal-sized buckets based on word length (number of characters).
46. Say the neural responses capture nothing but this binary distinction between long and short words, but do so perfectly.
47. Moreover, our mapping method, e.g., linear regression, learns this from training data.
48. Now, from this alone, the pairwise matching accuracy will converge toward µ = 0.75, since our model will do perfectly (1.0) on half of the data, and exhibit random performance (0.5) on the other half.
49. If the neural responses tracked word length (and not just the distinction between short and long words), performance would be even better.
50. In other words, Pairwise Matching Accuracy scores around 0.7-0.8 (observed in the studies above) may only reflect very shallow processing characteristics.
51. The fact that Minnema and Herbelot (2019) only observed good results with this metric, led them to adopt a rather critical stance, for good reasons.
52. Other metrics are clearly more conservative.
53. For a set of n candidate words, a random mapping will induce a precision@1-score of 1 n .
54. While hubs may inflate scores for larger values, the metric is extremely conservative for small values of k.
55. However, only Zou et al. (2022) use this metric, and they modify the experimental protocol substantially, making the task much easier by providing additional input to a non-linear model.
56. The small improvement from adding neural response input is interesting, but could potentially be explained by shallow processing characteristics.
57. They argue that analogy testing would provide a better evaluation protocol: one would ideally use standard metrics such as semantic relatedness judgment tasks, analogy tasks, etc.
58. [but] this is not possible due to the limited vocabulary sizes of the available brain datasets Such evaluation is possible on small scale, though, and increasingly larger fMRI datasets are becoming available (see above).
59. Zhang et al. (2020) have identified analogical reasoning in fMRI brain activation spaces.
60. The analogies are computed using vector offset and probe the systematicity of how semantic relations are encoded.
61. If a model encodes the capital-of relation systematically, we can retrieve the capital of Germany by subtracting the fMRI vector for 'Paris' from the sum of our the fMRI vectors for Germany and France.
62. This is the same kind of analogical reasoning found in language models (Mikolov et al., 2013).
63. Garneau et al. (2021) show that the more language models satisfy analogies, the more isomorphic they are.
64. So far, it seems that, with the possible exception of Zhang et al. (2020), there is little evidence for structural similarities, beyond what could be induced by shallow processing characteristics, but what about all the studies that report strong Pearson correlations?
65. Per-voxel correlation coefficients are low on average, but across the above studies, typically only around 4-40% of the voxels exhibit significant correlations (Huth et al., 2016;.
66. Since these correlations have been replicated across different datasets, they are generally not disputed, but could still reflect rather shallow processing characteristics.
67. On a more positive note, several studies show that larger (and better) language models align better with neural response measurements (Schrimpf et al., 2021;.
68. This suggests that language models in the future may align even better with such measurements, possibly reflecting properties of deep processing.
69. Such correlations with model quality and size are positive, making the results reported above more credible.
70. Generally, the conclusions we can draw from the above studies are somewhat vague.
71. There are two reasons for this: (i) Past studies have relied on permissible (pairwise matching accuracy) and ambiguous (Pearson correlation) performance metrics; and (ii) past studies have relied on small-sized datasets.
72. We believe that this calls for a meta-analysis of the above studies.
73. To provide grounds for such a meta-analysis, we have in this section taken steps to compare the metrics used in these studies.
74. We leave it for future work to explore various ways effect sizes can be computed across these studies."
264451714,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18d18d4ffdc070868ce06f216a2a8d040d42a4cb,s11,Factors Affecting Knowledge Retention,"['p11.0', 'p11.1', 'p11.2', 'p11.3']","['PLMs are diverse with respect to their architectures, pre-training objectives and their pre-training data.A compelling question is: how do all these factors affect knowledge retention in PLMs?', ""Large language models are known to perform generally better and hold more knowledge (Brown et al., 2020;Roberts et al., 2020).However, the model's architecture and pre-training objectives are more decisive for knowledge retention than its size (Li et al., 2022a).For example, pre-training with the Salient Span Masking objective (Guu et al., 2020) helps PLMs to absorb more facts (Roberts et al., 2020;Cole et al., 2023).Similarly, Xiong et al. (2020) demonstrate that training the model to predict if the original entities in the text have been replaced with other entities is beneficial for fact retrieval.More generally, Ye et al. (2021) conclude that a masking strategy matching the downstream task, positively affects the performance on that task."", 'A larger pre-training corpus with an encoderonly model (Liu et al., 2020) leads to higher knowledge retention (Zhang et al., 2021), but with an encoder-decoder model (Lewis et al., 2020), a larger corpus negatively affects knowledge retention Wang et al. (2021a).Recency (Chiang et al., 2020) and frequency (Kandpal et al., 2023), i.e., when and how often the data is observed at training, are also essential for knowledge retention.', 'Larger models and more pre-training data can improve knowledge retention if combined with the right choices for architecture and pre-training objective(s).However, scaling might not be sufficient (Kandpal et al., 2023).Even though many works propose new architectures and pre-training objectives to increase factual knowledge retention in PLMs and their robustness to prompts (Févry et al., 2020;Hosseini et al., 2021;Sadeq et al., 2022;Whitehouse et al., 2022;Min et al., 2023;Zhong et al., 2023), this is a promising future work direction, as there is more room for improvement.']","PLMs are diverse with respect to their architectures, pre-training objectives and their pre-training data.A compelling question is: how do all these factors affect knowledge retention in PLMs?

Large language models are known to perform generally better and hold more knowledge (Brown et al., 2020;Roberts et al., 2020).However, the model's architecture and pre-training objectives are more decisive for knowledge retention than its size (Li et al., 2022a).For example, pre-training with the Salient Span Masking objective (Guu et al., 2020) helps PLMs to absorb more facts (Roberts et al., 2020;Cole et al., 2023).Similarly, Xiong et al. (2020) demonstrate that training the model to predict if the original entities in the text have been replaced with other entities is beneficial for fact retrieval.More generally, Ye et al. (2021) conclude that a masking strategy matching the downstream task, positively affects the performance on that task.

A larger pre-training corpus with an encoderonly model (Liu et al., 2020) leads to higher knowledge retention (Zhang et al., 2021), but with an encoder-decoder model (Lewis et al., 2020), a larger corpus negatively affects knowledge retention Wang et al. (2021a).Recency (Chiang et al., 2020) and frequency (Kandpal et al., 2023), i.e., when and how often the data is observed at training, are also essential for knowledge retention.

Larger models and more pre-training data can improve knowledge retention if combined with the right choices for architecture and pre-training objective(s).However, scaling might not be sufficient (Kandpal et al., 2023).Even though many works propose new architectures and pre-training objectives to increase factual knowledge retention in PLMs and their robustness to prompts (Févry et al., 2020;Hosseini et al., 2021;Sadeq et al., 2022;Whitehouse et al., 2022;Min et al., 2023;Zhong et al., 2023), this is a promising future work direction, as there is more room for improvement.","(p11.0) PLMs are diverse with respect to their architectures, pre-training objectives and their pre-training data.A compelling question is: how do all these factors affect knowledge retention in PLMs?

(p11.1) Large language models are known to perform generally better and hold more knowledge (Brown et al., 2020;Roberts et al., 2020).However, the model's architecture and pre-training objectives are more decisive for knowledge retention than its size (Li et al., 2022a).For example, pre-training with the Salient Span Masking objective (Guu et al., 2020) helps PLMs to absorb more facts (Roberts et al., 2020;Cole et al., 2023).Similarly, Xiong et al. (2020) demonstrate that training the model to predict if the original entities in the text have been replaced with other entities is beneficial for fact retrieval.More generally, Ye et al. (2021) conclude that a masking strategy matching the downstream task, positively affects the performance on that task.

(p11.2) A larger pre-training corpus with an encoderonly model (Liu et al., 2020) leads to higher knowledge retention (Zhang et al., 2021), but with an encoder-decoder model (Lewis et al., 2020), a larger corpus negatively affects knowledge retention Wang et al. (2021a).Recency (Chiang et al., 2020) and frequency (Kandpal et al., 2023), i.e., when and how often the data is observed at training, are also essential for knowledge retention.

(p11.3) Larger models and more pre-training data can improve knowledge retention if combined with the right choices for architecture and pre-training objective(s).However, scaling might not be sufficient (Kandpal et al., 2023).Even though many works propose new architectures and pre-training objectives to increase factual knowledge retention in PLMs and their robustness to prompts (Févry et al., 2020;Hosseini et al., 2021;Sadeq et al., 2022;Whitehouse et al., 2022;Min et al., 2023;Zhong et al., 2023), this is a promising future work direction, as there is more room for improvement.","[[], ['b89', 'b28', 'b15', 'b107', 'b7', 'b109', 'b59'], ['b57', 'b49', None, 'b12', 'b64', 'b103'], ['b49', 'b73', 'b91', 'b24', 'b35', 'b106', 'b116']]","[[], ['b89', 'b28', 'b15', 'b107', 'b7', 'b109', 'b59'], ['b57', 'b49', None, 'b12', 'b64', 'b103'], ['b49', 'b73', 'b91', 'b24', 'b35', 'b106', 'b116']]",20,"1. PLMs are diverse with respect to their architectures, pre-training objectives and their pre-training data.
2. A compelling question is: how do all these factors affect knowledge retention in PLMs?
3. Large language models are known to perform generally better and hold more knowledge (Brown et al., 2020;Roberts et al., 2020).However, the model's architecture and pre-training objectives are more decisive for knowledge retention than its size (Li et al., 2022a).For example, pre-training with the Salient Span Masking objective (Guu et al., 2020) helps PLMs to absorb more facts (Roberts et al., 2020;Cole et al., 2023).Similarly, Xiong et al. (2020) demonstrate that training the model to predict if the original entities in the text have been replaced with other entities is beneficial for fact retrieval.
4. More generally, Ye et al. (2021) conclude that a masking strategy matching the downstream task, positively affects the performance on that task.
5. A larger pre-training corpus with an encoderonly model (Liu et al., 2020) leads to higher knowledge retention (Zhang et al., 2021), but with an encoder-decoder model (Lewis et al., 2020), a larger corpus negatively affects knowledge retention Wang et al. (2021a).Recency (Chiang et al., 2020) and frequency (Kandpal et al., 2023), i.e., when and how often the data is observed at training, are also essential for knowledge retention.
6. Larger models and more pre-training data can improve knowledge retention if combined with the right choices for architecture and pre-training objective(s).However, scaling might not be sufficient (Kandpal et al., 2023).Even though many works propose new architectures and pre-training objectives to increase factual knowledge retention in PLMs and their robustness to prompts (Févry et al., 2020;Hosseini et al., 2021;Sadeq et al., 2022;Whitehouse et al., 2022;Min et al., 2023;Zhong et al., 2023), this is a promising future work direction, as there is more room for improvement."
264451714,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18d18d4ffdc070868ce06f216a2a8d040d42a4cb,s9,Datasets for Factual Probing,"['p9.0', 'p9.1', 'p9.2', 'p9.3']","['We found a variety of datasets (44 in our corpus) that have been proposed or used for probing factual knowledge in PLMs: 18 datasets for probing general knowledge, 8 for domain-specific knowledge and 18 datasets that target other aspects, e.g, consistency of PLMs (cf.Table 2).', 'Datasets for general knowledge probing are used to quantify generic factual knowledge in PLMs with the most prominent being LAMA (Petroni et al., 2019).WIKI-UNI (Cao et al., 2021) is similar to LAMA, but with a uniform distribution of object entities.LAMA-UHN (Poerner et al., 2020) is a subset of LAMA without easy-to-guess examples.DLAMA (Keleg and Magdy, 2023) targets culturally diverse facts.While 16 datasets are solely English, there are three multilingual datasets (mLAMA (Kassner et al., 2021), X-FACTR (Jiang et al., 2020a) and DLAMA (Keleg and Magdy, 2023)).In-dicGLUE (Kakwani et al., 2020) contains 11 Indic languages.Most datasets consist of cloze prompts, while QA datasets (WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019)), PopQA and Enti-tyQuestions (Mallen et al., 2023) are also used to quantify factual knowledge (Roberts et al., 2020).Wang et al. (2021a) adapt SQuAD (Rajpurkar et al., 2018) for closed-book question answering.', '6 out of 8 datasets used for probing domainspecific knowledge target the biomedical domain (e.g., MedQA (Jin et al., 2021), Bio-LAMA (Sung et al., 2021) and MedLAMA (Meng et al., 2022b)).The multilingual dataset EX-AMS (Hardalov et al., 2020) focuses on scientific QA, whereas LEFT (Ciosici et al., 2021) contains questions from humanities and social sciences.', 'The community has constructed further datasets to investigate other aspects of using PLMs as knowledge bases.PARAREL (Elazar et al., 2021) and its multilingual counterpart mPARA-REL (Fierro and Søgaard, 2022)']","We found a variety of datasets (44 in our corpus) that have been proposed or used for probing factual knowledge in PLMs: 18 datasets for probing general knowledge, 8 for domain-specific knowledge and 18 datasets that target other aspects, e.g, consistency of PLMs (cf.Table 2).

Datasets for general knowledge probing are used to quantify generic factual knowledge in PLMs with the most prominent being LAMA (Petroni et al., 2019).WIKI-UNI (Cao et al., 2021) is similar to LAMA, but with a uniform distribution of object entities.LAMA-UHN (Poerner et al., 2020) is a subset of LAMA without easy-to-guess examples.DLAMA (Keleg and Magdy, 2023) targets culturally diverse facts.While 16 datasets are solely English, there are three multilingual datasets (mLAMA (Kassner et al., 2021), X-FACTR (Jiang et al., 2020a) and DLAMA (Keleg and Magdy, 2023)).In-dicGLUE (Kakwani et al., 2020) contains 11 Indic languages.Most datasets consist of cloze prompts, while QA datasets (WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019)), PopQA and Enti-tyQuestions (Mallen et al., 2023) are also used to quantify factual knowledge (Roberts et al., 2020).Wang et al. (2021a) adapt SQuAD (Rajpurkar et al., 2018) for closed-book question answering.

6 out of 8 datasets used for probing domainspecific knowledge target the biomedical domain (e.g., MedQA (Jin et al., 2021), Bio-LAMA (Sung et al., 2021) and MedLAMA (Meng et al., 2022b)).The multilingual dataset EX-AMS (Hardalov et al., 2020) focuses on scientific QA, whereas LEFT (Ciosici et al., 2021) contains questions from humanities and social sciences.

The community has constructed further datasets to investigate other aspects of using PLMs as knowledge bases.PARAREL (Elazar et al., 2021) and its multilingual counterpart mPARA-REL (Fierro and Søgaard, 2022)","(p9.0) We found a variety of datasets (44 in our corpus) that have been proposed or used for probing factual knowledge in PLMs: 18 datasets for probing general knowledge, 8 for domain-specific knowledge and 18 datasets that target other aspects, e.g, consistency of PLMs (cf.Table 2).

(p9.1) Datasets for general knowledge probing are used to quantify generic factual knowledge in PLMs with the most prominent being LAMA (Petroni et al., 2019).WIKI-UNI (Cao et al., 2021) is similar to LAMA, but with a uniform distribution of object entities.LAMA-UHN (Poerner et al., 2020) is a subset of LAMA without easy-to-guess examples.DLAMA (Keleg and Magdy, 2023) targets culturally diverse facts.While 16 datasets are solely English, there are three multilingual datasets (mLAMA (Kassner et al., 2021), X-FACTR (Jiang et al., 2020a) and DLAMA (Keleg and Magdy, 2023)).In-dicGLUE (Kakwani et al., 2020) contains 11 Indic languages.Most datasets consist of cloze prompts, while QA datasets (WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019)), PopQA and Enti-tyQuestions (Mallen et al., 2023) are also used to quantify factual knowledge (Roberts et al., 2020).Wang et al. (2021a) adapt SQuAD (Rajpurkar et al., 2018) for closed-book question answering.

(p9.2) 6 out of 8 datasets used for probing domainspecific knowledge target the biomedical domain (e.g., MedQA (Jin et al., 2021), Bio-LAMA (Sung et al., 2021) and MedLAMA (Meng et al., 2022b)).The multilingual dataset EX-AMS (Hardalov et al., 2020) focuses on scientific QA, whereas LEFT (Ciosici et al., 2021) contains questions from humanities and social sciences.

(p9.3) The community has constructed further datasets to investigate other aspects of using PLMs as knowledge bases.PARAREL (Elazar et al., 2021) and its multilingual counterpart mPARA-REL (Fierro and Søgaard, 2022)","[[], ['b89', 'b50', 'b41', 'b82', 'b84', 'b54', 'b45', 'b10', 'b87', 'b103', 'b52', 'b67', 'b44', 'b5'], ['b30', 'b98', 'b14', 'b71', 'b43'], ['b23', 'b26']]","[[], ['b89', 'b50', 'b41', 'b82', 'b84', 'b54', 'b45', 'b10', 'b87', 'b103', 'b52', 'b67', 'b44', 'b5'], ['b30', 'b98', 'b14', 'b71', 'b43'], ['b23', 'b26']]",21,"1. We found a variety of datasets (44 in our corpus) that have been proposed or used for probing factual knowledge in PLMs: 18 datasets for probing general knowledge, 8 for domain-specific knowledge and 18 datasets that target other aspects, e.g, consistency of PLMs (cf.Table 2).Datasets for general knowledge probing are used to quantify generic factual knowledge in PLMs with the most prominent being LAMA (Petroni et al., 2019).WIKI-UNI (Cao et al., 2021) is similar to LAMA, but with a uniform distribution of object entities.
2. LAMA-UHN (Poerner et al., 2020) is a subset of LAMA without easy-to-guess examples.
3. DLAMA (Keleg and Magdy, 2023) targets culturally diverse facts.
4. While 16 datasets are solely English, there are three multilingual datasets (mLAMA (Kassner et al., 2021), X-FACTR (Jiang et al., 2020a) and DLAMA (Keleg and Magdy, 2023)).In-dicGLUE (Kakwani et al., 2020) contains 11 Indic languages.
5. Most datasets consist of cloze prompts, while QA datasets (WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019)), PopQA and Enti-tyQuestions (Mallen et al., 2023) are also used to quantify factual knowledge (Roberts et al., 2020).Wang et al. (2021a) adapt SQuAD (Rajpurkar et al., 2018) for closed-book question answering.
6. 6 out of 8 datasets used for probing domainspecific knowledge target the biomedical domain (e.g., MedQA (Jin et al., 2021), Bio-LAMA (Sung et al., 2021) and MedLAMA (Meng et al., 2022b)).The multilingual dataset EX-AMS (Hardalov et al., 2020) focuses on scientific QA, whereas LEFT (Ciosici et al., 2021) contains questions from humanities and social sciences.
7. The community has constructed further datasets to investigate other aspects of using PLMs as knowledge bases.
8. PARAREL (Elazar et al., 2021) and its multilingual counterpart mPARA-REL (Fierro and Søgaard, 2022)"
264451714,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18d18d4ffdc070868ce06f216a2a8d040d42a4cb,s13,Obstacles to Adopting PLMs as KBs,"['p13.0', 'p13.1', 'p13.2', 'p13.3', 'p13.4']","['Consistency.A challenge to relying on PLMs as knowledge bases is their sensitivity to the input queries (Fierro and Søgaard, 2022).PLMs rely on shallow surface features and lexical correlations (Kassner and Schütze, 2020;Misra et al., 2020;Poerner et al., 2020;Rogers et al., 2020;Li et al., 2022b), which explains their high sensitivity to the way queries are formulated.Current solutions (Elazar et al., 2021;Newman et al., 2022) train PLMs to be robust to variations in inputs, but further improvements are needed to make PLMs reliable knowledge bases.PLMs are known to be highly sensitive to prompts, especially in languages other than English (Fierro and Søgaard, 2022), where less resources are available.Making PLMs more robust to prompts in non-English languages is a promising future work direction.', 'Interpretability.Identifying where facts are stored and how they are retrieved is essential to adopt PLMs as trustworthy knowledge sources.Several approaches locate knowledge in PLMs (Wallat et al., 2020;Podkorytov et al., 2021;Alkhaldi et al., 2022;Dai et al., 2022;Meng et al., 2022a), with different conclusions depending on the architecture (e.g., knowledge is located in the middle layers of GPT-like models (Meng et al., 2022a), or in the upper layers in BERT-like models (Dai et al., 2022)).Another line of work focuses on the data aspect, showing the dependence of PLMs on word co-occurrences and positionally close words (Li et al., 2022b), or tracing back predictions to training data (Akyurek et al., 2022;Park et al., 2023).Knowing how PLMs retrieve facts remains challenging, but necessary to make PLMs transparent fact retrievers.The introduction of a fact tracing benchmark (Akyurek et al., 2022) opens the door for works in this direction.', 'Updating Knowledge.PLMs come with a fixed set of pre-trained parameters that encode knowledge about the world.As time passes, this knowledge becomes partially outdated.Hence, editing existing knowledge in PLMs and augmenting them with new knowledge is crucial for their use as knowledge bases (Zini and Awad, 2022).', 'One line of research locates the modules responsible for factual predictions and modifies these to update the corresponding facts (Dai et al., 2022;De Cao et al., 2021;Meng et al., 2022a).Other lines of research keep the original PLM unchanged, but augment it with additional parameters to induce the desired changes (Wang et al., 2021b;Lee et al., 2022), or encode facts with time stamps in PLMs to make them ""time-aware"" (Dhingra et al., 2022).', 'When updating facts in PLMs, it is crucial that only the targeted facts are affected and that these facts are retrievable using different paraphrases (De Cao et al., 2021;Hase et al., 2023).However, current methods for facts editing (Meng et al., 2022a(Meng et al., , 2023) ) still do not fulfill these requirements (Hoelscher-Obermaier et al., 2023).Methods that introduce additional parameters should be made more scalable (Jang et al., 2022b).']","Consistency.A challenge to relying on PLMs as knowledge bases is their sensitivity to the input queries (Fierro and Søgaard, 2022).PLMs rely on shallow surface features and lexical correlations (Kassner and Schütze, 2020;Misra et al., 2020;Poerner et al., 2020;Rogers et al., 2020;Li et al., 2022b), which explains their high sensitivity to the way queries are formulated.Current solutions (Elazar et al., 2021;Newman et al., 2022) train PLMs to be robust to variations in inputs, but further improvements are needed to make PLMs reliable knowledge bases.PLMs are known to be highly sensitive to prompts, especially in languages other than English (Fierro and Søgaard, 2022), where less resources are available.Making PLMs more robust to prompts in non-English languages is a promising future work direction.

Interpretability.Identifying where facts are stored and how they are retrieved is essential to adopt PLMs as trustworthy knowledge sources.Several approaches locate knowledge in PLMs (Wallat et al., 2020;Podkorytov et al., 2021;Alkhaldi et al., 2022;Dai et al., 2022;Meng et al., 2022a), with different conclusions depending on the architecture (e.g., knowledge is located in the middle layers of GPT-like models (Meng et al., 2022a), or in the upper layers in BERT-like models (Dai et al., 2022)).Another line of work focuses on the data aspect, showing the dependence of PLMs on word co-occurrences and positionally close words (Li et al., 2022b), or tracing back predictions to training data (Akyurek et al., 2022;Park et al., 2023).Knowing how PLMs retrieve facts remains challenging, but necessary to make PLMs transparent fact retrievers.The introduction of a fact tracing benchmark (Akyurek et al., 2022) opens the door for works in this direction.

Updating Knowledge.PLMs come with a fixed set of pre-trained parameters that encode knowledge about the world.As time passes, this knowledge becomes partially outdated.Hence, editing existing knowledge in PLMs and augmenting them with new knowledge is crucial for their use as knowledge bases (Zini and Awad, 2022).

One line of research locates the modules responsible for factual predictions and modifies these to update the corresponding facts (Dai et al., 2022;De Cao et al., 2021;Meng et al., 2022a).Other lines of research keep the original PLM unchanged, but augment it with additional parameters to induce the desired changes (Wang et al., 2021b;Lee et al., 2022), or encode facts with time stamps in PLMs to make them ""time-aware"" (Dhingra et al., 2022).

When updating facts in PLMs, it is crucial that only the targeted facts are affected and that these facts are retrievable using different paraphrases (De Cao et al., 2021;Hase et al., 2023).However, current methods for facts editing (Meng et al., 2022a(Meng et al., , 2023) ) still do not fulfill these requirements (Hoelscher-Obermaier et al., 2023).Methods that introduce additional parameters should be made more scalable (Jang et al., 2022b).","(p13.0) Consistency.A challenge to relying on PLMs as knowledge bases is their sensitivity to the input queries (Fierro and Søgaard, 2022).PLMs rely on shallow surface features and lexical correlations (Kassner and Schütze, 2020;Misra et al., 2020;Poerner et al., 2020;Rogers et al., 2020;Li et al., 2022b), which explains their high sensitivity to the way queries are formulated.Current solutions (Elazar et al., 2021;Newman et al., 2022) train PLMs to be robust to variations in inputs, but further improvements are needed to make PLMs reliable knowledge bases.PLMs are known to be highly sensitive to prompts, especially in languages other than English (Fierro and Søgaard, 2022), where less resources are available.Making PLMs more robust to prompts in non-English languages is a promising future work direction.

(p13.1) Interpretability.Identifying where facts are stored and how they are retrieved is essential to adopt PLMs as trustworthy knowledge sources.Several approaches locate knowledge in PLMs (Wallat et al., 2020;Podkorytov et al., 2021;Alkhaldi et al., 2022;Dai et al., 2022;Meng et al., 2022a), with different conclusions depending on the architecture (e.g., knowledge is located in the middle layers of GPT-like models (Meng et al., 2022a), or in the upper layers in BERT-like models (Dai et al., 2022)).Another line of work focuses on the data aspect, showing the dependence of PLMs on word co-occurrences and positionally close words (Li et al., 2022b), or tracing back predictions to training data (Akyurek et al., 2022;Park et al., 2023).Knowing how PLMs retrieve facts remains challenging, but necessary to make PLMs transparent fact retrievers.The introduction of a fact tracing benchmark (Akyurek et al., 2022) opens the door for works in this direction.

(p13.2) Updating Knowledge.PLMs come with a fixed set of pre-trained parameters that encode knowledge about the world.As time passes, this knowledge becomes partially outdated.Hence, editing existing knowledge in PLMs and augmenting them with new knowledge is crucial for their use as knowledge bases (Zini and Awad, 2022).

(p13.3) One line of research locates the modules responsible for factual predictions and modifies these to update the corresponding facts (Dai et al., 2022;De Cao et al., 2021;Meng et al., 2022a).Other lines of research keep the original PLM unchanged, but augment it with additional parameters to induce the desired changes (Wang et al., 2021b;Lee et al., 2022), or encode facts with time stamps in PLMs to make them ""time-aware"" (Dhingra et al., 2022).

(p13.4) When updating facts in PLMs, it is crucial that only the targeted facts are affected and that these facts are retrievable using different paraphrases (De Cao et al., 2021;Hase et al., 2023).However, current methods for facts editing (Meng et al., 2022a(Meng et al., , 2023) ) still do not fulfill these requirements (Hoelscher-Obermaier et al., 2023).Methods that introduce additional parameters should be made more scalable (Jang et al., 2022b).","[['b60', 'b84', 'b26', 'b90', 'b75', 'b74', 'b23', 'b51'], ['b3', 'b69', 'b60', 'b80', 'b16', 'b102', 'b83', 'b1'], ['b118'], ['b55', 'b20', 'b18', 'b69', 'b16', 'b104'], ['b18', 'b69', 'b34', 'b39', 'b31', 'b70']]","[['b60', 'b84', 'b26', 'b90', 'b75', 'b74', 'b23', 'b51'], ['b3', 'b69', 'b60', 'b80', 'b16', 'b102', 'b83', 'b1'], ['b118'], ['b55', 'b20', 'b18', 'b69', 'b16', 'b104'], ['b18', 'b69', 'b34', 'b39', 'b31', 'b70']]",29,"1. Consistency. A challenge to relying on PLMs as knowledge bases is their sensitivity to the input queries (Fierro and Søgaard, 2022).PLMs rely on shallow surface features and lexical correlations (Kassner and Schütze, 2020;Misra et al., 2020;Poerner et al., 2020;Rogers et al., 2020;Li et al., 2022b), which explains their high sensitivity to the way queries are formulated.
2. Current solutions (Elazar et al., 2021;Newman et al., 2022)
3. train PLMs to be robust to variations in inputs, but further improvements are needed to make PLMs reliable knowledge bases.
4. PLMs are known to be highly sensitive to prompts, especially in languages other than English (Fierro and Søgaard, 2022), where less resources are available.
5. Making PLMs more robust to prompts in non-English languages is a promising future work direction.
6. Interpretability. Identifying where facts are stored and how they are retrieved is essential to adopt PLMs as trustworthy knowledge sources.
7. Several approaches locate knowledge in PLMs (Wallat et al., 2020;Podkorytov et al., 2021;Alkhaldi et al., 2022;Dai et al., 2022;Meng et al., 2022a), with different conclusions depending on the architecture (e.g., knowledge is located in the middle layers of GPT-like models (Meng et al., 2022a), or in the upper layers in BERT-like models (Dai et al., 2022)).Another line of work focuses on the data aspect, showing the dependence of PLMs on word co-occurrences and positionally close words (Li et al., 2022b), or tracing back predictions to training data (Akyurek et al., 2022;Park et al., 2023).Knowing how PLMs retrieve facts remains challenging, but necessary to make PLMs transparent fact retrievers.
8. The introduction of a fact tracing benchmark (Akyurek et al., 2022) opens the door for works in this direction.
9. Updating Knowledge. PLMs come with a fixed set of pre-trained parameters that encode knowledge about the world.
10. As time passes, this knowledge becomes partially outdated.
11. Hence, editing existing knowledge in PLMs and augmenting them with new knowledge is crucial for their use as knowledge bases (Zini and Awad, 2022).
12. One line of research locates the modules responsible for factual predictions and modifies these to update the corresponding facts (Dai et al., 2022;De Cao et al., 2021;Meng et al., 2022a).Other lines of research keep the original PLM unchanged, but augment it with additional parameters to induce the desired changes (Wang et al., 2021b;Lee et al., 2022), or encode facts with time stamps in PLMs to make them ""time-aware"" (Dhingra et al., 2022).
13. When updating facts in PLMs, it is crucial that only the targeted facts are affected and that these facts are retrievable using different paraphrases (De Cao et al., 2021;Hase et al., 2023).However, current methods for facts editing (Meng et al., 2022a(Meng et al., , 2023) ) still do not fulfill these requirements (Hoelscher-Obermaier et al., 2023).Methods that introduce additional parameters should be made more scalable (Jang et al., 2022b)."
264451714,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18d18d4ffdc070868ce06f216a2a8d040d42a4cb,s12,Should Prompts be Optimized?,"['p12.0', 'p12.1', 'p12.2']","['Prompt Optimizing leads to better probing performance (Jiang et al., 2020b;Shin et al., 2020;Kumar and Talukdar, 2021;Newman et al., 2022;Zhang et al., 2022) .However, it remains unclear whether this improvement is due to optimized prompts leaking new knowledge into the probed PLMs.', 'Optimized prompts can be mere paraphrases of manually created prompts (Bouraoui et al., 2020;Jiang et al., 2020b).These paraphrases might be better fact retrievers because of their similarity to the pre-training corpus (Cao et al., 2022).Other prompt optimization methods find better prompts in discrete or continuous spaces (Shin et al., 2020;Zhong et al., 2021).These prompts are largely uninterpretable, and can even retrieve facts from randomly initialized PLMs (Zhong et al., 2021;Ishibashi et al., 2023).', 'Performance improvements for optimized prompts can be attributed either to prompts becoming more similar to the pre-training data or overfitting the facts distribution.Evaluation should take the pre-training corpora and the facts distribution in the probing dataset into account (Cao et al., 2021(Cao et al., , 2022)).Future work should consider adapting prompt optimization methods to produce more interpretable prompts.This would keep the performance gains, and increase the trustworthiness of optimized prompts.']","Prompt Optimizing leads to better probing performance (Jiang et al., 2020b;Shin et al., 2020;Kumar and Talukdar, 2021;Newman et al., 2022;Zhang et al., 2022) .However, it remains unclear whether this improvement is due to optimized prompts leaking new knowledge into the probed PLMs.

Optimized prompts can be mere paraphrases of manually created prompts (Bouraoui et al., 2020;Jiang et al., 2020b).These paraphrases might be better fact retrievers because of their similarity to the pre-training corpus (Cao et al., 2022).Other prompt optimization methods find better prompts in discrete or continuous spaces (Shin et al., 2020;Zhong et al., 2021).These prompts are largely uninterpretable, and can even retrieve facts from randomly initialized PLMs (Zhong et al., 2021;Ishibashi et al., 2023).

Performance improvements for optimized prompts can be attributed either to prompts becoming more similar to the pre-training data or overfitting the facts distribution.Evaluation should take the pre-training corpora and the facts distribution in the probing dataset into account (Cao et al., 2021(Cao et al., , 2022)).Future work should consider adapting prompt optimization methods to produce more interpretable prompts.This would keep the performance gains, and increase the trustworthiness of optimized prompts.","(p12.0) Prompt Optimizing leads to better probing performance (Jiang et al., 2020b;Shin et al., 2020;Kumar and Talukdar, 2021;Newman et al., 2022;Zhang et al., 2022) .However, it remains unclear whether this improvement is due to optimized prompts leaking new knowledge into the probed PLMs.

(p12.1) Optimized prompts can be mere paraphrases of manually created prompts (Bouraoui et al., 2020;Jiang et al., 2020b).These paraphrases might be better fact retrievers because of their similarity to the pre-training corpus (Cao et al., 2022).Other prompt optimization methods find better prompts in discrete or continuous spaces (Shin et al., 2020;Zhong et al., 2021).These prompts are largely uninterpretable, and can even retrieve facts from randomly initialized PLMs (Zhong et al., 2021;Ishibashi et al., 2023).

(p12.2) Performance improvements for optimized prompts can be attributed either to prompts becoming more similar to the pre-training data or overfitting the facts distribution.Evaluation should take the pre-training corpora and the facts distribution in the probing dataset into account (Cao et al., 2021(Cao et al., , 2022)).Future work should consider adapting prompt optimization methods to produce more interpretable prompts.This would keep the performance gains, and increase the trustworthiness of optimized prompts.","[['b53', 'b114', 'b75', 'b96', 'b42'], ['b37', 'b8', 'b117', 'b6', 'b96', 'b42'], ['b10', 'b8']]","[['b53', 'b114', 'b75', 'b96', 'b42'], ['b37', 'b8', 'b117', 'b6', 'b96', 'b42'], ['b10', 'b8']]",13,"1. Prompt Optimizing leads to better probing performance (Jiang et al., 2020b;Shin et al., 2020;Kumar and Talukdar, 2021;Newman et al., 2022;Zhang et al., 2022) .However, it remains unclear whether this improvement is due to optimized prompts leaking new knowledge into the probed PLMs.
2. Optimized prompts can be mere paraphrases of manually created prompts (Bouraoui et al., 2020;Jiang et al., 2020b).These paraphrases might be better fact retrievers because of their similarity to the pre-training corpus (Cao et al., 2022).Other prompt optimization methods find better prompts in discrete or continuous spaces (Shin et al., 2020;Zhong et al., 2021).These prompts are largely uninterpretable, and can even retrieve facts from randomly initialized PLMs (Zhong et al., 2021;Ishibashi et al., 2023).
3. Performance improvements for optimized prompts can be attributed either to prompts becoming more similar to the pre-training data or overfitting the facts distribution.
4. Evaluation should take the pre-training corpora and the facts distribution in the probing dataset into account (Cao et al., 2021(Cao et al., , 2022)).Future work should consider adapting prompt optimization methods to produce more interpretable prompts.
5. This would keep the performance gains, and increase the trustworthiness of optimized prompts."
264451714,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18d18d4ffdc070868ce06f216a2a8d040d42a4cb,s15,Discussion and Future Work,"['p15.0', 'p15.1', 'p15.2', 'p15.3', 'p15.4']","['Factual probing methods are developed to extract as many facts as possible from the new smart pools of knowledge, namely PLMs.This gives us an estimate about how much PLMs have learned from pre-training, and help us to assess their suitability for use cases such as PLMs-as-KBs.Improving probing methods should go hand-in-hand with advances in PLMs themselves, to help us better assess and make use of PLMs.Our analysis (cf.Section 2) shows that current probing methods focus mostly on one the the three dimensions we use in our categorization (inputs, PLMs, outputs).Introducing adaptations across two or more of these dimensions (e.g., optimizing inputs while also debiasing outputs) might lead to further improvements with respect to factual knowledge retrieval.', 'Besides improving probing methods, it is also essential to pay attention to the benchmark datasets.Some probing datasets are shown to be biased towards certain entities (Cao et al., 2021).Constructing unbiased probing datasets is crucial to have unbiased estimates of factual knowledge in PLMs.At the same time, developing comprehensive datasets which correspond to the capacity of the recently published large PLMs, e.g., (OpenAI, 2023;Penedo et al., 2023;Touvron et al., 2023), is an important future work direction.', 'We also believe that it is necessary for current evaluation schemes to not be limited to counting how often PLMs answer correctly.Instead, we call for a comprehensive evaluation that includes further important factors such as the number and frequency of the answers in the pre-training corpus, creation period of the pre-training corpus, model size, and the number of training epochs.', ""For our corpus construction we relied on all the publications that cited (Petroni et al., 2019).Although this represents the first work that sparked the community's interest in the factual knowledge present in PLMs and their use as KBs, there might be parallel works or works that go into the same direction but do not directly cite Petroni et al. (2019)'s work, which are not included in our corpus.Additionally, we relied on the venue information provided by Semantic Scholar's API to filter out irrelevant publications.These information are not always accurate and might have affected our initial corpus."", 'In this work, we focused on works that revolve around factual knowledge, and excluded works that focus on other types of knowledge (e.g., linguistic knowledge and commonsense knowledge).However, there are methods that are used for other types of knowledge that could also be applied to factual knowledge and vice versa.We consciously excluded works that focused on other types of knowledge, but this does not mean that such methods are not applicable to factual knowledge probing.other non-peer-reviewed publications.After deduplication, the first annotation step yielded a total of 173 relevant papers.']","Factual probing methods are developed to extract as many facts as possible from the new smart pools of knowledge, namely PLMs.This gives us an estimate about how much PLMs have learned from pre-training, and help us to assess their suitability for use cases such as PLMs-as-KBs.Improving probing methods should go hand-in-hand with advances in PLMs themselves, to help us better assess and make use of PLMs.Our analysis (cf.Section 2) shows that current probing methods focus mostly on one the the three dimensions we use in our categorization (inputs, PLMs, outputs).Introducing adaptations across two or more of these dimensions (e.g., optimizing inputs while also debiasing outputs) might lead to further improvements with respect to factual knowledge retrieval.

Besides improving probing methods, it is also essential to pay attention to the benchmark datasets.Some probing datasets are shown to be biased towards certain entities (Cao et al., 2021).Constructing unbiased probing datasets is crucial to have unbiased estimates of factual knowledge in PLMs.At the same time, developing comprehensive datasets which correspond to the capacity of the recently published large PLMs, e.g., (OpenAI, 2023;Penedo et al., 2023;Touvron et al., 2023), is an important future work direction.

We also believe that it is necessary for current evaluation schemes to not be limited to counting how often PLMs answer correctly.Instead, we call for a comprehensive evaluation that includes further important factors such as the number and frequency of the answers in the pre-training corpus, creation period of the pre-training corpus, model size, and the number of training epochs.

For our corpus construction we relied on all the publications that cited (Petroni et al., 2019).Although this represents the first work that sparked the community's interest in the factual knowledge present in PLMs and their use as KBs, there might be parallel works or works that go into the same direction but do not directly cite Petroni et al. (2019)'s work, which are not included in our corpus.Additionally, we relied on the venue information provided by Semantic Scholar's API to filter out irrelevant publications.These information are not always accurate and might have affected our initial corpus.

In this work, we focused on works that revolve around factual knowledge, and excluded works that focus on other types of knowledge (e.g., linguistic knowledge and commonsense knowledge).However, there are methods that are used for other types of knowledge that could also be applied to factual knowledge and vice versa.We consciously excluded works that focused on other types of knowledge, but this does not mean that such methods are not applicable to factual knowledge probing.other non-peer-reviewed publications.After deduplication, the first annotation step yielded a total of 173 relevant papers.","(p15.0) Factual probing methods are developed to extract as many facts as possible from the new smart pools of knowledge, namely PLMs.This gives us an estimate about how much PLMs have learned from pre-training, and help us to assess their suitability for use cases such as PLMs-as-KBs.Improving probing methods should go hand-in-hand with advances in PLMs themselves, to help us better assess and make use of PLMs.Our analysis (cf.Section 2) shows that current probing methods focus mostly on one the the three dimensions we use in our categorization (inputs, PLMs, outputs).Introducing adaptations across two or more of these dimensions (e.g., optimizing inputs while also debiasing outputs) might lead to further improvements with respect to factual knowledge retrieval.

(p15.1) Besides improving probing methods, it is also essential to pay attention to the benchmark datasets.Some probing datasets are shown to be biased towards certain entities (Cao et al., 2021).Constructing unbiased probing datasets is crucial to have unbiased estimates of factual knowledge in PLMs.At the same time, developing comprehensive datasets which correspond to the capacity of the recently published large PLMs, e.g., (OpenAI, 2023;Penedo et al., 2023;Touvron et al., 2023), is an important future work direction.

(p15.2) We also believe that it is necessary for current evaluation schemes to not be limited to counting how often PLMs answer correctly.Instead, we call for a comprehensive evaluation that includes further important factors such as the number and frequency of the answers in the pre-training corpus, creation period of the pre-training corpus, model size, and the number of training epochs.

(p15.3) For our corpus construction we relied on all the publications that cited (Petroni et al., 2019).Although this represents the first work that sparked the community's interest in the factual knowledge present in PLMs and their use as KBs, there might be parallel works or works that go into the same direction but do not directly cite Petroni et al. (2019)'s work, which are not included in our corpus.Additionally, we relied on the venue information provided by Semantic Scholar's API to filter out irrelevant publications.These information are not always accurate and might have affected our initial corpus.

(p15.4) In this work, we focused on works that revolve around factual knowledge, and excluded works that focus on other types of knowledge (e.g., linguistic knowledge and commonsense knowledge).However, there are methods that are used for other types of knowledge that could also be applied to factual knowledge and vice versa.We consciously excluded works that focused on other types of knowledge, but this does not mean that such methods are not applicable to factual knowledge probing.other non-peer-reviewed publications.After deduplication, the first annotation step yielded a total of 173 relevant papers.","[[], [None, 'b10', 'b81', 'b101'], [], ['b82'], []]","[[], [None, 'b10', 'b81', 'b101'], [], ['b82'], []]",5,"1. Factual probing methods are developed to extract as many facts as possible from the new smart pools of knowledge, namely PLMs.
2. This gives us an estimate about how much PLMs have learned from pre-training, and help us to assess their suitability for use cases such as PLMs-as-KBs.
3. Improving probing methods should go hand-in-hand with advances in PLMs themselves, to help us better assess and make use of PLMs.
4. Our analysis (cf.Section 2) shows that current probing methods focus mostly on one the the three dimensions we use in our categorization (inputs, PLMs, outputs).Introducing adaptations across two or more of these dimensions (e.g., optimizing inputs while also debiasing outputs) might lead to further improvements with respect to factual knowledge retrieval.
5. Besides improving probing methods, it is also essential to pay attention to the benchmark datasets.
6. Some probing datasets are shown to be biased towards certain entities (Cao et al., 2021).Constructing unbiased probing datasets is crucial to have unbiased estimates of factual knowledge in PLMs.
7. At the same time, developing comprehensive datasets which correspond to the capacity of the recently published large PLMs, e.g., (OpenAI, 2023;Penedo et al., 2023;Touvron et al., 2023), is an important future work direction.
8. We also believe that it is necessary for current evaluation schemes to not be limited to counting how often PLMs answer correctly.
9. Instead, we call for a comprehensive evaluation that includes further important factors such as the number and frequency of the answers in the pre-training corpus, creation period of the pre-training corpus, model size, and the number of training epochs.
10. For our corpus construction we relied on all the publications that cited (Petroni et al., 2019).Although this represents the first work that sparked the community's interest in the factual knowledge present in PLMs and their use as KBs, there might be parallel works or works that go into the same direction but do not directly cite Petroni et al. (2019)'s work, which are not included in our corpus.
11. Additionally, we relied on the venue information provided by Semantic Scholar's API to filter out irrelevant publications.
12. These information are not always accurate and might have affected our initial corpus.
13. In this work, we focused on works that revolve around factual knowledge, and excluded works that focus on other types of knowledge (e.g., linguistic knowledge and commonsense knowledge).However, there are methods that are used for other types of knowledge that could also be applied to factual knowledge and vice versa.
14. We consciously excluded works that focused on other types of knowledge, but this does not mean that such methods are not applicable to factual knowledge probing.other non-peer-reviewed publications.
15. After deduplication, the first annotation step yielded a total of 173 relevant papers."
264451714,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18d18d4ffdc070868ce06f216a2a8d040d42a4cb,s5,Probed PLMs,['p5.0'],"['PLMs are probed for knowledge using either their original pre-trained parameters (Petroni et al., 2019;Jiang et al., 2020b), or after adapting these parameters (Roberts et al., 2020;Meng et al., 2022b).']","PLMs are probed for knowledge using either their original pre-trained parameters (Petroni et al., 2019;Jiang et al., 2020b), or after adapting these parameters (Roberts et al., 2020;Meng et al., 2022b).","(p5.0) PLMs are probed for knowledge using either their original pre-trained parameters (Petroni et al., 2019;Jiang et al., 2020b), or after adapting these parameters (Roberts et al., 2020;Meng et al., 2022b).","[['b89', 'b82', 'b42', 'b71']]","[['b89', 'b82', 'b42', 'b71']]",4,"1. PLMs are probed for knowledge using either their original pre-trained parameters (Petroni et al., 2019;Jiang et al., 2020b), or after adapting these parameters (Roberts et al., 2020;Meng et al., 2022b)."
264451714,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18d18d4ffdc070868ce06f216a2a8d040d42a4cb,s14,Related Work,['p14.0'],"['AlKhamissi et al. ( 2022) elaborate requirements for PLMs as knowledge bases and review recent literature w.r.t.those requirements.These requirements are widely known (e.g., consistency (Petroni et al., 2019) and updating knowledge (De Cao et al., 2021)).Our analysis leads to similar general observations (cf.Section 5), and additionally reviews more recent solutions to these obstacles.Cao et al. (2023) cover probing PLMs as part of the knowledge cycle in PLMs, but do not address factual knowledge probing at the same level of detail as we do.Liu et al. (2023) survey prompting methods in detail.However, they cover only a part of factual knowledge probing methods.Safavi and Koutra (2021) survey how PLMs acquire relational knowledge, organizing knowledge representations strategies in PLMs based on different levels of KBs supervision.We provide a novel categorization scheme and conduct a systematic analysis of methods for factual knowledge probing that goes beyond all existing surveys.We additionally provide a categorization of factual probing datasets.Furthermore, we discuss recent findings on knowledge retention, the use of optimized prompts, and challenges with corresponding recent solutions to adopting PLMs as KBs, shedding light on several future work directions.In contrast to other work, we employed a systematic approach to curate and analyze relevant literature to a comprehensive and unbiased representation of existing work.']","AlKhamissi et al. ( 2022) elaborate requirements for PLMs as knowledge bases and review recent literature w.r.t.those requirements.These requirements are widely known (e.g., consistency (Petroni et al., 2019) and updating knowledge (De Cao et al., 2021)).Our analysis leads to similar general observations (cf.Section 5), and additionally reviews more recent solutions to these obstacles.Cao et al. (2023) cover probing PLMs as part of the knowledge cycle in PLMs, but do not address factual knowledge probing at the same level of detail as we do.Liu et al. (2023) survey prompting methods in detail.However, they cover only a part of factual knowledge probing methods.Safavi and Koutra (2021) survey how PLMs acquire relational knowledge, organizing knowledge representations strategies in PLMs based on different levels of KBs supervision.We provide a novel categorization scheme and conduct a systematic analysis of methods for factual knowledge probing that goes beyond all existing surveys.We additionally provide a categorization of factual probing datasets.Furthermore, we discuss recent findings on knowledge retention, the use of optimized prompts, and challenges with corresponding recent solutions to adopting PLMs as KBs, shedding light on several future work directions.In contrast to other work, we employed a systematic approach to curate and analyze relevant literature to a comprehensive and unbiased representation of existing work.","(p14.0) AlKhamissi et al. ( 2022) elaborate requirements for PLMs as knowledge bases and review recent literature w.r.t.those requirements.These requirements are widely known (e.g., consistency (Petroni et al., 2019) and updating knowledge (De Cao et al., 2021)).Our analysis leads to similar general observations (cf.Section 5), and additionally reviews more recent solutions to these obstacles.Cao et al. (2023) cover probing PLMs as part of the knowledge cycle in PLMs, but do not address factual knowledge probing at the same level of detail as we do.Liu et al. (2023) survey prompting methods in detail.However, they cover only a part of factual knowledge probing methods.Safavi and Koutra (2021) survey how PLMs acquire relational knowledge, organizing knowledge representations strategies in PLMs based on different levels of KBs supervision.We provide a novel categorization scheme and conduct a systematic analysis of methods for factual knowledge probing that goes beyond all existing surveys.We additionally provide a categorization of factual probing datasets.Furthermore, we discuss recent findings on knowledge retention, the use of optimized prompts, and challenges with corresponding recent solutions to adopting PLMs as KBs, shedding light on several future work directions.In contrast to other work, we employed a systematic approach to curate and analyze relevant literature to a comprehensive and unbiased representation of existing work.","[['b18', 'b93', 'b82', 'b63', 'b9']]","[['b18', 'b93', 'b82', 'b63', 'b9']]",5,"1. AlKhamissi et al. ( 2022) elaborate requirements for PLMs as knowledge bases and review recent literature w.r.t.those requirements.
2. These requirements are widely known (e.g., consistency (Petroni et al., 2019) and updating knowledge (De Cao et al., 2021)).Our analysis leads to similar general observations (cf.Section 5), and additionally reviews more recent solutions to these obstacles.
3. Cao et al. (2023) cover probing PLMs as part of the knowledge cycle in PLMs, but do not address factual knowledge probing at the same level of detail as we do.
4. Liu et al. (2023) survey prompting methods in detail.
5. However, they cover only a part of factual knowledge probing methods.
6. Safavi and Koutra (2021) survey how PLMs acquire relational knowledge, organizing knowledge representations strategies in PLMs based on different levels of KBs supervision.
7. We provide a novel categorization scheme and conduct a systematic analysis of methods for factual knowledge probing that goes beyond all existing surveys.
8. We additionally provide a categorization of factual probing datasets.
9. Furthermore, we discuss recent findings on knowledge retention, the use of optimized prompts, and challenges with corresponding recent solutions to adopting PLMs as KBs, shedding light on several future work directions.
10. In contrast to other work, we employed a systematic approach to curate and analyze relevant literature to a comprehensive and unbiased representation of existing work."
264451714,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18d18d4ffdc070868ce06f216a2a8d040d42a4cb,s7,Adapted PLMs,"['p7.0', 'p7.1']","['Some works adapt the PLMs under evaluation to enable evaluation tasks, that do not correspond to any pre-training objective.The adaptation, however, is also coupled with risks such as train-test overlap (Lewis et al., 2021;Wang et al., 2021a).Supervised adaptation.Most methods finetune the probed PLMs in a supervised manner to adapt them to the probing task.Roberts et al. (2020) finetune T5 models for closed-book question answering, where models have only questions as inputs, while leaving out any context or external knowledge sources that might contain the answer.Similarly, Wang et al. (2021a) finetune BART to output a related passage, and then the answer.Bouraoui et al. (2020) finetune BERT to classify prompts based on whether the relation between the subject and object entities truly holds or not.Fichtel et al. (2021) finetune a BERT model with its masked language modeling head to predict the masked tokens in the provided prompts.Abaho et al. (2022) propose an additional position-attention layer on top of transformer models, where the position of the masked token is kept constant, and the remaining tokens are given positions relative to the masked token.This approach is considered to put more focus on the masked tokens and its interaction with the remaining tokens in the prompt.Chen et al. (2022) leverage a task description that depends on the relation between the subject and object entity, alongside a few labeled examples to train the probed PLMs.At inference time, the PLMs are kept frozen and are provided with unseen task descriptions and labeled examples to adapt to the task.Elazar et al. (2021) further train BERT with a consistency loss to increase its robustness to paraphrases that describe the same relation.Shi et al. (2021) finetune generative PLMs to generate entity descriptions depending only on their knoweldge from pre-training.Qin and Eisner (2021) do not directly change any parameters in PLMs, but rather introduce additional trainable parameters in each layer that change the hidden representations of the prompts to help make them more suitable for knowledge extraction.', 'Self-supervised adaptation.Adaptations in a self-supervised manner can introduce changes to the model without explicitly finetuning the model to the probing task.For example, Meng et al. (2022b) propose to re-wire the probed PLM in a self-supervised manner.Their method depends on using data from the pre-training phase, splitting each sentence into a head part and a tail part, and using a contrastive learning objective to push the representations of the matching head and tail pairs (positives) closer to one another, and that of the non-matching pairs (negatives) to be further apart.The evaluation is based on the similarity between the representations of the prompt and a predefined set of entities that represent potential answers.']","Some works adapt the PLMs under evaluation to enable evaluation tasks, that do not correspond to any pre-training objective.The adaptation, however, is also coupled with risks such as train-test overlap (Lewis et al., 2021;Wang et al., 2021a).Supervised adaptation.Most methods finetune the probed PLMs in a supervised manner to adapt them to the probing task.Roberts et al. (2020) finetune T5 models for closed-book question answering, where models have only questions as inputs, while leaving out any context or external knowledge sources that might contain the answer.Similarly, Wang et al. (2021a) finetune BART to output a related passage, and then the answer.Bouraoui et al. (2020) finetune BERT to classify prompts based on whether the relation between the subject and object entities truly holds or not.Fichtel et al. (2021) finetune a BERT model with its masked language modeling head to predict the masked tokens in the provided prompts.Abaho et al. (2022) propose an additional position-attention layer on top of transformer models, where the position of the masked token is kept constant, and the remaining tokens are given positions relative to the masked token.This approach is considered to put more focus on the masked tokens and its interaction with the remaining tokens in the prompt.Chen et al. (2022) leverage a task description that depends on the relation between the subject and object entity, alongside a few labeled examples to train the probed PLMs.At inference time, the PLMs are kept frozen and are provided with unseen task descriptions and labeled examples to adapt to the task.Elazar et al. (2021) further train BERT with a consistency loss to increase its robustness to paraphrases that describe the same relation.Shi et al. (2021) finetune generative PLMs to generate entity descriptions depending only on their knoweldge from pre-training.Qin and Eisner (2021) do not directly change any parameters in PLMs, but rather introduce additional trainable parameters in each layer that change the hidden representations of the prompts to help make them more suitable for knowledge extraction.

Self-supervised adaptation.Adaptations in a self-supervised manner can introduce changes to the model without explicitly finetuning the model to the probing task.For example, Meng et al. (2022b) propose to re-wire the probed PLM in a self-supervised manner.Their method depends on using data from the pre-training phase, splitting each sentence into a head part and a tail part, and using a contrastive learning objective to push the representations of the matching head and tail pairs (positives) closer to one another, and that of the non-matching pairs (negatives) to be further apart.The evaluation is based on the similarity between the representations of the prompt and a predefined set of entities that represent potential answers.","(p7.0) Some works adapt the PLMs under evaluation to enable evaluation tasks, that do not correspond to any pre-training objective.The adaptation, however, is also coupled with risks such as train-test overlap (Lewis et al., 2021;Wang et al., 2021a).Supervised adaptation.Most methods finetune the probed PLMs in a supervised manner to adapt them to the probing task.Roberts et al. (2020) finetune T5 models for closed-book question answering, where models have only questions as inputs, while leaving out any context or external knowledge sources that might contain the answer.Similarly, Wang et al. (2021a) finetune BART to output a related passage, and then the answer.Bouraoui et al. (2020) finetune BERT to classify prompts based on whether the relation between the subject and object entities truly holds or not.Fichtel et al. (2021) finetune a BERT model with its masked language modeling head to predict the masked tokens in the provided prompts.Abaho et al. (2022) propose an additional position-attention layer on top of transformer models, where the position of the masked token is kept constant, and the remaining tokens are given positions relative to the masked token.This approach is considered to put more focus on the masked tokens and its interaction with the remaining tokens in the prompt.Chen et al. (2022) leverage a task description that depends on the relation between the subject and object entity, alongside a few labeled examples to train the probed PLMs.At inference time, the PLMs are kept frozen and are provided with unseen task descriptions and labeled examples to adapt to the task.Elazar et al. (2021) further train BERT with a consistency loss to increase its robustness to paraphrases that describe the same relation.Shi et al. (2021) finetune generative PLMs to generate entity descriptions depending only on their knoweldge from pre-training.Qin and Eisner (2021) do not directly change any parameters in PLMs, but rather introduce additional trainable parameters in each layer that change the hidden representations of the prompts to help make them more suitable for knowledge extraction.

(p7.1) Self-supervised adaptation.Adaptations in a self-supervised manner can introduce changes to the model without explicitly finetuning the model to the probing task.For example, Meng et al. (2022b) propose to re-wire the probed PLM in a self-supervised manner.Their method depends on using data from the pre-training phase, splitting each sentence into a head part and a tail part, and using a contrastive learning objective to push the representations of the matching head and tail pairs (positives) closer to one another, and that of the non-matching pairs (negatives) to be further apart.The evaluation is based on the similarity between the representations of the prompt and a predefined set of entities that represent potential answers.","[['b89', 'b0', 'b85', 'b95', 'b11', 'b6', 'b25', 'b103', 'b23', 'b58'], ['b71']]","[['b89', 'b0', 'b85', 'b95', 'b11', 'b6', 'b25', 'b103', 'b23', 'b58'], ['b71']]",11,"1. Some works adapt the PLMs under evaluation to enable evaluation tasks, that do not correspond to any pre-training objective.
2. The adaptation, however, is also coupled with risks such as train-test overlap (Lewis et al., 2021;Wang et al., 2021a).Supervised adaptation.
3. Most methods finetune the probed PLMs in a supervised manner to adapt them to the probing task.
4. Roberts et al. (2020) finetune T5 models for closed-book question answering, where models have only questions as inputs, while leaving out any context or external knowledge sources that might contain the answer.
5. Similarly, Wang et al. (2021a) finetune BART to output a related passage, and then the answer.
6. Bouraoui et al. (2020) finetune BERT to classify prompts based on whether the relation between the subject and object entities truly holds or not.
7. Fichtel et al. (2021) finetune a BERT model with its masked language modeling head to predict the masked tokens in the provided prompts.
8. Abaho et al. (2022) propose an additional position-attention layer on top of transformer models, where the position of the masked token is kept constant, and the remaining tokens are given positions relative to the masked token.
9. This approach is considered to put more focus on the masked tokens and its interaction with the remaining tokens in the prompt.
10. Chen et al. (2022) leverage a task description that depends on the relation between the subject and object entity, alongside a few labeled examples to train the probed PLMs.
11. At inference time, the PLMs are kept frozen and are provided with unseen task descriptions and labeled examples to adapt to the task.
12. Elazar et al. (2021) further train BERT with a consistency loss to increase its robustness to paraphrases that describe the same relation.
13. Shi et al. (2021) finetune generative PLMs to generate entity descriptions depending only on their knoweldge from pre-training.
14. Qin and Eisner (2021) do not directly change any parameters in PLMs, but rather introduce additional trainable parameters in each layer that change the hidden representations of the prompts to help make them more suitable for knowledge extraction.
15. Self-supervised adaptation. Adaptations in a self-supervised manner can introduce changes to the model without explicitly finetuning the model to the probing task.
16. For example, Meng et al. (2022b) propose to re-wire the probed PLM in a self-supervised manner.
17. Their method depends on using data from the pre-training phase, splitting each sentence into a head part and a tail part, and using a contrastive learning objective to push the representations of the matching head and tail pairs (positives) closer to one another, and that of the non-matching pairs (negatives) to be further apart.
18. The evaluation is based on the similarity between the representations of the prompt and a predefined set of entities that represent potential answers."
264451714,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18d18d4ffdc070868ce06f216a2a8d040d42a4cb,s8,Outputs,"['p8.0', 'p8.1', 'p8.2', 'p8.3']","['Methods focusing on the outputs of PLMs address restricting the output space of PLMs, debiasing their outputs, and handling multi-token entities.', 'Typed querying.Kassner et al. (2021) propose to restrict the space of possible values for replacing the masked token (object) from the whole vocabulary to a specific set of tokens whose type matches the type of the ground truth object.For example, if the PLM is queried with the prompt: ""The smallest country in the world is [MASK]"", only entities of type country are considered to replace the [MASK] token.This method has two advantages: it reduces the number of objects under consideration and allows for a better comparison across PLMs with different vocabularies (Kassner et al., 2021).', 'Debiasing.Zhao et al. (2021) identify biases in the predictions of PLMs towards common and recent tokens, and propose a method that adapts the output probabilities by first estimating these biases using neutral examples and then correcting them.This debiasing method is shown to reduce the variance across prompts and has a positive effect on fact retrieval.Malkin et al. (2022) propose a method to increase the effect of distant tokens on the predictions of PLMs.The method depends on combining two output distributions over the vocabulary.One distribution is based on the full-length input, whereas the other is based on a shortened version of the same input.Wang et al. (2023) identify the problem of object bias in optimized prompts and propose to make all potential objects equally probable when no subject is provided, and increasing the probability of the correct object, when the subject is available.Yoshikawa and Okazaki (2023) output predictions only above a sufficient confidence threshold.This results in a less biased evaluation, and reflects the ability of PLMs in excluding uncertain predictions.To address the problems of multiple valid answers and frequency bias, i.e., the co-occurence of some subject and object entities despite not being in a factual relation to one another, Dong et al. (2022) use two templates, one contains the correct relation while the other contains an erroneous relation between the two entities, and compare the probability for the correct object under both relations.', 'Multi-token entities.To handle multi-token entities, Jiang et al. (2020a) propose using a predefined number of masked tokens and filling these using different strategies: 1) independent from each other, 2) sequentially (left-to-right for English), 3) starting with the most confident predictions.(Kalinsky et al., 2023) leverage the masked token representation to generate multiple tokens using a small generative model.']","Methods focusing on the outputs of PLMs address restricting the output space of PLMs, debiasing their outputs, and handling multi-token entities.

Typed querying.Kassner et al. (2021) propose to restrict the space of possible values for replacing the masked token (object) from the whole vocabulary to a specific set of tokens whose type matches the type of the ground truth object.For example, if the PLM is queried with the prompt: ""The smallest country in the world is [MASK]"", only entities of type country are considered to replace the [MASK] token.This method has two advantages: it reduces the number of objects under consideration and allows for a better comparison across PLMs with different vocabularies (Kassner et al., 2021).

Debiasing.Zhao et al. (2021) identify biases in the predictions of PLMs towards common and recent tokens, and propose a method that adapts the output probabilities by first estimating these biases using neutral examples and then correcting them.This debiasing method is shown to reduce the variance across prompts and has a positive effect on fact retrieval.Malkin et al. (2022) propose a method to increase the effect of distant tokens on the predictions of PLMs.The method depends on combining two output distributions over the vocabulary.One distribution is based on the full-length input, whereas the other is based on a shortened version of the same input.Wang et al. (2023) identify the problem of object bias in optimized prompts and propose to make all potential objects equally probable when no subject is provided, and increasing the probability of the correct object, when the subject is available.Yoshikawa and Okazaki (2023) output predictions only above a sufficient confidence threshold.This results in a less biased evaluation, and reflects the ability of PLMs in excluding uncertain predictions.To address the problems of multiple valid answers and frequency bias, i.e., the co-occurence of some subject and object entities despite not being in a factual relation to one another, Dong et al. (2022) use two templates, one contains the correct relation while the other contains an erroneous relation between the two entities, and compare the probability for the correct object under both relations.

Multi-token entities.To handle multi-token entities, Jiang et al. (2020a) propose using a predefined number of masked tokens and filling these using different strategies: 1) independent from each other, 2) sequentially (left-to-right for English), 3) starting with the most confident predictions.(Kalinsky et al., 2023) leverage the masked token representation to generate multiple tokens using a small generative model.","(p8.0) Methods focusing on the outputs of PLMs address restricting the output space of PLMs, debiasing their outputs, and handling multi-token entities.

(p8.1) Typed querying.Kassner et al. (2021) propose to restrict the space of possible values for replacing the masked token (object) from the whole vocabulary to a specific set of tokens whose type matches the type of the ground truth object.For example, if the PLM is queried with the prompt: ""The smallest country in the world is [MASK]"", only entities of type country are considered to replace the [MASK] token.This method has two advantages: it reduces the number of objects under consideration and allows for a better comparison across PLMs with different vocabularies (Kassner et al., 2021).

(p8.2) Debiasing.Zhao et al. (2021) identify biases in the predictions of PLMs towards common and recent tokens, and propose a method that adapts the output probabilities by first estimating these biases using neutral examples and then correcting them.This debiasing method is shown to reduce the variance across prompts and has a positive effect on fact retrieval.Malkin et al. (2022) propose a method to increase the effect of distant tokens on the predictions of PLMs.The method depends on combining two output distributions over the vocabulary.One distribution is based on the full-length input, whereas the other is based on a shortened version of the same input.Wang et al. (2023) identify the problem of object bias in optimized prompts and propose to make all potential objects equally probable when no subject is provided, and increasing the probability of the correct object, when the subject is available.Yoshikawa and Okazaki (2023) output predictions only above a sufficient confidence threshold.This results in a less biased evaluation, and reflects the ability of PLMs in excluding uncertain predictions.To address the problems of multiple valid answers and frequency bias, i.e., the co-occurence of some subject and object entities despite not being in a factual relation to one another, Dong et al. (2022) use two templates, one contains the correct relation while the other contains an erroneous relation between the two entities, and compare the probability for the correct object under both relations.

(p8.3) Multi-token entities.To handle multi-token entities, Jiang et al. (2020a) propose using a predefined number of masked tokens and filling these using different strategies: 1) independent from each other, 2) sequentially (left-to-right for English), 3) starting with the most confident predictions.(Kalinsky et al., 2023) leverage the masked token representation to generate multiple tokens using a small generative model.","[[], ['b50'], ['b21', 'b66', 'b110', 'b115', 'b105'], ['b46', 'b41']]","[[], ['b50'], ['b21', 'b66', 'b110', 'b115', 'b105'], ['b46', 'b41']]",8,"1. Methods focusing on the outputs of PLMs address restricting the output space of PLMs, debiasing their outputs, and handling multi-token entities.
2. Typed querying. Kassner et al. (2021) propose to restrict the space of possible values for replacing the masked token (object) from the whole vocabulary to a specific set of tokens whose type matches the type of the ground truth object.
3. For example, if the PLM is queried with the prompt: ""The smallest country in the world is [MASK]"", only entities of type country are considered to replace the [MASK] token.
4. This method has two advantages: it reduces the number of objects under consideration and allows for a better comparison across PLMs with different vocabularies (Kassner et al., 2021).
5. Debiasing. Zhao et al. (2021) identify biases in the predictions of PLMs towards common and recent tokens, and propose a method that adapts the output probabilities by first estimating these biases using neutral examples and then correcting them.
6. This debiasing method is shown to reduce the variance across prompts and has a positive effect on fact retrieval.
7. Malkin et al. (2022) propose a method to increase the effect of distant tokens on the predictions of PLMs.
8. The method depends on combining two output distributions over the vocabulary.
9. One distribution is based on the full-length input, whereas the other is based on a shortened version of the same input.
10. Wang et al. (2023) identify the problem of object bias in optimized prompts and propose to make all potential objects equally probable when no subject is provided, and increasing the probability of the correct object, when the subject is available.
11. Yoshikawa and Okazaki (2023) output predictions only above a sufficient confidence threshold.
12. This results in a less biased evaluation, and reflects the ability of PLMs in excluding uncertain predictions.
13. To address the problems of multiple valid answers and frequency bias, i.e., the co-occurence of some subject and object entities despite not being in a factual relation to one another, Dong et al. (2022) use two templates, one contains the correct relation while the other contains an erroneous relation between the two entities, and compare the probability for the correct object under both relations.
14. Multi-token entities. To handle multi-token entities, Jiang et al. (2020a) propose using a predefined number of masked tokens and filling these using different strategies: 1) independent from each other, 2) sequentially (left-to-right for English), 3) starting with the most confident predictions.(Kalinsky et al., 2023) leverage the masked token representation to generate multiple tokens using a small generative model."
264451714,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18d18d4ffdc070868ce06f216a2a8d040d42a4cb,s3,Non-optimized Inputs,"['p3.0', 'p3.1', 'p3.2', 'p3.3']","['Extracting factual knowledge from PLMs depends on providing them with short inputs that indirectly describe the sought-after information.These methods can take various forms (cloze prompts (Taylor, 1953), questions, or entities).Non-optimized inputs represent the simplest case, where the probing 2 For more details refer to Appendix A.1 inputs are not altered in any way.', ""Cloze prompts are widely used across several methods.Petroni et al. (2019) probe PLMs for factual knowledge by manually constructing clozestyle templates for several relations.Onoe et al. (2022) automatically construct cloze prompts from Wikipedia and Wikidata by masking out spans near entities of interest, in order to evaluate PLMs' knowledge about (unseen) entities.Abaho et al. (2022) construct cloze prompts from annotated PubMed abstracts to use PLMs as health outcome predictors.Chen et al. (2022) finetune PLMs using cloze prompts that consist of task descriptions alongside a few examples to elicit more facts."", 'Questions are the second input category.Several Question Answering datasets are used to finetune T5 models (Raffel et al., 2020), and evaluate the amount of knowledge implicitly present in their parameters in (Roberts et al., 2020).Multiple choice questions are used in (Hardalov et al., 2020) by providing PLMs with the questions followed by each option individually.The options are masked, and the final answer is selected based on the normalized log probabilities of the predicted tokens for each option.Kalo and Fichtel (2022) present a dataset based on Wikipedia, where inputs consist of several questions and answers, i.e., a few examples to implicitly indicate the task, and a similar question without an answer for evaluation.', ""Entities are used in methods that infer relational information or generate descriptions based on these entities.Some methods depend on a simple classifier or cosine similarity between the subject and object representations to determine the presence or absence of a relation.For example, to probe for geographical knowledge, Liétard et al. ( 2021) use fixed inputs that contain locations (e.g., countries or cities).These inputs are then used to extract representations for the respective locations from PLMs.Using these representations, the authors evaluate based on the ability of a simple classifier to solve certain tasks (e.g., predicting if two countries share border).Dufter et al. (2021) evaluate the amount of knowledge present in static word embeddings by matching a subject entity (the query) to an object entity from a pre-defined set of possible objects based on the cosine similarity between the representations of the subject and object entities.Shi et al. (2021) train generative PLMs to generate entities' descriptions while providing only the en-tities as inputs, and compare them to ground truth descriptions.""]","Extracting factual knowledge from PLMs depends on providing them with short inputs that indirectly describe the sought-after information.These methods can take various forms (cloze prompts (Taylor, 1953), questions, or entities).Non-optimized inputs represent the simplest case, where the probing 2 For more details refer to Appendix A.1 inputs are not altered in any way.

Cloze prompts are widely used across several methods.Petroni et al. (2019) probe PLMs for factual knowledge by manually constructing clozestyle templates for several relations.Onoe et al. (2022) automatically construct cloze prompts from Wikipedia and Wikidata by masking out spans near entities of interest, in order to evaluate PLMs' knowledge about (unseen) entities.Abaho et al. (2022) construct cloze prompts from annotated PubMed abstracts to use PLMs as health outcome predictors.Chen et al. (2022) finetune PLMs using cloze prompts that consist of task descriptions alongside a few examples to elicit more facts.

Questions are the second input category.Several Question Answering datasets are used to finetune T5 models (Raffel et al., 2020), and evaluate the amount of knowledge implicitly present in their parameters in (Roberts et al., 2020).Multiple choice questions are used in (Hardalov et al., 2020) by providing PLMs with the questions followed by each option individually.The options are masked, and the final answer is selected based on the normalized log probabilities of the predicted tokens for each option.Kalo and Fichtel (2022) present a dataset based on Wikipedia, where inputs consist of several questions and answers, i.e., a few examples to implicitly indicate the task, and a similar question without an answer for evaluation.

Entities are used in methods that infer relational information or generate descriptions based on these entities.Some methods depend on a simple classifier or cosine similarity between the subject and object representations to determine the presence or absence of a relation.For example, to probe for geographical knowledge, Liétard et al. ( 2021) use fixed inputs that contain locations (e.g., countries or cities).These inputs are then used to extract representations for the respective locations from PLMs.Using these representations, the authors evaluate based on the ability of a simple classifier to solve certain tasks (e.g., predicting if two countries share border).Dufter et al. (2021) evaluate the amount of knowledge present in static word embeddings by matching a subject entity (the query) to an object entity from a pre-defined set of possible objects based on the cosine similarity between the representations of the subject and object entities.Shi et al. (2021) train generative PLMs to generate entities' descriptions while providing only the en-tities as inputs, and compare them to ground truth descriptions.","(p3.0) Extracting factual knowledge from PLMs depends on providing them with short inputs that indirectly describe the sought-after information.These methods can take various forms (cloze prompts (Taylor, 1953), questions, or entities).Non-optimized inputs represent the simplest case, where the probing 2 For more details refer to Appendix A.1 inputs are not altered in any way.

(p3.1) Cloze prompts are widely used across several methods.Petroni et al. (2019) probe PLMs for factual knowledge by manually constructing clozestyle templates for several relations.Onoe et al. (2022) automatically construct cloze prompts from Wikipedia and Wikidata by masking out spans near entities of interest, in order to evaluate PLMs' knowledge about (unseen) entities.Abaho et al. (2022) construct cloze prompts from annotated PubMed abstracts to use PLMs as health outcome predictors.Chen et al. (2022) finetune PLMs using cloze prompts that consist of task descriptions alongside a few examples to elicit more facts.

(p3.2) Questions are the second input category.Several Question Answering datasets are used to finetune T5 models (Raffel et al., 2020), and evaluate the amount of knowledge implicitly present in their parameters in (Roberts et al., 2020).Multiple choice questions are used in (Hardalov et al., 2020) by providing PLMs with the questions followed by each option individually.The options are masked, and the final answer is selected based on the normalized log probabilities of the predicted tokens for each option.Kalo and Fichtel (2022) present a dataset based on Wikipedia, where inputs consist of several questions and answers, i.e., a few examples to implicitly indicate the task, and a similar question without an answer for evaluation.

(p3.3) Entities are used in methods that infer relational information or generate descriptions based on these entities.Some methods depend on a simple classifier or cosine similarity between the subject and object representations to determine the presence or absence of a relation.For example, to probe for geographical knowledge, Liétard et al. ( 2021) use fixed inputs that contain locations (e.g., countries or cities).These inputs are then used to extract representations for the respective locations from PLMs.Using these representations, the authors evaluate based on the ability of a simple classifier to solve certain tasks (e.g., predicting if two countries share border).Dufter et al. (2021) evaluate the amount of knowledge present in static word embeddings by matching a subject entity (the query) to an object entity from a pre-defined set of possible objects based on the cosine similarity between the representations of the subject and object entities.Shi et al. (2021) train generative PLMs to generate entities' descriptions while providing only the en-tities as inputs, and compare them to ground truth descriptions.","[['b99'], ['b77', 'b82', 'b0', 'b11'], ['b86', 'b89', 'b47', 'b30'], ['b22', 'b95']]","[['b99'], ['b77', 'b82', 'b0', 'b11'], ['b86', 'b89', 'b47', 'b30'], ['b22', 'b95']]",11,"1. Extracting factual knowledge from PLMs depends on providing them with short inputs that indirectly describe the sought-after information.
2. These methods can take various forms (cloze prompts (Taylor, 1953), questions, or entities).Non-optimized inputs represent the simplest case, where the probing 2 For more details refer to Appendix A.1 inputs are not altered in any way.
3. Cloze prompts are widely used across several methods.
4. Petroni et al. (2019) probe PLMs for factual knowledge by manually constructing clozestyle templates for several relations.
5. Onoe et al. (2022) automatically construct cloze prompts from Wikipedia and Wikidata by masking out spans near entities of interest, in order to evaluate PLMs' knowledge about (unseen) entities.
6. Abaho et al. (2022) construct cloze prompts from annotated PubMed abstracts to use PLMs as health outcome predictors.
7. Chen et al. (2022) finetune PLMs using cloze prompts that consist of task descriptions alongside a few examples to elicit more facts.
8. Questions are the second input category.
9. Several Question Answering datasets are used to finetune T5 models (Raffel et al., 2020), and evaluate the amount of knowledge implicitly present in their parameters in (Roberts et al., 2020).Multiple choice questions are used in (Hardalov et al., 2020) by providing PLMs with the questions followed by each option individually.
10. The options are masked, and the final answer is selected based on the normalized log probabilities of the predicted tokens for each option.
11. Kalo and Fichtel (2022) present a dataset based on Wikipedia, where inputs consist of several questions and answers, i.e., a few examples to implicitly indicate the task, and a similar question without an answer for evaluation.
12. Entities are used in methods that infer relational information or generate descriptions based on these entities.
13. Some methods depend on a simple classifier or cosine similarity between the subject and object representations to determine the presence or absence of a relation.
14. For example, to probe for geographical knowledge, Liétard et al. ( 2021) use fixed inputs that contain locations (e.g., countries or cities).These inputs are then used to extract representations for the respective locations from PLMs.
15. Using these representations, the authors evaluate based on the ability of a simple classifier to solve certain tasks (e.g., predicting if two countries share border).Dufter et al. (2021) evaluate the amount of knowledge present in static word embeddings by matching a subject entity (the query) to an object entity from a pre-defined set of possible objects based on the cosine similarity between the representations of the subject and object entities.
16. Shi et al. (2021) train generative PLMs to generate entities' descriptions while providing only the en-tities as inputs, and compare them to ground truth descriptions."
264451714,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18d18d4ffdc070868ce06f216a2a8d040d42a4cb,s4,Optimized Inputs,"['p4.0', 'p4.1', 'p4.2', 'p4.3']","[""Probing inputs contribute substantially to the probing procedure.PLMs are sensitive to the inputs (Petroni et al., 2019;Jiang et al., 2020b;Elazar et al., 2021), and even syntactical variations or distractors, that do not alter the meaning, cause the PLM's predictions to change (Heinzerling and Inui, 2021;Longpre et al., 2021;Pandia and Ettinger, 2021;Podkorytov et al., 2021;Li et al., 2022a).Therefore, depending on the probing inputs, the estimate on factual knowledge we obtain may vary significantly.Optimized inputs represent variations of the inputs, where the inputs are changed to account for the sensitivity of the probed PLMs."", 'Diversification and mining methods aim to diversify and optimize prompts by mining Wikipedia or other resources, and selecting the best performing prompts or a combination of them.For example, Jiang et al. (2020b) propose a mining-based and a paraphrasing-based approach to create alternative prompts that outperform manual ones.The final prompts are selected based on their performance on a training set, and can also be combined in an ensemble.Bouraoui et al. (2020) mine for prompts that contain the entities of interest, and filter these based on the ability of the probed PLMs to predict the masked objects.After the filtering step, the remaining prompts are utilized to create a dataset that consists of positive inputs, i.e., containing true subject-object pairs, and negative inputs, which contain false pairs.This dataset is then used for the final evaluation.', ""Direct optimization methods aim to directly optimize existing prompts.This optimization happens either in a discrete space, to keep the prompts in natural language, or in a continuous space where the prompts do not have to correspond to specific tokens from the vocabulary.Optimization could also target only the masked token or the order of the examples in the prompt, in case a few examples are provided in the prompt to better indicate the task.Shin et al. (2020)'s AUTOPROMPT extends manually created prompts by prompts with a pre-defined number of trigger tokens, and employs gradientbased search to sequentially replace the trigger tokens with concrete tokens.These tokens are chosen to increase the probability of predicting the correct object.OPTIPROMPT (Zhong et al., 2021) is sim-ilar to AUTOPROMPT, but allows for the trigger tokens to be replaced with vectors from a continuous embedding space.In a similar fashion, Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation.Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors.The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs.Newman et al. (2022) utilize adapters (Houlsby et al., 2019) to map the embedding vectors to continuous prompts in order to make the probed PLMs less sensitive to different phrasings of the same prompts.Saeed and Papotti (2022) augment the masked tokens with a special type of embeddings, called Type Embeddings.These embeddings are derived from several entities that share the same type, and are shown to help tie the probed PLM's predictions to the expected type of the masked entity.PERO (Kumar and Talukdar, 2021) depends on querying PLMs with prompts containing few training examples (or shots), which demonstrate the task to the queried PLMs.Since PLMs are quite sensitive to the order and the quality of the provided training examples in the prompt, PERO leverages a genetic algorithm to find an optimized prompt and a separator token to concatenate the examples in the prompts.(Li et al., 2022c) exploit the symmetry of the task, and optimize prompts in a continuous space so that the probability of predicting both the subject and the object is maximized using the resulting prompts."", 'Generation with PLM methods re-write prompts with the help of a secondary PLM.Haviv et al. (2021) re-write manual prompts using another version of the probed model.The re-writing model is trained to produce prompts that help extract more knowledge from the probed one, which is kept unchanged.Zhang et al. (2022) leverage a generative PLM to produce optimized prompts.']","Probing inputs contribute substantially to the probing procedure.PLMs are sensitive to the inputs (Petroni et al., 2019;Jiang et al., 2020b;Elazar et al., 2021), and even syntactical variations or distractors, that do not alter the meaning, cause the PLM's predictions to change (Heinzerling and Inui, 2021;Longpre et al., 2021;Pandia and Ettinger, 2021;Podkorytov et al., 2021;Li et al., 2022a).Therefore, depending on the probing inputs, the estimate on factual knowledge we obtain may vary significantly.Optimized inputs represent variations of the inputs, where the inputs are changed to account for the sensitivity of the probed PLMs.

Diversification and mining methods aim to diversify and optimize prompts by mining Wikipedia or other resources, and selecting the best performing prompts or a combination of them.For example, Jiang et al. (2020b) propose a mining-based and a paraphrasing-based approach to create alternative prompts that outperform manual ones.The final prompts are selected based on their performance on a training set, and can also be combined in an ensemble.Bouraoui et al. (2020) mine for prompts that contain the entities of interest, and filter these based on the ability of the probed PLMs to predict the masked objects.After the filtering step, the remaining prompts are utilized to create a dataset that consists of positive inputs, i.e., containing true subject-object pairs, and negative inputs, which contain false pairs.This dataset is then used for the final evaluation.

Direct optimization methods aim to directly optimize existing prompts.This optimization happens either in a discrete space, to keep the prompts in natural language, or in a continuous space where the prompts do not have to correspond to specific tokens from the vocabulary.Optimization could also target only the masked token or the order of the examples in the prompt, in case a few examples are provided in the prompt to better indicate the task.Shin et al. (2020)'s AUTOPROMPT extends manually created prompts by prompts with a pre-defined number of trigger tokens, and employs gradientbased search to sequentially replace the trigger tokens with concrete tokens.These tokens are chosen to increase the probability of predicting the correct object.OPTIPROMPT (Zhong et al., 2021) is sim-ilar to AUTOPROMPT, but allows for the trigger tokens to be replaced with vectors from a continuous embedding space.In a similar fashion, Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation.Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors.The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs.Newman et al. (2022) utilize adapters (Houlsby et al., 2019) to map the embedding vectors to continuous prompts in order to make the probed PLMs less sensitive to different phrasings of the same prompts.Saeed and Papotti (2022) augment the masked tokens with a special type of embeddings, called Type Embeddings.These embeddings are derived from several entities that share the same type, and are shown to help tie the probed PLM's predictions to the expected type of the masked entity.PERO (Kumar and Talukdar, 2021) depends on querying PLMs with prompts containing few training examples (or shots), which demonstrate the task to the queried PLMs.Since PLMs are quite sensitive to the order and the quality of the provided training examples in the prompt, PERO leverages a genetic algorithm to find an optimized prompt and a separator token to concatenate the examples in the prompts.(Li et al., 2022c) exploit the symmetry of the task, and optimize prompts in a continuous space so that the probability of predicting both the subject and the object is maximized using the resulting prompts.

Generation with PLM methods re-write prompts with the help of a secondary PLM.Haviv et al. (2021) re-write manual prompts using another version of the probed model.The re-writing model is trained to produce prompts that help extract more knowledge from the probed one, which is kept unchanged.Zhang et al. (2022) leverage a generative PLM to produce optimized prompts.","(p4.0) Probing inputs contribute substantially to the probing procedure.PLMs are sensitive to the inputs (Petroni et al., 2019;Jiang et al., 2020b;Elazar et al., 2021), and even syntactical variations or distractors, that do not alter the meaning, cause the PLM's predictions to change (Heinzerling and Inui, 2021;Longpre et al., 2021;Pandia and Ettinger, 2021;Podkorytov et al., 2021;Li et al., 2022a).Therefore, depending on the probing inputs, the estimate on factual knowledge we obtain may vary significantly.Optimized inputs represent variations of the inputs, where the inputs are changed to account for the sensitivity of the probed PLMs.

(p4.1) Diversification and mining methods aim to diversify and optimize prompts by mining Wikipedia or other resources, and selecting the best performing prompts or a combination of them.For example, Jiang et al. (2020b) propose a mining-based and a paraphrasing-based approach to create alternative prompts that outperform manual ones.The final prompts are selected based on their performance on a training set, and can also be combined in an ensemble.Bouraoui et al. (2020) mine for prompts that contain the entities of interest, and filter these based on the ability of the probed PLMs to predict the masked objects.After the filtering step, the remaining prompts are utilized to create a dataset that consists of positive inputs, i.e., containing true subject-object pairs, and negative inputs, which contain false pairs.This dataset is then used for the final evaluation.

(p4.2) Direct optimization methods aim to directly optimize existing prompts.This optimization happens either in a discrete space, to keep the prompts in natural language, or in a continuous space where the prompts do not have to correspond to specific tokens from the vocabulary.Optimization could also target only the masked token or the order of the examples in the prompt, in case a few examples are provided in the prompt to better indicate the task.Shin et al. (2020)'s AUTOPROMPT extends manually created prompts by prompts with a pre-defined number of trigger tokens, and employs gradientbased search to sequentially replace the trigger tokens with concrete tokens.These tokens are chosen to increase the probability of predicting the correct object.OPTIPROMPT (Zhong et al., 2021) is sim-ilar to AUTOPROMPT, but allows for the trigger tokens to be replaced with vectors from a continuous embedding space.In a similar fashion, Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation.Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors.The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs.Newman et al. (2022) utilize adapters (Houlsby et al., 2019) to map the embedding vectors to continuous prompts in order to make the probed PLMs less sensitive to different phrasings of the same prompts.Saeed and Papotti (2022) augment the masked tokens with a special type of embeddings, called Type Embeddings.These embeddings are derived from several entities that share the same type, and are shown to help tie the probed PLM's predictions to the expected type of the masked entity.PERO (Kumar and Talukdar, 2021) depends on querying PLMs with prompts containing few training examples (or shots), which demonstrate the task to the queried PLMs.Since PLMs are quite sensitive to the order and the quality of the provided training examples in the prompt, PERO leverages a genetic algorithm to find an optimized prompt and a separator token to concatenate the examples in the prompts.(Li et al., 2022c) exploit the symmetry of the task, and optimize prompts in a continuous space so that the probability of predicting both the subject and the object is maximized using the resulting prompts.

(p4.3) Generation with PLM methods re-write prompts with the help of a secondary PLM.Haviv et al. (2021) re-write manual prompts using another version of the probed model.The re-writing model is trained to produce prompts that help extract more knowledge from the probed one, which is kept unchanged.Zhang et al. (2022) leverage a generative PLM to produce optimized prompts.","[['b65', 'b33', 'b82', 'b79', 'b83', 'b59', 'b23', 'b42'], ['b6', 'b42'], ['b53', 'b36', 'b85', 'b92', 'b117', 'b75', 'b96', 'b61'], ['b114', 'b32']]","[['b65', 'b33', 'b82', 'b79', 'b83', 'b59', 'b23', 'b42'], ['b6', 'b42'], ['b53', 'b36', 'b85', 'b92', 'b117', 'b75', 'b96', 'b61'], ['b114', 'b32']]",20,"1. Probing inputs contribute substantially to the probing procedure.
2. PLMs are sensitive to the inputs (Petroni et al., 2019;Jiang et al., 2020b;Elazar et al., 2021), and even syntactical variations or distractors, that do not alter the meaning, cause the PLM's predictions to change (Heinzerling and Inui, 2021;Longpre et al., 2021;Pandia and Ettinger, 2021;Podkorytov et al., 2021;Li et al., 2022a).Therefore, depending on the probing inputs, the estimate on factual knowledge we obtain may vary significantly.
3. Optimized inputs represent variations of the inputs, where the inputs are changed to account for the sensitivity of the probed PLMs.
4. Diversification and mining methods aim to diversify and optimize prompts by mining Wikipedia or other resources, and selecting the best performing prompts or a combination of them.
5. For example, Jiang et al. (2020b) propose a mining-based and a paraphrasing-based approach to create alternative prompts that outperform manual ones.
6. The final prompts are selected based on their performance on a training set, and can also be combined in an ensemble.
7. Bouraoui et al. (2020) mine for prompts that contain the entities of interest, and filter these based on the ability of the probed PLMs to predict the masked objects.
8. After the filtering step, the remaining prompts are utilized to create a dataset that consists of positive inputs, i.e., containing true subject-object pairs, and negative inputs, which contain false pairs.
9. This dataset is then used for the final evaluation.
10. Direct optimization methods aim to directly optimize existing prompts.
11. This optimization happens either in a discrete space, to keep the prompts in natural language, or in a continuous space where the prompts do not have to correspond to specific tokens from the vocabulary.
12. Optimization could also target only the masked token or the order of the examples in the prompt, in case a few examples are provided in the prompt to better indicate the task.
13. Shin et al. (2020)'s AUTOPROMPT extends manually created prompts by prompts with a pre-defined number of trigger tokens, and employs gradientbased search to sequentially replace the trigger tokens with concrete tokens.
14. These tokens are chosen to increase the probability of predicting the correct object.
15. OPTIPROMPT (Zhong et al., 2021) is sim-ilar to AUTOPROMPT, but allows for the trigger tokens to be replaced with vectors from a continuous embedding space.
16. In a similar fashion, Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation.
17. Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors.
18. The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs.
19. Newman et al. (2022) utilize adapters (Houlsby et al., 2019) to map the embedding vectors to continuous prompts in order to make the probed PLMs less sensitive to different phrasings of the same prompts.
20. Saeed and Papotti (2022) augment the masked tokens with a special type of embeddings, called Type Embeddings.
21. These embeddings are derived from several entities that share the same type, and are shown to help tie the probed PLM's predictions to the expected type of the masked entity.
22. PERO (Kumar and Talukdar, 2021) depends on querying PLMs with prompts containing few training examples (or shots), which demonstrate the task to the queried PLMs.
23. Since PLMs are quite sensitive to the order and the quality of the provided training examples in the prompt, PERO leverages a genetic algorithm to find an optimized prompt and a separator token to concatenate the examples in the prompts.(Li et al., 2022c) exploit the symmetry of the task, and optimize prompts in a continuous space so that the probability of predicting both the subject and the object is maximized using the resulting prompts.
24. Generation with PLM methods re-write prompts with the help of a secondary PLM.Haviv et al. (2021) re-write manual prompts using another version of the probed model.
25. The re-writing model is trained to produce prompts that help extract more knowledge from the probed one, which is kept unchanged.
26. Zhang et al. (2022) leverage a generative PLM to produce optimized prompts."
264451714,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/18d18d4ffdc070868ce06f216a2a8d040d42a4cb,s6,Vanilla PLMs,"['p6.0', 'p6.1']","['Methods in this category do not induce any changes to the probed PLMs, and depend on pre-training ob-jectives to probe PLMs for factual knowledge.Using the pre-trained parameters is the most straightforward approach and is claimed to preserve the facts learned during pre-training (Elazar et al., 2021;Newman et al., 2022).', ""Most methods leverage the language modeling objectives from pre-training to probe for factual knowledge (Petroni et al., 2019;Jiang et al., 2020b;Shin et al., 2020;Haviv et al., 2021;Kumar and Talukdar, 2021;Zhong et al., 2021;Kalo and Fichtel, 2022;Newman et al., 2022;Onoe et al., 2022;Saeed and Papotti, 2022).Other methods rely on representations that come from the model's body, discarding task-specific parameters altogether (e.g., the Masked Language Modeling head in BERT-like models) (Liétard et al., 2021) or use representations of the subject and object entities in the case of static word embeddings (Dufter et al., 2021).""]","Methods in this category do not induce any changes to the probed PLMs, and depend on pre-training ob-jectives to probe PLMs for factual knowledge.Using the pre-trained parameters is the most straightforward approach and is claimed to preserve the facts learned during pre-training (Elazar et al., 2021;Newman et al., 2022).

Most methods leverage the language modeling objectives from pre-training to probe for factual knowledge (Petroni et al., 2019;Jiang et al., 2020b;Shin et al., 2020;Haviv et al., 2021;Kumar and Talukdar, 2021;Zhong et al., 2021;Kalo and Fichtel, 2022;Newman et al., 2022;Onoe et al., 2022;Saeed and Papotti, 2022).Other methods rely on representations that come from the model's body, discarding task-specific parameters altogether (e.g., the Masked Language Modeling head in BERT-like models) (Liétard et al., 2021) or use representations of the subject and object entities in the case of static word embeddings (Dufter et al., 2021).","(p6.0) Methods in this category do not induce any changes to the probed PLMs, and depend on pre-training ob-jectives to probe PLMs for factual knowledge.Using the pre-trained parameters is the most straightforward approach and is claimed to preserve the facts learned during pre-training (Elazar et al., 2021;Newman et al., 2022).

(p6.1) Most methods leverage the language modeling objectives from pre-training to probe for factual knowledge (Petroni et al., 2019;Jiang et al., 2020b;Shin et al., 2020;Haviv et al., 2021;Kumar and Talukdar, 2021;Zhong et al., 2021;Kalo and Fichtel, 2022;Newman et al., 2022;Onoe et al., 2022;Saeed and Papotti, 2022).Other methods rely on representations that come from the model's body, discarding task-specific parameters altogether (e.g., the Masked Language Modeling head in BERT-like models) (Liétard et al., 2021) or use representations of the subject and object entities in the case of static word embeddings (Dufter et al., 2021).","[['b23', 'b75'], ['b53', 'b77', 'b32', 'b62', 'b82', 'b92', 'b47', 'b117', 'b75', 'b96', 'b22', 'b42']]","[['b23', 'b75'], ['b53', 'b77', 'b32', 'b62', 'b82', 'b92', 'b47', 'b117', 'b75', 'b96', 'b22', 'b42']]",14,"1. Methods in this category do not induce any changes to the probed PLMs, and depend on pre-training ob-jectives to probe PLMs for factual knowledge.
2. Using the pre-trained parameters is the most straightforward approach and is claimed to preserve the facts learned during pre-training (Elazar et al., 2021;Newman et al., 2022).
3. Most methods leverage the language modeling objectives from pre-training to probe for factual knowledge (Petroni et al., 2019;Jiang et al., 2020b;Shin et al., 2020;Haviv et al., 2021;Kumar and Talukdar, 2021;Zhong et al., 2021;Kalo and Fichtel, 2022;Newman et al., 2022;Onoe et al., 2022;Saeed and Papotti, 2022).Other methods rely on representations that come from the model's body, discarding task-specific parameters altogether (e.g., the Masked Language Modeling head in BERT-like models)
4. (Liétard et al., 2021) or use representations of the subject and object entities in the case of static word embeddings (Dufter et al., 2021)."
264833196,Transformers as Recognizers of Formal Languages: A Survey on Expressivity,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/6624065bd2825f500e93624671793d963187d066,s34,First-order logic with majority,['p34.0'],"['Merrill and Sabharwal (2023a) further tighten the L-uniform TC 0 upper bound of Merrill and Sabharwal (2023b) to DLOGTIME-uniform TC 0 , and therefore FOM [BIT].The proof constructs subroutines to answer queries about the types of nodes and connectivity of pairs of nodes in the computation graph of a transformer, and shows that these queries can be translated to queries for a TC 0 circuit family with  (log ) time overhead.which is first-order logic with counting quantifiers, using two sorts for positions and counts (Immerman, 1999, p. 185-187), where positions have the MOD predicate (but not < or =), and counts have <, +, and =, capturing the fact that transformers can add and compare activations, but not positions.They show that this logic is intermediate in expressivity between  (1)-precision and infinite-precision transformers.The lower-bound proof makes use of a normal form that eliminates quantifiers over counts and makes quantifiers over positions have depth 1; a perhaps surprising consequence is that  (1)-precision transformers are no more powerful than 2-layer uniform-attention transformers.Weiss et al. (2021) define a programming language called RASP (Restricted Access Sequence Programming Language) and show that it can be compiled to transformers with average-hard attention and two extensions: • Attention weights are directly computed from the previous layer without being confined to dotproducts of query and key vectors.• Position-wise FFNs compute arbitrary computable functions.Lindner et al. (2023) describe a RASP compiler that outputs standard transformers.It compiles RASP selectors to dot-product attention, with syntactic restrictions on selectors and a maximum string length.Element-wise operations are approximately compiled to ReLU FFNs.Friedman et al. (2023) define Transformer Programs, a restricted class of transformers that can be translated into RASP programs.']","Merrill and Sabharwal (2023a) further tighten the L-uniform TC 0 upper bound of Merrill and Sabharwal (2023b) to DLOGTIME-uniform TC 0 , and therefore FOM [BIT].The proof constructs subroutines to answer queries about the types of nodes and connectivity of pairs of nodes in the computation graph of a transformer, and shows that these queries can be translated to queries for a TC 0 circuit family with  (log ) time overhead.which is first-order logic with counting quantifiers, using two sorts for positions and counts (Immerman, 1999, p. 185-187), where positions have the MOD predicate (but not < or =), and counts have <, +, and =, capturing the fact that transformers can add and compare activations, but not positions.They show that this logic is intermediate in expressivity between  (1)-precision and infinite-precision transformers.The lower-bound proof makes use of a normal form that eliminates quantifiers over counts and makes quantifiers over positions have depth 1; a perhaps surprising consequence is that  (1)-precision transformers are no more powerful than 2-layer uniform-attention transformers.Weiss et al. (2021) define a programming language called RASP (Restricted Access Sequence Programming Language) and show that it can be compiled to transformers with average-hard attention and two extensions: • Attention weights are directly computed from the previous layer without being confined to dotproducts of query and key vectors.• Position-wise FFNs compute arbitrary computable functions.Lindner et al. (2023) describe a RASP compiler that outputs standard transformers.It compiles RASP selectors to dot-product attention, with syntactic restrictions on selectors and a maximum string length.Element-wise operations are approximately compiled to ReLU FFNs.Friedman et al. (2023) define Transformer Programs, a restricted class of transformers that can be translated into RASP programs.","(p34.0) Merrill and Sabharwal (2023a) further tighten the L-uniform TC 0 upper bound of Merrill and Sabharwal (2023b) to DLOGTIME-uniform TC 0 , and therefore FOM [BIT].The proof constructs subroutines to answer queries about the types of nodes and connectivity of pairs of nodes in the computation graph of a transformer, and shows that these queries can be translated to queries for a TC 0 circuit family with  (log ) time overhead.which is first-order logic with counting quantifiers, using two sorts for positions and counts (Immerman, 1999, p. 185-187), where positions have the MOD predicate (but not < or =), and counts have <, +, and =, capturing the fact that transformers can add and compare activations, but not positions.They show that this logic is intermediate in expressivity between  (1)-precision and infinite-precision transformers.The lower-bound proof makes use of a normal form that eliminates quantifiers over counts and makes quantifiers over positions have depth 1; a perhaps surprising consequence is that  (1)-precision transformers are no more powerful than 2-layer uniform-attention transformers.Weiss et al. (2021) define a programming language called RASP (Restricted Access Sequence Programming Language) and show that it can be compiled to transformers with average-hard attention and two extensions: • Attention weights are directly computed from the previous layer without being confined to dotproducts of query and key vectors.• Position-wise FFNs compute arbitrary computable functions.Lindner et al. (2023) describe a RASP compiler that outputs standard transformers.It compiles RASP selectors to dot-product attention, with syntactic restrictions on selectors and a maximum string length.Element-wise operations are approximately compiled to ReLU FFNs.Friedman et al. (2023) define Transformer Programs, a restricted class of transformers that can be translated into RASP programs.","[['b20', 'b73', 'b36', 'b45', None]]","[['b20', 'b73', 'b36', 'b45', None]]",5,"1. Merrill and Sabharwal (2023a) further tighten the L-uniform TC 0 upper bound of Merrill and Sabharwal (2023b) to DLOGTIME-uniform TC 0 , and therefore FOM [BIT].The proof constructs subroutines to answer queries about the types of nodes and connectivity of pairs of nodes in the computation graph of a transformer, and shows that these queries can be translated to queries for a TC 0 circuit family with  (log ) time overhead.which is first-order logic with counting quantifiers, using two sorts for positions and counts (Immerman, 1999, p. 185-187), where positions have the MOD predicate (but not < or =), and counts have <, +, and =, capturing the fact that transformers can add and compare activations, but not positions.
2. They show that this logic is intermediate in expressivity between  (1)-precision and infinite-precision transformers.
3. The lower-bound proof makes use of a normal form that eliminates quantifiers over counts and makes quantifiers over positions have depth 1; a perhaps surprising consequence is that  (1)-precision transformers are no more powerful than 2-layer uniform-attention transformers.
4. Weiss et al. (2021) define a programming language called RASP (Restricted Access Sequence Programming Language) and show that it can be compiled to transformers with average-hard attention and two extensions: • Attention weights are directly computed from the previous layer without being confined to dotproducts of query and key vectors.• Position-wise FFNs compute arbitrary computable functions.
5. Lindner et al. (2023) describe a RASP compiler that outputs standard transformers.
6. It compiles RASP selectors to dot-product attention, with syntactic restrictions on selectors and a maximum string length.
7. Element-wise operations are approximately compiled to ReLU FFNs.
8. Friedman et al. (2023) define Transformer Programs, a restricted class of transformers that can be translated into RASP programs."
264833196,Transformers as Recognizers of Formal Languages: A Survey on Expressivity,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/6624065bd2825f500e93624671793d963187d066,s30,Other languages,['p30.0'],"['The language MAJORITY, like PARITY, is not in AC 0 , but Pérez et al. (2019) prove that a transformer encoder (rather, a transformer encoder-decoder with a trivial decoder) without any position encoding recognizes MAJORITY; Merrill et al. (2022) prove the same for transformer encoders.Bhattamishra et al. (2020a) experiment with training transformer encoders on some counter languages: DYCK-1, SHUFFLE-DYCK- for  ∈ {2, 4, 6}, -ary boolean expressions in prefix notation for  ∈ {3, 5}, {a  b  }, {a  b  c  }, {a  b  c  d  }, observing nearly perfect learning and generalization to longer lengths with masking and no positional encoding.They also experiment with learning regular languages including the Tomita (1981) languages, PARITY, (aa) * , (abab) * , and DYCK-(1, ) for  ∈ {2, 3, 4, 12}, where they find that only the star-free languages of dot-depth 1 are learned and generalized well.Delétang et al. (2023) study experimentally how well transformer encoders (and other networks) learn tasks at various levels of the Chomsky hierarchy, including generalization to longer strings.Languages include aΣ * a + bΣ * b, PARITY and MAJORITY.Of these three languages, they find that transformers learn only MAJORITY.3) takes the negative absolute value of the dot-product, and Eq. ( 4) uses average-hard attention.• The FFNs use sigmoids instead of ReLUs.As described above ( §4.3.3), the decoder is allowed to run for arbitrarily many time steps until an acceptance criterion is met.Under these assumptions, transformer encoder-decoders can recognize any recursively enumerable language. 2his result uses arbitrary precision, but as a corollary, Pérez et al. (2021) show that a  ()time-bounded Turing machine can be simulated in a transformer using  (log  ()) bits of precision.2023) study the ability of transformer decoders to simulate deterministic finite automata (DFAs), in the sense of computing not only the same acceptance decision but also the same state sequence.Although a transformer with depth  can simulate a DFA for  timesteps, Liu et al. show how to construct lower-depth shortcuts for subclasses roughly corresponding to classes of regular languages in Fig. 1.These constructions depend on , but in the context of this survey, a noteworthy finding is that any regular language in ACC 0 can be recognized up to length  by a transformer decoder whose FFNs use sine activations and whose number of parameters is independent of  (although the parameters themselves do depend on ).']","The language MAJORITY, like PARITY, is not in AC 0 , but Pérez et al. (2019) prove that a transformer encoder (rather, a transformer encoder-decoder with a trivial decoder) without any position encoding recognizes MAJORITY; Merrill et al. (2022) prove the same for transformer encoders.Bhattamishra et al. (2020a) experiment with training transformer encoders on some counter languages: DYCK-1, SHUFFLE-DYCK- for  ∈ {2, 4, 6}, -ary boolean expressions in prefix notation for  ∈ {3, 5}, {a  b  }, {a  b  c  }, {a  b  c  d  }, observing nearly perfect learning and generalization to longer lengths with masking and no positional encoding.They also experiment with learning regular languages including the Tomita (1981) languages, PARITY, (aa) * , (abab) * , and DYCK-(1, ) for  ∈ {2, 3, 4, 12}, where they find that only the star-free languages of dot-depth 1 are learned and generalized well.Delétang et al. (2023) study experimentally how well transformer encoders (and other networks) learn tasks at various levels of the Chomsky hierarchy, including generalization to longer strings.Languages include aΣ * a + bΣ * b, PARITY and MAJORITY.Of these three languages, they find that transformers learn only MAJORITY.3) takes the negative absolute value of the dot-product, and Eq. ( 4) uses average-hard attention.• The FFNs use sigmoids instead of ReLUs.As described above ( §4.3.3), the decoder is allowed to run for arbitrarily many time steps until an acceptance criterion is met.Under these assumptions, transformer encoder-decoders can recognize any recursively enumerable language. 2his result uses arbitrary precision, but as a corollary, Pérez et al. (2021) show that a  ()time-bounded Turing machine can be simulated in a transformer using  (log  ()) bits of precision.2023) study the ability of transformer decoders to simulate deterministic finite automata (DFAs), in the sense of computing not only the same acceptance decision but also the same state sequence.Although a transformer with depth  can simulate a DFA for  timesteps, Liu et al. show how to construct lower-depth shortcuts for subclasses roughly corresponding to classes of regular languages in Fig. 1.These constructions depend on , but in the context of this survey, a noteworthy finding is that any regular language in ACC 0 can be recognized up to length  by a transformer decoder whose FFNs use sine activations and whose number of parameters is independent of  (although the parameters themselves do depend on ).","(p30.0) The language MAJORITY, like PARITY, is not in AC 0 , but Pérez et al. (2019) prove that a transformer encoder (rather, a transformer encoder-decoder with a trivial decoder) without any position encoding recognizes MAJORITY; Merrill et al. (2022) prove the same for transformer encoders.Bhattamishra et al. (2020a) experiment with training transformer encoders on some counter languages: DYCK-1, SHUFFLE-DYCK- for  ∈ {2, 4, 6}, -ary boolean expressions in prefix notation for  ∈ {3, 5}, {a  b  }, {a  b  c  }, {a  b  c  d  }, observing nearly perfect learning and generalization to longer lengths with masking and no positional encoding.They also experiment with learning regular languages including the Tomita (1981) languages, PARITY, (aa) * , (abab) * , and DYCK-(1, ) for  ∈ {2, 3, 4, 12}, where they find that only the star-free languages of dot-depth 1 are learned and generalized well.Delétang et al. (2023) study experimentally how well transformer encoders (and other networks) learn tasks at various levels of the Chomsky hierarchy, including generalization to longer strings.Languages include aΣ * a + bΣ * b, PARITY and MAJORITY.Of these three languages, they find that transformers learn only MAJORITY.3) takes the negative absolute value of the dot-product, and Eq. ( 4) uses average-hard attention.• The FFNs use sigmoids instead of ReLUs.As described above ( §4.3.3), the decoder is allowed to run for arbitrarily many time steps until an acceptance criterion is met.Under these assumptions, transformer encoder-decoders can recognize any recursively enumerable language. 2his result uses arbitrary precision, but as a corollary, Pérez et al. (2021) show that a  ()time-bounded Turing machine can be simulated in a transformer using  (log  ()) bits of precision.2023) study the ability of transformer decoders to simulate deterministic finite automata (DFAs), in the sense of computing not only the same acceptance decision but also the same state sequence.Although a transformer with depth  can simulate a DFA for  timesteps, Liu et al. show how to construct lower-depth shortcuts for subclasses roughly corresponding to classes of regular languages in Fig. 1.These constructions depend on , but in the context of this survey, a noteworthy finding is that any regular language in ACC 0 can be recognized up to length  by a transformer decoder whose FFNs use sine activations and whose number of parameters is independent of  (although the parameters themselves do depend on ).","[['b56', 'b8', 'b16', 'b54', 'b46', 'b68']]","[['b56', 'b8', 'b16', 'b54', 'b46', 'b68']]",6,"1. The language MAJORITY, like PARITY, is not in AC 0 , but Pérez et al. (2019) prove that a transformer encoder (rather, a transformer encoder-decoder with a trivial decoder) without any position encoding recognizes MAJORITY; Merrill et al. (2022) prove the same for transformer encoders.
2. Bhattamishra et al. (2020a) experiment with training transformer encoders on some counter languages: DYCK-1, SHUFFLE-DYCK- for  ∈ {2, 4, 6}, -ary boolean expressions in prefix notation for  ∈ {3, 5}, {a  b  }, {a  b  c  }, {a  b  c  d  }, observing nearly perfect learning and generalization to longer lengths with masking and no positional encoding.
3. They also experiment with learning regular languages including the Tomita (1981) languages, PARITY, (aa) * , (abab) * , and DYCK-(1, ) for  ∈ {2, 3, 4, 12}, where they find that only the star-free languages of dot-depth 1 are learned and generalized well.
4. Delétang et al. (2023) study experimentally how well transformer encoders (and other networks) learn tasks at various levels of the Chomsky hierarchy, including generalization to longer strings.
5. Languages include aΣ * a + bΣ * b, PARITY and MAJORITY.Of these three languages, they find that transformers learn only MAJORITY.3) takes the negative absolute value of the dot-product, and Eq. ( 4) uses average-hard
6. attention.• The FFNs use sigmoids instead of ReLUs.
7. As described above ( §4.3.3), the decoder is allowed to run for arbitrarily many time steps until an acceptance criterion is met.
8. Under these assumptions, transformer encoder-decoders can recognize any recursively enumerable language.
9. 2his result uses arbitrary precision, but as a corollary, Pérez et al. (2021) show that a  ()time-bounded Turing machine can be simulated in a transformer using  (log  ()) bits of precision.2023) study the ability of transformer decoders to simulate deterministic finite automata (DFAs), in the sense of computing not only the same acceptance decision but also the same state sequence.
10. Although a transformer with depth  can simulate a DFA for  timesteps, Liu et al. show how to construct lower-depth shortcuts for subclasses roughly corresponding to classes of regular languages in Fig. 1.These constructions depend on , but in the context of this survey, a noteworthy finding is that any regular language in ACC 0 can be recognized up to length  by a transformer decoder whose FFNs use sine activations and whose number of parameters is independent of  (although the parameters themselves do depend on )."
264833196,Transformers as Recognizers of Formal Languages: A Survey on Expressivity,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/6624065bd2825f500e93624671793d963187d066,s28,PARITY,"['p28.0', 'p28.1', 'p28.2', 'p28.3']","[""As the classic example of a language in (uniform) TC 0 but not AC 0 (Ajtai, 1983;Furst et al., 1984), PARITY is a particularly interesting case-study.Hahn (2020) showed that leftmost-hard attention transformers cannot recognize PARITY, using a variant of Furst et al.'s random restriction method.He also showed that softmax attention transformers cannot generate PARITY under the following two conditions: • all position-wise functions are Lipschitzcontinuous, and"", '• generation is defined using the KL divergence criterion in Eq. ( 5).', ""On the other hand, Chiang and Cholak (2022) showed that transformer encoders whose PE includes / do recognize PARITY.They give two constructions, corresponding to Hahn's two assumptions.The first has Lipschitz-continuous position-wise functions, but has high cross-entropy ( §4.3.1); as a generator, it would not meet criterion (5).The second construction uses layernorm with  N = 0, which is not Lipschitz-continuous, but it has arbitrarily low cross-entropy."", ""The apparent contradiction is resolved by considering the different assumptions underlying each result.The fact that PARITY's recognizability is so sensitive to assumptions suggests that it is close to the borderline of what transformer encoders can recognize.Empirically, several authors (Bhattamishra et al., 2020a;Delétang et al., 2023) have found that transformer encoders do not learn PAR-ITY.""]","As the classic example of a language in (uniform) TC 0 but not AC 0 (Ajtai, 1983;Furst et al., 1984), PARITY is a particularly interesting case-study.Hahn (2020) showed that leftmost-hard attention transformers cannot recognize PARITY, using a variant of Furst et al.'s random restriction method.He also showed that softmax attention transformers cannot generate PARITY under the following two conditions: • all position-wise functions are Lipschitzcontinuous, and

• generation is defined using the KL divergence criterion in Eq. ( 5).

On the other hand, Chiang and Cholak (2022) showed that transformer encoders whose PE includes / do recognize PARITY.They give two constructions, corresponding to Hahn's two assumptions.The first has Lipschitz-continuous position-wise functions, but has high cross-entropy ( §4.3.1); as a generator, it would not meet criterion (5).The second construction uses layernorm with  N = 0, which is not Lipschitz-continuous, but it has arbitrarily low cross-entropy.

The apparent contradiction is resolved by considering the different assumptions underlying each result.The fact that PARITY's recognizability is so sensitive to assumptions suggests that it is close to the borderline of what transformer encoders can recognize.Empirically, several authors (Bhattamishra et al., 2020a;Delétang et al., 2023) have found that transformer encoders do not learn PAR-ITY.","(p28.0) As the classic example of a language in (uniform) TC 0 but not AC 0 (Ajtai, 1983;Furst et al., 1984), PARITY is a particularly interesting case-study.Hahn (2020) showed that leftmost-hard attention transformers cannot recognize PARITY, using a variant of Furst et al.'s random restriction method.He also showed that softmax attention transformers cannot generate PARITY under the following two conditions: • all position-wise functions are Lipschitzcontinuous, and

(p28.1) • generation is defined using the KL divergence criterion in Eq. ( 5).

(p28.2) On the other hand, Chiang and Cholak (2022) showed that transformer encoders whose PE includes / do recognize PARITY.They give two constructions, corresponding to Hahn's two assumptions.The first has Lipschitz-continuous position-wise functions, but has high cross-entropy ( §4.3.1); as a generator, it would not meet criterion (5).The second construction uses layernorm with  N = 0, which is not Lipschitz-continuous, but it has arbitrarily low cross-entropy.

(p28.3) The apparent contradiction is resolved by considering the different assumptions underlying each result.The fact that PARITY's recognizability is so sensitive to assumptions suggests that it is close to the borderline of what transformer encoders can recognize.Empirically, several authors (Bhattamishra et al., 2020a;Delétang et al., 2023) have found that transformer encoders do not learn PAR-ITY.","[['b21', 'b23', 'b1'], [], [], ['b16', 'b8']]","[['b21', 'b23', 'b1'], [], [], ['b16', 'b8']]",5,"1. As the classic example of a language in (uniform)
2. TC 0 but not AC 0 (Ajtai, 1983;Furst et al., 1984), PARITY is a particularly interesting case-study.
3. Hahn (2020) showed that leftmost-hard attention transformers cannot recognize PARITY, using a variant of Furst et al.'s random restriction method.
4. He also showed that softmax attention transformers cannot generate PARITY under the following two conditions: • all position-wise functions are Lipschitzcontinuous, and• generation is defined using the KL divergence criterion in Eq. ( 5).
5. On the other hand, Chiang and Cholak (2022) showed that transformer encoders whose PE includes / do recognize PARITY.They give two constructions, corresponding to Hahn's two assumptions.
6. The first has Lipschitz-continuous position-wise functions, but has high cross-entropy ( §4.3.1); as a generator, it would not meet criterion
7. (5).The second construction uses layernorm with  N = 0, which is not Lipschitz-continuous, but it has arbitrarily low cross-entropy.
8. The apparent contradiction is resolved by considering the different assumptions underlying each result.
9. The fact that PARITY's recognizability is so sensitive to assumptions suggests that it is close to the borderline of what transformer encoders can recognize.
10. Empirically, several authors (Bhattamishra et al., 2020a;Delétang et al., 2023) have found that transformer encoders do not learn PAR-ITY."
264833196,Transformers as Recognizers of Formal Languages: A Survey on Expressivity,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/6624065bd2825f500e93624671793d963187d066,s24,Beyond TC 0,"['p24.0', 'p24.1', 'p24.2', 'p24.3', 'p24.4', 'p24.5', 'p24.6']","['As we will see ( §7.3.2),some transformer variants lie within uniform TC 0 .What problems are beyond uniform TC 0 ?', 'Permutations and the language W( 5 ) A permutation of [] is a bijection  : [] → [], and   is the set of all permutations of [𝑘].We can write one as a list, for example, [0, 3, 1, 2] is the permutation that maps 0 to 0, 1 to 3, and so on.Composition of permutations is associative but not commutative.For example (applying the permutations from left to right):', 'By treating   as an alphabet and compositions of permutations as strings, we can define the language W(  ) of compositions of permutations of [] that equal the identity permutation.For example, W(', '. This is straightforward for finite automata to recognize-each state represents the current composed permutation-but difficult when given only fixed computation depth.In particular, the language W( 5 ) is known to be complete for NC 1 under DLOGTIME-uniform AC 0 reductions (Barrington, 1989).Thus it is not in DLOGTIMEuniform TC 0 assuming that TC 0 ⊊ NC 1 (as is widely believed).This makes it a valuable example of a regular language that transformer encoders probably cannot recognize.', ""The languages W(  ) have some relevance to natural language: Paperno (2022) study expressions like the child of the enemy of Ann where the interpretation of the child of is (roughly) a permutation of possible referents.Additionally, W(  ) resembles permutation problems that have been used to benchmark transformers' state-tracking abilities (Kim and Schuster, 2023)."", 'Other languages Besides W( 5 ), other problems that are (widely believed to be) not in uniform TC 0 include: • Undirected graph connectivity, which is Lcomplete (Reingold, 2008), so is not in Luniform TC 0 unless L-uniform TC 0 = L.', '• Solving linear equalities, which is P-complete (Greenlaw et al., 1991), so is outside L-uniform TC 0 unless L-uniform TC 0 = P. • Matrix permanent, which is known to be outside of DLOGTIME-uniform TC 0 (Allender, 1999).']","As we will see ( §7.3.2),some transformer variants lie within uniform TC 0 .What problems are beyond uniform TC 0 ?

Permutations and the language W( 5 ) A permutation of [] is a bijection  : [] → [], and   is the set of all permutations of [𝑘].We can write one as a list, for example, [0, 3, 1, 2] is the permutation that maps 0 to 0, 1 to 3, and so on.Composition of permutations is associative but not commutative.For example (applying the permutations from left to right):

By treating   as an alphabet and compositions of permutations as strings, we can define the language W(  ) of compositions of permutations of [] that equal the identity permutation.For example, W(

. This is straightforward for finite automata to recognize-each state represents the current composed permutation-but difficult when given only fixed computation depth.In particular, the language W( 5 ) is known to be complete for NC 1 under DLOGTIME-uniform AC 0 reductions (Barrington, 1989).Thus it is not in DLOGTIMEuniform TC 0 assuming that TC 0 ⊊ NC 1 (as is widely believed).This makes it a valuable example of a regular language that transformer encoders probably cannot recognize.

The languages W(  ) have some relevance to natural language: Paperno (2022) study expressions like the child of the enemy of Ann where the interpretation of the child of is (roughly) a permutation of possible referents.Additionally, W(  ) resembles permutation problems that have been used to benchmark transformers' state-tracking abilities (Kim and Schuster, 2023).

Other languages Besides W( 5 ), other problems that are (widely believed to be) not in uniform TC 0 include: • Undirected graph connectivity, which is Lcomplete (Reingold, 2008), so is not in Luniform TC 0 unless L-uniform TC 0 = L.

• Solving linear equalities, which is P-complete (Greenlaw et al., 1991), so is outside L-uniform TC 0 unless L-uniform TC 0 = P. • Matrix permanent, which is known to be outside of DLOGTIME-uniform TC 0 (Allender, 1999).","(p24.0) As we will see ( §7.3.2),some transformer variants lie within uniform TC 0 .What problems are beyond uniform TC 0 ?

(p24.1) Permutations and the language W( 5 ) A permutation of [] is a bijection  : [] → [], and   is the set of all permutations of [𝑘].We can write one as a list, for example, [0, 3, 1, 2] is the permutation that maps 0 to 0, 1 to 3, and so on.Composition of permutations is associative but not commutative.For example (applying the permutations from left to right):

(p24.2) By treating   as an alphabet and compositions of permutations as strings, we can define the language W(  ) of compositions of permutations of [] that equal the identity permutation.For example, W(

(p24.3) . This is straightforward for finite automata to recognize-each state represents the current composed permutation-but difficult when given only fixed computation depth.In particular, the language W( 5 ) is known to be complete for NC 1 under DLOGTIME-uniform AC 0 reductions (Barrington, 1989).Thus it is not in DLOGTIMEuniform TC 0 assuming that TC 0 ⊊ NC 1 (as is widely believed).This makes it a valuable example of a regular language that transformer encoders probably cannot recognize.

(p24.4) The languages W(  ) have some relevance to natural language: Paperno (2022) study expressions like the child of the enemy of Ann where the interpretation of the child of is (roughly) a permutation of possible referents.Additionally, W(  ) resembles permutation problems that have been used to benchmark transformers' state-tracking abilities (Kim and Schuster, 2023).

(p24.5) Other languages Besides W( 5 ), other problems that are (widely believed to be) not in uniform TC 0 include: • Undirected graph connectivity, which is Lcomplete (Reingold, 2008), so is not in Luniform TC 0 unless L-uniform TC 0 = L.

(p24.6) • Solving linear equalities, which is P-complete (Greenlaw et al., 1991), so is outside L-uniform TC 0 unless L-uniform TC 0 = P. • Matrix permanent, which is known to be outside of DLOGTIME-uniform TC 0 (Allender, 1999).","[[], [None], [], ['b6'], ['b31'], ['b58'], ['b22']]","[[], [None], [], ['b6'], ['b31'], ['b58'], ['b22']]",5,"1. As we will see ( §7.3.2),some transformer variants lie within uniform TC 0
2. .What problems are beyond uniform TC 0 ?
3. Permutations and the language W( 5 )
4. A permutation of [] is a bijection  : [] → [], and   is the set of all permutations of [𝑘].We can write one as a list, for example, [0, 3, 1, 2] is the permutation that maps 0 to 0, 1 to 3, and so on.
5. Composition of permutations is associative but not commutative.
6. For example (applying the permutations from left to right):By treating   as an alphabet and compositions of permutations as strings, we can define the language W(  ) of compositions of permutations of [] that equal the identity permutation.
7. For example, W(. This is straightforward for finite automata to recognize-each state represents the current composed permutation-but difficult when given only fixed computation depth.
8. In particular, the language W( 5 ) is known to be complete for NC 1 under DLOGTIME-uniform AC 0 reductions (Barrington, 1989).Thus it is not in DLOGTIMEuniform TC 0 assuming that TC 0 ⊊ NC 1
9. (as is widely believed).This makes it a valuable example of a regular language that transformer encoders probably cannot recognize.
10. The languages W(  ) have some relevance to natural language: Paperno (2022) study expressions like the child of the enemy of Ann where the interpretation of the child of is (roughly) a permutation of possible referents.
11. Additionally, W(  ) resembles permutation problems that have been used to benchmark transformers' state-tracking abilities (Kim and Schuster, 2023).
12. Other languages Besides W( 5 ), other problems that are (widely believed to be) not in uniform TC 0 include: • Undirected graph connectivity, which is Lcomplete (Reingold, 2008), so is not in Luniform TC 0 unless L-uniform TC 0 = L.• Solving linear equalities, which is P-complete (Greenlaw et al., 1991), so is outside L-uniform TC 0 unless L-uniform TC 0 = P. • Matrix permanent, which is known to be outside of DLOGTIME-uniform TC 0 (Allender, 1999)."
264833196,Transformers as Recognizers of Formal Languages: A Survey on Expressivity,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/6624065bd2825f500e93624671793d963187d066,s20,First-order logic,"['p20.0', 'p20.1', 'p20.2', 'p20.3', 'p20.4', 'p20.5', 'p20.6', 'p20.7', 'p20.8']","['A formal language can also be defined as a set of finite strings that satisfy a closed formula of a logic.For more details, refer to Thomas (1997) or Straubing (1994).', 'For example, in the first-order logic of strings, or FO, the formulas are the smallest set containing: • Variables , , and so on.', '• Atomic formulas   (),  = ,  < , where  ∈ Σ is a symbol and ,  are variables.', ' 2 are formulas.• ∀., ∃.,where  is a variable and  is a formula.Under the intended interpretation, variables stand for positions of a finite string , and   () is true iff   = .For example, ∀.∀. () ∧   () →  <  defines the regular language  *  * .The language defined by a closed formula  consists of those strings that satisfy .', 'The languages definable in FO are exactly the star-free languages (McNaughton and Papert, 1971).', 'We are also interested in variants that add other quantifiers:', '• FOC adds counting quantifiers ∃ = ., which hold iff there are exactly  values of  that make  true (Mix Barrington et al., 1990).• FOM adds majority quantifiers M., which hold iff at least half of the values of  make  true (Mix Barrington et al., 1990).We are also interested in various sets of predicates: • Modular predicates MOD   (), which hold iff  ≡  (mod ) (Mix Barrington et al., 1992).', '• BIT(, ), which holds iff the -th bit of  is 1.', '• ARB, the set of all possible predicates on one or more positions.A logic extended with predicates is conventionally written with the predicates in square brackets; for example, we write FO[BIT] for first-order logic with the BIT predicate.']","A formal language can also be defined as a set of finite strings that satisfy a closed formula of a logic.For more details, refer to Thomas (1997) or Straubing (1994).

For example, in the first-order logic of strings, or FO, the formulas are the smallest set containing: • Variables , , and so on.

• Atomic formulas   (),  = ,  < , where  ∈ Σ is a symbol and ,  are variables.

 2 are formulas.• ∀., ∃.,where  is a variable and  is a formula.Under the intended interpretation, variables stand for positions of a finite string , and   () is true iff   = .For example, ∀.∀. () ∧   () →  <  defines the regular language  *  * .The language defined by a closed formula  consists of those strings that satisfy .

The languages definable in FO are exactly the star-free languages (McNaughton and Papert, 1971).

We are also interested in variants that add other quantifiers:

• FOC adds counting quantifiers ∃ = ., which hold iff there are exactly  values of  that make  true (Mix Barrington et al., 1990).• FOM adds majority quantifiers M., which hold iff at least half of the values of  make  true (Mix Barrington et al., 1990).We are also interested in various sets of predicates: • Modular predicates MOD   (), which hold iff  ≡  (mod ) (Mix Barrington et al., 1992).

• BIT(, ), which holds iff the -th bit of  is 1.

• ARB, the set of all possible predicates on one or more positions.A logic extended with predicates is conventionally written with the predicates in square brackets; for example, we write FO[BIT] for first-order logic with the BIT predicate.","(p20.0) A formal language can also be defined as a set of finite strings that satisfy a closed formula of a logic.For more details, refer to Thomas (1997) or Straubing (1994).

(p20.1) For example, in the first-order logic of strings, or FO, the formulas are the smallest set containing: • Variables , , and so on.

(p20.2) • Atomic formulas   (),  = ,  < , where  ∈ Σ is a symbol and ,  are variables.

(p20.3)  2 are formulas.• ∀., ∃.,where  is a variable and  is a formula.Under the intended interpretation, variables stand for positions of a finite string , and   () is true iff   = .For example, ∀.∀. () ∧   () →  <  defines the regular language  *  * .The language defined by a closed formula  consists of those strings that satisfy .

(p20.4) The languages definable in FO are exactly the star-free languages (McNaughton and Papert, 1971).

(p20.5) We are also interested in variants that add other quantifiers:

(p20.6) • FOC adds counting quantifiers ∃ = ., which hold iff there are exactly  values of  that make  true (Mix Barrington et al., 1990).• FOM adds majority quantifiers M., which hold iff at least half of the values of  make  true (Mix Barrington et al., 1990).We are also interested in various sets of predicates: • Modular predicates MOD   (), which hold iff  ≡  (mod ) (Mix Barrington et al., 1992).

(p20.7) • BIT(, ), which holds iff the -th bit of  is 1.

(p20.8) • ARB, the set of all possible predicates on one or more positions.A logic extended with predicates is conventionally written with the predicates in square brackets; for example, we write FO[BIT] for first-order logic with the BIT predicate.","[['b67', 'b63'], [], [], [], ['b38'], [], ['b49', 'b50'], [], [None]]","[['b67', 'b63'], [], [], [], ['b38'], [], ['b49', 'b50'], [], [None]]",6,"1. A formal language can also be defined as a set of finite strings that satisfy a closed formula of a logic.
2. For more details, refer to Thomas (1997) or Straubing (1994).
3. For example, in the first-order logic of strings, or FO, the formulas are the smallest set containing: • Variables , , and so on.
4. • Atomic formulas   (),  = ,  < , where  ∈ Σ is a symbol and ,  are variables.
5. 2 are formulas.• ∀., ∃.,where  is a variable and  is a formula.
6. Under the intended interpretation, variables stand for positions of a finite string , and   () is true iff   = .For example, ∀.∀. () ∧   () →  <  defines the regular language  *
7. * .The language defined by a closed formula  consists of those strings that satisfy .
8. The languages definable in FO are exactly the star-free languages (McNaughton and Papert, 1971).
9. We are also interested in variants that add other quantifiers:• FOC adds counting quantifiers ∃ = ., which hold iff there are exactly  values of  that make  true (Mix Barrington et al., 1990).• FOM adds majority quantifiers M., which hold iff at least half of the values of  make  true (Mix Barrington et al., 1990).We are also interested in various sets of predicates: • Modular predicates MOD   (), which hold iff  ≡  (mod ) (Mix Barrington et al., 1992).• BIT(, ), which holds iff the -th bit of  is 1.
10. • ARB, the set of all possible predicates on one or more positions.
11. A logic extended with predicates is conventionally written with the predicates in square brackets; for example, we write FO[BIT] for first-order logic with the BIT predicate."
264833196,Transformers as Recognizers of Formal Languages: A Survey on Expressivity,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/6624065bd2825f500e93624671793d963187d066,s18,"Automata and classes L, NL, P","['p18.0', 'p18.1', 'p18.2', 'p18.3', 'p18.4', 'p18.5', 'p18.6']","['We assume familiarity with finite automata and Turing machines; for definitions, please see the textbook by Sipser (2013).Background on counter machines is given by Fischer et al. (1968).The standard deterministic model of a multi-tape Turing machine is used to define the language classes L (languages recognizable in space  (log )) and P (languages recognizable in polynomial time).The definition of NL (languages recognizable in non-deterministic space  (log )) uses a nondeterministic Turing machine.We also consider a random-access model of a deterministic Turing machine, which allows meaningful computation in time  (log ) (Mix Barrington et al., 1990).Problem reductions computable with bounded resources (e.g., in L) are used to define problems complete or hard for various classes of languages.', 'It is known that', 'but none of these inclusions are known to be strict.', '6.2 Circuits and classes AC 0 , ACC 0 , TC 0 , NC 1', 'Circuits are a model of parallel computation particularly relevant to transformers.For more details, please see the textbook by Arora and Barak (2009).Circuits operate on binary values.If we choose a fixed-length encoding of the symbols of Σ as strings of  = ⌈log 2 |Σ|⌉ bits, then a circuit can simulate input alphabet Σ by encoding the value of the -th input symbol into positions  to + (−1).For the rest of this section, we assume Σ = {0, 1}.', 'Circuits A circuit  with input length  is a directed acyclic graph with  input vertices  1 , . . .,   and zero or more gate vertices, each labeled by NOT, AND, or OR.Input vertices have fan-in (in-degree) zero, NOT gates have fan-in one, and the fan-in of AND and OR gates can be either two or unbounded.One (input or gate) vertex  is designated the output of the circuit.', 'Given an input string  ∈ {0, 1}  , each input vertex   is assigned the value   , and each gate vertex is assigned the value computed by applying its corresponding logical function to the values assigned to its in-neighbors.The circuit computes the boolean function  : {0, 1}  → {0, 1}, mapping each input string to the value assigned to .The depth of , denoted  (), is the length of the longest directed path from any   to .The size of , denoted | |, is the number of vertices in .Uniformity As defined, a circuit family contains a different circuit for each length , with no constraint on the relationship between the circuits.For example, let  be any unary language:  ⊆ {1} * .For  ∈ N, if 1  ∉ , define   to be a circuit for the constant 0 function (an OR gate with fan-in 0), and if 1  ∈ , define   to be a circuit for the AND of all the inputs.Thus, every unary language, even an undecidable one, is recognized by a circuit family of size  () and depth  (1).']","We assume familiarity with finite automata and Turing machines; for definitions, please see the textbook by Sipser (2013).Background on counter machines is given by Fischer et al. (1968).The standard deterministic model of a multi-tape Turing machine is used to define the language classes L (languages recognizable in space  (log )) and P (languages recognizable in polynomial time).The definition of NL (languages recognizable in non-deterministic space  (log )) uses a nondeterministic Turing machine.We also consider a random-access model of a deterministic Turing machine, which allows meaningful computation in time  (log ) (Mix Barrington et al., 1990).Problem reductions computable with bounded resources (e.g., in L) are used to define problems complete or hard for various classes of languages.

It is known that

but none of these inclusions are known to be strict.

6.2 Circuits and classes AC 0 , ACC 0 , TC 0 , NC 1

Circuits are a model of parallel computation particularly relevant to transformers.For more details, please see the textbook by Arora and Barak (2009).Circuits operate on binary values.If we choose a fixed-length encoding of the symbols of Σ as strings of  = ⌈log 2 |Σ|⌉ bits, then a circuit can simulate input alphabet Σ by encoding the value of the -th input symbol into positions  to + (−1).For the rest of this section, we assume Σ = {0, 1}.

Circuits A circuit  with input length  is a directed acyclic graph with  input vertices  1 , . . .,   and zero or more gate vertices, each labeled by NOT, AND, or OR.Input vertices have fan-in (in-degree) zero, NOT gates have fan-in one, and the fan-in of AND and OR gates can be either two or unbounded.One (input or gate) vertex  is designated the output of the circuit.

Given an input string  ∈ {0, 1}  , each input vertex   is assigned the value   , and each gate vertex is assigned the value computed by applying its corresponding logical function to the values assigned to its in-neighbors.The circuit computes the boolean function  : {0, 1}  → {0, 1}, mapping each input string to the value assigned to .The depth of , denoted  (), is the length of the longest directed path from any   to .The size of , denoted | |, is the number of vertices in .Uniformity As defined, a circuit family contains a different circuit for each length , with no constraint on the relationship between the circuits.For example, let  be any unary language:  ⊆ {1} * .For  ∈ N, if 1  ∉ , define   to be a circuit for the constant 0 function (an OR gate with fan-in 0), and if 1  ∈ , define   to be a circuit for the AND of all the inputs.Thus, every unary language, even an undecidable one, is recognized by a circuit family of size  () and depth  (1).","(p18.0) We assume familiarity with finite automata and Turing machines; for definitions, please see the textbook by Sipser (2013).Background on counter machines is given by Fischer et al. (1968).The standard deterministic model of a multi-tape Turing machine is used to define the language classes L (languages recognizable in space  (log )) and P (languages recognizable in polynomial time).The definition of NL (languages recognizable in non-deterministic space  (log )) uses a nondeterministic Turing machine.We also consider a random-access model of a deterministic Turing machine, which allows meaningful computation in time  (log ) (Mix Barrington et al., 1990).Problem reductions computable with bounded resources (e.g., in L) are used to define problems complete or hard for various classes of languages.

(p18.1) It is known that

(p18.2) but none of these inclusions are known to be strict.

(p18.3) 6.2 Circuits and classes AC 0 , ACC 0 , TC 0 , NC 1

(p18.4) Circuits are a model of parallel computation particularly relevant to transformers.For more details, please see the textbook by Arora and Barak (2009).Circuits operate on binary values.If we choose a fixed-length encoding of the symbols of Σ as strings of  = ⌈log 2 |Σ|⌉ bits, then a circuit can simulate input alphabet Σ by encoding the value of the -th input symbol into positions  to + (−1).For the rest of this section, we assume Σ = {0, 1}.

(p18.5) Circuits A circuit  with input length  is a directed acyclic graph with  input vertices  1 , . . .,   and zero or more gate vertices, each labeled by NOT, AND, or OR.Input vertices have fan-in (in-degree) zero, NOT gates have fan-in one, and the fan-in of AND and OR gates can be either two or unbounded.One (input or gate) vertex  is designated the output of the circuit.

(p18.6) Given an input string  ∈ {0, 1}  , each input vertex   is assigned the value   , and each gate vertex is assigned the value computed by applying its corresponding logical function to the values assigned to its in-neighbors.The circuit computes the boolean function  : {0, 1}  → {0, 1}, mapping each input string to the value assigned to .The depth of , denoted  (), is the length of the longest directed path from any   to .The size of , denoted | |, is the number of vertices in .Uniformity As defined, a circuit family contains a different circuit for each length , with no constraint on the relationship between the circuits.For example, let  be any unary language:  ⊆ {1} * .For  ∈ N, if 1  ∉ , define   to be a circuit for the constant 0 function (an OR gate with fan-in 0), and if 1  ∈ , define   to be a circuit for the AND of all the inputs.Thus, every unary language, even an undecidable one, is recognized by a circuit family of size  () and depth  (1).","[['b61', None, 'b50'], [], [], [], ['b3'], [], []]","[['b61', None, 'b50'], [], [], [], ['b3'], [], []]",4,"1. We assume familiarity with finite automata and Turing machines; for definitions, please see the textbook by Sipser (2013).Background on counter machines is given by Fischer et al. (1968).The standard deterministic model of a multi-tape Turing machine is used to define the language classes L (languages recognizable in space  (log )) and P (languages recognizable in polynomial time).The definition of NL (languages recognizable in non-deterministic space  (log )) uses a nondeterministic Turing machine.
2. We also consider a random-access model of a deterministic Turing machine, which allows meaningful computation in time  (log ) (Mix Barrington et al., 1990).Problem
3. reductions computable with bounded resources (e.g., in L) are used to define problems complete or hard for various classes of languages.
4. It is known thatbut none of these inclusions are known to be strict.
5. 6.2 Circuits and classes AC 0 , ACC 0 , TC 0 , NC 1Circuits are a model of parallel computation particularly relevant to transformers.
6. For more details, please see the textbook by Arora and Barak (2009).Circuits operate on binary values.
7. If we choose a fixed-length encoding of the symbols of Σ as strings of  = ⌈log 2 |Σ|⌉ bits, then a circuit can simulate input alphabet Σ by encoding the value of the -th input symbol into positions  to + (−1).For the rest of this section, we assume Σ = {0, 1}.
8. Circuits A circuit  with input length  is a directed acyclic graph with  input vertices  1 , . . .,   and zero or more gate vertices, each labeled by NOT, AND, or OR.Input vertices have fan-in (in-degree) zero, NOT gates have fan-in one, and the fan-in of AND and OR gates can be either two or unbounded.
9. One (input or gate) vertex  is designated the output of the circuit.
10. Given an input string  ∈ {0, 1}  , each input vertex   is assigned the value   , and each gate vertex is assigned the value computed by applying its corresponding logical function to the values assigned to its in-neighbors.
11. The circuit computes the boolean function  : {0, 1}  → {0, 1}, mapping each input string to the value assigned to .The depth of , denoted  (), is the length of the longest directed path from any   to .The size of , denoted | |, is the number of vertices in .Uniformity
12. As defined, a circuit family contains a different circuit for each length , with no constraint on the relationship between the circuits.
13. For example, let  be any unary language:  ⊆ {1} *
14. .For  ∈ N, if 1  ∉ , define   to be a circuit for the constant 0 function (an OR gate with fan-in 0), and if 1  ∈ , define   to be a circuit for the AND of all the inputs.
15. Thus, every unary language, even an undecidable one, is recognized by a circuit family of size  () and depth  (1)."
264833196,Transformers as Recognizers of Formal Languages: A Survey on Expressivity,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/6624065bd2825f500e93624671793d963187d066,s15,Number representations,"['p15.0', 'p15.1', 'p15.2']","['Transformers, like most neural networks, operate, in principle, on real numbers.While hard attention transformers could be defined using only rational numbers, even rational numbers can represent an arbitrary amount of information.In the area of expressivity of RNNs, the use of real or rational numbers has led to results that make RNNs appear more powerful in theory than in practice (Siegelmann andSontag, 1995, 1994;Weiss et al., 2018).Consequently, some studies limit numeric representations to have  (1) bits, as floating-point numbers do in practice (Chiang et al., 2023).', 'But the need to handle arbitrary lengths ( §5.2) also makes it reasonable for precision to depend on .Merrill and Sabharwal (2023a) argue that in  (1) precision, an attention head cannot attend uniformly to a string of length , because the attention weights () would all round down to zero.So  (log ) bits of precision is a common choice (Yao et al., 2021;Merrill and Sabharwal, 2023b,a).Other choices are possible as well: Merrill and Sabharwal (2023b) use the set F of rational numbers whose denominator is unbounded but constrained to be a power of two.', 'Restricting the intermediate activations to limited precision introduces numerous decisions about when and how rounding should take place, which can potentially affect the expressivity of the model.For example, when summing  numbers, one could round after each addition or only at the end of the summation.Better formalizing these decisions and their impact on expressivity is an area for future research.']","Transformers, like most neural networks, operate, in principle, on real numbers.While hard attention transformers could be defined using only rational numbers, even rational numbers can represent an arbitrary amount of information.In the area of expressivity of RNNs, the use of real or rational numbers has led to results that make RNNs appear more powerful in theory than in practice (Siegelmann andSontag, 1995, 1994;Weiss et al., 2018).Consequently, some studies limit numeric representations to have  (1) bits, as floating-point numbers do in practice (Chiang et al., 2023).

But the need to handle arbitrary lengths ( §5.2) also makes it reasonable for precision to depend on .Merrill and Sabharwal (2023a) argue that in  (1) precision, an attention head cannot attend uniformly to a string of length , because the attention weights () would all round down to zero.So  (log ) bits of precision is a common choice (Yao et al., 2021;Merrill and Sabharwal, 2023b,a).Other choices are possible as well: Merrill and Sabharwal (2023b) use the set F of rational numbers whose denominator is unbounded but constrained to be a power of two.

Restricting the intermediate activations to limited precision introduces numerous decisions about when and how rounding should take place, which can potentially affect the expressivity of the model.For example, when summing  numbers, one could round after each addition or only at the end of the summation.Better formalizing these decisions and their impact on expressivity is an area for future research.","(p15.0) Transformers, like most neural networks, operate, in principle, on real numbers.While hard attention transformers could be defined using only rational numbers, even rational numbers can represent an arbitrary amount of information.In the area of expressivity of RNNs, the use of real or rational numbers has led to results that make RNNs appear more powerful in theory than in practice (Siegelmann andSontag, 1995, 1994;Weiss et al., 2018).Consequently, some studies limit numeric representations to have  (1) bits, as floating-point numbers do in practice (Chiang et al., 2023).

(p15.1) But the need to handle arbitrary lengths ( §5.2) also makes it reasonable for precision to depend on .Merrill and Sabharwal (2023a) argue that in  (1) precision, an attention head cannot attend uniformly to a string of length , because the attention weights () would all round down to zero.So  (log ) bits of precision is a common choice (Yao et al., 2021;Merrill and Sabharwal, 2023b,a).Other choices are possible as well: Merrill and Sabharwal (2023b) use the set F of rational numbers whose denominator is unbounded but constrained to be a power of two.

(p15.2) Restricting the intermediate activations to limited precision introduces numerous decisions about when and how rounding should take place, which can potentially affect the expressivity of the model.For example, when summing  numbers, one could round after each addition or only at the end of the summation.Better formalizing these decisions and their impact on expressivity is an area for future research.","[['b72', 'b13', None], ['b44', 'b74', None], []]","[['b72', 'b13', None], ['b44', 'b74', None], []]",6,"1. Transformers, like most neural networks, operate, in principle, on real numbers.
2. While hard attention transformers could be defined using only rational numbers, even rational numbers can represent an arbitrary amount of information.
3. In the area of expressivity of RNNs, the use of real or rational numbers has led to results that make RNNs appear more powerful in theory than in practice (Siegelmann andSontag, 1995, 1994;Weiss et al., 2018).Consequently, some studies limit numeric representations to have  (1) bits, as floating-point numbers do in practice (Chiang et al., 2023).
4. But the need to handle arbitrary lengths ( §5.2) also makes it reasonable for precision to depend on .Merrill and Sabharwal (2023a) argue that in  (1) precision, an attention head cannot attend uniformly to a string of length , because the attention weights () would all round down to zero.
5. So  (log ) bits of precision is a common choice (Yao et al., 2021;Merrill and Sabharwal, 2023b,a).Other choices are possible as well: Merrill and Sabharwal (2023b) use the set F of rational numbers whose denominator is unbounded but constrained to be a power of two.
6. Restricting the intermediate activations to limited precision introduces numerous decisions about when and how rounding should take place, which can potentially affect the expressivity of the model.
7. For example, when summing  numbers, one could round after each addition or only at the end of the summation.
8. Better formalizing these decisions and their impact on expressivity is an area for future research."
264833196,Transformers as Recognizers of Formal Languages: A Survey on Expressivity,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/6624065bd2825f500e93624671793d963187d066,s13,Transformer encoder-decoders,"['p13.0', 'p13.1', 'p13.2']","['A transformer encoder-decoder combines a transformer encoder and decoder, adding to each layer of the decoder an additional attention sublayer, known as cross attention, which attends to the output of the encoder.', 'In the literature surveyed here, only the construction of Pérez et al. (2021) and related constructions (Bhattamishra et al., 2020b;Wei et al., 2022) employ an encoder-decoder architecture.In these constructions, a string  is fed to the encoder, and the decoder is allowed to run for an arbitrary number of steps.Then  is accepted iff the decoder eventually outputs a vector belonging to a fixed set of accept vectors.', 'As we will see ( §7.2.1), this setup vastly increases the model\'s power.It could be likened to a language model that is allowed to ""think step by step"" (Kojima et al., 2022) before generating a final accept decision.']","A transformer encoder-decoder combines a transformer encoder and decoder, adding to each layer of the decoder an additional attention sublayer, known as cross attention, which attends to the output of the encoder.

In the literature surveyed here, only the construction of Pérez et al. (2021) and related constructions (Bhattamishra et al., 2020b;Wei et al., 2022) employ an encoder-decoder architecture.In these constructions, a string  is fed to the encoder, and the decoder is allowed to run for an arbitrary number of steps.Then  is accepted iff the decoder eventually outputs a vector belonging to a fixed set of accept vectors.

As we will see ( §7.2.1), this setup vastly increases the model's power.It could be likened to a language model that is allowed to ""think step by step"" (Kojima et al., 2022) before generating a final accept decision.","(p13.0) A transformer encoder-decoder combines a transformer encoder and decoder, adding to each layer of the decoder an additional attention sublayer, known as cross attention, which attends to the output of the encoder.

(p13.1) In the literature surveyed here, only the construction of Pérez et al. (2021) and related constructions (Bhattamishra et al., 2020b;Wei et al., 2022) employ an encoder-decoder architecture.In these constructions, a string  is fed to the encoder, and the decoder is allowed to run for an arbitrary number of steps.Then  is accepted iff the decoder eventually outputs a vector belonging to a fixed set of accept vectors.

(p13.2) As we will see ( §7.2.1), this setup vastly increases the model's power.It could be likened to a language model that is allowed to ""think step by step"" (Kojima et al., 2022) before generating a final accept decision.","[[], ['b9', 'b54', 'b71'], ['b32']]","[[], ['b9', 'b54', 'b71'], ['b32']]",4,"1. A transformer encoder-decoder combines a transformer encoder and decoder, adding to each layer of the decoder an additional attention sublayer, known as cross attention, which attends to the output of the encoder.
2. In the literature surveyed here, only the construction of Pérez et al. (2021) and related constructions (Bhattamishra et al., 2020b;Wei et al., 2022) employ an encoder-decoder architecture.
3. In these constructions, a string  is fed to the encoder, and the decoder is allowed to run for an arbitrary number of steps.
4. Then  is accepted iff the decoder eventually outputs a vector belonging to a fixed set of accept vectors.
5. As we will see ( §7.2.1), this setup vastly increases the model's power.
6. It could be likened to a language model that is allowed to ""think step by step"" (Kojima et al., 2022) before generating a final accept decision."
264833196,Transformers as Recognizers of Formal Languages: A Survey on Expressivity,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/6624065bd2825f500e93624671793d963187d066,s7,Hard attention,['p7.0'],"['By substituting S h or S a for S in Eq. ( 4), we get leftmost-hard and average-hard attention, respectively.Leftmost-hard attention was previously called hard attention by Hahn (2020) and unique hard attention by Hao et al. (2022).Average-hard attention was also called hard attention by Pérez et al. (2021) and saturated attention by Merrill et al. (2022), and has been argued to be a realistic approximation to how trained transformers behave in practice (Merrill et al., 2021).']","By substituting S h or S a for S in Eq. ( 4), we get leftmost-hard and average-hard attention, respectively.Leftmost-hard attention was previously called hard attention by Hahn (2020) and unique hard attention by Hao et al. (2022).Average-hard attention was also called hard attention by Pérez et al. (2021) and saturated attention by Merrill et al. (2022), and has been argued to be a realistic approximation to how trained transformers behave in practice (Merrill et al., 2021).","(p7.0) By substituting S h or S a for S in Eq. ( 4), we get leftmost-hard and average-hard attention, respectively.Leftmost-hard attention was previously called hard attention by Hahn (2020) and unique hard attention by Hao et al. (2022).Average-hard attention was also called hard attention by Pérez et al. (2021) and saturated attention by Merrill et al. (2022), and has been argued to be a realistic approximation to how trained transformers behave in practice (Merrill et al., 2021).","[['b54', 'b46', 'b24', 'b23', 'b43']]","[['b54', 'b46', 'b24', 'b23', 'b43']]",5,"1. By substituting S h or S a for S in Eq. ( 4), we get leftmost-hard and average-hard attention, respectively.
2. Leftmost-hard attention was previously called hard attention by Hahn (2020) and unique hard attention by Hao et al. (2022).Average-hard attention was also called hard attention by Pérez et al. (2021) and saturated attention by Merrill et al. (2022), and has been argued to be a realistic approximation to how trained transformers behave in practice (Merrill et al., 2021)."
264833196,Transformers as Recognizers of Formal Languages: A Survey on Expressivity,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/6624065bd2825f500e93624671793d963187d066,s4,Input layer,"['p4.0', 'p4.1', 'p4.2', 'p4.3', 'p4.4']","['Strings are initially mapped to sequences of vectors using a length-preserving function  :', 'which is the sum of a word embedding WE : Σ → R  and a position(al) embedding (or encoding)', 'In theoretical constructions, the word embedding WE can be an arbitrary computable function.', 'The original transformer paper (Vaswani et al., 2017) introduced the following position encoding:', 'Theoretical papers have explored other position encodings, including  itself (Pérez et al., 2021), / (Yao et al., 2021;Chiang and Cholak, 2022), and 1/ or 1/ 2 (Pérez et al., 2021).Because the choice of position encoding can have a significant impact on the expressivity of the network, when stating a result, we always include the position encoding among its assumptions.']","Strings are initially mapped to sequences of vectors using a length-preserving function  :

which is the sum of a word embedding WE : Σ → R  and a position(al) embedding (or encoding)

In theoretical constructions, the word embedding WE can be an arbitrary computable function.

The original transformer paper (Vaswani et al., 2017) introduced the following position encoding:

Theoretical papers have explored other position encodings, including  itself (Pérez et al., 2021), / (Yao et al., 2021;Chiang and Cholak, 2022), and 1/ or 1/ 2 (Pérez et al., 2021).Because the choice of position encoding can have a significant impact on the expressivity of the network, when stating a result, we always include the position encoding among its assumptions.","(p4.0) Strings are initially mapped to sequences of vectors using a length-preserving function  :

(p4.1) which is the sum of a word embedding WE : Σ → R  and a position(al) embedding (or encoding)

(p4.2) In theoretical constructions, the word embedding WE can be an arbitrary computable function.

(p4.3) The original transformer paper (Vaswani et al., 2017) introduced the following position encoding:

(p4.4) Theoretical papers have explored other position encodings, including  itself (Pérez et al., 2021), / (Yao et al., 2021;Chiang and Cholak, 2022), and 1/ or 1/ 2 (Pérez et al., 2021).Because the choice of position encoding can have a significant impact on the expressivity of the network, when stating a result, we always include the position encoding among its assumptions.","[[], [], [], ['b69'], ['b12', 'b74', 'b54']]","[[], [], [], ['b69'], ['b12', 'b74', 'b54']]",4,"1. Strings are initially mapped to sequences of vectors using a length-preserving function  :which is the sum of a word embedding WE : Σ → R  and a position(al) embedding (or encoding)In theoretical constructions, the word embedding WE can be an arbitrary computable function.
2. The original transformer paper (Vaswani et al., 2017) introduced the following position encoding:Theoretical papers have explored other position encodings, including  itself (Pérez et al., 2021), / (Yao et al., 2021;Chiang and Cholak, 2022), and 1/ or 1/ 2 (Pérez et al., 2021).Because the choice of position encoding can have a significant impact on the expressivity of the network, when stating a result, we always include the position encoding among its assumptions."
264833196,Transformers as Recognizers of Formal Languages: A Survey on Expressivity,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/6624065bd2825f500e93624671793d963187d066,s31,Counter machines,['p31.0'],"['Counter machines are automata with integer-valued registers, which have been studied extensively in connection with LSTM RNNs (Weiss et al., 2018;Suzgun et al., 2019;Merrill, 2019Merrill, , 2020)).Bhattamishra et al. (2020a), following Merrill et al. (2020), define a subclass of counter machines called simplified and stateless -counter machines (SSCMs).These machines use a counter update function  : Σ → Z  to increment and decrement each counter based on the current input symbol, but they have no state and cannot look at the counter values until the end of the string.They then show that any SSCM can be converted to an equivalent transformer encoder with causal masking and no residual connections.']","Counter machines are automata with integer-valued registers, which have been studied extensively in connection with LSTM RNNs (Weiss et al., 2018;Suzgun et al., 2019;Merrill, 2019Merrill, , 2020)).Bhattamishra et al. (2020a), following Merrill et al. (2020), define a subclass of counter machines called simplified and stateless -counter machines (SSCMs).These machines use a counter update function  : Σ → Z  to increment and decrement each counter based on the current input symbol, but they have no state and cannot look at the counter values until the end of the string.They then show that any SSCM can be converted to an equivalent transformer encoder with causal masking and no residual connections.","(p31.0) Counter machines are automata with integer-valued registers, which have been studied extensively in connection with LSTM RNNs (Weiss et al., 2018;Suzgun et al., 2019;Merrill, 2019Merrill, , 2020)).Bhattamishra et al. (2020a), following Merrill et al. (2020), define a subclass of counter machines called simplified and stateless -counter machines (SSCMs).These machines use a counter update function  : Σ → Z  to increment and decrement each counter based on the current input symbol, but they have no state and cannot look at the counter values until the end of the string.They then show that any SSCM can be converted to an equivalent transformer encoder with causal masking and no residual connections.","[['b40', 'b66', 'b8', 'b72', 'b39', 'b47']]","[['b40', 'b66', 'b8', 'b72', 'b39', 'b47']]",6,"1. Counter machines are automata with integer-valued registers, which have been studied extensively in connection with LSTM RNNs (Weiss et al., 2018;Suzgun et al., 2019;Merrill, 2019Merrill, , 2020)).Bhattamishra et al. (2020a), following Merrill et al. (2020), define a subclass of counter machines called simplified and stateless -counter machines (SSCMs).These machines use a counter update function  : Σ → Z  to increment and decrement each counter based on the current input symbol, but they have no state and cannot look at the counter values until the end of the string.
2. They then show that any SSCM can be converted to an equivalent transformer encoder with causal masking and no residual connections."
