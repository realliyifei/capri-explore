corpusid,title,url,section_title,num_reference,section,section_sentence_prefixed,qud_analysis,qud_analysis_spans,qud_span,question_with_indexes,question,sentence_indexes,judge_ansvar,qa_status,answer_rephrased,QA_pair
252683270,A Decade of Knowledge Graphs in Natural Language Processing: A Survey,https://www.semanticscholar.org/paper/2341353cae858ce06225e46356c472b71dc63372,Knowledge Application,15,"Existing KGs can be used in a multitude of popular NLP tasks. Here we outline the most popular ones.

Question answering (QA) was found to be the most common NLP task using KGs. This task is typically divided into textual QA and question answering over knowledge bases (KBQA). Textual QA derives answers from unstructured documents while KBQA does so from predefined knowledge bases (Fu et al., 2020). KBQA is naturally tied to KGs while textual QA can also be approached by using KGs as a source of common-sense knowledge when answering questions. As Zhu et al. (2021) conclude, this approach is desired not only because it is helpful for generating answers, but also because it makes answers more interpretable.

Semantic search refers to ""search with meaning"", where the goal is not just to search for literal matches, but to understand the search intent and query context as well (Bast et al., 2016). This label denoted studies that use KGs for search, recommendations, and analytics. Examples are a big semantic network of everyday concepts called ConceptNet (Speer et al., 2017) and a KG of scholarly communications and the relationships, among them the Microsoft Academic Graph .

Conversational interfaces constitute another NLP field that can benefit from world knowledge contained in KGs. Zhou et al. (2018)  Natural language generation (NLG) is a subfield of NLP and computational linguistics that is concerned with models which generate natural language output from scratch. KGs are used in this subfield for producing natural language text from KGs (Koncel-Kedziorski et al., 2019), generating question-answer pairs (Reddy et al., 2017), the multi-modal task of image captioning (Lu et al., 2018), or data augmentation in low-resource settings (Sharifirad et al., 2018).

Text analysis combines various analytical NLP techniques and methods that are applied to process and understand textual data. Exemplary tasks are sentiment detection (Kumar et al., 2018), topic modeling , or word sense disambiguation .

Augmented language models are a combination of large pretrained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) with knowledge contained in KGs. Since PLMs derive their knowledge from huge amounts of unstructured training data, a rising research trend is in combining them with structured knowledge. Knowledge from KGs can be infused into language models in their input, architecture, output, or some combination thereof (Colon-Hernandez et al., 2021). Some notable examples we outline are ERNIE (Zhang et al., 2019b), COMET (Bosselut et al., 2019), K-BERT (Liu et al., 2020b), and KEPLER (Wang et al., 2021b). Table 3 shows the distribution of papers according to the different research and contribution types as defined in Table 4 and 5 in the Appendix. It shows that most papers conduct validation research, investigating new techniques or methods that have not yet been implemented in practice. A considerable number of papers, although significantly less, focus on solution proposals of approaches by demonstrating their advantages and applicability by a small example or argumentation. However, these papers usually lack a profound empirical evaluation. Secondary research accounts for only a small number of papers and is severely underrepresented in the research field of KGs in NLP. As already mentioned in Section 1 and Section 2, there is a notable lack of studies that summarize, compile, or synthesize existing research regarding KGs in NLP. Moreover, evaluation research papers that implement and evaluate approaches in an industry context are equally scarce. Opinion papers are almost non-existent.","sent1: Existing KGs can be used in a multitude of popular NLP tasks.
sent2: Here we outline the most popular ones.
sent3: Question answering (QA) was found to be the most common NLP task using KGs.
sent4: This task is typically divided into textual QA and question answering over knowledge bases (KBQA).
sent5: Textual QA derives answers from unstructured documents while KBQA does so from predefined knowledge bases (Fu et al., 2020).
sent6: KBQA is naturally tied to KGs while textual QA can also be approached by using KGs as a source of common-sense knowledge when answering questions.
sent7: As Zhu et al. (2021) conclude, this approach is desired not only because it is helpful for generating answers, but also because it makes answers more interpretable.
sent8: Semantic search refers to ""search with meaning"", where the goal is not just to search for literal matches, but to understand the search intent and query context as well (Bast et al., 2016).
sent9: This label denoted studies that use KGs for search, recommendations, and analytics.
sent10: Examples are a big semantic network of everyday concepts called ConceptNet (Speer et al., 2017) and a KG of scholarly communications and the relationships, among them the Microsoft Academic Graph .
sent11: Conversational interfaces constitute another NLP field that can benefit from world knowledge contained in KGs.
sent12: Zhou et al. (2018)  Natural language generation (NLG) is a subfield of NLP and computational linguistics that is concerned with models which generate natural language output from scratch.
sent13: KGs are used in this subfield for producing natural language text from KGs (Koncel-Kedziorski et al., 2019), generating question-answer pairs (Reddy et al., 2017), the multi-modal task of image captioning (Lu et al., 2018), or data augmentation in low-resource settings (Sharifirad et al., 2018).
sent14: Text analysis combines various analytical NLP techniques and methods that are applied to process and understand textual data.
sent15: Exemplary tasks are sentiment detection (Kumar et al., 2018), topic modeling , or word sense disambiguation .
sent16: Augmented language models are a combination of large pretrained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) with knowledge contained in KGs.
sent17: Since PLMs derive their knowledge from huge amounts of unstructured training data, a rising research trend is in combining them with structured knowledge.
sent18: Knowledge from KGs can be infused into language models in their input, architecture, output, or some combination thereof (Colon-Hernandez et al., 2021).
sent19: Some notable examples we outline are ERNIE (Zhang et al., 2019b), COMET (Bosselut et al., 2019), K-BERT (Liu et al., 2020b), and KEPLER (Wang et al., 2021b).
sent20: Table 3 shows the distribution of papers according to the different research and contribution types as defined in Table 4 and 5 in the Appendix.
sent21: It shows that most papers conduct validation research, investigating new techniques or methods that have not yet been implemented in practice.
sent22: A considerable number of papers, although significantly less, focus on solution proposals of approaches by demonstrating their advantages and applicability by a small example or argumentation.
sent23: However, these papers usually lack a profound empirical evaluation.
sent24: Secondary research accounts for only a small number of papers and is severely underrepresented in the research field of KGs in NLP.
sent25: As already mentioned in Section 1 and Section 2, there is a notable lack of studies that summarize, compile, or synthesize existing research regarding KGs in NLP.
sent26: Moreover, evaluation research papers that implement and evaluate approaches in an industry context are equally scarce.
sent27: Opinion papers are almost non-existent.","1. How can existing Knowledge Graphs (KGs) be utilized in NLP tasks? sent1
    1.1. What is the most common NLP task using KGs? sent3
        1.1.1. How is the task of question answering (QA) typically divided? sent4
            1.1.1.1. How do textual QA and KBQA differ in deriving answers? sent5
            1.1.1.2. Why is using KGs in textual QA considered beneficial? sent6, sent7
    1.2. What is semantic search and how are KGs used in it? sent8, sent9
        1.2.1. Can you provide examples of KGs used in semantic search? sent10
    1.3. How do conversational interfaces benefit from KGs? sent11
    1.4. What role do KGs play in natural language generation (NLG)? sent12, sent13
    1.5. How are KGs applied in text analysis? sent14
        1.5.1. What are some exemplary tasks in text analysis using KGs? sent15
    1.6. What are augmented language models and how do they relate to KGs? sent16
        1.6.1. Why is there a trend in combining PLMs with structured knowledge from KGs? sent17
        1.6.2. How can knowledge from KGs be infused into language models? sent18
        1.6.3. What are some notable examples of augmented language models? sent19
2. What does Table 3 show about the distribution of papers in the research field of KGs in NLP? sent20
    2.1. What type of research do most papers conduct according to Table 3? sent21
    2.2. What is the focus of a considerable number of papers, despite being less than validation research? sent22
        2.2.1. What do these papers usually lack? sent23
    2.3. How is secondary research represented in the field of KGs in NLP? sent24
    2.4. What notable lack is mentioned regarding studies in KGs in NLP? sent25
    2.5. How common are evaluation research papers and opinion papers in this field? sent26, sent27","1. How can existing Knowledge Graphs (KGs) be utilized in NLP tasks? sent1
    1.1. What is the most common NLP task using KGs? sent3
        1.1.1. How is the task of question answering (QA) typically divided? sent4
            1.1.1.1. How do textual QA and KBQA differ in deriving answers? sent5
            1.1.1.2. Why is using KGs in textual QA considered beneficial? sent6, sent7
    1.2. What is semantic search and how are KGs used in it? sent8, sent9
        1.2.1. Can you provide examples of KGs used in semantic search? sent10
    1.3. How do conversational interfaces benefit from KGs? sent11
    1.4. What role do KGs play in natural language generation (NLG)? sent12, sent13
    1.5. How are KGs applied in text analysis? sent14
        1.5.1. What are some exemplary tasks in text analysis using KGs? sent15
    1.6. What are augmented language models and how do they relate to KGs? sent16
        1.6.1. Why is there a trend in combining PLMs with structured knowledge from KGs? sent17
        1.6.2. How can knowledge from KGs be infused into language models? sent18
        1.6.3. What are some notable examples of augmented language models? sent19

    1.1. What is the most common NLP task using KGs? sent3
        1.1.1. How is the task of question answering (QA) typically divided? sent4
            1.1.1.1. How do textual QA and KBQA differ in deriving answers? sent5
            1.1.1.2. Why is using KGs in textual QA considered beneficial? sent6, sent7

        1.1.1. How is the task of question answering (QA) typically divided? sent4
            1.1.1.1. How do textual QA and KBQA differ in deriving answers? sent5
            1.1.1.2. Why is using KGs in textual QA considered beneficial? sent6, sent7

    1.6. What are augmented language models and how do they relate to KGs? sent16
        1.6.1. Why is there a trend in combining PLMs with structured knowledge from KGs? sent17
        1.6.2. How can knowledge from KGs be infused into language models? sent18
        1.6.3. What are some notable examples of augmented language models? sent19

2. What does Table 3 show about the distribution of papers in the research field of KGs in NLP? sent20
    2.1. What type of research do most papers conduct according to Table 3? sent21
    2.2. What is the focus of a considerable number of papers, despite being less than validation research? sent22
        2.2.1. What do these papers usually lack? sent23
    2.3. How is secondary research represented in the field of KGs in NLP? sent24
    2.4. What notable lack is mentioned regarding studies in KGs in NLP? sent25
    2.5. How common are evaluation research papers and opinion papers in this field? sent26, sent27","        1.1.1. How is the task of question answering (QA) typically divided? sent4
            1.1.1.1. How do textual QA and KBQA differ in deriving answers? sent5
            1.1.1.2. Why is using KGs in textual QA considered beneficial? sent6, sent7","Question: Why is the integration of knowledge graphs in textual question answering considered advantageous?

Supporting sentence: sent6, sent7",Why is the integration of knowledge graphs in textual question answering considered advantageous?,"sent6, sent7",4,implement,"Textual QA can be approached by using KGs as a source of common-sense knowledge when answering questions.

As Zhu et al. (2021) conclude, this approach is desired not only because it is helpful for generating answers, but also because it makes answers more interpretable.","Question:

Why is the integration of knowledge graphs in textual question answering considered advantageous?

Answer:

Textual QA can be approached by using KGs as a source of common-sense knowledge when answering questions.

As Zhu et al. (2021) conclude, this approach is desired not only because it is helpful for generating answers, but also because it makes answers more interpretable."
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,Internal classifier training Exit criterion,21,"DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy < θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ ) Voting  joint; sum of CE + diversity loss accumulated votes > θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ ) Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)

BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θ

CascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.

Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.  further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair. They optimize these weights by a cross-level optimization algorithm.

They adopt PABEE's patience-based strategy for exiting. Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners. Entropy is used as the exit criterion. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods. The inference is terminated when multiple layers are confident.

Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit. CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.

Cascading Cascading can be seen as a special form of early exit, performed at the model level. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading. CascadeBERT executes models one by one, from the smallest to the largest. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.

Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling. They use the maximum class probability as confidence on a per-token basis. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens. The model completely exits when every token exits. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","sent1: DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy <
sent2: θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ )
sent3: Voting  joint; sum of CE + diversity loss accumulated votes >
sent4: θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ )
sent5: Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θCascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.
sent6: Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation.
sent7: That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded.
sent8: Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions.
sent9: Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training.
sent10: They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers.
sent11: For inference, the model exits when k consecutive internal classifiers make the same prediction.
sent12: Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.
sent13: further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution.
sent14: They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold.
sent15: Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other.
sent16: However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair.
sent17: They optimize these weights by a cross-level optimization algorithm.
sent18: They adopt PABEE's patience-based strategy for exiting.
sent19: Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states.
sent20: For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners.
sent21: Entropy is used as the exit criterion.
sent22: PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods.
sent23: The inference is terminated when multiple layers are confident.
sent24: Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.
sent25: BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations.
sent26: They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction.
sent27: It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit.
sent28: CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.
sent29: Cascading Cascading can be seen as a special form of early exit, performed at the model level.
sent30: Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers.
sent31: Therefore, they propose to use a suite of complete models with different numbers of layers for cascading.
sent32: CascadeBERT executes models one by one, from the smallest to the largest.
sent33: It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.
sent34: Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings.
sent35: Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling.
sent36: They use the maximum class probability as confidence on a per-token basis.
sent37: Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers.
sent38: These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens.
sent39: The model completely exits when every token exits.
sent40: A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification.
sent41: Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking.
sent42: ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane.
sent43: They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","1. What is the focus of the section on Internal classifier training Exit criterion? sent1
    1.1. What are some methods mentioned for internal classifier training and exit criteria? sent1, sent2, sent3, sent4, sent5
    1.2. What is a drawback of confidence-based early exit? sent6
        1.2.1. How does ensemble-based early exit address this drawback? sent7, sent8
    1.3. How does PABEE improve performance and efficiency? sent9, sent10, sent11
        1.3.1. What additional benefit does PABEE provide? sent12
        1.3.2. What mechanism does PABEE use to encourage diversity? sent13, sent14
    1.4. How does LeeBERT differ from PABEE in its approach? sent15, sent16, sent17
        1.4.1. What strategy does LeeBERT adopt for exiting? sent18
    1.5. What is the approach of Liao et al. (2021) for training and inference? sent19, sent20, sent21
    1.6. How does PCEE-BERT combine different early exit methods? sent22, sent23
2. What is another stream of research in early exiting? sent24
    2.1. How does BERxiT implement learning-based early exit? sent25, sent26, sent27
    2.2. What is the approach of CAT for early exiting? sent28
3. What is cascading in the context of early exit? sent29
    3.1. What did Li et al. (2021a) find about shallow features and internal classifiers? sent30
    3.2. What is the proposal of Li et al. (2021a) for cascading? sent31, sent32, sent33
4. What are some applications of early exit beyond classification? sent34
    4.1. What is Token-Level Early-Exit and how does it work? sent35, sent36, sent37, sent38, sent39
    4.2. How is the idea of freezing hidden states applied in other works? sent40
    4.3. How is DeeBERT applied to document ranking? sent41
    4.4. What is ELUE and what does it evaluate? sent42, sent43","2. What is another stream of research in early exiting? sent24
    2.1. How does BERxiT implement learning-based early exit? sent25, sent26, sent27
    2.2. What is the approach of CAT for early exiting? sent28

3. What is cascading in the context of early exit? sent29
    3.1. What did Li et al. (2021a) find about shallow features and internal classifiers? sent30
    3.2. What is the proposal of Li et al. (2021a) for cascading? sent31, sent32, sent33

4. What are some applications of early exit beyond classification? sent34
    4.1. What is Token-Level Early-Exit and how does it work? sent35, sent36, sent37, sent38, sent39
    4.2. How is the idea of freezing hidden states applied in other works? sent40
    4.3. How is DeeBERT applied to document ranking? sent41
    4.4. What is ELUE and what does it evaluate? sent42, sent43","2. What is another stream of research in early exiting? sent24
    2.1. How does BERxiT implement learning-based early exit? sent25, sent26, sent27
    2.2. What is the approach of CAT for early exiting? sent28","Question: How does BERxiT utilize learning-based early exit to improve prediction accuracy in dynamic neural networks?

Supporting sentence: sent25, sent26, sent27",How does BERxiT utilize learning-based early exit to improve prediction accuracy in dynamic neural networks?,"sent25, sent26, sent27",2,implement,"BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of the Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations.

They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction.

It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit.","Question:

How does BERxiT utilize learning-based early exit to improve prediction accuracy in dynamic neural networks?

Answer:

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of the Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations.

They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction.

It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit."
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,Internal classifier training Exit criterion,21,"DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy < θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ ) Voting  joint; sum of CE + diversity loss accumulated votes > θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ ) Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)

BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θ

CascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.

Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.  further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair. They optimize these weights by a cross-level optimization algorithm.

They adopt PABEE's patience-based strategy for exiting. Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners. Entropy is used as the exit criterion. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods. The inference is terminated when multiple layers are confident.

Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit. CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.

Cascading Cascading can be seen as a special form of early exit, performed at the model level. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading. CascadeBERT executes models one by one, from the smallest to the largest. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.

Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling. They use the maximum class probability as confidence on a per-token basis. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens. The model completely exits when every token exits. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","sent1: DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy <
sent2: θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ )
sent3: Voting  joint; sum of CE + diversity loss accumulated votes >
sent4: θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ )
sent5: Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θCascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.
sent6: Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation.
sent7: That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded.
sent8: Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions.
sent9: Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training.
sent10: They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers.
sent11: For inference, the model exits when k consecutive internal classifiers make the same prediction.
sent12: Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.
sent13: further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution.
sent14: They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold.
sent15: Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other.
sent16: However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair.
sent17: They optimize these weights by a cross-level optimization algorithm.
sent18: They adopt PABEE's patience-based strategy for exiting.
sent19: Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states.
sent20: For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners.
sent21: Entropy is used as the exit criterion.
sent22: PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods.
sent23: The inference is terminated when multiple layers are confident.
sent24: Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.
sent25: BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations.
sent26: They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction.
sent27: It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit.
sent28: CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.
sent29: Cascading Cascading can be seen as a special form of early exit, performed at the model level.
sent30: Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers.
sent31: Therefore, they propose to use a suite of complete models with different numbers of layers for cascading.
sent32: CascadeBERT executes models one by one, from the smallest to the largest.
sent33: It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.
sent34: Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings.
sent35: Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling.
sent36: They use the maximum class probability as confidence on a per-token basis.
sent37: Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers.
sent38: These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens.
sent39: The model completely exits when every token exits.
sent40: A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification.
sent41: Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking.
sent42: ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane.
sent43: They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","1. What is the focus of the section on Internal classifier training Exit criterion? sent1
    1.1. What are some methods mentioned for internal classifier training and exit criteria? sent1, sent2, sent3, sent4, sent5
    1.2. What is a drawback of confidence-based early exit? sent6
        1.2.1. How does ensemble-based early exit address this drawback? sent7, sent8
    1.3. How does PABEE improve performance and efficiency? sent9, sent10, sent11
        1.3.1. What additional benefit does PABEE provide? sent12
        1.3.2. What mechanism does PABEE use to encourage diversity? sent13, sent14
    1.4. How does LeeBERT differ from PABEE in its approach? sent15, sent16, sent17
        1.4.1. What strategy does LeeBERT adopt for exiting? sent18
    1.5. What is the approach of Liao et al. (2021) for training and inference? sent19, sent20, sent21
    1.6. How does PCEE-BERT combine different early exit methods? sent22, sent23
2. What is another stream of research in early exiting? sent24
    2.1. How does BERxiT implement learning-based early exit? sent25, sent26, sent27
    2.2. What is the approach of CAT for early exiting? sent28
3. What is cascading in the context of early exit? sent29
    3.1. What did Li et al. (2021a) find about shallow features and internal classifiers? sent30
    3.2. What is the proposal of Li et al. (2021a) for cascading? sent31, sent32, sent33
4. What are some applications of early exit beyond classification? sent34
    4.1. What is Token-Level Early-Exit and how does it work? sent35, sent36, sent37, sent38, sent39
    4.2. How is the idea of freezing hidden states applied in other works? sent40
    4.3. How is DeeBERT applied to document ranking? sent41
    4.4. What is ELUE and what does it evaluate? sent42, sent43","2. What is another stream of research in early exiting? sent24
    2.1. How does BERxiT implement learning-based early exit? sent25, sent26, sent27
    2.2. What is the approach of CAT for early exiting? sent28

3. What is cascading in the context of early exit? sent29
    3.1. What did Li et al. (2021a) find about shallow features and internal classifiers? sent30
    3.2. What is the proposal of Li et al. (2021a) for cascading? sent31, sent32, sent33

4. What are some applications of early exit beyond classification? sent34
    4.1. What is Token-Level Early-Exit and how does it work? sent35, sent36, sent37, sent38, sent39
    4.2. How is the idea of freezing hidden states applied in other works? sent40
    4.3. How is DeeBERT applied to document ranking? sent41
    4.4. What is ELUE and what does it evaluate? sent42, sent43","3. What is cascading in the context of early exit? sent29
    3.1. What did Li et al. (2021a) find about shallow features and internal classifiers? sent30
    3.2. What is the proposal of Li et al. (2021a) for cascading? sent31, sent32, sent33","Question: How does CascadeBERT utilize cascading to improve early exit strategies in neural networks?

Supporting sentence: sent31, sent32, sent33",How does CascadeBERT utilize cascading to improve early exit strategies in neural networks?,"sent31, sent32, sent33",2,implement,"CascadeBERT utilizes cascading as a special form of early exit, performed at the model level.

Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers.

Therefore, they propose to use a suite of complete models with different numbers of layers for cascading.

CascadeBERT executes models one by one, from the smallest to the largest.

It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.","Question:

How does CascadeBERT utilize cascading to improve early exit strategies in neural networks?

Answer:

CascadeBERT utilizes cascading as a special form of early exit, performed at the model level.

Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers.

Therefore, they propose to use a suite of complete models with different numbers of layers for cascading.

CascadeBERT executes models one by one, from the smallest to the largest.

It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold."
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,Internal classifier training Exit criterion,21,"DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy < θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ ) Voting  joint; sum of CE + diversity loss accumulated votes > θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ ) Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)

BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θ

CascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.

Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.  further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair. They optimize these weights by a cross-level optimization algorithm.

They adopt PABEE's patience-based strategy for exiting. Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners. Entropy is used as the exit criterion. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods. The inference is terminated when multiple layers are confident.

Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit. CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.

Cascading Cascading can be seen as a special form of early exit, performed at the model level. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading. CascadeBERT executes models one by one, from the smallest to the largest. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.

Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling. They use the maximum class probability as confidence on a per-token basis. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens. The model completely exits when every token exits. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","sent1: DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy <
sent2: θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ )
sent3: Voting  joint; sum of CE + diversity loss accumulated votes >
sent4: θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ )
sent5: Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θCascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.
sent6: Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation.
sent7: That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded.
sent8: Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions.
sent9: Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training.
sent10: They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers.
sent11: For inference, the model exits when k consecutive internal classifiers make the same prediction.
sent12: Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.
sent13: further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution.
sent14: They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold.
sent15: Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other.
sent16: However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair.
sent17: They optimize these weights by a cross-level optimization algorithm.
sent18: They adopt PABEE's patience-based strategy for exiting.
sent19: Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states.
sent20: For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners.
sent21: Entropy is used as the exit criterion.
sent22: PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods.
sent23: The inference is terminated when multiple layers are confident.
sent24: Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.
sent25: BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations.
sent26: They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction.
sent27: It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit.
sent28: CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.
sent29: Cascading Cascading can be seen as a special form of early exit, performed at the model level.
sent30: Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers.
sent31: Therefore, they propose to use a suite of complete models with different numbers of layers for cascading.
sent32: CascadeBERT executes models one by one, from the smallest to the largest.
sent33: It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.
sent34: Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings.
sent35: Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling.
sent36: They use the maximum class probability as confidence on a per-token basis.
sent37: Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers.
sent38: These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens.
sent39: The model completely exits when every token exits.
sent40: A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification.
sent41: Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking.
sent42: ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane.
sent43: They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","1. What is the focus of the section on Internal classifier training Exit criterion? sent1
    1.1. What are some methods mentioned for internal classifier training and exit criteria? sent1, sent2, sent3, sent4, sent5
    1.2. What is a drawback of confidence-based early exit? sent6
        1.2.1. How does ensemble-based early exit address this drawback? sent7, sent8
    1.3. How does PABEE improve performance and efficiency? sent9, sent10, sent11
        1.3.1. What additional benefit does PABEE provide? sent12
        1.3.2. What mechanism does PABEE use to encourage diversity? sent13, sent14
    1.4. How does LeeBERT differ from PABEE in its approach? sent15, sent16, sent17
        1.4.1. What strategy does LeeBERT adopt for exiting? sent18
    1.5. What is the approach of Liao et al. (2021) for training and inference? sent19, sent20, sent21
    1.6. How does PCEE-BERT combine different early exit methods? sent22, sent23
2. What is another stream of research in early exiting? sent24
    2.1. How does BERxiT implement learning-based early exit? sent25, sent26, sent27
    2.2. What is the approach of CAT for early exiting? sent28
3. What is cascading in the context of early exit? sent29
    3.1. What did Li et al. (2021a) find about shallow features and internal classifiers? sent30
    3.2. What is the proposal of Li et al. (2021a) for cascading? sent31, sent32, sent33
4. What are some applications of early exit beyond classification? sent34
    4.1. What is Token-Level Early-Exit and how does it work? sent35, sent36, sent37, sent38, sent39
    4.2. How is the idea of freezing hidden states applied in other works? sent40
    4.3. How is DeeBERT applied to document ranking? sent41
    4.4. What is ELUE and what does it evaluate? sent42, sent43","2. What is another stream of research in early exiting? sent24
    2.1. How does BERxiT implement learning-based early exit? sent25, sent26, sent27
    2.2. What is the approach of CAT for early exiting? sent28

3. What is cascading in the context of early exit? sent29
    3.1. What did Li et al. (2021a) find about shallow features and internal classifiers? sent30
    3.2. What is the proposal of Li et al. (2021a) for cascading? sent31, sent32, sent33

4. What are some applications of early exit beyond classification? sent34
    4.1. What is Token-Level Early-Exit and how does it work? sent35, sent36, sent37, sent38, sent39
    4.2. How is the idea of freezing hidden states applied in other works? sent40
    4.3. How is DeeBERT applied to document ranking? sent41
    4.4. What is ELUE and what does it evaluate? sent42, sent43","4. What are some applications of early exit beyond classification? sent34
    4.1. What is Token-Level Early-Exit and how does it work? sent35, sent36, sent37, sent38, sent39
    4.2. How is the idea of freezing hidden states applied in other works? sent40
    4.3. How is DeeBERT applied to document ranking? sent41
    4.4. What is ELUE and what does it evaluate? sent42, sent43","Question: How is the concept of early exit applied to document ranking tasks, and what challenges does it address?

Supporting sentence: sent41","How is the concept of early exit applied to document ranking tasks, and what challenges does it address?",sent41,2,implement,"DeeBERT (Xin et al., 2020b) is applied to document ranking by setting different thresholds for the negative and positive classes for early exiting.

This approach is designed to accommodate the imbalanced class distribution in document ranking.","Question:

How is the concept of early exit applied to document ranking tasks, and what challenges does it address?

Answer:

DeeBERT (Xin et al., 2020b) is applied to document ranking by setting different thresholds for the negative and positive classes for early exiting.

This approach is designed to accommodate the imbalanced class distribution in document ranking."
248426721,What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,https://www.semanticscholar.org/paper/29a369d83a7d6a49f6a3259bee23bd4d95db0b16,The Relation Extraction Task,11,"Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","sent1: Conceptually, RE involves a pipeline of steps (see Figure 2).
sent2: Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.
sent3: Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).
sent4: 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.
sent5: One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.
sent6: This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
sent7: As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered).
sent8: The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.
sent9: Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.
sent10: Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).
sent11: Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.
sent12: Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).
sent13: Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set.
sent14: 6 What Do You Mean by Relation Extraction?
sent15: RE studies rarely address the whole pipeline.
sent16: We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.
sent17: 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC).
sent18: Table 2 shows such investigation.
sent19: We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE.
sent20: The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.
sent21: Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020).
sent22: Our analysis further shows that it is difficult to determine the RI setup.
sent23: While RC is always performed, the situation is different for RI (or no-rel).
sent24: Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances).
sent25: As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled.
sent26: These details are utterly important as they impact both model estimation and evaluation.
sent27: Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.
sent28: Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021).
sent29: However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models.
sent30: They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a).
sent31: They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
sent32: While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
sent33: Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.
sent34: Since the output label space is different, separate encoders could better capture distinct contextual information.
sent35: At the moment it is not clear if one approach is more suitable than the other for RE.
sent36: For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","1. What does the Relation Extraction (RE) task involve conceptually? sent1
    1.1. What is the first step in the RE pipeline? sent2
        1.1.1. What types of entities are involved in the first step of RE? sent3
    1.2. How do approaches vary after entities are identified in RE? sent4
        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8
        1.2.2. How do some studies merge RI and RC? sent9
        1.2.3. How do other studies simplify the RE task? sent10
        1.2.4. How is RI influenced by the RC setup? sent11
        1.2.5. How do some studies address the approximation in RI? sent12
            1.2.5.1. How does the NOTA label differ from no-rel? sent13
2. How often do RE studies address the whole pipeline? sent15
    2.1. How is entity extraction sometimes divided? sent16
    2.2. What does the analysis of ACL papers reveal about RE sub-tasks? sent17
        2.2.1. What is shown in Table 2? sent18
        2.2.2. What types of papers are excluded from the analysis? sent19
        2.2.3. What assumption is usually made about entities in RE? sent20
        2.2.4. What is lacking in end-to-end models according to recent work? sent21
        2.2.5. What difficulty is highlighted in determining the RI setup? sent22
        2.2.6. How is RC different from RI in terms of performance? sent23
        2.2.7. What is the issue with RI in some papers? sent24
        2.2.8. What proposal is made to improve clarity in RE studies? sent25
            2.2.8.1. Why are these details important? sent26
3. What is the traditional RE pipeline prone to? sent27
    3.1. What approaches have been proposed to alleviate error propagation? sent28
    3.2. What challenge is discussed by Taillé et al. (2020) regarding joint models? sent29
        3.2.1. What did they survey in recent works on end-to-end RE? sent30
        3.2.2. What observations did they make about end-to-end models? sent31
    3.3. What is still an open question regarding RE approaches? sent32
        3.3.1. What did Zhong and Chen (2021) find about using separate encoders? sent33
            3.3.1.1. Why might separate encoders be beneficial? sent34
        3.3.2. What is unclear about the suitability of RE approaches? sent35
4. What approach does the paper follow for RE and why? sent36","1. What does the Relation Extraction (RE) task involve conceptually? sent1
    1.1. What is the first step in the RE pipeline? sent2
        1.1.1. What types of entities are involved in the first step of RE? sent3
    1.2. How do approaches vary after entities are identified in RE? sent4
        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8
        1.2.2. How do some studies merge RI and RC? sent9
        1.2.3. How do other studies simplify the RE task? sent10
        1.2.4. How is RI influenced by the RC setup? sent11
        1.2.5. How do some studies address the approximation in RI? sent12
            1.2.5.1. How does the NOTA label differ from no-rel? sent13

    1.2. How do approaches vary after entities are identified in RE? sent4
        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8
        1.2.2. How do some studies merge RI and RC? sent9
        1.2.3. How do other studies simplify the RE task? sent10
        1.2.4. How is RI influenced by the RC setup? sent11
        1.2.5. How do some studies address the approximation in RI? sent12
            1.2.5.1. How does the NOTA label differ from no-rel? sent13

        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8

2. How often do RE studies address the whole pipeline? sent15
    2.1. How is entity extraction sometimes divided? sent16
    2.2. What does the analysis of ACL papers reveal about RE sub-tasks? sent17
        2.2.1. What is shown in Table 2? sent18
        2.2.2. What types of papers are excluded from the analysis? sent19
        2.2.3. What assumption is usually made about entities in RE? sent20
        2.2.4. What is lacking in end-to-end models according to recent work? sent21
        2.2.5. What difficulty is highlighted in determining the RI setup? sent22
        2.2.6. How is RC different from RI in terms of performance? sent23
        2.2.7. What is the issue with RI in some papers? sent24
        2.2.8. What proposal is made to improve clarity in RE studies? sent25
            2.2.8.1. Why are these details important? sent26

    2.2. What does the analysis of ACL papers reveal about RE sub-tasks? sent17
        2.2.1. What is shown in Table 2? sent18
        2.2.2. What types of papers are excluded from the analysis? sent19
        2.2.3. What assumption is usually made about entities in RE? sent20
        2.2.4. What is lacking in end-to-end models according to recent work? sent21
        2.2.5. What difficulty is highlighted in determining the RI setup? sent22
        2.2.6. How is RC different from RI in terms of performance? sent23
        2.2.7. What is the issue with RI in some papers? sent24
        2.2.8. What proposal is made to improve clarity in RE studies? sent25
            2.2.8.1. Why are these details important? sent26

3. What is the traditional RE pipeline prone to? sent27
    3.1. What approaches have been proposed to alleviate error propagation? sent28
    3.2. What challenge is discussed by Taillé et al. (2020) regarding joint models? sent29
        3.2.1. What did they survey in recent works on end-to-end RE? sent30
        3.2.2. What observations did they make about end-to-end models? sent31
    3.3. What is still an open question regarding RE approaches? sent32
        3.3.1. What did Zhong and Chen (2021) find about using separate encoders? sent33
            3.3.1.1. Why might separate encoders be beneficial? sent34
        3.3.2. What is unclear about the suitability of RE approaches? sent35

    3.3. What is still an open question regarding RE approaches? sent32
        3.3.1. What did Zhong and Chen (2021) find about using separate encoders? sent33
            3.3.1.1. Why might separate encoders be beneficial? sent34
        3.3.2. What is unclear about the suitability of RE approaches? sent35","1. What does the Relation Extraction (RE) task involve conceptually? sent1
    1.1. What is the first step in the RE pipeline? sent2
        1.1.1. What types of entities are involved in the first step of RE? sent3
    1.2. How do approaches vary after entities are identified in RE? sent4
        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8
        1.2.2. How do some studies merge RI and RC? sent9
        1.2.3. How do other studies simplify the RE task? sent10
        1.2.4. How is RI influenced by the RC setup? sent11
        1.2.5. How do some studies address the approximation in RI? sent12
            1.2.5.1. How does the NOTA label differ from no-rel? sent13","Question: How do some studies address the approximation in Relation Identification (RI) within the Relation Extraction (RE) task?

Supporting sentence: sent12, sent13",How do some studies address the approximation in Relation Identification (RI) within the Relation Extraction (RE) task?,"sent12, sent13",2,implement,"Some studies address the approximation in Relation Identification (RI) by distinguishing between the no-relation (no-rel) and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).

In this context, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered Relation Classification (RC) label set.","Question:

How do some studies address the approximation in Relation Identification (RI) within the Relation Extraction (RE) task?

Answer:

Some studies address the approximation in Relation Identification (RI) by distinguishing between the no-relation (no-rel) and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).

In this context, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered Relation Classification (RC) label set."
248426721,What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,https://www.semanticscholar.org/paper/29a369d83a7d6a49f6a3259bee23bd4d95db0b16,The Relation Extraction Task,11,"Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","sent1: Conceptually, RE involves a pipeline of steps (see Figure 2).
sent2: Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.
sent3: Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).
sent4: 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.
sent5: One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.
sent6: This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
sent7: As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered).
sent8: The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.
sent9: Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.
sent10: Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).
sent11: Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.
sent12: Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).
sent13: Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set.
sent14: 6 What Do You Mean by Relation Extraction?
sent15: RE studies rarely address the whole pipeline.
sent16: We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.
sent17: 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC).
sent18: Table 2 shows such investigation.
sent19: We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE.
sent20: The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.
sent21: Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020).
sent22: Our analysis further shows that it is difficult to determine the RI setup.
sent23: While RC is always performed, the situation is different for RI (or no-rel).
sent24: Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances).
sent25: As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled.
sent26: These details are utterly important as they impact both model estimation and evaluation.
sent27: Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.
sent28: Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021).
sent29: However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models.
sent30: They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a).
sent31: They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
sent32: While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
sent33: Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.
sent34: Since the output label space is different, separate encoders could better capture distinct contextual information.
sent35: At the moment it is not clear if one approach is more suitable than the other for RE.
sent36: For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","1. What does the Relation Extraction (RE) task involve conceptually? sent1
    1.1. What is the first step in the RE pipeline? sent2
        1.1.1. What types of entities are involved in the first step of RE? sent3
    1.2. How do approaches vary after entities are identified in RE? sent4
        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8
        1.2.2. How do some studies merge RI and RC? sent9
        1.2.3. How do other studies simplify the RE task? sent10
        1.2.4. How is RI influenced by the RC setup? sent11
        1.2.5. How do some studies address the approximation in RI? sent12
            1.2.5.1. How does the NOTA label differ from no-rel? sent13
2. How often do RE studies address the whole pipeline? sent15
    2.1. How is entity extraction sometimes divided? sent16
    2.2. What does the analysis of ACL papers reveal about RE sub-tasks? sent17
        2.2.1. What is shown in Table 2? sent18
        2.2.2. What types of papers are excluded from the analysis? sent19
        2.2.3. What assumption is usually made about entities in RE? sent20
        2.2.4. What is lacking in end-to-end models according to recent work? sent21
        2.2.5. What difficulty is highlighted in determining the RI setup? sent22
        2.2.6. How is RC different from RI in terms of performance? sent23
        2.2.7. What is the issue with RI in some papers? sent24
        2.2.8. What proposal is made to improve clarity in RE studies? sent25
            2.2.8.1. Why are these details important? sent26
3. What is the traditional RE pipeline prone to? sent27
    3.1. What approaches have been proposed to alleviate error propagation? sent28
    3.2. What challenge is discussed by Taillé et al. (2020) regarding joint models? sent29
        3.2.1. What did they survey in recent works on end-to-end RE? sent30
        3.2.2. What observations did they make about end-to-end models? sent31
    3.3. What is still an open question regarding RE approaches? sent32
        3.3.1. What did Zhong and Chen (2021) find about using separate encoders? sent33
            3.3.1.1. Why might separate encoders be beneficial? sent34
        3.3.2. What is unclear about the suitability of RE approaches? sent35
4. What approach does the paper follow for RE and why? sent36","1. What does the Relation Extraction (RE) task involve conceptually? sent1
    1.1. What is the first step in the RE pipeline? sent2
        1.1.1. What types of entities are involved in the first step of RE? sent3
    1.2. How do approaches vary after entities are identified in RE? sent4
        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8
        1.2.2. How do some studies merge RI and RC? sent9
        1.2.3. How do other studies simplify the RE task? sent10
        1.2.4. How is RI influenced by the RC setup? sent11
        1.2.5. How do some studies address the approximation in RI? sent12
            1.2.5.1. How does the NOTA label differ from no-rel? sent13

    1.2. How do approaches vary after entities are identified in RE? sent4
        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8
        1.2.2. How do some studies merge RI and RC? sent9
        1.2.3. How do other studies simplify the RE task? sent10
        1.2.4. How is RI influenced by the RC setup? sent11
        1.2.5. How do some studies address the approximation in RI? sent12
            1.2.5.1. How does the NOTA label differ from no-rel? sent13

        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8

2. How often do RE studies address the whole pipeline? sent15
    2.1. How is entity extraction sometimes divided? sent16
    2.2. What does the analysis of ACL papers reveal about RE sub-tasks? sent17
        2.2.1. What is shown in Table 2? sent18
        2.2.2. What types of papers are excluded from the analysis? sent19
        2.2.3. What assumption is usually made about entities in RE? sent20
        2.2.4. What is lacking in end-to-end models according to recent work? sent21
        2.2.5. What difficulty is highlighted in determining the RI setup? sent22
        2.2.6. How is RC different from RI in terms of performance? sent23
        2.2.7. What is the issue with RI in some papers? sent24
        2.2.8. What proposal is made to improve clarity in RE studies? sent25
            2.2.8.1. Why are these details important? sent26

    2.2. What does the analysis of ACL papers reveal about RE sub-tasks? sent17
        2.2.1. What is shown in Table 2? sent18
        2.2.2. What types of papers are excluded from the analysis? sent19
        2.2.3. What assumption is usually made about entities in RE? sent20
        2.2.4. What is lacking in end-to-end models according to recent work? sent21
        2.2.5. What difficulty is highlighted in determining the RI setup? sent22
        2.2.6. How is RC different from RI in terms of performance? sent23
        2.2.7. What is the issue with RI in some papers? sent24
        2.2.8. What proposal is made to improve clarity in RE studies? sent25
            2.2.8.1. Why are these details important? sent26

3. What is the traditional RE pipeline prone to? sent27
    3.1. What approaches have been proposed to alleviate error propagation? sent28
    3.2. What challenge is discussed by Taillé et al. (2020) regarding joint models? sent29
        3.2.1. What did they survey in recent works on end-to-end RE? sent30
        3.2.2. What observations did they make about end-to-end models? sent31
    3.3. What is still an open question regarding RE approaches? sent32
        3.3.1. What did Zhong and Chen (2021) find about using separate encoders? sent33
            3.3.1.1. Why might separate encoders be beneficial? sent34
        3.3.2. What is unclear about the suitability of RE approaches? sent35

    3.3. What is still an open question regarding RE approaches? sent32
        3.3.1. What did Zhong and Chen (2021) find about using separate encoders? sent33
            3.3.1.1. Why might separate encoders be beneficial? sent34
        3.3.2. What is unclear about the suitability of RE approaches? sent35","    1.2. How do approaches vary after entities are identified in RE? sent4
        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8
        1.2.2. How do some studies merge RI and RC? sent9
        1.2.3. How do other studies simplify the RE task? sent10
        1.2.4. How is RI influenced by the RC setup? sent11
        1.2.5. How do some studies address the approximation in RI? sent12
            1.2.5.1. How does the NOTA label differ from no-rel? sent13","Question: How do some studies address the approximation in Relation Identification (RI) within the Relation Extraction (RE) task?

Supporting sentence: sent12, sent13",How do some studies address the approximation in Relation Identification (RI) within the Relation Extraction (RE) task?,"sent12, sent13",2,implement,"Some studies address the approximation in Relation Identification (RI) by distinguishing between the no-relation (no-rel) and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).

In this context, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered Relation Classification (RC) label set.","Question:

How do some studies address the approximation in Relation Identification (RI) within the Relation Extraction (RE) task?

Answer:

Some studies address the approximation in Relation Identification (RI) by distinguishing between the no-relation (no-rel) and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).

In this context, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered Relation Classification (RC) label set."
248426721,What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,https://www.semanticscholar.org/paper/29a369d83a7d6a49f6a3259bee23bd4d95db0b16,The Relation Extraction Task,11,"Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","sent1: Conceptually, RE involves a pipeline of steps (see Figure 2).
sent2: Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.
sent3: Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).
sent4: 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.
sent5: One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.
sent6: This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
sent7: As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered).
sent8: The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.
sent9: Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.
sent10: Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).
sent11: Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.
sent12: Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).
sent13: Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set.
sent14: 6 What Do You Mean by Relation Extraction?
sent15: RE studies rarely address the whole pipeline.
sent16: We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.
sent17: 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC).
sent18: Table 2 shows such investigation.
sent19: We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE.
sent20: The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.
sent21: Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020).
sent22: Our analysis further shows that it is difficult to determine the RI setup.
sent23: While RC is always performed, the situation is different for RI (or no-rel).
sent24: Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances).
sent25: As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled.
sent26: These details are utterly important as they impact both model estimation and evaluation.
sent27: Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.
sent28: Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021).
sent29: However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models.
sent30: They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a).
sent31: They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
sent32: While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
sent33: Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.
sent34: Since the output label space is different, separate encoders could better capture distinct contextual information.
sent35: At the moment it is not clear if one approach is more suitable than the other for RE.
sent36: For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","1. What does the Relation Extraction (RE) task involve conceptually? sent1
    1.1. What is the first step in the RE pipeline? sent2
        1.1.1. What types of entities are involved in the first step of RE? sent3
    1.2. How do approaches vary after entities are identified in RE? sent4
        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8
        1.2.2. How do some studies merge RI and RC? sent9
        1.2.3. How do other studies simplify the RE task? sent10
        1.2.4. How is RI influenced by the RC setup? sent11
        1.2.5. How do some studies address the approximation in RI? sent12
            1.2.5.1. How does the NOTA label differ from no-rel? sent13
2. How often do RE studies address the whole pipeline? sent15
    2.1. How is entity extraction sometimes divided? sent16
    2.2. What does the analysis of ACL papers reveal about RE sub-tasks? sent17
        2.2.1. What is shown in Table 2? sent18
        2.2.2. What types of papers are excluded from the analysis? sent19
        2.2.3. What assumption is usually made about entities in RE? sent20
        2.2.4. What is lacking in end-to-end models according to recent work? sent21
        2.2.5. What difficulty is highlighted in determining the RI setup? sent22
        2.2.6. How is RC different from RI in terms of performance? sent23
        2.2.7. What is the issue with RI in some papers? sent24
        2.2.8. What proposal is made to improve clarity in RE studies? sent25
            2.2.8.1. Why are these details important? sent26
3. What is the traditional RE pipeline prone to? sent27
    3.1. What approaches have been proposed to alleviate error propagation? sent28
    3.2. What challenge is discussed by Taillé et al. (2020) regarding joint models? sent29
        3.2.1. What did they survey in recent works on end-to-end RE? sent30
        3.2.2. What observations did they make about end-to-end models? sent31
    3.3. What is still an open question regarding RE approaches? sent32
        3.3.1. What did Zhong and Chen (2021) find about using separate encoders? sent33
            3.3.1.1. Why might separate encoders be beneficial? sent34
        3.3.2. What is unclear about the suitability of RE approaches? sent35
4. What approach does the paper follow for RE and why? sent36","1. What does the Relation Extraction (RE) task involve conceptually? sent1
    1.1. What is the first step in the RE pipeline? sent2
        1.1.1. What types of entities are involved in the first step of RE? sent3
    1.2. How do approaches vary after entities are identified in RE? sent4
        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8
        1.2.2. How do some studies merge RI and RC? sent9
        1.2.3. How do other studies simplify the RE task? sent10
        1.2.4. How is RI influenced by the RC setup? sent11
        1.2.5. How do some studies address the approximation in RI? sent12
            1.2.5.1. How does the NOTA label differ from no-rel? sent13

    1.2. How do approaches vary after entities are identified in RE? sent4
        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8
        1.2.2. How do some studies merge RI and RC? sent9
        1.2.3. How do other studies simplify the RE task? sent10
        1.2.4. How is RI influenced by the RC setup? sent11
        1.2.5. How do some studies address the approximation in RI? sent12
            1.2.5.1. How does the NOTA label differ from no-rel? sent13

        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8

2. How often do RE studies address the whole pipeline? sent15
    2.1. How is entity extraction sometimes divided? sent16
    2.2. What does the analysis of ACL papers reveal about RE sub-tasks? sent17
        2.2.1. What is shown in Table 2? sent18
        2.2.2. What types of papers are excluded from the analysis? sent19
        2.2.3. What assumption is usually made about entities in RE? sent20
        2.2.4. What is lacking in end-to-end models according to recent work? sent21
        2.2.5. What difficulty is highlighted in determining the RI setup? sent22
        2.2.6. How is RC different from RI in terms of performance? sent23
        2.2.7. What is the issue with RI in some papers? sent24
        2.2.8. What proposal is made to improve clarity in RE studies? sent25
            2.2.8.1. Why are these details important? sent26

    2.2. What does the analysis of ACL papers reveal about RE sub-tasks? sent17
        2.2.1. What is shown in Table 2? sent18
        2.2.2. What types of papers are excluded from the analysis? sent19
        2.2.3. What assumption is usually made about entities in RE? sent20
        2.2.4. What is lacking in end-to-end models according to recent work? sent21
        2.2.5. What difficulty is highlighted in determining the RI setup? sent22
        2.2.6. How is RC different from RI in terms of performance? sent23
        2.2.7. What is the issue with RI in some papers? sent24
        2.2.8. What proposal is made to improve clarity in RE studies? sent25
            2.2.8.1. Why are these details important? sent26

3. What is the traditional RE pipeline prone to? sent27
    3.1. What approaches have been proposed to alleviate error propagation? sent28
    3.2. What challenge is discussed by Taillé et al. (2020) regarding joint models? sent29
        3.2.1. What did they survey in recent works on end-to-end RE? sent30
        3.2.2. What observations did they make about end-to-end models? sent31
    3.3. What is still an open question regarding RE approaches? sent32
        3.3.1. What did Zhong and Chen (2021) find about using separate encoders? sent33
            3.3.1.1. Why might separate encoders be beneficial? sent34
        3.3.2. What is unclear about the suitability of RE approaches? sent35

    3.3. What is still an open question regarding RE approaches? sent32
        3.3.1. What did Zhong and Chen (2021) find about using separate encoders? sent33
            3.3.1.1. Why might separate encoders be beneficial? sent34
        3.3.2. What is unclear about the suitability of RE approaches? sent35","        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8","Question: How is the Relation Identification (RI) step conducted in the Relation Extraction (RE) pipeline, and why are heuristics necessary?

Supporting sentence: sent6, sent7","How is the Relation Identification (RI) step conducted in the Relation Extraction (RE) pipeline, and why are heuristics necessary?","sent6, sent7",2,implement,"The Relation Identification (RI) step in the Relation Extraction (RE) pipeline involves identifying from all the possible entity pairs the ones which are in some kind of relation via a binary classification task.

The proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015).

A priori heuristics are generally applied to reduce the possible combinations, such as excluding entity pairs involving distant entities or entity type pairs not licensed by the relations.","Question:

How is the Relation Identification (RI) step conducted in the Relation Extraction (RE) pipeline, and why are heuristics necessary?

Answer:

The Relation Identification (RI) step in the Relation Extraction (RE) pipeline involves identifying from all the possible entity pairs the ones which are in some kind of relation via a binary classification task.

The proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015).

A priori heuristics are generally applied to reduce the possible combinations, such as excluding entity pairs involving distant entities or entity type pairs not licensed by the relations."
248426721,What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,https://www.semanticscholar.org/paper/29a369d83a7d6a49f6a3259bee23bd4d95db0b16,The Relation Extraction Task,11,"Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","sent1: Conceptually, RE involves a pipeline of steps (see Figure 2).
sent2: Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.
sent3: Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).
sent4: 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.
sent5: One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.
sent6: This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
sent7: As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered).
sent8: The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.
sent9: Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.
sent10: Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).
sent11: Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.
sent12: Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).
sent13: Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set.
sent14: 6 What Do You Mean by Relation Extraction?
sent15: RE studies rarely address the whole pipeline.
sent16: We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.
sent17: 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC).
sent18: Table 2 shows such investigation.
sent19: We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE.
sent20: The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.
sent21: Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020).
sent22: Our analysis further shows that it is difficult to determine the RI setup.
sent23: While RC is always performed, the situation is different for RI (or no-rel).
sent24: Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances).
sent25: As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled.
sent26: These details are utterly important as they impact both model estimation and evaluation.
sent27: Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.
sent28: Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021).
sent29: However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models.
sent30: They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a).
sent31: They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
sent32: While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
sent33: Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.
sent34: Since the output label space is different, separate encoders could better capture distinct contextual information.
sent35: At the moment it is not clear if one approach is more suitable than the other for RE.
sent36: For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","1. What does the Relation Extraction (RE) task involve conceptually? sent1
    1.1. What is the first step in the RE pipeline? sent2
        1.1.1. What types of entities are involved in the first step of RE? sent3
    1.2. How do approaches vary after entities are identified in RE? sent4
        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8
        1.2.2. How do some studies merge RI and RC? sent9
        1.2.3. How do other studies simplify the RE task? sent10
        1.2.4. How is RI influenced by the RC setup? sent11
        1.2.5. How do some studies address the approximation in RI? sent12
            1.2.5.1. How does the NOTA label differ from no-rel? sent13
2. How often do RE studies address the whole pipeline? sent15
    2.1. How is entity extraction sometimes divided? sent16
    2.2. What does the analysis of ACL papers reveal about RE sub-tasks? sent17
        2.2.1. What is shown in Table 2? sent18
        2.2.2. What types of papers are excluded from the analysis? sent19
        2.2.3. What assumption is usually made about entities in RE? sent20
        2.2.4. What is lacking in end-to-end models according to recent work? sent21
        2.2.5. What difficulty is highlighted in determining the RI setup? sent22
        2.2.6. How is RC different from RI in terms of performance? sent23
        2.2.7. What is the issue with RI in some papers? sent24
        2.2.8. What proposal is made to improve clarity in RE studies? sent25
            2.2.8.1. Why are these details important? sent26
3. What is the traditional RE pipeline prone to? sent27
    3.1. What approaches have been proposed to alleviate error propagation? sent28
    3.2. What challenge is discussed by Taillé et al. (2020) regarding joint models? sent29
        3.2.1. What did they survey in recent works on end-to-end RE? sent30
        3.2.2. What observations did they make about end-to-end models? sent31
    3.3. What is still an open question regarding RE approaches? sent32
        3.3.1. What did Zhong and Chen (2021) find about using separate encoders? sent33
            3.3.1.1. Why might separate encoders be beneficial? sent34
        3.3.2. What is unclear about the suitability of RE approaches? sent35
4. What approach does the paper follow for RE and why? sent36","1. What does the Relation Extraction (RE) task involve conceptually? sent1
    1.1. What is the first step in the RE pipeline? sent2
        1.1.1. What types of entities are involved in the first step of RE? sent3
    1.2. How do approaches vary after entities are identified in RE? sent4
        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8
        1.2.2. How do some studies merge RI and RC? sent9
        1.2.3. How do other studies simplify the RE task? sent10
        1.2.4. How is RI influenced by the RC setup? sent11
        1.2.5. How do some studies address the approximation in RI? sent12
            1.2.5.1. How does the NOTA label differ from no-rel? sent13

    1.2. How do approaches vary after entities are identified in RE? sent4
        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8
        1.2.2. How do some studies merge RI and RC? sent9
        1.2.3. How do other studies simplify the RE task? sent10
        1.2.4. How is RI influenced by the RC setup? sent11
        1.2.5. How do some studies address the approximation in RI? sent12
            1.2.5.1. How does the NOTA label differ from no-rel? sent13

        1.2.1. What is one way to approach RE after entity identification? sent5
            1.2.1.1. What does Relation Identification (RI) involve? sent6
            1.2.1.2. Why are a priori heuristics applied in RI? sent7
            1.2.1.3. What is the last step in the RE pipeline? sent8

2. How often do RE studies address the whole pipeline? sent15
    2.1. How is entity extraction sometimes divided? sent16
    2.2. What does the analysis of ACL papers reveal about RE sub-tasks? sent17
        2.2.1. What is shown in Table 2? sent18
        2.2.2. What types of papers are excluded from the analysis? sent19
        2.2.3. What assumption is usually made about entities in RE? sent20
        2.2.4. What is lacking in end-to-end models according to recent work? sent21
        2.2.5. What difficulty is highlighted in determining the RI setup? sent22
        2.2.6. How is RC different from RI in terms of performance? sent23
        2.2.7. What is the issue with RI in some papers? sent24
        2.2.8. What proposal is made to improve clarity in RE studies? sent25
            2.2.8.1. Why are these details important? sent26

    2.2. What does the analysis of ACL papers reveal about RE sub-tasks? sent17
        2.2.1. What is shown in Table 2? sent18
        2.2.2. What types of papers are excluded from the analysis? sent19
        2.2.3. What assumption is usually made about entities in RE? sent20
        2.2.4. What is lacking in end-to-end models according to recent work? sent21
        2.2.5. What difficulty is highlighted in determining the RI setup? sent22
        2.2.6. How is RC different from RI in terms of performance? sent23
        2.2.7. What is the issue with RI in some papers? sent24
        2.2.8. What proposal is made to improve clarity in RE studies? sent25
            2.2.8.1. Why are these details important? sent26

3. What is the traditional RE pipeline prone to? sent27
    3.1. What approaches have been proposed to alleviate error propagation? sent28
    3.2. What challenge is discussed by Taillé et al. (2020) regarding joint models? sent29
        3.2.1. What did they survey in recent works on end-to-end RE? sent30
        3.2.2. What observations did they make about end-to-end models? sent31
    3.3. What is still an open question regarding RE approaches? sent32
        3.3.1. What did Zhong and Chen (2021) find about using separate encoders? sent33
            3.3.1.1. Why might separate encoders be beneficial? sent34
        3.3.2. What is unclear about the suitability of RE approaches? sent35

    3.3. What is still an open question regarding RE approaches? sent32
        3.3.1. What did Zhong and Chen (2021) find about using separate encoders? sent33
            3.3.1.1. Why might separate encoders be beneficial? sent34
        3.3.2. What is unclear about the suitability of RE approaches? sent35","    3.3. What is still an open question regarding RE approaches? sent32
        3.3.1. What did Zhong and Chen (2021) find about using separate encoders? sent33
            3.3.1.1. Why might separate encoders be beneficial? sent34
        3.3.2. What is unclear about the suitability of RE approaches? sent35","Question: What are the current uncertainties regarding the suitability of joint versus pipeline approaches in relation extraction?

Supporting sentence: sent32, sent33, sent34, sent35",What are the current uncertainties regarding the suitability of joint versus pipeline approaches in relation extraction?,"sent32, sent33, sent34, sent35",4,implement,"The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.

Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016; Zhang et al., 2017a; Bekoulis et al., 2018a,b; Wang and Lu, 2020; Wang et al., 2021).

However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models.

They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting proposed by Bekoulis et al. (2018a).

They observe unfair comparisons and overestimations of end-to-end models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.

While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.

Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.

Since the output label space is different, separate encoders could better capture distinct contextual information.

At the moment it is not clear if one approach is more suitable than the other for RE.","Question:

What are the current uncertainties regarding the suitability of joint versus pipeline approaches in relation extraction?

Answer:

The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.

Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016; Zhang et al., 2017a; Bekoulis et al., 2018a,b; Wang and Lu, 2020; Wang et al., 2021).

However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models.

They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting proposed by Bekoulis et al. (2018a).

They observe unfair comparisons and overestimations of end-to-end models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.

While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.

Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.

Since the output label space is different, separate encoders could better capture distinct contextual information.

At the moment it is not clear if one approach is more suitable than the other for RE."
254854317,The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges,https://www.semanticscholar.org/paper/44ebfdb670007b3949507be0d1a1fca93bc3d5d5,Utilizing Neural Networks,7,"Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time. In contrast, the statistical and rule-based approaches are diminishing. Compared to ISCA, we see more adaptation of the pre-training model. This is because ACL work is more text-based focused, where pre-trained LMs are more widely available.

Neural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage. RNN and LSTM architectures are commonly used in sequence modeling, such as language modeling (Adel et al., 2013;Vu and Schultz, 2014;Adel et al., 2014c;Winata et al., 2018a;Garg et al., 2018a;Winata et al., 2019c) and CSW identification (Samih et al., 2016a). DNN-based and hybrid HMM-DNN models are used in speech recognition models .

Pre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer. Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017). A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018). A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b). Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021). Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b). In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.

Language Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a). These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages. Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets. Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text. Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context. While XLM-R provides the best result, it is also computationally heavy. There needed to be more exploration of larger models. We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach. Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available. However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations. CSW style can vary in different regions of the world, and it would be interesting to gather more datasets on unexplored and unknown styles, which can be useful for further research and investigation on linguistics and NLP. Therefore, one future direction is to broaden the language scope of CSW research.","sent1: Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time.
sent2: In contrast, the statistical and rule-based approaches are diminishing.
sent3: Compared to ISCA, we see more adaptation of the pre-training model.
sent4: This is because ACL work is more text-based focused, where pre-trained LMs are more widely available.
sent5: Neural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage.
sent6: RNN and LSTM architectures are commonly used in sequence modeling, such as language modeling (Adel et al., 2013;Vu and Schultz, 2014;Adel et al., 2014c;Winata et al., 2018a;Garg et al., 2018a;Winata et al., 2019c) and CSW identification (Samih et al., 2016a).
sent7: DNN-based and hybrid HMM-DNN models are used in speech recognition models .
sent8: Pre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer.
sent9: Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017).
sent10: A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018).
sent11: A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b).
sent12: Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021).
sent13: Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b).
sent14: In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.
sent15: Language Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a).
sent16: These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages.
sent17: Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets.
sent18: Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text.
sent19: Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context.
sent20: While XLM-R provides the best result, it is also computationally heavy.
sent21: There needed to be more exploration of larger models.
sent22: We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach.
sent23: Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available.
sent24: However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations.
sent25: CSW style can vary in different regions of the world, and it would be interesting to gather more datasets on unexplored and unknown styles, which can be useful for further research and investigation on linguistics and NLP.
sent26: Therefore, one future direction is to broaden the language scope of CSW research.","1. What trend is observed in the adoption of neural methods and pre-trained models in NLP? sent1
    1.1. How do statistical and rule-based approaches compare to neural methods in terms of popularity? sent2
    1.2. How does the adaptation of pre-training models differ between ISCA and ACL work? sent3, sent4
2. When did the trend of using neural-based models start, and how has the usage of rule/linguistic constraint and statistical methods changed over time? sent5
3. What architectures are commonly used in sequence modeling and CSW identification? sent6
4. What models are used in speech recognition? sent7
5. How are pre-trained embeddings used in neural-based approaches? sent8
    5.1. What are some common pre-trained embeddings used in the literature? sent9
    5.2. What is a standard method to utilize monolingual embeddings? sent10
    5.3. What recent approach is used to merge embeddings and form metaembeddings? sent11
    5.4. How have character-based embeddings been explored in the literature? sent12
    5.5. What approach is used to train bilingual embeddings? sent13
6. How is wav2vec 2.0 utilized in the speech domain? sent14
7. What multilingual LMs are commonly used to deal with CSW data? sent15
    7.1. How are these models often fine-tuned? sent16
    7.2. Why do some approaches use synthetic CSW data for fine-tuning? sent17
    7.3. What does Aguilar et al. (2021) propose for modeling noisy CSW text? sent18
    7.4. How does the performance of multilingual LM compare to language-specific LM for CSW context? sent19
        7.4.1. What is noted about XLM-R's performance and computational requirements? sent20
8. What is noted about the exploration of larger models? sent21
9. What benefits do pre-trained LMs provide on current benchmark tasks? sent22
    9.1. What theoretical possibility do pre-trained LMs offer for CSW tasks? sent23
    9.2. What is a downside of using pre-trained LMs, and what is encouraged to address this? sent24
10. How can CSW style vary, and what is suggested for future research? sent25
    10.1. What future direction is proposed for CSW research? sent26","1. What trend is observed in the adoption of neural methods and pre-trained models in NLP? sent1
    1.1. How do statistical and rule-based approaches compare to neural methods in terms of popularity? sent2
    1.2. How does the adaptation of pre-training models differ between ISCA and ACL work? sent3, sent4

5. How are pre-trained embeddings used in neural-based approaches? sent8
    5.1. What are some common pre-trained embeddings used in the literature? sent9
    5.2. What is a standard method to utilize monolingual embeddings? sent10
    5.3. What recent approach is used to merge embeddings and form metaembeddings? sent11
    5.4. How have character-based embeddings been explored in the literature? sent12
    5.5. What approach is used to train bilingual embeddings? sent13

7. What multilingual LMs are commonly used to deal with CSW data? sent15
    7.1. How are these models often fine-tuned? sent16
    7.2. Why do some approaches use synthetic CSW data for fine-tuning? sent17
    7.3. What does Aguilar et al. (2021) propose for modeling noisy CSW text? sent18
    7.4. How does the performance of multilingual LM compare to language-specific LM for CSW context? sent19
        7.4.1. What is noted about XLM-R's performance and computational requirements? sent20","5. How are pre-trained embeddings used in neural-based approaches? sent8
    5.1. What are some common pre-trained embeddings used in the literature? sent9
    5.2. What is a standard method to utilize monolingual embeddings? sent10
    5.3. What recent approach is used to merge embeddings and form metaembeddings? sent11
    5.4. How have character-based embeddings been explored in the literature? sent12
    5.5. What approach is used to train bilingual embeddings? sent13","Question: How have character-based embeddings been utilized to address out-of-vocabulary issues in code-switching research?

Supporting sentence: sent12",How have character-based embeddings been utilized to address out-of-vocabulary issues in code-switching research?,sent12,2,implement,"Character-based embeddings have been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b; Attia et al., 2018; Aguilar et al., 2021).","Question:

How have character-based embeddings been utilized to address out-of-vocabulary issues in code-switching research?

Answer:

Character-based embeddings have been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b; Attia et al., 2018; Aguilar et al., 2021)."
254854317,The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges,https://www.semanticscholar.org/paper/44ebfdb670007b3949507be0d1a1fca93bc3d5d5,Utilizing Neural Networks,7,"Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time. In contrast, the statistical and rule-based approaches are diminishing. Compared to ISCA, we see more adaptation of the pre-training model. This is because ACL work is more text-based focused, where pre-trained LMs are more widely available.

Neural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage. RNN and LSTM architectures are commonly used in sequence modeling, such as language modeling (Adel et al., 2013;Vu and Schultz, 2014;Adel et al., 2014c;Winata et al., 2018a;Garg et al., 2018a;Winata et al., 2019c) and CSW identification (Samih et al., 2016a). DNN-based and hybrid HMM-DNN models are used in speech recognition models .

Pre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer. Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017). A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018). A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b). Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021). Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b). In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.

Language Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a). These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages. Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets. Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text. Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context. While XLM-R provides the best result, it is also computationally heavy. There needed to be more exploration of larger models. We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach. Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available. However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations. CSW style can vary in different regions of the world, and it would be interesting to gather more datasets on unexplored and unknown styles, which can be useful for further research and investigation on linguistics and NLP. Therefore, one future direction is to broaden the language scope of CSW research.","sent1: Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time.
sent2: In contrast, the statistical and rule-based approaches are diminishing.
sent3: Compared to ISCA, we see more adaptation of the pre-training model.
sent4: This is because ACL work is more text-based focused, where pre-trained LMs are more widely available.
sent5: Neural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage.
sent6: RNN and LSTM architectures are commonly used in sequence modeling, such as language modeling (Adel et al., 2013;Vu and Schultz, 2014;Adel et al., 2014c;Winata et al., 2018a;Garg et al., 2018a;Winata et al., 2019c) and CSW identification (Samih et al., 2016a).
sent7: DNN-based and hybrid HMM-DNN models are used in speech recognition models .
sent8: Pre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer.
sent9: Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017).
sent10: A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018).
sent11: A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b).
sent12: Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021).
sent13: Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b).
sent14: In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.
sent15: Language Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a).
sent16: These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages.
sent17: Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets.
sent18: Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text.
sent19: Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context.
sent20: While XLM-R provides the best result, it is also computationally heavy.
sent21: There needed to be more exploration of larger models.
sent22: We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach.
sent23: Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available.
sent24: However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations.
sent25: CSW style can vary in different regions of the world, and it would be interesting to gather more datasets on unexplored and unknown styles, which can be useful for further research and investigation on linguistics and NLP.
sent26: Therefore, one future direction is to broaden the language scope of CSW research.","1. What trend is observed in the adoption of neural methods and pre-trained models in NLP? sent1
    1.1. How do statistical and rule-based approaches compare to neural methods in terms of popularity? sent2
    1.2. How does the adaptation of pre-training models differ between ISCA and ACL work? sent3, sent4
2. When did the trend of using neural-based models start, and how has the usage of rule/linguistic constraint and statistical methods changed over time? sent5
3. What architectures are commonly used in sequence modeling and CSW identification? sent6
4. What models are used in speech recognition? sent7
5. How are pre-trained embeddings used in neural-based approaches? sent8
    5.1. What are some common pre-trained embeddings used in the literature? sent9
    5.2. What is a standard method to utilize monolingual embeddings? sent10
    5.3. What recent approach is used to merge embeddings and form metaembeddings? sent11
    5.4. How have character-based embeddings been explored in the literature? sent12
    5.5. What approach is used to train bilingual embeddings? sent13
6. How is wav2vec 2.0 utilized in the speech domain? sent14
7. What multilingual LMs are commonly used to deal with CSW data? sent15
    7.1. How are these models often fine-tuned? sent16
    7.2. Why do some approaches use synthetic CSW data for fine-tuning? sent17
    7.3. What does Aguilar et al. (2021) propose for modeling noisy CSW text? sent18
    7.4. How does the performance of multilingual LM compare to language-specific LM for CSW context? sent19
        7.4.1. What is noted about XLM-R's performance and computational requirements? sent20
8. What is noted about the exploration of larger models? sent21
9. What benefits do pre-trained LMs provide on current benchmark tasks? sent22
    9.1. What theoretical possibility do pre-trained LMs offer for CSW tasks? sent23
    9.2. What is a downside of using pre-trained LMs, and what is encouraged to address this? sent24
10. How can CSW style vary, and what is suggested for future research? sent25
    10.1. What future direction is proposed for CSW research? sent26","1. What trend is observed in the adoption of neural methods and pre-trained models in NLP? sent1
    1.1. How do statistical and rule-based approaches compare to neural methods in terms of popularity? sent2
    1.2. How does the adaptation of pre-training models differ between ISCA and ACL work? sent3, sent4

5. How are pre-trained embeddings used in neural-based approaches? sent8
    5.1. What are some common pre-trained embeddings used in the literature? sent9
    5.2. What is a standard method to utilize monolingual embeddings? sent10
    5.3. What recent approach is used to merge embeddings and form metaembeddings? sent11
    5.4. How have character-based embeddings been explored in the literature? sent12
    5.5. What approach is used to train bilingual embeddings? sent13

7. What multilingual LMs are commonly used to deal with CSW data? sent15
    7.1. How are these models often fine-tuned? sent16
    7.2. Why do some approaches use synthetic CSW data for fine-tuning? sent17
    7.3. What does Aguilar et al. (2021) propose for modeling noisy CSW text? sent18
    7.4. How does the performance of multilingual LM compare to language-specific LM for CSW context? sent19
        7.4.1. What is noted about XLM-R's performance and computational requirements? sent20","7. What multilingual LMs are commonly used to deal with CSW data? sent15
    7.1. How are these models often fine-tuned? sent16
    7.2. Why do some approaches use synthetic CSW data for fine-tuning? sent17
    7.3. What does Aguilar et al. (2021) propose for modeling noisy CSW text? sent18
    7.4. How does the performance of multilingual LM compare to language-specific LM for CSW context? sent19
        7.4.1. What is noted about XLM-R's performance and computational requirements? sent20","Question: How do multilingual language models perform compared to language-specific models in code-switching contexts, and what are their computational requirements?

Supporting sentence: sent19, sent20","How do multilingual language models perform compared to language-specific models in code-switching contexts, and what are their computational requirements?","sent19, sent20",4,implement,"Multilingual language models, such as mBERT or XLM-R, are often fine-tuned with the downstream task or with code-switching (CSW) text to better adapt to the languages.

Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets.

Winata et al. (2021a) compare the performance of the multilingual language model versus the language-specific language model for CSW context.

While XLM-R provides the best result, it is also computationally heavy.

Pre-trained language models provide better empirical results on current benchmark tasks and enable an end-to-end approach.

However, the downside is that there is little understanding of how and when the language models would fail, thus more interpretability work on these language models in CSW settings is encouraged.","Question:

How do multilingual language models perform compared to language-specific models in code-switching contexts, and what are their computational requirements?

Answer:

Multilingual language models, such as mBERT or XLM-R, are often fine-tuned with the downstream task or with code-switching (CSW) text to better adapt to the languages.

Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets.

Winata et al. (2021a) compare the performance of the multilingual language model versus the language-specific language model for CSW context.

While XLM-R provides the best result, it is also computationally heavy.

Pre-trained language models provide better empirical results on current benchmark tasks and enable an end-to-end approach.

However, the downside is that there is little understanding of how and when the language models would fail, thus more interpretability work on these language models in CSW settings is encouraged."
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,Self-attention heads,7,"Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","sent1: Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);• attending to previous/next tokens,    (Kovaleva et al., 2019)
sent2: According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"".
sent3: However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 .
sent4: Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019).
sent5: This apparent redundancy must be related to the overparametrization issue (see section 7).
sent6: Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation.
sent7: Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them.
sent8: They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.
sent9: [SEP] gets increased attention starting in layer 5, but its importance for prediction drops.
sent10: If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.
sent11: Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.
sent12: Some BERT heads seem to specialize in certain types of syntactic relations.
sent13: Htut et al.(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions.
sent14: The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline.
sent15: The evidence for nsubj, advmod, and amod has some variation between these two studies.
sent16: The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context.
sent17: Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).
sent18: present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora.
sent19: Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.
sent20: Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,.
sent21: Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks.
sent22: Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998).
sent23: Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","1. Why is attention considered useful for understanding Transformer models? sent1
    1.1. What did Clark et al. (2019) state about attention weight? sent2
    1.2. What did Kovaleva et al. (2019) find about most self-attention heads? sent3
    1.3. What pattern did much of the model encode according to Clark et al. (2019)? sent4
    1.4. What issue is related to the apparent redundancy in attention patterns? sent5
    1.5. How is attention to [CLS] interpreted, and what other tokens does BERT attend to? sent6
    1.6. What hypothesis did Clark et al. (2019) propose about periods and commas? sent7
    1.7. What is suggested about the function of [SEP]? sent8
    1.8. How does attention to [SEP] change across layers, and what implication does this have for attention probing studies? sent9, sent10
2. What is the focus of studies analyzing the ""heterogeneous"" self-attention pattern? sent11
    2.1. What do some BERT heads specialize in according to studies? sent12
    2.2. What did Htut et al. (2019) and Clark et al. (2019) report about BERT heads and syntactic positions? sent13
    2.3. How do the datasets and methods differ in studies, and what do they find about obj role? sent14
    2.4. What variation exists in evidence for nsubj, advmod, and amod? sent15
    2.5. What overall conclusion is supported by Voita et al. (2019)? sent16
    2.6. What hypothesis did Hoover et al. (2019) propose about complex dependencies? sent17
3. What evidence is presented about attention weights and linguistic indicators? sent18
    3.1. How do BERT's self-attention weights compare to a uniform attention baseline? sent19
4. What did Clark et al. (2019) identify about a BERT head's capability in coreference resolution? sent20
5. What did Kovaleva et al. (2019) show about attention heads specializing in semantic relations? sent21
    5.1. What did Kovaleva et al. (2019) identify about two heads of base BERT? sent22
    5.2. What did a head ablation study reveal about these heads' importance for BERT's success on GLUE tasks? sent23","1. Why is attention considered useful for understanding Transformer models? sent1
    1.1. What did Clark et al. (2019) state about attention weight? sent2
    1.2. What did Kovaleva et al. (2019) find about most self-attention heads? sent3
    1.3. What pattern did much of the model encode according to Clark et al. (2019)? sent4
    1.4. What issue is related to the apparent redundancy in attention patterns? sent5
    1.5. How is attention to [CLS] interpreted, and what other tokens does BERT attend to? sent6
    1.6. What hypothesis did Clark et al. (2019) propose about periods and commas? sent7
    1.7. What is suggested about the function of [SEP]? sent8
    1.8. How does attention to [SEP] change across layers, and what implication does this have for attention probing studies? sent9, sent10

2. What is the focus of studies analyzing the ""heterogeneous"" self-attention pattern? sent11
    2.1. What do some BERT heads specialize in according to studies? sent12
    2.2. What did Htut et al. (2019) and Clark et al. (2019) report about BERT heads and syntactic positions? sent13
    2.3. How do the datasets and methods differ in studies, and what do they find about obj role? sent14
    2.4. What variation exists in evidence for nsubj, advmod, and amod? sent15
    2.5. What overall conclusion is supported by Voita et al. (2019)? sent16
    2.6. What hypothesis did Hoover et al. (2019) propose about complex dependencies? sent17","1. Why is attention considered useful for understanding Transformer models? sent1
    1.1. What did Clark et al. (2019) state about attention weight? sent2
    1.2. What did Kovaleva et al. (2019) find about most self-attention heads? sent3
    1.3. What pattern did much of the model encode according to Clark et al. (2019)? sent4
    1.4. What issue is related to the apparent redundancy in attention patterns? sent5
    1.5. How is attention to [CLS] interpreted, and what other tokens does BERT attend to? sent6
    1.6. What hypothesis did Clark et al. (2019) propose about periods and commas? sent7
    1.7. What is suggested about the function of [SEP]? sent8
    1.8. How does attention to [SEP] change across layers, and what implication does this have for attention probing studies? sent9, sent10","Question: How does BERT's attention to [SEP] and [CLS] tokens evolve across layers, and what are the implications for attention probing studies?

Supporting sentence: sent9, sent10","How does BERT's attention to [SEP] and [CLS] tokens evolve across layers, and what are the implications for attention probing studies?","sent9, sent10",2,implement,"Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation.

Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them.

They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops.

If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g. and Htut et al. (2019)) should perhaps be revisited.","Question:

How does BERT's attention to [SEP] and [CLS] tokens evolve across layers, and what are the implications for attention probing studies?

Answer:

Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation.

Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them.

They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops.

If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g. and Htut et al. (2019)) should perhaps be revisited."
234093015,A Survey of Data Augmentation Approaches for NLP,https://www.semanticscholar.org/paper/63d8426ba1f51a8525dd19fd8ec92934ec71aea5,Example Interpolation Techniques,5,"Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA). Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).

Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020). For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).

A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs. This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c). Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).","sent1: Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples.
sent2: This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA).
sent3: Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).
sent4: Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements.
sent5: Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism.
sent6: Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020).
sent7: For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes.
sent8: There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).
sent9: A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs.
sent10: This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c).
sent11: Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others.
sent12: SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α).
sent13: The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).","1. What is another class of data augmentation techniques pioneered by MIXUP? sent1
    1.1. What is this class of techniques sometimes referred to as? sent2
    1.2. What have ensuing works explored in relation to MIXUP? sent3
    1.3. What is another class of MIXUP extensions growing in the vision community? sent4
        1.3.1. Can you provide examples of this paradigm? sent6
            1.3.1.1. How does CUTMIX work as an example of this paradigm? sent7
    1.4. What potential does this paradigm have for NLP? sent8
2. What was a bottleneck to using MIXUP for NLP tasks, and how was it overcome? sent9, sent10
3. What later variants of MIXUP have been proposed? sent11
4. How does SEQ2MIXUP generalize MIXUP for sequence transduction tasks? sent12
    4.1. Which version of SEQ2MIXUP is found to outperform the other? sent13","1. What is another class of data augmentation techniques pioneered by MIXUP? sent1
    1.1. What is this class of techniques sometimes referred to as? sent2
    1.2. What have ensuing works explored in relation to MIXUP? sent3
    1.3. What is another class of MIXUP extensions growing in the vision community? sent4
        1.3.1. Can you provide examples of this paradigm? sent6
            1.3.1.1. How does CUTMIX work as an example of this paradigm? sent7
    1.4. What potential does this paradigm have for NLP? sent8","1. What is another class of data augmentation techniques pioneered by MIXUP? sent1
    1.1. What is this class of techniques sometimes referred to as? sent2
    1.2. What have ensuing works explored in relation to MIXUP? sent3
    1.3. What is another class of MIXUP extensions growing in the vision community? sent4
        1.3.1. Can you provide examples of this paradigm? sent6
            1.3.1.1. How does CUTMIX work as an example of this paradigm? sent7
    1.4. What potential does this paradigm have for NLP? sent8","Question: How have MIXUP techniques been adapted for NLP tasks, and what challenges have been addressed?

Supporting sentence: sent9, sent10, sent11, sent12, sent13","How have MIXUP techniques been adapted for NLP tasks, and what challenges have been addressed?","sent9, sent10, sent11, sent12, sent13",3,implement,"Another class of data augmentation techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples.

This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA).

A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs.

This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c).

SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways - the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α).

The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).","Question:

How have MIXUP techniques been adapted for NLP tasks, and what challenges have been addressed?

Answer:

Another class of data augmentation techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples.

This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA).

A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs.

This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c).

SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways - the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α).

The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a)."
251719280,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,https://www.semanticscholar.org/paper/a814b76e589ef27e3f4af379d319d02d2110faa1,Single-Domain Datasets,14,"Single-domain text-to-SQL datasets typically collect question-SQL pairs for a single database in some real-world tasks, including early ones such as Academic (Li and Jagadish, 2014), Advising (Finegan-Dollak et al., 2018), ATIS (Price, 1990;Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), Yelp and IMDB (Yaghmazadeh et al., 2017), Scholar (Iyer et al., 2017) and Restaurants (Tang and Mooney, 2000;Popescu et al., 2003), as well as recent ones such as SEDE (Hazoom et al., 2021), ESQL (Chen et al., 2021a) and MIMICSQL (Wang et al., 2020d). These single-domain datasets, particularly the early ones, are usually limited in size, containing only a few hundred to a few thousand examples. Because of the limited size and similar SQL patterns in training and testing phases, text-to-SQL models that are trained on these single-domain datasets can achieve decent performance by simply memorizing the SQL patterns and fail to generalize to unseen SQL queries or SQL queries from other unless otherwise specified.

domains (Finegan-Dollak et al., 2018;Yu et al., 2018c). However, since these datasets are adapted from real-life applications, most of them contain domain knowledge (Gan et al., 2021b) and dataset conventions (Suhr et al., 2020). Thus, they are still valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions.

Appendix B gives a detailed discussion on domain knowledge and dataset convention, and concrete text-to-SQL examples.

Large Scale Cross-domain Datasets Large cross-domain datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) are proposed to better evaluate deep neural models. Wik-iSQL uses tables extracted from Wikipedia and lets annotators paraphrase questions generated for the tables. Compared to other datasets, WikiSQL is an order of magnitude larger, containing 80,654 natural utterances in total (Zhong et al., 2017). However, WikiSQL contains only simple SQL queries, and only a single table is queried within each SQL query (Yu et al., 2018c). Yu et al. (2018c) propose Spider, which contains 200 databases with an average of 5 tables for each database, to test models' performance on complicated unseen SQL queries and their ability to generalize to new domains. Furthermore, researchers expand Spider to study various issues of their inter-est (Lei et al., 2020;Zeng et al., 2020;Gan et al., 2021b;Taniguchi et al., 2021;Gan et al., 2021a).

Besides, researchers build several large-scale text-to-SQL datasets in different languages such as CSpider (Min et al., 2019a), TableQA (Sun et al., 2020), DuSQL (Wang et al., 2020c) in Chinese, ViText2SQL (Tuan Nguyen et al., 2020) in Vietnamese, and PortugueseSpider (José and Cozman, 2021) in Portuguese. Given that human translation has shown to be more accurate than machine translation (Min et al., 2019a), these datasets are annotated mainly by human experts based on the English Spider dataset. These Spider-based datasets can serve as potential resources for multi-lingual text-to-SQL research.

Other Datasets Several context-dependent textto-SQL datasets have been proposed, which involve user interactions with the text-to-SQL system in English (Price, 1990;Dahl et al., 1994;Yu et al., 2019a,b) and Chinese (Guo et al., 2021). In addition, researchers collect datasets to study questions in text-to-SQL being answerable or not (Zhang et al., 2020), lexicon-level mapping (Shi et al., 2020b) and cross-domain evaluation for real Web databases (Lee et al., 2021).

Appendix C.1 discusses more details about datasets mentioned in § 2.","sent1: Single-domain text-to-SQL datasets typically collect question-SQL pairs for a single database in some real-world tasks, including early ones such as Academic (Li and Jagadish, 2014), Advising (Finegan-Dollak et al., 2018), ATIS (Price, 1990;Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), Yelp and IMDB (Yaghmazadeh et al., 2017), Scholar (Iyer et al., 2017) and Restaurants (Tang and Mooney, 2000;Popescu et al., 2003), as well as recent ones such as SEDE (Hazoom et al., 2021), ESQL (Chen et al., 2021a) and MIMICSQL (Wang et al., 2020d).
sent2: These single-domain datasets, particularly the early ones, are usually limited in size, containing only a few hundred to a few thousand examples.
sent3: Because of the limited size and similar SQL patterns in training and testing phases, text-to-SQL models that are trained on these single-domain datasets can achieve decent performance by simply memorizing the SQL patterns and fail to generalize to unseen SQL queries or SQL queries from other unless otherwise specified.domains (Finegan-Dollak et al., 2018;Yu et al., 2018c).
sent4: However, since these datasets are adapted from real-life applications, most of them contain domain knowledge (Gan et al., 2021b) and dataset conventions (Suhr et al., 2020).
sent5: Thus, they are still valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions.
sent6: Appendix B gives a detailed discussion on domain knowledge and dataset convention, and concrete text-to-SQL examples.
sent7: Large Scale Cross-domain Datasets Large cross-domain datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) are proposed to better evaluate deep neural models.
sent8: Wik-iSQL uses tables extracted from Wikipedia and lets annotators paraphrase questions generated for the tables.
sent9: Compared to other datasets, WikiSQL is an order of magnitude larger, containing 80,654 natural utterances in total (Zhong et al., 2017).
sent10: However, WikiSQL contains only simple SQL queries, and only a single table is queried within each SQL query (Yu et al., 2018c).
sent11: Yu et al. (2018c) propose Spider, which contains 200 databases with an average of 5 tables for each database, to test models' performance on complicated unseen SQL queries and their ability to generalize to new domains.
sent12: Furthermore, researchers expand Spider to study various issues of their inter-est (Lei et al., 2020;Zeng et al., 2020;Gan et al., 2021b;Taniguchi et al., 2021;Gan et al., 2021a).
sent13: Besides, researchers build several large-scale text-to-SQL datasets in different languages such as CSpider (Min et al., 2019a), TableQA (Sun et al., 2020), DuSQL (Wang et al., 2020c) in Chinese, ViText2SQL (Tuan Nguyen et al., 2020) in Vietnamese, and PortugueseSpider (José and Cozman, 2021) in Portuguese.
sent14: Given that human translation has shown to be more accurate than machine translation (Min et al., 2019a), these datasets are annotated mainly by human experts based on the English Spider dataset.
sent15: These Spider-based datasets can serve as potential resources for multi-lingual text-to-SQL research.
sent16: Other Datasets Several context-dependent textto-SQL datasets have been proposed, which involve user interactions with the text-to-SQL system in English (Price, 1990;Dahl et al., 1994;Yu et al., 2019a,b) and Chinese (Guo et al., 2021).
sent17: In addition, researchers collect datasets to study questions in text-to-SQL being answerable or not (Zhang et al., 2020), lexicon-level mapping (Shi et al., 2020b) and cross-domain evaluation for real Web databases (Lee et al., 2021).
sent18: Appendix C.1 discusses more details about datasets mentioned in § 2.","1. What do single-domain text-to-SQL datasets typically collect? sent1
    1.1. What is a characteristic of early single-domain datasets? sent2
    1.2. Why might text-to-SQL models trained on single-domain datasets fail to generalize? sent3
    1.3. What valuable aspects do single-domain datasets contain despite their limitations? sent4
        1.3.1. Why are single-domain datasets still valuable for evaluating models? sent5
        1.3.2. Where can a detailed discussion on domain knowledge and dataset convention be found? sent6
2. What are large-scale cross-domain datasets proposed for? sent7
    2.1. How is WikiSQL constructed and what is its size? sent8, sent9
        2.1.1. What limitation does WikiSQL have? sent10
    2.2. What is the purpose of the Spider dataset? sent11
        2.2.1. How have researchers expanded the Spider dataset? sent12
    2.3. What are some examples of large-scale text-to-SQL datasets in different languages? sent13
        2.3.1. Why are these datasets mainly annotated by human experts? sent14
        2.3.2. What potential do Spider-based datasets have? sent15
3. What are some other types of text-to-SQL datasets mentioned? sent16
    3.1. What additional research topics have datasets been collected for? sent17
    3.2. Where can more details about the datasets be found? sent18","1. What do single-domain text-to-SQL datasets typically collect? sent1
    1.1. What is a characteristic of early single-domain datasets? sent2
    1.2. Why might text-to-SQL models trained on single-domain datasets fail to generalize? sent3
    1.3. What valuable aspects do single-domain datasets contain despite their limitations? sent4
        1.3.1. Why are single-domain datasets still valuable for evaluating models? sent5
        1.3.2. Where can a detailed discussion on domain knowledge and dataset convention be found? sent6

2. What are large-scale cross-domain datasets proposed for? sent7
    2.1. How is WikiSQL constructed and what is its size? sent8, sent9
        2.1.1. What limitation does WikiSQL have? sent10
    2.2. What is the purpose of the Spider dataset? sent11
        2.2.1. How have researchers expanded the Spider dataset? sent12
    2.3. What are some examples of large-scale text-to-SQL datasets in different languages? sent13
        2.3.1. Why are these datasets mainly annotated by human experts? sent14
        2.3.2. What potential do Spider-based datasets have? sent15","1. What do single-domain text-to-SQL datasets typically collect? sent1
    1.1. What is a characteristic of early single-domain datasets? sent2
    1.2. Why might text-to-SQL models trained on single-domain datasets fail to generalize? sent3
    1.3. What valuable aspects do single-domain datasets contain despite their limitations? sent4
        1.3.1. Why are single-domain datasets still valuable for evaluating models? sent5
        1.3.2. Where can a detailed discussion on domain knowledge and dataset convention be found? sent6","Question: Why are single-domain text-to-SQL datasets still valuable for evaluating model generalization and domain knowledge incorporation?

Supporting sentence: sent4, sent5",Why are single-domain text-to-SQL datasets still valuable for evaluating model generalization and domain knowledge incorporation?,"sent4, sent5",2,implement,"Single-domain text-to-SQL datasets are adapted from real-life applications, and most of them contain domain knowledge (Gan et al., 2021b) and dataset conventions (Suhr et al., 2020).

Thus, they are valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions.","Question:

Why are single-domain text-to-SQL datasets still valuable for evaluating model generalization and domain knowledge incorporation?

Answer:

Single-domain text-to-SQL datasets are adapted from real-life applications, and most of them contain domain knowledge (Gan et al., 2021b) and dataset conventions (Suhr et al., 2020).

Thus, they are valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions."
251719280,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,https://www.semanticscholar.org/paper/a814b76e589ef27e3f4af379d319d02d2110faa1,Single-Domain Datasets,14,"Single-domain text-to-SQL datasets typically collect question-SQL pairs for a single database in some real-world tasks, including early ones such as Academic (Li and Jagadish, 2014), Advising (Finegan-Dollak et al., 2018), ATIS (Price, 1990;Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), Yelp and IMDB (Yaghmazadeh et al., 2017), Scholar (Iyer et al., 2017) and Restaurants (Tang and Mooney, 2000;Popescu et al., 2003), as well as recent ones such as SEDE (Hazoom et al., 2021), ESQL (Chen et al., 2021a) and MIMICSQL (Wang et al., 2020d). These single-domain datasets, particularly the early ones, are usually limited in size, containing only a few hundred to a few thousand examples. Because of the limited size and similar SQL patterns in training and testing phases, text-to-SQL models that are trained on these single-domain datasets can achieve decent performance by simply memorizing the SQL patterns and fail to generalize to unseen SQL queries or SQL queries from other unless otherwise specified.

domains (Finegan-Dollak et al., 2018;Yu et al., 2018c). However, since these datasets are adapted from real-life applications, most of them contain domain knowledge (Gan et al., 2021b) and dataset conventions (Suhr et al., 2020). Thus, they are still valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions.

Appendix B gives a detailed discussion on domain knowledge and dataset convention, and concrete text-to-SQL examples.

Large Scale Cross-domain Datasets Large cross-domain datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) are proposed to better evaluate deep neural models. Wik-iSQL uses tables extracted from Wikipedia and lets annotators paraphrase questions generated for the tables. Compared to other datasets, WikiSQL is an order of magnitude larger, containing 80,654 natural utterances in total (Zhong et al., 2017). However, WikiSQL contains only simple SQL queries, and only a single table is queried within each SQL query (Yu et al., 2018c). Yu et al. (2018c) propose Spider, which contains 200 databases with an average of 5 tables for each database, to test models' performance on complicated unseen SQL queries and their ability to generalize to new domains. Furthermore, researchers expand Spider to study various issues of their inter-est (Lei et al., 2020;Zeng et al., 2020;Gan et al., 2021b;Taniguchi et al., 2021;Gan et al., 2021a).

Besides, researchers build several large-scale text-to-SQL datasets in different languages such as CSpider (Min et al., 2019a), TableQA (Sun et al., 2020), DuSQL (Wang et al., 2020c) in Chinese, ViText2SQL (Tuan Nguyen et al., 2020) in Vietnamese, and PortugueseSpider (José and Cozman, 2021) in Portuguese. Given that human translation has shown to be more accurate than machine translation (Min et al., 2019a), these datasets are annotated mainly by human experts based on the English Spider dataset. These Spider-based datasets can serve as potential resources for multi-lingual text-to-SQL research.

Other Datasets Several context-dependent textto-SQL datasets have been proposed, which involve user interactions with the text-to-SQL system in English (Price, 1990;Dahl et al., 1994;Yu et al., 2019a,b) and Chinese (Guo et al., 2021). In addition, researchers collect datasets to study questions in text-to-SQL being answerable or not (Zhang et al., 2020), lexicon-level mapping (Shi et al., 2020b) and cross-domain evaluation for real Web databases (Lee et al., 2021).

Appendix C.1 discusses more details about datasets mentioned in § 2.","sent1: Single-domain text-to-SQL datasets typically collect question-SQL pairs for a single database in some real-world tasks, including early ones such as Academic (Li and Jagadish, 2014), Advising (Finegan-Dollak et al., 2018), ATIS (Price, 1990;Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), Yelp and IMDB (Yaghmazadeh et al., 2017), Scholar (Iyer et al., 2017) and Restaurants (Tang and Mooney, 2000;Popescu et al., 2003), as well as recent ones such as SEDE (Hazoom et al., 2021), ESQL (Chen et al., 2021a) and MIMICSQL (Wang et al., 2020d).
sent2: These single-domain datasets, particularly the early ones, are usually limited in size, containing only a few hundred to a few thousand examples.
sent3: Because of the limited size and similar SQL patterns in training and testing phases, text-to-SQL models that are trained on these single-domain datasets can achieve decent performance by simply memorizing the SQL patterns and fail to generalize to unseen SQL queries or SQL queries from other unless otherwise specified.domains (Finegan-Dollak et al., 2018;Yu et al., 2018c).
sent4: However, since these datasets are adapted from real-life applications, most of them contain domain knowledge (Gan et al., 2021b) and dataset conventions (Suhr et al., 2020).
sent5: Thus, they are still valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions.
sent6: Appendix B gives a detailed discussion on domain knowledge and dataset convention, and concrete text-to-SQL examples.
sent7: Large Scale Cross-domain Datasets Large cross-domain datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) are proposed to better evaluate deep neural models.
sent8: Wik-iSQL uses tables extracted from Wikipedia and lets annotators paraphrase questions generated for the tables.
sent9: Compared to other datasets, WikiSQL is an order of magnitude larger, containing 80,654 natural utterances in total (Zhong et al., 2017).
sent10: However, WikiSQL contains only simple SQL queries, and only a single table is queried within each SQL query (Yu et al., 2018c).
sent11: Yu et al. (2018c) propose Spider, which contains 200 databases with an average of 5 tables for each database, to test models' performance on complicated unseen SQL queries and their ability to generalize to new domains.
sent12: Furthermore, researchers expand Spider to study various issues of their inter-est (Lei et al., 2020;Zeng et al., 2020;Gan et al., 2021b;Taniguchi et al., 2021;Gan et al., 2021a).
sent13: Besides, researchers build several large-scale text-to-SQL datasets in different languages such as CSpider (Min et al., 2019a), TableQA (Sun et al., 2020), DuSQL (Wang et al., 2020c) in Chinese, ViText2SQL (Tuan Nguyen et al., 2020) in Vietnamese, and PortugueseSpider (José and Cozman, 2021) in Portuguese.
sent14: Given that human translation has shown to be more accurate than machine translation (Min et al., 2019a), these datasets are annotated mainly by human experts based on the English Spider dataset.
sent15: These Spider-based datasets can serve as potential resources for multi-lingual text-to-SQL research.
sent16: Other Datasets Several context-dependent textto-SQL datasets have been proposed, which involve user interactions with the text-to-SQL system in English (Price, 1990;Dahl et al., 1994;Yu et al., 2019a,b) and Chinese (Guo et al., 2021).
sent17: In addition, researchers collect datasets to study questions in text-to-SQL being answerable or not (Zhang et al., 2020), lexicon-level mapping (Shi et al., 2020b) and cross-domain evaluation for real Web databases (Lee et al., 2021).
sent18: Appendix C.1 discusses more details about datasets mentioned in § 2.","1. What do single-domain text-to-SQL datasets typically collect? sent1
    1.1. What is a characteristic of early single-domain datasets? sent2
    1.2. Why might text-to-SQL models trained on single-domain datasets fail to generalize? sent3
    1.3. What valuable aspects do single-domain datasets contain despite their limitations? sent4
        1.3.1. Why are single-domain datasets still valuable for evaluating models? sent5
        1.3.2. Where can a detailed discussion on domain knowledge and dataset convention be found? sent6
2. What are large-scale cross-domain datasets proposed for? sent7
    2.1. How is WikiSQL constructed and what is its size? sent8, sent9
        2.1.1. What limitation does WikiSQL have? sent10
    2.2. What is the purpose of the Spider dataset? sent11
        2.2.1. How have researchers expanded the Spider dataset? sent12
    2.3. What are some examples of large-scale text-to-SQL datasets in different languages? sent13
        2.3.1. Why are these datasets mainly annotated by human experts? sent14
        2.3.2. What potential do Spider-based datasets have? sent15
3. What are some other types of text-to-SQL datasets mentioned? sent16
    3.1. What additional research topics have datasets been collected for? sent17
    3.2. Where can more details about the datasets be found? sent18","1. What do single-domain text-to-SQL datasets typically collect? sent1
    1.1. What is a characteristic of early single-domain datasets? sent2
    1.2. Why might text-to-SQL models trained on single-domain datasets fail to generalize? sent3
    1.3. What valuable aspects do single-domain datasets contain despite their limitations? sent4
        1.3.1. Why are single-domain datasets still valuable for evaluating models? sent5
        1.3.2. Where can a detailed discussion on domain knowledge and dataset convention be found? sent6

2. What are large-scale cross-domain datasets proposed for? sent7
    2.1. How is WikiSQL constructed and what is its size? sent8, sent9
        2.1.1. What limitation does WikiSQL have? sent10
    2.2. What is the purpose of the Spider dataset? sent11
        2.2.1. How have researchers expanded the Spider dataset? sent12
    2.3. What are some examples of large-scale text-to-SQL datasets in different languages? sent13
        2.3.1. Why are these datasets mainly annotated by human experts? sent14
        2.3.2. What potential do Spider-based datasets have? sent15","2. What are large-scale cross-domain datasets proposed for? sent7
    2.1. How is WikiSQL constructed and what is its size? sent8, sent9
        2.1.1. What limitation does WikiSQL have? sent10
    2.2. What is the purpose of the Spider dataset? sent11
        2.2.1. How have researchers expanded the Spider dataset? sent12
    2.3. What are some examples of large-scale text-to-SQL datasets in different languages? sent13
        2.3.1. Why are these datasets mainly annotated by human experts? sent14
        2.3.2. What potential do Spider-based datasets have? sent15","Question: How have researchers expanded the Spider dataset to address various research issues?

Supporting sentence: sent11, sent12",How have researchers expanded the Spider dataset to address various research issues?,"sent11, sent12",4,implement,"Researchers have expanded the Spider dataset to study various issues of their interest.

This includes work by Lei et al. (2020), Zeng et al. (2020), Gan et al. (2021b), Taniguchi et al. (2021), and Gan et al. (2021a).","Question:

How have researchers expanded the Spider dataset to address various research issues?

Answer:

Researchers have expanded the Spider dataset to study various issues of their interest.

This includes work by Lei et al. (2020), Zeng et al. (2020), Gan et al. (2021b), Taniguchi et al. (2021), and Gan et al. (2021a)."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,Learning with Noisy Labels,12,"The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain more errors. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision. We categorize these into two ideas: noise filtering and noise modeling.

Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).

The noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances. Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.

In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly. Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).","sent1: The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations.
sent2: These labels tend, however, to contain more errors.
sent3: Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance.
sent4: Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision.
sent5: We categorize these into two ideas: noise filtering and noise modeling.
sent6: Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled.
sent7: This often includes training a classifier to make the filtering decision.
sent8: The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019).
sent9: Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).
sent10: The noise in the labels can also be modeled.
sent11: A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b).
sent12: The classifier is no longer trained directly on the noisily-labeled data.
sent13: Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution.
sent14: This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels.
sent15: In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing.
sent16: In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances.
sent17: Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.
sent18: In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method.
sent19: Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly.
sent20: Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).","1. What is the advantage of the above-presented methods for obtaining labeled data? sent1
    1.1. What is a downside of these labels? sent2
    1.2. How can training on noisily-labeled data affect performance? sent3
2. What approaches are used to handle noise in distant supervision? sent4
    2.1. How are these approaches categorized? sent5
        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9
        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16
3. How do Rehbein and Ruppenhofer (2017), Lison et al. (2020), and Ren et al. (2020) handle distant supervision? sent17
4. What is a common type of noise in distantly supervised labels for NER? sent18
    4.1. How does partial annotation learning address this issue? sent19
5. What are some related approaches to handling noise in labels? sent20","2. What approaches are used to handle noise in distant supervision? sent4
    2.1. How are these approaches categorized? sent5
        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9
        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16

    2.1. How are these approaches categorized? sent5
        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9
        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16

        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9

        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16","2. What approaches are used to handle noise in distant supervision? sent4
    2.1. How are these approaches categorized? sent5
        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9
        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16","Question: How do noise filtering and noise modeling methods address the challenges of training with noisy labels in NLP?

Supporting sentence: sent4, sent5, sent6, sent7, sent8, sent9, sent10, sent11, sent12, sent13",How do noise filtering and noise modeling methods address the challenges of training with noisy labels in NLP?,"sent4, sent5, sent6, sent7, sent8, sent9, sent10, sent11, sent12, sent13",2,implement,"The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations.

These labels tend, however, to contain more errors.

Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance.

Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision.

Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled.

This often includes training a classifier to make the filtering decision.

The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent (Nooralahzadeh et al., 2019).

Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).

The noise in the labels can also be modeled.

A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016; Luo et al., 2017; Hedderich and Klakow, 2018; Paul et al., 2019; Lange et al., 2019a,c; Chen et al., 2019; Wang et al., 2019; Hedderich et al., 2021b).

The classifier is no longer trained directly on the noisily-labeled data.

Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution.

This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels.

In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing.

In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances.","Question:

How do noise filtering and noise modeling methods address the challenges of training with noisy labels in NLP?

Answer:

The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations.

These labels tend, however, to contain more errors.

Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance.

Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision.

Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled.

This often includes training a classifier to make the filtering decision.

The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent (Nooralahzadeh et al., 2019).

Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).

The noise in the labels can also be modeled.

A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016; Luo et al., 2017; Hedderich and Klakow, 2018; Paul et al., 2019; Lange et al., 2019a,c; Chen et al., 2019; Wang et al., 2019; Hedderich et al., 2021b).

The classifier is no longer trained directly on the noisily-labeled data.

Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution.

This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels.

In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing.

In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,Learning with Noisy Labels,12,"The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain more errors. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision. We categorize these into two ideas: noise filtering and noise modeling.

Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).

The noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances. Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.

In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly. Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).","sent1: The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations.
sent2: These labels tend, however, to contain more errors.
sent3: Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance.
sent4: Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision.
sent5: We categorize these into two ideas: noise filtering and noise modeling.
sent6: Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled.
sent7: This often includes training a classifier to make the filtering decision.
sent8: The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019).
sent9: Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).
sent10: The noise in the labels can also be modeled.
sent11: A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b).
sent12: The classifier is no longer trained directly on the noisily-labeled data.
sent13: Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution.
sent14: This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels.
sent15: In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing.
sent16: In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances.
sent17: Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.
sent18: In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method.
sent19: Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly.
sent20: Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).","1. What is the advantage of the above-presented methods for obtaining labeled data? sent1
    1.1. What is a downside of these labels? sent2
    1.2. How can training on noisily-labeled data affect performance? sent3
2. What approaches are used to handle noise in distant supervision? sent4
    2.1. How are these approaches categorized? sent5
        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9
        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16
3. How do Rehbein and Ruppenhofer (2017), Lison et al. (2020), and Ren et al. (2020) handle distant supervision? sent17
4. What is a common type of noise in distantly supervised labels for NER? sent18
    4.1. How does partial annotation learning address this issue? sent19
5. What are some related approaches to handling noise in labels? sent20","2. What approaches are used to handle noise in distant supervision? sent4
    2.1. How are these approaches categorized? sent5
        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9
        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16

    2.1. How are these approaches categorized? sent5
        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9
        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16

        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9

        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16","    2.1. How are these approaches categorized? sent5
        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9
        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16","Question: How do noise filtering and noise modeling methods address the challenges of training with noisy labels?

Supporting sentence: sent5, sent6, sent7, sent8, sent9, sent10, sent11, sent12, sent13",How do noise filtering and noise modeling methods address the challenges of training with noisy labels?,"sent5, sent6, sent7, sent8, sent9, sent10, sent11, sent12, sent13",2,implement,"The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations.

These labels tend, however, to contain more errors.

Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance.

Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision.

Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled.

This often includes training a classifier to make the filtering decision.

The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent (Nooralahzadeh et al., 2019).

Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).

The noise in the labels can also be modeled.

A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016; Luo et al., 2017; Hedderich and Klakow, 2018; Paul et al., 2019; Lange et al., 2019a,c; Chen et al., 2019; Wang et al., 2019; Hedderich et al., 2021b).

The classifier is no longer trained directly on the noisily-labeled data.

Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution.

This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels.

In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing.

In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances.","Question:

How do noise filtering and noise modeling methods address the challenges of training with noisy labels?

Answer:

The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations.

These labels tend, however, to contain more errors.

Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance.

Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision.

Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled.

This often includes training a classifier to make the filtering decision.

The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent (Nooralahzadeh et al., 2019).

Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).

The noise in the labels can also be modeled.

A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016; Luo et al., 2017; Hedderich and Klakow, 2018; Paul et al., 2019; Lange et al., 2019a,c; Chen et al., 2019; Wang et al., 2019; Hedderich et al., 2021b).

The classifier is no longer trained directly on the noisily-labeled data.

Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution.

This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels.

In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing.

In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,Learning with Noisy Labels,12,"The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain more errors. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision. We categorize these into two ideas: noise filtering and noise modeling.

Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).

The noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances. Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.

In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly. Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).","sent1: The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations.
sent2: These labels tend, however, to contain more errors.
sent3: Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance.
sent4: Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision.
sent5: We categorize these into two ideas: noise filtering and noise modeling.
sent6: Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled.
sent7: This often includes training a classifier to make the filtering decision.
sent8: The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019).
sent9: Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).
sent10: The noise in the labels can also be modeled.
sent11: A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b).
sent12: The classifier is no longer trained directly on the noisily-labeled data.
sent13: Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution.
sent14: This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels.
sent15: In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing.
sent16: In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances.
sent17: Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.
sent18: In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method.
sent19: Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly.
sent20: Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).","1. What is the advantage of the above-presented methods for obtaining labeled data? sent1
    1.1. What is a downside of these labels? sent2
    1.2. How can training on noisily-labeled data affect performance? sent3
2. What approaches are used to handle noise in distant supervision? sent4
    2.1. How are these approaches categorized? sent5
        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9
        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16
3. How do Rehbein and Ruppenhofer (2017), Lison et al. (2020), and Ren et al. (2020) handle distant supervision? sent17
4. What is a common type of noise in distantly supervised labels for NER? sent18
    4.1. How does partial annotation learning address this issue? sent19
5. What are some related approaches to handling noise in labels? sent20","2. What approaches are used to handle noise in distant supervision? sent4
    2.1. How are these approaches categorized? sent5
        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9
        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16

    2.1. How are these approaches categorized? sent5
        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9
        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16

        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9

        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16","        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9","Question: How do noise filtering methods manage incorrectly labeled instances in training data for NLP tasks?

Supporting sentence: sent6, sent7, sent8, sent9",How do noise filtering methods manage incorrectly labeled instances in training data for NLP tasks?,"sent6, sent7, sent8, sent9",4,implement,"Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled.

This often includes training a classifier to make the filtering decision.

The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent (Nooralahzadeh et al., 2019).

Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).","Question:

How do noise filtering methods manage incorrectly labeled instances in training data for NLP tasks?

Answer:

Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled.

This often includes training a classifier to make the filtering decision.

The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent (Nooralahzadeh et al., 2019).

Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019)."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,Learning with Noisy Labels,12,"The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain more errors. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision. We categorize these into two ideas: noise filtering and noise modeling.

Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).

The noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances. Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.

In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly. Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).","sent1: The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations.
sent2: These labels tend, however, to contain more errors.
sent3: Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance.
sent4: Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision.
sent5: We categorize these into two ideas: noise filtering and noise modeling.
sent6: Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled.
sent7: This often includes training a classifier to make the filtering decision.
sent8: The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019).
sent9: Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).
sent10: The noise in the labels can also be modeled.
sent11: A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b).
sent12: The classifier is no longer trained directly on the noisily-labeled data.
sent13: Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution.
sent14: This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels.
sent15: In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing.
sent16: In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances.
sent17: Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.
sent18: In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method.
sent19: Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly.
sent20: Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).","1. What is the advantage of the above-presented methods for obtaining labeled data? sent1
    1.1. What is a downside of these labels? sent2
    1.2. How can training on noisily-labeled data affect performance? sent3
2. What approaches are used to handle noise in distant supervision? sent4
    2.1. How are these approaches categorized? sent5
        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9
        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16
3. How do Rehbein and Ruppenhofer (2017), Lison et al. (2020), and Ren et al. (2020) handle distant supervision? sent17
4. What is a common type of noise in distantly supervised labels for NER? sent18
    4.1. How does partial annotation learning address this issue? sent19
5. What are some related approaches to handling noise in labels? sent20","2. What approaches are used to handle noise in distant supervision? sent4
    2.1. How are these approaches categorized? sent5
        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9
        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16

    2.1. How are these approaches categorized? sent5
        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9
        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16

        2.1.1. What do noise filtering methods do? sent6
            2.1.1.1. How is the filtering decision often made? sent7
            2.1.1.2. What are some methods for completely removing instances from training data? sent8
            2.1.1.3. What is an alternative to completely removing instances? sent9

        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16","        2.1.2. How can noise in labels be modeled? sent10
            2.1.2.1. What is a common model used for noise modeling? sent11
            2.1.2.2. How does the noise model affect the training of the classifier? sent12, sent13
            2.1.2.3. How is the prediction adjusted during testing according to Ye et al. (2019)? sent15
            2.1.2.4. How do reinforcement agents contribute to noise handling in Chen et al. (2020a)? sent16","Question: How does noise modeling impact the training process of classifiers in NLP tasks with noisy labels?

Supporting sentence: sent10, sent11, sent12, sent13",How does noise modeling impact the training process of classifiers in NLP tasks with noisy labels?,"sent10, sent11, sent12, sent13",4,implement,"The noise in the labels can be modeled.

A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016; Luo et al., 2017; Hedderich and Klakow, 2018; Paul et al., 2019; Lange et al., 2019a,c; Chen et al., 2019; Wang et al., 2019; Hedderich et al., 2021b).

The classifier is no longer trained directly on the noisily-labeled data.

Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution.

This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels.

In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing.

In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances.","Question:

How does noise modeling impact the training process of classifiers in NLP tasks with noisy labels?

Answer:

The noise in the labels can be modeled.

A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016; Luo et al., 2017; Hedderich and Klakow, 2018; Paul et al., 2019; Lange et al., 2019a,c; Chen et al., 2019; Wang et al., 2019; Hedderich et al., 2021b).

The classifier is no longer trained directly on the noisily-labeled data.

Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution.

This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels.

In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing.

In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances."
236460206,Towards Argument Mining for Social Good: A Survey,https://www.semanticscholar.org/paper/dcb0b23685c9c116d8d53fe47e5157753659d3bd,Grounding AQ in deliberation: moderation as a real-world application,10,"Grounding AQ in a discourse perspective which quantifies ""team-playing"" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies. We propose to make a first step in tackling this challenge by mapping it into a concrete application: (semi-)automatic moderation implemented as a form of discourse optimization, or, as it is commonly referred to in the Social Sciences, facilitation (Kaner et al., 2007;Trénel, 2009). To illustrate the dynamics of moderation, let us start from concrete examples from a deliberation platform, RegulationRoom. This discussion forum has been employed by public institutions to gather citizens contributions on discussions targeting very heterogeneous issues (more details can be found in Appendix). Let us consider the following example from a discussion on the distracted driving by commercial vehicle operators (e.g., truckers and bus drivers). The posts we selected (arrows indicate comment nesting) are from the discussion sub-thread: Texting -what are the risks? 5 The example involves two users who clearly differ in their argumentation style and position. User 1 has a clear position on the topic (claim in bold: not just texting, but all cellphone interactions should be banned), which she/he supports with personal reports (underlined text) an emotional tone, and a style which is typical of social media text. User 2 replies, opening the post on a sarcastic note, which serves as the first premise to her/his (implicit) claim which is encoded in three rethorical questions (in bold): there should be no restrictions at all, because imposing them would be unfair. This is the case because (premises underlined): any distraction can cause an accident, some people are capable of using their phone while driving, people who spend lot of time in the car for professional reasons still need to communicate with loved ones. A moderator then joins the discussion to (a) provide a clarification as to why the focus is on texting and a link to further information on the matter, and (b) ask User 2 to elaborate on the personal communication issue, and to propose alternatives. In the Appendix we report another example from the same topic and thread, where the user acts as a problematizer, challenging the scope and definition of the rule under discussion and the moderator acts as a ""discourse traffic director"", pointing out that the user should read and contribute to different threads in the discussion.

The guidelines for human moderators in Reg-ulationRoom have been defined in advance in a 'moderator protocol' (eRulemaking Initiative et al., 2017) which reflect the moderator actions mentioned in the examples. In the protocol the moderator roles were divided into two main classes. Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (policing), or helping with technical difficulties. Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse. As the examples above clearly show, this can both mean that the moderator encourages exchanges between discourse participants and participation in other posts (broadening the scope of the discussion), or helping users to improve the content of their posts (requests for clarification, focusing on one topic, substantive reasoning, sharing personal experiences).

RegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants. However, to the best of our knowledge, there is little to no NLP work targeting moderation modeling. Park et al. (2012) used data from Regula-tionRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol. Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment. However this work is limited as it only focuses on two types of moderator interventions (broadening the scope of the discussion, improving argument quality) and as it does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as ""in need for moderation"".

Besides the concrete example of Regulation-Room, moderation and discourse facilitation have been, and still are, a crucial topic in digital democracy. 6 The know-how of digital democracy experts is an invaluable starting point for the application of AM to moderation, as current research targets both the integration of digital solutions to facilitate online campaigns, and a critical reflection of the effects of such innovations on the deliberation outcomes.

Digital innovation supporting deliberation Argument maps (Walton, 2005) are widely employed to support online discussions, as an emerging optimization of the deliberation. Given a specific topic, for example possible reactions to climate change, users who wish to contribute to the discussion are requested to structure their contribution by producing an item in a conceptual map and optionally writing an accompanying post. Their contribution to the argument maps is often reviewed by a moderator. So in a sense, the argument map for a given deliberation process is the outcome of a process that comes both from below (the user) and above (the moderator).

Thanks to argument maps, the overall discourse picture can be overviewed and it is easier for the group of contributors to express support for one (or many) of the available options, without having to read a large number of long posts. An example of this approach is represented in Deliberatorium 7 , an e-deliberation platform which has been extensively employed in many reference studies on the effect of digital innovation on deliberation (Klein, 2011). Another example of a digital deliberation platform which integrates argument maps and offers an option for moderation is COLAGREE (Yang et al., 2021;Ito, 2018). Among the studies testing the impact of such digital platforms on online deliberation, Spada et al. (2015) tests the effect of Deliberatorium's argument maps on an online discussion among the supporters of the Italian Democratic party concerning the desired features of electoral law to be proposed by the party to the Parliament. This study compared the discussion of users employing Deliberatorium and a control group using a traditional forum format which was then encoded into argument maps. The comparison showed that the argument map modality did not discourage participation, and while it appeared to make users less creative (fewer new ideas as compared to the traditional forum), it also reduced the rate of claims without further discussion.

Yet, the need for trained moderators tends to be a significant bottleneck (both in terms of time and of costs) in digital deliberation. Moreover, empirical research on the effect of moderation on deliberation has uncovered the risks of biased moderation. For example, the experiment in Spada and Vreeland (2013) tests the extent to which moderators can influence participants' behavior by expressing their views during the moderation process.","sent1: Grounding AQ in a discourse perspective which quantifies ""team-playing"" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies.
sent2: We propose to make a first step in tackling this challenge by mapping it into a concrete application: (semi-)automatic moderation implemented as a form of discourse optimization, or, as it is commonly referred to in the Social Sciences, facilitation (Kaner et al., 2007;Trénel, 2009).
sent3: To illustrate the dynamics of moderation, let us start from concrete examples from a deliberation platform, RegulationRoom.
sent4: This discussion forum has been employed by public institutions to gather citizens contributions on discussions targeting very heterogeneous issues (more details can be found in Appendix).
sent5: Let us consider the following example from a discussion on the distracted driving by commercial vehicle operators (e.g., truckers and bus drivers).
sent6: The posts we selected (arrows indicate comment nesting) are from the discussion sub-thread: Texting -what are the risks?
sent7: 5 The example involves two users who clearly differ in their argumentation style and position.
sent8: User 1 has a clear position on the topic (claim in bold: not just texting, but all cellphone interactions should be banned), which she/he supports with personal reports (underlined text) an emotional tone, and a style which is typical of social media text.
sent9: User 2 replies, opening the post on a sarcastic note, which serves as the first premise to her/his (implicit) claim which is encoded in three rethorical questions (in bold): there should be no restrictions at all, because imposing them would be unfair.
sent10: This is the case because (premises underlined): any distraction can cause an accident, some people are capable of using their phone while driving, people who spend lot of time in the car for professional reasons still need to communicate with loved ones.
sent11: A moderator then joins the discussion to (a) provide a clarification as to why the focus is on texting and a link to further information on the matter, and (b) ask User 2 to elaborate on the personal communication issue, and to propose alternatives.
sent12: In the Appendix we report another example from the same topic and thread, where the user acts as a problematizer, challenging the scope and definition of the rule under discussion and the moderator acts as a ""discourse traffic director"", pointing out that the user should read and contribute to different threads in the discussion.
sent13: The guidelines for human moderators in Reg-ulationRoom have been defined in advance in a 'moderator protocol' (eRulemaking Initiative et al., 2017) which reflect the moderator actions mentioned in the examples.
sent14: In the protocol the moderator roles were divided into two main classes.
sent15: Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (policing), or helping with technical difficulties.
sent16: Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse.
sent17: As the examples above clearly show, this can both mean that the moderator encourages exchanges between discourse participants and participation in other posts (broadening the scope of the discussion), or helping users to improve the content of their posts (requests for clarification, focusing on one topic, substantive reasoning, sharing personal experiences).RegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants.
sent18: However, to the best of our knowledge, there is little to no NLP work targeting moderation modeling.
sent19: Park et al. (2012) used data from Regula-tionRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol.
sent20: Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment.
sent21: However this work is limited as it only focuses on two types of moderator interventions (broadening the scope of the discussion, improving argument quality) and as it does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as ""in need for moderation"".
sent22: Besides the concrete example of Regulation-Room, moderation and discourse facilitation have been, and still are, a crucial topic in digital democracy.
sent23: 6 The know-how of digital democracy experts is an invaluable starting point for the application of AM to moderation, as current research targets both the integration of digital solutions to facilitate online campaigns, and a critical reflection of the effects of such innovations on the deliberation outcomes.
sent24: Digital innovation supporting deliberation Argument maps (Walton, 2005) are widely employed to support online discussions, as an emerging optimization of the deliberation.
sent25: Given a specific topic, for example possible reactions to climate change, users who wish to contribute to the discussion are requested to structure their contribution by producing an item in a conceptual map and optionally writing an accompanying post.
sent26: Their contribution to the argument maps is often reviewed by a moderator.
sent27: So in a sense, the argument map for a given deliberation process is the outcome of a process that comes both from below (the user) and above (the moderator).
sent28: Thanks to argument maps, the overall discourse picture can be overviewed and it is easier for the group of contributors to express support for one (or many) of the available options, without having to read a large number of long posts.
sent29: An example of this approach is represented in Deliberatorium 7 , an e-deliberation platform which has been extensively employed in many reference studies on the effect of digital innovation on deliberation (Klein, 2011).
sent30: Another example of a digital deliberation platform which integrates argument maps and offers an option for moderation is COLAGREE (Yang et al., 2021;Ito, 2018).
sent31: Among the studies testing the impact of such digital platforms on online deliberation, Spada et al. (2015) tests the effect of Deliberatorium's argument maps on an online discussion among the supporters of the Italian Democratic party concerning the desired features of electoral law to be proposed by the party to the Parliament.
sent32: This study compared the discussion of users employing Deliberatorium and a control group using a traditional forum format which was then encoded into argument maps.
sent33: The comparison showed that the argument map modality did not discourage participation, and while it appeared to make users less creative (fewer new ideas as compared to the traditional forum), it also reduced the rate of claims without further discussion.
sent34: Yet, the need for trained moderators tends to be a significant bottleneck (both in terms of time and of costs) in digital deliberation.
sent35: Moreover, empirical research on the effect of moderation on deliberation has uncovered the risks of biased moderation.
sent36: For example, the experiment in Spada and Vreeland (2013) tests the extent to which moderators can influence participants' behavior by expressing their views during the moderation process.","1. What is the challenge in grounding AQ in a discourse perspective? sent1
2. How do the authors propose to tackle the challenge of grounding AQ in discourse? sent2
3. What platform is used to illustrate the dynamics of moderation? sent3
    3.1. What is the purpose of the RegulationRoom platform? sent4
    3.2. Can you provide an example of a discussion topic on RegulationRoom? sent5
    3.3. How are comments structured in the discussion sub-thread? sent6
    3.4. What is the argumentation style of User 1 in the example? sent7, sent8
    3.5. How does User 2 respond to User 1's argument? sent9, sent10
    3.6. What role does the moderator play in the discussion? sent11
    3.7. Where can another example from the same topic and thread be found? sent12
4. What is the 'moderator protocol' in RegulationRoom? sent13
    4.1. How are moderator roles divided in the protocol? sent14
    4.2. What are supervision functions in the context of moderation? sent15
    4.3. What are substantive moderator functions? sent16
        4.3.1. How do substantive moderator functions benefit the discussion? sent17
5. What is the current state of NLP work targeting moderation modeling? sent18
    5.1. What study used data from RegulationRoom to categorize moderator interventions? sent19
    5.2. What were the limitations of the study conducted by Park et al. (2012)? sent20, sent21
6. How is moderation and discourse facilitation relevant to digital democracy? sent22
    6.1. What role do digital democracy experts play in the application of AM to moderation? sent23
7. How do argument maps support online discussions? sent24
    7.1. How are user contributions structured in argument maps? sent25
    7.2. What role does a moderator play in argument maps? sent26
    7.3. How do argument maps benefit the deliberation process? sent27, sent28
    7.4. What is an example of a platform that uses argument maps? sent29
    7.5. What is another platform that integrates argument maps and offers moderation? sent30
8. What study tested the impact of digital platforms on online deliberation? sent31
    8.1. How was the study conducted by Spada et al. (2015) structured? sent32
    8.2. What were the findings of the study comparing argument maps and traditional forums? sent33
9. What is a significant bottleneck in digital deliberation? sent34
10. What risks have been uncovered in empirical research on moderation's effect on deliberation? sent35
    10.1. Can you provide an example of how moderators can influence participants' behavior? sent36","3. What platform is used to illustrate the dynamics of moderation? sent3
    3.1. What is the purpose of the RegulationRoom platform? sent4
    3.2. Can you provide an example of a discussion topic on RegulationRoom? sent5
    3.3. How are comments structured in the discussion sub-thread? sent6
    3.4. What is the argumentation style of User 1 in the example? sent7, sent8
    3.5. How does User 2 respond to User 1's argument? sent9, sent10
    3.6. What role does the moderator play in the discussion? sent11
    3.7. Where can another example from the same topic and thread be found? sent12

4. What is the 'moderator protocol' in RegulationRoom? sent13
    4.1. How are moderator roles divided in the protocol? sent14
    4.2. What are supervision functions in the context of moderation? sent15
    4.3. What are substantive moderator functions? sent16
        4.3.1. How do substantive moderator functions benefit the discussion? sent17

5. What is the current state of NLP work targeting moderation modeling? sent18
    5.1. What study used data from RegulationRoom to categorize moderator interventions? sent19
    5.2. What were the limitations of the study conducted by Park et al. (2012)? sent20, sent21

7. How do argument maps support online discussions? sent24
    7.1. How are user contributions structured in argument maps? sent25
    7.2. What role does a moderator play in argument maps? sent26
    7.3. How do argument maps benefit the deliberation process? sent27, sent28
    7.4. What is an example of a platform that uses argument maps? sent29
    7.5. What is another platform that integrates argument maps and offers moderation? sent30","3. What platform is used to illustrate the dynamics of moderation? sent3
    3.1. What is the purpose of the RegulationRoom platform? sent4
    3.2. Can you provide an example of a discussion topic on RegulationRoom? sent5
    3.3. How are comments structured in the discussion sub-thread? sent6
    3.4. What is the argumentation style of User 1 in the example? sent7, sent8
    3.5. How does User 2 respond to User 1's argument? sent9, sent10
    3.6. What role does the moderator play in the discussion? sent11
    3.7. Where can another example from the same topic and thread be found? sent12","Question: How does the moderator contribute to the discussion dynamics in the RegulationRoom platform?

Supporting sentence: sent11, sent12",How does the moderator contribute to the discussion dynamics in the RegulationRoom platform?,"sent11, sent12",4,implement,"Grounding AQ in a discourse perspective which quantifies ""team-playing"" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies.

A moderator joins the discussion to provide a clarification as to why the focus is on texting and a link to further information on the matter, and to ask User 2 to elaborate on the personal communication issue, and to propose alternatives.

In another example, the moderator acts as a ""discourse traffic director"", pointing out that the user should read and contribute to different threads in the discussion.

The guidelines for human moderators in RegulationRoom have been defined in advance in a 'moderator protocol' which reflect the moderator actions mentioned in the examples.

In the protocol, the moderator roles were divided into two main classes.

Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (policing), or helping with technical difficulties.

Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse.

This can mean that the moderator encourages exchanges between discourse participants and participation in other posts (broadening the scope of the discussion), or helping users to improve the content of their posts (requests for clarification, focusing on one topic, substantive reasoning, sharing personal experiences).

RegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants.","Question:

How does the moderator contribute to the discussion dynamics in the RegulationRoom platform?

Answer:

Grounding AQ in a discourse perspective which quantifies ""team-playing"" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies.

A moderator joins the discussion to provide a clarification as to why the focus is on texting and a link to further information on the matter, and to ask User 2 to elaborate on the personal communication issue, and to propose alternatives.

In another example, the moderator acts as a ""discourse traffic director"", pointing out that the user should read and contribute to different threads in the discussion.

The guidelines for human moderators in RegulationRoom have been defined in advance in a 'moderator protocol' which reflect the moderator actions mentioned in the examples.

In the protocol, the moderator roles were divided into two main classes.

Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (policing), or helping with technical difficulties.

Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse.

This can mean that the moderator encourages exchanges between discourse participants and participation in other posts (broadening the scope of the discussion), or helping users to improve the content of their posts (requests for clarification, focusing on one topic, substantive reasoning, sharing personal experiences).

RegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants."
236460206,Towards Argument Mining for Social Good: A Survey,https://www.semanticscholar.org/paper/dcb0b23685c9c116d8d53fe47e5157753659d3bd,Grounding AQ in deliberation: moderation as a real-world application,10,"Grounding AQ in a discourse perspective which quantifies ""team-playing"" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies. We propose to make a first step in tackling this challenge by mapping it into a concrete application: (semi-)automatic moderation implemented as a form of discourse optimization, or, as it is commonly referred to in the Social Sciences, facilitation (Kaner et al., 2007;Trénel, 2009). To illustrate the dynamics of moderation, let us start from concrete examples from a deliberation platform, RegulationRoom. This discussion forum has been employed by public institutions to gather citizens contributions on discussions targeting very heterogeneous issues (more details can be found in Appendix). Let us consider the following example from a discussion on the distracted driving by commercial vehicle operators (e.g., truckers and bus drivers). The posts we selected (arrows indicate comment nesting) are from the discussion sub-thread: Texting -what are the risks? 5 The example involves two users who clearly differ in their argumentation style and position. User 1 has a clear position on the topic (claim in bold: not just texting, but all cellphone interactions should be banned), which she/he supports with personal reports (underlined text) an emotional tone, and a style which is typical of social media text. User 2 replies, opening the post on a sarcastic note, which serves as the first premise to her/his (implicit) claim which is encoded in three rethorical questions (in bold): there should be no restrictions at all, because imposing them would be unfair. This is the case because (premises underlined): any distraction can cause an accident, some people are capable of using their phone while driving, people who spend lot of time in the car for professional reasons still need to communicate with loved ones. A moderator then joins the discussion to (a) provide a clarification as to why the focus is on texting and a link to further information on the matter, and (b) ask User 2 to elaborate on the personal communication issue, and to propose alternatives. In the Appendix we report another example from the same topic and thread, where the user acts as a problematizer, challenging the scope and definition of the rule under discussion and the moderator acts as a ""discourse traffic director"", pointing out that the user should read and contribute to different threads in the discussion.

The guidelines for human moderators in Reg-ulationRoom have been defined in advance in a 'moderator protocol' (eRulemaking Initiative et al., 2017) which reflect the moderator actions mentioned in the examples. In the protocol the moderator roles were divided into two main classes. Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (policing), or helping with technical difficulties. Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse. As the examples above clearly show, this can both mean that the moderator encourages exchanges between discourse participants and participation in other posts (broadening the scope of the discussion), or helping users to improve the content of their posts (requests for clarification, focusing on one topic, substantive reasoning, sharing personal experiences).

RegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants. However, to the best of our knowledge, there is little to no NLP work targeting moderation modeling. Park et al. (2012) used data from Regula-tionRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol. Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment. However this work is limited as it only focuses on two types of moderator interventions (broadening the scope of the discussion, improving argument quality) and as it does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as ""in need for moderation"".

Besides the concrete example of Regulation-Room, moderation and discourse facilitation have been, and still are, a crucial topic in digital democracy. 6 The know-how of digital democracy experts is an invaluable starting point for the application of AM to moderation, as current research targets both the integration of digital solutions to facilitate online campaigns, and a critical reflection of the effects of such innovations on the deliberation outcomes.

Digital innovation supporting deliberation Argument maps (Walton, 2005) are widely employed to support online discussions, as an emerging optimization of the deliberation. Given a specific topic, for example possible reactions to climate change, users who wish to contribute to the discussion are requested to structure their contribution by producing an item in a conceptual map and optionally writing an accompanying post. Their contribution to the argument maps is often reviewed by a moderator. So in a sense, the argument map for a given deliberation process is the outcome of a process that comes both from below (the user) and above (the moderator).

Thanks to argument maps, the overall discourse picture can be overviewed and it is easier for the group of contributors to express support for one (or many) of the available options, without having to read a large number of long posts. An example of this approach is represented in Deliberatorium 7 , an e-deliberation platform which has been extensively employed in many reference studies on the effect of digital innovation on deliberation (Klein, 2011). Another example of a digital deliberation platform which integrates argument maps and offers an option for moderation is COLAGREE (Yang et al., 2021;Ito, 2018). Among the studies testing the impact of such digital platforms on online deliberation, Spada et al. (2015) tests the effect of Deliberatorium's argument maps on an online discussion among the supporters of the Italian Democratic party concerning the desired features of electoral law to be proposed by the party to the Parliament. This study compared the discussion of users employing Deliberatorium and a control group using a traditional forum format which was then encoded into argument maps. The comparison showed that the argument map modality did not discourage participation, and while it appeared to make users less creative (fewer new ideas as compared to the traditional forum), it also reduced the rate of claims without further discussion.

Yet, the need for trained moderators tends to be a significant bottleneck (both in terms of time and of costs) in digital deliberation. Moreover, empirical research on the effect of moderation on deliberation has uncovered the risks of biased moderation. For example, the experiment in Spada and Vreeland (2013) tests the extent to which moderators can influence participants' behavior by expressing their views during the moderation process.","sent1: Grounding AQ in a discourse perspective which quantifies ""team-playing"" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies.
sent2: We propose to make a first step in tackling this challenge by mapping it into a concrete application: (semi-)automatic moderation implemented as a form of discourse optimization, or, as it is commonly referred to in the Social Sciences, facilitation (Kaner et al., 2007;Trénel, 2009).
sent3: To illustrate the dynamics of moderation, let us start from concrete examples from a deliberation platform, RegulationRoom.
sent4: This discussion forum has been employed by public institutions to gather citizens contributions on discussions targeting very heterogeneous issues (more details can be found in Appendix).
sent5: Let us consider the following example from a discussion on the distracted driving by commercial vehicle operators (e.g., truckers and bus drivers).
sent6: The posts we selected (arrows indicate comment nesting) are from the discussion sub-thread: Texting -what are the risks?
sent7: 5 The example involves two users who clearly differ in their argumentation style and position.
sent8: User 1 has a clear position on the topic (claim in bold: not just texting, but all cellphone interactions should be banned), which she/he supports with personal reports (underlined text) an emotional tone, and a style which is typical of social media text.
sent9: User 2 replies, opening the post on a sarcastic note, which serves as the first premise to her/his (implicit) claim which is encoded in three rethorical questions (in bold): there should be no restrictions at all, because imposing them would be unfair.
sent10: This is the case because (premises underlined): any distraction can cause an accident, some people are capable of using their phone while driving, people who spend lot of time in the car for professional reasons still need to communicate with loved ones.
sent11: A moderator then joins the discussion to (a) provide a clarification as to why the focus is on texting and a link to further information on the matter, and (b) ask User 2 to elaborate on the personal communication issue, and to propose alternatives.
sent12: In the Appendix we report another example from the same topic and thread, where the user acts as a problematizer, challenging the scope and definition of the rule under discussion and the moderator acts as a ""discourse traffic director"", pointing out that the user should read and contribute to different threads in the discussion.
sent13: The guidelines for human moderators in Reg-ulationRoom have been defined in advance in a 'moderator protocol' (eRulemaking Initiative et al., 2017) which reflect the moderator actions mentioned in the examples.
sent14: In the protocol the moderator roles were divided into two main classes.
sent15: Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (policing), or helping with technical difficulties.
sent16: Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse.
sent17: As the examples above clearly show, this can both mean that the moderator encourages exchanges between discourse participants and participation in other posts (broadening the scope of the discussion), or helping users to improve the content of their posts (requests for clarification, focusing on one topic, substantive reasoning, sharing personal experiences).RegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants.
sent18: However, to the best of our knowledge, there is little to no NLP work targeting moderation modeling.
sent19: Park et al. (2012) used data from Regula-tionRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol.
sent20: Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment.
sent21: However this work is limited as it only focuses on two types of moderator interventions (broadening the scope of the discussion, improving argument quality) and as it does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as ""in need for moderation"".
sent22: Besides the concrete example of Regulation-Room, moderation and discourse facilitation have been, and still are, a crucial topic in digital democracy.
sent23: 6 The know-how of digital democracy experts is an invaluable starting point for the application of AM to moderation, as current research targets both the integration of digital solutions to facilitate online campaigns, and a critical reflection of the effects of such innovations on the deliberation outcomes.
sent24: Digital innovation supporting deliberation Argument maps (Walton, 2005) are widely employed to support online discussions, as an emerging optimization of the deliberation.
sent25: Given a specific topic, for example possible reactions to climate change, users who wish to contribute to the discussion are requested to structure their contribution by producing an item in a conceptual map and optionally writing an accompanying post.
sent26: Their contribution to the argument maps is often reviewed by a moderator.
sent27: So in a sense, the argument map for a given deliberation process is the outcome of a process that comes both from below (the user) and above (the moderator).
sent28: Thanks to argument maps, the overall discourse picture can be overviewed and it is easier for the group of contributors to express support for one (or many) of the available options, without having to read a large number of long posts.
sent29: An example of this approach is represented in Deliberatorium 7 , an e-deliberation platform which has been extensively employed in many reference studies on the effect of digital innovation on deliberation (Klein, 2011).
sent30: Another example of a digital deliberation platform which integrates argument maps and offers an option for moderation is COLAGREE (Yang et al., 2021;Ito, 2018).
sent31: Among the studies testing the impact of such digital platforms on online deliberation, Spada et al. (2015) tests the effect of Deliberatorium's argument maps on an online discussion among the supporters of the Italian Democratic party concerning the desired features of electoral law to be proposed by the party to the Parliament.
sent32: This study compared the discussion of users employing Deliberatorium and a control group using a traditional forum format which was then encoded into argument maps.
sent33: The comparison showed that the argument map modality did not discourage participation, and while it appeared to make users less creative (fewer new ideas as compared to the traditional forum), it also reduced the rate of claims without further discussion.
sent34: Yet, the need for trained moderators tends to be a significant bottleneck (both in terms of time and of costs) in digital deliberation.
sent35: Moreover, empirical research on the effect of moderation on deliberation has uncovered the risks of biased moderation.
sent36: For example, the experiment in Spada and Vreeland (2013) tests the extent to which moderators can influence participants' behavior by expressing their views during the moderation process.","1. What is the challenge in grounding AQ in a discourse perspective? sent1
2. How do the authors propose to tackle the challenge of grounding AQ in discourse? sent2
3. What platform is used to illustrate the dynamics of moderation? sent3
    3.1. What is the purpose of the RegulationRoom platform? sent4
    3.2. Can you provide an example of a discussion topic on RegulationRoom? sent5
    3.3. How are comments structured in the discussion sub-thread? sent6
    3.4. What is the argumentation style of User 1 in the example? sent7, sent8
    3.5. How does User 2 respond to User 1's argument? sent9, sent10
    3.6. What role does the moderator play in the discussion? sent11
    3.7. Where can another example from the same topic and thread be found? sent12
4. What is the 'moderator protocol' in RegulationRoom? sent13
    4.1. How are moderator roles divided in the protocol? sent14
    4.2. What are supervision functions in the context of moderation? sent15
    4.3. What are substantive moderator functions? sent16
        4.3.1. How do substantive moderator functions benefit the discussion? sent17
5. What is the current state of NLP work targeting moderation modeling? sent18
    5.1. What study used data from RegulationRoom to categorize moderator interventions? sent19
    5.2. What were the limitations of the study conducted by Park et al. (2012)? sent20, sent21
6. How is moderation and discourse facilitation relevant to digital democracy? sent22
    6.1. What role do digital democracy experts play in the application of AM to moderation? sent23
7. How do argument maps support online discussions? sent24
    7.1. How are user contributions structured in argument maps? sent25
    7.2. What role does a moderator play in argument maps? sent26
    7.3. How do argument maps benefit the deliberation process? sent27, sent28
    7.4. What is an example of a platform that uses argument maps? sent29
    7.5. What is another platform that integrates argument maps and offers moderation? sent30
8. What study tested the impact of digital platforms on online deliberation? sent31
    8.1. How was the study conducted by Spada et al. (2015) structured? sent32
    8.2. What were the findings of the study comparing argument maps and traditional forums? sent33
9. What is a significant bottleneck in digital deliberation? sent34
10. What risks have been uncovered in empirical research on moderation's effect on deliberation? sent35
    10.1. Can you provide an example of how moderators can influence participants' behavior? sent36","3. What platform is used to illustrate the dynamics of moderation? sent3
    3.1. What is the purpose of the RegulationRoom platform? sent4
    3.2. Can you provide an example of a discussion topic on RegulationRoom? sent5
    3.3. How are comments structured in the discussion sub-thread? sent6
    3.4. What is the argumentation style of User 1 in the example? sent7, sent8
    3.5. How does User 2 respond to User 1's argument? sent9, sent10
    3.6. What role does the moderator play in the discussion? sent11
    3.7. Where can another example from the same topic and thread be found? sent12

4. What is the 'moderator protocol' in RegulationRoom? sent13
    4.1. How are moderator roles divided in the protocol? sent14
    4.2. What are supervision functions in the context of moderation? sent15
    4.3. What are substantive moderator functions? sent16
        4.3.1. How do substantive moderator functions benefit the discussion? sent17

5. What is the current state of NLP work targeting moderation modeling? sent18
    5.1. What study used data from RegulationRoom to categorize moderator interventions? sent19
    5.2. What were the limitations of the study conducted by Park et al. (2012)? sent20, sent21

7. How do argument maps support online discussions? sent24
    7.1. How are user contributions structured in argument maps? sent25
    7.2. What role does a moderator play in argument maps? sent26
    7.3. How do argument maps benefit the deliberation process? sent27, sent28
    7.4. What is an example of a platform that uses argument maps? sent29
    7.5. What is another platform that integrates argument maps and offers moderation? sent30","5. What is the current state of NLP work targeting moderation modeling? sent18
    5.1. What study used data from RegulationRoom to categorize moderator interventions? sent19
    5.2. What were the limitations of the study conducted by Park et al. (2012)? sent20, sent21","Question: What are the limitations of existing studies on moderation modeling using data from RegulationRoom?

Supporting sentence: sent20, sent21",What are the limitations of existing studies on moderation modeling using data from RegulationRoom?,"sent20, sent21",4,implement,"Park et al. (2012) used data from RegulationRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol.

Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment.

However, this work is limited as it only focuses on two types of moderator interventions (broadening the scope of the discussion, improving argument quality) and does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as ""in need for moderation"".","Question:

What are the limitations of existing studies on moderation modeling using data from RegulationRoom?

Answer:

Park et al. (2012) used data from RegulationRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol.

Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment.

However, this work is limited as it only focuses on two types of moderator interventions (broadening the scope of the discussion, improving argument quality) and does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as ""in need for moderation""."
251402499,Abstractive Meeting Summarization: A Survey,https://www.semanticscholar.org/paper/6dd2ffed96ab44004b2e4cb1bf36a44daae14d42,Evaluation methods,8,"As noted in Section 2, the subjectivity of meeting summaries, stemming in part from the preference for abstractive summarization, complicates the evaluation task. Evaluation requires both verifying that the content in a system-based summary follows from the original transcript and measuring the overlap with a gold-standard summary. However, given the expressive freedom encouraged by abstractive approaches, checking for semantic entailment and overlap of summary-worthy content requires deep semantic understanding, not just recognition that the same words are used.

Unfortunately, the ROUGE metric (Lin, 2004), which remains the standard for both meeting and general text summarization, scores systemproduced summaries based purely on surface lexicographic matches with a (usually single) gold summary, making it unideal for assessing abstractive summaries. If we take the abstractive summary from Figure 1 (""Some part of the casing will be made of a spongy material"") as a gold example, ROUGE would assign a higher score to a system that produces ""Some part of the casing will be made of broccoli"" than one that output ""A portion of the outer layer will be constructed from a sponge-like material,"" even though the latter is a perfect reformulation of the gold summary, while the former says something very different (and false).

A reasonable way to try to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap. Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text. Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022;Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks.

Despite their rapid development, recent studies on meta-evaluation of these metrics show mixed results. Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, e.g., there is no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate. Bhandari et al. (2020b) argue that there is no one-size-fitsall metric that correlates better with human judgement than the others, and that can outperform others on all datasets.

Clearly, evaluation is itself a very challenging task. And we note that none of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency. When we summarize a meeting or detail decisions and actions items in our own words, it is important to get the facts straight. Otherwise, the resulting summaries are not reliable for an end user. While we do not know of current work that focuses on evaluation of factuality explicitly for the meeting domain, the study of factual consistency in summarization more generally is a budding research area that we discuss in Section 6.

In the absence of a clear winner for summary evaluation metrics, none of the alternatives has yet to be widely adopted. In our comparison of different summarization systems in Section 5, we therefore stick with ROUGE, which offers the additional advantage of conceptual simplicity and lower computational cost compared to metrics based on contextual embeddings and pretrained language models.","sent1: As noted in Section 2, the subjectivity of meeting summaries, stemming in part from the preference for abstractive summarization, complicates the evaluation task.
sent2: Evaluation requires both verifying that the content in a system-based summary follows from the original transcript and measuring the overlap with a gold-standard summary.
sent3: However, given the expressive freedom encouraged by abstractive approaches, checking for semantic entailment and overlap of summary-worthy content requires deep semantic understanding, not just recognition that the same words are used.
sent4: Unfortunately, the ROUGE metric (Lin, 2004), which remains the standard for both meeting and general text summarization, scores systemproduced summaries based purely on surface lexicographic matches with a (usually single) gold summary, making it unideal for assessing abstractive summaries.
sent5: If we take the abstractive summary from Figure 1 (""Some part of the casing will be made of a spongy material"") as a gold example, ROUGE would assign a higher score to a system that produces ""Some part of the casing will be made of broccoli"" than one that output ""A portion of the outer layer will be constructed from a sponge-like material,"" even though the latter is a perfect reformulation of the gold summary, while the former says something very different (and false).
sent6: A reasonable way to try to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap.
sent7: Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text.
sent8: Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022;Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks.
sent9: Despite their rapid development, recent studies on meta-evaluation of these metrics show mixed results.
sent10: Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, e.g., there is no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate.
sent11: Bhandari et al. (2020b) argue that there is no one-size-fitsall metric that correlates better with human judgement than the others, and that can outperform others on all datasets.
sent12: Clearly, evaluation is itself a very challenging task.
sent13: And we note that none of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency.
sent14: When we summarize a meeting or detail decisions and actions items in our own words, it is important to get the facts straight.
sent15: Otherwise, the resulting summaries are not reliable for an end user.
sent16: While we do not know of current work that focuses on evaluation of factuality explicitly for the meeting domain, the study of factual consistency in summarization more generally is a budding research area that we discuss in Section 6.
sent17: In the absence of a clear winner for summary evaluation metrics, none of the alternatives has yet to be widely adopted.
sent18: In our comparison of different summarization systems in Section 5, we therefore stick with ROUGE, which offers the additional advantage of conceptual simplicity and lower computational cost compared to metrics based on contextual embeddings and pretrained language models.","1. What complicates the evaluation task of meeting summaries? sent1
2. What are the requirements for evaluating system-based summaries? sent2
3. Why does evaluating abstractive summaries require deep semantic understanding? sent3
4. Why is the ROUGE metric considered unideal for assessing abstractive summaries? sent4
    4.1. Can you provide an example illustrating the limitations of the ROUGE metric? sent5
5. What is a suggested way to improve over the ROUGE metric? sent6
    5.1. What are some recent efforts that pursue this direction? sent7
    5.2. How do DATScore and FrugalScore aim to improve performance? sent8
6. What do recent studies on meta-evaluation of these metrics show? sent9
    6.1. What do Peyrard (2019) and Bhandari et al. (2020a) reveal about the performance of these metrics? sent10
    6.2. What argument does Bhandari et al. (2020b) make regarding summary evaluation metrics? sent11
7. What is another central challenge for summary evaluation that these metrics do not address? sent13
    7.1. Why is factual consistency important in meeting summaries? sent14, sent15
    7.2. Is there current work focusing on the evaluation of factuality in the meeting domain? sent16
8. What is the current status of summary evaluation metrics in terms of adoption? sent17
9. Why do the authors stick with the ROUGE metric in their comparison of different summarization systems? sent18","7. What is another central challenge for summary evaluation that these metrics do not address? sent13
    7.1. Why is factual consistency important in meeting summaries? sent14, sent15
    7.2. Is there current work focusing on the evaluation of factuality in the meeting domain? sent16","7. What is another central challenge for summary evaluation that these metrics do not address? sent13
    7.1. Why is factual consistency important in meeting summaries? sent14, sent15
    7.2. Is there current work focusing on the evaluation of factuality in the meeting domain? sent16","Question: Why is factual consistency crucial in the evaluation of meeting summaries?

Supporting sentence: sent14, sent15",Why is factual consistency crucial in the evaluation of meeting summaries?,"sent14, sent15",2,implement,"When summarizing a meeting or detailing decisions and action items in one's own words, it is important to get the facts straight.

Otherwise, the resulting summaries are not reliable for an end user.","Question:

Why is factual consistency crucial in the evaluation of meeting summaries?

Answer:

When summarizing a meeting or detailing decisions and action items in one's own words, it is important to get the facts straight.

Otherwise, the resulting summaries are not reliable for an end user."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,Standalone GNNs,8,"We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.

The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.

The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.

The classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.

The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.

An older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.

A model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.

This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.

Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.

HAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.

The results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","sent1: We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by .
sent2: We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS.
sent3: An illustration of the general concepts presented here can be seen in Figure 3.
sent4: The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes.
sent5: The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge.
sent6: The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document.
sent7: This is a flexible structure, as it can be used in a single-document but also multidocument setting.
sent8: Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism.
sent9: Inspired by HSG. The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word.
sent10: The neural network consists of a modified GAT layer.
sent11: The GAT is modified to consider the TF-IDF value of the connecting edge.
sent12: Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution.
sent13: In total three convolution layers are used, word-sentence, sentence-word and word-document.
sent14: The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.
sent15: The classification itself is done by a single linear layer.
sent16: The model then does not directly use the predicted nodes to produce the summary.
sent17: Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.
sent18: The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset.
sent19: One should especially note the flexibility and ability to use this model for two tasks.
sent20: An older model by Muratore et al. (2010) can be considered a precursor to this architecture.
sent21: A simple extension to the HSG model is proposed by Ya et al. (2021).
sent22: In their extension they modify the model for query constraints for the summary.
sent23: This is achieved by adding a query node to the graph structure.
sent24: Additionally, they introduce a mu-tual information maximization mechanism during training.
sent25: A model which further follows this structure is the one by Linmei et al. (2019).
sent26: The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution.
sent27: The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels.
sent28: In particular, they encode the semantic and syntactical relationship between sentences within the graph.
sent29: This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019).
sent30: They introduce an additional universal feature vector which is added to each sentence node embedding.
sent31: This universal feature vector is learned from a large unrelated and general corpus.
sent32: This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.
sent33: Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a).
sent34: The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities.
sent35: The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens.
sent36: The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence.
sent37: Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram.
sent38: Additionally, sequentially occurring words and entities are connected with a directed edge.
sent39: This setup shows how one can encode a substantial amount of implicit information in an explicit manner.
sent40: HAHSum uses a GAT for each of the five node type combinations found within the graph.
sent41: Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.
sent42: The results for HAHSum show that GNNs can perform very well.
sent43: The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset.
sent44: The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS .
sent45: The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness.
sent46: Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by .
sent47: Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents.
sent48: That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution.
sent49: These sub-graphs are then ranked and thereby selected for a summary.
sent50: This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem.
sent51: Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","1. What is the focus of the discussion on standalone GNN models in this section? sent1
    1.1. Why is the HeterSumGraph (HSG) model used as an example? sent2
    1.2. How is the HSG model structured in terms of nodes and connections? sent4, sent5, sent6
    1.3. What flexibility does the HSG model offer? sent7
    1.4. How are feature vectors and edge weights obtained in the HSG model? sent9
    1.5. What modifications are made to the GAT layer in the HSG model? sent10, sent11
    1.6. What additional layer is applied in the HSG model, and how many convolution layers are used? sent12, sent13
    1.7. What is the training task for the HSG model? sent14
    1.8. How is the classification performed in the HSG model? sent15
    1.9. How does the HSG model ensure sparsity in the generated summary? sent16, sent17
    1.10. How does the HSG model perform compared to other models? sent18
    1.11. What is notable about the flexibility and ability of the HSG model? sent19
    1.12. What older model is considered a precursor to the HSG architecture? sent20
2. What extension to the HSG model is proposed by Ya et al. (2021)? sent21
    2.1. How is the HSG model modified for query constraints? sent22, sent23
    2.2. What mechanism is introduced during training in the extension by Ya et al. (2021)? sent24
3. What further development follows the structure of the HSG model? sent25
    3.1. How does Linmei et al. (2019) extend the attention mechanism? sent26
    3.2. How does the GNN model by Jing et al. (2021) encode additional information? sent27, sent28
    3.3. What additional feature is introduced by Antognini and Faltings (2019)? sent29, sent30
    3.4. What is unique about the model by Antognini and Faltings (2019)? sent31, sent32
4. What is the HAHSum model, and how does it build on the basic structure of HSG? sent33
    4.1. How is the input graph for HAHSum constructed? sent34, sent35, sent36, sent37, sent38
    4.2. What does the setup of HAHSum demonstrate? sent39
    4.3. How does HAHSum utilize GAT and FFN? sent40, sent41
    4.4. How does HAHSum perform in comparison to other models? sent42, sent43, sent44
    4.5. What do the results of an Amazon Mechanical Turk experiment show about HAHSum? sent45
5. What is the SgSum model, and how does it differ from previous approaches? sent46
    5.1. How does the SgSum model use graph pooling? sent47, sent48
    5.2. What innovative approach does the SgSum model take? sent49, sent50
    5.3. What is the output format of the SgSum model? sent51","1. What is the focus of the discussion on standalone GNN models in this section? sent1
    1.1. Why is the HeterSumGraph (HSG) model used as an example? sent2
    1.2. How is the HSG model structured in terms of nodes and connections? sent4, sent5, sent6
    1.3. What flexibility does the HSG model offer? sent7
    1.4. How are feature vectors and edge weights obtained in the HSG model? sent9
    1.5. What modifications are made to the GAT layer in the HSG model? sent10, sent11
    1.6. What additional layer is applied in the HSG model, and how many convolution layers are used? sent12, sent13
    1.7. What is the training task for the HSG model? sent14
    1.8. How is the classification performed in the HSG model? sent15
    1.9. How does the HSG model ensure sparsity in the generated summary? sent16, sent17
    1.10. How does the HSG model perform compared to other models? sent18
    1.11. What is notable about the flexibility and ability of the HSG model? sent19
    1.12. What older model is considered a precursor to the HSG architecture? sent20

2. What extension to the HSG model is proposed by Ya et al. (2021)? sent21
    2.1. How is the HSG model modified for query constraints? sent22, sent23
    2.2. What mechanism is introduced during training in the extension by Ya et al. (2021)? sent24

3. What further development follows the structure of the HSG model? sent25
    3.1. How does Linmei et al. (2019) extend the attention mechanism? sent26
    3.2. How does the GNN model by Jing et al. (2021) encode additional information? sent27, sent28
    3.3. What additional feature is introduced by Antognini and Faltings (2019)? sent29, sent30
    3.4. What is unique about the model by Antognini and Faltings (2019)? sent31, sent32

4. What is the HAHSum model, and how does it build on the basic structure of HSG? sent33
    4.1. How is the input graph for HAHSum constructed? sent34, sent35, sent36, sent37, sent38
    4.2. What does the setup of HAHSum demonstrate? sent39
    4.3. How does HAHSum utilize GAT and FFN? sent40, sent41
    4.4. How does HAHSum perform in comparison to other models? sent42, sent43, sent44
    4.5. What do the results of an Amazon Mechanical Turk experiment show about HAHSum? sent45

5. What is the SgSum model, and how does it differ from previous approaches? sent46
    5.1. How does the SgSum model use graph pooling? sent47, sent48
    5.2. What innovative approach does the SgSum model take? sent49, sent50
    5.3. What is the output format of the SgSum model? sent51","1. What is the focus of the discussion on standalone GNN models in this section? sent1
    1.1. Why is the HeterSumGraph (HSG) model used as an example? sent2
    1.2. How is the HSG model structured in terms of nodes and connections? sent4, sent5, sent6
    1.3. What flexibility does the HSG model offer? sent7
    1.4. How are feature vectors and edge weights obtained in the HSG model? sent9
    1.5. What modifications are made to the GAT layer in the HSG model? sent10, sent11
    1.6. What additional layer is applied in the HSG model, and how many convolution layers are used? sent12, sent13
    1.7. What is the training task for the HSG model? sent14
    1.8. How is the classification performed in the HSG model? sent15
    1.9. How does the HSG model ensure sparsity in the generated summary? sent16, sent17
    1.10. How does the HSG model perform compared to other models? sent18
    1.11. What is notable about the flexibility and ability of the HSG model? sent19
    1.12. What older model is considered a precursor to the HSG architecture? sent20","Question: How is the HeterSumGraph (HSG) model structured in terms of nodes and connections, and what flexibility does it offer?

Supporting sentence: sent4, sent5, sent6, sent7","How is the HeterSumGraph (HSG) model structured in terms of nodes and connections, and what flexibility does it offer?","sent4, sent5, sent6, sent7",2,implement,"The HeterSumGraph (HSG) model encodes each text into a graph with three node types: sentence nodes, word nodes, and document nodes.

The connection between these nodes is decided by inclusion; if the word represented by a word node occurs in a sentence, then their respective nodes are connected by an edge.

The same principle applies to document nodes, which are connected depending on whether a word, represented by a word node, occurs within the document.

This structure is flexible, as it can be used in both single-document and multi-document settings.","Question:

How is the HeterSumGraph (HSG) model structured in terms of nodes and connections, and what flexibility does it offer?

Answer:

The HeterSumGraph (HSG) model encodes each text into a graph with three node types: sentence nodes, word nodes, and document nodes.

The connection between these nodes is decided by inclusion; if the word represented by a word node occurs in a sentence, then their respective nodes are connected by an edge.

The same principle applies to document nodes, which are connected depending on whether a word, represented by a word node, occurs within the document.

This structure is flexible, as it can be used in both single-document and multi-document settings."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,Standalone GNNs,8,"We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.

The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.

The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.

The classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.

The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.

An older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.

A model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.

This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.

Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.

HAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.

The results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","sent1: We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by .
sent2: We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS.
sent3: An illustration of the general concepts presented here can be seen in Figure 3.
sent4: The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes.
sent5: The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge.
sent6: The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document.
sent7: This is a flexible structure, as it can be used in a single-document but also multidocument setting.
sent8: Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism.
sent9: Inspired by HSG. The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word.
sent10: The neural network consists of a modified GAT layer.
sent11: The GAT is modified to consider the TF-IDF value of the connecting edge.
sent12: Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution.
sent13: In total three convolution layers are used, word-sentence, sentence-word and word-document.
sent14: The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.
sent15: The classification itself is done by a single linear layer.
sent16: The model then does not directly use the predicted nodes to produce the summary.
sent17: Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.
sent18: The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset.
sent19: One should especially note the flexibility and ability to use this model for two tasks.
sent20: An older model by Muratore et al. (2010) can be considered a precursor to this architecture.
sent21: A simple extension to the HSG model is proposed by Ya et al. (2021).
sent22: In their extension they modify the model for query constraints for the summary.
sent23: This is achieved by adding a query node to the graph structure.
sent24: Additionally, they introduce a mu-tual information maximization mechanism during training.
sent25: A model which further follows this structure is the one by Linmei et al. (2019).
sent26: The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution.
sent27: The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels.
sent28: In particular, they encode the semantic and syntactical relationship between sentences within the graph.
sent29: This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019).
sent30: They introduce an additional universal feature vector which is added to each sentence node embedding.
sent31: This universal feature vector is learned from a large unrelated and general corpus.
sent32: This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.
sent33: Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a).
sent34: The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities.
sent35: The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens.
sent36: The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence.
sent37: Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram.
sent38: Additionally, sequentially occurring words and entities are connected with a directed edge.
sent39: This setup shows how one can encode a substantial amount of implicit information in an explicit manner.
sent40: HAHSum uses a GAT for each of the five node type combinations found within the graph.
sent41: Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.
sent42: The results for HAHSum show that GNNs can perform very well.
sent43: The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset.
sent44: The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS .
sent45: The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness.
sent46: Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by .
sent47: Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents.
sent48: That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution.
sent49: These sub-graphs are then ranked and thereby selected for a summary.
sent50: This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem.
sent51: Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","1. What is the focus of the discussion on standalone GNN models in this section? sent1
    1.1. Why is the HeterSumGraph (HSG) model used as an example? sent2
    1.2. How is the HSG model structured in terms of nodes and connections? sent4, sent5, sent6
    1.3. What flexibility does the HSG model offer? sent7
    1.4. How are feature vectors and edge weights obtained in the HSG model? sent9
    1.5. What modifications are made to the GAT layer in the HSG model? sent10, sent11
    1.6. What additional layer is applied in the HSG model, and how many convolution layers are used? sent12, sent13
    1.7. What is the training task for the HSG model? sent14
    1.8. How is the classification performed in the HSG model? sent15
    1.9. How does the HSG model ensure sparsity in the generated summary? sent16, sent17
    1.10. How does the HSG model perform compared to other models? sent18
    1.11. What is notable about the flexibility and ability of the HSG model? sent19
    1.12. What older model is considered a precursor to the HSG architecture? sent20
2. What extension to the HSG model is proposed by Ya et al. (2021)? sent21
    2.1. How is the HSG model modified for query constraints? sent22, sent23
    2.2. What mechanism is introduced during training in the extension by Ya et al. (2021)? sent24
3. What further development follows the structure of the HSG model? sent25
    3.1. How does Linmei et al. (2019) extend the attention mechanism? sent26
    3.2. How does the GNN model by Jing et al. (2021) encode additional information? sent27, sent28
    3.3. What additional feature is introduced by Antognini and Faltings (2019)? sent29, sent30
    3.4. What is unique about the model by Antognini and Faltings (2019)? sent31, sent32
4. What is the HAHSum model, and how does it build on the basic structure of HSG? sent33
    4.1. How is the input graph for HAHSum constructed? sent34, sent35, sent36, sent37, sent38
    4.2. What does the setup of HAHSum demonstrate? sent39
    4.3. How does HAHSum utilize GAT and FFN? sent40, sent41
    4.4. How does HAHSum perform in comparison to other models? sent42, sent43, sent44
    4.5. What do the results of an Amazon Mechanical Turk experiment show about HAHSum? sent45
5. What is the SgSum model, and how does it differ from previous approaches? sent46
    5.1. How does the SgSum model use graph pooling? sent47, sent48
    5.2. What innovative approach does the SgSum model take? sent49, sent50
    5.3. What is the output format of the SgSum model? sent51","1. What is the focus of the discussion on standalone GNN models in this section? sent1
    1.1. Why is the HeterSumGraph (HSG) model used as an example? sent2
    1.2. How is the HSG model structured in terms of nodes and connections? sent4, sent5, sent6
    1.3. What flexibility does the HSG model offer? sent7
    1.4. How are feature vectors and edge weights obtained in the HSG model? sent9
    1.5. What modifications are made to the GAT layer in the HSG model? sent10, sent11
    1.6. What additional layer is applied in the HSG model, and how many convolution layers are used? sent12, sent13
    1.7. What is the training task for the HSG model? sent14
    1.8. How is the classification performed in the HSG model? sent15
    1.9. How does the HSG model ensure sparsity in the generated summary? sent16, sent17
    1.10. How does the HSG model perform compared to other models? sent18
    1.11. What is notable about the flexibility and ability of the HSG model? sent19
    1.12. What older model is considered a precursor to the HSG architecture? sent20

2. What extension to the HSG model is proposed by Ya et al. (2021)? sent21
    2.1. How is the HSG model modified for query constraints? sent22, sent23
    2.2. What mechanism is introduced during training in the extension by Ya et al. (2021)? sent24

3. What further development follows the structure of the HSG model? sent25
    3.1. How does Linmei et al. (2019) extend the attention mechanism? sent26
    3.2. How does the GNN model by Jing et al. (2021) encode additional information? sent27, sent28
    3.3. What additional feature is introduced by Antognini and Faltings (2019)? sent29, sent30
    3.4. What is unique about the model by Antognini and Faltings (2019)? sent31, sent32

4. What is the HAHSum model, and how does it build on the basic structure of HSG? sent33
    4.1. How is the input graph for HAHSum constructed? sent34, sent35, sent36, sent37, sent38
    4.2. What does the setup of HAHSum demonstrate? sent39
    4.3. How does HAHSum utilize GAT and FFN? sent40, sent41
    4.4. How does HAHSum perform in comparison to other models? sent42, sent43, sent44
    4.5. What do the results of an Amazon Mechanical Turk experiment show about HAHSum? sent45

5. What is the SgSum model, and how does it differ from previous approaches? sent46
    5.1. How does the SgSum model use graph pooling? sent47, sent48
    5.2. What innovative approach does the SgSum model take? sent49, sent50
    5.3. What is the output format of the SgSum model? sent51","2. What extension to the HSG model is proposed by Ya et al. (2021)? sent21
    2.1. How is the HSG model modified for query constraints? sent22, sent23
    2.2. What mechanism is introduced during training in the extension by Ya et al. (2021)? sent24","Question: How does the extension by Ya et al. (2021) modify the HSG model for query constraints in text summarization?

Supporting sentence: sent21, sent22, sent23",How does the extension by Ya et al. (2021) modify the HSG model for query constraints in text summarization?,"sent21, sent22, sent23",1,implement,"A simple extension to the HSG model is proposed by Ya et al. (2021).

In their extension, they modify the model for query constraints for the summary.

This is achieved by adding a query node to the graph structure.

Additionally, they introduce a mutual information maximization mechanism during training.","Question:

How does the extension by Ya et al. (2021) modify the HSG model for query constraints in text summarization?

Answer:

A simple extension to the HSG model is proposed by Ya et al. (2021).

In their extension, they modify the model for query constraints for the summary.

This is achieved by adding a query node to the graph structure.

Additionally, they introduce a mutual information maximization mechanism during training."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,Standalone GNNs,8,"We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.

The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.

The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.

The classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.

The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.

An older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.

A model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.

This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.

Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.

HAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.

The results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","sent1: We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by .
sent2: We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS.
sent3: An illustration of the general concepts presented here can be seen in Figure 3.
sent4: The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes.
sent5: The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge.
sent6: The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document.
sent7: This is a flexible structure, as it can be used in a single-document but also multidocument setting.
sent8: Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism.
sent9: Inspired by HSG. The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word.
sent10: The neural network consists of a modified GAT layer.
sent11: The GAT is modified to consider the TF-IDF value of the connecting edge.
sent12: Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution.
sent13: In total three convolution layers are used, word-sentence, sentence-word and word-document.
sent14: The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.
sent15: The classification itself is done by a single linear layer.
sent16: The model then does not directly use the predicted nodes to produce the summary.
sent17: Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.
sent18: The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset.
sent19: One should especially note the flexibility and ability to use this model for two tasks.
sent20: An older model by Muratore et al. (2010) can be considered a precursor to this architecture.
sent21: A simple extension to the HSG model is proposed by Ya et al. (2021).
sent22: In their extension they modify the model for query constraints for the summary.
sent23: This is achieved by adding a query node to the graph structure.
sent24: Additionally, they introduce a mu-tual information maximization mechanism during training.
sent25: A model which further follows this structure is the one by Linmei et al. (2019).
sent26: The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution.
sent27: The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels.
sent28: In particular, they encode the semantic and syntactical relationship between sentences within the graph.
sent29: This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019).
sent30: They introduce an additional universal feature vector which is added to each sentence node embedding.
sent31: This universal feature vector is learned from a large unrelated and general corpus.
sent32: This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.
sent33: Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a).
sent34: The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities.
sent35: The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens.
sent36: The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence.
sent37: Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram.
sent38: Additionally, sequentially occurring words and entities are connected with a directed edge.
sent39: This setup shows how one can encode a substantial amount of implicit information in an explicit manner.
sent40: HAHSum uses a GAT for each of the five node type combinations found within the graph.
sent41: Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.
sent42: The results for HAHSum show that GNNs can perform very well.
sent43: The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset.
sent44: The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS .
sent45: The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness.
sent46: Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by .
sent47: Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents.
sent48: That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution.
sent49: These sub-graphs are then ranked and thereby selected for a summary.
sent50: This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem.
sent51: Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","1. What is the focus of the discussion on standalone GNN models in this section? sent1
    1.1. Why is the HeterSumGraph (HSG) model used as an example? sent2
    1.2. How is the HSG model structured in terms of nodes and connections? sent4, sent5, sent6
    1.3. What flexibility does the HSG model offer? sent7
    1.4. How are feature vectors and edge weights obtained in the HSG model? sent9
    1.5. What modifications are made to the GAT layer in the HSG model? sent10, sent11
    1.6. What additional layer is applied in the HSG model, and how many convolution layers are used? sent12, sent13
    1.7. What is the training task for the HSG model? sent14
    1.8. How is the classification performed in the HSG model? sent15
    1.9. How does the HSG model ensure sparsity in the generated summary? sent16, sent17
    1.10. How does the HSG model perform compared to other models? sent18
    1.11. What is notable about the flexibility and ability of the HSG model? sent19
    1.12. What older model is considered a precursor to the HSG architecture? sent20
2. What extension to the HSG model is proposed by Ya et al. (2021)? sent21
    2.1. How is the HSG model modified for query constraints? sent22, sent23
    2.2. What mechanism is introduced during training in the extension by Ya et al. (2021)? sent24
3. What further development follows the structure of the HSG model? sent25
    3.1. How does Linmei et al. (2019) extend the attention mechanism? sent26
    3.2. How does the GNN model by Jing et al. (2021) encode additional information? sent27, sent28
    3.3. What additional feature is introduced by Antognini and Faltings (2019)? sent29, sent30
    3.4. What is unique about the model by Antognini and Faltings (2019)? sent31, sent32
4. What is the HAHSum model, and how does it build on the basic structure of HSG? sent33
    4.1. How is the input graph for HAHSum constructed? sent34, sent35, sent36, sent37, sent38
    4.2. What does the setup of HAHSum demonstrate? sent39
    4.3. How does HAHSum utilize GAT and FFN? sent40, sent41
    4.4. How does HAHSum perform in comparison to other models? sent42, sent43, sent44
    4.5. What do the results of an Amazon Mechanical Turk experiment show about HAHSum? sent45
5. What is the SgSum model, and how does it differ from previous approaches? sent46
    5.1. How does the SgSum model use graph pooling? sent47, sent48
    5.2. What innovative approach does the SgSum model take? sent49, sent50
    5.3. What is the output format of the SgSum model? sent51","1. What is the focus of the discussion on standalone GNN models in this section? sent1
    1.1. Why is the HeterSumGraph (HSG) model used as an example? sent2
    1.2. How is the HSG model structured in terms of nodes and connections? sent4, sent5, sent6
    1.3. What flexibility does the HSG model offer? sent7
    1.4. How are feature vectors and edge weights obtained in the HSG model? sent9
    1.5. What modifications are made to the GAT layer in the HSG model? sent10, sent11
    1.6. What additional layer is applied in the HSG model, and how many convolution layers are used? sent12, sent13
    1.7. What is the training task for the HSG model? sent14
    1.8. How is the classification performed in the HSG model? sent15
    1.9. How does the HSG model ensure sparsity in the generated summary? sent16, sent17
    1.10. How does the HSG model perform compared to other models? sent18
    1.11. What is notable about the flexibility and ability of the HSG model? sent19
    1.12. What older model is considered a precursor to the HSG architecture? sent20

2. What extension to the HSG model is proposed by Ya et al. (2021)? sent21
    2.1. How is the HSG model modified for query constraints? sent22, sent23
    2.2. What mechanism is introduced during training in the extension by Ya et al. (2021)? sent24

3. What further development follows the structure of the HSG model? sent25
    3.1. How does Linmei et al. (2019) extend the attention mechanism? sent26
    3.2. How does the GNN model by Jing et al. (2021) encode additional information? sent27, sent28
    3.3. What additional feature is introduced by Antognini and Faltings (2019)? sent29, sent30
    3.4. What is unique about the model by Antognini and Faltings (2019)? sent31, sent32

4. What is the HAHSum model, and how does it build on the basic structure of HSG? sent33
    4.1. How is the input graph for HAHSum constructed? sent34, sent35, sent36, sent37, sent38
    4.2. What does the setup of HAHSum demonstrate? sent39
    4.3. How does HAHSum utilize GAT and FFN? sent40, sent41
    4.4. How does HAHSum perform in comparison to other models? sent42, sent43, sent44
    4.5. What do the results of an Amazon Mechanical Turk experiment show about HAHSum? sent45

5. What is the SgSum model, and how does it differ from previous approaches? sent46
    5.1. How does the SgSum model use graph pooling? sent47, sent48
    5.2. What innovative approach does the SgSum model take? sent49, sent50
    5.3. What is the output format of the SgSum model? sent51","3. What further development follows the structure of the HSG model? sent25
    3.1. How does Linmei et al. (2019) extend the attention mechanism? sent26
    3.2. How does the GNN model by Jing et al. (2021) encode additional information? sent27, sent28
    3.3. What additional feature is introduced by Antognini and Faltings (2019)? sent29, sent30
    3.4. What is unique about the model by Antognini and Faltings (2019)? sent31, sent32","Question: How does the GNN model by Jing et al. (2021) incorporate semantic and syntactical relationships in text summarization?

Supporting sentence: sent27, sent28",How does the GNN model by Jing et al. (2021) incorporate semantic and syntactical relationships in text summarization?,"sent27, sent28",2,implement,"The GNN model by Jing et al. (2021) encodes more information into the graph by considering the relation between sentences on a number of different levels.

In particular, they encode the semantic and syntactical relationship between sentences within the graph.","Question:

How does the GNN model by Jing et al. (2021) incorporate semantic and syntactical relationships in text summarization?

Answer:

The GNN model by Jing et al. (2021) encodes more information into the graph by considering the relation between sentences on a number of different levels.

In particular, they encode the semantic and syntactical relationship between sentences within the graph."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,Standalone GNNs,8,"We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.

The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.

The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.

The classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.

The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.

An older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.

A model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.

This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.

Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.

HAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.

The results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","sent1: We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by .
sent2: We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS.
sent3: An illustration of the general concepts presented here can be seen in Figure 3.
sent4: The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes.
sent5: The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge.
sent6: The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document.
sent7: This is a flexible structure, as it can be used in a single-document but also multidocument setting.
sent8: Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism.
sent9: Inspired by HSG. The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word.
sent10: The neural network consists of a modified GAT layer.
sent11: The GAT is modified to consider the TF-IDF value of the connecting edge.
sent12: Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution.
sent13: In total three convolution layers are used, word-sentence, sentence-word and word-document.
sent14: The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.
sent15: The classification itself is done by a single linear layer.
sent16: The model then does not directly use the predicted nodes to produce the summary.
sent17: Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.
sent18: The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset.
sent19: One should especially note the flexibility and ability to use this model for two tasks.
sent20: An older model by Muratore et al. (2010) can be considered a precursor to this architecture.
sent21: A simple extension to the HSG model is proposed by Ya et al. (2021).
sent22: In their extension they modify the model for query constraints for the summary.
sent23: This is achieved by adding a query node to the graph structure.
sent24: Additionally, they introduce a mu-tual information maximization mechanism during training.
sent25: A model which further follows this structure is the one by Linmei et al. (2019).
sent26: The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution.
sent27: The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels.
sent28: In particular, they encode the semantic and syntactical relationship between sentences within the graph.
sent29: This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019).
sent30: They introduce an additional universal feature vector which is added to each sentence node embedding.
sent31: This universal feature vector is learned from a large unrelated and general corpus.
sent32: This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.
sent33: Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a).
sent34: The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities.
sent35: The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens.
sent36: The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence.
sent37: Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram.
sent38: Additionally, sequentially occurring words and entities are connected with a directed edge.
sent39: This setup shows how one can encode a substantial amount of implicit information in an explicit manner.
sent40: HAHSum uses a GAT for each of the five node type combinations found within the graph.
sent41: Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.
sent42: The results for HAHSum show that GNNs can perform very well.
sent43: The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset.
sent44: The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS .
sent45: The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness.
sent46: Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by .
sent47: Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents.
sent48: That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution.
sent49: These sub-graphs are then ranked and thereby selected for a summary.
sent50: This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem.
sent51: Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","1. What is the focus of the discussion on standalone GNN models in this section? sent1
    1.1. Why is the HeterSumGraph (HSG) model used as an example? sent2
    1.2. How is the HSG model structured in terms of nodes and connections? sent4, sent5, sent6
    1.3. What flexibility does the HSG model offer? sent7
    1.4. How are feature vectors and edge weights obtained in the HSG model? sent9
    1.5. What modifications are made to the GAT layer in the HSG model? sent10, sent11
    1.6. What additional layer is applied in the HSG model, and how many convolution layers are used? sent12, sent13
    1.7. What is the training task for the HSG model? sent14
    1.8. How is the classification performed in the HSG model? sent15
    1.9. How does the HSG model ensure sparsity in the generated summary? sent16, sent17
    1.10. How does the HSG model perform compared to other models? sent18
    1.11. What is notable about the flexibility and ability of the HSG model? sent19
    1.12. What older model is considered a precursor to the HSG architecture? sent20
2. What extension to the HSG model is proposed by Ya et al. (2021)? sent21
    2.1. How is the HSG model modified for query constraints? sent22, sent23
    2.2. What mechanism is introduced during training in the extension by Ya et al. (2021)? sent24
3. What further development follows the structure of the HSG model? sent25
    3.1. How does Linmei et al. (2019) extend the attention mechanism? sent26
    3.2. How does the GNN model by Jing et al. (2021) encode additional information? sent27, sent28
    3.3. What additional feature is introduced by Antognini and Faltings (2019)? sent29, sent30
    3.4. What is unique about the model by Antognini and Faltings (2019)? sent31, sent32
4. What is the HAHSum model, and how does it build on the basic structure of HSG? sent33
    4.1. How is the input graph for HAHSum constructed? sent34, sent35, sent36, sent37, sent38
    4.2. What does the setup of HAHSum demonstrate? sent39
    4.3. How does HAHSum utilize GAT and FFN? sent40, sent41
    4.4. How does HAHSum perform in comparison to other models? sent42, sent43, sent44
    4.5. What do the results of an Amazon Mechanical Turk experiment show about HAHSum? sent45
5. What is the SgSum model, and how does it differ from previous approaches? sent46
    5.1. How does the SgSum model use graph pooling? sent47, sent48
    5.2. What innovative approach does the SgSum model take? sent49, sent50
    5.3. What is the output format of the SgSum model? sent51","1. What is the focus of the discussion on standalone GNN models in this section? sent1
    1.1. Why is the HeterSumGraph (HSG) model used as an example? sent2
    1.2. How is the HSG model structured in terms of nodes and connections? sent4, sent5, sent6
    1.3. What flexibility does the HSG model offer? sent7
    1.4. How are feature vectors and edge weights obtained in the HSG model? sent9
    1.5. What modifications are made to the GAT layer in the HSG model? sent10, sent11
    1.6. What additional layer is applied in the HSG model, and how many convolution layers are used? sent12, sent13
    1.7. What is the training task for the HSG model? sent14
    1.8. How is the classification performed in the HSG model? sent15
    1.9. How does the HSG model ensure sparsity in the generated summary? sent16, sent17
    1.10. How does the HSG model perform compared to other models? sent18
    1.11. What is notable about the flexibility and ability of the HSG model? sent19
    1.12. What older model is considered a precursor to the HSG architecture? sent20

2. What extension to the HSG model is proposed by Ya et al. (2021)? sent21
    2.1. How is the HSG model modified for query constraints? sent22, sent23
    2.2. What mechanism is introduced during training in the extension by Ya et al. (2021)? sent24

3. What further development follows the structure of the HSG model? sent25
    3.1. How does Linmei et al. (2019) extend the attention mechanism? sent26
    3.2. How does the GNN model by Jing et al. (2021) encode additional information? sent27, sent28
    3.3. What additional feature is introduced by Antognini and Faltings (2019)? sent29, sent30
    3.4. What is unique about the model by Antognini and Faltings (2019)? sent31, sent32

4. What is the HAHSum model, and how does it build on the basic structure of HSG? sent33
    4.1. How is the input graph for HAHSum constructed? sent34, sent35, sent36, sent37, sent38
    4.2. What does the setup of HAHSum demonstrate? sent39
    4.3. How does HAHSum utilize GAT and FFN? sent40, sent41
    4.4. How does HAHSum perform in comparison to other models? sent42, sent43, sent44
    4.5. What do the results of an Amazon Mechanical Turk experiment show about HAHSum? sent45

5. What is the SgSum model, and how does it differ from previous approaches? sent46
    5.1. How does the SgSum model use graph pooling? sent47, sent48
    5.2. What innovative approach does the SgSum model take? sent49, sent50
    5.3. What is the output format of the SgSum model? sent51","4. What is the HAHSum model, and how does it build on the basic structure of HSG? sent33
    4.1. How is the input graph for HAHSum constructed? sent34, sent35, sent36, sent37, sent38
    4.2. What does the setup of HAHSum demonstrate? sent39
    4.3. How does HAHSum utilize GAT and FFN? sent40, sent41
    4.4. How does HAHSum perform in comparison to other models? sent42, sent43, sent44
    4.5. What do the results of an Amazon Mechanical Turk experiment show about HAHSum? sent45","Question: How does the HAHSum model construct its input graph and utilize GAT and FFN for summarization?

Supporting sentence: sent34, sent35, sent36, sent37, sent38, sent40, sent41",How does the HAHSum model construct its input graph and utilize GAT and FFN for summarization?,"sent34, sent35, sent36, sent37, sent38, sent40, sent41",1,implement,"The HAHSum model constructs its input graph by utilizing named entities to significantly reduce semantic sparsity.

The model uses three types of nodes: named entity nodes, word nodes, and sentence nodes, with the named entity nodes being anonymized tokens.

The graph is built by connecting word nodes with a directed edge to a sentence node if they occur within the sentence.

Two named entities are connected with an undirected edge if they represent the same entity, and two sentence nodes are connected with an undirected edge if they share a trigram.

Additionally, sequentially occurring words and entities are connected with a directed edge.

HAHSum uses a Graph Attention Network (GAT) for each of the five node type combinations found within the graph.

A positionwise feed-forward network (FFN) is applied after the multi-head attention.

A linear layer is used to perform the binary classification of the sentence nodes.","Question:

How does the HAHSum model construct its input graph and utilize GAT and FFN for summarization?

Answer:

The HAHSum model constructs its input graph by utilizing named entities to significantly reduce semantic sparsity.

The model uses three types of nodes: named entity nodes, word nodes, and sentence nodes, with the named entity nodes being anonymized tokens.

The graph is built by connecting word nodes with a directed edge to a sentence node if they occur within the sentence.

Two named entities are connected with an undirected edge if they represent the same entity, and two sentence nodes are connected with an undirected edge if they share a trigram.

Additionally, sequentially occurring words and entities are connected with a directed edge.

HAHSum uses a Graph Attention Network (GAT) for each of the five node type combinations found within the graph.

A positionwise feed-forward network (FFN) is applied after the multi-head attention.

A linear layer is used to perform the binary classification of the sentence nodes."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,Standalone GNNs,8,"We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.

The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.

The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.

The classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.

The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.

An older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.

A model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.

This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.

Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.

HAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.

The results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","sent1: We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by .
sent2: We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS.
sent3: An illustration of the general concepts presented here can be seen in Figure 3.
sent4: The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes.
sent5: The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge.
sent6: The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document.
sent7: This is a flexible structure, as it can be used in a single-document but also multidocument setting.
sent8: Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism.
sent9: Inspired by HSG. The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word.
sent10: The neural network consists of a modified GAT layer.
sent11: The GAT is modified to consider the TF-IDF value of the connecting edge.
sent12: Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution.
sent13: In total three convolution layers are used, word-sentence, sentence-word and word-document.
sent14: The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.
sent15: The classification itself is done by a single linear layer.
sent16: The model then does not directly use the predicted nodes to produce the summary.
sent17: Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.
sent18: The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset.
sent19: One should especially note the flexibility and ability to use this model for two tasks.
sent20: An older model by Muratore et al. (2010) can be considered a precursor to this architecture.
sent21: A simple extension to the HSG model is proposed by Ya et al. (2021).
sent22: In their extension they modify the model for query constraints for the summary.
sent23: This is achieved by adding a query node to the graph structure.
sent24: Additionally, they introduce a mu-tual information maximization mechanism during training.
sent25: A model which further follows this structure is the one by Linmei et al. (2019).
sent26: The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution.
sent27: The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels.
sent28: In particular, they encode the semantic and syntactical relationship between sentences within the graph.
sent29: This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019).
sent30: They introduce an additional universal feature vector which is added to each sentence node embedding.
sent31: This universal feature vector is learned from a large unrelated and general corpus.
sent32: This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.
sent33: Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a).
sent34: The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities.
sent35: The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens.
sent36: The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence.
sent37: Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram.
sent38: Additionally, sequentially occurring words and entities are connected with a directed edge.
sent39: This setup shows how one can encode a substantial amount of implicit information in an explicit manner.
sent40: HAHSum uses a GAT for each of the five node type combinations found within the graph.
sent41: Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.
sent42: The results for HAHSum show that GNNs can perform very well.
sent43: The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset.
sent44: The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS .
sent45: The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness.
sent46: Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by .
sent47: Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents.
sent48: That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution.
sent49: These sub-graphs are then ranked and thereby selected for a summary.
sent50: This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem.
sent51: Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","1. What is the focus of the discussion on standalone GNN models in this section? sent1
    1.1. Why is the HeterSumGraph (HSG) model used as an example? sent2
    1.2. How is the HSG model structured in terms of nodes and connections? sent4, sent5, sent6
    1.3. What flexibility does the HSG model offer? sent7
    1.4. How are feature vectors and edge weights obtained in the HSG model? sent9
    1.5. What modifications are made to the GAT layer in the HSG model? sent10, sent11
    1.6. What additional layer is applied in the HSG model, and how many convolution layers are used? sent12, sent13
    1.7. What is the training task for the HSG model? sent14
    1.8. How is the classification performed in the HSG model? sent15
    1.9. How does the HSG model ensure sparsity in the generated summary? sent16, sent17
    1.10. How does the HSG model perform compared to other models? sent18
    1.11. What is notable about the flexibility and ability of the HSG model? sent19
    1.12. What older model is considered a precursor to the HSG architecture? sent20
2. What extension to the HSG model is proposed by Ya et al. (2021)? sent21
    2.1. How is the HSG model modified for query constraints? sent22, sent23
    2.2. What mechanism is introduced during training in the extension by Ya et al. (2021)? sent24
3. What further development follows the structure of the HSG model? sent25
    3.1. How does Linmei et al. (2019) extend the attention mechanism? sent26
    3.2. How does the GNN model by Jing et al. (2021) encode additional information? sent27, sent28
    3.3. What additional feature is introduced by Antognini and Faltings (2019)? sent29, sent30
    3.4. What is unique about the model by Antognini and Faltings (2019)? sent31, sent32
4. What is the HAHSum model, and how does it build on the basic structure of HSG? sent33
    4.1. How is the input graph for HAHSum constructed? sent34, sent35, sent36, sent37, sent38
    4.2. What does the setup of HAHSum demonstrate? sent39
    4.3. How does HAHSum utilize GAT and FFN? sent40, sent41
    4.4. How does HAHSum perform in comparison to other models? sent42, sent43, sent44
    4.5. What do the results of an Amazon Mechanical Turk experiment show about HAHSum? sent45
5. What is the SgSum model, and how does it differ from previous approaches? sent46
    5.1. How does the SgSum model use graph pooling? sent47, sent48
    5.2. What innovative approach does the SgSum model take? sent49, sent50
    5.3. What is the output format of the SgSum model? sent51","1. What is the focus of the discussion on standalone GNN models in this section? sent1
    1.1. Why is the HeterSumGraph (HSG) model used as an example? sent2
    1.2. How is the HSG model structured in terms of nodes and connections? sent4, sent5, sent6
    1.3. What flexibility does the HSG model offer? sent7
    1.4. How are feature vectors and edge weights obtained in the HSG model? sent9
    1.5. What modifications are made to the GAT layer in the HSG model? sent10, sent11
    1.6. What additional layer is applied in the HSG model, and how many convolution layers are used? sent12, sent13
    1.7. What is the training task for the HSG model? sent14
    1.8. How is the classification performed in the HSG model? sent15
    1.9. How does the HSG model ensure sparsity in the generated summary? sent16, sent17
    1.10. How does the HSG model perform compared to other models? sent18
    1.11. What is notable about the flexibility and ability of the HSG model? sent19
    1.12. What older model is considered a precursor to the HSG architecture? sent20

2. What extension to the HSG model is proposed by Ya et al. (2021)? sent21
    2.1. How is the HSG model modified for query constraints? sent22, sent23
    2.2. What mechanism is introduced during training in the extension by Ya et al. (2021)? sent24

3. What further development follows the structure of the HSG model? sent25
    3.1. How does Linmei et al. (2019) extend the attention mechanism? sent26
    3.2. How does the GNN model by Jing et al. (2021) encode additional information? sent27, sent28
    3.3. What additional feature is introduced by Antognini and Faltings (2019)? sent29, sent30
    3.4. What is unique about the model by Antognini and Faltings (2019)? sent31, sent32

4. What is the HAHSum model, and how does it build on the basic structure of HSG? sent33
    4.1. How is the input graph for HAHSum constructed? sent34, sent35, sent36, sent37, sent38
    4.2. What does the setup of HAHSum demonstrate? sent39
    4.3. How does HAHSum utilize GAT and FFN? sent40, sent41
    4.4. How does HAHSum perform in comparison to other models? sent42, sent43, sent44
    4.5. What do the results of an Amazon Mechanical Turk experiment show about HAHSum? sent45

5. What is the SgSum model, and how does it differ from previous approaches? sent46
    5.1. How does the SgSum model use graph pooling? sent47, sent48
    5.2. What innovative approach does the SgSum model take? sent49, sent50
    5.3. What is the output format of the SgSum model? sent51","5. What is the SgSum model, and how does it differ from previous approaches? sent46
    5.1. How does the SgSum model use graph pooling? sent47, sent48
    5.2. What innovative approach does the SgSum model take? sent49, sent50
    5.3. What is the output format of the SgSum model? sent51","Question: How does the SgSum model innovate in multi-document summarization using graph pooling and subgraph selection?

Supporting sentence: sent46, sent47, sent48, sent49, sent50",How does the SgSum model innovate in multi-document summarization using graph pooling and subgraph selection?,"sent46, sent47, sent48, sent49, sent50",2,implement,"The SgSum model innovates in multi-document summarization by using graph pooling to extract sub-graphs from encoded documents.

It first transforms the documents into a large graph.

Then, it generates a number of sub-graphs via pooling and convolution.

These sub-graphs are ranked and selected for a summary.

This approach is innovative as it casts the problem of multi-document summarization as a simple subgraph selection problem.

Additionally, it outputs an integral summary, meaning the entire summary is output by the model in the form of the sub-graph of sentences.","Question:

How does the SgSum model innovate in multi-document summarization using graph pooling and subgraph selection?

Answer:

The SgSum model innovates in multi-document summarization by using graph pooling to extract sub-graphs from encoded documents.

It first transforms the documents into a large graph.

Then, it generates a number of sub-graphs via pooling and convolution.

These sub-graphs are ranked and selected for a summary.

This approach is innovative as it casts the problem of multi-document summarization as a simple subgraph selection problem.

Additionally, it outputs an integral summary, meaning the entire summary is output by the model in the form of the sub-graph of sentences."
251402499,Abstractive Meeting Summarization: A Survey,https://www.semanticscholar.org/paper/6dd2ffed96ab44004b2e4cb1bf36a44daae14d42,Long input processing,9,"A straightforward solution to the length problem is to segment a long document into smaller segments to be processed. Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary. Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework. Within each stage, it first splits the source input into sufficiently short segments. Coarse summaries are then generated for each segment and then concatenated as the input to the next stage. This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.

While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem. Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022). Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions. Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems. Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription. The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure. First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors. That sequence is then processed by the turn-level encoder. The transformer decoder makes use of both levels of representation via cross-attention layers. Rohde et al. (2021) propose Hierachical Attention Transformer (HAT). Utterances are first prepended with a special BOS token. Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations. Finally, the decoder leverages the outputs at both levels to produce a final summary. Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.","sent1: A straightforward solution to the length problem is to segment a long document into smaller segments to be processed.
sent2: Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary.
sent3: Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework.
sent4: Within each stage, it first splits the source input into sufficiently short segments.
sent5: Coarse summaries are then generated for each segment and then concatenated as the input to the next stage.
sent6: This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.
sent7: While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem.
sent8: Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022).
sent9: Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions.
sent10: Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems.
sent11: Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription.
sent12: The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure.
sent13: First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors.
sent14: That sequence is then processed by the turn-level encoder.
sent15: The transformer decoder makes use of both levels of representation via cross-attention layers.
sent16: Rohde et al. (2021) propose Hierachical Attention Transformer (HAT).
sent17: Utterances are first prepended with a special BOS token.
sent18: Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations.
sent19: Finally, the decoder leverages the outputs at both levels to produce a final summary.
sent20: Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.","1. What is a straightforward solution to the length problem in meeting summarization? sent1
    1.1. How do Koay et al. (2021) address the length problem in ICSI meetings? sent2
    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6
2. What is a potential downside of segmenting approaches to the length problem? sent7
3. What are some transformer-based approaches to address the lengthy input problem? sent8
    3.1. How does Longformer address the lengthy input problem? sent9
    3.2. How do recent benchmarks evaluate the performance of long-sequence transformers for abstractive meeting summarization? sent10
4. How do hierarchical transformers process long meeting transcripts? sent11
    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15
    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19
    4.3. How does the hierarchical encoder proposed by Zhao et al. (2019b) function? sent20","1. What is a straightforward solution to the length problem in meeting summarization? sent1
    1.1. How do Koay et al. (2021) address the length problem in ICSI meetings? sent2
    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6

    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6

4. How do hierarchical transformers process long meeting transcripts? sent11
    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15
    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19
    4.3. How does the hierarchical encoder proposed by Zhao et al. (2019b) function? sent20

    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15

    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19","1. What is a straightforward solution to the length problem in meeting summarization? sent1
    1.1. How do Koay et al. (2021) address the length problem in ICSI meetings? sent2
    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6","Question: How does the SUMM N framework process long meeting inputs to produce a final summary?

Supporting sentence: sent3, sent4, sent5, sent6",How does the SUMM N framework process long meeting inputs to produce a final summary?,"sent3, sent4, sent5, sent6",2,implement,"The SUMM N framework is a multi-stage split-then-summarize approach.

Within each stage, it first splits the source input into sufficiently short segments.

Coarse summaries are then generated for each segment and concatenated as the input to the next stage.

This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.","Question:

How does the SUMM N framework process long meeting inputs to produce a final summary?

Answer:

The SUMM N framework is a multi-stage split-then-summarize approach.

Within each stage, it first splits the source input into sufficiently short segments.

Coarse summaries are then generated for each segment and concatenated as the input to the next stage.

This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced."
251402499,Abstractive Meeting Summarization: A Survey,https://www.semanticscholar.org/paper/6dd2ffed96ab44004b2e4cb1bf36a44daae14d42,Long input processing,9,"A straightforward solution to the length problem is to segment a long document into smaller segments to be processed. Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary. Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework. Within each stage, it first splits the source input into sufficiently short segments. Coarse summaries are then generated for each segment and then concatenated as the input to the next stage. This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.

While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem. Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022). Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions. Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems. Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription. The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure. First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors. That sequence is then processed by the turn-level encoder. The transformer decoder makes use of both levels of representation via cross-attention layers. Rohde et al. (2021) propose Hierachical Attention Transformer (HAT). Utterances are first prepended with a special BOS token. Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations. Finally, the decoder leverages the outputs at both levels to produce a final summary. Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.","sent1: A straightforward solution to the length problem is to segment a long document into smaller segments to be processed.
sent2: Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary.
sent3: Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework.
sent4: Within each stage, it first splits the source input into sufficiently short segments.
sent5: Coarse summaries are then generated for each segment and then concatenated as the input to the next stage.
sent6: This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.
sent7: While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem.
sent8: Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022).
sent9: Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions.
sent10: Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems.
sent11: Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription.
sent12: The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure.
sent13: First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors.
sent14: That sequence is then processed by the turn-level encoder.
sent15: The transformer decoder makes use of both levels of representation via cross-attention layers.
sent16: Rohde et al. (2021) propose Hierachical Attention Transformer (HAT).
sent17: Utterances are first prepended with a special BOS token.
sent18: Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations.
sent19: Finally, the decoder leverages the outputs at both levels to produce a final summary.
sent20: Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.","1. What is a straightforward solution to the length problem in meeting summarization? sent1
    1.1. How do Koay et al. (2021) address the length problem in ICSI meetings? sent2
    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6
2. What is a potential downside of segmenting approaches to the length problem? sent7
3. What are some transformer-based approaches to address the lengthy input problem? sent8
    3.1. How does Longformer address the lengthy input problem? sent9
    3.2. How do recent benchmarks evaluate the performance of long-sequence transformers for abstractive meeting summarization? sent10
4. How do hierarchical transformers process long meeting transcripts? sent11
    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15
    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19
    4.3. How does the hierarchical encoder proposed by Zhao et al. (2019b) function? sent20","1. What is a straightforward solution to the length problem in meeting summarization? sent1
    1.1. How do Koay et al. (2021) address the length problem in ICSI meetings? sent2
    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6

    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6

4. How do hierarchical transformers process long meeting transcripts? sent11
    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15
    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19
    4.3. How does the hierarchical encoder proposed by Zhao et al. (2019b) function? sent20

    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15

    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19","    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6","Question: How does the SUMM N framework address the challenge of processing long inputs in meeting summarization?

Supporting sentence: sent3, sent4, sent5, sent6",How does the SUMM N framework address the challenge of processing long inputs in meeting summarization?,"sent3, sent4, sent5, sent6",2,implement,"The SUMM N framework addresses the challenge of processing long inputs in meeting summarization by employing a multi-stage split-then-summarize approach.

Within each stage, it first splits the source input into sufficiently short segments.

Coarse summaries are then generated for each segment and concatenated as the input to the next stage.

This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.","Question:

How does the SUMM N framework address the challenge of processing long inputs in meeting summarization?

Answer:

The SUMM N framework addresses the challenge of processing long inputs in meeting summarization by employing a multi-stage split-then-summarize approach.

Within each stage, it first splits the source input into sufficiently short segments.

Coarse summaries are then generated for each segment and concatenated as the input to the next stage.

This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced."
251402499,Abstractive Meeting Summarization: A Survey,https://www.semanticscholar.org/paper/6dd2ffed96ab44004b2e4cb1bf36a44daae14d42,Long input processing,9,"A straightforward solution to the length problem is to segment a long document into smaller segments to be processed. Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary. Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework. Within each stage, it first splits the source input into sufficiently short segments. Coarse summaries are then generated for each segment and then concatenated as the input to the next stage. This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.

While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem. Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022). Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions. Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems. Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription. The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure. First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors. That sequence is then processed by the turn-level encoder. The transformer decoder makes use of both levels of representation via cross-attention layers. Rohde et al. (2021) propose Hierachical Attention Transformer (HAT). Utterances are first prepended with a special BOS token. Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations. Finally, the decoder leverages the outputs at both levels to produce a final summary. Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.","sent1: A straightforward solution to the length problem is to segment a long document into smaller segments to be processed.
sent2: Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary.
sent3: Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework.
sent4: Within each stage, it first splits the source input into sufficiently short segments.
sent5: Coarse summaries are then generated for each segment and then concatenated as the input to the next stage.
sent6: This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.
sent7: While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem.
sent8: Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022).
sent9: Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions.
sent10: Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems.
sent11: Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription.
sent12: The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure.
sent13: First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors.
sent14: That sequence is then processed by the turn-level encoder.
sent15: The transformer decoder makes use of both levels of representation via cross-attention layers.
sent16: Rohde et al. (2021) propose Hierachical Attention Transformer (HAT).
sent17: Utterances are first prepended with a special BOS token.
sent18: Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations.
sent19: Finally, the decoder leverages the outputs at both levels to produce a final summary.
sent20: Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.","1. What is a straightforward solution to the length problem in meeting summarization? sent1
    1.1. How do Koay et al. (2021) address the length problem in ICSI meetings? sent2
    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6
2. What is a potential downside of segmenting approaches to the length problem? sent7
3. What are some transformer-based approaches to address the lengthy input problem? sent8
    3.1. How does Longformer address the lengthy input problem? sent9
    3.2. How do recent benchmarks evaluate the performance of long-sequence transformers for abstractive meeting summarization? sent10
4. How do hierarchical transformers process long meeting transcripts? sent11
    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15
    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19
    4.3. How does the hierarchical encoder proposed by Zhao et al. (2019b) function? sent20","1. What is a straightforward solution to the length problem in meeting summarization? sent1
    1.1. How do Koay et al. (2021) address the length problem in ICSI meetings? sent2
    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6

    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6

4. How do hierarchical transformers process long meeting transcripts? sent11
    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15
    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19
    4.3. How does the hierarchical encoder proposed by Zhao et al. (2019b) function? sent20

    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15

    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19","4. How do hierarchical transformers process long meeting transcripts? sent11
    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15
    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19
    4.3. How does the hierarchical encoder proposed by Zhao et al. (2019b) function? sent20","Question: How do hierarchical transformers utilize different levels of representation to process long meeting transcripts?

Supporting sentence: sent11, sent12, sent13, sent14, sent15, sent16, sent17, sent18, sent19, sent20",How do hierarchical transformers utilize different levels of representation to process long meeting transcripts?,"sent11, sent12, sent13, sent14, sent15, sent16, sent17, sent18, sent19, sent20",2,implement,"Hierarchical transformers leverage transformers in a hierarchical manner, breaking down a long meeting transcript into multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription.

The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure.

First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors.

That sequence is then processed by the turn-level encoder.

The transformer decoder makes use of both levels of representation via cross-attention layers.

Rohde et al. (2021) propose Hierarchical Attention Transformer (HAT).

Utterances are first prepended with a special BOS token.

Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations.

Finally, the decoder leverages the outputs at both levels to produce a final summary.

Similarly, the hierarchical encoder of Zhao et al. (2019b) consists of three levels, sequentially encoding word, utterance, and topic segment embeddings.","Question:

How do hierarchical transformers utilize different levels of representation to process long meeting transcripts?

Answer:

Hierarchical transformers leverage transformers in a hierarchical manner, breaking down a long meeting transcript into multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription.

The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure.

First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors.

That sequence is then processed by the turn-level encoder.

The transformer decoder makes use of both levels of representation via cross-attention layers.

Rohde et al. (2021) propose Hierarchical Attention Transformer (HAT).

Utterances are first prepended with a special BOS token.

Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations.

Finally, the decoder leverages the outputs at both levels to produce a final summary.

Similarly, the hierarchical encoder of Zhao et al. (2019b) consists of three levels, sequentially encoding word, utterance, and topic segment embeddings."
251402499,Abstractive Meeting Summarization: A Survey,https://www.semanticscholar.org/paper/6dd2ffed96ab44004b2e4cb1bf36a44daae14d42,Long input processing,9,"A straightforward solution to the length problem is to segment a long document into smaller segments to be processed. Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary. Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework. Within each stage, it first splits the source input into sufficiently short segments. Coarse summaries are then generated for each segment and then concatenated as the input to the next stage. This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.

While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem. Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022). Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions. Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems. Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription. The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure. First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors. That sequence is then processed by the turn-level encoder. The transformer decoder makes use of both levels of representation via cross-attention layers. Rohde et al. (2021) propose Hierachical Attention Transformer (HAT). Utterances are first prepended with a special BOS token. Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations. Finally, the decoder leverages the outputs at both levels to produce a final summary. Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.","sent1: A straightforward solution to the length problem is to segment a long document into smaller segments to be processed.
sent2: Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary.
sent3: Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework.
sent4: Within each stage, it first splits the source input into sufficiently short segments.
sent5: Coarse summaries are then generated for each segment and then concatenated as the input to the next stage.
sent6: This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.
sent7: While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem.
sent8: Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022).
sent9: Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions.
sent10: Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems.
sent11: Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription.
sent12: The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure.
sent13: First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors.
sent14: That sequence is then processed by the turn-level encoder.
sent15: The transformer decoder makes use of both levels of representation via cross-attention layers.
sent16: Rohde et al. (2021) propose Hierachical Attention Transformer (HAT).
sent17: Utterances are first prepended with a special BOS token.
sent18: Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations.
sent19: Finally, the decoder leverages the outputs at both levels to produce a final summary.
sent20: Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.","1. What is a straightforward solution to the length problem in meeting summarization? sent1
    1.1. How do Koay et al. (2021) address the length problem in ICSI meetings? sent2
    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6
2. What is a potential downside of segmenting approaches to the length problem? sent7
3. What are some transformer-based approaches to address the lengthy input problem? sent8
    3.1. How does Longformer address the lengthy input problem? sent9
    3.2. How do recent benchmarks evaluate the performance of long-sequence transformers for abstractive meeting summarization? sent10
4. How do hierarchical transformers process long meeting transcripts? sent11
    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15
    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19
    4.3. How does the hierarchical encoder proposed by Zhao et al. (2019b) function? sent20","1. What is a straightforward solution to the length problem in meeting summarization? sent1
    1.1. How do Koay et al. (2021) address the length problem in ICSI meetings? sent2
    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6

    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6

4. How do hierarchical transformers process long meeting transcripts? sent11
    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15
    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19
    4.3. How does the hierarchical encoder proposed by Zhao et al. (2019b) function? sent20

    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15

    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19","    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15","Question: How does the HMNet model utilize hierarchical transformers to encode and process meeting transcripts?

Supporting sentence: sent12, sent13, sent14, sent15",How does the HMNet model utilize hierarchical transformers to encode and process meeting transcripts?,"sent12, sent13, sent14, sent15",1,implement,"The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure.

First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors.

That sequence is then processed by the turn-level encoder.

The transformer decoder makes use of both levels of representation via cross-attention layers.","Question:

How does the HMNet model utilize hierarchical transformers to encode and process meeting transcripts?

Answer:

The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure.

First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors.

That sequence is then processed by the turn-level encoder.

The transformer decoder makes use of both levels of representation via cross-attention layers."
251402499,Abstractive Meeting Summarization: A Survey,https://www.semanticscholar.org/paper/6dd2ffed96ab44004b2e4cb1bf36a44daae14d42,Long input processing,9,"A straightforward solution to the length problem is to segment a long document into smaller segments to be processed. Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary. Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework. Within each stage, it first splits the source input into sufficiently short segments. Coarse summaries are then generated for each segment and then concatenated as the input to the next stage. This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.

While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem. Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022). Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions. Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems. Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription. The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure. First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors. That sequence is then processed by the turn-level encoder. The transformer decoder makes use of both levels of representation via cross-attention layers. Rohde et al. (2021) propose Hierachical Attention Transformer (HAT). Utterances are first prepended with a special BOS token. Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations. Finally, the decoder leverages the outputs at both levels to produce a final summary. Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.","sent1: A straightforward solution to the length problem is to segment a long document into smaller segments to be processed.
sent2: Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary.
sent3: Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework.
sent4: Within each stage, it first splits the source input into sufficiently short segments.
sent5: Coarse summaries are then generated for each segment and then concatenated as the input to the next stage.
sent6: This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.
sent7: While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem.
sent8: Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022).
sent9: Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions.
sent10: Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems.
sent11: Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription.
sent12: The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure.
sent13: First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors.
sent14: That sequence is then processed by the turn-level encoder.
sent15: The transformer decoder makes use of both levels of representation via cross-attention layers.
sent16: Rohde et al. (2021) propose Hierachical Attention Transformer (HAT).
sent17: Utterances are first prepended with a special BOS token.
sent18: Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations.
sent19: Finally, the decoder leverages the outputs at both levels to produce a final summary.
sent20: Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.","1. What is a straightforward solution to the length problem in meeting summarization? sent1
    1.1. How do Koay et al. (2021) address the length problem in ICSI meetings? sent2
    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6
2. What is a potential downside of segmenting approaches to the length problem? sent7
3. What are some transformer-based approaches to address the lengthy input problem? sent8
    3.1. How does Longformer address the lengthy input problem? sent9
    3.2. How do recent benchmarks evaluate the performance of long-sequence transformers for abstractive meeting summarization? sent10
4. How do hierarchical transformers process long meeting transcripts? sent11
    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15
    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19
    4.3. How does the hierarchical encoder proposed by Zhao et al. (2019b) function? sent20","1. What is a straightforward solution to the length problem in meeting summarization? sent1
    1.1. How do Koay et al. (2021) address the length problem in ICSI meetings? sent2
    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6

    1.2. What framework do Zhang et al. (2022) propose for handling long inputs? sent3
        1.2.1. How does the SUMM N framework process long inputs? sent4, sent5, sent6

4. How do hierarchical transformers process long meeting transcripts? sent11
    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15
    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19
    4.3. How does the hierarchical encoder proposed by Zhao et al. (2019b) function? sent20

    4.1. What is the structure of the HMNet model proposed by Zhu et al. (2020)? sent12
        4.1.1. How does the HMNet model encode and process meeting transcripts? sent13, sent14, sent15

    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19","    4.2. What approach does Rohde et al. (2021) propose for hierarchical attention in transformers? sent16
        4.2.1. How does the Hierarchical Attention Transformer (HAT) process utterances and produce summaries? sent17, sent18, sent19","Question: How does the Hierarchical Attention Transformer (HAT) utilize hierarchical attention to process utterances and generate summaries?

Supporting sentence: sent16, sent17, sent18, sent19",How does the Hierarchical Attention Transformer (HAT) utilize hierarchical attention to process utterances and generate summaries?,"sent16, sent17, sent18, sent19",2,implement,"Utterances are first prepended with a special BOS token.

Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations.

Finally, the decoder leverages the outputs at both levels to produce a final summary.","Question:

How does the Hierarchical Attention Transformer (HAT) utilize hierarchical attention to process utterances and generate summaries?

Answer:

Utterances are first prepended with a special BOS token.

Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations.

Finally, the decoder leverages the outputs at both levels to produce a final summary."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,Self Contrastive Learning,10,"Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q ′ , d ′+/− ) from D, then uses them to supervise training of a NR model. The objective is:

where L is the ranking loss as in Eq 1. Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q ′ , d ′+ ). There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based. An overview is in Table 2.

Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair. The intuition is that perturbed text should still be relevant to the original text. Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).

Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other. The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document. They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness. Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.

Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021). For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus. A term from it is treated as the answer and replaced with a special token. Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents. Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.

Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant . For example, Chang et al.

(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic. A passage from another page containing hyperlinks to p is treated as a positive document. Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question. A passage from its hyperlinked document that contains the same entity word is treated as a positive sample. Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.  ","sent1: Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q ′ , d ′+/− ) from D, then uses them to supervise training of a NR model.
sent2: The objective is:where L is the ranking loss as in Eq 1.
sent3: Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q ′ , d ′+ ).
sent4: There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based.
sent5: An overview is in Table 2.Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair.
sent6: The intuition is that perturbed text should still be relevant to the original text.
sent7: Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).
sent8: Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other.
sent9: The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document.
sent10: They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness.
sent11: Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021).
sent12: For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus.
sent13: A term from it is treated as the answer and replaced with a special token.
sent14: Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents.
sent15: Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.
sent16: Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant .
sent17: For example, Chang et al.(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic.
sent18: A passage from another page containing hyperlinks to p is treated as a positive document.
sent19: Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question.
sent20: A passage from its hyperlinked document that contains the same entity word is treated as a positive sample.
sent21: Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.","1. What is self contrastive learning and how does it construct pseudo question-document pairs? sent1
    1.1. What is the objective of self contrastive learning? sent2
    1.2. What is the main difficulty in constructing positive pseudo pairs? sent3
    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21","1. What is self contrastive learning and how does it construct pseudo question-document pairs? sent1
    1.1. What is the objective of self contrastive learning? sent2
    1.2. What is the main difficulty in constructing positive pseudo pairs? sent3
    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21

    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21

        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11

        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15

        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21","1. What is self contrastive learning and how does it construct pseudo question-document pairs? sent1
    1.1. What is the objective of self contrastive learning? sent2
    1.2. What is the main difficulty in constructing positive pseudo pairs? sent3
    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21","Question: What are the popular heuristics for constructing positive pairs in self contrastive learning, and how do they function?

Supporting sentence: sent4, sent5, sent6, sent7, sent8, sent9, sent10, sent11, sent12, sent13, sent14, sent15, sent16, sent17, sent18, sent19, sent20, sent21","What are the popular heuristics for constructing positive pairs in self contrastive learning, and how do they function?","sent4, sent5, sent6, sent7, sent8, sent9, sent10, sent11, sent12, sent13, sent14, sent15, sent16, sent17, sent18, sent19, sent20, sent21",3,implement,"There are five popular heuristics to construct positive pairs in self contrastive learning: perturbation-based, summary-based, proximity-based, cooccurrence-based, and hyperlink-based.

Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair.

The intuition is that perturbed text should still be relevant to the original text.

Typical choices of perturbations include word deletion, substitution, and permutation (Zhu et al., 2021b; Meng et al., 2021)

(Ma et al., 2021a). Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other.

The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document.

They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness.

Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc. Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021).

For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus.

A term from it is treated as the answer and replaced with a special token.

Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents.

Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.

Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant.

For example, Chang et al. (2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic.

A passage from another page containing hyperlinks to p is treated as a positive document.

Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question.

A passage from its hyperlinked document that contains the same entity word is treated as a positive sample.

Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.","Question:

What are the popular heuristics for constructing positive pairs in self contrastive learning, and how do they function?

Answer:

There are five popular heuristics to construct positive pairs in self contrastive learning: perturbation-based, summary-based, proximity-based, cooccurrence-based, and hyperlink-based.

Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair.

The intuition is that perturbed text should still be relevant to the original text.

Typical choices of perturbations include word deletion, substitution, and permutation (Zhu et al., 2021b; Meng et al., 2021)

(Ma et al., 2021a). Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other.

The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document.

They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness.

Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc. Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021).

For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus.

A term from it is treated as the answer and replaced with a special token.

Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents.

Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.

Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant.

For example, Chang et al. (2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic.

A passage from another page containing hyperlinks to p is treated as a positive document.

Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question.

A passage from its hyperlinked document that contains the same entity word is treated as a positive sample.

Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,Self Contrastive Learning,10,"Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q ′ , d ′+/− ) from D, then uses them to supervise training of a NR model. The objective is:

where L is the ranking loss as in Eq 1. Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q ′ , d ′+ ). There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based. An overview is in Table 2.

Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair. The intuition is that perturbed text should still be relevant to the original text. Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).

Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other. The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document. They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness. Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.

Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021). For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus. A term from it is treated as the answer and replaced with a special token. Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents. Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.

Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant . For example, Chang et al.

(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic. A passage from another page containing hyperlinks to p is treated as a positive document. Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question. A passage from its hyperlinked document that contains the same entity word is treated as a positive sample. Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.  ","sent1: Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q ′ , d ′+/− ) from D, then uses them to supervise training of a NR model.
sent2: The objective is:where L is the ranking loss as in Eq 1.
sent3: Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q ′ , d ′+ ).
sent4: There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based.
sent5: An overview is in Table 2.Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair.
sent6: The intuition is that perturbed text should still be relevant to the original text.
sent7: Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).
sent8: Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other.
sent9: The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document.
sent10: They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness.
sent11: Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021).
sent12: For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus.
sent13: A term from it is treated as the answer and replaced with a special token.
sent14: Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents.
sent15: Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.
sent16: Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant .
sent17: For example, Chang et al.(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic.
sent18: A passage from another page containing hyperlinks to p is treated as a positive document.
sent19: Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question.
sent20: A passage from its hyperlinked document that contains the same entity word is treated as a positive sample.
sent21: Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.","1. What is self contrastive learning and how does it construct pseudo question-document pairs? sent1
    1.1. What is the objective of self contrastive learning? sent2
    1.2. What is the main difficulty in constructing positive pseudo pairs? sent3
    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21","1. What is self contrastive learning and how does it construct pseudo question-document pairs? sent1
    1.1. What is the objective of self contrastive learning? sent2
    1.2. What is the main difficulty in constructing positive pseudo pairs? sent3
    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21

    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21

        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11

        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15

        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21","    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21","Question: What are the popular heuristics for constructing positive pairs in self contrastive learning, and how do they function?

Supporting sentence: sent4, sent5, sent6, sent7, sent8, sent9, sent10, sent11, sent12, sent13, sent14, sent15, sent16, sent17, sent18, sent19, sent20, sent21","What are the popular heuristics for constructing positive pairs in self contrastive learning, and how do they function?","sent4, sent5, sent6, sent7, sent8, sent9, sent10, sent11, sent12, sent13, sent14, sent15, sent16, sent17, sent18, sent19, sent20, sent21",3,implement,"There are five popular heuristics to construct positive pairs in self contrastive learning: perturbation-based, summary-based, proximity-based, cooccurrence-based, and hyperlink-based.

Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair.

The intuition is that perturbed text should still be relevant to the original text.

Typical choices of perturbations include word deletion, substitution, and permutation (Zhu et al., 2021b; Meng et al., 2021)

(Ma et al., 2021a). Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other.

The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document.

They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness.

Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc. Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021).

For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus.

A term from it is treated as the answer and replaced with a special token.

Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents.

Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.

Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant.

For example, Chang et al. (2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic.

A passage from another page containing hyperlinks to p is treated as a positive document.

Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question.

A passage from its hyperlinked document that contains the same entity word is treated as a positive sample.

Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.","Question:

What are the popular heuristics for constructing positive pairs in self contrastive learning, and how do they function?

Answer:

There are five popular heuristics to construct positive pairs in self contrastive learning: perturbation-based, summary-based, proximity-based, cooccurrence-based, and hyperlink-based.

Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair.

The intuition is that perturbed text should still be relevant to the original text.

Typical choices of perturbations include word deletion, substitution, and permutation (Zhu et al., 2021b; Meng et al., 2021)

(Ma et al., 2021a). Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other.

The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document.

They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness.

Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc. Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021).

For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus.

A term from it is treated as the answer and replaced with a special token.

Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents.

Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.

Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant.

For example, Chang et al. (2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic.

A passage from another page containing hyperlinks to p is treated as a positive document.

Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question.

A passage from its hyperlinked document that contains the same entity word is treated as a positive sample.

Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,Self Contrastive Learning,10,"Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q ′ , d ′+/− ) from D, then uses them to supervise training of a NR model. The objective is:

where L is the ranking loss as in Eq 1. Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q ′ , d ′+ ). There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based. An overview is in Table 2.

Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair. The intuition is that perturbed text should still be relevant to the original text. Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).

Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other. The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document. They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness. Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.

Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021). For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus. A term from it is treated as the answer and replaced with a special token. Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents. Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.

Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant . For example, Chang et al.

(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic. A passage from another page containing hyperlinks to p is treated as a positive document. Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question. A passage from its hyperlinked document that contains the same entity word is treated as a positive sample. Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.  ","sent1: Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q ′ , d ′+/− ) from D, then uses them to supervise training of a NR model.
sent2: The objective is:where L is the ranking loss as in Eq 1.
sent3: Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q ′ , d ′+ ).
sent4: There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based.
sent5: An overview is in Table 2.Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair.
sent6: The intuition is that perturbed text should still be relevant to the original text.
sent7: Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).
sent8: Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other.
sent9: The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document.
sent10: They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness.
sent11: Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021).
sent12: For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus.
sent13: A term from it is treated as the answer and replaced with a special token.
sent14: Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents.
sent15: Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.
sent16: Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant .
sent17: For example, Chang et al.(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic.
sent18: A passage from another page containing hyperlinks to p is treated as a positive document.
sent19: Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question.
sent20: A passage from its hyperlinked document that contains the same entity word is treated as a positive sample.
sent21: Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.","1. What is self contrastive learning and how does it construct pseudo question-document pairs? sent1
    1.1. What is the objective of self contrastive learning? sent2
    1.2. What is the main difficulty in constructing positive pseudo pairs? sent3
    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21","1. What is self contrastive learning and how does it construct pseudo question-document pairs? sent1
    1.1. What is the objective of self contrastive learning? sent2
    1.2. What is the main difficulty in constructing positive pseudo pairs? sent3
    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21

    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21

        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11

        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15

        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21","        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11","Question: What are the various methods used in proximity-based heuristics for self contrastive learning, and how do they function?

Supporting sentence: sent8, sent9, sent10, sent11","What are the various methods used in proximity-based heuristics for self contrastive learning, and how do they function?","sent8, sent9, sent10, sent11",3,implement,"Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other.

The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document.

They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness.

Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.","Question:

What are the various methods used in proximity-based heuristics for self contrastive learning, and how do they function?

Answer:

Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other.

The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document.

They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness.

Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,Self Contrastive Learning,10,"Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q ′ , d ′+/− ) from D, then uses them to supervise training of a NR model. The objective is:

where L is the ranking loss as in Eq 1. Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q ′ , d ′+ ). There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based. An overview is in Table 2.

Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair. The intuition is that perturbed text should still be relevant to the original text. Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).

Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other. The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document. They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness. Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.

Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021). For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus. A term from it is treated as the answer and replaced with a special token. Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents. Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.

Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant . For example, Chang et al.

(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic. A passage from another page containing hyperlinks to p is treated as a positive document. Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question. A passage from its hyperlinked document that contains the same entity word is treated as a positive sample. Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.  ","sent1: Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q ′ , d ′+/− ) from D, then uses them to supervise training of a NR model.
sent2: The objective is:where L is the ranking loss as in Eq 1.
sent3: Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q ′ , d ′+ ).
sent4: There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based.
sent5: An overview is in Table 2.Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair.
sent6: The intuition is that perturbed text should still be relevant to the original text.
sent7: Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).
sent8: Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other.
sent9: The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document.
sent10: They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness.
sent11: Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021).
sent12: For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus.
sent13: A term from it is treated as the answer and replaced with a special token.
sent14: Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents.
sent15: Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.
sent16: Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant .
sent17: For example, Chang et al.(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic.
sent18: A passage from another page containing hyperlinks to p is treated as a positive document.
sent19: Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question.
sent20: A passage from its hyperlinked document that contains the same entity word is treated as a positive sample.
sent21: Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.","1. What is self contrastive learning and how does it construct pseudo question-document pairs? sent1
    1.1. What is the objective of self contrastive learning? sent2
    1.2. What is the main difficulty in constructing positive pseudo pairs? sent3
    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21","1. What is self contrastive learning and how does it construct pseudo question-document pairs? sent1
    1.1. What is the objective of self contrastive learning? sent2
    1.2. What is the main difficulty in constructing positive pseudo pairs? sent3
    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21

    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21

        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11

        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15

        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21","        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15","Question: How do cooccurrence-based heuristics utilize spans to construct positive samples in self contrastive learning?

Supporting sentence: sent11, sent12, sent13, sent14, sent15",How do cooccurrence-based heuristics utilize spans to construct positive samples in self contrastive learning?,"sent11, sent12, sent13, sent14, sent15",2,implement,"Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021).

For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus.

A term from it is treated as the answer and replaced with a special token.

Passages retrieved with BM25 which also contain the answer term are treated as pseudo positive documents.

Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.","Question:

How do cooccurrence-based heuristics utilize spans to construct positive samples in self contrastive learning?

Answer:

Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021).

For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus.

A term from it is treated as the answer and replaced with a special token.

Passages retrieved with BM25 which also contain the answer term are treated as pseudo positive documents.

Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document."
258378266,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,https://www.semanticscholar.org/paper/c22621ebbdd9c5d73b2eeb2b57dbc9f3547b780e,Self Contrastive Learning,10,"Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q ′ , d ′+/− ) from D, then uses them to supervise training of a NR model. The objective is:

where L is the ranking loss as in Eq 1. Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q ′ , d ′+ ). There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based. An overview is in Table 2.

Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair. The intuition is that perturbed text should still be relevant to the original text. Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).

Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other. The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document. They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness. Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.

Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021). For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus. A term from it is treated as the answer and replaced with a special token. Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents. Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.

Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant . For example, Chang et al.

(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic. A passage from another page containing hyperlinks to p is treated as a positive document. Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question. A passage from its hyperlinked document that contains the same entity word is treated as a positive sample. Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.  ","sent1: Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q ′ , d ′+/− ) from D, then uses them to supervise training of a NR model.
sent2: The objective is:where L is the ranking loss as in Eq 1.
sent3: Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q ′ , d ′+ ).
sent4: There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based.
sent5: An overview is in Table 2.Perturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair.
sent6: The intuition is that perturbed text should still be relevant to the original text.
sent7: Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).
sent8: Proximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other.
sent9: The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document.
sent10: They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness.
sent11: Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.Cooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021).
sent12: For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus.
sent13: A term from it is treated as the answer and replaced with a special token.
sent14: Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents.
sent15: Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.
sent16: Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant .
sent17: For example, Chang et al.(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic.
sent18: A passage from another page containing hyperlinks to p is treated as a positive document.
sent19: Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question.
sent20: A passage from its hyperlinked document that contains the same entity word is treated as a positive sample.
sent21: Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.","1. What is self contrastive learning and how does it construct pseudo question-document pairs? sent1
    1.1. What is the objective of self contrastive learning? sent2
    1.2. What is the main difficulty in constructing positive pseudo pairs? sent3
    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21","1. What is self contrastive learning and how does it construct pseudo question-document pairs? sent1
    1.1. What is the objective of self contrastive learning? sent2
    1.2. What is the main difficulty in constructing positive pseudo pairs? sent3
    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21

    1.3. What are the popular heuristics for constructing positive pairs? sent4
        1.3.1. What is perturbation-based heuristic and its intuition? sent5, sent6
            1.3.1.1. What are typical choices of perturbations in perturbation-based heuristics? sent7
        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11
        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15
        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21

        1.3.2. How do proximity-based heuristics work and what is their intuition? sent8
            1.3.2.1. What is the inverse-cloze task in proximity-based heuristics? sent9
            1.3.2.2. How can proximity-based heuristics be combined with noise injection methods? sent10
            1.3.2.3. What are other methods used in proximity-based heuristics? sent11

        1.3.3. How do cooccurrence-based heuristics construct positive samples? sent12
            1.3.3.1. How does Glass et al. (2020) construct pseudo questions and positive documents? sent13, sent14
            1.3.3.2. How does Ram et al. (2022) use spans for constructing positive documents? sent15

        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21","        1.3.4. How do hyperlink-based heuristics leverage hyperlink information? sent16
            1.3.4.1. How does Chang et al. (2020) use hyperlink-based heuristics to construct positive documents? sent17, sent18
            1.3.4.2. How does Yue et al. (2022a) form pseudo questions using hyperlink-based heuristics? sent19, sent20
            1.3.4.3. What are the two typologies used by Zhou et al. (2022) in hyperlink-based heuristics? sent21","Question: How do hyperlink-based heuristics utilize hyperlink information to construct positive samples in self contrastive learning?

Supporting sentence: sent16, sent17, sent18, sent19, sent20, sent21",How do hyperlink-based heuristics utilize hyperlink information to construct positive samples in self contrastive learning?,"sent16, sent17, sent18, sent19, sent20, sent21",2,implement,"Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant.

For example, Chang et al. (2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic.

A passage from another page containing hyperlinks to p is treated as a positive document.

Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question.

A passage from its hyperlinked document that contains the same entity word is treated as a positive sample.

Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document.","Question:

How do hyperlink-based heuristics utilize hyperlink information to construct positive samples in self contrastive learning?

Answer:

Hyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant.

For example, Chang et al. (2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic.

A passage from another page containing hyperlinks to p is treated as a positive document.

Yue et al. (2022a) replace an entity word with a question phrase like ""what/when"" to form a pseudo question.

A passage from its hyperlinked document that contains the same entity word is treated as a positive sample.

Zhou et al. (2022) build positive samples with two typologies: ""dual-link"" where two passages have hyperlinks pointed to each other, and ""co-mention"" where two passages both have a hyperlink to the same third-party document."
259108815,Mapping Brains with Language Models: A Survey,https://www.semanticscholar.org/paper/a92c82231c0ea7027ab20b25fe5f82565047aad7,How to predict linguistic stimuli?,10,"Decoding models work in the other direction and aim to predict linguistic features of the stimuli from recordings of brain response. Pereira et al. (2018) introduce a decoder that predicts stimuli representation of semantic features given fMRI data.They introduce a novel dataset of neural responses aligned with annotation of concrete and abstract semantic categories (such as pleasure, ignorance, cooking etc.). They evaluate static word embeddings by applying ridge regression to predict per-word fMRI vectors. A separate regression model is trained per dimension, allowing for dimension-wise regularization. The model is evaluated in terms of pairwise matching accuracy, but also in terms of percentile rank, adapted to the decoding scenario.

Gauthier and Levy (2019) also train linear regression models which map from the response measurements in Pereira et al. (2018), but to representations of the same sentences produced by the BERT language model finetuned on different natural language understanding tasks. The regression models are evaluated using two metrics: mean squared error and average percentile rank. Their results show that fine-tuning with different NLU objectives leads to worse alignment and that, somewhat surprisingly, the only objective which does lead to better alignment is a scrambled language modeling task where the model is trained to predict scrambled sentences.

Minnema and Herbelot (2019) re-examine the work of Pereira et al. (2018) using various metrics (pairwise matching accuracy, percentile rank, cosine distance, R 2 , RSA), comparing decoder models (ridge regression, perceptron, and convolutional neural networks). 3 They show that positive results are only obtained using pairwise matching accuracy. Abdou et al. (2021) investigate whether aligning language models with brain recordings can be improved by biasing their attention with annotations from syntactic or semantic formalisms. They fine-tune the BERT models using several syntactosemantic formalisms and evaluate their alignment with brain activity measurements from the Wehbe et al. (2014a) and Pereira et al. (2018) datasets. Their results -obtained using Pearson correlation as performance metric -are positive for two in three formalisms. Zou et al. (2022) propose a new evaluation method for decoding, a so-called cross-modal cloze task. They generate the data for the task from the neural response measures in Mitchell et al. (2008) and Wehbe et al. (2014a). The task itself amounts to a cloze task in which the context is prefixed by the fMRI image of the masked word. They evaluate models using precision@k. Note how this task is considerably easier than linearly mapping from language model representations into fMRI images, and precision@k results therefore cannot be compared to those obtained in other settings. Their best precision@1 scores are around 0.3, but only marginally (0.03) better than a unimodal LM.

Finally, Pascual et al. (2022) try a more realistic setup by predicting language from fMRI scans of subjects not included in the training. They use the (Pereira et al., 2018) dataset and evaluate the regression models based on pairwise accuracy and precision@k (or top-k accuracy). They propose evaluating with direct classification as a more demanding setup to evaluate and understand current brain decoding models.

Intermediate summary Decoding studies also differ in many respects. Several metrics are used: pairwise-matching accuracy, Pearson correlation, percentile rank, cosine distance, precision@k, and representational similarity analysis; and several datasets are used. Gauthier and Ivanova (2018) criticize the evaluation techniques of decoding studies and suggest adopting task and mechanism explicit models. It is of particular interest to our study that both Minnema and Herbelot (2019) only report positive results for pairwise matching accuracy compared to other metrics. This suggests pairwise matching accuracy is a less conservative metric (and maybe less reliable).","sent1: Decoding models work in the other direction and aim to predict linguistic features of the stimuli from recordings of brain response.
sent2: Pereira et al. (2018) introduce a decoder that predicts stimuli representation of semantic features given fMRI data.
sent3: They introduce a novel dataset of neural responses aligned with annotation of concrete and abstract semantic categories (such as pleasure, ignorance, cooking etc.).
sent4: They evaluate static word embeddings by applying ridge regression to predict per-word fMRI vectors.
sent5: A separate regression model is trained per dimension, allowing for dimension-wise regularization.
sent6: The model is evaluated in terms of pairwise matching accuracy, but also in terms of percentile rank, adapted to the decoding scenario.
sent7: Gauthier and Levy (2019) also train linear regression models which map from the response measurements in Pereira et al. (2018), but to representations of the same sentences produced by the BERT language model finetuned on different natural language understanding tasks.
sent8: The regression models are evaluated using two metrics: mean squared error and average percentile rank.
sent9: Their results show that fine-tuning with different NLU objectives leads to worse alignment and that, somewhat surprisingly, the only objective which does lead to better alignment is a scrambled language modeling task where the model is trained to predict scrambled sentences.
sent10: Minnema and Herbelot (2019) re-examine the work of Pereira et al. (2018) using various metrics (pairwise matching accuracy, percentile rank, cosine distance, R 2 , RSA), comparing decoder models (ridge regression, perceptron, and convolutional neural networks). 3
sent11: They show that positive results are only obtained using pairwise matching accuracy.
sent12: Abdou et al. (2021) investigate whether aligning language models with brain recordings can be improved by biasing their attention with annotations from syntactic or semantic formalisms.
sent13: They fine-tune the BERT models using several syntactosemantic formalisms and evaluate their alignment with brain activity measurements from the Wehbe et al. (2014a) and Pereira et al. (2018) datasets.
sent14: Their results -obtained using Pearson correlation as performance metric -are positive for two in three formalisms.
sent15: Zou et al. (2022) propose a new evaluation method for decoding, a so-called cross-modal cloze task.
sent16: They generate the data for the task from the neural response measures in Mitchell et al. (2008) and Wehbe et al. (2014a).
sent17: The task itself amounts to a cloze task in which the context is prefixed by the fMRI image of the masked word.
sent18: They evaluate models using precision@k.
sent19: Note how this task is considerably easier than linearly mapping from language model representations into fMRI images, and precision@k results therefore cannot be compared to those obtained in other settings.
sent20: Their best precision@1 scores are around 0.3, but only marginally (0.03) better than a unimodal LM.
sent21: Finally, Pascual et al. (2022) try a more realistic setup by predicting language from fMRI scans of subjects not included in the training.
sent22: They use the (Pereira et al., 2018) dataset and evaluate the regression models based on pairwise accuracy and precision@k (or top-k accuracy).
sent23: They propose evaluating with direct classification as a more demanding setup to evaluate and understand current brain decoding models.
sent24: Intermediate summary Decoding studies also differ in many respects.
sent25: Several metrics are used: pairwise-matching accuracy, Pearson correlation, percentile rank, cosine distance, precision@k, and representational similarity analysis; and several datasets are used.
sent26: Gauthier and Ivanova (2018) criticize the evaluation techniques of decoding studies and suggest adopting task and mechanism explicit models.
sent27: It is of particular interest to our study that both Minnema and Herbelot (2019) only report positive results for pairwise matching accuracy compared to other metrics.
sent28: This suggests pairwise matching accuracy is a less conservative metric (and maybe less reliable).","1. What is the aim of decoding models in the context of predicting linguistic stimuli? sent1
    1.1. What did Pereira et al. (2018) introduce in their study? sent2
        1.1.1. What novel dataset did they introduce? sent3
        1.1.2. How did they evaluate static word embeddings? sent4
            1.1.2.1. What is the purpose of training a separate regression model per dimension? sent5
            1.1.2.2. How is the model evaluated? sent6
    1.2. How do Gauthier and Levy (2019) approach the mapping from brain response measurements? sent7
        1.2.1. What metrics are used to evaluate their regression models? sent8
        1.2.2. What were the findings regarding fine-tuning with different NLU objectives? sent9
    1.3. How do Minnema and Herbelot (2019) re-examine Pereira et al. (2018)'s work? sent10
        1.3.1. What do they show about the results obtained using different metrics? sent11
    1.4. What do Abdou et al. (2021) investigate in their study? sent12
        1.4.1. How do they fine-tune BERT models and evaluate their alignment? sent13
        1.4.2. What are the results of their study? sent14
    1.5. What new evaluation method do Zou et al. (2022) propose? sent15
        1.5.1. How is the data for the cross-modal cloze task generated? sent16
        1.5.2. What does the task involve and how are models evaluated? sent17, sent18
        1.5.3. How does this task compare to other settings? sent19
        1.5.4. What are the precision@1 scores achieved in their study? sent20
    1.6. What setup do Pascual et al. (2022) try in their study? sent21
        1.6.1. What dataset do they use and how do they evaluate the regression models? sent22
        1.6.2. What do they propose as a more demanding setup for evaluation? sent23
2. What are some differences in decoding studies? sent24
    2.1. What metrics and datasets are used in decoding studies? sent25
    2.2. What criticism do Gauthier and Ivanova (2018) have regarding evaluation techniques? sent26
    2.3. What is noted about the results reported by Minnema and Herbelot (2019)? sent27
        2.3.1. What does this suggest about pairwise matching accuracy? sent28","1. What is the aim of decoding models in the context of predicting linguistic stimuli? sent1
    1.1. What did Pereira et al. (2018) introduce in their study? sent2
        1.1.1. What novel dataset did they introduce? sent3
        1.1.2. How did they evaluate static word embeddings? sent4
            1.1.2.1. What is the purpose of training a separate regression model per dimension? sent5
            1.1.2.2. How is the model evaluated? sent6
    1.2. How do Gauthier and Levy (2019) approach the mapping from brain response measurements? sent7
        1.2.1. What metrics are used to evaluate their regression models? sent8
        1.2.2. What were the findings regarding fine-tuning with different NLU objectives? sent9
    1.3. How do Minnema and Herbelot (2019) re-examine Pereira et al. (2018)'s work? sent10
        1.3.1. What do they show about the results obtained using different metrics? sent11
    1.4. What do Abdou et al. (2021) investigate in their study? sent12
        1.4.1. How do they fine-tune BERT models and evaluate their alignment? sent13
        1.4.2. What are the results of their study? sent14
    1.5. What new evaluation method do Zou et al. (2022) propose? sent15
        1.5.1. How is the data for the cross-modal cloze task generated? sent16
        1.5.2. What does the task involve and how are models evaluated? sent17, sent18
        1.5.3. How does this task compare to other settings? sent19
        1.5.4. What are the precision@1 scores achieved in their study? sent20
    1.6. What setup do Pascual et al. (2022) try in their study? sent21
        1.6.1. What dataset do they use and how do they evaluate the regression models? sent22
        1.6.2. What do they propose as a more demanding setup for evaluation? sent23

    1.1. What did Pereira et al. (2018) introduce in their study? sent2
        1.1.1. What novel dataset did they introduce? sent3
        1.1.2. How did they evaluate static word embeddings? sent4
            1.1.2.1. What is the purpose of training a separate regression model per dimension? sent5
            1.1.2.2. How is the model evaluated? sent6

    1.5. What new evaluation method do Zou et al. (2022) propose? sent15
        1.5.1. How is the data for the cross-modal cloze task generated? sent16
        1.5.2. What does the task involve and how are models evaluated? sent17, sent18
        1.5.3. How does this task compare to other settings? sent19
        1.5.4. What are the precision@1 scores achieved in their study? sent20

2. What are some differences in decoding studies? sent24
    2.1. What metrics and datasets are used in decoding studies? sent25
    2.2. What criticism do Gauthier and Ivanova (2018) have regarding evaluation techniques? sent26
    2.3. What is noted about the results reported by Minnema and Herbelot (2019)? sent27
        2.3.1. What does this suggest about pairwise matching accuracy? sent28","    1.1. What did Pereira et al. (2018) introduce in their study? sent2
        1.1.1. What novel dataset did they introduce? sent3
        1.1.2. How did they evaluate static word embeddings? sent4
            1.1.2.1. What is the purpose of training a separate regression model per dimension? sent5
            1.1.2.2. How is the model evaluated? sent6","Question: How do Pereira et al. (2018) evaluate static word embeddings using fMRI data in their study?

Supporting sentence: sent4, sent5, sent6",How do Pereira et al. (2018) evaluate static word embeddings using fMRI data in their study?,"sent4, sent5, sent6",1,implement,"Pereira et al. (2018) introduce a decoder that predicts stimuli representation of semantic features given fMRI data.

They evaluate static word embeddings by applying ridge regression to predict per-word fMRI vectors.

A separate regression model is trained per dimension, allowing for dimension-wise regularization.

The model is evaluated in terms of pairwise matching accuracy, but also in terms of percentile rank, adapted to the decoding scenario.","Question:

How do Pereira et al. (2018) evaluate static word embeddings using fMRI data in their study?

Answer:

Pereira et al. (2018) introduce a decoder that predicts stimuli representation of semantic features given fMRI data.

They evaluate static word embeddings by applying ridge regression to predict per-word fMRI vectors.

A separate regression model is trained per dimension, allowing for dimension-wise regularization.

The model is evaluated in terms of pairwise matching accuracy, but also in terms of percentile rank, adapted to the decoding scenario."
259108815,Mapping Brains with Language Models: A Survey,https://www.semanticscholar.org/paper/a92c82231c0ea7027ab20b25fe5f82565047aad7,How to predict linguistic stimuli?,10,"Decoding models work in the other direction and aim to predict linguistic features of the stimuli from recordings of brain response. Pereira et al. (2018) introduce a decoder that predicts stimuli representation of semantic features given fMRI data.They introduce a novel dataset of neural responses aligned with annotation of concrete and abstract semantic categories (such as pleasure, ignorance, cooking etc.). They evaluate static word embeddings by applying ridge regression to predict per-word fMRI vectors. A separate regression model is trained per dimension, allowing for dimension-wise regularization. The model is evaluated in terms of pairwise matching accuracy, but also in terms of percentile rank, adapted to the decoding scenario.

Gauthier and Levy (2019) also train linear regression models which map from the response measurements in Pereira et al. (2018), but to representations of the same sentences produced by the BERT language model finetuned on different natural language understanding tasks. The regression models are evaluated using two metrics: mean squared error and average percentile rank. Their results show that fine-tuning with different NLU objectives leads to worse alignment and that, somewhat surprisingly, the only objective which does lead to better alignment is a scrambled language modeling task where the model is trained to predict scrambled sentences.

Minnema and Herbelot (2019) re-examine the work of Pereira et al. (2018) using various metrics (pairwise matching accuracy, percentile rank, cosine distance, R 2 , RSA), comparing decoder models (ridge regression, perceptron, and convolutional neural networks). 3 They show that positive results are only obtained using pairwise matching accuracy. Abdou et al. (2021) investigate whether aligning language models with brain recordings can be improved by biasing their attention with annotations from syntactic or semantic formalisms. They fine-tune the BERT models using several syntactosemantic formalisms and evaluate their alignment with brain activity measurements from the Wehbe et al. (2014a) and Pereira et al. (2018) datasets. Their results -obtained using Pearson correlation as performance metric -are positive for two in three formalisms. Zou et al. (2022) propose a new evaluation method for decoding, a so-called cross-modal cloze task. They generate the data for the task from the neural response measures in Mitchell et al. (2008) and Wehbe et al. (2014a). The task itself amounts to a cloze task in which the context is prefixed by the fMRI image of the masked word. They evaluate models using precision@k. Note how this task is considerably easier than linearly mapping from language model representations into fMRI images, and precision@k results therefore cannot be compared to those obtained in other settings. Their best precision@1 scores are around 0.3, but only marginally (0.03) better than a unimodal LM.

Finally, Pascual et al. (2022) try a more realistic setup by predicting language from fMRI scans of subjects not included in the training. They use the (Pereira et al., 2018) dataset and evaluate the regression models based on pairwise accuracy and precision@k (or top-k accuracy). They propose evaluating with direct classification as a more demanding setup to evaluate and understand current brain decoding models.

Intermediate summary Decoding studies also differ in many respects. Several metrics are used: pairwise-matching accuracy, Pearson correlation, percentile rank, cosine distance, precision@k, and representational similarity analysis; and several datasets are used. Gauthier and Ivanova (2018) criticize the evaluation techniques of decoding studies and suggest adopting task and mechanism explicit models. It is of particular interest to our study that both Minnema and Herbelot (2019) only report positive results for pairwise matching accuracy compared to other metrics. This suggests pairwise matching accuracy is a less conservative metric (and maybe less reliable).","sent1: Decoding models work in the other direction and aim to predict linguistic features of the stimuli from recordings of brain response.
sent2: Pereira et al. (2018) introduce a decoder that predicts stimuli representation of semantic features given fMRI data.
sent3: They introduce a novel dataset of neural responses aligned with annotation of concrete and abstract semantic categories (such as pleasure, ignorance, cooking etc.).
sent4: They evaluate static word embeddings by applying ridge regression to predict per-word fMRI vectors.
sent5: A separate regression model is trained per dimension, allowing for dimension-wise regularization.
sent6: The model is evaluated in terms of pairwise matching accuracy, but also in terms of percentile rank, adapted to the decoding scenario.
sent7: Gauthier and Levy (2019) also train linear regression models which map from the response measurements in Pereira et al. (2018), but to representations of the same sentences produced by the BERT language model finetuned on different natural language understanding tasks.
sent8: The regression models are evaluated using two metrics: mean squared error and average percentile rank.
sent9: Their results show that fine-tuning with different NLU objectives leads to worse alignment and that, somewhat surprisingly, the only objective which does lead to better alignment is a scrambled language modeling task where the model is trained to predict scrambled sentences.
sent10: Minnema and Herbelot (2019) re-examine the work of Pereira et al. (2018) using various metrics (pairwise matching accuracy, percentile rank, cosine distance, R 2 , RSA), comparing decoder models (ridge regression, perceptron, and convolutional neural networks). 3
sent11: They show that positive results are only obtained using pairwise matching accuracy.
sent12: Abdou et al. (2021) investigate whether aligning language models with brain recordings can be improved by biasing their attention with annotations from syntactic or semantic formalisms.
sent13: They fine-tune the BERT models using several syntactosemantic formalisms and evaluate their alignment with brain activity measurements from the Wehbe et al. (2014a) and Pereira et al. (2018) datasets.
sent14: Their results -obtained using Pearson correlation as performance metric -are positive for two in three formalisms.
sent15: Zou et al. (2022) propose a new evaluation method for decoding, a so-called cross-modal cloze task.
sent16: They generate the data for the task from the neural response measures in Mitchell et al. (2008) and Wehbe et al. (2014a).
sent17: The task itself amounts to a cloze task in which the context is prefixed by the fMRI image of the masked word.
sent18: They evaluate models using precision@k.
sent19: Note how this task is considerably easier than linearly mapping from language model representations into fMRI images, and precision@k results therefore cannot be compared to those obtained in other settings.
sent20: Their best precision@1 scores are around 0.3, but only marginally (0.03) better than a unimodal LM.
sent21: Finally, Pascual et al. (2022) try a more realistic setup by predicting language from fMRI scans of subjects not included in the training.
sent22: They use the (Pereira et al., 2018) dataset and evaluate the regression models based on pairwise accuracy and precision@k (or top-k accuracy).
sent23: They propose evaluating with direct classification as a more demanding setup to evaluate and understand current brain decoding models.
sent24: Intermediate summary Decoding studies also differ in many respects.
sent25: Several metrics are used: pairwise-matching accuracy, Pearson correlation, percentile rank, cosine distance, precision@k, and representational similarity analysis; and several datasets are used.
sent26: Gauthier and Ivanova (2018) criticize the evaluation techniques of decoding studies and suggest adopting task and mechanism explicit models.
sent27: It is of particular interest to our study that both Minnema and Herbelot (2019) only report positive results for pairwise matching accuracy compared to other metrics.
sent28: This suggests pairwise matching accuracy is a less conservative metric (and maybe less reliable).","1. What is the aim of decoding models in the context of predicting linguistic stimuli? sent1
    1.1. What did Pereira et al. (2018) introduce in their study? sent2
        1.1.1. What novel dataset did they introduce? sent3
        1.1.2. How did they evaluate static word embeddings? sent4
            1.1.2.1. What is the purpose of training a separate regression model per dimension? sent5
            1.1.2.2. How is the model evaluated? sent6
    1.2. How do Gauthier and Levy (2019) approach the mapping from brain response measurements? sent7
        1.2.1. What metrics are used to evaluate their regression models? sent8
        1.2.2. What were the findings regarding fine-tuning with different NLU objectives? sent9
    1.3. How do Minnema and Herbelot (2019) re-examine Pereira et al. (2018)'s work? sent10
        1.3.1. What do they show about the results obtained using different metrics? sent11
    1.4. What do Abdou et al. (2021) investigate in their study? sent12
        1.4.1. How do they fine-tune BERT models and evaluate their alignment? sent13
        1.4.2. What are the results of their study? sent14
    1.5. What new evaluation method do Zou et al. (2022) propose? sent15
        1.5.1. How is the data for the cross-modal cloze task generated? sent16
        1.5.2. What does the task involve and how are models evaluated? sent17, sent18
        1.5.3. How does this task compare to other settings? sent19
        1.5.4. What are the precision@1 scores achieved in their study? sent20
    1.6. What setup do Pascual et al. (2022) try in their study? sent21
        1.6.1. What dataset do they use and how do they evaluate the regression models? sent22
        1.6.2. What do they propose as a more demanding setup for evaluation? sent23
2. What are some differences in decoding studies? sent24
    2.1. What metrics and datasets are used in decoding studies? sent25
    2.2. What criticism do Gauthier and Ivanova (2018) have regarding evaluation techniques? sent26
    2.3. What is noted about the results reported by Minnema and Herbelot (2019)? sent27
        2.3.1. What does this suggest about pairwise matching accuracy? sent28","1. What is the aim of decoding models in the context of predicting linguistic stimuli? sent1
    1.1. What did Pereira et al. (2018) introduce in their study? sent2
        1.1.1. What novel dataset did they introduce? sent3
        1.1.2. How did they evaluate static word embeddings? sent4
            1.1.2.1. What is the purpose of training a separate regression model per dimension? sent5
            1.1.2.2. How is the model evaluated? sent6
    1.2. How do Gauthier and Levy (2019) approach the mapping from brain response measurements? sent7
        1.2.1. What metrics are used to evaluate their regression models? sent8
        1.2.2. What were the findings regarding fine-tuning with different NLU objectives? sent9
    1.3. How do Minnema and Herbelot (2019) re-examine Pereira et al. (2018)'s work? sent10
        1.3.1. What do they show about the results obtained using different metrics? sent11
    1.4. What do Abdou et al. (2021) investigate in their study? sent12
        1.4.1. How do they fine-tune BERT models and evaluate their alignment? sent13
        1.4.2. What are the results of their study? sent14
    1.5. What new evaluation method do Zou et al. (2022) propose? sent15
        1.5.1. How is the data for the cross-modal cloze task generated? sent16
        1.5.2. What does the task involve and how are models evaluated? sent17, sent18
        1.5.3. How does this task compare to other settings? sent19
        1.5.4. What are the precision@1 scores achieved in their study? sent20
    1.6. What setup do Pascual et al. (2022) try in their study? sent21
        1.6.1. What dataset do they use and how do they evaluate the regression models? sent22
        1.6.2. What do they propose as a more demanding setup for evaluation? sent23

    1.1. What did Pereira et al. (2018) introduce in their study? sent2
        1.1.1. What novel dataset did they introduce? sent3
        1.1.2. How did they evaluate static word embeddings? sent4
            1.1.2.1. What is the purpose of training a separate regression model per dimension? sent5
            1.1.2.2. How is the model evaluated? sent6

    1.5. What new evaluation method do Zou et al. (2022) propose? sent15
        1.5.1. How is the data for the cross-modal cloze task generated? sent16
        1.5.2. What does the task involve and how are models evaluated? sent17, sent18
        1.5.3. How does this task compare to other settings? sent19
        1.5.4. What are the precision@1 scores achieved in their study? sent20

2. What are some differences in decoding studies? sent24
    2.1. What metrics and datasets are used in decoding studies? sent25
    2.2. What criticism do Gauthier and Ivanova (2018) have regarding evaluation techniques? sent26
    2.3. What is noted about the results reported by Minnema and Herbelot (2019)? sent27
        2.3.1. What does this suggest about pairwise matching accuracy? sent28","    1.5. What new evaluation method do Zou et al. (2022) propose? sent15
        1.5.1. How is the data for the cross-modal cloze task generated? sent16
        1.5.2. What does the task involve and how are models evaluated? sent17, sent18
        1.5.3. How does this task compare to other settings? sent19
        1.5.4. What are the precision@1 scores achieved in their study? sent20","Question: How does the cross-modal cloze task differ from traditional decoding tasks in terms of evaluation and difficulty?

Supporting sentence: sent15, sent16, sent17, sent18, sent19",How does the cross-modal cloze task differ from traditional decoding tasks in terms of evaluation and difficulty?,"sent15, sent16, sent17, sent18, sent19",2,implement,"Zou et al. (2022) propose a new evaluation method for decoding, a so-called cross-modal cloze task.

The task itself amounts to a cloze task in which the context is prefixed by the fMRI image of the masked word.

They evaluate models using precision@k.

Note how this task is considerably easier than linearly mapping from language model representations into fMRI images, and precision@k results therefore cannot be compared to those obtained in other settings.","Question:

How does the cross-modal cloze task differ from traditional decoding tasks in terms of evaluation and difficulty?

Answer:

Zou et al. (2022) propose a new evaluation method for decoding, a so-called cross-modal cloze task.

The task itself amounts to a cloze task in which the context is prefixed by the fMRI image of the masked word.

They evaluate models using precision@k.

Note how this task is considerably easier than linearly mapping from language model representations into fMRI images, and precision@k results therefore cannot be compared to those obtained in other settings."
258426970,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef,Decoding with Feedback Models,5,"As mentioned, feedback models have the advantage that they can be queried cheaply for feedback once trained. Perhaps for this reason, most approaches that leverage feedback models by sampling a large number of candidate generations, and reranking them according to the feedback model:

whereĥ ϕ is a trained (numerical) feedback model and C is a set of S candidate generations given specific regularization terms, such as the KL terms in PPO (Schulman et al., 2017). by the model (for example, by sampling from its distribution multiple times).

In machine translation, Fernandes et al. (2022) andFreitag et al. (2022a) build upon recent advances in automatic quality estimation and evaluation via feedback model training to improve generation. Their framework comprises a candidate generation stage followed by a ranking stage, in which the candidates are scored using quality metrics trained to regress on human assessments (reward models) (Rei et al., 2020a,b) via N -best list reranking or minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2002). The highest-scoring candidate is then chosen as the final translation. Li et al. (2022) collected a dataset of both numerical and natural language feedback for responses from a QA system, and finetuned a pretrained model to predict both kinds of feedback, using the predicted scores from this feedback model to re-rank the predictions from the model. Gao et al. (2022) also used this approach to study the scaling properties of feedback models and the problem of ""overoptimization"" (see below).

Additionally, there are several works combining MT and APE systems at decoding time, in which the output of an MT system is further improved by an APE system (Bhattacharyya et al., 2022).

Feedback Model Overoptimization One problem that arises when optimizing a system with a feedback model is that this model is only an imperfect proxy for the ground truth human feedback, therefore, ""overoptimizing"" for them can lead to systems that receive good feedback from the model, but not humans. This problem is known as the overoptimization problem, and is the main reason for the regularization term in Equation 11 Gao et al. (2022) studies the overoptimization problem in preference models, by both optimizing against it with reinforcement learning (training) and reranking outputs with it (decoding). They found that both using preference models during training or decoding led to similar levels of overoptimization, and that the scale of the generation model helps little with this problem.","sent1: As mentioned, feedback models have the advantage that they can be queried cheaply for feedback once trained.
sent2: Perhaps for this reason, most approaches that leverage feedback models by sampling a large number of candidate generations, and reranking them according to the feedback model:whereĥ ϕ is a trained (numerical) feedback model and C is a set of S candidate generations given specific regularization terms, such as the KL terms in PPO (Schulman et al., 2017).
sent3: by the model (for example, by sampling from its distribution multiple times).
sent4: In machine translation, Fernandes et al. (2022) andFreitag et al. (2022a) build upon recent advances in automatic quality estimation and evaluation via feedback model training to improve generation.
sent5: Their framework comprises a candidate generation stage followed by a ranking stage, in which the candidates are scored using quality metrics trained to regress on human assessments (reward models) (Rei et al., 2020a,b) via N -best list reranking or minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2002).
sent6: The highest-scoring candidate is then chosen as the final translation.
sent7: Li et al. (2022) collected a dataset of both numerical and natural language feedback for responses from a QA system, and finetuned a pretrained model to predict both kinds of feedback, using the predicted scores from this feedback model to re-rank the predictions from the model.
sent8: Gao et al. (2022) also used this approach to study the scaling properties of feedback models and the problem of ""overoptimization"" (see below).
sent9: Additionally, there are several works combining MT and APE systems at decoding time, in which the output of an MT system is further improved by an APE system (Bhattacharyya et al., 2022).
sent10: Feedback Model Overoptimization One problem that arises when optimizing a system with a feedback model is that this model is only an imperfect proxy for the ground truth human feedback, therefore, ""overoptimizing"" for them can lead to systems that receive good feedback from the model, but not humans.
sent11: This problem is known as the overoptimization problem, and is the main reason for the regularization term in Equation 11 Gao et al. (2022) studies the overoptimization problem in preference models, by both optimizing against it with reinforcement learning (training) and reranking outputs with it (decoding).
sent12: They found that both using preference models during training or decoding led to similar levels of overoptimization, and that the scale of the generation model helps little with this problem.","1. What advantage do feedback models have once they are trained? sent1
2. How do most approaches leverage feedback models in natural language generation? sent2
    2.1. What is the process used in machine translation to improve generation using feedback models? sent4
        2.1.1. What does the framework for improving machine translation with feedback models involve? sent5
        2.1.2. How is the final translation chosen in this framework? sent6
    2.2. How did Li et al. (2022) utilize feedback for a QA system? sent7
    2.3. What did Gao et al. (2022) study regarding feedback models? sent8
    2.4. How are MT and APE systems combined at decoding time? sent9
3. What is the overoptimization problem in feedback models? sent10
    3.1. How is the overoptimization problem addressed in preference models? sent11
    3.2. What did Gao et al. (2022) find about the use of preference models during training or decoding? sent12","2. How do most approaches leverage feedback models in natural language generation? sent2
    2.1. What is the process used in machine translation to improve generation using feedback models? sent4
        2.1.1. What does the framework for improving machine translation with feedback models involve? sent5
        2.1.2. How is the final translation chosen in this framework? sent6
    2.2. How did Li et al. (2022) utilize feedback for a QA system? sent7
    2.3. What did Gao et al. (2022) study regarding feedback models? sent8
    2.4. How are MT and APE systems combined at decoding time? sent9","2. How do most approaches leverage feedback models in natural language generation? sent2
    2.1. What is the process used in machine translation to improve generation using feedback models? sent4
        2.1.1. What does the framework for improving machine translation with feedback models involve? sent5
        2.1.2. How is the final translation chosen in this framework? sent6
    2.2. How did Li et al. (2022) utilize feedback for a QA system? sent7
    2.3. What did Gao et al. (2022) study regarding feedback models? sent8
    2.4. How are MT and APE systems combined at decoding time? sent9","Question: How is the process of improving machine translation generation structured using feedback models?

Supporting sentence: sent4, sent5, sent6",How is the process of improving machine translation generation structured using feedback models?,"sent4, sent5, sent6",2,implement,"The process of improving machine translation generation using feedback models involves a candidate generation stage followed by a ranking stage.

In the candidate generation stage, a large number of candidate generations are sampled.

These candidates are then reranked according to a trained numerical feedback model, denoted as ĥ ϕ, which evaluates the quality of the candidates.

Quality metrics are trained to regress on human assessments, often referred to as reward models, and are used in the ranking stage.

The candidates are scored using these quality metrics via N-best list reranking or minimum Bayes risk (MBR) decoding.

The highest-scoring candidate is then chosen as the final translation.

This approach builds upon recent advances in automatic quality estimation and evaluation via feedback model training to improve generation.","Question:

How is the process of improving machine translation generation structured using feedback models?

Answer:

The process of improving machine translation generation using feedback models involves a candidate generation stage followed by a ranking stage.

In the candidate generation stage, a large number of candidate generations are sampled.

These candidates are then reranked according to a trained numerical feedback model, denoted as ĥ ϕ, which evaluates the quality of the candidates.

Quality metrics are trained to regress on human assessments, often referred to as reward models, and are used in the ranking stage.

The candidates are scored using these quality metrics via N-best list reranking or minimum Bayes risk (MBR) decoding.

The highest-scoring candidate is then chosen as the final translation.

This approach builds upon recent advances in automatic quality estimation and evaluation via feedback model training to improve generation."
258740687,A Survey on Zero Pronoun Translation,https://www.semanticscholar.org/paper/8fa265a8ca46c9c0ad35d2c5b519a86e2a092dfe,Overview,7,"There are three kinds of automatic metrics to evaluate performances of related models:

• Accuracy of ZP Recovery: this aims to measure model performance on detecting and predicting ZPs of sentences in one pro-drop language. For instance, the micro F1-score is used to evaluating Chinese ZPR systems Song et al. (2020). 11 • General Translation Quality: there are a number of automatic evaluation metrics for measuring general performance of MT systems (Snover  Table 4: Correlation between the manual evaluation and other automatic metrics, which are applied on different ZPT benchmarks, which are same as in Table 2. et al., 2006). BLEU (Papineni et al., 2002) is the most widely-used one, which measures the precision of n-grams of the MT output compared to the reference, weighted by a brevity penalty to punish overly short translations. ME-TEOR (Banerjee and Lavie, 2005) incorporates semantic information by calculating either exact match, stem match, or synonymy match. Furthermore, COMET (Rei et al., 2020) is a neural framework for training multilingual MT evaluation models which obtains new SOTA levels of correlation with human judgements. • Pronoun-Aware Translation Quality: Previous works usually evaluate ZPT using the BLEU metric (Wang et al., 2016a(Wang et al., , 2018aRi et al., 2021), however, general-purpose metrics cannot characterize the performance of ZP translation. As shown in Table 3, the missed or incorrect pronouns may not affect BLEU scores but severely harm true performances. To fix this gap, some works proposed pronoun-targeted evaluation metrics (Werlen and Popescu-Belis, 2017; Läubli et al., 2018).","sent1: There are three kinds of automatic metrics to evaluate performances of related models:• Accuracy of ZP Recovery: this aims to measure model performance on detecting and predicting ZPs of sentences in one pro-drop language.
sent2: For instance, the micro F1-score is used to evaluating Chinese ZPR systems Song et al. (2020).
sent3: 11 • General Translation Quality: there are a number of automatic evaluation metrics for measuring general performance of MT systems (Snover  Table 4: Correlation between the manual evaluation and other automatic metrics, which are applied on different ZPT benchmarks, which are same as in Table 2. et al., 2006).
sent4: BLEU (Papineni et al., 2002) is the most widely-used one, which measures the precision of n-grams of the MT output compared to the reference, weighted by a brevity penalty to punish overly short translations.
sent5: ME-TEOR (Banerjee and Lavie, 2005) incorporates semantic information by calculating either exact match, stem match, or synonymy match.
sent6: Furthermore, COMET (Rei et al., 2020) is a neural framework for training multilingual MT evaluation models which obtains new SOTA levels of correlation with human judgements.
sent7: • Pronoun-Aware Translation Quality: Previous works usually evaluate ZPT using the BLEU metric (Wang et al., 2016a(Wang et al., , 2018aRi et al., 2021), however, general-purpose metrics cannot characterize the performance of ZP translation.
sent8: As shown in Table 3, the missed or incorrect pronouns may not affect BLEU scores but severely harm true performances.
sent9: To fix this gap, some works proposed pronoun-targeted evaluation metrics (Werlen and Popescu-Belis, 2017; Läubli et al., 2018).","1. What are the three kinds of automatic metrics used to evaluate the performance of models in zero pronoun translation? sent1
    1.1. How is the accuracy of ZP recovery measured? sent2
    1.2. What are some automatic evaluation metrics for measuring general translation quality? sent3
        1.2.1. What is BLEU and how does it measure translation quality? sent4
        1.2.2. How does ME-TEOR incorporate semantic information in its evaluation? sent5
        1.2.3. What is COMET and what does it achieve in MT evaluation? sent6
    1.3. Why are general-purpose metrics insufficient for evaluating pronoun-aware translation quality? sent7
        1.3.1. How do missed or incorrect pronouns affect BLEU scores and true performance? sent8
        1.3.2. What has been proposed to address the gap in pronoun-aware translation evaluation? sent9","1. What are the three kinds of automatic metrics used to evaluate the performance of models in zero pronoun translation? sent1
    1.1. How is the accuracy of ZP recovery measured? sent2
    1.2. What are some automatic evaluation metrics for measuring general translation quality? sent3
        1.2.1. What is BLEU and how does it measure translation quality? sent4
        1.2.2. How does ME-TEOR incorporate semantic information in its evaluation? sent5
        1.2.3. What is COMET and what does it achieve in MT evaluation? sent6
    1.3. Why are general-purpose metrics insufficient for evaluating pronoun-aware translation quality? sent7
        1.3.1. How do missed or incorrect pronouns affect BLEU scores and true performance? sent8
        1.3.2. What has been proposed to address the gap in pronoun-aware translation evaluation? sent9

    1.2. What are some automatic evaluation metrics for measuring general translation quality? sent3
        1.2.1. What is BLEU and how does it measure translation quality? sent4
        1.2.2. How does ME-TEOR incorporate semantic information in its evaluation? sent5
        1.2.3. What is COMET and what does it achieve in MT evaluation? sent6","1. What are the three kinds of automatic metrics used to evaluate the performance of models in zero pronoun translation? sent1
    1.1. How is the accuracy of ZP recovery measured? sent2
    1.2. What are some automatic evaluation metrics for measuring general translation quality? sent3
        1.2.1. What is BLEU and how does it measure translation quality? sent4
        1.2.2. How does ME-TEOR incorporate semantic information in its evaluation? sent5
        1.2.3. What is COMET and what does it achieve in MT evaluation? sent6
    1.3. Why are general-purpose metrics insufficient for evaluating pronoun-aware translation quality? sent7
        1.3.1. How do missed or incorrect pronouns affect BLEU scores and true performance? sent8
        1.3.2. What has been proposed to address the gap in pronoun-aware translation evaluation? sent9","Question: Why are general-purpose metrics insufficient for evaluating pronoun-aware translation quality in zero pronoun translation?

Supporting sentence: sent7, sent8, sent9",Why are general-purpose metrics insufficient for evaluating pronoun-aware translation quality in zero pronoun translation?,"sent7, sent8, sent9",2,implement,"General-purpose metrics cannot characterize the performance of zero pronoun (ZP) translation.

Previous works usually evaluate ZP translation using the BLEU metric (Wang et al., 2016a; Wang et al., 2018a; Ri et al., 2021).

However, missed or incorrect pronouns may not affect BLEU scores but severely harm true performances.

To address this gap, some works proposed pronoun-targeted evaluation metrics (Werlen and Popescu-Belis, 2017; Läubli et al., 2018).","Question:

Why are general-purpose metrics insufficient for evaluating pronoun-aware translation quality in zero pronoun translation?

Answer:

General-purpose metrics cannot characterize the performance of zero pronoun (ZP) translation.

Previous works usually evaluate ZP translation using the BLEU metric (Wang et al., 2016a; Wang et al., 2018a; Ri et al., 2021).

However, missed or incorrect pronouns may not affect BLEU scores but severely harm true performances.

To address this gap, some works proposed pronoun-targeted evaluation metrics (Werlen and Popescu-Belis, 2017; Läubli et al., 2018)."
258740687,A Survey on Zero Pronoun Translation,https://www.semanticscholar.org/paper/8fa265a8ca46c9c0ad35d2c5b519a86e2a092dfe,Overview,7,"There are three kinds of automatic metrics to evaluate performances of related models:

• Accuracy of ZP Recovery: this aims to measure model performance on detecting and predicting ZPs of sentences in one pro-drop language. For instance, the micro F1-score is used to evaluating Chinese ZPR systems Song et al. (2020). 11 • General Translation Quality: there are a number of automatic evaluation metrics for measuring general performance of MT systems (Snover  Table 4: Correlation between the manual evaluation and other automatic metrics, which are applied on different ZPT benchmarks, which are same as in Table 2. et al., 2006). BLEU (Papineni et al., 2002) is the most widely-used one, which measures the precision of n-grams of the MT output compared to the reference, weighted by a brevity penalty to punish overly short translations. ME-TEOR (Banerjee and Lavie, 2005) incorporates semantic information by calculating either exact match, stem match, or synonymy match. Furthermore, COMET (Rei et al., 2020) is a neural framework for training multilingual MT evaluation models which obtains new SOTA levels of correlation with human judgements. • Pronoun-Aware Translation Quality: Previous works usually evaluate ZPT using the BLEU metric (Wang et al., 2016a(Wang et al., , 2018aRi et al., 2021), however, general-purpose metrics cannot characterize the performance of ZP translation. As shown in Table 3, the missed or incorrect pronouns may not affect BLEU scores but severely harm true performances. To fix this gap, some works proposed pronoun-targeted evaluation metrics (Werlen and Popescu-Belis, 2017; Läubli et al., 2018).","sent1: There are three kinds of automatic metrics to evaluate performances of related models:• Accuracy of ZP Recovery: this aims to measure model performance on detecting and predicting ZPs of sentences in one pro-drop language.
sent2: For instance, the micro F1-score is used to evaluating Chinese ZPR systems Song et al. (2020).
sent3: 11 • General Translation Quality: there are a number of automatic evaluation metrics for measuring general performance of MT systems (Snover  Table 4: Correlation between the manual evaluation and other automatic metrics, which are applied on different ZPT benchmarks, which are same as in Table 2. et al., 2006).
sent4: BLEU (Papineni et al., 2002) is the most widely-used one, which measures the precision of n-grams of the MT output compared to the reference, weighted by a brevity penalty to punish overly short translations.
sent5: ME-TEOR (Banerjee and Lavie, 2005) incorporates semantic information by calculating either exact match, stem match, or synonymy match.
sent6: Furthermore, COMET (Rei et al., 2020) is a neural framework for training multilingual MT evaluation models which obtains new SOTA levels of correlation with human judgements.
sent7: • Pronoun-Aware Translation Quality: Previous works usually evaluate ZPT using the BLEU metric (Wang et al., 2016a(Wang et al., , 2018aRi et al., 2021), however, general-purpose metrics cannot characterize the performance of ZP translation.
sent8: As shown in Table 3, the missed or incorrect pronouns may not affect BLEU scores but severely harm true performances.
sent9: To fix this gap, some works proposed pronoun-targeted evaluation metrics (Werlen and Popescu-Belis, 2017; Läubli et al., 2018).","1. What are the three kinds of automatic metrics used to evaluate the performance of models in zero pronoun translation? sent1
    1.1. How is the accuracy of ZP recovery measured? sent2
    1.2. What are some automatic evaluation metrics for measuring general translation quality? sent3
        1.2.1. What is BLEU and how does it measure translation quality? sent4
        1.2.2. How does ME-TEOR incorporate semantic information in its evaluation? sent5
        1.2.3. What is COMET and what does it achieve in MT evaluation? sent6
    1.3. Why are general-purpose metrics insufficient for evaluating pronoun-aware translation quality? sent7
        1.3.1. How do missed or incorrect pronouns affect BLEU scores and true performance? sent8
        1.3.2. What has been proposed to address the gap in pronoun-aware translation evaluation? sent9","1. What are the three kinds of automatic metrics used to evaluate the performance of models in zero pronoun translation? sent1
    1.1. How is the accuracy of ZP recovery measured? sent2
    1.2. What are some automatic evaluation metrics for measuring general translation quality? sent3
        1.2.1. What is BLEU and how does it measure translation quality? sent4
        1.2.2. How does ME-TEOR incorporate semantic information in its evaluation? sent5
        1.2.3. What is COMET and what does it achieve in MT evaluation? sent6
    1.3. Why are general-purpose metrics insufficient for evaluating pronoun-aware translation quality? sent7
        1.3.1. How do missed or incorrect pronouns affect BLEU scores and true performance? sent8
        1.3.2. What has been proposed to address the gap in pronoun-aware translation evaluation? sent9

    1.2. What are some automatic evaluation metrics for measuring general translation quality? sent3
        1.2.1. What is BLEU and how does it measure translation quality? sent4
        1.2.2. How does ME-TEOR incorporate semantic information in its evaluation? sent5
        1.2.3. What is COMET and what does it achieve in MT evaluation? sent6","    1.2. What are some automatic evaluation metrics for measuring general translation quality? sent3
        1.2.1. What is BLEU and how does it measure translation quality? sent4
        1.2.2. How does ME-TEOR incorporate semantic information in its evaluation? sent5
        1.2.3. What is COMET and what does it achieve in MT evaluation? sent6","Question: How do automatic evaluation metrics like BLEU, ME-TEOR, and COMET assess general translation quality in MT systems?

Supporting sentence: sent3, sent4, sent5, sent6","How do automatic evaluation metrics like BLEU, ME-TEOR, and COMET assess general translation quality in MT systems?","sent3, sent4, sent5, sent6",2,implement,"There are a number of automatic evaluation metrics for measuring general performance of MT systems.

BLEU (Papineni et al., 2002) is the most widely-used one, which measures the precision of n-grams of the MT output compared to the reference, weighted by a brevity penalty to punish overly short translations.

ME-TEOR (Banerjee and Lavie, 2005) incorporates semantic information by calculating either exact match, stem match, or synonymy match.

Furthermore, COMET (Rei et al., 2020) is a neural framework for training multilingual MT evaluation models which obtains new SOTA levels of correlation with human judgements.","Question:

How do automatic evaluation metrics like BLEU, ME-TEOR, and COMET assess general translation quality in MT systems?

Answer:

There are a number of automatic evaluation metrics for measuring general performance of MT systems.

BLEU (Papineni et al., 2002) is the most widely-used one, which measures the precision of n-grams of the MT output compared to the reference, weighted by a brevity penalty to punish overly short translations.

ME-TEOR (Banerjee and Lavie, 2005) incorporates semantic information by calculating either exact match, stem match, or synonymy match.

Furthermore, COMET (Rei et al., 2020) is a neural framework for training multilingual MT evaluation models which obtains new SOTA levels of correlation with human judgements."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,Multilingual Language Models,4,"Analogously to low-resource domains, lowresource languages can also benefit from labeled resources available in other high-resource languages. This usually requires the training of multilingual language representations by combining monolingual representations (Lange et al., 2020a) or training a single model for many languages, such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020) . These models are trained using unlabeled, monolingual corpora from different languages and can be used in crossand multilingual settings, due to many languages seen during pre-training.

In cross-lingual zero-shot learning, no taskspecific labeled data is available in the low-resource target language. Instead, labeled data from a high-resource language is leveraged. A multilingual model can be trained on the target task in a high-resource language and afterwards, applied to the unseen target languages, such as for named entity recognition ( 2020) proposed adding a minimal amount of target-task and -language data (in the range of 10 to 100 labeled sentences) which resulted in a significant boost in performance for classification in low-resource languages.

The transfer between two languages can be improved by creating a common multilingual embedding space of multiple languages. This is useful for standard word embeddings  as well as pre-trained language models. For example, by aligning the languages inside a single multilin- This alignment is typically done by computing a mapping between two different embedding spaces, such that the words in both embeddings share similar feature vectors after the mapping (Mikolov et al., 2013;Joulin et al., 2018). This allows to use different embeddings inside the same model and helps when two languages do not share the same space inside a single model (Cao et al., 2020). For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the highresource language English and three low-resource African languages, Swahili, Tagalog, and Somali, to improve document retrieval performance for the African languages.

Open Issues: While these multilingual models are a tremendous step towards enabling NLP in many languages, possible claims that these are universal language models do not hold. For example, mBERT covers 104 and XLM-R 100 languages, which is a third of all languages in Wikipedia as outlined earlier. Further, Wu and Dredze (2020) showed that, in particular, low-resource languages are not well-represented in mBERT. Figure 2 shows which language families with at least 1 million speakers are covered by mBERT and XLM-RoBERTa 2 . In particular, African and American languages are not well-represented within the transformer models, even though millions of people speak these languages. This can be problematic, as languages from more distant language families are less suited for transfer learning, as Lauscher et al.

(2020) showed.","sent1: Analogously to low-resource domains, lowresource languages can also benefit from labeled resources available in other high-resource languages.
sent2: This usually requires the training of multilingual language representations by combining monolingual representations (Lange et al., 2020a) or training a single model for many languages, such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020) .
sent3: These models are trained using unlabeled, monolingual corpora from different languages and can be used in crossand multilingual settings, due to many languages seen during pre-training.
sent4: In cross-lingual zero-shot learning, no taskspecific labeled data is available in the low-resource target language.
sent5: Instead, labeled data from a high-resource language is leveraged.
sent6: A multilingual model can be trained on the target task in a high-resource language and afterwards, applied to the unseen target languages, such as for named entity recognition ( 2020) proposed adding a minimal amount of target-task and -language data (in the range of 10 to 100 labeled sentences) which resulted in a significant boost in performance for classification in low-resource languages.
sent7: The transfer between two languages can be improved by creating a common multilingual embedding space of multiple languages.
sent8: This is useful for standard word embeddings  as well as pre-trained language models.
sent9: For example, by aligning the languages inside a single multilin-
sent10: This alignment is typically done by computing a mapping between two different embedding spaces, such that the words in both embeddings share similar feature vectors after the mapping (Mikolov et al., 2013;Joulin et al., 2018).
sent11: This allows to use different embeddings inside the same model and helps when two languages do not share the same space inside a single model (Cao et al., 2020).
sent12: For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the highresource language English and three low-resource African languages, Swahili, Tagalog, and Somali, to improve document retrieval performance for the African languages.
sent13: Open Issues: While these multilingual models are a tremendous step towards enabling NLP in many languages, possible claims that these are universal language models do not hold.
sent14: For example, mBERT covers 104 and XLM-R 100 languages, which is a third of all languages in Wikipedia as outlined earlier.
sent15: Further, Wu and Dredze (2020) showed that, in particular, low-resource languages are not well-represented in mBERT.
sent16: Figure 2 shows which language families with at least 1 million speakers are covered by mBERT and XLM-RoBERTa 2 .
sent17: In particular, African and American languages are not well-represented within the transformer models, even though millions of people speak these languages.
sent18: This can be problematic, as languages from more distant language families are less suited for transfer learning, as Lauscher et al.(2020) showed.","1. How can low-resource languages benefit from resources available in high-resource languages? sent1
2. What is required to train multilingual language representations? sent2
3. How are multilingual models trained and used in cross- and multilingual settings? sent3
4. What is cross-lingual zero-shot learning? sent4
    4.1. How is labeled data from a high-resource language leveraged in cross-lingual zero-shot learning? sent5
    4.2. How can a multilingual model be applied to unseen target languages? sent6
5. How can the transfer between two languages be improved? sent7
    5.1. What is the usefulness of creating a common multilingual embedding space? sent8
    5.2. How is alignment typically done between two different embedding spaces? sent9, sent10
    5.3. What is the benefit of using different embeddings inside the same model? sent11
    5.4. Can you provide an example of using bilingual representations to improve performance in low-resource languages? sent12
6. What are the open issues with multilingual models? sent13
    6.1. How many languages do mBERT and XLM-R cover, and what is the implication of this coverage? sent14
    6.2. What did Wu and Dredze (2020) show about low-resource languages in mBERT? sent15
    6.3. Which language families are not well-represented in transformer models, and why is this problematic? sent16, sent17, sent18","5. How can the transfer between two languages be improved? sent7
    5.1. What is the usefulness of creating a common multilingual embedding space? sent8
    5.2. How is alignment typically done between two different embedding spaces? sent9, sent10
    5.3. What is the benefit of using different embeddings inside the same model? sent11
    5.4. Can you provide an example of using bilingual representations to improve performance in low-resource languages? sent12

6. What are the open issues with multilingual models? sent13
    6.1. How many languages do mBERT and XLM-R cover, and what is the implication of this coverage? sent14
    6.2. What did Wu and Dredze (2020) show about low-resource languages in mBERT? sent15
    6.3. Which language families are not well-represented in transformer models, and why is this problematic? sent16, sent17, sent18","5. How can the transfer between two languages be improved? sent7
    5.1. What is the usefulness of creating a common multilingual embedding space? sent8
    5.2. How is alignment typically done between two different embedding spaces? sent9, sent10
    5.3. What is the benefit of using different embeddings inside the same model? sent11
    5.4. Can you provide an example of using bilingual representations to improve performance in low-resource languages? sent12","Question: How can bilingual representations enhance document retrieval performance in low-resource languages?

Supporting sentence: sent12",How can bilingual representations enhance document retrieval performance in low-resource languages?,sent12,4,implement,"Bilingual representations can enhance document retrieval performance in low-resource languages by creating cross-lingual word embeddings using a small set of parallel sentences between a high-resource language and low-resource languages.

For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the high-resource language English and three low-resource African languages, Swahili, Tagalog, and Somali.

This approach improved document retrieval performance for the African languages.","Question:

How can bilingual representations enhance document retrieval performance in low-resource languages?

Answer:

Bilingual representations can enhance document retrieval performance in low-resource languages by creating cross-lingual word embeddings using a small set of parallel sentences between a high-resource language and low-resource languages.

For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the high-resource language English and three low-resource African languages, Swahili, Tagalog, and Somali.

This approach improved document retrieval performance for the African languages."
252200083,"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",https://www.semanticscholar.org/paper/77696f90afa79ac06ea58abcc91bf5e10f72b934,Theoretical Research in Irony 2.1 Irony Theories,5,"Various definitions have been given to irony. Early studies suggested that irony is the expression whose real meaning is contradictory to its literal meaning (Grice, 1975). The Merriam-Webster Dictionary, The Oxford English Dictionary, and The Collins English Dictionary all adopted this definition and used the words ""opposite"" or ""contrary"" to explain the relationship between the literal and contextual meanings of irony.

However, more research into various types of ironic examples revealed that the contextual meaning of irony does not have to be ""opposite"" or ""contrary"" to the literal one. According to Sperber and Wilson (1986); Wilson and Sperber (2012), some expressions have no ""literal meaning"" to be challenged because no ""literal meaning"" is mentioned in the context, based on which they raised relevance theory and the ""echoic"" concept. They considered irony as ""an echoic use of language in which the speaker tacitly dissociates herself from an attributed utterance or thought"" (Wilson, 2006). That is, if the ""echoic use"" is incongruous in some ways, the expression can be ironic. Based on this theory, Seto (1998) put forward that there are some ""echo-markers"" like definitely, really, and indeed.

Li and Huang (2020) provided instances to show that ""incongruity"" does not have to be between the literal and contextual meanings of irony in certain circumstances. They believed that irony's true nature is a psychological activity as much as a verbal representation. The speaker or listeners must finish the ""reversal"" process on a psychological level for it to be completed. When compared to the concepts of ""echoic"" and ""incongruity,"" ""reversal"" is concerned not only with the results but also with the psychological processes that the speakers/listeners go through.","sent1: Various definitions have been given to irony.
sent2: Early studies suggested that irony is the expression whose real meaning is contradictory to its literal meaning (Grice, 1975).
sent3: The Merriam-Webster Dictionary, The Oxford English Dictionary, and The Collins English Dictionary all adopted this definition and used the words ""opposite"" or ""contrary"" to explain the relationship between the literal and contextual meanings of irony.
sent4: However, more research into various types of ironic examples revealed that the contextual meaning of irony does not have to be ""opposite"" or ""contrary"" to the literal one.
sent5: According to Sperber and Wilson (1986); Wilson and Sperber (2012), some expressions have no ""literal meaning"" to be challenged because no ""literal meaning"" is mentioned in the context, based on which they raised relevance theory and the ""echoic"" concept.
sent6: They considered irony as ""an echoic use of language in which the speaker tacitly dissociates herself from an attributed utterance or thought"" (Wilson, 2006).
sent7: That is, if the ""echoic use"" is incongruous in some ways, the expression can be ironic.
sent8: Based on this theory, Seto (1998) put forward that there are some ""echo-markers"" like definitely, really, and indeed.
sent9: Li and Huang (2020) provided instances to show that ""incongruity"" does not have to be between the literal and contextual meanings of irony in certain circumstances.
sent10: They believed that irony's true nature is a psychological activity as much as a verbal representation.
sent11: The speaker or listeners must finish the ""reversal"" process on a psychological level for it to be completed.
sent12: When compared to the concepts of ""echoic"" and ""incongruity,"" ""reversal"" is concerned not only with the results but also with the psychological processes that the speakers/listeners go through.","1. How has irony been defined in various studies? sent1
    1.1. What was the early definition of irony according to Grice? sent2
    1.2. How do major dictionaries define irony? sent3
    1.3. What did further research reveal about the contextual meaning of irony? sent4
2. What is the relevance theory and ""echoic"" concept proposed by Sperber and Wilson? sent5
    2.1. How do Sperber and Wilson define irony? sent6
    2.2. What makes an expression ironic according to the ""echoic use"" concept? sent7
    2.3. What are ""echo-markers"" according to Seto? sent8
3. How do Li and Huang view the nature of irony? sent9
    3.1. What do Li and Huang believe about irony's true nature? sent10
    3.2. What is the ""reversal"" process in irony according to Li and Huang? sent11
    3.3. How does ""reversal"" compare to ""echoic"" and ""incongruity""? sent12","1. How has irony been defined in various studies? sent1
    1.1. What was the early definition of irony according to Grice? sent2
    1.2. How do major dictionaries define irony? sent3
    1.3. What did further research reveal about the contextual meaning of irony? sent4

2. What is the relevance theory and ""echoic"" concept proposed by Sperber and Wilson? sent5
    2.1. How do Sperber and Wilson define irony? sent6
    2.2. What makes an expression ironic according to the ""echoic use"" concept? sent7
    2.3. What are ""echo-markers"" according to Seto? sent8

3. How do Li and Huang view the nature of irony? sent9
    3.1. What do Li and Huang believe about irony's true nature? sent10
    3.2. What is the ""reversal"" process in irony according to Li and Huang? sent11
    3.3. How does ""reversal"" compare to ""echoic"" and ""incongruity""? sent12","2. What is the relevance theory and ""echoic"" concept proposed by Sperber and Wilson? sent5
    2.1. How do Sperber and Wilson define irony? sent6
    2.2. What makes an expression ironic according to the ""echoic use"" concept? sent7
    2.3. What are ""echo-markers"" according to Seto? sent8","Question: How do Sperber and Wilson's relevance theory and ""echoic"" concept redefine the understanding of irony?

Supporting sentence: sent5, sent6, sent7","How do Sperber and Wilson's relevance theory and ""echoic"" concept redefine the understanding of irony?","sent5, sent6, sent7",3,implement,"Sperber and Wilson (1986); Wilson and Sperber (2012) proposed that some expressions have no ""literal meaning"" to be challenged because no ""literal meaning"" is mentioned in the context.

They introduced relevance theory and the ""echoic"" concept, considering irony as ""an echoic use of language in which the speaker tacitly dissociates herself from an attributed utterance or thought"" (Wilson, 2006).

If the ""echoic use"" is incongruous in some ways, the expression can be ironic.","Question:

How do Sperber and Wilson's relevance theory and ""echoic"" concept redefine the understanding of irony?

Answer:

Sperber and Wilson (1986); Wilson and Sperber (2012) proposed that some expressions have no ""literal meaning"" to be challenged because no ""literal meaning"" is mentioned in the context.

They introduced relevance theory and the ""echoic"" concept, considering irony as ""an echoic use of language in which the speaker tacitly dissociates herself from an attributed utterance or thought"" (Wilson, 2006).

If the ""echoic use"" is incongruous in some ways, the expression can be ironic."
252200083,"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",https://www.semanticscholar.org/paper/77696f90afa79ac06ea58abcc91bf5e10f72b934,Theoretical Research in Irony 2.1 Irony Theories,5,"Various definitions have been given to irony. Early studies suggested that irony is the expression whose real meaning is contradictory to its literal meaning (Grice, 1975). The Merriam-Webster Dictionary, The Oxford English Dictionary, and The Collins English Dictionary all adopted this definition and used the words ""opposite"" or ""contrary"" to explain the relationship between the literal and contextual meanings of irony.

However, more research into various types of ironic examples revealed that the contextual meaning of irony does not have to be ""opposite"" or ""contrary"" to the literal one. According to Sperber and Wilson (1986); Wilson and Sperber (2012), some expressions have no ""literal meaning"" to be challenged because no ""literal meaning"" is mentioned in the context, based on which they raised relevance theory and the ""echoic"" concept. They considered irony as ""an echoic use of language in which the speaker tacitly dissociates herself from an attributed utterance or thought"" (Wilson, 2006). That is, if the ""echoic use"" is incongruous in some ways, the expression can be ironic. Based on this theory, Seto (1998) put forward that there are some ""echo-markers"" like definitely, really, and indeed.

Li and Huang (2020) provided instances to show that ""incongruity"" does not have to be between the literal and contextual meanings of irony in certain circumstances. They believed that irony's true nature is a psychological activity as much as a verbal representation. The speaker or listeners must finish the ""reversal"" process on a psychological level for it to be completed. When compared to the concepts of ""echoic"" and ""incongruity,"" ""reversal"" is concerned not only with the results but also with the psychological processes that the speakers/listeners go through.","sent1: Various definitions have been given to irony.
sent2: Early studies suggested that irony is the expression whose real meaning is contradictory to its literal meaning (Grice, 1975).
sent3: The Merriam-Webster Dictionary, The Oxford English Dictionary, and The Collins English Dictionary all adopted this definition and used the words ""opposite"" or ""contrary"" to explain the relationship between the literal and contextual meanings of irony.
sent4: However, more research into various types of ironic examples revealed that the contextual meaning of irony does not have to be ""opposite"" or ""contrary"" to the literal one.
sent5: According to Sperber and Wilson (1986); Wilson and Sperber (2012), some expressions have no ""literal meaning"" to be challenged because no ""literal meaning"" is mentioned in the context, based on which they raised relevance theory and the ""echoic"" concept.
sent6: They considered irony as ""an echoic use of language in which the speaker tacitly dissociates herself from an attributed utterance or thought"" (Wilson, 2006).
sent7: That is, if the ""echoic use"" is incongruous in some ways, the expression can be ironic.
sent8: Based on this theory, Seto (1998) put forward that there are some ""echo-markers"" like definitely, really, and indeed.
sent9: Li and Huang (2020) provided instances to show that ""incongruity"" does not have to be between the literal and contextual meanings of irony in certain circumstances.
sent10: They believed that irony's true nature is a psychological activity as much as a verbal representation.
sent11: The speaker or listeners must finish the ""reversal"" process on a psychological level for it to be completed.
sent12: When compared to the concepts of ""echoic"" and ""incongruity,"" ""reversal"" is concerned not only with the results but also with the psychological processes that the speakers/listeners go through.","1. How has irony been defined in various studies? sent1
    1.1. What was the early definition of irony according to Grice? sent2
    1.2. How do major dictionaries define irony? sent3
    1.3. What did further research reveal about the contextual meaning of irony? sent4
2. What is the relevance theory and ""echoic"" concept proposed by Sperber and Wilson? sent5
    2.1. How do Sperber and Wilson define irony? sent6
    2.2. What makes an expression ironic according to the ""echoic use"" concept? sent7
    2.3. What are ""echo-markers"" according to Seto? sent8
3. How do Li and Huang view the nature of irony? sent9
    3.1. What do Li and Huang believe about irony's true nature? sent10
    3.2. What is the ""reversal"" process in irony according to Li and Huang? sent11
    3.3. How does ""reversal"" compare to ""echoic"" and ""incongruity""? sent12","1. How has irony been defined in various studies? sent1
    1.1. What was the early definition of irony according to Grice? sent2
    1.2. How do major dictionaries define irony? sent3
    1.3. What did further research reveal about the contextual meaning of irony? sent4

2. What is the relevance theory and ""echoic"" concept proposed by Sperber and Wilson? sent5
    2.1. How do Sperber and Wilson define irony? sent6
    2.2. What makes an expression ironic according to the ""echoic use"" concept? sent7
    2.3. What are ""echo-markers"" according to Seto? sent8

3. How do Li and Huang view the nature of irony? sent9
    3.1. What do Li and Huang believe about irony's true nature? sent10
    3.2. What is the ""reversal"" process in irony according to Li and Huang? sent11
    3.3. How does ""reversal"" compare to ""echoic"" and ""incongruity""? sent12","3. How do Li and Huang view the nature of irony? sent9
    3.1. What do Li and Huang believe about irony's true nature? sent10
    3.2. What is the ""reversal"" process in irony according to Li and Huang? sent11
    3.3. How does ""reversal"" compare to ""echoic"" and ""incongruity""? sent12","Question: How does the ""reversal"" process in irony differ from the concepts of ""echoic"" and ""incongruity""?

Supporting sentence: sent11, sent12","How does the ""reversal"" process in irony differ from the concepts of ""echoic"" and ""incongruity""?","sent11, sent12",2,implement,"The ""reversal"" process in irony is concerned not only with the results but also with the psychological processes that the speakers/listeners go through.

In contrast, the concept of ""echoic"" involves an ""echoic use of language in which the speaker tacitly dissociates herself from an attributed utterance or thought"" (Wilson, 2006).

The ""echoic use"" can be ironic if it is incongruous in some ways.

The concept of ""incongruity"" does not necessarily involve a contradiction between the literal and contextual meanings of irony in certain circumstances.

Therefore, while ""echoic"" and ""incongruity"" focus on the linguistic aspects of irony, ""reversal"" emphasizes the psychological activity involved in processing irony.","Question:

How does the ""reversal"" process in irony differ from the concepts of ""echoic"" and ""incongruity""?

Answer:

The ""reversal"" process in irony is concerned not only with the results but also with the psychological processes that the speakers/listeners go through.

In contrast, the concept of ""echoic"" involves an ""echoic use of language in which the speaker tacitly dissociates herself from an attributed utterance or thought"" (Wilson, 2006).

The ""echoic use"" can be ironic if it is incongruous in some ways.

The concept of ""incongruity"" does not necessarily involve a contradiction between the literal and contextual meanings of irony in certain circumstances.

Therefore, while ""echoic"" and ""incongruity"" focus on the linguistic aspects of irony, ""reversal"" emphasizes the psychological activity involved in processing irony."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,Spatial Convolution and Message Passing,4,"One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.

Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:

This first equation describes how messages are generated. A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.

The above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function ρ. This function aggregates all incoming messages to a node. Then another differen-tiable function ψ combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.

The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.

where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.

The above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1

where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with

. This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows

). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.

There are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.

In ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.

Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.

We want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","sent1: One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs.
sent2: As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image.
sent3: The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node.
sent4: Generalizing this idea to a non-regular grids leads to spatial convolution.
sent5: However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation.
sent6: The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.
sent7: Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme.
sent8: However, such a description suffers from scalability issues due to it directly operating over the entire graph.
sent9: As such modern GNNs use, what is commonly referred to as, message passing.
sent10: In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations.
sent11: Thereby the network is able to diffuse information throughout the graph.
sent12: Consequently, the more iterations, the further outwards information is propagated throughout the graph.
sent13: In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution.
sent14: Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:This first equation describes how messages are generated.
sent15: A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.
sent16: The above equation is the core of the message passing framework and describes how each node feature is updated.
sent17: The first part consists in the application of a permutation-invariant reduction function ρ.
sent18: This function aggregates all incoming messages to a node.
sent19: Then another differen-tiable function ψ combines the reduced messages received with the previous state.
sent20: Using these two equations one can utilize message passing for learnable layers.
sent21: The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1
sent22: i,j = x t j and the aggregation is the normalized sum of messages, i.e.where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree.
sent23: Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.
sent24: The above presented convolution layer does not allow the model to filter unimportant neighbours.
sent25: Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour.
sent26: A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure.
sent27: The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages.
sent28: Just as in transformers GAT is formulated with multi-head attention.
sent29: The modification to the previously presented convolution layer follows closely the common attention formulation.
sent30: Formally, x t+1where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads.
sent31: The attention scores are computed with.
sent32: This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ).
sent33: We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2.
sent34: Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be.
sent35: The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity.
sent36: Taking the above description the attention score for GATv2 is modified as follows).
sent37: In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.
sent38: There are numerous modifications and extensions to the basic convolution presented here.
sent39: However, for ATS models, GAT layers are dominating as the workhorse for most models.
sent40: The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST.
sent41: We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT.
sent42: Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.
sent43: In ATS the graphs used are in nearly all cases not homogeneous.
sent44: However, the equations presented here do not work for heterogeneous graphs.
sent45: The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph.
sent46: In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.
sent47: Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models.
sent48: Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes.
sent49: This is usually done with some function f where f is commonly the mean, max or sum.
sent50: We want to explicitly point out to the reader that the construction of GNNs does not require special datasets.
sent51: All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail.
sent52: The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","1. How is spatial convolution in GNNs viewed in relation to convolution in neural networks like CNNs? sent1
    1.1. How is 2D convolution applied in the context of images? sent2
        1.1.1. What is the result of applying 2D convolution to a target node in images? sent3
    1.2. How is the idea of convolution generalized to non-regular grids? sent4
    1.3. What challenges arise when extending convolution to graphs? sent5, sent6
2. What is the mathematical formulation of convolution in GNNs, and what issues does it face? sent7, sent8
3. What is message passing in modern GNNs, and how does it function? sent9, sent10
    3.1. How does message passing affect information propagation in graphs? sent11, sent12
    3.2. How is the concept of receptive field in CNNs related to message passing in GNNs? sent13
4. How is message passing formally defined, and what are its core components? sent14, sent15, sent16
    4.1. What role does the permutation-invariant reduction function play in message passing? sent17, sent18
    4.2. How are reduced messages combined with the previous state in message passing? sent19
    4.3. How can message passing be utilized for learnable layers in GNNs? sent20
5. How is the convolution layer defined in GNNs, and what is important about the reduction function? sent21, sent22, sent23
6. What limitation does the presented convolution layer have, and how is it addressed by GAT? sent24, sent25
    6.1. How does the introduction of attention scores in GAT affect spatial convolution? sent26, sent27
    6.2. How is GAT formulated in relation to multi-head attention? sent28, sent29
7. What modification does GATv2 introduce, and what is its impact? sent30, sent31, sent32, sent33, sent34, sent35, sent36, sent37
8. What is the current status of GAT layers in ATS models, and what is expected for GATv2? sent38, sent39, sent40, sent41, sent42
9. What challenges do heterogeneous graphs present, and how are they addressed? sent43, sent44, sent45, sent46
10. What is the role of pooling in GNNs, and how is it achieved? sent47, sent48, sent49
11. What datasets are used for GNNs in ATS, and what is required for their construction? sent50, sent51, sent52","1. How is spatial convolution in GNNs viewed in relation to convolution in neural networks like CNNs? sent1
    1.1. How is 2D convolution applied in the context of images? sent2
        1.1.1. What is the result of applying 2D convolution to a target node in images? sent3
    1.2. How is the idea of convolution generalized to non-regular grids? sent4
    1.3. What challenges arise when extending convolution to graphs? sent5, sent6

3. What is message passing in modern GNNs, and how does it function? sent9, sent10
    3.1. How does message passing affect information propagation in graphs? sent11, sent12
    3.2. How is the concept of receptive field in CNNs related to message passing in GNNs? sent13

4. How is message passing formally defined, and what are its core components? sent14, sent15, sent16
    4.1. What role does the permutation-invariant reduction function play in message passing? sent17, sent18
    4.2. How are reduced messages combined with the previous state in message passing? sent19
    4.3. How can message passing be utilized for learnable layers in GNNs? sent20

6. What limitation does the presented convolution layer have, and how is it addressed by GAT? sent24, sent25
    6.1. How does the introduction of attention scores in GAT affect spatial convolution? sent26, sent27
    6.2. How is GAT formulated in relation to multi-head attention? sent28, sent29","1. How is spatial convolution in GNNs viewed in relation to convolution in neural networks like CNNs? sent1
    1.1. How is 2D convolution applied in the context of images? sent2
        1.1.1. What is the result of applying 2D convolution to a target node in images? sent3
    1.2. How is the idea of convolution generalized to non-regular grids? sent4
    1.3. What challenges arise when extending convolution to graphs? sent5, sent6","Question: What challenges are encountered when applying spatial convolution to graphs, and how do they differ from regular grids?

Supporting sentence: sent5, sent6","What challenges are encountered when applying spatial convolution to graphs, and how do they differ from regular grids?","sent5, sent6",2,implement,"One can view spatial convolution as used in GNNs as a generalization of the convolution used in neural networks such as CNNs.

In the case of images, 2D convolution is applied to a regular grid of nodes where each node represents a pixel in the image.

The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbors of the target node.

Generalizing this idea to non-regular grids leads to spatial convolution.

However, different from images and regular grids, in graphs, the neighbors of each target node are unordered and can vary in number and their feature vector representation.

The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighborhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.","Question:

What challenges are encountered when applying spatial convolution to graphs, and how do they differ from regular grids?

Answer:

One can view spatial convolution as used in GNNs as a generalization of the convolution used in neural networks such as CNNs.

In the case of images, 2D convolution is applied to a regular grid of nodes where each node represents a pixel in the image.

The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbors of the target node.

Generalizing this idea to non-regular grids leads to spatial convolution.

However, different from images and regular grids, in graphs, the neighbors of each target node are unordered and can vary in number and their feature vector representation.

The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighborhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,Spatial Convolution and Message Passing,4,"One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.

Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:

This first equation describes how messages are generated. A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.

The above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function ρ. This function aggregates all incoming messages to a node. Then another differen-tiable function ψ combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.

The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.

where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.

The above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1

where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with

. This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows

). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.

There are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.

In ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.

Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.

We want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","sent1: One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs.
sent2: As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image.
sent3: The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node.
sent4: Generalizing this idea to a non-regular grids leads to spatial convolution.
sent5: However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation.
sent6: The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.
sent7: Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme.
sent8: However, such a description suffers from scalability issues due to it directly operating over the entire graph.
sent9: As such modern GNNs use, what is commonly referred to as, message passing.
sent10: In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations.
sent11: Thereby the network is able to diffuse information throughout the graph.
sent12: Consequently, the more iterations, the further outwards information is propagated throughout the graph.
sent13: In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution.
sent14: Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:This first equation describes how messages are generated.
sent15: A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.
sent16: The above equation is the core of the message passing framework and describes how each node feature is updated.
sent17: The first part consists in the application of a permutation-invariant reduction function ρ.
sent18: This function aggregates all incoming messages to a node.
sent19: Then another differen-tiable function ψ combines the reduced messages received with the previous state.
sent20: Using these two equations one can utilize message passing for learnable layers.
sent21: The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1
sent22: i,j = x t j and the aggregation is the normalized sum of messages, i.e.where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree.
sent23: Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.
sent24: The above presented convolution layer does not allow the model to filter unimportant neighbours.
sent25: Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour.
sent26: A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure.
sent27: The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages.
sent28: Just as in transformers GAT is formulated with multi-head attention.
sent29: The modification to the previously presented convolution layer follows closely the common attention formulation.
sent30: Formally, x t+1where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads.
sent31: The attention scores are computed with.
sent32: This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ).
sent33: We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2.
sent34: Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be.
sent35: The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity.
sent36: Taking the above description the attention score for GATv2 is modified as follows).
sent37: In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.
sent38: There are numerous modifications and extensions to the basic convolution presented here.
sent39: However, for ATS models, GAT layers are dominating as the workhorse for most models.
sent40: The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST.
sent41: We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT.
sent42: Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.
sent43: In ATS the graphs used are in nearly all cases not homogeneous.
sent44: However, the equations presented here do not work for heterogeneous graphs.
sent45: The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph.
sent46: In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.
sent47: Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models.
sent48: Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes.
sent49: This is usually done with some function f where f is commonly the mean, max or sum.
sent50: We want to explicitly point out to the reader that the construction of GNNs does not require special datasets.
sent51: All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail.
sent52: The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","1. How is spatial convolution in GNNs viewed in relation to convolution in neural networks like CNNs? sent1
    1.1. How is 2D convolution applied in the context of images? sent2
        1.1.1. What is the result of applying 2D convolution to a target node in images? sent3
    1.2. How is the idea of convolution generalized to non-regular grids? sent4
    1.3. What challenges arise when extending convolution to graphs? sent5, sent6
2. What is the mathematical formulation of convolution in GNNs, and what issues does it face? sent7, sent8
3. What is message passing in modern GNNs, and how does it function? sent9, sent10
    3.1. How does message passing affect information propagation in graphs? sent11, sent12
    3.2. How is the concept of receptive field in CNNs related to message passing in GNNs? sent13
4. How is message passing formally defined, and what are its core components? sent14, sent15, sent16
    4.1. What role does the permutation-invariant reduction function play in message passing? sent17, sent18
    4.2. How are reduced messages combined with the previous state in message passing? sent19
    4.3. How can message passing be utilized for learnable layers in GNNs? sent20
5. How is the convolution layer defined in GNNs, and what is important about the reduction function? sent21, sent22, sent23
6. What limitation does the presented convolution layer have, and how is it addressed by GAT? sent24, sent25
    6.1. How does the introduction of attention scores in GAT affect spatial convolution? sent26, sent27
    6.2. How is GAT formulated in relation to multi-head attention? sent28, sent29
7. What modification does GATv2 introduce, and what is its impact? sent30, sent31, sent32, sent33, sent34, sent35, sent36, sent37
8. What is the current status of GAT layers in ATS models, and what is expected for GATv2? sent38, sent39, sent40, sent41, sent42
9. What challenges do heterogeneous graphs present, and how are they addressed? sent43, sent44, sent45, sent46
10. What is the role of pooling in GNNs, and how is it achieved? sent47, sent48, sent49
11. What datasets are used for GNNs in ATS, and what is required for their construction? sent50, sent51, sent52","1. How is spatial convolution in GNNs viewed in relation to convolution in neural networks like CNNs? sent1
    1.1. How is 2D convolution applied in the context of images? sent2
        1.1.1. What is the result of applying 2D convolution to a target node in images? sent3
    1.2. How is the idea of convolution generalized to non-regular grids? sent4
    1.3. What challenges arise when extending convolution to graphs? sent5, sent6

3. What is message passing in modern GNNs, and how does it function? sent9, sent10
    3.1. How does message passing affect information propagation in graphs? sent11, sent12
    3.2. How is the concept of receptive field in CNNs related to message passing in GNNs? sent13

4. How is message passing formally defined, and what are its core components? sent14, sent15, sent16
    4.1. What role does the permutation-invariant reduction function play in message passing? sent17, sent18
    4.2. How are reduced messages combined with the previous state in message passing? sent19
    4.3. How can message passing be utilized for learnable layers in GNNs? sent20

6. What limitation does the presented convolution layer have, and how is it addressed by GAT? sent24, sent25
    6.1. How does the introduction of attention scores in GAT affect spatial convolution? sent26, sent27
    6.2. How is GAT formulated in relation to multi-head attention? sent28, sent29","3. What is message passing in modern GNNs, and how does it function? sent9, sent10
    3.1. How does message passing affect information propagation in graphs? sent11, sent12
    3.2. How is the concept of receptive field in CNNs related to message passing in GNNs? sent13","Question: How does message passing in GNNs facilitate information propagation and relate to the concept of receptive field in CNNs?

Supporting sentence: sent9, sent10, sent11, sent12, sent13",How does message passing in GNNs facilitate information propagation and relate to the concept of receptive field in CNNs?,"sent9, sent10, sent11, sent12, sent13",2,implement,"Message passing in GNNs facilitates information propagation by allowing nodes within the graph to exchange messages (perform convolutions) with their neighbors for a number of iterations.

This process enables the network to diffuse information throughout the graph.

Consequently, the more iterations, the further outwards information is propagated throughout the graph.

In the terminology of CNNs, one would say that the more message passing iterations, the larger the receptive field of the convolution.","Question:

How does message passing in GNNs facilitate information propagation and relate to the concept of receptive field in CNNs?

Answer:

Message passing in GNNs facilitates information propagation by allowing nodes within the graph to exchange messages (perform convolutions) with their neighbors for a number of iterations.

This process enables the network to diffuse information throughout the graph.

Consequently, the more iterations, the further outwards information is propagated throughout the graph.

In the terminology of CNNs, one would say that the more message passing iterations, the larger the receptive field of the convolution."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,Spatial Convolution and Message Passing,4,"One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.

Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:

This first equation describes how messages are generated. A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.

The above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function ρ. This function aggregates all incoming messages to a node. Then another differen-tiable function ψ combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.

The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.

where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.

The above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1

where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with

. This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows

). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.

There are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.

In ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.

Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.

We want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","sent1: One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs.
sent2: As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image.
sent3: The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node.
sent4: Generalizing this idea to a non-regular grids leads to spatial convolution.
sent5: However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation.
sent6: The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.
sent7: Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme.
sent8: However, such a description suffers from scalability issues due to it directly operating over the entire graph.
sent9: As such modern GNNs use, what is commonly referred to as, message passing.
sent10: In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations.
sent11: Thereby the network is able to diffuse information throughout the graph.
sent12: Consequently, the more iterations, the further outwards information is propagated throughout the graph.
sent13: In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution.
sent14: Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:This first equation describes how messages are generated.
sent15: A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.
sent16: The above equation is the core of the message passing framework and describes how each node feature is updated.
sent17: The first part consists in the application of a permutation-invariant reduction function ρ.
sent18: This function aggregates all incoming messages to a node.
sent19: Then another differen-tiable function ψ combines the reduced messages received with the previous state.
sent20: Using these two equations one can utilize message passing for learnable layers.
sent21: The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1
sent22: i,j = x t j and the aggregation is the normalized sum of messages, i.e.where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree.
sent23: Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.
sent24: The above presented convolution layer does not allow the model to filter unimportant neighbours.
sent25: Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour.
sent26: A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure.
sent27: The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages.
sent28: Just as in transformers GAT is formulated with multi-head attention.
sent29: The modification to the previously presented convolution layer follows closely the common attention formulation.
sent30: Formally, x t+1where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads.
sent31: The attention scores are computed with.
sent32: This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ).
sent33: We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2.
sent34: Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be.
sent35: The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity.
sent36: Taking the above description the attention score for GATv2 is modified as follows).
sent37: In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.
sent38: There are numerous modifications and extensions to the basic convolution presented here.
sent39: However, for ATS models, GAT layers are dominating as the workhorse for most models.
sent40: The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST.
sent41: We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT.
sent42: Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.
sent43: In ATS the graphs used are in nearly all cases not homogeneous.
sent44: However, the equations presented here do not work for heterogeneous graphs.
sent45: The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph.
sent46: In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.
sent47: Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models.
sent48: Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes.
sent49: This is usually done with some function f where f is commonly the mean, max or sum.
sent50: We want to explicitly point out to the reader that the construction of GNNs does not require special datasets.
sent51: All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail.
sent52: The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","1. How is spatial convolution in GNNs viewed in relation to convolution in neural networks like CNNs? sent1
    1.1. How is 2D convolution applied in the context of images? sent2
        1.1.1. What is the result of applying 2D convolution to a target node in images? sent3
    1.2. How is the idea of convolution generalized to non-regular grids? sent4
    1.3. What challenges arise when extending convolution to graphs? sent5, sent6
2. What is the mathematical formulation of convolution in GNNs, and what issues does it face? sent7, sent8
3. What is message passing in modern GNNs, and how does it function? sent9, sent10
    3.1. How does message passing affect information propagation in graphs? sent11, sent12
    3.2. How is the concept of receptive field in CNNs related to message passing in GNNs? sent13
4. How is message passing formally defined, and what are its core components? sent14, sent15, sent16
    4.1. What role does the permutation-invariant reduction function play in message passing? sent17, sent18
    4.2. How are reduced messages combined with the previous state in message passing? sent19
    4.3. How can message passing be utilized for learnable layers in GNNs? sent20
5. How is the convolution layer defined in GNNs, and what is important about the reduction function? sent21, sent22, sent23
6. What limitation does the presented convolution layer have, and how is it addressed by GAT? sent24, sent25
    6.1. How does the introduction of attention scores in GAT affect spatial convolution? sent26, sent27
    6.2. How is GAT formulated in relation to multi-head attention? sent28, sent29
7. What modification does GATv2 introduce, and what is its impact? sent30, sent31, sent32, sent33, sent34, sent35, sent36, sent37
8. What is the current status of GAT layers in ATS models, and what is expected for GATv2? sent38, sent39, sent40, sent41, sent42
9. What challenges do heterogeneous graphs present, and how are they addressed? sent43, sent44, sent45, sent46
10. What is the role of pooling in GNNs, and how is it achieved? sent47, sent48, sent49
11. What datasets are used for GNNs in ATS, and what is required for their construction? sent50, sent51, sent52","1. How is spatial convolution in GNNs viewed in relation to convolution in neural networks like CNNs? sent1
    1.1. How is 2D convolution applied in the context of images? sent2
        1.1.1. What is the result of applying 2D convolution to a target node in images? sent3
    1.2. How is the idea of convolution generalized to non-regular grids? sent4
    1.3. What challenges arise when extending convolution to graphs? sent5, sent6

3. What is message passing in modern GNNs, and how does it function? sent9, sent10
    3.1. How does message passing affect information propagation in graphs? sent11, sent12
    3.2. How is the concept of receptive field in CNNs related to message passing in GNNs? sent13

4. How is message passing formally defined, and what are its core components? sent14, sent15, sent16
    4.1. What role does the permutation-invariant reduction function play in message passing? sent17, sent18
    4.2. How are reduced messages combined with the previous state in message passing? sent19
    4.3. How can message passing be utilized for learnable layers in GNNs? sent20

6. What limitation does the presented convolution layer have, and how is it addressed by GAT? sent24, sent25
    6.1. How does the introduction of attention scores in GAT affect spatial convolution? sent26, sent27
    6.2. How is GAT formulated in relation to multi-head attention? sent28, sent29","4. How is message passing formally defined, and what are its core components? sent14, sent15, sent16
    4.1. What role does the permutation-invariant reduction function play in message passing? sent17, sent18
    4.2. How are reduced messages combined with the previous state in message passing? sent19
    4.3. How can message passing be utilized for learnable layers in GNNs? sent20","Question: How is message passing utilized in GNNs to enable learnable layers and what are its core components?

Supporting sentence: sent14, sent15, sent16, sent17, sent18, sent19, sent20",How is message passing utilized in GNNs to enable learnable layers and what are its core components?,"sent14, sent15, sent16, sent17, sent18, sent19, sent20",2,implement,"Message passing in GNNs is utilized to enable learnable layers by allowing nodes within the graph to exchange messages with their neighbors for a number of iterations.

This process diffuses information throughout the graph, and the more iterations, the further outwards information is propagated.

Formally, message passing is defined for each time step as two equations.

The first equation describes how messages are generated using a differentiable function that generates messages for each edge connecting nodes using the node features and edge feature present.

The second equation describes how each node feature is updated by applying a permutation-invariant reduction function that aggregates all incoming messages to a node.

Then, another differentiable function combines the reduced messages received with the previous state.

The convolution layer for a GNN is defined with a learnable weight, where the message per edge is the node feature, and the aggregation is the normalized sum of messages.

The reduction function, in this case a sum function, must be permutation-invariant to handle the unordered nature of graphs.","Question:

How is message passing utilized in GNNs to enable learnable layers and what are its core components?

Answer:

Message passing in GNNs is utilized to enable learnable layers by allowing nodes within the graph to exchange messages with their neighbors for a number of iterations.

This process diffuses information throughout the graph, and the more iterations, the further outwards information is propagated.

Formally, message passing is defined for each time step as two equations.

The first equation describes how messages are generated using a differentiable function that generates messages for each edge connecting nodes using the node features and edge feature present.

The second equation describes how each node feature is updated by applying a permutation-invariant reduction function that aggregates all incoming messages to a node.

Then, another differentiable function combines the reduced messages received with the previous state.

The convolution layer for a GNN is defined with a learnable weight, where the message per edge is the node feature, and the aggregation is the normalized sum of messages.

The reduction function, in this case a sum function, must be permutation-invariant to handle the unordered nature of graphs."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,Spatial Convolution and Message Passing,4,"One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.

Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:

This first equation describes how messages are generated. A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.

The above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function ρ. This function aggregates all incoming messages to a node. Then another differen-tiable function ψ combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.

The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.

where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.

The above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1

where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with

. This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows

). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.

There are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.

In ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.

Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.

We want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","sent1: One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs.
sent2: As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image.
sent3: The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node.
sent4: Generalizing this idea to a non-regular grids leads to spatial convolution.
sent5: However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation.
sent6: The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.
sent7: Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme.
sent8: However, such a description suffers from scalability issues due to it directly operating over the entire graph.
sent9: As such modern GNNs use, what is commonly referred to as, message passing.
sent10: In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations.
sent11: Thereby the network is able to diffuse information throughout the graph.
sent12: Consequently, the more iterations, the further outwards information is propagated throughout the graph.
sent13: In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution.
sent14: Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:This first equation describes how messages are generated.
sent15: A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.
sent16: The above equation is the core of the message passing framework and describes how each node feature is updated.
sent17: The first part consists in the application of a permutation-invariant reduction function ρ.
sent18: This function aggregates all incoming messages to a node.
sent19: Then another differen-tiable function ψ combines the reduced messages received with the previous state.
sent20: Using these two equations one can utilize message passing for learnable layers.
sent21: The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1
sent22: i,j = x t j and the aggregation is the normalized sum of messages, i.e.where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree.
sent23: Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.
sent24: The above presented convolution layer does not allow the model to filter unimportant neighbours.
sent25: Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour.
sent26: A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure.
sent27: The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages.
sent28: Just as in transformers GAT is formulated with multi-head attention.
sent29: The modification to the previously presented convolution layer follows closely the common attention formulation.
sent30: Formally, x t+1where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads.
sent31: The attention scores are computed with.
sent32: This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ).
sent33: We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2.
sent34: Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be.
sent35: The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity.
sent36: Taking the above description the attention score for GATv2 is modified as follows).
sent37: In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.
sent38: There are numerous modifications and extensions to the basic convolution presented here.
sent39: However, for ATS models, GAT layers are dominating as the workhorse for most models.
sent40: The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST.
sent41: We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT.
sent42: Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.
sent43: In ATS the graphs used are in nearly all cases not homogeneous.
sent44: However, the equations presented here do not work for heterogeneous graphs.
sent45: The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph.
sent46: In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.
sent47: Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models.
sent48: Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes.
sent49: This is usually done with some function f where f is commonly the mean, max or sum.
sent50: We want to explicitly point out to the reader that the construction of GNNs does not require special datasets.
sent51: All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail.
sent52: The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","1. How is spatial convolution in GNNs viewed in relation to convolution in neural networks like CNNs? sent1
    1.1. How is 2D convolution applied in the context of images? sent2
        1.1.1. What is the result of applying 2D convolution to a target node in images? sent3
    1.2. How is the idea of convolution generalized to non-regular grids? sent4
    1.3. What challenges arise when extending convolution to graphs? sent5, sent6
2. What is the mathematical formulation of convolution in GNNs, and what issues does it face? sent7, sent8
3. What is message passing in modern GNNs, and how does it function? sent9, sent10
    3.1. How does message passing affect information propagation in graphs? sent11, sent12
    3.2. How is the concept of receptive field in CNNs related to message passing in GNNs? sent13
4. How is message passing formally defined, and what are its core components? sent14, sent15, sent16
    4.1. What role does the permutation-invariant reduction function play in message passing? sent17, sent18
    4.2. How are reduced messages combined with the previous state in message passing? sent19
    4.3. How can message passing be utilized for learnable layers in GNNs? sent20
5. How is the convolution layer defined in GNNs, and what is important about the reduction function? sent21, sent22, sent23
6. What limitation does the presented convolution layer have, and how is it addressed by GAT? sent24, sent25
    6.1. How does the introduction of attention scores in GAT affect spatial convolution? sent26, sent27
    6.2. How is GAT formulated in relation to multi-head attention? sent28, sent29
7. What modification does GATv2 introduce, and what is its impact? sent30, sent31, sent32, sent33, sent34, sent35, sent36, sent37
8. What is the current status of GAT layers in ATS models, and what is expected for GATv2? sent38, sent39, sent40, sent41, sent42
9. What challenges do heterogeneous graphs present, and how are they addressed? sent43, sent44, sent45, sent46
10. What is the role of pooling in GNNs, and how is it achieved? sent47, sent48, sent49
11. What datasets are used for GNNs in ATS, and what is required for their construction? sent50, sent51, sent52","1. How is spatial convolution in GNNs viewed in relation to convolution in neural networks like CNNs? sent1
    1.1. How is 2D convolution applied in the context of images? sent2
        1.1.1. What is the result of applying 2D convolution to a target node in images? sent3
    1.2. How is the idea of convolution generalized to non-regular grids? sent4
    1.3. What challenges arise when extending convolution to graphs? sent5, sent6

3. What is message passing in modern GNNs, and how does it function? sent9, sent10
    3.1. How does message passing affect information propagation in graphs? sent11, sent12
    3.2. How is the concept of receptive field in CNNs related to message passing in GNNs? sent13

4. How is message passing formally defined, and what are its core components? sent14, sent15, sent16
    4.1. What role does the permutation-invariant reduction function play in message passing? sent17, sent18
    4.2. How are reduced messages combined with the previous state in message passing? sent19
    4.3. How can message passing be utilized for learnable layers in GNNs? sent20

6. What limitation does the presented convolution layer have, and how is it addressed by GAT? sent24, sent25
    6.1. How does the introduction of attention scores in GAT affect spatial convolution? sent26, sent27
    6.2. How is GAT formulated in relation to multi-head attention? sent28, sent29","6. What limitation does the presented convolution layer have, and how is it addressed by GAT? sent24, sent25
    6.1. How does the introduction of attention scores in GAT affect spatial convolution? sent26, sent27
    6.2. How is GAT formulated in relation to multi-head attention? sent28, sent29","Question: How do graph attention networks (GAT) enhance spatial convolution in graph neural networks?

Supporting sentence: sent24, sent25, sent26, sent27",How do graph attention networks (GAT) enhance spatial convolution in graph neural networks?,"sent24, sent25, sent26, sent27",2,implement,"Graph attention networks (GAT) enhance spatial convolution in graph neural networks by assigning attention scores to each neighbor.

The introduction of attention scores allows the model to explicitly assign importance to certain nodes and their messages.

Just as in transformers, GAT is formulated with multi-head attention.

The modification to the previously presented convolution layer follows closely the common attention formulation.

Formally, the attention score between node i and node j is denoted as α i,j, and K denotes the number of concatenated heads.

The attention scores are computed and then normalized to obtain the attention score per edge α i,j = softmax i (r i,j).

A recent development, GATv2, improves upon this by separating the learnable parameters a and W, thus achieving more expressive power while not increasing computational complexity.

In both synthetic and real datasets, this modification shows superior performance, supported by a theoretical analysis.","Question:

How do graph attention networks (GAT) enhance spatial convolution in graph neural networks?

Answer:

Graph attention networks (GAT) enhance spatial convolution in graph neural networks by assigning attention scores to each neighbor.

The introduction of attention scores allows the model to explicitly assign importance to certain nodes and their messages.

Just as in transformers, GAT is formulated with multi-head attention.

The modification to the previously presented convolution layer follows closely the common attention formulation.

Formally, the attention score between node i and node j is denoted as α i,j, and K denotes the number of concatenated heads.

The attention scores are computed and then normalized to obtain the attention score per edge α i,j = softmax i (r i,j).

A recent development, GATv2, improves upon this by separating the learnable parameters a and W, thus achieving more expressive power while not increasing computational complexity.

In both synthetic and real datasets, this modification shows superior performance, supported by a theoretical analysis."
236460241,A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,https://www.semanticscholar.org/paper/ee6d66efc86746d42ace14db30fcbaf9d3380e25,"Code-switching, Borrowing, Transfer, Loan Translation",5,"While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruöz and Backus, 2009). 4.İlkokul-uİstanbul-da yap-tı-m.

primary.school-ACCİstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher après 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","sent1: While C-S implies active alternation between grammatical systems, borrowing does not.
sent2: It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).
sent3: When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).
sent4: Similarly, what looks like complex C-S may not be perceived as switching at all.
sent5: Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.
sent6: These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two.
sent7: Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate.
sent8: Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.
sent9: In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruöz and Backus, 2009).
sent10: 4.İlkokul-uİstanbul-da yap-tı-m.primary.school-ACCİstanbul-LOC do-past-1sg.
sent11: 'I finished primary school in Istanbul.'
sent12: In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.
sent13: Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French.
sent14: In Brussels French (Belgium), the construction chercher après 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'.
sent15: In reference French (France), there is normally no particle following the verb.
sent16: The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","1. How does code-switching (C-S) differ from borrowing? sent1
2. Why is it difficult to determine if a lone word insertion is a borrowing or a C-S? sent2
3. What are most lone-item insertions analyzable as, according to analyses? sent3
4. How might complex C-S be perceived? sent4
5. What continuum of mixing types does Auer (1999) distinguish? sent5
    5.1. How can the latter types of mixing be modeled? sent6
    5.2. How does Bilaniuk (2004) describe the Surzhyk spoken by urban Russian-Ukrainian bilinguals? sent7
6. What do loan translation and transfer involve? sent8
    6.1. Can you provide an example of loan translation involving Turkish and Dutch? sent9, sent10, sent11
    6.2. How does transfer differ from loan translation? sent12
        6.2.1. Can you provide an example of transfer involving verb particles from Germanic languages into French? sent13, sent14, sent15
7. What is unknown about the presence of linguistic features like loan translation and transfer alongside C-S? sent16","6. What do loan translation and transfer involve? sent8
    6.1. Can you provide an example of loan translation involving Turkish and Dutch? sent9, sent10, sent11
    6.2. How does transfer differ from loan translation? sent12
        6.2.1. Can you provide an example of transfer involving verb particles from Germanic languages into French? sent13, sent14, sent15

    6.2. How does transfer differ from loan translation? sent12
        6.2.1. Can you provide an example of transfer involving verb particles from Germanic languages into French? sent13, sent14, sent15","6. What do loan translation and transfer involve? sent8
    6.1. Can you provide an example of loan translation involving Turkish and Dutch? sent9, sent10, sent11
    6.2. How does transfer differ from loan translation? sent12
        6.2.1. Can you provide an example of transfer involving verb particles from Germanic languages into French? sent13, sent14, sent15","Question: How do loan translation and transfer manifest in multilingual contexts, and how do they differ from each other?

Supporting sentence: sent8, sent9, sent10, sent11, sent12, sent13, sent14, sent15","How do loan translation and transfer manifest in multilingual contexts, and how do they differ from each other?","sent8, sent9, sent10, sent11, sent12, sent13, sent14, sent15",3,implement,"Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.

In loan translation, a word or phrase from one language is translated literally into another language, maintaining the original meaning.

For example, in Turkish spoken in the Netherlands, the Turkish verb ""yapmak,"" meaning 'to do,' takes on the Dutch meaning of ""doen"" (Dogruöz and Backus, 2009).

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.

Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French.

In Brussels French (Belgium), the construction ""chercher après"" ('look after') is a translation of the Dutch equivalent, and in Ontario French (Canada), ""chercher pour"" is the translation equivalent of English 'look for'.

In reference French (France), there is normally no particle following the verb.","Question:

How do loan translation and transfer manifest in multilingual contexts, and how do they differ from each other?

Answer:

Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.

In loan translation, a word or phrase from one language is translated literally into another language, maintaining the original meaning.

For example, in Turkish spoken in the Netherlands, the Turkish verb ""yapmak,"" meaning 'to do,' takes on the Dutch meaning of ""doen"" (Dogruöz and Backus, 2009).

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.

Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French.

In Brussels French (Belgium), the construction ""chercher après"" ('look after') is a translation of the Dutch equivalent, and in Ontario French (Canada), ""chercher pour"" is the translation equivalent of English 'look for'.

In reference French (France), there is normally no particle following the verb."
254877753,Towards Reasoning in Large Language Models: A Survey,https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,What is Reasoning?,8,"Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018). Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information. Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things. To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:

Deductive reasoning. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true. For example:

• Premise: All mammals have kidneys. • Premise: All whales are mammals. • Conclusion: All whales have kidneys.

Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain. For example:

• Observation: Every time we see a creature with wings, it is a bird. • Observation: We see a creature with wings. • Conclusion: The creature is likely to be a bird.

Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain. For example:

• Observation: The car cannot start and there is a puddle of liquid under the engine. • Conclusion: The most likely explanation is that the car has a leak in the radiator.

Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.

Formal Reasoning vs Informal Reasoning. Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic. Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life. Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable. We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.

Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails. In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia). Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia). In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.","sent1: Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018).
sent2: Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information.
sent3: Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things.
sent4: To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:Deductive reasoning.
sent5: Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises.
sent6: In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true.
sent7: For example:• Premise: All mammals have kidneys.
sent8: • Premise: All whales are mammals.
sent9: • Conclusion: All whales have kidneys.
sent10: Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence.
sent11: The conclusion is likely to be true based on the available evidence, but it is not necessarily certain.
sent12: For example:• Observation: Every time we see a creature with wings, it is a bird.
sent13: • Observation: We see a creature with wings.
sent14: • Conclusion: The creature is likely to be a bird.
sent15: Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations.
sent16: The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain.
sent17: For example:• Observation: The car cannot start and there is a puddle of liquid under the engine.
sent18: • Conclusion: The most likely explanation is that the car has a leak in the radiator.
sent19: Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.
sent20: Formal Reasoning vs Informal Reasoning.
sent21: Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic.
sent22: Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life.
sent23: Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable.
sent24: We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.
sent25: Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails.
sent26: In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia).
sent27: Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia).
sent28: In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.","1. What is reasoning, and how is it defined? sent1
    1.1. What does reasoning involve? sent2
    1.2. How is the term ""reasoning"" commonly used, and what does it refer to? sent3
    1.3. What are the main categories of reasoning recognized in the literature? sent4
        1.3.1. What is deductive reasoning? sent5
            1.3.1.1. How does deductive reasoning ensure the truth of the conclusion? sent6
            1.3.1.2. Can you provide an example of deductive reasoning? sent7, sent8, sent9
        1.3.2. What is inductive reasoning? sent10
            1.3.2.1. How certain is the conclusion in inductive reasoning? sent11
            1.3.2.2. Can you provide an example of inductive reasoning? sent12, sent13, sent14
        1.3.3. What is abductive reasoning? sent15
            1.3.3.1. How certain is the conclusion in abductive reasoning? sent16
            1.3.3.2. Can you provide an example of abductive reasoning? sent17, sent18
        1.3.4. What are other types of reasoning mentioned? sent19
2. What is the distinction between formal and informal reasoning? sent20
    2.1. How is formal reasoning characterized? sent21
    2.2. How is informal reasoning characterized? sent22
    2.3. What are the differences in reliability and adaptability between formal and informal reasoning? sent23
    2.4. Where can one find a detailed distinction between formal and informal reasoning? sent24
3. How is reasoning in language models described in the literature? sent25
    3.1. How is the term ""reasoning"" often used in the context of language models? sent26
    3.2. What forms of reasoning may be used in language models, and how do they vary? sent27
    3.3. What is the focus of this paper regarding reasoning in large language models? sent28","1. What is reasoning, and how is it defined? sent1
    1.1. What does reasoning involve? sent2
    1.2. How is the term ""reasoning"" commonly used, and what does it refer to? sent3
    1.3. What are the main categories of reasoning recognized in the literature? sent4
        1.3.1. What is deductive reasoning? sent5
            1.3.1.1. How does deductive reasoning ensure the truth of the conclusion? sent6
            1.3.1.2. Can you provide an example of deductive reasoning? sent7, sent8, sent9
        1.3.2. What is inductive reasoning? sent10
            1.3.2.1. How certain is the conclusion in inductive reasoning? sent11
            1.3.2.2. Can you provide an example of inductive reasoning? sent12, sent13, sent14
        1.3.3. What is abductive reasoning? sent15
            1.3.3.1. How certain is the conclusion in abductive reasoning? sent16
            1.3.3.2. Can you provide an example of abductive reasoning? sent17, sent18
        1.3.4. What are other types of reasoning mentioned? sent19

    1.3. What are the main categories of reasoning recognized in the literature? sent4
        1.3.1. What is deductive reasoning? sent5
            1.3.1.1. How does deductive reasoning ensure the truth of the conclusion? sent6
            1.3.1.2. Can you provide an example of deductive reasoning? sent7, sent8, sent9
        1.3.2. What is inductive reasoning? sent10
            1.3.2.1. How certain is the conclusion in inductive reasoning? sent11
            1.3.2.2. Can you provide an example of inductive reasoning? sent12, sent13, sent14
        1.3.3. What is abductive reasoning? sent15
            1.3.3.1. How certain is the conclusion in abductive reasoning? sent16
            1.3.3.2. Can you provide an example of abductive reasoning? sent17, sent18
        1.3.4. What are other types of reasoning mentioned? sent19

        1.3.1. What is deductive reasoning? sent5
            1.3.1.1. How does deductive reasoning ensure the truth of the conclusion? sent6
            1.3.1.2. Can you provide an example of deductive reasoning? sent7, sent8, sent9

        1.3.2. What is inductive reasoning? sent10
            1.3.2.1. How certain is the conclusion in inductive reasoning? sent11
            1.3.2.2. Can you provide an example of inductive reasoning? sent12, sent13, sent14

        1.3.3. What is abductive reasoning? sent15
            1.3.3.1. How certain is the conclusion in abductive reasoning? sent16
            1.3.3.2. Can you provide an example of abductive reasoning? sent17, sent18

2. What is the distinction between formal and informal reasoning? sent20
    2.1. How is formal reasoning characterized? sent21
    2.2. How is informal reasoning characterized? sent22
    2.3. What are the differences in reliability and adaptability between formal and informal reasoning? sent23
    2.4. Where can one find a detailed distinction between formal and informal reasoning? sent24

3. How is reasoning in language models described in the literature? sent25
    3.1. How is the term ""reasoning"" often used in the context of language models? sent26
    3.2. What forms of reasoning may be used in language models, and how do they vary? sent27
    3.3. What is the focus of this paper regarding reasoning in large language models? sent28","        1.3.1. What is deductive reasoning? sent5
            1.3.1.1. How does deductive reasoning ensure the truth of the conclusion? sent6
            1.3.1.2. Can you provide an example of deductive reasoning? sent7, sent8, sent9","Question: How does deductive reasoning ensure the truth of the conclusion, and can you provide an example?

Supporting sentence: sent6, sent7, sent8, sent9","How does deductive reasoning ensure the truth of the conclusion, and can you provide an example?","sent6, sent7, sent8, sent9",1,implement,"Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises.

In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true.

For example: • Premise: All mammals have kidneys.

• Premise: All whales are mammals.

• Conclusion: All whales have kidneys.","Question:

How does deductive reasoning ensure the truth of the conclusion, and can you provide an example?

Answer:

Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises.

In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true.

For example: • Premise: All mammals have kidneys.

• Premise: All whales are mammals.

• Conclusion: All whales have kidneys."
254877753,Towards Reasoning in Large Language Models: A Survey,https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,What is Reasoning?,8,"Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018). Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information. Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things. To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:

Deductive reasoning. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true. For example:

• Premise: All mammals have kidneys. • Premise: All whales are mammals. • Conclusion: All whales have kidneys.

Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain. For example:

• Observation: Every time we see a creature with wings, it is a bird. • Observation: We see a creature with wings. • Conclusion: The creature is likely to be a bird.

Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain. For example:

• Observation: The car cannot start and there is a puddle of liquid under the engine. • Conclusion: The most likely explanation is that the car has a leak in the radiator.

Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.

Formal Reasoning vs Informal Reasoning. Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic. Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life. Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable. We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.

Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails. In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia). Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia). In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.","sent1: Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018).
sent2: Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information.
sent3: Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things.
sent4: To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:Deductive reasoning.
sent5: Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises.
sent6: In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true.
sent7: For example:• Premise: All mammals have kidneys.
sent8: • Premise: All whales are mammals.
sent9: • Conclusion: All whales have kidneys.
sent10: Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence.
sent11: The conclusion is likely to be true based on the available evidence, but it is not necessarily certain.
sent12: For example:• Observation: Every time we see a creature with wings, it is a bird.
sent13: • Observation: We see a creature with wings.
sent14: • Conclusion: The creature is likely to be a bird.
sent15: Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations.
sent16: The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain.
sent17: For example:• Observation: The car cannot start and there is a puddle of liquid under the engine.
sent18: • Conclusion: The most likely explanation is that the car has a leak in the radiator.
sent19: Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.
sent20: Formal Reasoning vs Informal Reasoning.
sent21: Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic.
sent22: Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life.
sent23: Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable.
sent24: We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.
sent25: Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails.
sent26: In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia).
sent27: Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia).
sent28: In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.","1. What is reasoning, and how is it defined? sent1
    1.1. What does reasoning involve? sent2
    1.2. How is the term ""reasoning"" commonly used, and what does it refer to? sent3
    1.3. What are the main categories of reasoning recognized in the literature? sent4
        1.3.1. What is deductive reasoning? sent5
            1.3.1.1. How does deductive reasoning ensure the truth of the conclusion? sent6
            1.3.1.2. Can you provide an example of deductive reasoning? sent7, sent8, sent9
        1.3.2. What is inductive reasoning? sent10
            1.3.2.1. How certain is the conclusion in inductive reasoning? sent11
            1.3.2.2. Can you provide an example of inductive reasoning? sent12, sent13, sent14
        1.3.3. What is abductive reasoning? sent15
            1.3.3.1. How certain is the conclusion in abductive reasoning? sent16
            1.3.3.2. Can you provide an example of abductive reasoning? sent17, sent18
        1.3.4. What are other types of reasoning mentioned? sent19
2. What is the distinction between formal and informal reasoning? sent20
    2.1. How is formal reasoning characterized? sent21
    2.2. How is informal reasoning characterized? sent22
    2.3. What are the differences in reliability and adaptability between formal and informal reasoning? sent23
    2.4. Where can one find a detailed distinction between formal and informal reasoning? sent24
3. How is reasoning in language models described in the literature? sent25
    3.1. How is the term ""reasoning"" often used in the context of language models? sent26
    3.2. What forms of reasoning may be used in language models, and how do they vary? sent27
    3.3. What is the focus of this paper regarding reasoning in large language models? sent28","1. What is reasoning, and how is it defined? sent1
    1.1. What does reasoning involve? sent2
    1.2. How is the term ""reasoning"" commonly used, and what does it refer to? sent3
    1.3. What are the main categories of reasoning recognized in the literature? sent4
        1.3.1. What is deductive reasoning? sent5
            1.3.1.1. How does deductive reasoning ensure the truth of the conclusion? sent6
            1.3.1.2. Can you provide an example of deductive reasoning? sent7, sent8, sent9
        1.3.2. What is inductive reasoning? sent10
            1.3.2.1. How certain is the conclusion in inductive reasoning? sent11
            1.3.2.2. Can you provide an example of inductive reasoning? sent12, sent13, sent14
        1.3.3. What is abductive reasoning? sent15
            1.3.3.1. How certain is the conclusion in abductive reasoning? sent16
            1.3.3.2. Can you provide an example of abductive reasoning? sent17, sent18
        1.3.4. What are other types of reasoning mentioned? sent19

    1.3. What are the main categories of reasoning recognized in the literature? sent4
        1.3.1. What is deductive reasoning? sent5
            1.3.1.1. How does deductive reasoning ensure the truth of the conclusion? sent6
            1.3.1.2. Can you provide an example of deductive reasoning? sent7, sent8, sent9
        1.3.2. What is inductive reasoning? sent10
            1.3.2.1. How certain is the conclusion in inductive reasoning? sent11
            1.3.2.2. Can you provide an example of inductive reasoning? sent12, sent13, sent14
        1.3.3. What is abductive reasoning? sent15
            1.3.3.1. How certain is the conclusion in abductive reasoning? sent16
            1.3.3.2. Can you provide an example of abductive reasoning? sent17, sent18
        1.3.4. What are other types of reasoning mentioned? sent19

        1.3.1. What is deductive reasoning? sent5
            1.3.1.1. How does deductive reasoning ensure the truth of the conclusion? sent6
            1.3.1.2. Can you provide an example of deductive reasoning? sent7, sent8, sent9

        1.3.2. What is inductive reasoning? sent10
            1.3.2.1. How certain is the conclusion in inductive reasoning? sent11
            1.3.2.2. Can you provide an example of inductive reasoning? sent12, sent13, sent14

        1.3.3. What is abductive reasoning? sent15
            1.3.3.1. How certain is the conclusion in abductive reasoning? sent16
            1.3.3.2. Can you provide an example of abductive reasoning? sent17, sent18

2. What is the distinction between formal and informal reasoning? sent20
    2.1. How is formal reasoning characterized? sent21
    2.2. How is informal reasoning characterized? sent22
    2.3. What are the differences in reliability and adaptability between formal and informal reasoning? sent23
    2.4. Where can one find a detailed distinction between formal and informal reasoning? sent24

3. How is reasoning in language models described in the literature? sent25
    3.1. How is the term ""reasoning"" often used in the context of language models? sent26
    3.2. What forms of reasoning may be used in language models, and how do they vary? sent27
    3.3. What is the focus of this paper regarding reasoning in large language models? sent28","        1.3.2. What is inductive reasoning? sent10
            1.3.2.1. How certain is the conclusion in inductive reasoning? sent11
            1.3.2.2. Can you provide an example of inductive reasoning? sent12, sent13, sent14","Question: How is inductive reasoning characterized, and can you provide an example illustrating its application?

Supporting sentence: sent10, sent11, sent12, sent13, sent14","How is inductive reasoning characterized, and can you provide an example illustrating its application?","sent10, sent11, sent12, sent13, sent14",4,implement,"Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence.

The conclusion is likely to be true based on the available evidence, but it is not necessarily certain.

For example:  • Observation: Every time we see a creature with wings, it is a bird.

• Observation: We see a creature with wings.

• Conclusion: The creature is likely to be a bird.","Question:

How is inductive reasoning characterized, and can you provide an example illustrating its application?

Answer:

Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence.

The conclusion is likely to be true based on the available evidence, but it is not necessarily certain.

For example:  • Observation: Every time we see a creature with wings, it is a bird.

• Observation: We see a creature with wings.

• Conclusion: The creature is likely to be a bird."
254877753,Towards Reasoning in Large Language Models: A Survey,https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,What is Reasoning?,8,"Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018). Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information. Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things. To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:

Deductive reasoning. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true. For example:

• Premise: All mammals have kidneys. • Premise: All whales are mammals. • Conclusion: All whales have kidneys.

Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain. For example:

• Observation: Every time we see a creature with wings, it is a bird. • Observation: We see a creature with wings. • Conclusion: The creature is likely to be a bird.

Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain. For example:

• Observation: The car cannot start and there is a puddle of liquid under the engine. • Conclusion: The most likely explanation is that the car has a leak in the radiator.

Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.

Formal Reasoning vs Informal Reasoning. Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic. Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life. Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable. We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.

Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails. In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia). Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia). In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.","sent1: Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018).
sent2: Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information.
sent3: Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things.
sent4: To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:Deductive reasoning.
sent5: Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises.
sent6: In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true.
sent7: For example:• Premise: All mammals have kidneys.
sent8: • Premise: All whales are mammals.
sent9: • Conclusion: All whales have kidneys.
sent10: Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence.
sent11: The conclusion is likely to be true based on the available evidence, but it is not necessarily certain.
sent12: For example:• Observation: Every time we see a creature with wings, it is a bird.
sent13: • Observation: We see a creature with wings.
sent14: • Conclusion: The creature is likely to be a bird.
sent15: Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations.
sent16: The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain.
sent17: For example:• Observation: The car cannot start and there is a puddle of liquid under the engine.
sent18: • Conclusion: The most likely explanation is that the car has a leak in the radiator.
sent19: Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.
sent20: Formal Reasoning vs Informal Reasoning.
sent21: Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic.
sent22: Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life.
sent23: Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable.
sent24: We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.
sent25: Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails.
sent26: In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia).
sent27: Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia).
sent28: In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.","1. What is reasoning, and how is it defined? sent1
    1.1. What does reasoning involve? sent2
    1.2. How is the term ""reasoning"" commonly used, and what does it refer to? sent3
    1.3. What are the main categories of reasoning recognized in the literature? sent4
        1.3.1. What is deductive reasoning? sent5
            1.3.1.1. How does deductive reasoning ensure the truth of the conclusion? sent6
            1.3.1.2. Can you provide an example of deductive reasoning? sent7, sent8, sent9
        1.3.2. What is inductive reasoning? sent10
            1.3.2.1. How certain is the conclusion in inductive reasoning? sent11
            1.3.2.2. Can you provide an example of inductive reasoning? sent12, sent13, sent14
        1.3.3. What is abductive reasoning? sent15
            1.3.3.1. How certain is the conclusion in abductive reasoning? sent16
            1.3.3.2. Can you provide an example of abductive reasoning? sent17, sent18
        1.3.4. What are other types of reasoning mentioned? sent19
2. What is the distinction between formal and informal reasoning? sent20
    2.1. How is formal reasoning characterized? sent21
    2.2. How is informal reasoning characterized? sent22
    2.3. What are the differences in reliability and adaptability between formal and informal reasoning? sent23
    2.4. Where can one find a detailed distinction between formal and informal reasoning? sent24
3. How is reasoning in language models described in the literature? sent25
    3.1. How is the term ""reasoning"" often used in the context of language models? sent26
    3.2. What forms of reasoning may be used in language models, and how do they vary? sent27
    3.3. What is the focus of this paper regarding reasoning in large language models? sent28","1. What is reasoning, and how is it defined? sent1
    1.1. What does reasoning involve? sent2
    1.2. How is the term ""reasoning"" commonly used, and what does it refer to? sent3
    1.3. What are the main categories of reasoning recognized in the literature? sent4
        1.3.1. What is deductive reasoning? sent5
            1.3.1.1. How does deductive reasoning ensure the truth of the conclusion? sent6
            1.3.1.2. Can you provide an example of deductive reasoning? sent7, sent8, sent9
        1.3.2. What is inductive reasoning? sent10
            1.3.2.1. How certain is the conclusion in inductive reasoning? sent11
            1.3.2.2. Can you provide an example of inductive reasoning? sent12, sent13, sent14
        1.3.3. What is abductive reasoning? sent15
            1.3.3.1. How certain is the conclusion in abductive reasoning? sent16
            1.3.3.2. Can you provide an example of abductive reasoning? sent17, sent18
        1.3.4. What are other types of reasoning mentioned? sent19

    1.3. What are the main categories of reasoning recognized in the literature? sent4
        1.3.1. What is deductive reasoning? sent5
            1.3.1.1. How does deductive reasoning ensure the truth of the conclusion? sent6
            1.3.1.2. Can you provide an example of deductive reasoning? sent7, sent8, sent9
        1.3.2. What is inductive reasoning? sent10
            1.3.2.1. How certain is the conclusion in inductive reasoning? sent11
            1.3.2.2. Can you provide an example of inductive reasoning? sent12, sent13, sent14
        1.3.3. What is abductive reasoning? sent15
            1.3.3.1. How certain is the conclusion in abductive reasoning? sent16
            1.3.3.2. Can you provide an example of abductive reasoning? sent17, sent18
        1.3.4. What are other types of reasoning mentioned? sent19

        1.3.1. What is deductive reasoning? sent5
            1.3.1.1. How does deductive reasoning ensure the truth of the conclusion? sent6
            1.3.1.2. Can you provide an example of deductive reasoning? sent7, sent8, sent9

        1.3.2. What is inductive reasoning? sent10
            1.3.2.1. How certain is the conclusion in inductive reasoning? sent11
            1.3.2.2. Can you provide an example of inductive reasoning? sent12, sent13, sent14

        1.3.3. What is abductive reasoning? sent15
            1.3.3.1. How certain is the conclusion in abductive reasoning? sent16
            1.3.3.2. Can you provide an example of abductive reasoning? sent17, sent18

2. What is the distinction between formal and informal reasoning? sent20
    2.1. How is formal reasoning characterized? sent21
    2.2. How is informal reasoning characterized? sent22
    2.3. What are the differences in reliability and adaptability between formal and informal reasoning? sent23
    2.4. Where can one find a detailed distinction between formal and informal reasoning? sent24

3. How is reasoning in language models described in the literature? sent25
    3.1. How is the term ""reasoning"" often used in the context of language models? sent26
    3.2. What forms of reasoning may be used in language models, and how do they vary? sent27
    3.3. What is the focus of this paper regarding reasoning in large language models? sent28","        1.3.3. What is abductive reasoning? sent15
            1.3.3.1. How certain is the conclusion in abductive reasoning? sent16
            1.3.3.2. Can you provide an example of abductive reasoning? sent17, sent18","Question: Can you provide an example of abductive reasoning and explain the certainty of its conclusion?

Supporting sentence: sent15, sent16, sent17, sent18",Can you provide an example of abductive reasoning and explain the certainty of its conclusion?,"sent15, sent16, sent17, sent18",2,implement,"Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations.

For example, if the observation is that the car cannot start and there is a puddle of liquid under the engine, the conclusion might be that the most likely explanation is that the car has a leak in the radiator.

The conclusion in abductive reasoning is the most likely explanation based on the available evidence, but it is not necessarily certain.","Question:

Can you provide an example of abductive reasoning and explain the certainty of its conclusion?

Answer:

Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations.

For example, if the observation is that the car cannot start and there is a puddle of liquid under the engine, the conclusion might be that the most likely explanation is that the car has a leak in the radiator.

The conclusion in abductive reasoning is the most likely explanation based on the available evidence, but it is not necessarily certain."
253736389,Transformers for Tabular Data Representation: A Survey of Models and Applications,https://www.semanticscholar.org/paper/18ff1542d5a2a4490c7b3f21522bf1343889f700,Training Datasets,5,"We present both the datasets used for pre-training and for fine-tuning in the downstream tasks. Pre-training tables are not annotated, in some cases scraped from the web, while data used for fine-tuning have task-dependent annotation labels. The datasets consist of tables and their context, such as table metadata, surrounding texts, claims or questions. To construct large pre-training datasets and in an attempt to reduce bias, multiple sources can be used, independently of the target task at hand. For instance, unlike TAPAS (Herzig et al., 2020), which only uses Wikipedia Tables for QA, and TABULARNET , which only uses Spreadsheets for TMP, TABERT  uses Wikipedia Tables and WDC  for SP; GRAPPA uses Wikipedia Tables, Spider, and WikiSQL for SP; MMR (Kostić et al., 2021) uses NQ, OTT-QA, and WikiSQL for TR; MATE  uses Wikipedia Tables and HybridQA for QA; and TUTA  uses Wikipedia Tables, WDC, and spreadsheets for TMP. In general, it is recommended to utilize different data sources for pre-training to ensure covering different kinds and content, and thus, improve the scope of representations. For instance, Wikipedia tables has a large number of relational tables (Bhagavatula et al., 2015), while WDC and Spreadsheets include also entity tables and spreadsheets with complex structure. Table 2 summarizes the main characteristics for the most common datasets. We mark the tasks for which the dataset has been used by ✔ under the column ''Task''. We note that the top four datasets are mostly used for pre-training, while the others can be used for fine-tuning as well since they include annotations for the target task, for example, questions/answers for QA. The column ''Large Tables'' is a binary indicator, where ✔ and ✘ indicate whether or not the tabular corpus include large tables and hence whether or not some pre-processing is needed to reduce table content to meet the limits of the transformer architecture (512 input tokens in most cases). Some works, such as TABERT, TAPEX (Liu et al., 2021a), and CLTR (Pan et al., 2021), apply filtering in any case to reduce noisy input. Finally, the ''Context'' column describes additional text that come with the tables. This can be text describing the table, such as caption or title of the document containing the table; table metadata, such as table orientation, header row, and keys; or questions and claims that can be addressed with the table.","sent1: We present both the datasets used for pre-training and for fine-tuning in the downstream tasks.
sent2: Pre-training tables are not annotated, in some cases scraped from the web, while data used for fine-tuning have task-dependent annotation labels.
sent3: The datasets consist of tables and their context, such as table metadata, surrounding texts, claims or questions.
sent4: To construct large pre-training datasets and in an attempt to reduce bias, multiple sources can be used, independently of the target task at hand.
sent5: For instance, unlike TAPAS (Herzig et al., 2020), which only uses Wikipedia Tables for QA, and TABULARNET , which only uses Spreadsheets for TMP, TABERT  uses Wikipedia Tables and WDC  for SP; GRAPPA uses Wikipedia Tables, Spider, and WikiSQL for SP; MMR (Kostić et al., 2021) uses NQ, OTT-QA, and WikiSQL for TR; MATE  uses Wikipedia Tables and HybridQA for QA; and TUTA  uses Wikipedia Tables, WDC, and spreadsheets for TMP.
sent6: In general, it is recommended to utilize different data sources for pre-training to ensure covering different kinds and content, and thus, improve the scope of representations.
sent7: For instance, Wikipedia tables has a large number of relational tables (Bhagavatula et al., 2015), while WDC and Spreadsheets include also entity tables and spreadsheets with complex structure.
sent8: Table 2 summarizes the main characteristics for the most common datasets.
sent9: We mark the tasks for which the dataset has been used by ✔ under the column ''Task''.
sent10: We note that the top four datasets are mostly used for pre-training, while the others can be used for fine-tuning as well since they include annotations for the target task, for example, questions/answers for QA.
sent11: The column ''Large Tables'' is a binary indicator, where ✔ and ✘ indicate whether or not the tabular corpus include large tables and hence whether or not some pre-processing is needed to reduce table content to meet the limits of the transformer architecture (512 input tokens in most cases).
sent12: Some works, such as TABERT, TAPEX (Liu et al., 2021a), and CLTR (Pan et al., 2021), apply filtering in any case to reduce noisy input.
sent13: Finally, the ''Context'' column describes additional text that come with the tables.
sent14: This can be text describing the table, such as caption or title of the document containing the table; table metadata, such as table orientation, header row, and keys; or questions and claims that can be addressed with the table.","1. What types of datasets are presented in the section? sent1
    1.1. How are pre-training and fine-tuning datasets different in terms of annotation? sent2
    1.2. What do the datasets consist of? sent3
    1.3. How can large pre-training datasets be constructed to reduce bias? sent4
        1.3.1. Can you provide examples of different data sources used for pre-training? sent5
        1.3.2. Why is it recommended to use different data sources for pre-training? sent6
            1.3.2.1. What are some characteristics of Wikipedia tables, WDC, and Spreadsheets? sent7
    1.4. What does Table 2 summarize? sent8
        1.4.1. How are tasks marked in Table 2? sent9
        1.4.2. Which datasets are mostly used for pre-training and which for fine-tuning? sent10
        1.4.3. What does the ""Large Tables"" column indicate? sent11
        1.4.4. What is the purpose of filtering in some works like TABERT, TAPEX, and CLTR? sent12
        1.4.5. What information is described in the ""Context"" column? sent13
            1.4.5.1. What types of additional text can accompany the tables? sent14","1. What types of datasets are presented in the section? sent1
    1.1. How are pre-training and fine-tuning datasets different in terms of annotation? sent2
    1.2. What do the datasets consist of? sent3
    1.3. How can large pre-training datasets be constructed to reduce bias? sent4
        1.3.1. Can you provide examples of different data sources used for pre-training? sent5
        1.3.2. Why is it recommended to use different data sources for pre-training? sent6
            1.3.2.1. What are some characteristics of Wikipedia tables, WDC, and Spreadsheets? sent7
    1.4. What does Table 2 summarize? sent8
        1.4.1. How are tasks marked in Table 2? sent9
        1.4.2. Which datasets are mostly used for pre-training and which for fine-tuning? sent10
        1.4.3. What does the ""Large Tables"" column indicate? sent11
        1.4.4. What is the purpose of filtering in some works like TABERT, TAPEX, and CLTR? sent12
        1.4.5. What information is described in the ""Context"" column? sent13
            1.4.5.1. What types of additional text can accompany the tables? sent14

    1.3. How can large pre-training datasets be constructed to reduce bias? sent4
        1.3.1. Can you provide examples of different data sources used for pre-training? sent5
        1.3.2. Why is it recommended to use different data sources for pre-training? sent6
            1.3.2.1. What are some characteristics of Wikipedia tables, WDC, and Spreadsheets? sent7

    1.4. What does Table 2 summarize? sent8
        1.4.1. How are tasks marked in Table 2? sent9
        1.4.2. Which datasets are mostly used for pre-training and which for fine-tuning? sent10
        1.4.3. What does the ""Large Tables"" column indicate? sent11
        1.4.4. What is the purpose of filtering in some works like TABERT, TAPEX, and CLTR? sent12
        1.4.5. What information is described in the ""Context"" column? sent13
            1.4.5.1. What types of additional text can accompany the tables? sent14","    1.3. How can large pre-training datasets be constructed to reduce bias? sent4
        1.3.1. Can you provide examples of different data sources used for pre-training? sent5
        1.3.2. Why is it recommended to use different data sources for pre-training? sent6
            1.3.2.1. What are some characteristics of Wikipedia tables, WDC, and Spreadsheets? sent7","Question: Why is it beneficial to use diverse data sources for pre-training tabular data models?

Supporting sentence: sent4, sent5, sent6, sent7",Why is it beneficial to use diverse data sources for pre-training tabular data models?,"sent4, sent5, sent6, sent7",3,implement,"In general, it is recommended to utilize different data sources for pre-training to ensure covering different kinds and content, and thus, improve the scope of representations.

For instance, Wikipedia tables have a large number of relational tables (Bhagavatula et al., 2015), while WDC and Spreadsheets also include entity tables and spreadsheets with complex structures.","Question:

Why is it beneficial to use diverse data sources for pre-training tabular data models?

Answer:

In general, it is recommended to utilize different data sources for pre-training to ensure covering different kinds and content, and thus, improve the scope of representations.

For instance, Wikipedia tables have a large number of relational tables (Bhagavatula et al., 2015), while WDC and Spreadsheets also include entity tables and spreadsheets with complex structures."
253736389,Transformers for Tabular Data Representation: A Survey of Models and Applications,https://www.semanticscholar.org/paper/18ff1542d5a2a4490c7b3f21522bf1343889f700,Training Datasets,5,"We present both the datasets used for pre-training and for fine-tuning in the downstream tasks. Pre-training tables are not annotated, in some cases scraped from the web, while data used for fine-tuning have task-dependent annotation labels. The datasets consist of tables and their context, such as table metadata, surrounding texts, claims or questions. To construct large pre-training datasets and in an attempt to reduce bias, multiple sources can be used, independently of the target task at hand. For instance, unlike TAPAS (Herzig et al., 2020), which only uses Wikipedia Tables for QA, and TABULARNET , which only uses Spreadsheets for TMP, TABERT  uses Wikipedia Tables and WDC  for SP; GRAPPA uses Wikipedia Tables, Spider, and WikiSQL for SP; MMR (Kostić et al., 2021) uses NQ, OTT-QA, and WikiSQL for TR; MATE  uses Wikipedia Tables and HybridQA for QA; and TUTA  uses Wikipedia Tables, WDC, and spreadsheets for TMP. In general, it is recommended to utilize different data sources for pre-training to ensure covering different kinds and content, and thus, improve the scope of representations. For instance, Wikipedia tables has a large number of relational tables (Bhagavatula et al., 2015), while WDC and Spreadsheets include also entity tables and spreadsheets with complex structure. Table 2 summarizes the main characteristics for the most common datasets. We mark the tasks for which the dataset has been used by ✔ under the column ''Task''. We note that the top four datasets are mostly used for pre-training, while the others can be used for fine-tuning as well since they include annotations for the target task, for example, questions/answers for QA. The column ''Large Tables'' is a binary indicator, where ✔ and ✘ indicate whether or not the tabular corpus include large tables and hence whether or not some pre-processing is needed to reduce table content to meet the limits of the transformer architecture (512 input tokens in most cases). Some works, such as TABERT, TAPEX (Liu et al., 2021a), and CLTR (Pan et al., 2021), apply filtering in any case to reduce noisy input. Finally, the ''Context'' column describes additional text that come with the tables. This can be text describing the table, such as caption or title of the document containing the table; table metadata, such as table orientation, header row, and keys; or questions and claims that can be addressed with the table.","sent1: We present both the datasets used for pre-training and for fine-tuning in the downstream tasks.
sent2: Pre-training tables are not annotated, in some cases scraped from the web, while data used for fine-tuning have task-dependent annotation labels.
sent3: The datasets consist of tables and their context, such as table metadata, surrounding texts, claims or questions.
sent4: To construct large pre-training datasets and in an attempt to reduce bias, multiple sources can be used, independently of the target task at hand.
sent5: For instance, unlike TAPAS (Herzig et al., 2020), which only uses Wikipedia Tables for QA, and TABULARNET , which only uses Spreadsheets for TMP, TABERT  uses Wikipedia Tables and WDC  for SP; GRAPPA uses Wikipedia Tables, Spider, and WikiSQL for SP; MMR (Kostić et al., 2021) uses NQ, OTT-QA, and WikiSQL for TR; MATE  uses Wikipedia Tables and HybridQA for QA; and TUTA  uses Wikipedia Tables, WDC, and spreadsheets for TMP.
sent6: In general, it is recommended to utilize different data sources for pre-training to ensure covering different kinds and content, and thus, improve the scope of representations.
sent7: For instance, Wikipedia tables has a large number of relational tables (Bhagavatula et al., 2015), while WDC and Spreadsheets include also entity tables and spreadsheets with complex structure.
sent8: Table 2 summarizes the main characteristics for the most common datasets.
sent9: We mark the tasks for which the dataset has been used by ✔ under the column ''Task''.
sent10: We note that the top four datasets are mostly used for pre-training, while the others can be used for fine-tuning as well since they include annotations for the target task, for example, questions/answers for QA.
sent11: The column ''Large Tables'' is a binary indicator, where ✔ and ✘ indicate whether or not the tabular corpus include large tables and hence whether or not some pre-processing is needed to reduce table content to meet the limits of the transformer architecture (512 input tokens in most cases).
sent12: Some works, such as TABERT, TAPEX (Liu et al., 2021a), and CLTR (Pan et al., 2021), apply filtering in any case to reduce noisy input.
sent13: Finally, the ''Context'' column describes additional text that come with the tables.
sent14: This can be text describing the table, such as caption or title of the document containing the table; table metadata, such as table orientation, header row, and keys; or questions and claims that can be addressed with the table.","1. What types of datasets are presented in the section? sent1
    1.1. How are pre-training and fine-tuning datasets different in terms of annotation? sent2
    1.2. What do the datasets consist of? sent3
    1.3. How can large pre-training datasets be constructed to reduce bias? sent4
        1.3.1. Can you provide examples of different data sources used for pre-training? sent5
        1.3.2. Why is it recommended to use different data sources for pre-training? sent6
            1.3.2.1. What are some characteristics of Wikipedia tables, WDC, and Spreadsheets? sent7
    1.4. What does Table 2 summarize? sent8
        1.4.1. How are tasks marked in Table 2? sent9
        1.4.2. Which datasets are mostly used for pre-training and which for fine-tuning? sent10
        1.4.3. What does the ""Large Tables"" column indicate? sent11
        1.4.4. What is the purpose of filtering in some works like TABERT, TAPEX, and CLTR? sent12
        1.4.5. What information is described in the ""Context"" column? sent13
            1.4.5.1. What types of additional text can accompany the tables? sent14","1. What types of datasets are presented in the section? sent1
    1.1. How are pre-training and fine-tuning datasets different in terms of annotation? sent2
    1.2. What do the datasets consist of? sent3
    1.3. How can large pre-training datasets be constructed to reduce bias? sent4
        1.3.1. Can you provide examples of different data sources used for pre-training? sent5
        1.3.2. Why is it recommended to use different data sources for pre-training? sent6
            1.3.2.1. What are some characteristics of Wikipedia tables, WDC, and Spreadsheets? sent7
    1.4. What does Table 2 summarize? sent8
        1.4.1. How are tasks marked in Table 2? sent9
        1.4.2. Which datasets are mostly used for pre-training and which for fine-tuning? sent10
        1.4.3. What does the ""Large Tables"" column indicate? sent11
        1.4.4. What is the purpose of filtering in some works like TABERT, TAPEX, and CLTR? sent12
        1.4.5. What information is described in the ""Context"" column? sent13
            1.4.5.1. What types of additional text can accompany the tables? sent14

    1.3. How can large pre-training datasets be constructed to reduce bias? sent4
        1.3.1. Can you provide examples of different data sources used for pre-training? sent5
        1.3.2. Why is it recommended to use different data sources for pre-training? sent6
            1.3.2.1. What are some characteristics of Wikipedia tables, WDC, and Spreadsheets? sent7

    1.4. What does Table 2 summarize? sent8
        1.4.1. How are tasks marked in Table 2? sent9
        1.4.2. Which datasets are mostly used for pre-training and which for fine-tuning? sent10
        1.4.3. What does the ""Large Tables"" column indicate? sent11
        1.4.4. What is the purpose of filtering in some works like TABERT, TAPEX, and CLTR? sent12
        1.4.5. What information is described in the ""Context"" column? sent13
            1.4.5.1. What types of additional text can accompany the tables? sent14","    1.4. What does Table 2 summarize? sent8
        1.4.1. How are tasks marked in Table 2? sent9
        1.4.2. Which datasets are mostly used for pre-training and which for fine-tuning? sent10
        1.4.3. What does the ""Large Tables"" column indicate? sent11
        1.4.4. What is the purpose of filtering in some works like TABERT, TAPEX, and CLTR? sent12
        1.4.5. What information is described in the ""Context"" column? sent13
            1.4.5.1. What types of additional text can accompany the tables? sent14","Question: What types of additional text are described in the ""Context"" column of Table 2?

Supporting sentence: sent13, sent14","What types of additional text are described in the ""Context"" column of Table 2?","sent13, sent14",1,implement,"The ""Context"" column describes additional text that comes with the tables.

This can be text describing the table, such as the caption or title of the document containing the table.

It can also include table metadata, such as table orientation, header row, and keys.

Additionally, it may contain questions and claims that can be addressed with the table.","Question:

What types of additional text are described in the ""Context"" column of Table 2?

Answer:

The ""Context"" column describes additional text that comes with the tables.

This can be text describing the table, such as the caption or title of the document containing the table.

It can also include table metadata, such as table orientation, header row, and keys.

Additionally, it may contain questions and claims that can be addressed with the table."
232320384,A Survey on Multimodal Disinformation Detection,https://www.semanticscholar.org/paper/71d2dc1fc38e0c48c865de5f5c023ccf7c5ad018,Network and Temporal Information,10,"The rationale for leveraging network information stems from early work (Shao et al., 2018;Vosoughi et al., 2018a) that showed that propagation and interaction networks of fake news are deeper and wider than those of real news. Vosoughi et al. (2018a) further found that fake information spreads faster than factual one, thus advocating for the use of temporal information.

Propagation networks can be homogeneous or heterogeneous (e.g., encompassing news articles, publishers, users, and posts) and they can be analyzed at different scales (e.g., node-level, ego-level, triad-level, community-level and the overall network, as shown in Figure 3, in Appendix) (Zhou and Zafarani, 2019). Shu et al. (2020) tackled the fake news classification task by proposing an approach based on hierarchical propagation networks. At both micro-and macro-scale, they extracted and jointly considered network features, temporal features, and linguistic features. Experiments on PolitiFact and GossipCop datasets revealed that temporal features have maximum contribution, followed by network and linguistic features. Shu et al. (2019) provided one of the most thorough multimodal frameworks for fake news classification. Their experimental results suggest that social context (i.e., network-derived) features are more informative than news content ones. Vosoughi et al. (2017) proposed Rumor Gauge, a system that jointly exploits temporal and propagation features, in conjunction with linguistic and user credibility features, for checking the veracity of rumors. In particular, Rumor Gauge leverages text, and network propagation. The temporal modality does not directly provide features, but is instead considered by recomputing all other features at regular time steps, thus yielding multiple time series. Results by Vosoughi et al. (2017) and Kwon et al. (2017) also demonstrated that the contribution of the different data modalities change over time.

To mitigate the ""cold start"" problem of propagation-based early detection of fake news, Liu and Wu (2018) proposed an approach that is primarily based on user and temporal information. First, they built a propagation path of each news as a time series of user representations. The time series for a given news only contains the ordered representations of those users that shared such news. Then, they learned two vector representations of each propagation path via GRUs and CNNs, respectively. Zannettou et al. (2018) analyzed different aspects of memes, such as how they evolve and propagate in different mainstream and fringe web communities, and variants of memes that propa-gate. Finally, Nguyen et al. (2020) proposed Factual News Graph (FANG) to exploit the social structure and the engagement patterns of users for fake news detection.","sent1: The rationale for leveraging network information stems from early work (Shao et al., 2018;Vosoughi et al., 2018a) that showed that propagation and interaction networks of fake news are deeper and wider than those of real news.
sent2: Vosoughi et al. (2018a) further found that fake information spreads faster than factual one, thus advocating for the use of temporal information.
sent3: Propagation networks can be homogeneous or heterogeneous (e.g., encompassing news articles, publishers, users, and posts)
sent4: and they can be analyzed at different scales
sent5: (e.g., node-level, ego-level, triad-level, community-level and the overall network, as shown in Figure 3, in Appendix) (Zhou and Zafarani, 2019).
sent6: Shu et al. (2020) tackled the fake news classification task by proposing an approach based on hierarchical propagation networks.
sent7: At both micro-and macro-scale, they extracted and jointly considered network features, temporal features, and linguistic features.
sent8: Experiments on PolitiFact and GossipCop datasets revealed that temporal features have maximum contribution, followed by network and linguistic features.
sent9: Shu et al. (2019) provided one of the most thorough multimodal frameworks for fake news classification.
sent10: Their experimental results suggest that social context (i.e., network-derived) features are more informative than news content ones.
sent11: Vosoughi et al. (2017) proposed Rumor Gauge, a system that jointly exploits temporal and propagation features, in conjunction with linguistic and user credibility features, for checking the veracity of rumors.
sent12: In particular, Rumor Gauge leverages text, and network propagation.
sent13: The temporal modality does not directly provide features, but is instead considered by recomputing all other features at regular time steps, thus yielding multiple time series.
sent14: Results by Vosoughi et al. (2017) and Kwon et al. (2017) also demonstrated that the contribution of the different data modalities change over time.
sent15: To mitigate the ""cold start"" problem of propagation-based early detection of fake news, Liu and Wu (2018) proposed an approach that is primarily based on user and temporal information.
sent16: First, they built a propagation path of each news as a time series of user representations.
sent17: The time series for a given news only contains the ordered representations of those users that shared such news.
sent18: Then, they learned two vector representations of each propagation path via GRUs and CNNs, respectively.
sent19: Zannettou et al. (2018) analyzed different aspects of memes, such as how they evolve and propagate in different mainstream and fringe web communities, and variants of memes that propa-gate.
sent20: Finally, Nguyen et al. (2020) proposed Factual News Graph (FANG) to exploit the social structure and the engagement patterns of users for fake news detection.","1. Why is network information leveraged in disinformation detection? sent1
    1.1. What did Vosoughi et al. (2018a) find about the spread of fake information? sent2
2. What types of propagation networks exist, and how can they be analyzed? sent3, sent4, sent5
3. How did Shu et al. (2020) approach the fake news classification task? sent6
    3.1. What features did they consider in their approach? sent7
    3.2. What did experiments on PolitiFact and GossipCop datasets reveal about feature contributions? sent8
4. What did Shu et al. (2019) contribute to fake news classification? sent9
    4.1. What do their experimental results suggest about social context features? sent10
5. What is Rumor Gauge, and what features does it exploit? sent11
    5.1. How does Rumor Gauge leverage temporal modality? sent12, sent13
    5.2. What did results by Vosoughi et al. (2017) and Kwon et al. (2017) demonstrate about data modalities? sent14
6. How did Liu and Wu (2018) address the ""cold start"" problem in fake news detection? sent15
    6.1. How did they build a propagation path for each news item? sent16, sent17
    6.2. How did they learn vector representations of propagation paths? sent18
7. What did Zannettou et al. (2018) analyze regarding memes? sent19
8. What is the Factual News Graph (FANG) proposed by Nguyen et al. (2020)? sent20","5. What is Rumor Gauge, and what features does it exploit? sent11
    5.1. How does Rumor Gauge leverage temporal modality? sent12, sent13
    5.2. What did results by Vosoughi et al. (2017) and Kwon et al. (2017) demonstrate about data modalities? sent14

6. How did Liu and Wu (2018) address the ""cold start"" problem in fake news detection? sent15
    6.1. How did they build a propagation path for each news item? sent16, sent17
    6.2. How did they learn vector representations of propagation paths? sent18","5. What is Rumor Gauge, and what features does it exploit? sent11
    5.1. How does Rumor Gauge leverage temporal modality? sent12, sent13
    5.2. What did results by Vosoughi et al. (2017) and Kwon et al. (2017) demonstrate about data modalities? sent14","Question: How do Vosoughi et al. (2017) and Kwon et al. (2017) demonstrate the changing contribution of data modalities over time?

Supporting sentence: sent14",How do Vosoughi et al. (2017) and Kwon et al. (2017) demonstrate the changing contribution of data modalities over time?,sent14,2,implement,"Vosoughi et al. (2017) proposed Rumor Gauge, a system that jointly exploits temporal and propagation features, in conjunction with linguistic and user credibility features, for checking the veracity of rumors.

In particular, Rumor Gauge leverages text and network propagation.

The temporal modality does not directly provide features, but is instead considered by recomputing all other features at regular time steps, thus yielding multiple time series.  Results by Vosoughi et al. (2017) and Kwon et al. (2017) also demonstrated that the contribution of the different data modalities change over time.","Question:

How do Vosoughi et al. (2017) and Kwon et al. (2017) demonstrate the changing contribution of data modalities over time?

Answer:

Vosoughi et al. (2017) proposed Rumor Gauge, a system that jointly exploits temporal and propagation features, in conjunction with linguistic and user credibility features, for checking the veracity of rumors.

In particular, Rumor Gauge leverages text and network propagation.

The temporal modality does not directly provide features, but is instead considered by recomputing all other features at regular time steps, thus yielding multiple time series.  Results by Vosoughi et al. (2017) and Kwon et al. (2017) also demonstrated that the contribution of the different data modalities change over time."
232320384,A Survey on Multimodal Disinformation Detection,https://www.semanticscholar.org/paper/71d2dc1fc38e0c48c865de5f5c023ccf7c5ad018,Network and Temporal Information,10,"The rationale for leveraging network information stems from early work (Shao et al., 2018;Vosoughi et al., 2018a) that showed that propagation and interaction networks of fake news are deeper and wider than those of real news. Vosoughi et al. (2018a) further found that fake information spreads faster than factual one, thus advocating for the use of temporal information.

Propagation networks can be homogeneous or heterogeneous (e.g., encompassing news articles, publishers, users, and posts) and they can be analyzed at different scales (e.g., node-level, ego-level, triad-level, community-level and the overall network, as shown in Figure 3, in Appendix) (Zhou and Zafarani, 2019). Shu et al. (2020) tackled the fake news classification task by proposing an approach based on hierarchical propagation networks. At both micro-and macro-scale, they extracted and jointly considered network features, temporal features, and linguistic features. Experiments on PolitiFact and GossipCop datasets revealed that temporal features have maximum contribution, followed by network and linguistic features. Shu et al. (2019) provided one of the most thorough multimodal frameworks for fake news classification. Their experimental results suggest that social context (i.e., network-derived) features are more informative than news content ones. Vosoughi et al. (2017) proposed Rumor Gauge, a system that jointly exploits temporal and propagation features, in conjunction with linguistic and user credibility features, for checking the veracity of rumors. In particular, Rumor Gauge leverages text, and network propagation. The temporal modality does not directly provide features, but is instead considered by recomputing all other features at regular time steps, thus yielding multiple time series. Results by Vosoughi et al. (2017) and Kwon et al. (2017) also demonstrated that the contribution of the different data modalities change over time.

To mitigate the ""cold start"" problem of propagation-based early detection of fake news, Liu and Wu (2018) proposed an approach that is primarily based on user and temporal information. First, they built a propagation path of each news as a time series of user representations. The time series for a given news only contains the ordered representations of those users that shared such news. Then, they learned two vector representations of each propagation path via GRUs and CNNs, respectively. Zannettou et al. (2018) analyzed different aspects of memes, such as how they evolve and propagate in different mainstream and fringe web communities, and variants of memes that propa-gate. Finally, Nguyen et al. (2020) proposed Factual News Graph (FANG) to exploit the social structure and the engagement patterns of users for fake news detection.","sent1: The rationale for leveraging network information stems from early work (Shao et al., 2018;Vosoughi et al., 2018a) that showed that propagation and interaction networks of fake news are deeper and wider than those of real news.
sent2: Vosoughi et al. (2018a) further found that fake information spreads faster than factual one, thus advocating for the use of temporal information.
sent3: Propagation networks can be homogeneous or heterogeneous (e.g., encompassing news articles, publishers, users, and posts)
sent4: and they can be analyzed at different scales
sent5: (e.g., node-level, ego-level, triad-level, community-level and the overall network, as shown in Figure 3, in Appendix) (Zhou and Zafarani, 2019).
sent6: Shu et al. (2020) tackled the fake news classification task by proposing an approach based on hierarchical propagation networks.
sent7: At both micro-and macro-scale, they extracted and jointly considered network features, temporal features, and linguistic features.
sent8: Experiments on PolitiFact and GossipCop datasets revealed that temporal features have maximum contribution, followed by network and linguistic features.
sent9: Shu et al. (2019) provided one of the most thorough multimodal frameworks for fake news classification.
sent10: Their experimental results suggest that social context (i.e., network-derived) features are more informative than news content ones.
sent11: Vosoughi et al. (2017) proposed Rumor Gauge, a system that jointly exploits temporal and propagation features, in conjunction with linguistic and user credibility features, for checking the veracity of rumors.
sent12: In particular, Rumor Gauge leverages text, and network propagation.
sent13: The temporal modality does not directly provide features, but is instead considered by recomputing all other features at regular time steps, thus yielding multiple time series.
sent14: Results by Vosoughi et al. (2017) and Kwon et al. (2017) also demonstrated that the contribution of the different data modalities change over time.
sent15: To mitigate the ""cold start"" problem of propagation-based early detection of fake news, Liu and Wu (2018) proposed an approach that is primarily based on user and temporal information.
sent16: First, they built a propagation path of each news as a time series of user representations.
sent17: The time series for a given news only contains the ordered representations of those users that shared such news.
sent18: Then, they learned two vector representations of each propagation path via GRUs and CNNs, respectively.
sent19: Zannettou et al. (2018) analyzed different aspects of memes, such as how they evolve and propagate in different mainstream and fringe web communities, and variants of memes that propa-gate.
sent20: Finally, Nguyen et al. (2020) proposed Factual News Graph (FANG) to exploit the social structure and the engagement patterns of users for fake news detection.","1. Why is network information leveraged in disinformation detection? sent1
    1.1. What did Vosoughi et al. (2018a) find about the spread of fake information? sent2
2. What types of propagation networks exist, and how can they be analyzed? sent3, sent4, sent5
3. How did Shu et al. (2020) approach the fake news classification task? sent6
    3.1. What features did they consider in their approach? sent7
    3.2. What did experiments on PolitiFact and GossipCop datasets reveal about feature contributions? sent8
4. What did Shu et al. (2019) contribute to fake news classification? sent9
    4.1. What do their experimental results suggest about social context features? sent10
5. What is Rumor Gauge, and what features does it exploit? sent11
    5.1. How does Rumor Gauge leverage temporal modality? sent12, sent13
    5.2. What did results by Vosoughi et al. (2017) and Kwon et al. (2017) demonstrate about data modalities? sent14
6. How did Liu and Wu (2018) address the ""cold start"" problem in fake news detection? sent15
    6.1. How did they build a propagation path for each news item? sent16, sent17
    6.2. How did they learn vector representations of propagation paths? sent18
7. What did Zannettou et al. (2018) analyze regarding memes? sent19
8. What is the Factual News Graph (FANG) proposed by Nguyen et al. (2020)? sent20","5. What is Rumor Gauge, and what features does it exploit? sent11
    5.1. How does Rumor Gauge leverage temporal modality? sent12, sent13
    5.2. What did results by Vosoughi et al. (2017) and Kwon et al. (2017) demonstrate about data modalities? sent14

6. How did Liu and Wu (2018) address the ""cold start"" problem in fake news detection? sent15
    6.1. How did they build a propagation path for each news item? sent16, sent17
    6.2. How did they learn vector representations of propagation paths? sent18","6. How did Liu and Wu (2018) address the ""cold start"" problem in fake news detection? sent15
    6.1. How did they build a propagation path for each news item? sent16, sent17
    6.2. How did they learn vector representations of propagation paths? sent18","Question: How did Liu and Wu (2018) construct and represent propagation paths to address the ""cold start"" problem in fake news detection?

Supporting sentence: sent15, sent16, sent17, sent18","How did Liu and Wu (2018) construct and represent propagation paths to address the ""cold start"" problem in fake news detection?","sent15, sent16, sent17, sent18",1,implement,"Liu and Wu (2018) proposed an approach that is primarily based on user and temporal information to mitigate the ""cold start"" problem of propagation-based early detection of fake news.

First, they built a propagation path of each news as a time series of user representations.

The time series for a given news only contains the ordered representations of those users that shared such news.

Then, they learned two vector representations of each propagation path via GRUs and CNNs, respectively.","Question:

How did Liu and Wu (2018) construct and represent propagation paths to address the ""cold start"" problem in fake news detection?

Answer:

Liu and Wu (2018) proposed an approach that is primarily based on user and temporal information to mitigate the ""cold start"" problem of propagation-based early detection of fake news.

First, they built a propagation path of each news as a time series of user representations.

The time series for a given news only contains the ordered representations of those users that shared such news.

Then, they learned two vector representations of each propagation path via GRUs and CNNs, respectively."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,Extraction: Automated Extraction of Information,20,"Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples (or as [subject, predicate, object], respectively). Mehrabi et al. (2020) showed that the NER system CoreNLP  exhibits binary gender bias. They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names 5 from 139 years of census data. The model returned more erroneous tags for female names. Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders. A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF) (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy 6 NER models). Across models, nonwhite names yielded on average lower performance scores than white names. Generally, ELMo exhibited the least bias. Although ConceptNet is debiased for gender and ethnicity 7 , it was found to produce strongly varied accuracy values. Gaut et al. (2020) analyzed binary gender bias in a popular open-source neural relation extraction (NRE) model, OpenNRE (Han et al., 2019). For this purpose, the authors created a new dataset, named WikiGenderBias (sourced from Wikipedia and DBpedia). All sentences describe a gendered subject with one of four relations: spouse, hypernym, birthData, or birthPlace (DBpedia mostly uses occupation-related hypernyms). The most notable bias found was the spouse relation. It was more reliably predicted for male than female entities. This observation stands in contrast to the predominance of female instances with spouse relation in WikiGenderBias. The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016). Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.

Nowadays, contextualized transformer-based en-coders are used in various NLP applications, including NER and NRE. Several works have analyzed the various societal biases encoded in large-scale word embeddings (like word2vec (Mikolov et al., 2013;Bolukbasi et al., 2016) or BERT (Devlin et al., 2019;Kurita et al., 2019)) or language models (like GPT-2 (Radford et al., 2019;Kirk et al., 2021) and GPT-3 (Brown et al., 2020;Abid et al., 2021)). Thus, it is likely that these biases also affect the downstream tasks discussed here.  used two types of tasks to analyze bias in BERT-based RE on the newly created Wiki80 and TACRED (Zhang et al., 2017) benchmarks. For the first task, they masked only entity names with a special token (masked-entity; ME), whereas for the second task, only the entity names were given (onlyentity; OE). The model maintained higher performances in the OE setting, indicating that the entity names were more informative of the predicted relation than the contextual information. This hints at what the authors call semantic bias.

A Note on Reporting Bias Generally, when extracting knowledge from text, one should be aware that the frequency with which facts are reported is not representative of their real-world prevalence. Humans tend to mention only events, outcomes, or properties that are out of their perceived ordinary (Gordon and Van Durme, 2013) (e.g., ""a banana is yellow"" is too trivial to be reported). This phenomenon is called reporting bias and likely stems from a need to be as informative and non-redundant as possible when sharing knowledge.","sent1: Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples (or as [subject, predicate, object], respectively).
sent2: Mehrabi et al. (2020) showed that the NER system CoreNLP  exhibits binary gender bias.
sent3: They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names 5 from 139 years of census data.
sent4: The model returned more erroneous tags for female names.
sent5: Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders.
sent6: A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF)
sent7: (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy 6 NER models).
sent8: Across models, nonwhite names yielded on average lower performance scores than white names.
sent9: Generally, ELMo exhibited the least bias.
sent10: Although ConceptNet is debiased for gender and ethnicity 7 , it was found to produce strongly varied accuracy values.
sent11: Gaut et al. (2020) analyzed binary gender bias in a popular open-source neural relation extraction (NRE) model, OpenNRE (Han et al., 2019).
sent12: For this purpose, the authors created a new dataset, named WikiGenderBias (sourced from Wikipedia and DBpedia).
sent13: All sentences describe a gendered subject with one of four relations: spouse, hypernym, birthData, or birthPlace (DBpedia mostly uses occupation-related hypernyms).
sent14: The most notable bias found was the spouse relation.
sent15: It was more reliably predicted for male than female entities.
sent16: This observation stands in contrast to the predominance of female instances with spouse relation in WikiGenderBias.
sent17: The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016).
sent18: Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.
sent19: Nowadays, contextualized transformer-based en-coders are used in various NLP applications, including NER and NRE.
sent20: Several works have analyzed the various societal biases encoded in large-scale word embeddings (like word2vec (Mikolov et al., 2013;Bolukbasi et al., 2016) or BERT (Devlin et al., 2019;Kurita et al., 2019)) or language models (like GPT-2 (Radford et al., 2019;Kirk et al., 2021) and GPT-3 (Brown et al., 2020;Abid et al., 2021)).
sent21: Thus, it is likely that these biases also affect the downstream tasks discussed here.
sent22: used two types of tasks to analyze bias in BERT-based RE on the newly created Wiki80 and TACRED (Zhang et al., 2017) benchmarks.
sent23: For the first task, they masked only entity names with a special token (masked-entity; ME), whereas for the second task, only the entity names were given (onlyentity; OE).
sent24: The model maintained higher performances in the OE setting, indicating that the entity names were more informative of the predicted relation than the contextual information.
sent25: This hints at what the authors call semantic bias.
sent26: A Note on Reporting Bias Generally, when extracting knowledge from text, one should be aware that the frequency with which facts are reported is not representative of their real-world prevalence.
sent27: Humans tend to mention only events, outcomes, or properties that are out of their perceived ordinary (Gordon and Van Durme, 2013) (e.g., ""a banana is yellow"" is too trivial to be reported).
sent28: This phenomenon is called reporting bias and likely stems from a need to be as informative and non-redundant as possible when sharing knowledge.","1. How can NLP methods be used in the context of information extraction? sent1
    1.1. What bias was found in the NER system CoreNLP according to Mehrabi et al. (2020)? sent2
        1.1.1. How did Mehrabi et al. (2020) demonstrate this bias? sent3, sent4
    1.2. What did Mishra et al. (2020) do to evaluate NER systems? sent5
        1.2.1. What types of NER systems were evaluated? sent6, sent7
        1.2.2. What was the performance outcome for nonwhite names across models? sent8
        1.2.3. Which model exhibited the least bias? sent9
        1.2.4. What was found about ConceptNet's performance? sent10
2. What did Gaut et al. (2020) analyze in relation to binary gender bias? sent11
    2.1. What dataset did they create for their analysis? sent12
    2.2. What relations were described in the WikiGenderBias dataset? sent13
    2.3. What was the most notable bias found in their analysis? sent14, sent15
    2.4. How does this bias contrast with the data in WikiGenderBias? sent16
    2.5. What mitigation strategies did the authors experiment with? sent17
        2.5.1. Which strategy was effective without reducing model performance? sent18
3. What modern techniques are used in NLP applications like NER and NRE? sent19
    3.1. What societal biases have been analyzed in large-scale word embeddings and language models? sent20
    3.2. How might these biases affect downstream tasks? sent21
4. How was bias analyzed in BERT-based RE according to the study? sent22
    4.1. What were the two types of tasks used in the analysis? sent23
    4.2. What did the performance in the OE setting indicate? sent24
    4.3. What does this performance hint at according to the authors? sent25
5. What is reporting bias in the context of knowledge extraction? sent26
    5.1. Why do humans tend to mention only certain events or properties? sent27
    5.2. What is the likely reason for reporting bias? sent28","1. How can NLP methods be used in the context of information extraction? sent1
    1.1. What bias was found in the NER system CoreNLP according to Mehrabi et al. (2020)? sent2
        1.1.1. How did Mehrabi et al. (2020) demonstrate this bias? sent3, sent4
    1.2. What did Mishra et al. (2020) do to evaluate NER systems? sent5
        1.2.1. What types of NER systems were evaluated? sent6, sent7
        1.2.2. What was the performance outcome for nonwhite names across models? sent8
        1.2.3. Which model exhibited the least bias? sent9
        1.2.4. What was found about ConceptNet's performance? sent10

    1.2. What did Mishra et al. (2020) do to evaluate NER systems? sent5
        1.2.1. What types of NER systems were evaluated? sent6, sent7
        1.2.2. What was the performance outcome for nonwhite names across models? sent8
        1.2.3. Which model exhibited the least bias? sent9
        1.2.4. What was found about ConceptNet's performance? sent10

2. What did Gaut et al. (2020) analyze in relation to binary gender bias? sent11
    2.1. What dataset did they create for their analysis? sent12
    2.2. What relations were described in the WikiGenderBias dataset? sent13
    2.3. What was the most notable bias found in their analysis? sent14, sent15
    2.4. How does this bias contrast with the data in WikiGenderBias? sent16
    2.5. What mitigation strategies did the authors experiment with? sent17
        2.5.1. Which strategy was effective without reducing model performance? sent18

4. How was bias analyzed in BERT-based RE according to the study? sent22
    4.1. What were the two types of tasks used in the analysis? sent23
    4.2. What did the performance in the OE setting indicate? sent24
    4.3. What does this performance hint at according to the authors? sent25","1. How can NLP methods be used in the context of information extraction? sent1
    1.1. What bias was found in the NER system CoreNLP according to Mehrabi et al. (2020)? sent2
        1.1.1. How did Mehrabi et al. (2020) demonstrate this bias? sent3, sent4
    1.2. What did Mishra et al. (2020) do to evaluate NER systems? sent5
        1.2.1. What types of NER systems were evaluated? sent6, sent7
        1.2.2. What was the performance outcome for nonwhite names across models? sent8
        1.2.3. Which model exhibited the least bias? sent9
        1.2.4. What was found about ConceptNet's performance? sent10","Question: How do biases manifest in NER systems, and what methods have been used to evaluate and address them?

Supporting sentence: sent2, sent3, sent4, sent5, sent6, sent7, sent8, sent9, sent10","How do biases manifest in NER systems, and what methods have been used to evaluate and address them?","sent2, sent3, sent4, sent5, sent6, sent7, sent8, sent9, sent10",4,implement,"Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples.

Mehrabi et al. (2020) showed that the NER system CoreNLP exhibits binary gender bias.

They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names from 139 years of census data.

The model returned more erroneous tags for female names.

Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders.

A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF)

(Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy NER models).

Across models, nonwhite names yielded on average lower performance scores than white names.

Generally, ELMo exhibited the least bias.

Although ConceptNet is debiased for gender and ethnicity, it was found to produce strongly varied accuracy values.","Question:

How do biases manifest in NER systems, and what methods have been used to evaluate and address them?

Answer:

Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples.

Mehrabi et al. (2020) showed that the NER system CoreNLP exhibits binary gender bias.

They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names from 139 years of census data.

The model returned more erroneous tags for female names.

Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders.

A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF)

(Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy NER models).

Across models, nonwhite names yielded on average lower performance scores than white names.

Generally, ELMo exhibited the least bias.

Although ConceptNet is debiased for gender and ethnicity, it was found to produce strongly varied accuracy values."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,Extraction: Automated Extraction of Information,20,"Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples (or as [subject, predicate, object], respectively). Mehrabi et al. (2020) showed that the NER system CoreNLP  exhibits binary gender bias. They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names 5 from 139 years of census data. The model returned more erroneous tags for female names. Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders. A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF) (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy 6 NER models). Across models, nonwhite names yielded on average lower performance scores than white names. Generally, ELMo exhibited the least bias. Although ConceptNet is debiased for gender and ethnicity 7 , it was found to produce strongly varied accuracy values. Gaut et al. (2020) analyzed binary gender bias in a popular open-source neural relation extraction (NRE) model, OpenNRE (Han et al., 2019). For this purpose, the authors created a new dataset, named WikiGenderBias (sourced from Wikipedia and DBpedia). All sentences describe a gendered subject with one of four relations: spouse, hypernym, birthData, or birthPlace (DBpedia mostly uses occupation-related hypernyms). The most notable bias found was the spouse relation. It was more reliably predicted for male than female entities. This observation stands in contrast to the predominance of female instances with spouse relation in WikiGenderBias. The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016). Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.

Nowadays, contextualized transformer-based en-coders are used in various NLP applications, including NER and NRE. Several works have analyzed the various societal biases encoded in large-scale word embeddings (like word2vec (Mikolov et al., 2013;Bolukbasi et al., 2016) or BERT (Devlin et al., 2019;Kurita et al., 2019)) or language models (like GPT-2 (Radford et al., 2019;Kirk et al., 2021) and GPT-3 (Brown et al., 2020;Abid et al., 2021)). Thus, it is likely that these biases also affect the downstream tasks discussed here.  used two types of tasks to analyze bias in BERT-based RE on the newly created Wiki80 and TACRED (Zhang et al., 2017) benchmarks. For the first task, they masked only entity names with a special token (masked-entity; ME), whereas for the second task, only the entity names were given (onlyentity; OE). The model maintained higher performances in the OE setting, indicating that the entity names were more informative of the predicted relation than the contextual information. This hints at what the authors call semantic bias.

A Note on Reporting Bias Generally, when extracting knowledge from text, one should be aware that the frequency with which facts are reported is not representative of their real-world prevalence. Humans tend to mention only events, outcomes, or properties that are out of their perceived ordinary (Gordon and Van Durme, 2013) (e.g., ""a banana is yellow"" is too trivial to be reported). This phenomenon is called reporting bias and likely stems from a need to be as informative and non-redundant as possible when sharing knowledge.","sent1: Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples (or as [subject, predicate, object], respectively).
sent2: Mehrabi et al. (2020) showed that the NER system CoreNLP  exhibits binary gender bias.
sent3: They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names 5 from 139 years of census data.
sent4: The model returned more erroneous tags for female names.
sent5: Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders.
sent6: A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF)
sent7: (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy 6 NER models).
sent8: Across models, nonwhite names yielded on average lower performance scores than white names.
sent9: Generally, ELMo exhibited the least bias.
sent10: Although ConceptNet is debiased for gender and ethnicity 7 , it was found to produce strongly varied accuracy values.
sent11: Gaut et al. (2020) analyzed binary gender bias in a popular open-source neural relation extraction (NRE) model, OpenNRE (Han et al., 2019).
sent12: For this purpose, the authors created a new dataset, named WikiGenderBias (sourced from Wikipedia and DBpedia).
sent13: All sentences describe a gendered subject with one of four relations: spouse, hypernym, birthData, or birthPlace (DBpedia mostly uses occupation-related hypernyms).
sent14: The most notable bias found was the spouse relation.
sent15: It was more reliably predicted for male than female entities.
sent16: This observation stands in contrast to the predominance of female instances with spouse relation in WikiGenderBias.
sent17: The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016).
sent18: Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.
sent19: Nowadays, contextualized transformer-based en-coders are used in various NLP applications, including NER and NRE.
sent20: Several works have analyzed the various societal biases encoded in large-scale word embeddings (like word2vec (Mikolov et al., 2013;Bolukbasi et al., 2016) or BERT (Devlin et al., 2019;Kurita et al., 2019)) or language models (like GPT-2 (Radford et al., 2019;Kirk et al., 2021) and GPT-3 (Brown et al., 2020;Abid et al., 2021)).
sent21: Thus, it is likely that these biases also affect the downstream tasks discussed here.
sent22: used two types of tasks to analyze bias in BERT-based RE on the newly created Wiki80 and TACRED (Zhang et al., 2017) benchmarks.
sent23: For the first task, they masked only entity names with a special token (masked-entity; ME), whereas for the second task, only the entity names were given (onlyentity; OE).
sent24: The model maintained higher performances in the OE setting, indicating that the entity names were more informative of the predicted relation than the contextual information.
sent25: This hints at what the authors call semantic bias.
sent26: A Note on Reporting Bias Generally, when extracting knowledge from text, one should be aware that the frequency with which facts are reported is not representative of their real-world prevalence.
sent27: Humans tend to mention only events, outcomes, or properties that are out of their perceived ordinary (Gordon and Van Durme, 2013) (e.g., ""a banana is yellow"" is too trivial to be reported).
sent28: This phenomenon is called reporting bias and likely stems from a need to be as informative and non-redundant as possible when sharing knowledge.","1. How can NLP methods be used in the context of information extraction? sent1
    1.1. What bias was found in the NER system CoreNLP according to Mehrabi et al. (2020)? sent2
        1.1.1. How did Mehrabi et al. (2020) demonstrate this bias? sent3, sent4
    1.2. What did Mishra et al. (2020) do to evaluate NER systems? sent5
        1.2.1. What types of NER systems were evaluated? sent6, sent7
        1.2.2. What was the performance outcome for nonwhite names across models? sent8
        1.2.3. Which model exhibited the least bias? sent9
        1.2.4. What was found about ConceptNet's performance? sent10
2. What did Gaut et al. (2020) analyze in relation to binary gender bias? sent11
    2.1. What dataset did they create for their analysis? sent12
    2.2. What relations were described in the WikiGenderBias dataset? sent13
    2.3. What was the most notable bias found in their analysis? sent14, sent15
    2.4. How does this bias contrast with the data in WikiGenderBias? sent16
    2.5. What mitigation strategies did the authors experiment with? sent17
        2.5.1. Which strategy was effective without reducing model performance? sent18
3. What modern techniques are used in NLP applications like NER and NRE? sent19
    3.1. What societal biases have been analyzed in large-scale word embeddings and language models? sent20
    3.2. How might these biases affect downstream tasks? sent21
4. How was bias analyzed in BERT-based RE according to the study? sent22
    4.1. What were the two types of tasks used in the analysis? sent23
    4.2. What did the performance in the OE setting indicate? sent24
    4.3. What does this performance hint at according to the authors? sent25
5. What is reporting bias in the context of knowledge extraction? sent26
    5.1. Why do humans tend to mention only certain events or properties? sent27
    5.2. What is the likely reason for reporting bias? sent28","1. How can NLP methods be used in the context of information extraction? sent1
    1.1. What bias was found in the NER system CoreNLP according to Mehrabi et al. (2020)? sent2
        1.1.1. How did Mehrabi et al. (2020) demonstrate this bias? sent3, sent4
    1.2. What did Mishra et al. (2020) do to evaluate NER systems? sent5
        1.2.1. What types of NER systems were evaluated? sent6, sent7
        1.2.2. What was the performance outcome for nonwhite names across models? sent8
        1.2.3. Which model exhibited the least bias? sent9
        1.2.4. What was found about ConceptNet's performance? sent10

    1.2. What did Mishra et al. (2020) do to evaluate NER systems? sent5
        1.2.1. What types of NER systems were evaluated? sent6, sent7
        1.2.2. What was the performance outcome for nonwhite names across models? sent8
        1.2.3. Which model exhibited the least bias? sent9
        1.2.4. What was found about ConceptNet's performance? sent10

2. What did Gaut et al. (2020) analyze in relation to binary gender bias? sent11
    2.1. What dataset did they create for their analysis? sent12
    2.2. What relations were described in the WikiGenderBias dataset? sent13
    2.3. What was the most notable bias found in their analysis? sent14, sent15
    2.4. How does this bias contrast with the data in WikiGenderBias? sent16
    2.5. What mitigation strategies did the authors experiment with? sent17
        2.5.1. Which strategy was effective without reducing model performance? sent18

4. How was bias analyzed in BERT-based RE according to the study? sent22
    4.1. What were the two types of tasks used in the analysis? sent23
    4.2. What did the performance in the OE setting indicate? sent24
    4.3. What does this performance hint at according to the authors? sent25","    1.2. What did Mishra et al. (2020) do to evaluate NER systems? sent5
        1.2.1. What types of NER systems were evaluated? sent6, sent7
        1.2.2. What was the performance outcome for nonwhite names across models? sent8
        1.2.3. Which model exhibited the least bias? sent9
        1.2.4. What was found about ConceptNet's performance? sent10","Question: How do different NER systems perform in terms of bias when evaluating names associated with various ethnicities?

Supporting sentence: sent5, sent6, sent7, sent8, sent9, sent10",How do different NER systems perform in terms of bias when evaluating names associated with various ethnicities?,"sent5, sent6, sent7, sent8, sent9, sent10",4,implement,"Mehrabi et al. (2020) showed that the NER system CoreNLP exhibits binary gender bias.

They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names from 139 years of census data.

The model returned more erroneous tags for female names.

Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders.

A range of different NER systems were evaluated, including bidirectional LSTMs with Conditional Random Field (BiLSTM CRF) (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy NER models.

Across models, nonwhite names yielded on average lower performance scores than white names.

Generally, ELMo exhibited the least bias.

Although ConceptNet is debiased for gender and ethnicity, it was found to produce strongly varied accuracy values.","Question:

How do different NER systems perform in terms of bias when evaluating names associated with various ethnicities?

Answer:

Mehrabi et al. (2020) showed that the NER system CoreNLP exhibits binary gender bias.

They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names from 139 years of census data.

The model returned more erroneous tags for female names.

Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders.

A range of different NER systems were evaluated, including bidirectional LSTMs with Conditional Random Field (BiLSTM CRF) (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy NER models.

Across models, nonwhite names yielded on average lower performance scores than white names.

Generally, ELMo exhibited the least bias.

Although ConceptNet is debiased for gender and ethnicity, it was found to produce strongly varied accuracy values."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,Extraction: Automated Extraction of Information,20,"Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples (or as [subject, predicate, object], respectively). Mehrabi et al. (2020) showed that the NER system CoreNLP  exhibits binary gender bias. They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names 5 from 139 years of census data. The model returned more erroneous tags for female names. Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders. A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF) (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy 6 NER models). Across models, nonwhite names yielded on average lower performance scores than white names. Generally, ELMo exhibited the least bias. Although ConceptNet is debiased for gender and ethnicity 7 , it was found to produce strongly varied accuracy values. Gaut et al. (2020) analyzed binary gender bias in a popular open-source neural relation extraction (NRE) model, OpenNRE (Han et al., 2019). For this purpose, the authors created a new dataset, named WikiGenderBias (sourced from Wikipedia and DBpedia). All sentences describe a gendered subject with one of four relations: spouse, hypernym, birthData, or birthPlace (DBpedia mostly uses occupation-related hypernyms). The most notable bias found was the spouse relation. It was more reliably predicted for male than female entities. This observation stands in contrast to the predominance of female instances with spouse relation in WikiGenderBias. The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016). Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.

Nowadays, contextualized transformer-based en-coders are used in various NLP applications, including NER and NRE. Several works have analyzed the various societal biases encoded in large-scale word embeddings (like word2vec (Mikolov et al., 2013;Bolukbasi et al., 2016) or BERT (Devlin et al., 2019;Kurita et al., 2019)) or language models (like GPT-2 (Radford et al., 2019;Kirk et al., 2021) and GPT-3 (Brown et al., 2020;Abid et al., 2021)). Thus, it is likely that these biases also affect the downstream tasks discussed here.  used two types of tasks to analyze bias in BERT-based RE on the newly created Wiki80 and TACRED (Zhang et al., 2017) benchmarks. For the first task, they masked only entity names with a special token (masked-entity; ME), whereas for the second task, only the entity names were given (onlyentity; OE). The model maintained higher performances in the OE setting, indicating that the entity names were more informative of the predicted relation than the contextual information. This hints at what the authors call semantic bias.

A Note on Reporting Bias Generally, when extracting knowledge from text, one should be aware that the frequency with which facts are reported is not representative of their real-world prevalence. Humans tend to mention only events, outcomes, or properties that are out of their perceived ordinary (Gordon and Van Durme, 2013) (e.g., ""a banana is yellow"" is too trivial to be reported). This phenomenon is called reporting bias and likely stems from a need to be as informative and non-redundant as possible when sharing knowledge.","sent1: Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples (or as [subject, predicate, object], respectively).
sent2: Mehrabi et al. (2020) showed that the NER system CoreNLP  exhibits binary gender bias.
sent3: They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names 5 from 139 years of census data.
sent4: The model returned more erroneous tags for female names.
sent5: Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders.
sent6: A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF)
sent7: (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy 6 NER models).
sent8: Across models, nonwhite names yielded on average lower performance scores than white names.
sent9: Generally, ELMo exhibited the least bias.
sent10: Although ConceptNet is debiased for gender and ethnicity 7 , it was found to produce strongly varied accuracy values.
sent11: Gaut et al. (2020) analyzed binary gender bias in a popular open-source neural relation extraction (NRE) model, OpenNRE (Han et al., 2019).
sent12: For this purpose, the authors created a new dataset, named WikiGenderBias (sourced from Wikipedia and DBpedia).
sent13: All sentences describe a gendered subject with one of four relations: spouse, hypernym, birthData, or birthPlace (DBpedia mostly uses occupation-related hypernyms).
sent14: The most notable bias found was the spouse relation.
sent15: It was more reliably predicted for male than female entities.
sent16: This observation stands in contrast to the predominance of female instances with spouse relation in WikiGenderBias.
sent17: The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016).
sent18: Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.
sent19: Nowadays, contextualized transformer-based en-coders are used in various NLP applications, including NER and NRE.
sent20: Several works have analyzed the various societal biases encoded in large-scale word embeddings (like word2vec (Mikolov et al., 2013;Bolukbasi et al., 2016) or BERT (Devlin et al., 2019;Kurita et al., 2019)) or language models (like GPT-2 (Radford et al., 2019;Kirk et al., 2021) and GPT-3 (Brown et al., 2020;Abid et al., 2021)).
sent21: Thus, it is likely that these biases also affect the downstream tasks discussed here.
sent22: used two types of tasks to analyze bias in BERT-based RE on the newly created Wiki80 and TACRED (Zhang et al., 2017) benchmarks.
sent23: For the first task, they masked only entity names with a special token (masked-entity; ME), whereas for the second task, only the entity names were given (onlyentity; OE).
sent24: The model maintained higher performances in the OE setting, indicating that the entity names were more informative of the predicted relation than the contextual information.
sent25: This hints at what the authors call semantic bias.
sent26: A Note on Reporting Bias Generally, when extracting knowledge from text, one should be aware that the frequency with which facts are reported is not representative of their real-world prevalence.
sent27: Humans tend to mention only events, outcomes, or properties that are out of their perceived ordinary (Gordon and Van Durme, 2013) (e.g., ""a banana is yellow"" is too trivial to be reported).
sent28: This phenomenon is called reporting bias and likely stems from a need to be as informative and non-redundant as possible when sharing knowledge.","1. How can NLP methods be used in the context of information extraction? sent1
    1.1. What bias was found in the NER system CoreNLP according to Mehrabi et al. (2020)? sent2
        1.1.1. How did Mehrabi et al. (2020) demonstrate this bias? sent3, sent4
    1.2. What did Mishra et al. (2020) do to evaluate NER systems? sent5
        1.2.1. What types of NER systems were evaluated? sent6, sent7
        1.2.2. What was the performance outcome for nonwhite names across models? sent8
        1.2.3. Which model exhibited the least bias? sent9
        1.2.4. What was found about ConceptNet's performance? sent10
2. What did Gaut et al. (2020) analyze in relation to binary gender bias? sent11
    2.1. What dataset did they create for their analysis? sent12
    2.2. What relations were described in the WikiGenderBias dataset? sent13
    2.3. What was the most notable bias found in their analysis? sent14, sent15
    2.4. How does this bias contrast with the data in WikiGenderBias? sent16
    2.5. What mitigation strategies did the authors experiment with? sent17
        2.5.1. Which strategy was effective without reducing model performance? sent18
3. What modern techniques are used in NLP applications like NER and NRE? sent19
    3.1. What societal biases have been analyzed in large-scale word embeddings and language models? sent20
    3.2. How might these biases affect downstream tasks? sent21
4. How was bias analyzed in BERT-based RE according to the study? sent22
    4.1. What were the two types of tasks used in the analysis? sent23
    4.2. What did the performance in the OE setting indicate? sent24
    4.3. What does this performance hint at according to the authors? sent25
5. What is reporting bias in the context of knowledge extraction? sent26
    5.1. Why do humans tend to mention only certain events or properties? sent27
    5.2. What is the likely reason for reporting bias? sent28","1. How can NLP methods be used in the context of information extraction? sent1
    1.1. What bias was found in the NER system CoreNLP according to Mehrabi et al. (2020)? sent2
        1.1.1. How did Mehrabi et al. (2020) demonstrate this bias? sent3, sent4
    1.2. What did Mishra et al. (2020) do to evaluate NER systems? sent5
        1.2.1. What types of NER systems were evaluated? sent6, sent7
        1.2.2. What was the performance outcome for nonwhite names across models? sent8
        1.2.3. Which model exhibited the least bias? sent9
        1.2.4. What was found about ConceptNet's performance? sent10

    1.2. What did Mishra et al. (2020) do to evaluate NER systems? sent5
        1.2.1. What types of NER systems were evaluated? sent6, sent7
        1.2.2. What was the performance outcome for nonwhite names across models? sent8
        1.2.3. Which model exhibited the least bias? sent9
        1.2.4. What was found about ConceptNet's performance? sent10

2. What did Gaut et al. (2020) analyze in relation to binary gender bias? sent11
    2.1. What dataset did they create for their analysis? sent12
    2.2. What relations were described in the WikiGenderBias dataset? sent13
    2.3. What was the most notable bias found in their analysis? sent14, sent15
    2.4. How does this bias contrast with the data in WikiGenderBias? sent16
    2.5. What mitigation strategies did the authors experiment with? sent17
        2.5.1. Which strategy was effective without reducing model performance? sent18

4. How was bias analyzed in BERT-based RE according to the study? sent22
    4.1. What were the two types of tasks used in the analysis? sent23
    4.2. What did the performance in the OE setting indicate? sent24
    4.3. What does this performance hint at according to the authors? sent25","2. What did Gaut et al. (2020) analyze in relation to binary gender bias? sent11
    2.1. What dataset did they create for their analysis? sent12
    2.2. What relations were described in the WikiGenderBias dataset? sent13
    2.3. What was the most notable bias found in their analysis? sent14, sent15
    2.4. How does this bias contrast with the data in WikiGenderBias? sent16
    2.5. What mitigation strategies did the authors experiment with? sent17
        2.5.1. Which strategy was effective without reducing model performance? sent18","Question: What mitigation strategies were explored to address gender bias in neural relation extraction models, and which was effective without performance loss?

Supporting sentence: sent17, sent18","What mitigation strategies were explored to address gender bias in neural relation extraction models, and which was effective without performance loss?","sent17, sent18",2,implement,"The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016).

Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.","Question:

What mitigation strategies were explored to address gender bias in neural relation extraction models, and which was effective without performance loss?

Answer:

The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016).

Only downsampling facilitated a reduction of bias that did not come at the cost of model performance."
252762171,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",https://www.semanticscholar.org/paper/8d74c7cde3ec806546c09274a968b87620442396,Extraction: Automated Extraction of Information,20,"Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples (or as [subject, predicate, object], respectively). Mehrabi et al. (2020) showed that the NER system CoreNLP  exhibits binary gender bias. They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names 5 from 139 years of census data. The model returned more erroneous tags for female names. Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders. A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF) (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy 6 NER models). Across models, nonwhite names yielded on average lower performance scores than white names. Generally, ELMo exhibited the least bias. Although ConceptNet is debiased for gender and ethnicity 7 , it was found to produce strongly varied accuracy values. Gaut et al. (2020) analyzed binary gender bias in a popular open-source neural relation extraction (NRE) model, OpenNRE (Han et al., 2019). For this purpose, the authors created a new dataset, named WikiGenderBias (sourced from Wikipedia and DBpedia). All sentences describe a gendered subject with one of four relations: spouse, hypernym, birthData, or birthPlace (DBpedia mostly uses occupation-related hypernyms). The most notable bias found was the spouse relation. It was more reliably predicted for male than female entities. This observation stands in contrast to the predominance of female instances with spouse relation in WikiGenderBias. The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016). Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.

Nowadays, contextualized transformer-based en-coders are used in various NLP applications, including NER and NRE. Several works have analyzed the various societal biases encoded in large-scale word embeddings (like word2vec (Mikolov et al., 2013;Bolukbasi et al., 2016) or BERT (Devlin et al., 2019;Kurita et al., 2019)) or language models (like GPT-2 (Radford et al., 2019;Kirk et al., 2021) and GPT-3 (Brown et al., 2020;Abid et al., 2021)). Thus, it is likely that these biases also affect the downstream tasks discussed here.  used two types of tasks to analyze bias in BERT-based RE on the newly created Wiki80 and TACRED (Zhang et al., 2017) benchmarks. For the first task, they masked only entity names with a special token (masked-entity; ME), whereas for the second task, only the entity names were given (onlyentity; OE). The model maintained higher performances in the OE setting, indicating that the entity names were more informative of the predicted relation than the contextual information. This hints at what the authors call semantic bias.

A Note on Reporting Bias Generally, when extracting knowledge from text, one should be aware that the frequency with which facts are reported is not representative of their real-world prevalence. Humans tend to mention only events, outcomes, or properties that are out of their perceived ordinary (Gordon and Van Durme, 2013) (e.g., ""a banana is yellow"" is too trivial to be reported). This phenomenon is called reporting bias and likely stems from a need to be as informative and non-redundant as possible when sharing knowledge.","sent1: Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples (or as [subject, predicate, object], respectively).
sent2: Mehrabi et al. (2020) showed that the NER system CoreNLP  exhibits binary gender bias.
sent3: They used a number of template sentences, like ""<Name> is going to school"" or ""<Name> is a person"" using male and female names 5 from 139 years of census data.
sent4: The model returned more erroneous tags for female names.
sent5: Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders.
sent6: A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF)
sent7: (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy 6 NER models).
sent8: Across models, nonwhite names yielded on average lower performance scores than white names.
sent9: Generally, ELMo exhibited the least bias.
sent10: Although ConceptNet is debiased for gender and ethnicity 7 , it was found to produce strongly varied accuracy values.
sent11: Gaut et al. (2020) analyzed binary gender bias in a popular open-source neural relation extraction (NRE) model, OpenNRE (Han et al., 2019).
sent12: For this purpose, the authors created a new dataset, named WikiGenderBias (sourced from Wikipedia and DBpedia).
sent13: All sentences describe a gendered subject with one of four relations: spouse, hypernym, birthData, or birthPlace (DBpedia mostly uses occupation-related hypernyms).
sent14: The most notable bias found was the spouse relation.
sent15: It was more reliably predicted for male than female entities.
sent16: This observation stands in contrast to the predominance of female instances with spouse relation in WikiGenderBias.
sent17: The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016).
sent18: Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.
sent19: Nowadays, contextualized transformer-based en-coders are used in various NLP applications, including NER and NRE.
sent20: Several works have analyzed the various societal biases encoded in large-scale word embeddings (like word2vec (Mikolov et al., 2013;Bolukbasi et al., 2016) or BERT (Devlin et al., 2019;Kurita et al., 2019)) or language models (like GPT-2 (Radford et al., 2019;Kirk et al., 2021) and GPT-3 (Brown et al., 2020;Abid et al., 2021)).
sent21: Thus, it is likely that these biases also affect the downstream tasks discussed here.
sent22: used two types of tasks to analyze bias in BERT-based RE on the newly created Wiki80 and TACRED (Zhang et al., 2017) benchmarks.
sent23: For the first task, they masked only entity names with a special token (masked-entity; ME), whereas for the second task, only the entity names were given (onlyentity; OE).
sent24: The model maintained higher performances in the OE setting, indicating that the entity names were more informative of the predicted relation than the contextual information.
sent25: This hints at what the authors call semantic bias.
sent26: A Note on Reporting Bias Generally, when extracting knowledge from text, one should be aware that the frequency with which facts are reported is not representative of their real-world prevalence.
sent27: Humans tend to mention only events, outcomes, or properties that are out of their perceived ordinary (Gordon and Van Durme, 2013) (e.g., ""a banana is yellow"" is too trivial to be reported).
sent28: This phenomenon is called reporting bias and likely stems from a need to be as informative and non-redundant as possible when sharing knowledge.","1. How can NLP methods be used in the context of information extraction? sent1
    1.1. What bias was found in the NER system CoreNLP according to Mehrabi et al. (2020)? sent2
        1.1.1. How did Mehrabi et al. (2020) demonstrate this bias? sent3, sent4
    1.2. What did Mishra et al. (2020) do to evaluate NER systems? sent5
        1.2.1. What types of NER systems were evaluated? sent6, sent7
        1.2.2. What was the performance outcome for nonwhite names across models? sent8
        1.2.3. Which model exhibited the least bias? sent9
        1.2.4. What was found about ConceptNet's performance? sent10
2. What did Gaut et al. (2020) analyze in relation to binary gender bias? sent11
    2.1. What dataset did they create for their analysis? sent12
    2.2. What relations were described in the WikiGenderBias dataset? sent13
    2.3. What was the most notable bias found in their analysis? sent14, sent15
    2.4. How does this bias contrast with the data in WikiGenderBias? sent16
    2.5. What mitigation strategies did the authors experiment with? sent17
        2.5.1. Which strategy was effective without reducing model performance? sent18
3. What modern techniques are used in NLP applications like NER and NRE? sent19
    3.1. What societal biases have been analyzed in large-scale word embeddings and language models? sent20
    3.2. How might these biases affect downstream tasks? sent21
4. How was bias analyzed in BERT-based RE according to the study? sent22
    4.1. What were the two types of tasks used in the analysis? sent23
    4.2. What did the performance in the OE setting indicate? sent24
    4.3. What does this performance hint at according to the authors? sent25
5. What is reporting bias in the context of knowledge extraction? sent26
    5.1. Why do humans tend to mention only certain events or properties? sent27
    5.2. What is the likely reason for reporting bias? sent28","1. How can NLP methods be used in the context of information extraction? sent1
    1.1. What bias was found in the NER system CoreNLP according to Mehrabi et al. (2020)? sent2
        1.1.1. How did Mehrabi et al. (2020) demonstrate this bias? sent3, sent4
    1.2. What did Mishra et al. (2020) do to evaluate NER systems? sent5
        1.2.1. What types of NER systems were evaluated? sent6, sent7
        1.2.2. What was the performance outcome for nonwhite names across models? sent8
        1.2.3. Which model exhibited the least bias? sent9
        1.2.4. What was found about ConceptNet's performance? sent10

    1.2. What did Mishra et al. (2020) do to evaluate NER systems? sent5
        1.2.1. What types of NER systems were evaluated? sent6, sent7
        1.2.2. What was the performance outcome for nonwhite names across models? sent8
        1.2.3. Which model exhibited the least bias? sent9
        1.2.4. What was found about ConceptNet's performance? sent10

2. What did Gaut et al. (2020) analyze in relation to binary gender bias? sent11
    2.1. What dataset did they create for their analysis? sent12
    2.2. What relations were described in the WikiGenderBias dataset? sent13
    2.3. What was the most notable bias found in their analysis? sent14, sent15
    2.4. How does this bias contrast with the data in WikiGenderBias? sent16
    2.5. What mitigation strategies did the authors experiment with? sent17
        2.5.1. Which strategy was effective without reducing model performance? sent18

4. How was bias analyzed in BERT-based RE according to the study? sent22
    4.1. What were the two types of tasks used in the analysis? sent23
    4.2. What did the performance in the OE setting indicate? sent24
    4.3. What does this performance hint at according to the authors? sent25","4. How was bias analyzed in BERT-based RE according to the study? sent22
    4.1. What were the two types of tasks used in the analysis? sent23
    4.2. What did the performance in the OE setting indicate? sent24
    4.3. What does this performance hint at according to the authors? sent25","Question: What are the implications of semantic bias in BERT-based relation extraction models?

Supporting sentence: sent22, sent23, sent24, sent25",What are the implications of semantic bias in BERT-based relation extraction models?,"sent22, sent23, sent24, sent25",4,implement,"The model maintained higher performances in the only-entity (OE) setting, indicating that the entity names were more informative of the predicted relation than the contextual information.

This hints at what the authors call semantic bias.","Question:

What are the implications of semantic bias in BERT-based relation extraction models?

Answer:

The model maintained higher performances in the only-entity (OE) setting, indicating that the entity names were more informative of the predicted relation than the contextual information.

This hints at what the authors call semantic bias."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,John F. Kennedy,15,"Negotiation is one of the crucial abilities in human communication that involves two or more individuals discussing goals and tactics to resolve conflicts, achieve mutual benefit, or find mutually acceptable solutions (Fershtman, 1990;Bazerman and Neale, 1993;Lewicki et al., 2011). It is a common aspect of human interaction, occurring whenever people communicate in order to manage conflict or reach a compromise. Scientifically, one of the long-term goals of dialogue research is to empower intelligent agents with such ability. Agent effectively negotiating with a human in natural language could have significant benefits in many scenarios, from bargaining prices in everyday trade-in (He et al., 2018) to high-stakes political or legal situations (Basave and He, 2016). Negotiation dialogue systems (Lewandowska, 1982;Lambert and Carberry, 1992;Chawla et al., 2021c) is an emerging research field that aims to build intelligent conversational agents that can automatically negotiate with a human in natural languages, e.g., CICERO 1 from Meta AI. Agents negotiate with human through multi-turn interaction using logically reasoning (Sycara and Dai, 2010) over goals (Zhang et al., 2020), strategies (Zhou et al., 2020) and psychology factors (Yang et al., 2021). As illustrated in Figure 1, negotiation dialogue agents interact with the human through multiturn cycles. A successful negotiation process involves efficient information exchange, strategic discussion toward their goals, and a closing section.

Despite the significant amount of research that has been conducted on the task, there is a lack of a systematic review of the topic. In this work, we aim to fill this gap by reviewing contemporary work in the emerging field of negotiation dialogue systems, covering aspects such as benchmarks, evaluation, methodology, and future directions. In recent years, various benchmarks have been proposed for negotiation dialogue systems, ranging from bargaining (Lewis et al., 2017) and game scenarios (Asher et al., 2016) to job interviews  and items exchanging (Chawla et al., 2021c). Our survey will provide an overview of these benchmarks and discuss how they have been used to evaluate the performance of negotiation dialogue systems.

Modeling the negotiation process for conversational agents also imposes challenges. Firstly, these agents must be able to reason about and employ various strategies in different situations. In addition to strategy modeling, it is also necessary to model the personalities (e.g., mind, emotion, and behaviors) of the negotiators. Thirdly, an effective policy learning method is essential for the successful use of language. To address these challenges, we can categorize existing solutions into three areas: (1) Personality modeling helps us understand negotiator's preferences, (2) Strategy modeling enables agents to make reasonable decisions based on gathered information, and (3) Policy learning methods utilize information effectively to maximize results.

In summary, our contributions are three-fold: (1) To the best of our knowledge, we systematically categorize current negotiation dialogue benchmarks from the perspective of distributive and integrative, with each category based on different goal types of negotiation dialogue tasks. (2) We categorize typical evaluation methods and current solutions into an appropriate taxonomy. (3) We pointed out the current limitation and promising research directions in the future.","sent1: Negotiation is one of the crucial abilities in human communication that involves two or more individuals discussing goals and tactics to resolve conflicts, achieve mutual benefit, or find mutually acceptable solutions (Fershtman, 1990;Bazerman and Neale, 1993;Lewicki et al., 2011).
sent2: It is a common aspect of human interaction, occurring whenever people communicate in order to manage conflict or reach a compromise.
sent3: Scientifically, one of the long-term goals of dialogue research is to empower intelligent agents with such ability.
sent4: Agent effectively negotiating with a human in natural language could have significant benefits in many scenarios, from bargaining prices in everyday trade-in (He et al., 2018) to high-stakes political or legal situations (Basave and He, 2016).
sent5: Negotiation dialogue systems (Lewandowska, 1982;Lambert and Carberry, 1992;Chawla et al., 2021c) is an emerging research field that aims to build intelligent conversational agents that can automatically negotiate with a human in natural languages, e.g., CICERO 1 from Meta AI.
sent6: Agents negotiate with human through multi-turn interaction using logically reasoning (Sycara and Dai, 2010) over goals (Zhang et al., 2020), strategies (Zhou et al., 2020) and psychology factors (Yang et al., 2021).
sent7: As illustrated in Figure 1, negotiation dialogue agents interact with the human through multiturn cycles.
sent8: A successful negotiation process involves efficient information exchange, strategic discussion toward their goals, and a closing section.
sent9: Despite the significant amount of research that has been conducted on the task, there is a lack of a systematic review of the topic.
sent10: In this work, we aim to fill this gap by reviewing contemporary work in the emerging field of negotiation dialogue systems, covering aspects such as benchmarks, evaluation, methodology, and future directions.
sent11: In recent years, various benchmarks have been proposed for negotiation dialogue systems, ranging from bargaining (Lewis et al., 2017) and game scenarios (Asher et al., 2016) to job interviews  and items exchanging (Chawla et al., 2021c).
sent12: Our survey will provide an overview of these benchmarks and discuss how they have been used to evaluate the performance of negotiation dialogue systems.
sent13: Modeling the negotiation process for conversational agents also imposes challenges.
sent14: Firstly, these agents must be able to reason about and employ various strategies in different situations.
sent15: In addition to strategy modeling, it is also necessary to model the personalities (e.g., mind, emotion, and behaviors) of the negotiators.
sent16: Thirdly, an effective policy learning method is essential for the successful use of language.
sent17: To address these challenges, we can categorize existing solutions into three areas: (1) Personality modeling helps us understand negotiator's preferences, (2) Strategy modeling enables agents to make reasonable decisions based on gathered information, and (3) Policy learning methods utilize information effectively to maximize results.
sent18: In summary, our contributions are three-fold: (1) To the best of our knowledge, we systematically categorize current negotiation dialogue benchmarks from the perspective of distributive and integrative, with each category based on different goal types of negotiation dialogue tasks.
sent19: (2) We categorize typical evaluation methods and current solutions into an appropriate taxonomy.
sent20: (3) We pointed out the current limitation and promising research directions in the future.","1. What is negotiation in the context of human communication? sent1
    1.1. How common is negotiation in human interaction? sent2
    1.2. What is one of the long-term goals of dialogue research related to negotiation? sent3
    1.3. What are some potential benefits of agents effectively negotiating with humans? sent4
2. What is the emerging research field of negotiation dialogue systems? sent5
    2.1. How do agents negotiate with humans in negotiation dialogue systems? sent6
        2.1.1. How do negotiation dialogue agents interact with humans? sent7
        2.1.2. What does a successful negotiation process involve? sent8
3. What is the current state of research on negotiation dialogue systems? sent9
    3.1. What does this work aim to achieve in the field of negotiation dialogue systems? sent10
    3.2. What types of benchmarks have been proposed for negotiation dialogue systems? sent11
        3.2.1. How will the survey address these benchmarks? sent12
4. What challenges are involved in modeling the negotiation process for conversational agents? sent13
    4.1. What is the first challenge related to strategy modeling? sent14
    4.2. What is necessary in addition to strategy modeling? sent15
    4.3. What is the third challenge related to policy learning? sent16
    4.4. How can existing solutions to these challenges be categorized? sent17
5. What are the contributions of this work? sent18, sent19, sent20","1. What is negotiation in the context of human communication? sent1
    1.1. How common is negotiation in human interaction? sent2
    1.2. What is one of the long-term goals of dialogue research related to negotiation? sent3
    1.3. What are some potential benefits of agents effectively negotiating with humans? sent4

2. What is the emerging research field of negotiation dialogue systems? sent5
    2.1. How do agents negotiate with humans in negotiation dialogue systems? sent6
        2.1.1. How do negotiation dialogue agents interact with humans? sent7
        2.1.2. What does a successful negotiation process involve? sent8

3. What is the current state of research on negotiation dialogue systems? sent9
    3.1. What does this work aim to achieve in the field of negotiation dialogue systems? sent10
    3.2. What types of benchmarks have been proposed for negotiation dialogue systems? sent11
        3.2.1. How will the survey address these benchmarks? sent12

4. What challenges are involved in modeling the negotiation process for conversational agents? sent13
    4.1. What is the first challenge related to strategy modeling? sent14
    4.2. What is necessary in addition to strategy modeling? sent15
    4.3. What is the third challenge related to policy learning? sent16
    4.4. How can existing solutions to these challenges be categorized? sent17","2. What is the emerging research field of negotiation dialogue systems? sent5
    2.1. How do agents negotiate with humans in negotiation dialogue systems? sent6
        2.1.1. How do negotiation dialogue agents interact with humans? sent7
        2.1.2. What does a successful negotiation process involve? sent8","Question: What are the key components and processes involved in a successful negotiation dialogue system?

Supporting sentence: sent6, sent7, sent8",What are the key components and processes involved in a successful negotiation dialogue system?,"sent6, sent7, sent8",4,implement,"Negotiation is one of the crucial abilities in human communication that involves two or more individuals discussing goals and tactics to resolve conflicts, achieve mutual benefit, or find mutually acceptable solutions (Fershtman, 1990; Bazerman and Neale, 1993; Lewicki et al., 2011).

Negotiation dialogue systems aim to build intelligent conversational agents that can automatically negotiate with a human in natural languages.

Agents negotiate with humans through multi-turn interaction using logical reasoning over goals, strategies, and psychological factors.

A successful negotiation process involves efficient information exchange, strategic discussion toward their goals, and a closing section.

Modeling the negotiation process for conversational agents imposes challenges, such as reasoning about and employing various strategies in different situations.

In addition to strategy modeling, it is necessary to model the personalities (e.g., mind, emotion, and behaviors) of the negotiators.

An effective policy learning method is essential for the successful use of language.

To address these challenges, existing solutions can be categorized into three areas: personality modeling, strategy modeling, and policy learning methods.","Question:

What are the key components and processes involved in a successful negotiation dialogue system?

Answer:

Negotiation is one of the crucial abilities in human communication that involves two or more individuals discussing goals and tactics to resolve conflicts, achieve mutual benefit, or find mutually acceptable solutions (Fershtman, 1990; Bazerman and Neale, 1993; Lewicki et al., 2011).

Negotiation dialogue systems aim to build intelligent conversational agents that can automatically negotiate with a human in natural languages.

Agents negotiate with humans through multi-turn interaction using logical reasoning over goals, strategies, and psychological factors.

A successful negotiation process involves efficient information exchange, strategic discussion toward their goals, and a closing section.

Modeling the negotiation process for conversational agents imposes challenges, such as reasoning about and employing various strategies in different situations.

In addition to strategy modeling, it is necessary to model the personalities (e.g., mind, emotion, and behaviors) of the negotiators.

An effective policy learning method is essential for the successful use of language.

To address these challenges, existing solutions can be categorized into three areas: personality modeling, strategy modeling, and policy learning methods."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,John F. Kennedy,15,"Negotiation is one of the crucial abilities in human communication that involves two or more individuals discussing goals and tactics to resolve conflicts, achieve mutual benefit, or find mutually acceptable solutions (Fershtman, 1990;Bazerman and Neale, 1993;Lewicki et al., 2011). It is a common aspect of human interaction, occurring whenever people communicate in order to manage conflict or reach a compromise. Scientifically, one of the long-term goals of dialogue research is to empower intelligent agents with such ability. Agent effectively negotiating with a human in natural language could have significant benefits in many scenarios, from bargaining prices in everyday trade-in (He et al., 2018) to high-stakes political or legal situations (Basave and He, 2016). Negotiation dialogue systems (Lewandowska, 1982;Lambert and Carberry, 1992;Chawla et al., 2021c) is an emerging research field that aims to build intelligent conversational agents that can automatically negotiate with a human in natural languages, e.g., CICERO 1 from Meta AI. Agents negotiate with human through multi-turn interaction using logically reasoning (Sycara and Dai, 2010) over goals (Zhang et al., 2020), strategies (Zhou et al., 2020) and psychology factors (Yang et al., 2021). As illustrated in Figure 1, negotiation dialogue agents interact with the human through multiturn cycles. A successful negotiation process involves efficient information exchange, strategic discussion toward their goals, and a closing section.

Despite the significant amount of research that has been conducted on the task, there is a lack of a systematic review of the topic. In this work, we aim to fill this gap by reviewing contemporary work in the emerging field of negotiation dialogue systems, covering aspects such as benchmarks, evaluation, methodology, and future directions. In recent years, various benchmarks have been proposed for negotiation dialogue systems, ranging from bargaining (Lewis et al., 2017) and game scenarios (Asher et al., 2016) to job interviews  and items exchanging (Chawla et al., 2021c). Our survey will provide an overview of these benchmarks and discuss how they have been used to evaluate the performance of negotiation dialogue systems.

Modeling the negotiation process for conversational agents also imposes challenges. Firstly, these agents must be able to reason about and employ various strategies in different situations. In addition to strategy modeling, it is also necessary to model the personalities (e.g., mind, emotion, and behaviors) of the negotiators. Thirdly, an effective policy learning method is essential for the successful use of language. To address these challenges, we can categorize existing solutions into three areas: (1) Personality modeling helps us understand negotiator's preferences, (2) Strategy modeling enables agents to make reasonable decisions based on gathered information, and (3) Policy learning methods utilize information effectively to maximize results.

In summary, our contributions are three-fold: (1) To the best of our knowledge, we systematically categorize current negotiation dialogue benchmarks from the perspective of distributive and integrative, with each category based on different goal types of negotiation dialogue tasks. (2) We categorize typical evaluation methods and current solutions into an appropriate taxonomy. (3) We pointed out the current limitation and promising research directions in the future.","sent1: Negotiation is one of the crucial abilities in human communication that involves two or more individuals discussing goals and tactics to resolve conflicts, achieve mutual benefit, or find mutually acceptable solutions (Fershtman, 1990;Bazerman and Neale, 1993;Lewicki et al., 2011).
sent2: It is a common aspect of human interaction, occurring whenever people communicate in order to manage conflict or reach a compromise.
sent3: Scientifically, one of the long-term goals of dialogue research is to empower intelligent agents with such ability.
sent4: Agent effectively negotiating with a human in natural language could have significant benefits in many scenarios, from bargaining prices in everyday trade-in (He et al., 2018) to high-stakes political or legal situations (Basave and He, 2016).
sent5: Negotiation dialogue systems (Lewandowska, 1982;Lambert and Carberry, 1992;Chawla et al., 2021c) is an emerging research field that aims to build intelligent conversational agents that can automatically negotiate with a human in natural languages, e.g., CICERO 1 from Meta AI.
sent6: Agents negotiate with human through multi-turn interaction using logically reasoning (Sycara and Dai, 2010) over goals (Zhang et al., 2020), strategies (Zhou et al., 2020) and psychology factors (Yang et al., 2021).
sent7: As illustrated in Figure 1, negotiation dialogue agents interact with the human through multiturn cycles.
sent8: A successful negotiation process involves efficient information exchange, strategic discussion toward their goals, and a closing section.
sent9: Despite the significant amount of research that has been conducted on the task, there is a lack of a systematic review of the topic.
sent10: In this work, we aim to fill this gap by reviewing contemporary work in the emerging field of negotiation dialogue systems, covering aspects such as benchmarks, evaluation, methodology, and future directions.
sent11: In recent years, various benchmarks have been proposed for negotiation dialogue systems, ranging from bargaining (Lewis et al., 2017) and game scenarios (Asher et al., 2016) to job interviews  and items exchanging (Chawla et al., 2021c).
sent12: Our survey will provide an overview of these benchmarks and discuss how they have been used to evaluate the performance of negotiation dialogue systems.
sent13: Modeling the negotiation process for conversational agents also imposes challenges.
sent14: Firstly, these agents must be able to reason about and employ various strategies in different situations.
sent15: In addition to strategy modeling, it is also necessary to model the personalities (e.g., mind, emotion, and behaviors) of the negotiators.
sent16: Thirdly, an effective policy learning method is essential for the successful use of language.
sent17: To address these challenges, we can categorize existing solutions into three areas: (1) Personality modeling helps us understand negotiator's preferences, (2) Strategy modeling enables agents to make reasonable decisions based on gathered information, and (3) Policy learning methods utilize information effectively to maximize results.
sent18: In summary, our contributions are three-fold: (1) To the best of our knowledge, we systematically categorize current negotiation dialogue benchmarks from the perspective of distributive and integrative, with each category based on different goal types of negotiation dialogue tasks.
sent19: (2) We categorize typical evaluation methods and current solutions into an appropriate taxonomy.
sent20: (3) We pointed out the current limitation and promising research directions in the future.","1. What is negotiation in the context of human communication? sent1
    1.1. How common is negotiation in human interaction? sent2
    1.2. What is one of the long-term goals of dialogue research related to negotiation? sent3
    1.3. What are some potential benefits of agents effectively negotiating with humans? sent4
2. What is the emerging research field of negotiation dialogue systems? sent5
    2.1. How do agents negotiate with humans in negotiation dialogue systems? sent6
        2.1.1. How do negotiation dialogue agents interact with humans? sent7
        2.1.2. What does a successful negotiation process involve? sent8
3. What is the current state of research on negotiation dialogue systems? sent9
    3.1. What does this work aim to achieve in the field of negotiation dialogue systems? sent10
    3.2. What types of benchmarks have been proposed for negotiation dialogue systems? sent11
        3.2.1. How will the survey address these benchmarks? sent12
4. What challenges are involved in modeling the negotiation process for conversational agents? sent13
    4.1. What is the first challenge related to strategy modeling? sent14
    4.2. What is necessary in addition to strategy modeling? sent15
    4.3. What is the third challenge related to policy learning? sent16
    4.4. How can existing solutions to these challenges be categorized? sent17
5. What are the contributions of this work? sent18, sent19, sent20","1. What is negotiation in the context of human communication? sent1
    1.1. How common is negotiation in human interaction? sent2
    1.2. What is one of the long-term goals of dialogue research related to negotiation? sent3
    1.3. What are some potential benefits of agents effectively negotiating with humans? sent4

2. What is the emerging research field of negotiation dialogue systems? sent5
    2.1. How do agents negotiate with humans in negotiation dialogue systems? sent6
        2.1.1. How do negotiation dialogue agents interact with humans? sent7
        2.1.2. What does a successful negotiation process involve? sent8

3. What is the current state of research on negotiation dialogue systems? sent9
    3.1. What does this work aim to achieve in the field of negotiation dialogue systems? sent10
    3.2. What types of benchmarks have been proposed for negotiation dialogue systems? sent11
        3.2.1. How will the survey address these benchmarks? sent12

4. What challenges are involved in modeling the negotiation process for conversational agents? sent13
    4.1. What is the first challenge related to strategy modeling? sent14
    4.2. What is necessary in addition to strategy modeling? sent15
    4.3. What is the third challenge related to policy learning? sent16
    4.4. How can existing solutions to these challenges be categorized? sent17","4. What challenges are involved in modeling the negotiation process for conversational agents? sent13
    4.1. What is the first challenge related to strategy modeling? sent14
    4.2. What is necessary in addition to strategy modeling? sent15
    4.3. What is the third challenge related to policy learning? sent16
    4.4. How can existing solutions to these challenges be categorized? sent17","Question: How are existing solutions to challenges in negotiation dialogue systems categorized?

Supporting sentence: sent17",How are existing solutions to challenges in negotiation dialogue systems categorized?,sent17,4,implement,"Existing solutions to challenges in negotiation dialogue systems can be categorized into three areas.

Firstly, personality modeling helps in understanding negotiators' preferences.

Secondly, strategy modeling enables agents to make reasonable decisions based on gathered information.

Thirdly, policy learning methods utilize information effectively to maximize results.","Question:

How are existing solutions to challenges in negotiation dialogue systems categorized?

Answer:

Existing solutions to challenges in negotiation dialogue systems can be categorized into three areas.

Firstly, personality modeling helps in understanding negotiators' preferences.

Secondly, strategy modeling enables agents to make reasonable decisions based on gathered information.

Thirdly, policy learning methods utilize information effectively to maximize results."
252992688,A Survey of Active Learning for Natural Language Processing,https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,Output Uncertainty,11,"Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","sent1: Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.
sent2: It prefers the most uncertain instances judged by the model outputs.
sent3: For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).
sent4: Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.
sent5: For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).
sent6: Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.
sent7: If an instance is near the decision boundary, the model's outputs may be different within its local region.
sent8: In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","1. What is uncertainty sampling, and why is it commonly used? sent1
    1.1. How does uncertainty sampling select instances? sent2
    1.2. What are typical uncertainty sampling strategies for probabilistic models? sent3
        1.2.1. How have uncertainty-based strategies been revisited with Transformer-based models? sent4
    1.3. How can uncertainty sampling be applied to non-probabilistic models? sent5
2. What is another way to measure output uncertainty? sent6
    2.1. How does the proximity to the decision boundary affect a model's output? sent7
    2.2. What are some recent methods to examine instances' local divergence? sent8","1. What is uncertainty sampling, and why is it commonly used? sent1
    1.1. How does uncertainty sampling select instances? sent2
    1.2. What are typical uncertainty sampling strategies for probabilistic models? sent3
        1.2.1. How have uncertainty-based strategies been revisited with Transformer-based models? sent4
    1.3. How can uncertainty sampling be applied to non-probabilistic models? sent5","1. What is uncertainty sampling, and why is it commonly used? sent1
    1.1. How does uncertainty sampling select instances? sent2
    1.2. What are typical uncertainty sampling strategies for probabilistic models? sent3
        1.2.1. How have uncertainty-based strategies been revisited with Transformer-based models? sent4
    1.3. How can uncertainty sampling be applied to non-probabilistic models? sent5","Question: How is output uncertainty measured in non-probabilistic models, and what are some recent methods to assess it?

Supporting sentence: sent5, sent6, sent7, sent8","How is output uncertainty measured in non-probabilistic models, and what are some recent methods to assess it?","sent5, sent6, sent7, sent8",4,implement,"For non-probabilistic models, output uncertainty can be measured by selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000; Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.

If an instance is near the decision boundary, the model's outputs may be different within its local region.

In this spirit, recent works examine different ways to check instances' local divergence, such as nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020).","Question:

How is output uncertainty measured in non-probabilistic models, and what are some recent methods to assess it?

Answer:

For non-probabilistic models, output uncertainty can be measured by selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000; Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.

If an instance is near the decision boundary, the model's outputs may be different within its local region.

In this spirit, recent works examine different ways to check instances' local divergence, such as nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020)."
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,Syntactic knowledge,5,"As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","sent1: As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it.
sent2: Htut et al.(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root.
sent3: Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation.
sent4: However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2).
sent5: Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).
sent6: Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task.
sent7: This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences.
sent8: A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations.
sent9: The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input.
sent10: In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019).
sent11: This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","1. How is syntactic information represented in BERT's self-attention weights? sent1
    1.1. What did Htut et al. (2019) find regarding extracting parse trees from BERT heads? sent2
    1.2. What did Jawahar et al. (2019) illustrate about dependency trees and self-attention weights? sent3
    1.3. How did Hewitt and Manning (2019) contribute to understanding syntactic structure in BERT? sent4
    1.4. What did Jawahar et al. (2019) conclude about BERT representations and decomposition schemes? sent5
2. What syntactic competence does BERT's MLM demonstrate according to Goldberg (2019)? sent6
    2.1. How does BERT handle sentences with distractor clauses and meaningless sentences? sent7
3. What did Warstadt et al. (2019) find about BERT's ability to detect negative polarity items (NPIs)? sent8
4. What limitations in BERT's syntactic knowledge are highlighted by its handling of negation and malformed input? sent9
    4.1. How does BERT's prediction behavior change with altered sentence structures? sent10
    4.2. What do recent findings on adversarial attacks suggest about BERT's reliance on syntactic knowledge? sent11","1. How is syntactic information represented in BERT's self-attention weights? sent1
    1.1. What did Htut et al. (2019) find regarding extracting parse trees from BERT heads? sent2
    1.2. What did Jawahar et al. (2019) illustrate about dependency trees and self-attention weights? sent3
    1.3. How did Hewitt and Manning (2019) contribute to understanding syntactic structure in BERT? sent4
    1.4. What did Jawahar et al. (2019) conclude about BERT representations and decomposition schemes? sent5","1. How is syntactic information represented in BERT's self-attention weights? sent1
    1.1. What did Htut et al. (2019) find regarding extracting parse trees from BERT heads? sent2
    1.2. What did Jawahar et al. (2019) illustrate about dependency trees and self-attention weights? sent3
    1.3. How did Hewitt and Manning (2019) contribute to understanding syntactic structure in BERT? sent4
    1.4. What did Jawahar et al. (2019) conclude about BERT representations and decomposition schemes? sent5","Question: How do studies illustrate the representation and transformation of syntactic structure in BERT's self-attention weights?

Supporting sentence: sent1, sent2, sent3, sent4, sent5",How do studies illustrate the representation and transformation of syntactic structure in BERT's self-attention weights?,"sent1, sent2, sent3, sent4, sent5",2,implement,"Syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it.

Htut et al. (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root.

Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation.

However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data.

Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks, concluding that the dependency trees are the best match among 5 decomposition schemes, although the reported MSE differences are very small.","Question:

How do studies illustrate the representation and transformation of syntactic structure in BERT's self-attention weights?

Answer:

Syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it.

Htut et al. (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root.

Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation.

However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data.

Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks, concluding that the dependency trees are the best match among 5 decomposition schemes, although the reported MSE differences are very small."
258832362,Beyond Words: A Comprehensive Survey of Sentence Representations,https://www.semanticscholar.org/paper/8579ad4a8e835cada64c0eae142a00205ce857b5,Model Level,4,"Another approach to generating positive examples is by leveraging the distinctive characteristics of the backbone model utilized in contrastive learning. These characteristics might be the architectural choices or using representation from certain components of the model.

Dropout is a regularization technique used in deep learning to prevent overfitting of a model. During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times. These different representations can be used as positive examples for sentence representations. Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy. Several other works have also incorporated this technique and improved upon it: promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d).

Specific components of language models can be trained to generate semantically similar representations. One example is the use of prefix modules (Li and Liang, 2021), which are small, trainable modules added to a pretrained language model. In a recent study, Wang and Lu (2022) trained prefix modules on NLI data before using them to generate positive examples for unsupervised sentence representations. a","sent1: Another approach to generating positive examples is by leveraging the distinctive characteristics of the backbone model utilized in contrastive learning.
sent2: These characteristics might be the architectural choices or using representation from certain components of the model.
sent3: Dropout is a regularization technique used in deep learning to prevent overfitting of a model.
sent4: During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times.
sent5: These different representations can be used as positive examples for sentence representations.
sent6: Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy.
sent7: Several other works have also incorporated this technique and improved upon it: promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d).
sent8: Specific components of language models can be trained to generate semantically similar representations.
sent9: One example is the use of prefix modules (Li and Liang, 2021), which are small, trainable modules added to a pretrained language model.
sent10: In a recent study, Wang and Lu (2022) trained prefix modules on NLI data before using them to generate positive examples for unsupervised sentence representations.
sent11: a","1. How can positive examples be generated in contrastive learning using the backbone model's characteristics? sent1
    1.1. What characteristics of the backbone model might be leveraged? sent2
2. What is dropout, and how is it used in deep learning? sent3
    2.1. How does dropout affect neuron activation during training? sent4
    2.2. How can dropout-generated representations be used in sentence representations? sent5
    2.3. What have recent studies shown about using dropout as an augmentation strategy? sent6
    2.4. How have other works improved upon the dropout technique? sent7
3. How can specific components of language models be trained for semantically similar representations? sent8
    3.1. What is an example of a component used to generate semantically similar representations? sent9
    3.2. How were prefix modules used in a recent study for generating positive examples? sent10","2. What is dropout, and how is it used in deep learning? sent3
    2.1. How does dropout affect neuron activation during training? sent4
    2.2. How can dropout-generated representations be used in sentence representations? sent5
    2.3. What have recent studies shown about using dropout as an augmentation strategy? sent6
    2.4. How have other works improved upon the dropout technique? sent7","2. What is dropout, and how is it used in deep learning? sent3
    2.1. How does dropout affect neuron activation during training? sent4
    2.2. How can dropout-generated representations be used in sentence representations? sent5
    2.3. What have recent studies shown about using dropout as an augmentation strategy? sent6
    2.4. How have other works improved upon the dropout technique? sent7","Question: How is dropout used to generate positive examples for sentence representations, and what improvements have been made to this technique?

Supporting sentence: sent3, sent4, sent5, sent6, sent7","How is dropout used to generate positive examples for sentence representations, and what improvements have been made to this technique?","sent3, sent4, sent5, sent6, sent7",2,implement,"Dropout is a regularization technique used in deep learning to prevent overfitting of a model.

During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times.

These different representations can be used as positive examples for sentence representations.

Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy.

Several other works have also incorporated this technique and improved upon it: promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d).","Question:

How is dropout used to generate positive examples for sentence representations, and what improvements have been made to this technique?

Answer:

Dropout is a regularization technique used in deep learning to prevent overfitting of a model.

During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times.

These different representations can be used as positive examples for sentence representations.

Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy.

Several other works have also incorporated this technique and improved upon it: promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d)."
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,Method Decision based on,26,"Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc. Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)

PoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; ""flush"" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information. LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed. Structural information that naturally exists in sentences can also play a role in skimming. Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).

In the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers. The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores. The number of tokens to be dropped, referred to as the ""schedule,"" is optimized by combining the sparsity of a soft mask layer with the original loss function. This results in an improved balance between accuracy and processing time. TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained. In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them. The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning. The schedule for LAT is found through an evolutionary search algorithm. LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule. It simply drops tokens with attention scores lower than the learned threshold. Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer. This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.

Computation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely. VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step. Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.

Instead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN. At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token. If the model decides to skim, the small RNN will update only a fraction of the hid-den states. Otherwise, a regular full update will be conducted by the big RNN.

Dynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed. HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture. In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.

In question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed. Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering. It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.","sent1: Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc.
sent2: Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)PoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens
sent3: Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; ""flush"" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information.
sent4: LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed.
sent5: Structural information that naturally exists in sentences can also play a role in skimming.
sent6: Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).
sent7: In the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers.
sent8: The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores.
sent9: The number of tokens to be dropped, referred to as the ""schedule,"" is optimized by combining the sparsity of a soft mask layer with the original loss function.
sent10: This results in an improved balance between accuracy and processing time.
sent11: TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained.
sent12: In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them.
sent13: The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning.
sent14: The schedule for LAT is found through an evolutionary search algorithm.
sent15: LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule.
sent16: It simply drops tokens with attention scores lower than the learned threshold.
sent17: Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer.
sent18: This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.
sent19: Computation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely.
sent20: VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step.
sent21: Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.
sent22: Instead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN.
sent23: At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token.
sent24: If the model decides to skim, the small RNN will update only a fraction of the hid-den states.
sent25: Otherwise, a regular full update will be conducted by the big RNN.
sent26: Dynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed.
sent27: HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture.
sent28: In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.
sent29: In question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed.
sent30: Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering.
sent31: It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.","1. What are some operation options for dynamic neural networks in NLP? sent1
    1.1. What are some specific models that utilize these operation options? sent2
    1.2. How does LSTM-Shuttle utilize a bidirectional shuttling mechanism? sent4
2. How can structural information in sentences aid in skimming? sent5
    2.1. How does Structural Jump-LSTM utilize structural information for skimming? sent6
3. How have Transformers been adapted to reduce computation? sent7
    3.1. How does the PoWER-BERT model reduce the number of tokens processed? sent8
        3.1.1. How is the ""schedule"" for token dropping optimized in PoWER-BERT? sent9
        3.1.2. What is the result of optimizing the schedule in PoWER-BERT? sent10
    3.2. How does TR-BERT determine which tokens to skip? sent11
        3.2.1. How does TR-BERT handle skipped tokens differently from PoWER-BERT? sent12
    3.3. How does the Length-Adaptive Transformer (LAT) utilize LengthDrop? sent13
        3.3.1. How is the schedule for LAT found? sent14
    3.4. How does LTP train a threshold for each Transformer layer? sent15
        3.4.1. How does LTP decide which tokens to drop? sent16
    3.5. How does Transkimmer incorporate a skim predictor module? sent17
        3.5.1. What is the purpose of the skim loss in Transkimmer? sent18
4. What is the difference between computation reduction and skipping in dynamic neural networks? sent19
    4.1. How does VCRNN decide the proportion of computation to use for each time step? sent20
        4.1.1. What happens after VCRNN makes a decision on computation proportion? sent21
    4.2. How does Skim-RNN decide whether to read or skim at each time step? sent22
        4.2.1. What happens if Skim-RNN decides to skim? sent23, sent24
        4.2.2. What happens if Skim-RNN decides not to skim? sent25
5. How does dynamic hierarchical RNN differ from other skimming methods? sent26
    5.1. How does HM-RNN discover hierarchical multi-scale structures? sent27
        5.1.1. What additional operation does HM-RNN include compared to Skip RNN? sent28
    5.2. How does Focused Hierarchical RNN aim to improve question answering? sent29
        5.2.1. How does Focused Hierarchical RNN control updates to the upper layer of the RNN? sent30, sent31","3. How have Transformers been adapted to reduce computation? sent7
    3.1. How does the PoWER-BERT model reduce the number of tokens processed? sent8
        3.1.1. How is the ""schedule"" for token dropping optimized in PoWER-BERT? sent9
        3.1.2. What is the result of optimizing the schedule in PoWER-BERT? sent10
    3.2. How does TR-BERT determine which tokens to skip? sent11
        3.2.1. How does TR-BERT handle skipped tokens differently from PoWER-BERT? sent12
    3.3. How does the Length-Adaptive Transformer (LAT) utilize LengthDrop? sent13
        3.3.1. How is the schedule for LAT found? sent14
    3.4. How does LTP train a threshold for each Transformer layer? sent15
        3.4.1. How does LTP decide which tokens to drop? sent16
    3.5. How does Transkimmer incorporate a skim predictor module? sent17
        3.5.1. What is the purpose of the skim loss in Transkimmer? sent18

4. What is the difference between computation reduction and skipping in dynamic neural networks? sent19
    4.1. How does VCRNN decide the proportion of computation to use for each time step? sent20
        4.1.1. What happens after VCRNN makes a decision on computation proportion? sent21
    4.2. How does Skim-RNN decide whether to read or skim at each time step? sent22
        4.2.1. What happens if Skim-RNN decides to skim? sent23, sent24
        4.2.2. What happens if Skim-RNN decides not to skim? sent25

    4.2. How does Skim-RNN decide whether to read or skim at each time step? sent22
        4.2.1. What happens if Skim-RNN decides to skim? sent23, sent24
        4.2.2. What happens if Skim-RNN decides not to skim? sent25

5. How does dynamic hierarchical RNN differ from other skimming methods? sent26
    5.1. How does HM-RNN discover hierarchical multi-scale structures? sent27
        5.1.1. What additional operation does HM-RNN include compared to Skip RNN? sent28
    5.2. How does Focused Hierarchical RNN aim to improve question answering? sent29
        5.2.1. How does Focused Hierarchical RNN control updates to the upper layer of the RNN? sent30, sent31","3. How have Transformers been adapted to reduce computation? sent7
    3.1. How does the PoWER-BERT model reduce the number of tokens processed? sent8
        3.1.1. How is the ""schedule"" for token dropping optimized in PoWER-BERT? sent9
        3.1.2. What is the result of optimizing the schedule in PoWER-BERT? sent10
    3.2. How does TR-BERT determine which tokens to skip? sent11
        3.2.1. How does TR-BERT handle skipped tokens differently from PoWER-BERT? sent12
    3.3. How does the Length-Adaptive Transformer (LAT) utilize LengthDrop? sent13
        3.3.1. How is the schedule for LAT found? sent14
    3.4. How does LTP train a threshold for each Transformer layer? sent15
        3.4.1. How does LTP decide which tokens to drop? sent16
    3.5. How does Transkimmer incorporate a skim predictor module? sent17
        3.5.1. What is the purpose of the skim loss in Transkimmer? sent18","Question: How do various Transformer models implement token skipping to enhance computational efficiency?

Supporting sentence: sent7, sent8, sent9, sent10, sent11, sent12, sent13, sent14, sent15, sent16, sent17, sent18",How do various Transformer models implement token skipping to enhance computational efficiency?,"sent7, sent8, sent9, sent10, sent11, sent12, sent13, sent14, sent15, sent16, sent17, sent18",2,implement,"To enhance computational efficiency, various Transformer models implement token skipping in different ways.

The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores, optimizing the number of tokens to be dropped by combining the sparsity of a soft mask layer with the original loss function.

TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, employing reinforcement learning to train the model with a reward system that prioritizes classifier confidence while penalizing the number of tokens retained.

Unlike PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them.

The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, with the schedule for LAT found through an evolutionary search algorithm.

LTP trains a threshold for each Transformer layer, dropping tokens with attention scores lower than the learned threshold.

Transkimmer (Guan et al., 2022) incorporates a skim predictor module before each layer, consisting of a small MLP and Gumbel-Softmax reparameterization, which outputs a mask to determine whether a token should be dropped, optimizing the ratio of skipped tokens to total tokens to promote sparsity.","Question:

How do various Transformer models implement token skipping to enhance computational efficiency?

Answer:

To enhance computational efficiency, various Transformer models implement token skipping in different ways.

The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores, optimizing the number of tokens to be dropped by combining the sparsity of a soft mask layer with the original loss function.

TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, employing reinforcement learning to train the model with a reward system that prioritizes classifier confidence while penalizing the number of tokens retained.

Unlike PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them.

The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, with the schedule for LAT found through an evolutionary search algorithm.

LTP trains a threshold for each Transformer layer, dropping tokens with attention scores lower than the learned threshold.

Transkimmer (Guan et al., 2022) incorporates a skim predictor module before each layer, consisting of a small MLP and Gumbel-Softmax reparameterization, which outputs a mask to determine whether a token should be dropped, optimizing the ratio of skipped tokens to total tokens to promote sparsity."
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,Method Decision based on,26,"Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc. Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)

PoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; ""flush"" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information. LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed. Structural information that naturally exists in sentences can also play a role in skimming. Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).

In the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers. The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores. The number of tokens to be dropped, referred to as the ""schedule,"" is optimized by combining the sparsity of a soft mask layer with the original loss function. This results in an improved balance between accuracy and processing time. TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained. In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them. The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning. The schedule for LAT is found through an evolutionary search algorithm. LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule. It simply drops tokens with attention scores lower than the learned threshold. Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer. This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.

Computation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely. VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step. Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.

Instead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN. At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token. If the model decides to skim, the small RNN will update only a fraction of the hid-den states. Otherwise, a regular full update will be conducted by the big RNN.

Dynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed. HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture. In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.

In question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed. Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering. It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.","sent1: Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc.
sent2: Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)PoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens
sent3: Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; ""flush"" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information.
sent4: LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed.
sent5: Structural information that naturally exists in sentences can also play a role in skimming.
sent6: Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).
sent7: In the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers.
sent8: The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores.
sent9: The number of tokens to be dropped, referred to as the ""schedule,"" is optimized by combining the sparsity of a soft mask layer with the original loss function.
sent10: This results in an improved balance between accuracy and processing time.
sent11: TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained.
sent12: In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them.
sent13: The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning.
sent14: The schedule for LAT is found through an evolutionary search algorithm.
sent15: LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule.
sent16: It simply drops tokens with attention scores lower than the learned threshold.
sent17: Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer.
sent18: This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.
sent19: Computation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely.
sent20: VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step.
sent21: Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.
sent22: Instead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN.
sent23: At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token.
sent24: If the model decides to skim, the small RNN will update only a fraction of the hid-den states.
sent25: Otherwise, a regular full update will be conducted by the big RNN.
sent26: Dynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed.
sent27: HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture.
sent28: In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.
sent29: In question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed.
sent30: Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering.
sent31: It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.","1. What are some operation options for dynamic neural networks in NLP? sent1
    1.1. What are some specific models that utilize these operation options? sent2
    1.2. How does LSTM-Shuttle utilize a bidirectional shuttling mechanism? sent4
2. How can structural information in sentences aid in skimming? sent5
    2.1. How does Structural Jump-LSTM utilize structural information for skimming? sent6
3. How have Transformers been adapted to reduce computation? sent7
    3.1. How does the PoWER-BERT model reduce the number of tokens processed? sent8
        3.1.1. How is the ""schedule"" for token dropping optimized in PoWER-BERT? sent9
        3.1.2. What is the result of optimizing the schedule in PoWER-BERT? sent10
    3.2. How does TR-BERT determine which tokens to skip? sent11
        3.2.1. How does TR-BERT handle skipped tokens differently from PoWER-BERT? sent12
    3.3. How does the Length-Adaptive Transformer (LAT) utilize LengthDrop? sent13
        3.3.1. How is the schedule for LAT found? sent14
    3.4. How does LTP train a threshold for each Transformer layer? sent15
        3.4.1. How does LTP decide which tokens to drop? sent16
    3.5. How does Transkimmer incorporate a skim predictor module? sent17
        3.5.1. What is the purpose of the skim loss in Transkimmer? sent18
4. What is the difference between computation reduction and skipping in dynamic neural networks? sent19
    4.1. How does VCRNN decide the proportion of computation to use for each time step? sent20
        4.1.1. What happens after VCRNN makes a decision on computation proportion? sent21
    4.2. How does Skim-RNN decide whether to read or skim at each time step? sent22
        4.2.1. What happens if Skim-RNN decides to skim? sent23, sent24
        4.2.2. What happens if Skim-RNN decides not to skim? sent25
5. How does dynamic hierarchical RNN differ from other skimming methods? sent26
    5.1. How does HM-RNN discover hierarchical multi-scale structures? sent27
        5.1.1. What additional operation does HM-RNN include compared to Skip RNN? sent28
    5.2. How does Focused Hierarchical RNN aim to improve question answering? sent29
        5.2.1. How does Focused Hierarchical RNN control updates to the upper layer of the RNN? sent30, sent31","3. How have Transformers been adapted to reduce computation? sent7
    3.1. How does the PoWER-BERT model reduce the number of tokens processed? sent8
        3.1.1. How is the ""schedule"" for token dropping optimized in PoWER-BERT? sent9
        3.1.2. What is the result of optimizing the schedule in PoWER-BERT? sent10
    3.2. How does TR-BERT determine which tokens to skip? sent11
        3.2.1. How does TR-BERT handle skipped tokens differently from PoWER-BERT? sent12
    3.3. How does the Length-Adaptive Transformer (LAT) utilize LengthDrop? sent13
        3.3.1. How is the schedule for LAT found? sent14
    3.4. How does LTP train a threshold for each Transformer layer? sent15
        3.4.1. How does LTP decide which tokens to drop? sent16
    3.5. How does Transkimmer incorporate a skim predictor module? sent17
        3.5.1. What is the purpose of the skim loss in Transkimmer? sent18

4. What is the difference between computation reduction and skipping in dynamic neural networks? sent19
    4.1. How does VCRNN decide the proportion of computation to use for each time step? sent20
        4.1.1. What happens after VCRNN makes a decision on computation proportion? sent21
    4.2. How does Skim-RNN decide whether to read or skim at each time step? sent22
        4.2.1. What happens if Skim-RNN decides to skim? sent23, sent24
        4.2.2. What happens if Skim-RNN decides not to skim? sent25

    4.2. How does Skim-RNN decide whether to read or skim at each time step? sent22
        4.2.1. What happens if Skim-RNN decides to skim? sent23, sent24
        4.2.2. What happens if Skim-RNN decides not to skim? sent25

5. How does dynamic hierarchical RNN differ from other skimming methods? sent26
    5.1. How does HM-RNN discover hierarchical multi-scale structures? sent27
        5.1.1. What additional operation does HM-RNN include compared to Skip RNN? sent28
    5.2. How does Focused Hierarchical RNN aim to improve question answering? sent29
        5.2.1. How does Focused Hierarchical RNN control updates to the upper layer of the RNN? sent30, sent31","4. What is the difference between computation reduction and skipping in dynamic neural networks? sent19
    4.1. How does VCRNN decide the proportion of computation to use for each time step? sent20
        4.1.1. What happens after VCRNN makes a decision on computation proportion? sent21
    4.2. How does Skim-RNN decide whether to read or skim at each time step? sent22
        4.2.1. What happens if Skim-RNN decides to skim? sent23, sent24
        4.2.2. What happens if Skim-RNN decides not to skim? sent25","Question: How does Skim-RNN manage computation when deciding to skim or not at each time step?

Supporting sentence: sent22, sent23, sent24, sent25",How does Skim-RNN manage computation when deciding to skim or not at each time step?,"sent22, sent23, sent24, sent25",1,implement,"At each time step, Skim-RNN (Seo et al., 2018) decides whether to read or skim based on hidden states from the last time step and the input token.

If the model decides to skim, the small RNN will update only a fraction of the hidden states.

Otherwise, a regular full update will be conducted by the big RNN.","Question:

How does Skim-RNN manage computation when deciding to skim or not at each time step?

Answer:

At each time step, Skim-RNN (Seo et al., 2018) decides whether to read or skim based on hidden states from the last time step and the input token.

If the model decides to skim, the small RNN will update only a fraction of the hidden states.

Otherwise, a regular full update will be conducted by the big RNN."
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,Method Decision based on,26,"Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc. Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)

PoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; ""flush"" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information. LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed. Structural information that naturally exists in sentences can also play a role in skimming. Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).

In the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers. The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores. The number of tokens to be dropped, referred to as the ""schedule,"" is optimized by combining the sparsity of a soft mask layer with the original loss function. This results in an improved balance between accuracy and processing time. TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained. In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them. The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning. The schedule for LAT is found through an evolutionary search algorithm. LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule. It simply drops tokens with attention scores lower than the learned threshold. Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer. This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.

Computation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely. VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step. Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.

Instead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN. At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token. If the model decides to skim, the small RNN will update only a fraction of the hid-den states. Otherwise, a regular full update will be conducted by the big RNN.

Dynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed. HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture. In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.

In question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed. Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering. It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.","sent1: Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc.
sent2: Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)PoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens
sent3: Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; ""flush"" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information.
sent4: LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed.
sent5: Structural information that naturally exists in sentences can also play a role in skimming.
sent6: Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).
sent7: In the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers.
sent8: The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores.
sent9: The number of tokens to be dropped, referred to as the ""schedule,"" is optimized by combining the sparsity of a soft mask layer with the original loss function.
sent10: This results in an improved balance between accuracy and processing time.
sent11: TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained.
sent12: In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them.
sent13: The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning.
sent14: The schedule for LAT is found through an evolutionary search algorithm.
sent15: LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule.
sent16: It simply drops tokens with attention scores lower than the learned threshold.
sent17: Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer.
sent18: This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.
sent19: Computation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely.
sent20: VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step.
sent21: Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.
sent22: Instead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN.
sent23: At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token.
sent24: If the model decides to skim, the small RNN will update only a fraction of the hid-den states.
sent25: Otherwise, a regular full update will be conducted by the big RNN.
sent26: Dynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed.
sent27: HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture.
sent28: In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.
sent29: In question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed.
sent30: Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering.
sent31: It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.","1. What are some operation options for dynamic neural networks in NLP? sent1
    1.1. What are some specific models that utilize these operation options? sent2
    1.2. How does LSTM-Shuttle utilize a bidirectional shuttling mechanism? sent4
2. How can structural information in sentences aid in skimming? sent5
    2.1. How does Structural Jump-LSTM utilize structural information for skimming? sent6
3. How have Transformers been adapted to reduce computation? sent7
    3.1. How does the PoWER-BERT model reduce the number of tokens processed? sent8
        3.1.1. How is the ""schedule"" for token dropping optimized in PoWER-BERT? sent9
        3.1.2. What is the result of optimizing the schedule in PoWER-BERT? sent10
    3.2. How does TR-BERT determine which tokens to skip? sent11
        3.2.1. How does TR-BERT handle skipped tokens differently from PoWER-BERT? sent12
    3.3. How does the Length-Adaptive Transformer (LAT) utilize LengthDrop? sent13
        3.3.1. How is the schedule for LAT found? sent14
    3.4. How does LTP train a threshold for each Transformer layer? sent15
        3.4.1. How does LTP decide which tokens to drop? sent16
    3.5. How does Transkimmer incorporate a skim predictor module? sent17
        3.5.1. What is the purpose of the skim loss in Transkimmer? sent18
4. What is the difference between computation reduction and skipping in dynamic neural networks? sent19
    4.1. How does VCRNN decide the proportion of computation to use for each time step? sent20
        4.1.1. What happens after VCRNN makes a decision on computation proportion? sent21
    4.2. How does Skim-RNN decide whether to read or skim at each time step? sent22
        4.2.1. What happens if Skim-RNN decides to skim? sent23, sent24
        4.2.2. What happens if Skim-RNN decides not to skim? sent25
5. How does dynamic hierarchical RNN differ from other skimming methods? sent26
    5.1. How does HM-RNN discover hierarchical multi-scale structures? sent27
        5.1.1. What additional operation does HM-RNN include compared to Skip RNN? sent28
    5.2. How does Focused Hierarchical RNN aim to improve question answering? sent29
        5.2.1. How does Focused Hierarchical RNN control updates to the upper layer of the RNN? sent30, sent31","3. How have Transformers been adapted to reduce computation? sent7
    3.1. How does the PoWER-BERT model reduce the number of tokens processed? sent8
        3.1.1. How is the ""schedule"" for token dropping optimized in PoWER-BERT? sent9
        3.1.2. What is the result of optimizing the schedule in PoWER-BERT? sent10
    3.2. How does TR-BERT determine which tokens to skip? sent11
        3.2.1. How does TR-BERT handle skipped tokens differently from PoWER-BERT? sent12
    3.3. How does the Length-Adaptive Transformer (LAT) utilize LengthDrop? sent13
        3.3.1. How is the schedule for LAT found? sent14
    3.4. How does LTP train a threshold for each Transformer layer? sent15
        3.4.1. How does LTP decide which tokens to drop? sent16
    3.5. How does Transkimmer incorporate a skim predictor module? sent17
        3.5.1. What is the purpose of the skim loss in Transkimmer? sent18

4. What is the difference between computation reduction and skipping in dynamic neural networks? sent19
    4.1. How does VCRNN decide the proportion of computation to use for each time step? sent20
        4.1.1. What happens after VCRNN makes a decision on computation proportion? sent21
    4.2. How does Skim-RNN decide whether to read or skim at each time step? sent22
        4.2.1. What happens if Skim-RNN decides to skim? sent23, sent24
        4.2.2. What happens if Skim-RNN decides not to skim? sent25

    4.2. How does Skim-RNN decide whether to read or skim at each time step? sent22
        4.2.1. What happens if Skim-RNN decides to skim? sent23, sent24
        4.2.2. What happens if Skim-RNN decides not to skim? sent25

5. How does dynamic hierarchical RNN differ from other skimming methods? sent26
    5.1. How does HM-RNN discover hierarchical multi-scale structures? sent27
        5.1.1. What additional operation does HM-RNN include compared to Skip RNN? sent28
    5.2. How does Focused Hierarchical RNN aim to improve question answering? sent29
        5.2.1. How does Focused Hierarchical RNN control updates to the upper layer of the RNN? sent30, sent31","    4.2. How does Skim-RNN decide whether to read or skim at each time step? sent22
        4.2.1. What happens if Skim-RNN decides to skim? sent23, sent24
        4.2.2. What happens if Skim-RNN decides not to skim? sent25","Question: How does Skim-RNN manage computational efficiency when deciding to skim or not at each time step?

Supporting sentence: sent22, sent23, sent24, sent25",How does Skim-RNN manage computational efficiency when deciding to skim or not at each time step?,"sent22, sent23, sent24, sent25",2,implement,"Skim-RNN (Seo et al., 2018) manages computational efficiency by having a big RNN and a separate small RNN.

At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token.

If the model decides to skim, the small RNN will update only a fraction of the hidden states.

Otherwise, a regular full update will be conducted by the big RNN.","Question:

How does Skim-RNN manage computational efficiency when deciding to skim or not at each time step?

Answer:

Skim-RNN (Seo et al., 2018) manages computational efficiency by having a big RNN and a separate small RNN.

At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token.

If the model decides to skim, the small RNN will update only a fraction of the hidden states.

Otherwise, a regular full update will be conducted by the big RNN."
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,Method Decision based on,26,"Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc. Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)

PoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; ""flush"" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information. LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed. Structural information that naturally exists in sentences can also play a role in skimming. Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).

In the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers. The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores. The number of tokens to be dropped, referred to as the ""schedule,"" is optimized by combining the sparsity of a soft mask layer with the original loss function. This results in an improved balance between accuracy and processing time. TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained. In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them. The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning. The schedule for LAT is found through an evolutionary search algorithm. LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule. It simply drops tokens with attention scores lower than the learned threshold. Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer. This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.

Computation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely. VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step. Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.

Instead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN. At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token. If the model decides to skim, the small RNN will update only a fraction of the hid-den states. Otherwise, a regular full update will be conducted by the big RNN.

Dynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed. HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture. In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.

In question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed. Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering. It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.","sent1: Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc.
sent2: Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)PoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens
sent3: Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; ""flush"" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information.
sent4: LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed.
sent5: Structural information that naturally exists in sentences can also play a role in skimming.
sent6: Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).
sent7: In the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers.
sent8: The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores.
sent9: The number of tokens to be dropped, referred to as the ""schedule,"" is optimized by combining the sparsity of a soft mask layer with the original loss function.
sent10: This results in an improved balance between accuracy and processing time.
sent11: TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained.
sent12: In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them.
sent13: The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning.
sent14: The schedule for LAT is found through an evolutionary search algorithm.
sent15: LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule.
sent16: It simply drops tokens with attention scores lower than the learned threshold.
sent17: Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer.
sent18: This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.
sent19: Computation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely.
sent20: VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step.
sent21: Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.
sent22: Instead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN.
sent23: At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token.
sent24: If the model decides to skim, the small RNN will update only a fraction of the hid-den states.
sent25: Otherwise, a regular full update will be conducted by the big RNN.
sent26: Dynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed.
sent27: HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture.
sent28: In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.
sent29: In question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed.
sent30: Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering.
sent31: It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.","1. What are some operation options for dynamic neural networks in NLP? sent1
    1.1. What are some specific models that utilize these operation options? sent2
    1.2. How does LSTM-Shuttle utilize a bidirectional shuttling mechanism? sent4
2. How can structural information in sentences aid in skimming? sent5
    2.1. How does Structural Jump-LSTM utilize structural information for skimming? sent6
3. How have Transformers been adapted to reduce computation? sent7
    3.1. How does the PoWER-BERT model reduce the number of tokens processed? sent8
        3.1.1. How is the ""schedule"" for token dropping optimized in PoWER-BERT? sent9
        3.1.2. What is the result of optimizing the schedule in PoWER-BERT? sent10
    3.2. How does TR-BERT determine which tokens to skip? sent11
        3.2.1. How does TR-BERT handle skipped tokens differently from PoWER-BERT? sent12
    3.3. How does the Length-Adaptive Transformer (LAT) utilize LengthDrop? sent13
        3.3.1. How is the schedule for LAT found? sent14
    3.4. How does LTP train a threshold for each Transformer layer? sent15
        3.4.1. How does LTP decide which tokens to drop? sent16
    3.5. How does Transkimmer incorporate a skim predictor module? sent17
        3.5.1. What is the purpose of the skim loss in Transkimmer? sent18
4. What is the difference between computation reduction and skipping in dynamic neural networks? sent19
    4.1. How does VCRNN decide the proportion of computation to use for each time step? sent20
        4.1.1. What happens after VCRNN makes a decision on computation proportion? sent21
    4.2. How does Skim-RNN decide whether to read or skim at each time step? sent22
        4.2.1. What happens if Skim-RNN decides to skim? sent23, sent24
        4.2.2. What happens if Skim-RNN decides not to skim? sent25
5. How does dynamic hierarchical RNN differ from other skimming methods? sent26
    5.1. How does HM-RNN discover hierarchical multi-scale structures? sent27
        5.1.1. What additional operation does HM-RNN include compared to Skip RNN? sent28
    5.2. How does Focused Hierarchical RNN aim to improve question answering? sent29
        5.2.1. How does Focused Hierarchical RNN control updates to the upper layer of the RNN? sent30, sent31","3. How have Transformers been adapted to reduce computation? sent7
    3.1. How does the PoWER-BERT model reduce the number of tokens processed? sent8
        3.1.1. How is the ""schedule"" for token dropping optimized in PoWER-BERT? sent9
        3.1.2. What is the result of optimizing the schedule in PoWER-BERT? sent10
    3.2. How does TR-BERT determine which tokens to skip? sent11
        3.2.1. How does TR-BERT handle skipped tokens differently from PoWER-BERT? sent12
    3.3. How does the Length-Adaptive Transformer (LAT) utilize LengthDrop? sent13
        3.3.1. How is the schedule for LAT found? sent14
    3.4. How does LTP train a threshold for each Transformer layer? sent15
        3.4.1. How does LTP decide which tokens to drop? sent16
    3.5. How does Transkimmer incorporate a skim predictor module? sent17
        3.5.1. What is the purpose of the skim loss in Transkimmer? sent18

4. What is the difference between computation reduction and skipping in dynamic neural networks? sent19
    4.1. How does VCRNN decide the proportion of computation to use for each time step? sent20
        4.1.1. What happens after VCRNN makes a decision on computation proportion? sent21
    4.2. How does Skim-RNN decide whether to read or skim at each time step? sent22
        4.2.1. What happens if Skim-RNN decides to skim? sent23, sent24
        4.2.2. What happens if Skim-RNN decides not to skim? sent25

    4.2. How does Skim-RNN decide whether to read or skim at each time step? sent22
        4.2.1. What happens if Skim-RNN decides to skim? sent23, sent24
        4.2.2. What happens if Skim-RNN decides not to skim? sent25

5. How does dynamic hierarchical RNN differ from other skimming methods? sent26
    5.1. How does HM-RNN discover hierarchical multi-scale structures? sent27
        5.1.1. What additional operation does HM-RNN include compared to Skip RNN? sent28
    5.2. How does Focused Hierarchical RNN aim to improve question answering? sent29
        5.2.1. How does Focused Hierarchical RNN control updates to the upper layer of the RNN? sent30, sent31","5. How does dynamic hierarchical RNN differ from other skimming methods? sent26
    5.1. How does HM-RNN discover hierarchical multi-scale structures? sent27
        5.1.1. What additional operation does HM-RNN include compared to Skip RNN? sent28
    5.2. How does Focused Hierarchical RNN aim to improve question answering? sent29
        5.2.1. How does Focused Hierarchical RNN control updates to the upper layer of the RNN? sent30, sent31","Question: How does Focused Hierarchical RNN selectively process information for question answering tasks?

Supporting sentence: sent29, sent30, sent31",How does Focused Hierarchical RNN selectively process information for question answering tasks?,"sent29, sent30, sent31",2,implement,"Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering.

It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.","Question:

How does Focused Hierarchical RNN selectively process information for question answering tasks?

Answer:

Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering.

It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding."
251402499,Abstractive Meeting Summarization: A Survey,https://www.semanticscholar.org/paper/6dd2ffed96ab44004b2e4cb1bf36a44daae14d42,Discursive information,13,"While most work on abstractive meeting summarization implicitly assumes that conversation is merely a linear sequence of utterances without semantic relations between them, example (1) underscores the importance of semantic relations for conversational understanding. Drawing on similar insights, certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993;Lascarides and Asher, 2008), to improve summarization. Accounts like RST and SDRT maintain that in a coherent conversation, each (roughly) clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, Contrast, etc. to reflect its contribution to the larger discourse. Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations. Figure 2 il-  , 2021a). The * indicates summary-level (with sentence split) ROUGE-L score (Lin, 2004).

lustrates a possible SDRT graph for example (1).

To the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries. They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora. Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices. Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary. A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.

An alternative approach to discourse interpretation that developed largely independently of RST and SDRT is dialogue act classification. Detailing the differences between the approaches is out of the scope of this paper, but in a nutshell, dialogue acts provide a shallower notion of discourse structure (Jurafsky et al., 1997) in that they do not entail a full graph structure over a conversation. On the other hand, systems for dialogue act labeling, such as DAMSL (Allen and Core, 1997; Core and Allen, 1997) or DiAML (Bunt et al., 2010(Bunt et al., , 2012, place more emphasis on interactive acts such as stalling to hold the floor, assessing other discourse moves, suggesting or informing.

Both the AMI and ICSI corpora provide gold dialogue act labels, and Goo and Chen (2018) use the labels from the AMI corpus to develop a sentence-gated mechanism that jointly models the relationships between dialogue acts and topic summaries to improve abstractive meeting summarization. In particular, they show that dialogue acts of the type inform are more closely linked to summary-worthy material than acts such as stall or assess. Using LSTM, their model consists of three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder.

Dialogue acts have also been used to good effect for summarizing decisions. Fernández et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution. Then, key fragments of the decision related utterances are retained to form an extractive decision summary. Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006(Purver et al., , 2007.","sent1: While most work on abstractive meeting summarization implicitly assumes that conversation is merely a linear sequence of utterances without semantic relations between them, example (1) underscores the importance of semantic relations for conversational understanding.
sent2: Drawing on similar insights, certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993;Lascarides and Asher, 2008), to improve summarization.
sent3: Accounts like RST and SDRT maintain that in a coherent conversation, each (roughly) clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, Contrast, etc. to reflect its contribution to the larger discourse.
sent4: Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations.
sent5: Figure 2 il-  , 2021a). The * indicates summary-level (with sentence split) ROUGE-L score (Lin, 2004).lustrates a possible SDRT graph for example (1).
sent6: To the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries.
sent7: They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora.
sent8: Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices.
sent9: Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary.
sent10: A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.
sent11: An alternative approach to discourse interpretation that developed largely independently of RST and SDRT is dialogue act classification.
sent12: Detailing the differences between the approaches is out of the scope of this paper, but in a nutshell, dialogue acts provide a shallower notion of discourse structure (Jurafsky et al., 1997) in that they do not entail a full graph structure over a conversation.
sent13: On the other hand, systems for dialogue act labeling, such as DAMSL (Allen and Core, 1997; Core and Allen, 1997) or DiAML (Bunt et al., 2010(Bunt et al., , 2012, place more emphasis on interactive acts such as stalling to hold the floor, assessing other discourse moves, suggesting or informing.
sent14: Both the AMI and ICSI corpora provide gold dialogue act labels, and Goo and Chen (2018) use the labels from the AMI corpus to develop a sentence-gated mechanism that jointly models the relationships between dialogue acts and topic summaries to improve abstractive meeting summarization.
sent15: In particular, they show that dialogue acts of the type inform are more closely linked to summary-worthy material than acts such as stall or assess.
sent16: Using LSTM, their model consists of three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder.
sent17: Dialogue acts have also been used to good effect for summarizing decisions.
sent18: Fernández et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution.
sent19: Then, key fragments of the decision related utterances are retained to form an extractive decision summary.
sent20: Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006(Purver et al., , 2007.","1. What assumption do most works on abstractive meeting summarization make about conversation? sent1
    1.1. What do certain recent approaches exploit to improve summarization? sent2
2. What do RST and SDRT maintain about coherent conversations? sent3
    2.1. How can each coherent discourse be represented? sent4
        2.1.1. What does Figure 2 illustrate? sent5
3. What is the first work to exploit discourse graphs for abstractive meeting summaries? sent6
    3.1. How do Feng et al. (2020) obtain discourse graphs for meeting corpora? sent7
    3.2. What transformation is used to turn graph edges into vertices? sent8
    3.3. What does their graph-to-sequence model consist of? sent9
    3.4. What strategy is introduced to pretrain the model? sent10
4. What is an alternative approach to discourse interpretation? sent11
    4.1. How do dialogue acts differ from RST and SDRT? sent12
    4.2. What do systems for dialogue act labeling emphasize? sent13
5. What do the AMI and ICSI corpora provide, and how are they used by Goo and Chen (2018)? sent14
    5.1. What do Goo and Chen (2018) show about dialogue acts? sent15
    5.2. What does their model consist of? sent16
6. How have dialogue acts been used for summarizing decisions? sent17
    6.1. What do Fernández et al. (2008) and Bui et al. (2009) identify and classify in meetings? sent18
    6.2. How are key fragments of decision-related utterances used? sent19
7. What similar ideas exist in the literature regarding dialogue acts in meetings? sent20","3. What is the first work to exploit discourse graphs for abstractive meeting summaries? sent6
    3.1. How do Feng et al. (2020) obtain discourse graphs for meeting corpora? sent7
    3.2. What transformation is used to turn graph edges into vertices? sent8
    3.3. What does their graph-to-sequence model consist of? sent9
    3.4. What strategy is introduced to pretrain the model? sent10","3. What is the first work to exploit discourse graphs for abstractive meeting summaries? sent6
    3.1. How do Feng et al. (2020) obtain discourse graphs for meeting corpora? sent7
    3.2. What transformation is used to turn graph edges into vertices? sent8
    3.3. What does their graph-to-sequence model consist of? sent9
    3.4. What strategy is introduced to pretrain the model? sent10","Question: How do Feng et al. (2020) utilize discourse graphs and graph-to-sequence models for abstractive meeting summarization?

Supporting sentence: sent6, sent7, sent8, sent9, sent10",How do Feng et al. (2020) utilize discourse graphs and graph-to-sequence models for abstractive meeting summarization?,"sent6, sent7, sent8, sent9, sent10",2,implement,"Feng et al. (2020) utilize discourse graphs to generate abstractive meeting summaries.

They employ a sequential discourse parser trained on the STAC corpus of multi-party chats to automatically obtain discourse graphs for the AMI and ICSI meeting corpora.

Levi graph transformation is then used to turn graph edges labeled with discourse relation types into vertices.

Their graph-to-sequence model consists of a graph convolutional network encoder that takes a meeting discourse graph as input and a PGN decoder to generate the final summary.

A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.","Question:

How do Feng et al. (2020) utilize discourse graphs and graph-to-sequence models for abstractive meeting summarization?

Answer:

Feng et al. (2020) utilize discourse graphs to generate abstractive meeting summaries.

They employ a sequential discourse parser trained on the STAC corpus of multi-party chats to automatically obtain discourse graphs for the AMI and ICSI meeting corpora.

Levi graph transformation is then used to turn graph edges labeled with discourse relation types into vertices.

Their graph-to-sequence model consists of a graph convolutional network encoder that takes a meeting discourse graph as input and a PGN decoder to generate the final summary.

A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,Initial Instruction,5,"In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","sent1: In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room.
sent2: Turn left and go through the door in the middle.""
sent3: Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal.
sent4: Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017).
sent5: An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views.
sent6: R2R is extended to create other VLN benchmarks.
sent7: Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019).
sent8: Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments.
sent9: In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object.
sent10: Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.
sent11: Some work uses natural language to guide drones.
sent12: LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions.
sent13: Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","1. What is provided to the agent in many VLN benchmarks for navigation? sent1
    1.1. What is an example of a natural language instruction given to the agent? sent2
2. What is required for fine-grained navigation in VLN tasks? sent3
3. What dataset did Anderson et al. (2018b) create, and what is it based on? sent4
    3.1. How does an embodied agent navigate in the R2R dataset? sent5
    3.2. How has R2R been extended to create other VLN benchmarks? sent6
        3.2.1. What is Roomfor-Room, and how does it relate to R2R? sent7
4. What are some examples of VLN benchmarks and datasets mentioned in the section? sent8
5. What is the task in the TOUCHDOWN dataset? sent9
6. What are some photo-realistic outdoor VLN datasets based on, and can you name a few? sent10
7. How is natural language used in guiding drones? sent11
8. What is LANI, and what does it involve? sent12
9. Where do current datasets on drone navigation usually fall, and can you provide an example? sent13","3. What dataset did Anderson et al. (2018b) create, and what is it based on? sent4
    3.1. How does an embodied agent navigate in the R2R dataset? sent5
    3.2. How has R2R been extended to create other VLN benchmarks? sent6
        3.2.1. What is Roomfor-Room, and how does it relate to R2R? sent7","3. What dataset did Anderson et al. (2018b) create, and what is it based on? sent4
    3.1. How does an embodied agent navigate in the R2R dataset? sent5
    3.2. How has R2R been extended to create other VLN benchmarks? sent6
        3.2.1. What is Roomfor-Room, and how does it relate to R2R? sent7","Question: How has the R2R dataset been extended to develop new vision-and-language navigation benchmarks?

Supporting sentence: sent6, sent7",How has the R2R dataset been extended to develop new vision-and-language navigation benchmarks?,"sent6, sent7",2,implement,"The R2R dataset, based on the Matterport3D simulator, has been extended to create other vision-and-language navigation benchmarks.

Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019).

Other extensions include Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021), LANI (Misra et al., 2018), RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), and SOON (Zhu et al., 2021a).","Question:

How has the R2R dataset been extended to develop new vision-and-language navigation benchmarks?

Answer:

The R2R dataset, based on the Matterport3D simulator, has been extended to create other vision-and-language navigation benchmarks.

Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019).

Other extensions include Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021), LANI (Misra et al., 2018), RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), and SOON (Zhu et al., 2021a)."
258557362,Large Language Models Meet NL2Code: A Survey,https://www.semanticscholar.org/paper/4f939f0751e5484f54089f6a97598e39afdcb3b5,Large and Premium Data,6,"As the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases. This highlights the importance of selecting and pre-processing high-quality data. In this section, we will discuss various commonly used data sources and pre-processing strategies that are essential for training LLMs.

Early models were trained using manually annotated data pairs of NL and code, and the data sources include CodeSearchNet (Husain et al., 2019), CoST (Zhu et al., 2022b), and XL-CoST (Zhu et al., 2022a). However, manual annotation is labour-intensive and time-consuming. There are also models like GPT-3 (Brown et al., 2020), GPT-Neo (Black et al., 2021), and GPT-J (Wang and Komatsuzaki, 2021)  In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data. We carefully review the data pre-processing methods of five powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b), and identify several commonalities. One is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless. Additionally, specific rules are employed to filter out uncommon code files. These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate. In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.","sent1: As the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases.
sent2: This highlights the importance of selecting and pre-processing high-quality data.
sent3: In this section, we will discuss various commonly used data sources and pre-processing strategies that are essential for training LLMs.
sent4: Early models were trained using manually annotated data pairs of NL and code, and the data sources include CodeSearchNet (Husain et al., 2019), CoST (Zhu et al., 2022b), and XL-CoST (Zhu et al., 2022a).
sent5: However, manual annotation is labour-intensive and time-consuming.
sent6: There are also models like GPT-3 (Brown et al., 2020), GPT-Neo (Black et al., 2021), and GPT-J (Wang and Komatsuzaki, 2021)
sent7: In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data.
sent8: We carefully review the data pre-processing methods of five powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b), and identify several commonalities.
sent9: One is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless.
sent10: Additionally, specific rules are employed to filter out uncommon code files.
sent11: These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate.
sent12: In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.","1. How does the increase in the size of LLMs affect the scale of the training corpus in the field of NL2Code? sent1
2. Why is selecting and pre-processing high-quality data important for training LLMs? sent2
3. What will be discussed in this section regarding data sources and pre-processing strategies? sent3
4. What were early models in the field of NL2Code trained on, and what are some examples of data sources used? sent4
    4.1. What are the challenges associated with manual annotation of data pairs? sent5
5. What are some examples of models that do not rely on manual annotation? sent6
6. Why is data preprocessing important for LLMs, and what does it involve? sent7
7. Which five powerful LLMs' data pre-processing methods are reviewed, and what commonalities are identified? sent8
    7.1. What is one common data pre-processing method used by these LLMs? sent9
    7.2. What specific rules are employed to filter out uncommon code files? sent10, sent11
8. What is the overall goal of the pre-processing strategies discussed? sent12","7. Which five powerful LLMs' data pre-processing methods are reviewed, and what commonalities are identified? sent8
    7.1. What is one common data pre-processing method used by these LLMs? sent9
    7.2. What specific rules are employed to filter out uncommon code files? sent10, sent11","7. Which five powerful LLMs' data pre-processing methods are reviewed, and what commonalities are identified? sent8
    7.1. What is one common data pre-processing method used by these LLMs? sent9
    7.2. What specific rules are employed to filter out uncommon code files? sent10, sent11","Question: What specific rules are used to filter uncommon code files in LLM data pre-processing?

Supporting sentence: sent10, sent11",What specific rules are used to filter uncommon code files in LLM data pre-processing?,"sent10, sent11",2,implement,"Specific rules are employed to filter out uncommon code files in LLM data pre-processing.

These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate.","Question:

What specific rules are used to filter uncommon code files in LLM data pre-processing?

Answer:

Specific rules are employed to filter out uncommon code files in LLM data pre-processing.

These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate."
222133962,A Survey of Unsupervised Dependency Parsing,https://www.semanticscholar.org/paper/16160fbefc830117a8395a4b665c39feae834e48,Models,8,"A generative approach models the joint probability of the sentence and the corresponding parse tree. Traditional generative models are mostly based on probabilistic grammars. To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions. The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else. Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference. However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.

Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree. Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner. The generation of a child token is conditioned on the head token and the dependency direction. In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden  propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).

Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.  use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV.","sent1: A generative approach models the joint probability of the sentence and the corresponding parse tree.
sent2: Traditional generative models are mostly based on probabilistic grammars.
sent3: To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions.
sent4: The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else.
sent5: Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference.
sent6: However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.
sent7: Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree.
sent8: Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner.
sent9: The generation of a child token is conditioned on the head token and the dependency direction.
sent10: In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously.
sent11: Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token.
sent12: Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token.
sent13: Headden  propose to also introduce the valence into the condition of decision sampling.
sent14: Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.
sent15: Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information.
sent16: In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).
sent17: Similar tokens may have similar syntactic behaviors in a grammar.
sent18: For example, all the verbs are very likely to generate a noun to the left as the subject.
sent19: One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.
sent20: use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features.
sent21: Both approaches are based on DMV.","1. What does a generative approach model in unsupervised dependency parsing? sent1
2. What are traditional generative models based on? sent2
3. What assumptions do traditional generative models make to enable efficient inference? sent3
    3.1. What is the simplest assumption made by these models? sent4
    3.2. How do these assumptions affect the joint probability and inference? sent5
    3.3. What is a downside of these assumptions? sent6
4. How do different generative models specify the generation processes of the sentence and parse tree? sent7
    4.1. How do Paskin (2002) and Carroll and Charniak (1992) approach the generation process? sent8
        4.1.1. On what is the generation of a child token conditioned in their approach? sent9
    4.2. How does the Dependency Model with Valence (DMV) differ in its generation process? sent10
        4.2.1. What decision must each head token make in DMV before generating a child token? sent11
        4.2.2. What additional condition is considered in the generation of a child token in DMV? sent12
        4.2.3. What modification does Headden propose for DMV? sent13
        4.2.4. What additional contexts do Spitkovsky et al. (2012) consider in their model? sent14
        4.2.5. What extension does Yang et al. (2020) propose for DMV? sent15
5. What other grammar formalisms have been used for unsupervised dependency parsing? sent16
6. How do similar tokens behave in a grammar, and how can this be captured? sent17
    6.1. Can you provide an example of similar syntactic behavior among tokens? sent18
    6.2. How can generation probabilities be computed to capture syntactic similarity? sent19
        6.2.1. What approaches are used to compute these probabilities, and on what are they based? sent20, sent21","3. What assumptions do traditional generative models make to enable efficient inference? sent3
    3.1. What is the simplest assumption made by these models? sent4
    3.2. How do these assumptions affect the joint probability and inference? sent5
    3.3. What is a downside of these assumptions? sent6

4. How do different generative models specify the generation processes of the sentence and parse tree? sent7
    4.1. How do Paskin (2002) and Carroll and Charniak (1992) approach the generation process? sent8
        4.1.1. On what is the generation of a child token conditioned in their approach? sent9
    4.2. How does the Dependency Model with Valence (DMV) differ in its generation process? sent10
        4.2.1. What decision must each head token make in DMV before generating a child token? sent11
        4.2.2. What additional condition is considered in the generation of a child token in DMV? sent12
        4.2.3. What modification does Headden propose for DMV? sent13
        4.2.4. What additional contexts do Spitkovsky et al. (2012) consider in their model? sent14
        4.2.5. What extension does Yang et al. (2020) propose for DMV? sent15

    4.2. How does the Dependency Model with Valence (DMV) differ in its generation process? sent10
        4.2.1. What decision must each head token make in DMV before generating a child token? sent11
        4.2.2. What additional condition is considered in the generation of a child token in DMV? sent12
        4.2.3. What modification does Headden propose for DMV? sent13
        4.2.4. What additional contexts do Spitkovsky et al. (2012) consider in their model? sent14
        4.2.5. What extension does Yang et al. (2020) propose for DMV? sent15

6. How do similar tokens behave in a grammar, and how can this be captured? sent17
    6.1. Can you provide an example of similar syntactic behavior among tokens? sent18
    6.2. How can generation probabilities be computed to capture syntactic similarity? sent19
        6.2.1. What approaches are used to compute these probabilities, and on what are they based? sent20, sent21","3. What assumptions do traditional generative models make to enable efficient inference? sent3
    3.1. What is the simplest assumption made by these models? sent4
    3.2. How do these assumptions affect the joint probability and inference? sent5
    3.3. What is a downside of these assumptions? sent6","Question: How do traditional generative models' assumptions impact the joint probability and inference in unsupervised dependency parsing?

Supporting sentence: sent3, sent4, sent5, sent6",How do traditional generative models' assumptions impact the joint probability and inference in unsupervised dependency parsing?,"sent3, sent4, sent5, sent6",2,implement,"Traditional generative models are mostly based on probabilistic grammars.

To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions.

The simplest assumption, known as the context-free assumption, states that the generation of a token is only dependent on its head token and is independent of anything else.

Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference.

However, they also lead to unavailability of useful information, such as context and generation history, in generating each token.","Question:

How do traditional generative models' assumptions impact the joint probability and inference in unsupervised dependency parsing?

Answer:

Traditional generative models are mostly based on probabilistic grammars.

To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions.

The simplest assumption, known as the context-free assumption, states that the generation of a token is only dependent on its head token and is independent of anything else.

Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference.

However, they also lead to unavailability of useful information, such as context and generation history, in generating each token."
222133962,A Survey of Unsupervised Dependency Parsing,https://www.semanticscholar.org/paper/16160fbefc830117a8395a4b665c39feae834e48,Models,8,"A generative approach models the joint probability of the sentence and the corresponding parse tree. Traditional generative models are mostly based on probabilistic grammars. To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions. The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else. Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference. However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.

Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree. Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner. The generation of a child token is conditioned on the head token and the dependency direction. In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden  propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).

Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.  use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV.","sent1: A generative approach models the joint probability of the sentence and the corresponding parse tree.
sent2: Traditional generative models are mostly based on probabilistic grammars.
sent3: To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions.
sent4: The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else.
sent5: Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference.
sent6: However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.
sent7: Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree.
sent8: Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner.
sent9: The generation of a child token is conditioned on the head token and the dependency direction.
sent10: In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously.
sent11: Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token.
sent12: Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token.
sent13: Headden  propose to also introduce the valence into the condition of decision sampling.
sent14: Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.
sent15: Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information.
sent16: In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).
sent17: Similar tokens may have similar syntactic behaviors in a grammar.
sent18: For example, all the verbs are very likely to generate a noun to the left as the subject.
sent19: One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.
sent20: use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features.
sent21: Both approaches are based on DMV.","1. What does a generative approach model in unsupervised dependency parsing? sent1
2. What are traditional generative models based on? sent2
3. What assumptions do traditional generative models make to enable efficient inference? sent3
    3.1. What is the simplest assumption made by these models? sent4
    3.2. How do these assumptions affect the joint probability and inference? sent5
    3.3. What is a downside of these assumptions? sent6
4. How do different generative models specify the generation processes of the sentence and parse tree? sent7
    4.1. How do Paskin (2002) and Carroll and Charniak (1992) approach the generation process? sent8
        4.1.1. On what is the generation of a child token conditioned in their approach? sent9
    4.2. How does the Dependency Model with Valence (DMV) differ in its generation process? sent10
        4.2.1. What decision must each head token make in DMV before generating a child token? sent11
        4.2.2. What additional condition is considered in the generation of a child token in DMV? sent12
        4.2.3. What modification does Headden propose for DMV? sent13
        4.2.4. What additional contexts do Spitkovsky et al. (2012) consider in their model? sent14
        4.2.5. What extension does Yang et al. (2020) propose for DMV? sent15
5. What other grammar formalisms have been used for unsupervised dependency parsing? sent16
6. How do similar tokens behave in a grammar, and how can this be captured? sent17
    6.1. Can you provide an example of similar syntactic behavior among tokens? sent18
    6.2. How can generation probabilities be computed to capture syntactic similarity? sent19
        6.2.1. What approaches are used to compute these probabilities, and on what are they based? sent20, sent21","3. What assumptions do traditional generative models make to enable efficient inference? sent3
    3.1. What is the simplest assumption made by these models? sent4
    3.2. How do these assumptions affect the joint probability and inference? sent5
    3.3. What is a downside of these assumptions? sent6

4. How do different generative models specify the generation processes of the sentence and parse tree? sent7
    4.1. How do Paskin (2002) and Carroll and Charniak (1992) approach the generation process? sent8
        4.1.1. On what is the generation of a child token conditioned in their approach? sent9
    4.2. How does the Dependency Model with Valence (DMV) differ in its generation process? sent10
        4.2.1. What decision must each head token make in DMV before generating a child token? sent11
        4.2.2. What additional condition is considered in the generation of a child token in DMV? sent12
        4.2.3. What modification does Headden propose for DMV? sent13
        4.2.4. What additional contexts do Spitkovsky et al. (2012) consider in their model? sent14
        4.2.5. What extension does Yang et al. (2020) propose for DMV? sent15

    4.2. How does the Dependency Model with Valence (DMV) differ in its generation process? sent10
        4.2.1. What decision must each head token make in DMV before generating a child token? sent11
        4.2.2. What additional condition is considered in the generation of a child token in DMV? sent12
        4.2.3. What modification does Headden propose for DMV? sent13
        4.2.4. What additional contexts do Spitkovsky et al. (2012) consider in their model? sent14
        4.2.5. What extension does Yang et al. (2020) propose for DMV? sent15

6. How do similar tokens behave in a grammar, and how can this be captured? sent17
    6.1. Can you provide an example of similar syntactic behavior among tokens? sent18
    6.2. How can generation probabilities be computed to capture syntactic similarity? sent19
        6.2.1. What approaches are used to compute these probabilities, and on what are they based? sent20, sent21","4. How do different generative models specify the generation processes of the sentence and parse tree? sent7
    4.1. How do Paskin (2002) and Carroll and Charniak (1992) approach the generation process? sent8
        4.1.1. On what is the generation of a child token conditioned in their approach? sent9
    4.2. How does the Dependency Model with Valence (DMV) differ in its generation process? sent10
        4.2.1. What decision must each head token make in DMV before generating a child token? sent11
        4.2.2. What additional condition is considered in the generation of a child token in DMV? sent12
        4.2.3. What modification does Headden propose for DMV? sent13
        4.2.4. What additional contexts do Spitkovsky et al. (2012) consider in their model? sent14
        4.2.5. What extension does Yang et al. (2020) propose for DMV? sent15","Question: How do different generative models incorporate context and dependencies in unsupervised dependency parsing?

Supporting sentence: sent7, sent8, sent9, sent10, sent11, sent12, sent13, sent14, sent15",How do different generative models incorporate context and dependencies in unsupervised dependency parsing?,"sent7, sent8, sent9, sent10, sent11, sent12, sent13, sent14, sent15",4,implement,"A generative approach models the joint probability of the sentence and the corresponding parse tree.

Traditional generative models are mostly based on probabilistic grammars.

To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions.

The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else.

Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference.

However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.

Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree.

Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner.

The generation of a child token is conditioned on the head token and the dependency direction.

In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously.

Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token.

Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token.

Headden propose to also introduce the valence into the condition of decision sampling.

Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.

Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information.","Question:

How do different generative models incorporate context and dependencies in unsupervised dependency parsing?

Answer:

A generative approach models the joint probability of the sentence and the corresponding parse tree.

Traditional generative models are mostly based on probabilistic grammars.

To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions.

The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else.

Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference.

However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.

Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree.

Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner.

The generation of a child token is conditioned on the head token and the dependency direction.

In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously.

Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token.

Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token.

Headden propose to also introduce the valence into the condition of decision sampling.

Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.

Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information."
222133962,A Survey of Unsupervised Dependency Parsing,https://www.semanticscholar.org/paper/16160fbefc830117a8395a4b665c39feae834e48,Models,8,"A generative approach models the joint probability of the sentence and the corresponding parse tree. Traditional generative models are mostly based on probabilistic grammars. To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions. The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else. Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference. However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.

Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree. Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner. The generation of a child token is conditioned on the head token and the dependency direction. In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden  propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).

Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.  use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV.","sent1: A generative approach models the joint probability of the sentence and the corresponding parse tree.
sent2: Traditional generative models are mostly based on probabilistic grammars.
sent3: To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions.
sent4: The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else.
sent5: Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference.
sent6: However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.
sent7: Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree.
sent8: Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner.
sent9: The generation of a child token is conditioned on the head token and the dependency direction.
sent10: In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously.
sent11: Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token.
sent12: Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token.
sent13: Headden  propose to also introduce the valence into the condition of decision sampling.
sent14: Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.
sent15: Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information.
sent16: In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).
sent17: Similar tokens may have similar syntactic behaviors in a grammar.
sent18: For example, all the verbs are very likely to generate a noun to the left as the subject.
sent19: One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.
sent20: use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features.
sent21: Both approaches are based on DMV.","1. What does a generative approach model in unsupervised dependency parsing? sent1
2. What are traditional generative models based on? sent2
3. What assumptions do traditional generative models make to enable efficient inference? sent3
    3.1. What is the simplest assumption made by these models? sent4
    3.2. How do these assumptions affect the joint probability and inference? sent5
    3.3. What is a downside of these assumptions? sent6
4. How do different generative models specify the generation processes of the sentence and parse tree? sent7
    4.1. How do Paskin (2002) and Carroll and Charniak (1992) approach the generation process? sent8
        4.1.1. On what is the generation of a child token conditioned in their approach? sent9
    4.2. How does the Dependency Model with Valence (DMV) differ in its generation process? sent10
        4.2.1. What decision must each head token make in DMV before generating a child token? sent11
        4.2.2. What additional condition is considered in the generation of a child token in DMV? sent12
        4.2.3. What modification does Headden propose for DMV? sent13
        4.2.4. What additional contexts do Spitkovsky et al. (2012) consider in their model? sent14
        4.2.5. What extension does Yang et al. (2020) propose for DMV? sent15
5. What other grammar formalisms have been used for unsupervised dependency parsing? sent16
6. How do similar tokens behave in a grammar, and how can this be captured? sent17
    6.1. Can you provide an example of similar syntactic behavior among tokens? sent18
    6.2. How can generation probabilities be computed to capture syntactic similarity? sent19
        6.2.1. What approaches are used to compute these probabilities, and on what are they based? sent20, sent21","3. What assumptions do traditional generative models make to enable efficient inference? sent3
    3.1. What is the simplest assumption made by these models? sent4
    3.2. How do these assumptions affect the joint probability and inference? sent5
    3.3. What is a downside of these assumptions? sent6

4. How do different generative models specify the generation processes of the sentence and parse tree? sent7
    4.1. How do Paskin (2002) and Carroll and Charniak (1992) approach the generation process? sent8
        4.1.1. On what is the generation of a child token conditioned in their approach? sent9
    4.2. How does the Dependency Model with Valence (DMV) differ in its generation process? sent10
        4.2.1. What decision must each head token make in DMV before generating a child token? sent11
        4.2.2. What additional condition is considered in the generation of a child token in DMV? sent12
        4.2.3. What modification does Headden propose for DMV? sent13
        4.2.4. What additional contexts do Spitkovsky et al. (2012) consider in their model? sent14
        4.2.5. What extension does Yang et al. (2020) propose for DMV? sent15

    4.2. How does the Dependency Model with Valence (DMV) differ in its generation process? sent10
        4.2.1. What decision must each head token make in DMV before generating a child token? sent11
        4.2.2. What additional condition is considered in the generation of a child token in DMV? sent12
        4.2.3. What modification does Headden propose for DMV? sent13
        4.2.4. What additional contexts do Spitkovsky et al. (2012) consider in their model? sent14
        4.2.5. What extension does Yang et al. (2020) propose for DMV? sent15

6. How do similar tokens behave in a grammar, and how can this be captured? sent17
    6.1. Can you provide an example of similar syntactic behavior among tokens? sent18
    6.2. How can generation probabilities be computed to capture syntactic similarity? sent19
        6.2.1. What approaches are used to compute these probabilities, and on what are they based? sent20, sent21","    4.2. How does the Dependency Model with Valence (DMV) differ in its generation process? sent10
        4.2.1. What decision must each head token make in DMV before generating a child token? sent11
        4.2.2. What additional condition is considered in the generation of a child token in DMV? sent12
        4.2.3. What modification does Headden propose for DMV? sent13
        4.2.4. What additional contexts do Spitkovsky et al. (2012) consider in their model? sent14
        4.2.5. What extension does Yang et al. (2020) propose for DMV? sent15","Question: How does the Dependency Model with Valence (DMV) incorporate additional syntactic information in its generation process?

Supporting sentence: sent10, sent11, sent12, sent13, sent14, sent15",How does the Dependency Model with Valence (DMV) incorporate additional syntactic information in its generation process?,"sent10, sent11, sent12, sent13, sent14, sent15",2,implement,"The Dependency Model with Valence (DMV) generates the sentence and the parse tree simultaneously.

Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token.

The generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token.

Headden proposes to also introduce the valence into the condition of decision sampling.

Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.

Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information.","Question:

How does the Dependency Model with Valence (DMV) incorporate additional syntactic information in its generation process?

Answer:

The Dependency Model with Valence (DMV) generates the sentence and the parse tree simultaneously.

Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token.

The generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token.

Headden proposes to also introduce the valence into the condition of decision sampling.

Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.

Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information."
222133962,A Survey of Unsupervised Dependency Parsing,https://www.semanticscholar.org/paper/16160fbefc830117a8395a4b665c39feae834e48,Models,8,"A generative approach models the joint probability of the sentence and the corresponding parse tree. Traditional generative models are mostly based on probabilistic grammars. To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions. The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else. Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference. However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.

Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree. Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner. The generation of a child token is conditioned on the head token and the dependency direction. In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden  propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).

Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.  use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV.","sent1: A generative approach models the joint probability of the sentence and the corresponding parse tree.
sent2: Traditional generative models are mostly based on probabilistic grammars.
sent3: To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions.
sent4: The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else.
sent5: Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference.
sent6: However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.
sent7: Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree.
sent8: Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner.
sent9: The generation of a child token is conditioned on the head token and the dependency direction.
sent10: In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously.
sent11: Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token.
sent12: Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token.
sent13: Headden  propose to also introduce the valence into the condition of decision sampling.
sent14: Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.
sent15: Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information.
sent16: In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).
sent17: Similar tokens may have similar syntactic behaviors in a grammar.
sent18: For example, all the verbs are very likely to generate a noun to the left as the subject.
sent19: One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.
sent20: use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features.
sent21: Both approaches are based on DMV.","1. What does a generative approach model in unsupervised dependency parsing? sent1
2. What are traditional generative models based on? sent2
3. What assumptions do traditional generative models make to enable efficient inference? sent3
    3.1. What is the simplest assumption made by these models? sent4
    3.2. How do these assumptions affect the joint probability and inference? sent5
    3.3. What is a downside of these assumptions? sent6
4. How do different generative models specify the generation processes of the sentence and parse tree? sent7
    4.1. How do Paskin (2002) and Carroll and Charniak (1992) approach the generation process? sent8
        4.1.1. On what is the generation of a child token conditioned in their approach? sent9
    4.2. How does the Dependency Model with Valence (DMV) differ in its generation process? sent10
        4.2.1. What decision must each head token make in DMV before generating a child token? sent11
        4.2.2. What additional condition is considered in the generation of a child token in DMV? sent12
        4.2.3. What modification does Headden propose for DMV? sent13
        4.2.4. What additional contexts do Spitkovsky et al. (2012) consider in their model? sent14
        4.2.5. What extension does Yang et al. (2020) propose for DMV? sent15
5. What other grammar formalisms have been used for unsupervised dependency parsing? sent16
6. How do similar tokens behave in a grammar, and how can this be captured? sent17
    6.1. Can you provide an example of similar syntactic behavior among tokens? sent18
    6.2. How can generation probabilities be computed to capture syntactic similarity? sent19
        6.2.1. What approaches are used to compute these probabilities, and on what are they based? sent20, sent21","3. What assumptions do traditional generative models make to enable efficient inference? sent3
    3.1. What is the simplest assumption made by these models? sent4
    3.2. How do these assumptions affect the joint probability and inference? sent5
    3.3. What is a downside of these assumptions? sent6

4. How do different generative models specify the generation processes of the sentence and parse tree? sent7
    4.1. How do Paskin (2002) and Carroll and Charniak (1992) approach the generation process? sent8
        4.1.1. On what is the generation of a child token conditioned in their approach? sent9
    4.2. How does the Dependency Model with Valence (DMV) differ in its generation process? sent10
        4.2.1. What decision must each head token make in DMV before generating a child token? sent11
        4.2.2. What additional condition is considered in the generation of a child token in DMV? sent12
        4.2.3. What modification does Headden propose for DMV? sent13
        4.2.4. What additional contexts do Spitkovsky et al. (2012) consider in their model? sent14
        4.2.5. What extension does Yang et al. (2020) propose for DMV? sent15

    4.2. How does the Dependency Model with Valence (DMV) differ in its generation process? sent10
        4.2.1. What decision must each head token make in DMV before generating a child token? sent11
        4.2.2. What additional condition is considered in the generation of a child token in DMV? sent12
        4.2.3. What modification does Headden propose for DMV? sent13
        4.2.4. What additional contexts do Spitkovsky et al. (2012) consider in their model? sent14
        4.2.5. What extension does Yang et al. (2020) propose for DMV? sent15

6. How do similar tokens behave in a grammar, and how can this be captured? sent17
    6.1. Can you provide an example of similar syntactic behavior among tokens? sent18
    6.2. How can generation probabilities be computed to capture syntactic similarity? sent19
        6.2.1. What approaches are used to compute these probabilities, and on what are they based? sent20, sent21","6. How do similar tokens behave in a grammar, and how can this be captured? sent17
    6.1. Can you provide an example of similar syntactic behavior among tokens? sent18
    6.2. How can generation probabilities be computed to capture syntactic similarity? sent19
        6.2.1. What approaches are used to compute these probabilities, and on what are they based? sent20, sent21","Question: How can generation probabilities be computed to capture syntactic similarity in unsupervised dependency parsing?

Supporting sentence: sent19, sent20, sent21",How can generation probabilities be computed to capture syntactic similarity in unsupervised dependency parsing?,"sent19, sent20, sent21",2,implement,"One way to capture syntactic similarity is to compute generation probabilities from a set of features that conveys syntactic similarity.

A log-linear model can be used based on manually-designed local morpho-syntactic features, such as whether a word is a noun.

Additionally, a neural network can be employed to automatically learn such features.

Both approaches are based on the Dependency Model with Valence (DMV).","Question:

How can generation probabilities be computed to capture syntactic similarity in unsupervised dependency parsing?

Answer:

One way to capture syntactic similarity is to compute generation probabilities from a set of features that conveys syntactic similarity.

A log-linear model can be used based on manually-designed local morpho-syntactic features, such as whether a word is a noun.

Additionally, a neural network can be employed to automatically learn such features.

Both approaches are based on the Dependency Model with Valence (DMV)."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,Integrative Strategy,4,"Integrative strategy (known as win-win) modeling aims to achieve mutual gain among participants. For instance, Zhao et al. (2019) proposes to model the discourse-level strategy using a latent action reinforcement learning (LaRL) framework. LaRL can model strategy transition within a latent space. However, due to the lack of explicit strategy labels, LaRL can only analysis strategies in implicit space.

To resolve the problem, Chawla et al. (2021c) define a series of explicit strategies such as Elicit-Preference, Coordination and Empathy. While Elicit-Preference is a strategy attempting to discover the preference of the opponent, Coordination promotes mutual benefits by explicit offer or implicit suggestion. In order to capture user's preference, Chawla et al. (2022) utilize those strategies using a hierarchical neural model. Besides, Yamaguchi et al. (2021b) present another collaborative strategy set to negotiate workload and salaries during the interview, which goal is to reach an agreement between employer and employee. It assists humans in becoming better negotiators during this process, e.g., communicating politely, addressing concerns, and providing side offers.","sent1: Integrative strategy (known as win-win) modeling aims to achieve mutual gain among participants.
sent2: For instance, Zhao et al. (2019) proposes to model the discourse-level strategy using a latent action reinforcement learning (LaRL) framework.
sent3: LaRL can model strategy transition within a latent space.
sent4: However, due to the lack of explicit strategy labels, LaRL can only analysis strategies in implicit space.
sent5: To resolve the problem, Chawla et al. (2021c) define a series of explicit strategies such as Elicit-Preference, Coordination and Empathy.
sent6: While Elicit-Preference is a strategy attempting to discover the preference of the opponent, Coordination promotes mutual benefits by explicit offer or implicit suggestion.
sent7: In order to capture user's preference, Chawla et al. (2022) utilize those strategies using a hierarchical neural model.
sent8: Besides, Yamaguchi et al. (2021b) present another collaborative strategy set to negotiate workload and salaries during the interview, which goal is to reach an agreement between employer and employee.
sent9: It assists humans in becoming better negotiators during this process, e.g., communicating politely, addressing concerns, and providing side offers.","1. What is the aim of integrative strategy modeling in negotiation dialogue systems? sent1
    1.1. How does Zhao et al. (2019) propose to model the discourse-level strategy? sent2
        1.1.1. What can LaRL model within a latent space? sent3
        1.1.2. What limitation does LaRL face due to the lack of explicit strategy labels? sent4
    1.2. How do Chawla et al. (2021c) address the problem of implicit strategy analysis? sent5
        1.2.1. What are some explicit strategies defined by Chawla et al. (2021c)? sent6
        1.2.2. How are these strategies utilized to capture user preferences? sent7
    1.3. What collaborative strategy set is presented by Yamaguchi et al. (2021b), and what is its goal? sent8
        1.3.1. How does this strategy set assist humans in negotiation? sent9","1. What is the aim of integrative strategy modeling in negotiation dialogue systems? sent1
    1.1. How does Zhao et al. (2019) propose to model the discourse-level strategy? sent2
        1.1.1. What can LaRL model within a latent space? sent3
        1.1.2. What limitation does LaRL face due to the lack of explicit strategy labels? sent4
    1.2. How do Chawla et al. (2021c) address the problem of implicit strategy analysis? sent5
        1.2.1. What are some explicit strategies defined by Chawla et al. (2021c)? sent6
        1.2.2. How are these strategies utilized to capture user preferences? sent7
    1.3. What collaborative strategy set is presented by Yamaguchi et al. (2021b), and what is its goal? sent8
        1.3.1. How does this strategy set assist humans in negotiation? sent9","1. What is the aim of integrative strategy modeling in negotiation dialogue systems? sent1
    1.1. How does Zhao et al. (2019) propose to model the discourse-level strategy? sent2
        1.1.1. What can LaRL model within a latent space? sent3
        1.1.2. What limitation does LaRL face due to the lack of explicit strategy labels? sent4
    1.2. How do Chawla et al. (2021c) address the problem of implicit strategy analysis? sent5
        1.2.1. What are some explicit strategies defined by Chawla et al. (2021c)? sent6
        1.2.2. How are these strategies utilized to capture user preferences? sent7
    1.3. What collaborative strategy set is presented by Yamaguchi et al. (2021b), and what is its goal? sent8
        1.3.1. How does this strategy set assist humans in negotiation? sent9","Question: How do explicit strategies in negotiation dialogue systems enhance the understanding and capture of user preferences?

Supporting sentence: sent5, sent6, sent7",How do explicit strategies in negotiation dialogue systems enhance the understanding and capture of user preferences?,"sent5, sent6, sent7",4,implement,"Explicit strategies in negotiation dialogue systems enhance the understanding and capture of user preferences by defining a series of strategies such as Elicit-Preference, Coordination, and Empathy.

Elicit-Preference is a strategy that attempts to discover the preference of the opponent.

Coordination promotes mutual benefits by explicit offer or implicit suggestion.

In order to capture user preferences, these strategies are utilized using a hierarchical neural model (Chawla et al., 2022).","Question:

How do explicit strategies in negotiation dialogue systems enhance the understanding and capture of user preferences?

Answer:

Explicit strategies in negotiation dialogue systems enhance the understanding and capture of user preferences by defining a series of strategies such as Elicit-Preference, Coordination, and Empathy.

Elicit-Preference is a strategy that attempts to discover the preference of the opponent.

Coordination promotes mutual benefits by explicit offer or implicit suggestion.

In order to capture user preferences, these strategies are utilized using a hierarchical neural model (Chawla et al., 2022)."
254854317,The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges,https://www.semanticscholar.org/paper/44ebfdb670007b3949507be0d1a1fca93bc3d5d5,Linguistic-Driven Approaches,4,"Equivalence Constraint In a well-formed codeswitched sentence, the switching takes place at those points where the grammatical constraints of both languages are satisfied (Poplack, 1980). Fung (2012, 2013) incorporate this syntactic constraint to a statistical code-switch language model (LM) and evaluate the model on Chinese-English code-switched speech recognition. On the same line of work, Pratapa et al. (2018a); Pratapa and Choudhury (2021) implement the same constraint to Hindi-English CSW data by producing parse trees of parallel sentences and matching the surface order of child nodes in the trees. Winata et al. (2019c) apply the constraint to generate synthetic CSW text and find that combining the real CSW data with synthetic CSW data can effectively improve the perplexity. They also treat parallel sentences as a linear structure and only allow switching on non-crossing alignments.

Matrix-Embedded Language Framework (MLF) Myers-Scotton (1997) proposed that in bilingual CSW, there exists an asymmetrical relationship between the dominant matrix language and the subordinate embedded language.

Matrix language provides the frame of the sentence by governing all or most of the most of the grammatical morphemes as well as word order, whereas syntactic elements that bear no or only limited grammatical function can be provided by the embedded language (Johanson, 1999;Myers-Scotton, 2005 Functional Head Constraint Belazi et al. (1994) posit that it is impossible to switch languages between a functional head and its complement because of the strong relationship between the two constituents. Li and Fung (2014) use the constraint of the LM by first expanding the search network with a translation model and then using parsing to restrict paths to those permissible under the constraint.","sent1: Equivalence Constraint In a well-formed codeswitched sentence, the switching takes place at those points where the grammatical constraints of both languages are satisfied (Poplack, 1980).
sent2: Fung (2012, 2013) incorporate this syntactic constraint to a statistical code-switch language model (LM) and evaluate the model on Chinese-English code-switched speech recognition.
sent3: On the same line of work, Pratapa et al. (2018a); Pratapa and Choudhury (2021) implement the same constraint to Hindi-English CSW data by producing parse trees of parallel sentences and matching the surface order of child nodes in the trees.
sent4: Winata et al. (2019c) apply the constraint to generate synthetic CSW text and find that combining the real CSW data with synthetic CSW data can effectively improve the perplexity.
sent5: They also treat parallel sentences as a linear structure and only allow switching on non-crossing alignments.
sent6: Matrix-Embedded Language Framework (MLF) Myers-Scotton (1997) proposed that in bilingual CSW, there exists an asymmetrical relationship between the dominant matrix language and the subordinate embedded language.
sent7: Matrix language provides the frame of the sentence by governing all or most of the most of the grammatical morphemes as well as word order, whereas syntactic elements that bear no or only limited grammatical function can be provided by the embedded language
sent8: (Johanson, 1999;Myers-Scotton, 2005 Functional Head Constraint Belazi et al. (1994) posit that it is impossible to switch languages between a functional head and its complement because of the strong relationship between the two constituents.
sent9: Li and Fung (2014) use the constraint of the LM by first expanding the search network with a translation model and then using parsing to restrict paths to those permissible under the constraint.","1. What is the Equivalence Constraint in code-switching? sent1
    1.1. How has Fung incorporated the Equivalence Constraint into a statistical code-switch language model? sent2
    1.2. How have Pratapa et al. implemented the Equivalence Constraint in Hindi-English code-switching data? sent3
    1.3. How did Winata et al. apply the Equivalence Constraint to generate synthetic code-switched text? sent4
        1.3.1. How do they treat parallel sentences in their approach? sent5
2. What is the Matrix-Embedded Language Framework (MLF) in bilingual code-switching? sent6
    2.1. How does the matrix language function in the MLF framework? sent7
3. What is the Functional Head Constraint in code-switching? sent8
    3.1. How did Li and Fung utilize the Functional Head Constraint in their language model? sent9","1. What is the Equivalence Constraint in code-switching? sent1
    1.1. How has Fung incorporated the Equivalence Constraint into a statistical code-switch language model? sent2
    1.2. How have Pratapa et al. implemented the Equivalence Constraint in Hindi-English code-switching data? sent3
    1.3. How did Winata et al. apply the Equivalence Constraint to generate synthetic code-switched text? sent4
        1.3.1. How do they treat parallel sentences in their approach? sent5","1. What is the Equivalence Constraint in code-switching? sent1
    1.1. How has Fung incorporated the Equivalence Constraint into a statistical code-switch language model? sent2
    1.2. How have Pratapa et al. implemented the Equivalence Constraint in Hindi-English code-switching data? sent3
    1.3. How did Winata et al. apply the Equivalence Constraint to generate synthetic code-switched text? sent4
        1.3.1. How do they treat parallel sentences in their approach? sent5","Question: How is the Equivalence Constraint applied in generating synthetic code-switched text, and how are parallel sentences treated?

Supporting sentence: sent4, sent5","How is the Equivalence Constraint applied in generating synthetic code-switched text, and how are parallel sentences treated?","sent4, sent5",2,implement,"The Equivalence Constraint is applied in generating synthetic code-switched text by ensuring that the switching takes place at points where the grammatical constraints of both languages are satisfied.

Winata et al. (2019c) apply this constraint to generate synthetic code-switched text and find that combining the real code-switched data with synthetic code-switched data can effectively improve the perplexity.

They also treat parallel sentences as a linear structure and only allow switching on non-crossing alignments.","Question:

How is the Equivalence Constraint applied in generating synthetic code-switched text, and how are parallel sentences treated?

Answer:

The Equivalence Constraint is applied in generating synthetic code-switched text by ensuring that the switching takes place at points where the grammatical constraints of both languages are satisfied.

Winata et al. (2019c) apply this constraint to generate synthetic code-switched text and find that combining the real code-switched data with synthetic code-switched data can effectively improve the perplexity.

They also treat parallel sentences as a linear structure and only allow switching on non-crossing alignments."
258557362,Large Language Models Meet NL2Code: A Survey,https://www.semanticscholar.org/paper/4f939f0751e5484f54089f6a97598e39afdcb3b5,Large Language Models for NL2Code,9,"Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code. To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2 We summarize the related surveys in Appendix A.  problem domain. Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters. There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task. Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility. For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes. One trend observed is that these large language models are consistently growing in size as the research field advances. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.

Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation. Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code. While they show surprisingly good performance on NL2Code, most of them are not readily accessible. At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios. For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages. Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing. Details of the website can be found in Appendix B.

These models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023). One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions. Other notable products include CodeGeeX 3 and CodeWhisperer 4 . A summary of 10 products can be found in Appendix Table 5. Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users. There is still room for improvement before LLMs can be fully practical and capable of coding like humans.","sent1: Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code.
sent2: To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2
sent3: We summarize the related surveys in Appendix A.  problem domain.
sent4: Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus.
sent5: For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens.
sent6: During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters.
sent7: There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance.
sent8: We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task.
sent9: Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility.
sent10: For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes.
sent11: One trend observed is that these large language models are consistently growing in size as the research field advances.
sent12: Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.
sent13: Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation.
sent14: Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus.
sent15: Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code.
sent16: While they show surprisingly good performance on NL2Code, most of them are not readily accessible.
sent17: At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code.
sent18: Besides, recent studies have proposed various approaches to address specific NL2Code scenarios.
sent19: For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages.
sent20: Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code.
sent21: As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing.
sent22: Details of the website can be found in Appendix B.These models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023).
sent23: One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions.
sent24: Other notable products include CodeGeeX 3 and CodeWhisperer 4 .
sent25: A summary of 10 products can be found in Appendix Table 5.
sent26: Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users.
sent27: There is still room for improvement before LLMs can be fully practical and capable of coding like humans.","1. What is the NL2Code task? sent1
    1.1. How is the NL2Code task visually illustrated in the paper? sent2
    1.2. What is summarized in Appendix A? sent3
2. What are existing large language models for the NL2Code task based on? sent4
    2.1. What training objective do most LLMs use for better code generation performance? sent5
    2.2. How can an LLM tackle NL2Code problems during inference? sent6
    2.3. What methods are used to further boost performance in NL2Code tasks? sent7
3. How many representative LLMs for the NL2Code task are investigated in the paper? sent8
    3.1. What details are summarized in Table 1? sent9
    3.2. How are the models presented in Figure 2? sent10
    3.3. What trend is observed in the size of large language models? sent11
    3.4. Which architecture is favored for pre-trained models with larger sizes? sent12
4. How do early works like GPT-C, PyMT5, and PLBART perform in zero-shot code generation? sent13
5. What limitations do large-scale models like GPT-Neo and GPT-J have in the NL2Code task? sent14
6. What are some powerful LLMs recently proposed for NL2Code? sent15
    6.1. What is a limitation of these powerful LLMs? sent16
7. What are some excellent open-source models proposed for NL2Code? sent17
8. What approaches have recent studies proposed for specific NL2Code scenarios? sent18
    8.1. Can you provide examples of models designed for specific scenarios or languages? sent19
    8.2. What additional capabilities do models like InCoder, FIM, and SantaCoder have? sent20
9. How is the paper keeping up-to-date with the latest advances in LLMs for NL2Code? sent21
    9.1. Where can details of the website be found? sent22
10. How are LLMs for NL2Code applied in real-world products? sent23
    10.1. What are some notable products mentioned in the paper? sent24
    10.2. Where can a summary of 10 products be found? sent25
11. What have recent studies shown about the recommendations provided by these products? sent26
12. What is the current state of LLMs in terms of being practical and capable of coding like humans? sent27","2. What are existing large language models for the NL2Code task based on? sent4
    2.1. What training objective do most LLMs use for better code generation performance? sent5
    2.2. How can an LLM tackle NL2Code problems during inference? sent6
    2.3. What methods are used to further boost performance in NL2Code tasks? sent7

3. How many representative LLMs for the NL2Code task are investigated in the paper? sent8
    3.1. What details are summarized in Table 1? sent9
    3.2. How are the models presented in Figure 2? sent10
    3.3. What trend is observed in the size of large language models? sent11
    3.4. Which architecture is favored for pre-trained models with larger sizes? sent12","2. What are existing large language models for the NL2Code task based on? sent4
    2.1. What training objective do most LLMs use for better code generation performance? sent5
    2.2. How can an LLM tackle NL2Code problems during inference? sent6
    2.3. What methods are used to further boost performance in NL2Code tasks? sent7","Question: What methods are employed to enhance the performance of large language models in NL2Code tasks?

Supporting sentence: sent7",What methods are employed to enhance the performance of large language models in NL2Code tasks?,sent7,4,implement,"Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code-related unlabelled corpus.

For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens.

During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters.

There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance.

Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.

Recent studies have proposed various approaches to address specific NL2Code scenarios.

For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), and BLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages.

Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code.","Question:

What methods are employed to enhance the performance of large language models in NL2Code tasks?

Answer:

Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code-related unlabelled corpus.

For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens.

During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters.

There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance.

Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.

Recent studies have proposed various approaches to address specific NL2Code scenarios.

For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), and BLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages.

Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code."
258557362,Large Language Models Meet NL2Code: A Survey,https://www.semanticscholar.org/paper/4f939f0751e5484f54089f6a97598e39afdcb3b5,Large Language Models for NL2Code,9,"Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code. To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2 We summarize the related surveys in Appendix A.  problem domain. Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters. There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task. Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility. For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes. One trend observed is that these large language models are consistently growing in size as the research field advances. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.

Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation. Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code. While they show surprisingly good performance on NL2Code, most of them are not readily accessible. At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios. For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages. Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing. Details of the website can be found in Appendix B.

These models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023). One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions. Other notable products include CodeGeeX 3 and CodeWhisperer 4 . A summary of 10 products can be found in Appendix Table 5. Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users. There is still room for improvement before LLMs can be fully practical and capable of coding like humans.","sent1: Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code.
sent2: To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2
sent3: We summarize the related surveys in Appendix A.  problem domain.
sent4: Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus.
sent5: For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens.
sent6: During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters.
sent7: There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance.
sent8: We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task.
sent9: Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility.
sent10: For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes.
sent11: One trend observed is that these large language models are consistently growing in size as the research field advances.
sent12: Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.
sent13: Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation.
sent14: Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus.
sent15: Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code.
sent16: While they show surprisingly good performance on NL2Code, most of them are not readily accessible.
sent17: At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code.
sent18: Besides, recent studies have proposed various approaches to address specific NL2Code scenarios.
sent19: For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages.
sent20: Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code.
sent21: As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing.
sent22: Details of the website can be found in Appendix B.These models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023).
sent23: One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions.
sent24: Other notable products include CodeGeeX 3 and CodeWhisperer 4 .
sent25: A summary of 10 products can be found in Appendix Table 5.
sent26: Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users.
sent27: There is still room for improvement before LLMs can be fully practical and capable of coding like humans.","1. What is the NL2Code task? sent1
    1.1. How is the NL2Code task visually illustrated in the paper? sent2
    1.2. What is summarized in Appendix A? sent3
2. What are existing large language models for the NL2Code task based on? sent4
    2.1. What training objective do most LLMs use for better code generation performance? sent5
    2.2. How can an LLM tackle NL2Code problems during inference? sent6
    2.3. What methods are used to further boost performance in NL2Code tasks? sent7
3. How many representative LLMs for the NL2Code task are investigated in the paper? sent8
    3.1. What details are summarized in Table 1? sent9
    3.2. How are the models presented in Figure 2? sent10
    3.3. What trend is observed in the size of large language models? sent11
    3.4. Which architecture is favored for pre-trained models with larger sizes? sent12
4. How do early works like GPT-C, PyMT5, and PLBART perform in zero-shot code generation? sent13
5. What limitations do large-scale models like GPT-Neo and GPT-J have in the NL2Code task? sent14
6. What are some powerful LLMs recently proposed for NL2Code? sent15
    6.1. What is a limitation of these powerful LLMs? sent16
7. What are some excellent open-source models proposed for NL2Code? sent17
8. What approaches have recent studies proposed for specific NL2Code scenarios? sent18
    8.1. Can you provide examples of models designed for specific scenarios or languages? sent19
    8.2. What additional capabilities do models like InCoder, FIM, and SantaCoder have? sent20
9. How is the paper keeping up-to-date with the latest advances in LLMs for NL2Code? sent21
    9.1. Where can details of the website be found? sent22
10. How are LLMs for NL2Code applied in real-world products? sent23
    10.1. What are some notable products mentioned in the paper? sent24
    10.2. Where can a summary of 10 products be found? sent25
11. What have recent studies shown about the recommendations provided by these products? sent26
12. What is the current state of LLMs in terms of being practical and capable of coding like humans? sent27","2. What are existing large language models for the NL2Code task based on? sent4
    2.1. What training objective do most LLMs use for better code generation performance? sent5
    2.2. How can an LLM tackle NL2Code problems during inference? sent6
    2.3. What methods are used to further boost performance in NL2Code tasks? sent7

3. How many representative LLMs for the NL2Code task are investigated in the paper? sent8
    3.1. What details are summarized in Table 1? sent9
    3.2. How are the models presented in Figure 2? sent10
    3.3. What trend is observed in the size of large language models? sent11
    3.4. Which architecture is favored for pre-trained models with larger sizes? sent12","3. How many representative LLMs for the NL2Code task are investigated in the paper? sent8
    3.1. What details are summarized in Table 1? sent9
    3.2. How are the models presented in Figure 2? sent10
    3.3. What trend is observed in the size of large language models? sent11
    3.4. Which architecture is favored for pre-trained models with larger sizes? sent12","Question: What trend is observed in the development of large language models for the NL2Code task?

Supporting sentence: sent11, sent12",What trend is observed in the development of large language models for the NL2Code task?,"sent11, sent12",4,implement,"One trend observed is that these large language models are consistently growing in size as the research field advances.

Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.","Question:

What trend is observed in the development of large language models for the NL2Code task?

Answer:

One trend observed is that these large language models are consistently growing in size as the research field advances.

Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes."
254854669,Let's Negotiate! A Survey of Negotiation Dialogue Systems,https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f,Supervised Learning,6,"Supervised learning (SL) is another popular paradigm for policy learning. (Lewis et al., 2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data. However, supervised learning only aims to mimic the average human behavior, so He et al. (2018) propose to finetune the supervised model to directly optimize for a particular dialogue reward function, which is defined as i) the utility function of the final price for the buyer and seller ii) the difference between two agents' utilities iii) the number of utterances in the dialogue. Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training. The system response would be generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy. In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm. Recently, Dutt et al. (2021b) propose a generalised framework for identifying resisting strategies in persuasive negotiations using a pre-trained BERT model (Devlin et al., 2019).","sent1: Supervised learning (SL) is another popular paradigm for policy learning.
sent2: (Lewis et al., 2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data.
sent3: However, supervised learning only aims to mimic the average human behavior, so He et al. (2018) propose to finetune the supervised model to directly optimize for a particular dialogue reward function, which is defined as i) the utility function of the final price for the buyer and seller ii) the difference between two agents' utilities iii) the number of utterances in the dialogue.
sent4: Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training.
sent5: The system response would be generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy.
sent6: In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm.
sent7: Recently, Dutt et al. (2021b) propose a generalised framework for identifying resisting strategies in persuasive negotiations using a pre-trained BERT model (Devlin et al., 2019).","1. What is supervised learning (SL) in the context of policy learning? sent1
    1.1. How is a Seq2Seq model used in supervised learning for negotiation dialogue systems? sent2
    1.2. What is a limitation of supervised learning, and how is it addressed by He et al. (2018)? sent3
    1.3. How do Zhou et al. (2020) utilize supervised training in negotiation dialogue systems? sent4
        1.3.1. How is the system response generated in Zhou et al. (2020)'s approach? sent5
    1.4. How do Joshi et al. (2021) enhance interpretability in policy learning? sent6
    1.5. What recent framework is proposed by Dutt et al. (2021b) for persuasive negotiations? sent7","1. What is supervised learning (SL) in the context of policy learning? sent1
    1.1. How is a Seq2Seq model used in supervised learning for negotiation dialogue systems? sent2
    1.2. What is a limitation of supervised learning, and how is it addressed by He et al. (2018)? sent3
    1.3. How do Zhou et al. (2020) utilize supervised training in negotiation dialogue systems? sent4
        1.3.1. How is the system response generated in Zhou et al. (2020)'s approach? sent5
    1.4. How do Joshi et al. (2021) enhance interpretability in policy learning? sent6
    1.5. What recent framework is proposed by Dutt et al. (2021b) for persuasive negotiations? sent7","1. What is supervised learning (SL) in the context of policy learning? sent1
    1.1. How is a Seq2Seq model used in supervised learning for negotiation dialogue systems? sent2
    1.2. What is a limitation of supervised learning, and how is it addressed by He et al. (2018)? sent3
    1.3. How do Zhou et al. (2020) utilize supervised training in negotiation dialogue systems? sent4
        1.3.1. How is the system response generated in Zhou et al. (2020)'s approach? sent5
    1.4. How do Joshi et al. (2021) enhance interpretability in policy learning? sent6
    1.5. What recent framework is proposed by Dutt et al. (2021b) for persuasive negotiations? sent7","Question: How is supervised learning applied to negotiation dialogue systems, and what are its limitations and enhancements?

Supporting sentence: sent1, sent2, sent3, sent4, sent5, sent6, sent7","How is supervised learning applied to negotiation dialogue systems, and what are its limitations and enhancements?","sent1, sent2, sent3, sent4, sent5, sent6, sent7",4,implement,"Supervised learning (SL) is a popular paradigm for policy learning in negotiation dialogue systems.

Lewis et al. (2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data.

However, supervised learning only aims to mimic the average human behavior.

He et al. (2018) propose to finetune the supervised model to directly optimize for a particular dialogue reward function, which is defined as the utility function of the final price for the buyer and seller, the difference between two agents' utilities, and the number of utterances in the dialogue.

Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training.

The system response would be generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy.

In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm.

Recently, Dutt et al. (2021b) propose a generalized framework for identifying resisting strategies in persuasive negotiations using a pre-trained BERT model (Devlin et al., 2019).","Question:

How is supervised learning applied to negotiation dialogue systems, and what are its limitations and enhancements?

Answer:

Supervised learning (SL) is a popular paradigm for policy learning in negotiation dialogue systems.

Lewis et al. (2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data.

However, supervised learning only aims to mimic the average human behavior.

He et al. (2018) propose to finetune the supervised model to directly optimize for a particular dialogue reward function, which is defined as the utility function of the final price for the buyer and seller, the difference between two agents' utilities, and the number of utterances in the dialogue.

Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training.

The system response would be generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy.

In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm.

Recently, Dutt et al. (2021b) propose a generalized framework for identifying resisting strategies in persuasive negotiations using a pre-trained BERT model (Devlin et al., 2019)."
258832362,Beyond Words: A Comprehensive Survey of Sentence Representations,https://www.semanticscholar.org/paper/8579ad4a8e835cada64c0eae142a00205ce857b5,Natural Language Inference,4,"Natural Language Inference (NLI) is the process of determining the logical relationship between a premise (an assumed true sentence) and a hypothesis (a possibly true sentence). The objective of NLI is to determine whether the hypothesis can be logically inferred from the premise (entailment), contradicts the premise (contradiction), or is neutral with respect to it (Dagan et al., 2013). NLI serves as a proxy for evaluating natural language understanding. According to Conneau et al. (2017), learning sentence representations using NLI data can be effectively transferred to other NLP tasks, demonstrating the generality of this approach.

In § 2.3, we discussed Siamese-BERT networks as presented in Reimers and Gurevych (2019). There are two noteworthy components to this model. First, processing inputs individually without promoting interaction between words; second, using an encoder like BERT that is not generative as its backbone model. The first component is computationally efficient but has been found to result in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019). This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings. In order to solve this, Cheng (2021) incorporate word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks. Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015): using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT.

Meanwhile, generative models have been pretrained on huge amounts of text data, and can perform a myriad of tasks. Ni et al. (2022) examined the use of generative models as backbone for extracting sentence embeddings. They ex-plore three methods using pre-trained T5 encoderdecoder models: using the representation of the first token of the encoder, using the representation of the first generated token of the decoder, or using the mean of the representations from the encoder. This is one of the first studies that shows the utility of generative models for obtaining sentence representations.","sent1: Natural Language Inference (NLI) is the process of determining the logical relationship between a premise (an assumed true sentence) and a hypothesis (a possibly true sentence).
sent2: The objective of NLI is to determine whether the hypothesis can be logically inferred from the premise (entailment), contradicts the premise (contradiction), or is neutral with respect to it (Dagan et al., 2013).
sent3: NLI serves as a proxy for evaluating natural language understanding.
sent4: According to Conneau et al. (2017), learning sentence representations using NLI data can be effectively transferred to other NLP tasks, demonstrating the generality of this approach.
sent5: In § 2.3, we discussed Siamese-BERT networks as presented in Reimers and Gurevych (2019).
sent6: There are two noteworthy components to this model.
sent7: First, processing inputs individually without promoting interaction between words; second, using an encoder like BERT that is not generative as its backbone model.
sent8: The first component is computationally efficient but has been found to result in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019).
sent9: This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings.
sent10: In order to solve this, Cheng (2021) incorporate word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks.
sent11: Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015): using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT.
sent12: Meanwhile, generative models have been pretrained on huge amounts of text data, and can perform a myriad of tasks.
sent13: Ni et al. (2022) examined the use of generative models as backbone for extracting sentence embeddings.
sent14: They ex-plore three methods using pre-trained T5 encoderdecoder models: using the representation of the first token of the encoder, using the representation of the first generated token of the decoder, or using the mean of the representations from the encoder.
sent15: This is one of the first studies that shows the utility of generative models for obtaining sentence representations.","1. What is Natural Language Inference (NLI)? sent1
    1.1. What is the objective of NLI? sent2
    1.2. How does NLI serve in evaluating natural language understanding? sent3
    1.3. How can learning sentence representations using NLI data be beneficial? sent4
2. What model is discussed in § 2.3, and who presented it? sent5
    2.1. What are the two noteworthy components of the Siamese-BERT model? sent6
        2.1.1. What is the first component of the Siamese-BERT model, and what is its impact on performance? sent7, sent8
        2.1.2. How does the lack of interaction in the first component affect the network's ability? sent9
        2.1.3. How does Cheng (2021) address the limitations of the Siamese-BERT model? sent10
            2.1.3.1. What approach does Cheng (2021) use to improve the performance of Siamese-BERT? sent11
3. How have generative models been utilized in sentence representation? sent12
    3.1. What did Ni et al. (2022) examine regarding generative models? sent13
    3.2. What methods do Ni et al. (2022) explore using pre-trained T5 models? sent14
    3.3. What significance does the study by Ni et al. (2022) hold? sent15","1. What is Natural Language Inference (NLI)? sent1
    1.1. What is the objective of NLI? sent2
    1.2. How does NLI serve in evaluating natural language understanding? sent3
    1.3. How can learning sentence representations using NLI data be beneficial? sent4

2. What model is discussed in § 2.3, and who presented it? sent5
    2.1. What are the two noteworthy components of the Siamese-BERT model? sent6
        2.1.1. What is the first component of the Siamese-BERT model, and what is its impact on performance? sent7, sent8
        2.1.2. How does the lack of interaction in the first component affect the network's ability? sent9
        2.1.3. How does Cheng (2021) address the limitations of the Siamese-BERT model? sent10
            2.1.3.1. What approach does Cheng (2021) use to improve the performance of Siamese-BERT? sent11

    2.1. What are the two noteworthy components of the Siamese-BERT model? sent6
        2.1.1. What is the first component of the Siamese-BERT model, and what is its impact on performance? sent7, sent8
        2.1.2. How does the lack of interaction in the first component affect the network's ability? sent9
        2.1.3. How does Cheng (2021) address the limitations of the Siamese-BERT model? sent10
            2.1.3.1. What approach does Cheng (2021) use to improve the performance of Siamese-BERT? sent11

3. How have generative models been utilized in sentence representation? sent12
    3.1. What did Ni et al. (2022) examine regarding generative models? sent13
    3.2. What methods do Ni et al. (2022) explore using pre-trained T5 models? sent14
    3.3. What significance does the study by Ni et al. (2022) hold? sent15","1. What is Natural Language Inference (NLI)? sent1
    1.1. What is the objective of NLI? sent2
    1.2. How does NLI serve in evaluating natural language understanding? sent3
    1.3. How can learning sentence representations using NLI data be beneficial? sent4","Question: How can learning sentence representations using NLI data be effectively transferred to other NLP tasks?

Supporting sentence: sent4",How can learning sentence representations using NLI data be effectively transferred to other NLP tasks?,sent4,4,implement,"Learning sentence representations using NLI data can be effectively transferred to other NLP tasks by demonstrating the generality of this approach, as noted by Conneau et al. (2017).","Question:

How can learning sentence representations using NLI data be effectively transferred to other NLP tasks?

Answer:

Learning sentence representations using NLI data can be effectively transferred to other NLP tasks by demonstrating the generality of this approach, as noted by Conneau et al. (2017)."
258832362,Beyond Words: A Comprehensive Survey of Sentence Representations,https://www.semanticscholar.org/paper/8579ad4a8e835cada64c0eae142a00205ce857b5,Natural Language Inference,4,"Natural Language Inference (NLI) is the process of determining the logical relationship between a premise (an assumed true sentence) and a hypothesis (a possibly true sentence). The objective of NLI is to determine whether the hypothesis can be logically inferred from the premise (entailment), contradicts the premise (contradiction), or is neutral with respect to it (Dagan et al., 2013). NLI serves as a proxy for evaluating natural language understanding. According to Conneau et al. (2017), learning sentence representations using NLI data can be effectively transferred to other NLP tasks, demonstrating the generality of this approach.

In § 2.3, we discussed Siamese-BERT networks as presented in Reimers and Gurevych (2019). There are two noteworthy components to this model. First, processing inputs individually without promoting interaction between words; second, using an encoder like BERT that is not generative as its backbone model. The first component is computationally efficient but has been found to result in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019). This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings. In order to solve this, Cheng (2021) incorporate word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks. Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015): using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT.

Meanwhile, generative models have been pretrained on huge amounts of text data, and can perform a myriad of tasks. Ni et al. (2022) examined the use of generative models as backbone for extracting sentence embeddings. They ex-plore three methods using pre-trained T5 encoderdecoder models: using the representation of the first token of the encoder, using the representation of the first generated token of the decoder, or using the mean of the representations from the encoder. This is one of the first studies that shows the utility of generative models for obtaining sentence representations.","sent1: Natural Language Inference (NLI) is the process of determining the logical relationship between a premise (an assumed true sentence) and a hypothesis (a possibly true sentence).
sent2: The objective of NLI is to determine whether the hypothesis can be logically inferred from the premise (entailment), contradicts the premise (contradiction), or is neutral with respect to it (Dagan et al., 2013).
sent3: NLI serves as a proxy for evaluating natural language understanding.
sent4: According to Conneau et al. (2017), learning sentence representations using NLI data can be effectively transferred to other NLP tasks, demonstrating the generality of this approach.
sent5: In § 2.3, we discussed Siamese-BERT networks as presented in Reimers and Gurevych (2019).
sent6: There are two noteworthy components to this model.
sent7: First, processing inputs individually without promoting interaction between words; second, using an encoder like BERT that is not generative as its backbone model.
sent8: The first component is computationally efficient but has been found to result in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019).
sent9: This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings.
sent10: In order to solve this, Cheng (2021) incorporate word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks.
sent11: Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015): using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT.
sent12: Meanwhile, generative models have been pretrained on huge amounts of text data, and can perform a myriad of tasks.
sent13: Ni et al. (2022) examined the use of generative models as backbone for extracting sentence embeddings.
sent14: They ex-plore three methods using pre-trained T5 encoderdecoder models: using the representation of the first token of the encoder, using the representation of the first generated token of the decoder, or using the mean of the representations from the encoder.
sent15: This is one of the first studies that shows the utility of generative models for obtaining sentence representations.","1. What is Natural Language Inference (NLI)? sent1
    1.1. What is the objective of NLI? sent2
    1.2. How does NLI serve in evaluating natural language understanding? sent3
    1.3. How can learning sentence representations using NLI data be beneficial? sent4
2. What model is discussed in § 2.3, and who presented it? sent5
    2.1. What are the two noteworthy components of the Siamese-BERT model? sent6
        2.1.1. What is the first component of the Siamese-BERT model, and what is its impact on performance? sent7, sent8
        2.1.2. How does the lack of interaction in the first component affect the network's ability? sent9
        2.1.3. How does Cheng (2021) address the limitations of the Siamese-BERT model? sent10
            2.1.3.1. What approach does Cheng (2021) use to improve the performance of Siamese-BERT? sent11
3. How have generative models been utilized in sentence representation? sent12
    3.1. What did Ni et al. (2022) examine regarding generative models? sent13
    3.2. What methods do Ni et al. (2022) explore using pre-trained T5 models? sent14
    3.3. What significance does the study by Ni et al. (2022) hold? sent15","1. What is Natural Language Inference (NLI)? sent1
    1.1. What is the objective of NLI? sent2
    1.2. How does NLI serve in evaluating natural language understanding? sent3
    1.3. How can learning sentence representations using NLI data be beneficial? sent4

2. What model is discussed in § 2.3, and who presented it? sent5
    2.1. What are the two noteworthy components of the Siamese-BERT model? sent6
        2.1.1. What is the first component of the Siamese-BERT model, and what is its impact on performance? sent7, sent8
        2.1.2. How does the lack of interaction in the first component affect the network's ability? sent9
        2.1.3. How does Cheng (2021) address the limitations of the Siamese-BERT model? sent10
            2.1.3.1. What approach does Cheng (2021) use to improve the performance of Siamese-BERT? sent11

    2.1. What are the two noteworthy components of the Siamese-BERT model? sent6
        2.1.1. What is the first component of the Siamese-BERT model, and what is its impact on performance? sent7, sent8
        2.1.2. How does the lack of interaction in the first component affect the network's ability? sent9
        2.1.3. How does Cheng (2021) address the limitations of the Siamese-BERT model? sent10
            2.1.3.1. What approach does Cheng (2021) use to improve the performance of Siamese-BERT? sent11

3. How have generative models been utilized in sentence representation? sent12
    3.1. What did Ni et al. (2022) examine regarding generative models? sent13
    3.2. What methods do Ni et al. (2022) explore using pre-trained T5 models? sent14
    3.3. What significance does the study by Ni et al. (2022) hold? sent15","2. What model is discussed in § 2.3, and who presented it? sent5
    2.1. What are the two noteworthy components of the Siamese-BERT model? sent6
        2.1.1. What is the first component of the Siamese-BERT model, and what is its impact on performance? sent7, sent8
        2.1.2. How does the lack of interaction in the first component affect the network's ability? sent9
        2.1.3. How does Cheng (2021) address the limitations of the Siamese-BERT model? sent10
            2.1.3.1. What approach does Cheng (2021) use to improve the performance of Siamese-BERT? sent11","Question: How does the lack of word interaction in Siamese-BERT affect sentence embeddings, and how is this limitation addressed?

Supporting sentence: sent7, sent8, sent9, sent10, sent11","How does the lack of word interaction in Siamese-BERT affect sentence embeddings, and how is this limitation addressed?","sent7, sent8, sent9, sent10, sent11",2,implement,"The lack of word interaction in Siamese-BERT networks results in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019).

This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings.

To address this limitation, Cheng (2021) incorporates word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks.

Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015), using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT.","Question:

How does the lack of word interaction in Siamese-BERT affect sentence embeddings, and how is this limitation addressed?

Answer:

The lack of word interaction in Siamese-BERT networks results in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019).

This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings.

To address this limitation, Cheng (2021) incorporates word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks.

Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015), using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT."
258832362,Beyond Words: A Comprehensive Survey of Sentence Representations,https://www.semanticscholar.org/paper/8579ad4a8e835cada64c0eae142a00205ce857b5,Natural Language Inference,4,"Natural Language Inference (NLI) is the process of determining the logical relationship between a premise (an assumed true sentence) and a hypothesis (a possibly true sentence). The objective of NLI is to determine whether the hypothesis can be logically inferred from the premise (entailment), contradicts the premise (contradiction), or is neutral with respect to it (Dagan et al., 2013). NLI serves as a proxy for evaluating natural language understanding. According to Conneau et al. (2017), learning sentence representations using NLI data can be effectively transferred to other NLP tasks, demonstrating the generality of this approach.

In § 2.3, we discussed Siamese-BERT networks as presented in Reimers and Gurevych (2019). There are two noteworthy components to this model. First, processing inputs individually without promoting interaction between words; second, using an encoder like BERT that is not generative as its backbone model. The first component is computationally efficient but has been found to result in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019). This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings. In order to solve this, Cheng (2021) incorporate word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks. Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015): using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT.

Meanwhile, generative models have been pretrained on huge amounts of text data, and can perform a myriad of tasks. Ni et al. (2022) examined the use of generative models as backbone for extracting sentence embeddings. They ex-plore three methods using pre-trained T5 encoderdecoder models: using the representation of the first token of the encoder, using the representation of the first generated token of the decoder, or using the mean of the representations from the encoder. This is one of the first studies that shows the utility of generative models for obtaining sentence representations.","sent1: Natural Language Inference (NLI) is the process of determining the logical relationship between a premise (an assumed true sentence) and a hypothesis (a possibly true sentence).
sent2: The objective of NLI is to determine whether the hypothesis can be logically inferred from the premise (entailment), contradicts the premise (contradiction), or is neutral with respect to it (Dagan et al., 2013).
sent3: NLI serves as a proxy for evaluating natural language understanding.
sent4: According to Conneau et al. (2017), learning sentence representations using NLI data can be effectively transferred to other NLP tasks, demonstrating the generality of this approach.
sent5: In § 2.3, we discussed Siamese-BERT networks as presented in Reimers and Gurevych (2019).
sent6: There are two noteworthy components to this model.
sent7: First, processing inputs individually without promoting interaction between words; second, using an encoder like BERT that is not generative as its backbone model.
sent8: The first component is computationally efficient but has been found to result in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019).
sent9: This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings.
sent10: In order to solve this, Cheng (2021) incorporate word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks.
sent11: Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015): using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT.
sent12: Meanwhile, generative models have been pretrained on huge amounts of text data, and can perform a myriad of tasks.
sent13: Ni et al. (2022) examined the use of generative models as backbone for extracting sentence embeddings.
sent14: They ex-plore three methods using pre-trained T5 encoderdecoder models: using the representation of the first token of the encoder, using the representation of the first generated token of the decoder, or using the mean of the representations from the encoder.
sent15: This is one of the first studies that shows the utility of generative models for obtaining sentence representations.","1. What is Natural Language Inference (NLI)? sent1
    1.1. What is the objective of NLI? sent2
    1.2. How does NLI serve in evaluating natural language understanding? sent3
    1.3. How can learning sentence representations using NLI data be beneficial? sent4
2. What model is discussed in § 2.3, and who presented it? sent5
    2.1. What are the two noteworthy components of the Siamese-BERT model? sent6
        2.1.1. What is the first component of the Siamese-BERT model, and what is its impact on performance? sent7, sent8
        2.1.2. How does the lack of interaction in the first component affect the network's ability? sent9
        2.1.3. How does Cheng (2021) address the limitations of the Siamese-BERT model? sent10
            2.1.3.1. What approach does Cheng (2021) use to improve the performance of Siamese-BERT? sent11
3. How have generative models been utilized in sentence representation? sent12
    3.1. What did Ni et al. (2022) examine regarding generative models? sent13
    3.2. What methods do Ni et al. (2022) explore using pre-trained T5 models? sent14
    3.3. What significance does the study by Ni et al. (2022) hold? sent15","1. What is Natural Language Inference (NLI)? sent1
    1.1. What is the objective of NLI? sent2
    1.2. How does NLI serve in evaluating natural language understanding? sent3
    1.3. How can learning sentence representations using NLI data be beneficial? sent4

2. What model is discussed in § 2.3, and who presented it? sent5
    2.1. What are the two noteworthy components of the Siamese-BERT model? sent6
        2.1.1. What is the first component of the Siamese-BERT model, and what is its impact on performance? sent7, sent8
        2.1.2. How does the lack of interaction in the first component affect the network's ability? sent9
        2.1.3. How does Cheng (2021) address the limitations of the Siamese-BERT model? sent10
            2.1.3.1. What approach does Cheng (2021) use to improve the performance of Siamese-BERT? sent11

    2.1. What are the two noteworthy components of the Siamese-BERT model? sent6
        2.1.1. What is the first component of the Siamese-BERT model, and what is its impact on performance? sent7, sent8
        2.1.2. How does the lack of interaction in the first component affect the network's ability? sent9
        2.1.3. How does Cheng (2021) address the limitations of the Siamese-BERT model? sent10
            2.1.3.1. What approach does Cheng (2021) use to improve the performance of Siamese-BERT? sent11

3. How have generative models been utilized in sentence representation? sent12
    3.1. What did Ni et al. (2022) examine regarding generative models? sent13
    3.2. What methods do Ni et al. (2022) explore using pre-trained T5 models? sent14
    3.3. What significance does the study by Ni et al. (2022) hold? sent15","    2.1. What are the two noteworthy components of the Siamese-BERT model? sent6
        2.1.1. What is the first component of the Siamese-BERT model, and what is its impact on performance? sent7, sent8
        2.1.2. How does the lack of interaction in the first component affect the network's ability? sent9
        2.1.3. How does Cheng (2021) address the limitations of the Siamese-BERT model? sent10
            2.1.3.1. What approach does Cheng (2021) use to improve the performance of Siamese-BERT? sent11","Question: How does the lack of word interaction in Siamese-BERT affect sentence embeddings, and how is this limitation addressed?

Supporting sentence: sent7, sent8, sent9, sent10, sent11","How does the lack of word interaction in Siamese-BERT affect sentence embeddings, and how is this limitation addressed?","sent7, sent8, sent9, sent10, sent11",2,implement,"The lack of word interaction in Siamese-BERT networks results in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019).

This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings.

To address this limitation, Cheng (2021) incorporates word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks.

Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015), using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT.","Question:

How does the lack of word interaction in Siamese-BERT affect sentence embeddings, and how is this limitation addressed?

Answer:

The lack of word interaction in Siamese-BERT networks results in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019).

This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings.

To address this limitation, Cheng (2021) incorporates word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks.

Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015), using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT."
222133962,A Survey of Unsupervised Dependency Parsing,https://www.semanticscholar.org/paper/16160fbefc830117a8395a4b665c39feae834e48,Variational Autoencoder-Based Approaches,4,"As mentioned in Section 3.1, the training objective of a generative model is typically the probability of the training sentence and the dependency tree is marginalized as a hidden variable. However, the marginalized probability cannot usually be calculated accurately for more complex models that do not make strict independence assumption. Instead, a variational autoencoder maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability. Since the intermediate representation follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema.

Three unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1). There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.

Recurrent Neural Network Grammars (RNNG) ) is a transition-based constituent parser, with a discriminative and a generative variant. Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing. Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence. The probability of each operation is calculated by a neural network. Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNNG as the encoder and decoder of a variational autoencoder respectively. However, because RNNG has a strong expressive power, it is prone to overfitting in the unsupervised setting. Li et al. (2019) propose to use posterior regularization to introduce linguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent.

The model proposed by Corro and Titov (2018) is also based on a variational autoencoder. It is designed for semi-supervised dependency parsing, but in principle it can also be applied for unsupervised dependency parsing. The encoder of this model is a conditional random field model while the decoder generates a sentence based on a graph convolutional neural network whose structure is specified by the dependency tree. Since the variational autoencoder needs Monte Carlo sampling to approximate the gradient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel random perturbation. Jang et al. (2017) use differentiable dynamic programming to design an efficient approximate sampling algorithm.

The variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also specifies a Gaussian prior over the intermediate continuous vector.","sent1: As mentioned in Section 3.1, the training objective of a generative model is typically the probability of the training sentence and the dependency tree is marginalized as a hidden variable.
sent2: However, the marginalized probability cannot usually be calculated accurately for more complex models that do not make strict independence assumption.
sent3: Instead, a variational autoencoder maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability.
sent4: Since the intermediate representation follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema.
sent5: Three unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1).
sent6: There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.
sent7: Recurrent Neural Network Grammars (RNNG) ) is a transition-based constituent parser, with a discriminative and a generative variant.
sent8: Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing.
sent9: Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence.
sent10: The probability of each operation is calculated by a neural network.
sent11: Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNNG as the encoder and decoder of a variational autoencoder respectively.
sent12: However, because RNNG has a strong expressive power, it is prone to overfitting in the unsupervised setting.
sent13: Li et al. (2019) propose to use posterior regularization to introduce linguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent.
sent14: The model proposed by Corro and Titov (2018) is also based on a variational autoencoder.
sent15: It is designed for semi-supervised dependency parsing, but in principle it can also be applied for unsupervised dependency parsing.
sent16: The encoder of this model is a conditional random field model while the decoder generates a sentence based on a graph convolutional neural network whose structure is specified by the dependency tree.
sent17: Since the variational autoencoder needs Monte Carlo sampling to approximate the gradient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel random perturbation.
sent18: Jang et al. (2017) use differentiable dynamic programming to design an efficient approximate sampling algorithm.
sent19: The variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution.
sent20: It also specifies a Gaussian prior over the intermediate continuous vector.","1. What is the training objective of a generative model in the context of unsupervised dependency parsing? sent1
    1.1. Why can't the marginalized probability be calculated accurately for more complex models? sent2
    1.2. How does a variational autoencoder address the issue of calculating marginalized probability? sent3
        1.2.1. What role do different sampling approaches play in optimizing the objective function of a variational autoencoder? sent4
2. How many unsupervised dependency parsing models based on variational autoencoders have been proposed recently? sent5
3. What are the three probabilities involved in the Evidence Lower Bound (ELBO)? sent6
4. What is Recurrent Neural Network Grammars (RNNG) and what are its variants? sent7
    4.1. How does the discriminative RNNG construct the constituency tree of the input sentence? sent8
    4.2. How does the generative RNNG differ from the discriminative RNNG? sent9
        4.2.1. How is the probability of each operation in RNNG calculated? sent10
    4.3. How is RNNG modified for dependency parsing by Li et al. (2019)? sent11
        4.3.1. What problem does RNNG face in the unsupervised setting and how is it mitigated? sent12, sent13
5. What is the model proposed by Corro and Titov (2018) based on, and what is its primary design purpose? sent14, sent15
    5.1. What are the components of the encoder and decoder in Corro and Titov's model? sent16
    5.2. How do Corro and Titov (2018) address the complexity of sampling a dependency tree? sent17
        5.2.1. What alternative method is used by Jang et al. (2017) for efficient approximate sampling? sent18
6. How does the variational variant of D-NDMV differ from its deterministic variant? sent19
    6.1. What does the variational variant of D-NDMV specify over the intermediate continuous vector? sent20","1. What is the training objective of a generative model in the context of unsupervised dependency parsing? sent1
    1.1. Why can't the marginalized probability be calculated accurately for more complex models? sent2
    1.2. How does a variational autoencoder address the issue of calculating marginalized probability? sent3
        1.2.1. What role do different sampling approaches play in optimizing the objective function of a variational autoencoder? sent4

4. What is Recurrent Neural Network Grammars (RNNG) and what are its variants? sent7
    4.1. How does the discriminative RNNG construct the constituency tree of the input sentence? sent8
    4.2. How does the generative RNNG differ from the discriminative RNNG? sent9
        4.2.1. How is the probability of each operation in RNNG calculated? sent10
    4.3. How is RNNG modified for dependency parsing by Li et al. (2019)? sent11
        4.3.1. What problem does RNNG face in the unsupervised setting and how is it mitigated? sent12, sent13

5. What is the model proposed by Corro and Titov (2018) based on, and what is its primary design purpose? sent14, sent15
    5.1. What are the components of the encoder and decoder in Corro and Titov's model? sent16
    5.2. How do Corro and Titov (2018) address the complexity of sampling a dependency tree? sent17
        5.2.1. What alternative method is used by Jang et al. (2017) for efficient approximate sampling? sent18","1. What is the training objective of a generative model in the context of unsupervised dependency parsing? sent1
    1.1. Why can't the marginalized probability be calculated accurately for more complex models? sent2
    1.2. How does a variational autoencoder address the issue of calculating marginalized probability? sent3
        1.2.1. What role do different sampling approaches play in optimizing the objective function of a variational autoencoder? sent4","Question: How do variational autoencoders optimize the objective function in unsupervised dependency parsing models?

Supporting sentence: sent3, sent4",How do variational autoencoders optimize the objective function in unsupervised dependency parsing models?,"sent3, sent4",2,implement,"A variational autoencoder maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability.

Since the intermediate representation follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema.

There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.

Since the variational autoencoder needs Monte Carlo sampling to approximate the gradient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel random perturbation.

Jang et al. (2017) use differentiable dynamic programming to design an efficient approximate sampling algorithm.","Question:

How do variational autoencoders optimize the objective function in unsupervised dependency parsing models?

Answer:

A variational autoencoder maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability.

Since the intermediate representation follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema.

There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.

Since the variational autoencoder needs Monte Carlo sampling to approximate the gradient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel random perturbation.

Jang et al. (2017) use differentiable dynamic programming to design an efficient approximate sampling algorithm."
222133962,A Survey of Unsupervised Dependency Parsing,https://www.semanticscholar.org/paper/16160fbefc830117a8395a4b665c39feae834e48,Variational Autoencoder-Based Approaches,4,"As mentioned in Section 3.1, the training objective of a generative model is typically the probability of the training sentence and the dependency tree is marginalized as a hidden variable. However, the marginalized probability cannot usually be calculated accurately for more complex models that do not make strict independence assumption. Instead, a variational autoencoder maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability. Since the intermediate representation follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema.

Three unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1). There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.

Recurrent Neural Network Grammars (RNNG) ) is a transition-based constituent parser, with a discriminative and a generative variant. Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing. Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence. The probability of each operation is calculated by a neural network. Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNNG as the encoder and decoder of a variational autoencoder respectively. However, because RNNG has a strong expressive power, it is prone to overfitting in the unsupervised setting. Li et al. (2019) propose to use posterior regularization to introduce linguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent.

The model proposed by Corro and Titov (2018) is also based on a variational autoencoder. It is designed for semi-supervised dependency parsing, but in principle it can also be applied for unsupervised dependency parsing. The encoder of this model is a conditional random field model while the decoder generates a sentence based on a graph convolutional neural network whose structure is specified by the dependency tree. Since the variational autoencoder needs Monte Carlo sampling to approximate the gradient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel random perturbation. Jang et al. (2017) use differentiable dynamic programming to design an efficient approximate sampling algorithm.

The variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also specifies a Gaussian prior over the intermediate continuous vector.","sent1: As mentioned in Section 3.1, the training objective of a generative model is typically the probability of the training sentence and the dependency tree is marginalized as a hidden variable.
sent2: However, the marginalized probability cannot usually be calculated accurately for more complex models that do not make strict independence assumption.
sent3: Instead, a variational autoencoder maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability.
sent4: Since the intermediate representation follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema.
sent5: Three unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1).
sent6: There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.
sent7: Recurrent Neural Network Grammars (RNNG) ) is a transition-based constituent parser, with a discriminative and a generative variant.
sent8: Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing.
sent9: Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence.
sent10: The probability of each operation is calculated by a neural network.
sent11: Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNNG as the encoder and decoder of a variational autoencoder respectively.
sent12: However, because RNNG has a strong expressive power, it is prone to overfitting in the unsupervised setting.
sent13: Li et al. (2019) propose to use posterior regularization to introduce linguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent.
sent14: The model proposed by Corro and Titov (2018) is also based on a variational autoencoder.
sent15: It is designed for semi-supervised dependency parsing, but in principle it can also be applied for unsupervised dependency parsing.
sent16: The encoder of this model is a conditional random field model while the decoder generates a sentence based on a graph convolutional neural network whose structure is specified by the dependency tree.
sent17: Since the variational autoencoder needs Monte Carlo sampling to approximate the gradient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel random perturbation.
sent18: Jang et al. (2017) use differentiable dynamic programming to design an efficient approximate sampling algorithm.
sent19: The variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution.
sent20: It also specifies a Gaussian prior over the intermediate continuous vector.","1. What is the training objective of a generative model in the context of unsupervised dependency parsing? sent1
    1.1. Why can't the marginalized probability be calculated accurately for more complex models? sent2
    1.2. How does a variational autoencoder address the issue of calculating marginalized probability? sent3
        1.2.1. What role do different sampling approaches play in optimizing the objective function of a variational autoencoder? sent4
2. How many unsupervised dependency parsing models based on variational autoencoders have been proposed recently? sent5
3. What are the three probabilities involved in the Evidence Lower Bound (ELBO)? sent6
4. What is Recurrent Neural Network Grammars (RNNG) and what are its variants? sent7
    4.1. How does the discriminative RNNG construct the constituency tree of the input sentence? sent8
    4.2. How does the generative RNNG differ from the discriminative RNNG? sent9
        4.2.1. How is the probability of each operation in RNNG calculated? sent10
    4.3. How is RNNG modified for dependency parsing by Li et al. (2019)? sent11
        4.3.1. What problem does RNNG face in the unsupervised setting and how is it mitigated? sent12, sent13
5. What is the model proposed by Corro and Titov (2018) based on, and what is its primary design purpose? sent14, sent15
    5.1. What are the components of the encoder and decoder in Corro and Titov's model? sent16
    5.2. How do Corro and Titov (2018) address the complexity of sampling a dependency tree? sent17
        5.2.1. What alternative method is used by Jang et al. (2017) for efficient approximate sampling? sent18
6. How does the variational variant of D-NDMV differ from its deterministic variant? sent19
    6.1. What does the variational variant of D-NDMV specify over the intermediate continuous vector? sent20","1. What is the training objective of a generative model in the context of unsupervised dependency parsing? sent1
    1.1. Why can't the marginalized probability be calculated accurately for more complex models? sent2
    1.2. How does a variational autoencoder address the issue of calculating marginalized probability? sent3
        1.2.1. What role do different sampling approaches play in optimizing the objective function of a variational autoencoder? sent4

4. What is Recurrent Neural Network Grammars (RNNG) and what are its variants? sent7
    4.1. How does the discriminative RNNG construct the constituency tree of the input sentence? sent8
    4.2. How does the generative RNNG differ from the discriminative RNNG? sent9
        4.2.1. How is the probability of each operation in RNNG calculated? sent10
    4.3. How is RNNG modified for dependency parsing by Li et al. (2019)? sent11
        4.3.1. What problem does RNNG face in the unsupervised setting and how is it mitigated? sent12, sent13

5. What is the model proposed by Corro and Titov (2018) based on, and what is its primary design purpose? sent14, sent15
    5.1. What are the components of the encoder and decoder in Corro and Titov's model? sent16
    5.2. How do Corro and Titov (2018) address the complexity of sampling a dependency tree? sent17
        5.2.1. What alternative method is used by Jang et al. (2017) for efficient approximate sampling? sent18","4. What is Recurrent Neural Network Grammars (RNNG) and what are its variants? sent7
    4.1. How does the discriminative RNNG construct the constituency tree of the input sentence? sent8
    4.2. How does the generative RNNG differ from the discriminative RNNG? sent9
        4.2.1. How is the probability of each operation in RNNG calculated? sent10
    4.3. How is RNNG modified for dependency parsing by Li et al. (2019)? sent11
        4.3.1. What problem does RNNG face in the unsupervised setting and how is it mitigated? sent12, sent13","Question: How is Recurrent Neural Network Grammars (RNNG) adapted for dependency parsing, and what challenges does it face in unsupervised settings?

Supporting sentence: sent11, sent12, sent13","How is Recurrent Neural Network Grammars (RNNG) adapted for dependency parsing, and what challenges does it face in unsupervised settings?","sent11, sent12, sent13",2,implement,"Recurrent Neural Network Grammars (RNNG) is adapted for dependency parsing by modifying it to use discriminative RNNG and generative RNNG as the encoder and decoder of a variational autoencoder, respectively.

Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing.

Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence.

The probability of each operation is calculated by a neural network.

However, because RNNG has a strong expressive power, it is prone to overfitting in the unsupervised setting.

Li et al. (2019) propose to use posterior regularization to introduce linguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent.","Question:

How is Recurrent Neural Network Grammars (RNNG) adapted for dependency parsing, and what challenges does it face in unsupervised settings?

Answer:

Recurrent Neural Network Grammars (RNNG) is adapted for dependency parsing by modifying it to use discriminative RNNG and generative RNNG as the encoder and decoder of a variational autoencoder, respectively.

Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing.

Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence.

The probability of each operation is calculated by a neural network.

However, because RNNG has a strong expressive power, it is prone to overfitting in the unsupervised setting.

Li et al. (2019) propose to use posterior regularization to introduce linguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent."
222133962,A Survey of Unsupervised Dependency Parsing,https://www.semanticscholar.org/paper/16160fbefc830117a8395a4b665c39feae834e48,Variational Autoencoder-Based Approaches,4,"As mentioned in Section 3.1, the training objective of a generative model is typically the probability of the training sentence and the dependency tree is marginalized as a hidden variable. However, the marginalized probability cannot usually be calculated accurately for more complex models that do not make strict independence assumption. Instead, a variational autoencoder maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability. Since the intermediate representation follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema.

Three unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1). There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.

Recurrent Neural Network Grammars (RNNG) ) is a transition-based constituent parser, with a discriminative and a generative variant. Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing. Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence. The probability of each operation is calculated by a neural network. Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNNG as the encoder and decoder of a variational autoencoder respectively. However, because RNNG has a strong expressive power, it is prone to overfitting in the unsupervised setting. Li et al. (2019) propose to use posterior regularization to introduce linguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent.

The model proposed by Corro and Titov (2018) is also based on a variational autoencoder. It is designed for semi-supervised dependency parsing, but in principle it can also be applied for unsupervised dependency parsing. The encoder of this model is a conditional random field model while the decoder generates a sentence based on a graph convolutional neural network whose structure is specified by the dependency tree. Since the variational autoencoder needs Monte Carlo sampling to approximate the gradient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel random perturbation. Jang et al. (2017) use differentiable dynamic programming to design an efficient approximate sampling algorithm.

The variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also specifies a Gaussian prior over the intermediate continuous vector.","sent1: As mentioned in Section 3.1, the training objective of a generative model is typically the probability of the training sentence and the dependency tree is marginalized as a hidden variable.
sent2: However, the marginalized probability cannot usually be calculated accurately for more complex models that do not make strict independence assumption.
sent3: Instead, a variational autoencoder maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability.
sent4: Since the intermediate representation follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema.
sent5: Three unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1).
sent6: There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.
sent7: Recurrent Neural Network Grammars (RNNG) ) is a transition-based constituent parser, with a discriminative and a generative variant.
sent8: Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing.
sent9: Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence.
sent10: The probability of each operation is calculated by a neural network.
sent11: Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNNG as the encoder and decoder of a variational autoencoder respectively.
sent12: However, because RNNG has a strong expressive power, it is prone to overfitting in the unsupervised setting.
sent13: Li et al. (2019) propose to use posterior regularization to introduce linguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent.
sent14: The model proposed by Corro and Titov (2018) is also based on a variational autoencoder.
sent15: It is designed for semi-supervised dependency parsing, but in principle it can also be applied for unsupervised dependency parsing.
sent16: The encoder of this model is a conditional random field model while the decoder generates a sentence based on a graph convolutional neural network whose structure is specified by the dependency tree.
sent17: Since the variational autoencoder needs Monte Carlo sampling to approximate the gradient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel random perturbation.
sent18: Jang et al. (2017) use differentiable dynamic programming to design an efficient approximate sampling algorithm.
sent19: The variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution.
sent20: It also specifies a Gaussian prior over the intermediate continuous vector.","1. What is the training objective of a generative model in the context of unsupervised dependency parsing? sent1
    1.1. Why can't the marginalized probability be calculated accurately for more complex models? sent2
    1.2. How does a variational autoencoder address the issue of calculating marginalized probability? sent3
        1.2.1. What role do different sampling approaches play in optimizing the objective function of a variational autoencoder? sent4
2. How many unsupervised dependency parsing models based on variational autoencoders have been proposed recently? sent5
3. What are the three probabilities involved in the Evidence Lower Bound (ELBO)? sent6
4. What is Recurrent Neural Network Grammars (RNNG) and what are its variants? sent7
    4.1. How does the discriminative RNNG construct the constituency tree of the input sentence? sent8
    4.2. How does the generative RNNG differ from the discriminative RNNG? sent9
        4.2.1. How is the probability of each operation in RNNG calculated? sent10
    4.3. How is RNNG modified for dependency parsing by Li et al. (2019)? sent11
        4.3.1. What problem does RNNG face in the unsupervised setting and how is it mitigated? sent12, sent13
5. What is the model proposed by Corro and Titov (2018) based on, and what is its primary design purpose? sent14, sent15
    5.1. What are the components of the encoder and decoder in Corro and Titov's model? sent16
    5.2. How do Corro and Titov (2018) address the complexity of sampling a dependency tree? sent17
        5.2.1. What alternative method is used by Jang et al. (2017) for efficient approximate sampling? sent18
6. How does the variational variant of D-NDMV differ from its deterministic variant? sent19
    6.1. What does the variational variant of D-NDMV specify over the intermediate continuous vector? sent20","1. What is the training objective of a generative model in the context of unsupervised dependency parsing? sent1
    1.1. Why can't the marginalized probability be calculated accurately for more complex models? sent2
    1.2. How does a variational autoencoder address the issue of calculating marginalized probability? sent3
        1.2.1. What role do different sampling approaches play in optimizing the objective function of a variational autoencoder? sent4

4. What is Recurrent Neural Network Grammars (RNNG) and what are its variants? sent7
    4.1. How does the discriminative RNNG construct the constituency tree of the input sentence? sent8
    4.2. How does the generative RNNG differ from the discriminative RNNG? sent9
        4.2.1. How is the probability of each operation in RNNG calculated? sent10
    4.3. How is RNNG modified for dependency parsing by Li et al. (2019)? sent11
        4.3.1. What problem does RNNG face in the unsupervised setting and how is it mitigated? sent12, sent13

5. What is the model proposed by Corro and Titov (2018) based on, and what is its primary design purpose? sent14, sent15
    5.1. What are the components of the encoder and decoder in Corro and Titov's model? sent16
    5.2. How do Corro and Titov (2018) address the complexity of sampling a dependency tree? sent17
        5.2.1. What alternative method is used by Jang et al. (2017) for efficient approximate sampling? sent18","5. What is the model proposed by Corro and Titov (2018) based on, and what is its primary design purpose? sent14, sent15
    5.1. What are the components of the encoder and decoder in Corro and Titov's model? sent16
    5.2. How do Corro and Titov (2018) address the complexity of sampling a dependency tree? sent17
        5.2.1. What alternative method is used by Jang et al. (2017) for efficient approximate sampling? sent18","Question: How do Corro and Titov (2018) address the complexity of sampling a dependency tree in their model?

Supporting sentence: sent17, sent18",How do Corro and Titov (2018) address the complexity of sampling a dependency tree in their model?,"sent17, sent18",2,implement,Corro and Titov (2018) address the complexity of sampling a dependency tree in their model by using Gumbel random perturbation.,"Question:

How do Corro and Titov (2018) address the complexity of sampling a dependency tree in their model?

Answer:

Corro and Titov (2018) address the complexity of sampling a dependency tree in their model by using Gumbel random perturbation."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,Why graph neural networks ?,9,"Contemporary solutions to the task of ATS suffer from a number of issues, chiefly an inconsistent evaluation protocol and, somewhat, a lack of progress, as noted by Kryściński et al. (2019). In recent years GNNs have been successfully applied to a number of downstream NLP tasks such as classification    and translation (Xu et al., 2021) (Yin et al., 2020). Although GNNs may not be able to solve all problems related to the task of ATS, we believe that they can at least give a new perspective to this task. Generally GNNs bring a number of advantages to ATS which we believe to be significant enough to warrant further research, and this survey. In particular we want to highlight the following aspects of GNNs:

• Scalability and Flexibility. A vast number of ATS models are based on BERT (Devlin et al., 2019). However, the computational complexity of BERT-based ATS models grows quadratic with the input length; due to the selfattention operation. This fact renders them impractical for long, or even medium sized text documents. Recently some work has been done in order to circumvent this limiting factor (Ding et al., 2020) (Zhang et al., 2021). In contrast, GNNs can scale by their nature to graphs of thousands of nodes and more. This is in part due to the linear scaling of the memory cost with regards to the input size. The total memory cost of a GNN model depends on the size of the graph, the number of layers and the feature vector size of the nodes present. Formally, for L layers and an input of N nodes with each node's feature vector being of size H the memory complexity is O(LN H). But even for very large graphs on the scale of millions of nodes one can utilize GNNs. This can be achieved using methods such as neighbour sampling or distributing the graph over multiple GPUs, as done for example by Jia et al. (2020b). We recommend the paper by  for insights as to how one can train large and very deep GNNs. As the input of a GNN is a graph, the input can vary in size, therefore GNNs are also able to cope with changing text sizes and structures. Both of these aspects combined allow GNNs to produce summaries which are not restricted by hard-coded limits related to input or output size.

• Understanding and Explainability. It is often difficult to understand why a model arrived at a certain conclusion. Additionally it is often difficult to see how the model aggregates information. This is not the case with GNNs, as with the help of methods such as GNN Explainer (Ying et al., 2019) one can understand which nodes were used by the model to reach its output. This removes a layer of the blackbox magic present in many current non-GNN models. We recommend the survey by Yuan et al. (2020) for an overview of methods for generating explanations for GNNs.","sent1: Contemporary solutions to the task of ATS suffer from a number of issues, chiefly an inconsistent evaluation protocol and, somewhat, a lack of progress, as noted by Kryściński et al. (2019).
sent2: In recent years GNNs have been successfully applied to a number of downstream NLP tasks such as classification    and translation (Xu et al., 2021) (Yin et al., 2020).
sent3: Although GNNs may not be able to solve all problems related to the task of ATS, we believe that they can at least give a new perspective to this task.
sent4: Generally GNNs bring a number of advantages to ATS which we believe to be significant enough to warrant further research, and this survey.
sent5: In particular we want to highlight the following aspects of GNNs:• Scalability and Flexibility.
sent6: A vast number of ATS models are based on BERT (Devlin et al., 2019).
sent7: However, the computational complexity of BERT-based ATS models grows quadratic with the input length; due to the selfattention operation.
sent8: This fact renders them impractical for long, or even medium sized text documents.
sent9: Recently some work has been done in order to circumvent this limiting factor (Ding et al., 2020) (Zhang et al., 2021).
sent10: In contrast, GNNs can scale by their nature to graphs of thousands of nodes and more.
sent11: This is in part due to the linear scaling of the memory cost with regards to the input size.
sent12: The total memory cost of a GNN model depends on the size of the graph, the number of layers and the feature vector size of the nodes present.
sent13: Formally, for L layers and an input of N nodes with each node's feature vector being of size H the memory complexity is O(LN H).
sent14: But even for very large graphs on the scale of millions of nodes one can utilize GNNs.
sent15: This can be achieved using methods such as neighbour sampling or distributing the graph over multiple GPUs, as done for example by Jia et al. (2020b).
sent16: We recommend the paper by  for insights as to how one can train large and very deep GNNs.
sent17: As the input of a GNN is a graph, the input can vary in size, therefore GNNs are also able to cope with changing text sizes and structures.
sent18: Both of these aspects combined allow GNNs to produce summaries which are not restricted by hard-coded limits related to input or output size.
sent19: • Understanding and Explainability.
sent20: It is often difficult to understand why a model arrived at a certain conclusion.
sent21: Additionally it is often difficult to see how the model aggregates information.
sent22: This is not the case with GNNs, as with the help of methods such as GNN Explainer (Ying et al., 2019) one can understand which nodes were used by the model to reach its output.
sent23: This removes a layer of the blackbox magic present in many current non-GNN models.
sent24: We recommend the survey by Yuan et al. (2020) for an overview of methods for generating explanations for GNNs.","1. What issues do contemporary solutions to the task of ATS suffer from? sent1
2. In what NLP tasks have GNNs been successfully applied in recent years? sent2
3. Can GNNs solve all problems related to the task of ATS? sent3
4. What advantages do GNNs bring to ATS? sent4
    4.1. What specific aspects of GNNs are highlighted as significant for ATS? sent5
5. What is a common basis for a vast number of ATS models? sent6
6. What is the computational complexity issue with BERT-based ATS models? sent7
    6.1. Why are BERT-based ATS models impractical for certain text documents? sent8
    6.2. Has any work been done to address the limiting factor of BERT-based models? sent9
7. How do GNNs compare to BERT-based models in terms of scalability? sent10
    7.1. What contributes to the linear scaling of memory cost in GNNs? sent11
    7.2. What factors determine the total memory cost of a GNN model? sent12
    7.3. What is the formal memory complexity of a GNN model? sent13
    7.4. Can GNNs be utilized for very large graphs, and how? sent14, sent15
    7.5. What is recommended for insights on training large and deep GNNs? sent16
8. How do GNNs handle varying input sizes and structures? sent17
    8.1. What does this ability allow GNNs to do in terms of summaries? sent18
9. What is a challenge with understanding model conclusions in non-GNN models? sent19, sent20
    9.1. How do GNNs address the challenge of understanding model conclusions? sent21, sent22
    9.2. What does the use of GNNs remove in terms of model understanding? sent23
    9.3. What is recommended for an overview of methods for generating explanations for GNNs? sent24","7. How do GNNs compare to BERT-based models in terms of scalability? sent10
    7.1. What contributes to the linear scaling of memory cost in GNNs? sent11
    7.2. What factors determine the total memory cost of a GNN model? sent12
    7.3. What is the formal memory complexity of a GNN model? sent13
    7.4. Can GNNs be utilized for very large graphs, and how? sent14, sent15
    7.5. What is recommended for insights on training large and deep GNNs? sent16

9. What is a challenge with understanding model conclusions in non-GNN models? sent19, sent20
    9.1. How do GNNs address the challenge of understanding model conclusions? sent21, sent22
    9.2. What does the use of GNNs remove in terms of model understanding? sent23
    9.3. What is recommended for an overview of methods for generating explanations for GNNs? sent24","7. How do GNNs compare to BERT-based models in terms of scalability? sent10
    7.1. What contributes to the linear scaling of memory cost in GNNs? sent11
    7.2. What factors determine the total memory cost of a GNN model? sent12
    7.3. What is the formal memory complexity of a GNN model? sent13
    7.4. Can GNNs be utilized for very large graphs, and how? sent14, sent15
    7.5. What is recommended for insights on training large and deep GNNs? sent16","Question: How can GNNs handle very large graphs, and what methods facilitate this capability?

Supporting sentence: sent14, sent15","How can GNNs handle very large graphs, and what methods facilitate this capability?","sent14, sent15",2,implement,"GNNs can scale by their nature to graphs of thousands of nodes and more.

This is in part due to the linear scaling of the memory cost with regards to the input size.

The total memory cost of a GNN model depends on the size of the graph, the number of layers, and the feature vector size of the nodes present.

Formally, for L layers and an input of N nodes with each node's feature vector being of size H, the memory complexity is O(LNH).

Even for very large graphs on the scale of millions of nodes, one can utilize GNNs.

This can be achieved using methods such as neighbor sampling or distributing the graph over multiple GPUs, as done by Jia et al. (2020b).","Question:

How can GNNs handle very large graphs, and what methods facilitate this capability?

Answer:

GNNs can scale by their nature to graphs of thousands of nodes and more.

This is in part due to the linear scaling of the memory cost with regards to the input size.

The total memory cost of a GNN model depends on the size of the graph, the number of layers, and the feature vector size of the nodes present.

Formally, for L layers and an input of N nodes with each node's feature vector being of size H, the memory complexity is O(LNH).

Even for very large graphs on the scale of millions of nodes, one can utilize GNNs.

This can be achieved using methods such as neighbor sampling or distributing the graph over multiple GPUs, as done by Jia et al. (2020b)."
252819333,A Survey of Automatic Text Summarization using Graph Neural Networks,https://www.semanticscholar.org/paper/923a7db0522f53e7a01e1c7303d45fef0091fcb8,Why graph neural networks ?,9,"Contemporary solutions to the task of ATS suffer from a number of issues, chiefly an inconsistent evaluation protocol and, somewhat, a lack of progress, as noted by Kryściński et al. (2019). In recent years GNNs have been successfully applied to a number of downstream NLP tasks such as classification    and translation (Xu et al., 2021) (Yin et al., 2020). Although GNNs may not be able to solve all problems related to the task of ATS, we believe that they can at least give a new perspective to this task. Generally GNNs bring a number of advantages to ATS which we believe to be significant enough to warrant further research, and this survey. In particular we want to highlight the following aspects of GNNs:

• Scalability and Flexibility. A vast number of ATS models are based on BERT (Devlin et al., 2019). However, the computational complexity of BERT-based ATS models grows quadratic with the input length; due to the selfattention operation. This fact renders them impractical for long, or even medium sized text documents. Recently some work has been done in order to circumvent this limiting factor (Ding et al., 2020) (Zhang et al., 2021). In contrast, GNNs can scale by their nature to graphs of thousands of nodes and more. This is in part due to the linear scaling of the memory cost with regards to the input size. The total memory cost of a GNN model depends on the size of the graph, the number of layers and the feature vector size of the nodes present. Formally, for L layers and an input of N nodes with each node's feature vector being of size H the memory complexity is O(LN H). But even for very large graphs on the scale of millions of nodes one can utilize GNNs. This can be achieved using methods such as neighbour sampling or distributing the graph over multiple GPUs, as done for example by Jia et al. (2020b). We recommend the paper by  for insights as to how one can train large and very deep GNNs. As the input of a GNN is a graph, the input can vary in size, therefore GNNs are also able to cope with changing text sizes and structures. Both of these aspects combined allow GNNs to produce summaries which are not restricted by hard-coded limits related to input or output size.

• Understanding and Explainability. It is often difficult to understand why a model arrived at a certain conclusion. Additionally it is often difficult to see how the model aggregates information. This is not the case with GNNs, as with the help of methods such as GNN Explainer (Ying et al., 2019) one can understand which nodes were used by the model to reach its output. This removes a layer of the blackbox magic present in many current non-GNN models. We recommend the survey by Yuan et al. (2020) for an overview of methods for generating explanations for GNNs.","sent1: Contemporary solutions to the task of ATS suffer from a number of issues, chiefly an inconsistent evaluation protocol and, somewhat, a lack of progress, as noted by Kryściński et al. (2019).
sent2: In recent years GNNs have been successfully applied to a number of downstream NLP tasks such as classification    and translation (Xu et al., 2021) (Yin et al., 2020).
sent3: Although GNNs may not be able to solve all problems related to the task of ATS, we believe that they can at least give a new perspective to this task.
sent4: Generally GNNs bring a number of advantages to ATS which we believe to be significant enough to warrant further research, and this survey.
sent5: In particular we want to highlight the following aspects of GNNs:• Scalability and Flexibility.
sent6: A vast number of ATS models are based on BERT (Devlin et al., 2019).
sent7: However, the computational complexity of BERT-based ATS models grows quadratic with the input length; due to the selfattention operation.
sent8: This fact renders them impractical for long, or even medium sized text documents.
sent9: Recently some work has been done in order to circumvent this limiting factor (Ding et al., 2020) (Zhang et al., 2021).
sent10: In contrast, GNNs can scale by their nature to graphs of thousands of nodes and more.
sent11: This is in part due to the linear scaling of the memory cost with regards to the input size.
sent12: The total memory cost of a GNN model depends on the size of the graph, the number of layers and the feature vector size of the nodes present.
sent13: Formally, for L layers and an input of N nodes with each node's feature vector being of size H the memory complexity is O(LN H).
sent14: But even for very large graphs on the scale of millions of nodes one can utilize GNNs.
sent15: This can be achieved using methods such as neighbour sampling or distributing the graph over multiple GPUs, as done for example by Jia et al. (2020b).
sent16: We recommend the paper by  for insights as to how one can train large and very deep GNNs.
sent17: As the input of a GNN is a graph, the input can vary in size, therefore GNNs are also able to cope with changing text sizes and structures.
sent18: Both of these aspects combined allow GNNs to produce summaries which are not restricted by hard-coded limits related to input or output size.
sent19: • Understanding and Explainability.
sent20: It is often difficult to understand why a model arrived at a certain conclusion.
sent21: Additionally it is often difficult to see how the model aggregates information.
sent22: This is not the case with GNNs, as with the help of methods such as GNN Explainer (Ying et al., 2019) one can understand which nodes were used by the model to reach its output.
sent23: This removes a layer of the blackbox magic present in many current non-GNN models.
sent24: We recommend the survey by Yuan et al. (2020) for an overview of methods for generating explanations for GNNs.","1. What issues do contemporary solutions to the task of ATS suffer from? sent1
2. In what NLP tasks have GNNs been successfully applied in recent years? sent2
3. Can GNNs solve all problems related to the task of ATS? sent3
4. What advantages do GNNs bring to ATS? sent4
    4.1. What specific aspects of GNNs are highlighted as significant for ATS? sent5
5. What is a common basis for a vast number of ATS models? sent6
6. What is the computational complexity issue with BERT-based ATS models? sent7
    6.1. Why are BERT-based ATS models impractical for certain text documents? sent8
    6.2. Has any work been done to address the limiting factor of BERT-based models? sent9
7. How do GNNs compare to BERT-based models in terms of scalability? sent10
    7.1. What contributes to the linear scaling of memory cost in GNNs? sent11
    7.2. What factors determine the total memory cost of a GNN model? sent12
    7.3. What is the formal memory complexity of a GNN model? sent13
    7.4. Can GNNs be utilized for very large graphs, and how? sent14, sent15
    7.5. What is recommended for insights on training large and deep GNNs? sent16
8. How do GNNs handle varying input sizes and structures? sent17
    8.1. What does this ability allow GNNs to do in terms of summaries? sent18
9. What is a challenge with understanding model conclusions in non-GNN models? sent19, sent20
    9.1. How do GNNs address the challenge of understanding model conclusions? sent21, sent22
    9.2. What does the use of GNNs remove in terms of model understanding? sent23
    9.3. What is recommended for an overview of methods for generating explanations for GNNs? sent24","7. How do GNNs compare to BERT-based models in terms of scalability? sent10
    7.1. What contributes to the linear scaling of memory cost in GNNs? sent11
    7.2. What factors determine the total memory cost of a GNN model? sent12
    7.3. What is the formal memory complexity of a GNN model? sent13
    7.4. Can GNNs be utilized for very large graphs, and how? sent14, sent15
    7.5. What is recommended for insights on training large and deep GNNs? sent16

9. What is a challenge with understanding model conclusions in non-GNN models? sent19, sent20
    9.1. How do GNNs address the challenge of understanding model conclusions? sent21, sent22
    9.2. What does the use of GNNs remove in terms of model understanding? sent23
    9.3. What is recommended for an overview of methods for generating explanations for GNNs? sent24","9. What is a challenge with understanding model conclusions in non-GNN models? sent19, sent20
    9.1. How do GNNs address the challenge of understanding model conclusions? sent21, sent22
    9.2. What does the use of GNNs remove in terms of model understanding? sent23
    9.3. What is recommended for an overview of methods for generating explanations for GNNs? sent24","Question: How do GNNs improve the understanding and explainability of model conclusions in automatic text summarization?

Supporting sentence: sent19, sent20, sent21, sent22, sent23",How do GNNs improve the understanding and explainability of model conclusions in automatic text summarization?,"sent19, sent20, sent21, sent22, sent23",4,implement,"It is often difficult to understand why a model arrived at a certain conclusion.

Additionally, it is often difficult to see how the model aggregates information.

This is not the case with GNNs, as with the help of methods such as GNN Explainer (Ying et al., 2019), one can understand which nodes were used by the model to reach its output.

This removes a layer of the blackbox magic present in many current non-GNN models.","Question:

How do GNNs improve the understanding and explainability of model conclusions in automatic text summarization?

Answer:

It is often difficult to understand why a model arrived at a certain conclusion.

Additionally, it is often difficult to see how the model aggregates information.

This is not the case with GNNs, as with the help of methods such as GNN Explainer (Ying et al., 2019), one can understand which nodes were used by the model to reach its output.

This removes a layer of the blackbox magic present in many current non-GNN models."
251719280,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,https://www.semanticscholar.org/paper/a814b76e589ef27e3f4af379d319d02d2110faa1,Graph-based Methods,12,"Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures. As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing. As a result, modeling DB schema receives little attention. Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas. Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure. In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation. RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.

Graphs have also been used to encode questions together with DB schema. Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema. Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently. SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking. Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance. To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.

Finally, graph-based techniques are also exploited in context-dependent text-to-SQL. For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.

Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.

RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns. DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.

Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task. PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas. The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021). Other methods adjust the embeddings by PLMs. On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings. Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.

HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on. After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction. Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question. They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question. Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns. By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.

Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task. For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT. Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered. GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.","sent1: Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures.
sent2: As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing.
sent3: As a result, modeling DB schema receives little attention.
sent4: Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas.
sent5: Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure.
sent6: In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation.
sent7: RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.
sent8: Graphs have also been used to encode questions together with DB schema.
sent9: Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema.
sent10: Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently.
sent11: SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking.
sent12: Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance.
sent13: To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.
sent14: Finally, graph-based techniques are also exploited in context-dependent text-to-SQL.
sent15: For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.
sent16: Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.
sent17: RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns.
sent18: DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.
sent19: Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task.
sent20: PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas.
sent21: The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021).
sent22: Other methods adjust the embeddings by PLMs.
sent23: On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings.
sent24: Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.
sent25: HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on.
sent26: After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction.
sent27: Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question.
sent28: They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question.
sent29: Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns.
sent30: By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.
sent31: Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task.
sent32: For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT.
sent33: Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered.
sent34: GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.","1. Why are graph-based methods used in modeling DB schemas? sent1
2. What was the focus of datasets prior to Spider, and how did it affect DB schema modeling? sent2, sent3
3. How does Spider differ from previous datasets, and what approach did Bogin et al. (2019a) propose for DB schemas? sent4
    3.1. How do Bogin et al. (2019a) represent and encode DB schemas using graphs? sent5
    3.2. What advancements did Bogin et al. (2019b) make in capturing DB structures? sent6
4. How does RAT-SQL enhance the encoding of DB schemas? sent7
5. How are graphs used to encode questions along with DB schemas? sent8
6. What types of graphs are used to capture semantics in natural language and facilitate linking with table schema? sent9
    6.1. How does Cao et al. (2021) utilize line graphs for multi-hop semantics? sent10
    6.2. How does SADGA use graph structures for question-schema linking? sent11
    6.3. How does S 2 SQL integrate syntax dependency into graphs? sent12
7. How does ShawdowGNN improve generalization for unseen domains? sent13
8. How are graph-based techniques applied in context-dependent text-to-SQL? sent14
    8.1. How does IGSQL utilize historical information of DB schemas? sent15
9. What role does self-attention play in graph-based methods for text-to-SQL? sent16
    9.1. How does RAT-SQL apply relation-aware self-attention? sent17
    9.2. How does DuoRAT utilize relation-aware self-attention? sent18
10. How are pre-trained language models (PLMs) adapted for text-to-SQL tasks? sent19
    10.1. How are PLMs like BERT used to encode questions and DB schemas? sent20, sent21
    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30
11. What are some pre-training objectives and data used to align transformer-based encoders with text-to-SQL tasks? sent31
    11.1. How does TaBERT use tabular data for pre-training? sent32
    11.2. How does Grappa synthesize question-SQL pairs for pre-training? sent33
    11.3. How does GAP pre-train BART for text-to-SQL tasks? sent34","6. What types of graphs are used to capture semantics in natural language and facilitate linking with table schema? sent9
    6.1. How does Cao et al. (2021) utilize line graphs for multi-hop semantics? sent10
    6.2. How does SADGA use graph structures for question-schema linking? sent11
    6.3. How does S 2 SQL integrate syntax dependency into graphs? sent12

10. How are pre-trained language models (PLMs) adapted for text-to-SQL tasks? sent19
    10.1. How are PLMs like BERT used to encode questions and DB schemas? sent20, sent21
    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

11. What are some pre-training objectives and data used to align transformer-based encoders with text-to-SQL tasks? sent31
    11.1. How does TaBERT use tabular data for pre-training? sent32
    11.2. How does Grappa synthesize question-SQL pairs for pre-training? sent33
    11.3. How does GAP pre-train BART for text-to-SQL tasks? sent34","6. What types of graphs are used to capture semantics in natural language and facilitate linking with table schema? sent9
    6.1. How does Cao et al. (2021) utilize line graphs for multi-hop semantics? sent10
    6.2. How does SADGA use graph structures for question-schema linking? sent11
    6.3. How does S 2 SQL integrate syntax dependency into graphs? sent12","Question: How do different graph-based methods enhance question-schema linking in text-to-SQL tasks?

Supporting sentence: sent9, sent10, sent11, sent12",How do different graph-based methods enhance question-schema linking in text-to-SQL tasks?,"sent9, sent10, sent11, sent12",3,implement,"Graph-based methods are used to better encode the structures of DB schemas, which contain rich structural information.

Bogin et al. (2019a) propose using graphs to represent the structure of DB schemas, with nodes representing tables and columns, and edges representing relationships such as primary key and foreign key constraints.

Graph neural networks (GNNs) are then used to encode the graph structure.

In subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select relevant DB information for SQL generation.

RAT-SQL encodes more relationships for DB schemas, such as ""both columns are from the same table,"" in their graph.

Graphs have also been used to encode questions together with DB schema, facilitating linking between natural language (NL) and table schema.

Cao et al. (2021) adopt line graphs to capture multi-hop semantics by meta-path and distinguish between local and nonlocal neighbors, allowing different tables and columns to be attended differently.

SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking.

S2SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance.

ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.

Graph-based techniques are also exploited in context-dependent text-to-SQL, such as IGSQL (Cai and Wan, 2020), which uses a graph encoder to utilize historical information of DB schemas in previous turns.","Question:

How do different graph-based methods enhance question-schema linking in text-to-SQL tasks?

Answer:

Graph-based methods are used to better encode the structures of DB schemas, which contain rich structural information.

Bogin et al. (2019a) propose using graphs to represent the structure of DB schemas, with nodes representing tables and columns, and edges representing relationships such as primary key and foreign key constraints.

Graph neural networks (GNNs) are then used to encode the graph structure.

In subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select relevant DB information for SQL generation.

RAT-SQL encodes more relationships for DB schemas, such as ""both columns are from the same table,"" in their graph.

Graphs have also been used to encode questions together with DB schema, facilitating linking between natural language (NL) and table schema.

Cao et al. (2021) adopt line graphs to capture multi-hop semantics by meta-path and distinguish between local and nonlocal neighbors, allowing different tables and columns to be attended differently.

SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking.

S2SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance.

ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.

Graph-based techniques are also exploited in context-dependent text-to-SQL, such as IGSQL (Cai and Wan, 2020), which uses a graph encoder to utilize historical information of DB schemas in previous turns."
251719280,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,https://www.semanticscholar.org/paper/a814b76e589ef27e3f4af379d319d02d2110faa1,Graph-based Methods,12,"Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures. As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing. As a result, modeling DB schema receives little attention. Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas. Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure. In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation. RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.

Graphs have also been used to encode questions together with DB schema. Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema. Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently. SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking. Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance. To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.

Finally, graph-based techniques are also exploited in context-dependent text-to-SQL. For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.

Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.

RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns. DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.

Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task. PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas. The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021). Other methods adjust the embeddings by PLMs. On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings. Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.

HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on. After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction. Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question. They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question. Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns. By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.

Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task. For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT. Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered. GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.","sent1: Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures.
sent2: As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing.
sent3: As a result, modeling DB schema receives little attention.
sent4: Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas.
sent5: Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure.
sent6: In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation.
sent7: RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.
sent8: Graphs have also been used to encode questions together with DB schema.
sent9: Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema.
sent10: Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently.
sent11: SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking.
sent12: Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance.
sent13: To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.
sent14: Finally, graph-based techniques are also exploited in context-dependent text-to-SQL.
sent15: For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.
sent16: Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.
sent17: RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns.
sent18: DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.
sent19: Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task.
sent20: PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas.
sent21: The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021).
sent22: Other methods adjust the embeddings by PLMs.
sent23: On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings.
sent24: Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.
sent25: HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on.
sent26: After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction.
sent27: Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question.
sent28: They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question.
sent29: Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns.
sent30: By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.
sent31: Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task.
sent32: For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT.
sent33: Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered.
sent34: GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.","1. Why are graph-based methods used in modeling DB schemas? sent1
2. What was the focus of datasets prior to Spider, and how did it affect DB schema modeling? sent2, sent3
3. How does Spider differ from previous datasets, and what approach did Bogin et al. (2019a) propose for DB schemas? sent4
    3.1. How do Bogin et al. (2019a) represent and encode DB schemas using graphs? sent5
    3.2. What advancements did Bogin et al. (2019b) make in capturing DB structures? sent6
4. How does RAT-SQL enhance the encoding of DB schemas? sent7
5. How are graphs used to encode questions along with DB schemas? sent8
6. What types of graphs are used to capture semantics in natural language and facilitate linking with table schema? sent9
    6.1. How does Cao et al. (2021) utilize line graphs for multi-hop semantics? sent10
    6.2. How does SADGA use graph structures for question-schema linking? sent11
    6.3. How does S 2 SQL integrate syntax dependency into graphs? sent12
7. How does ShawdowGNN improve generalization for unseen domains? sent13
8. How are graph-based techniques applied in context-dependent text-to-SQL? sent14
    8.1. How does IGSQL utilize historical information of DB schemas? sent15
9. What role does self-attention play in graph-based methods for text-to-SQL? sent16
    9.1. How does RAT-SQL apply relation-aware self-attention? sent17
    9.2. How does DuoRAT utilize relation-aware self-attention? sent18
10. How are pre-trained language models (PLMs) adapted for text-to-SQL tasks? sent19
    10.1. How are PLMs like BERT used to encode questions and DB schemas? sent20, sent21
    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30
11. What are some pre-training objectives and data used to align transformer-based encoders with text-to-SQL tasks? sent31
    11.1. How does TaBERT use tabular data for pre-training? sent32
    11.2. How does Grappa synthesize question-SQL pairs for pre-training? sent33
    11.3. How does GAP pre-train BART for text-to-SQL tasks? sent34","6. What types of graphs are used to capture semantics in natural language and facilitate linking with table schema? sent9
    6.1. How does Cao et al. (2021) utilize line graphs for multi-hop semantics? sent10
    6.2. How does SADGA use graph structures for question-schema linking? sent11
    6.3. How does S 2 SQL integrate syntax dependency into graphs? sent12

10. How are pre-trained language models (PLMs) adapted for text-to-SQL tasks? sent19
    10.1. How are PLMs like BERT used to encode questions and DB schemas? sent20, sent21
    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

11. What are some pre-training objectives and data used to align transformer-based encoders with text-to-SQL tasks? sent31
    11.1. How does TaBERT use tabular data for pre-training? sent32
    11.2. How does Grappa synthesize question-SQL pairs for pre-training? sent33
    11.3. How does GAP pre-train BART for text-to-SQL tasks? sent34","10. How are pre-trained language models (PLMs) adapted for text-to-SQL tasks? sent19
    10.1. How are PLMs like BERT used to encode questions and DB schemas? sent20, sent21
    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30","Question: How do Liu et al. (2021b) enhance the alignment of PLMs with text-to-SQL tasks using an auxiliary concept prediction module?

Supporting sentence: sent27, sent28, sent29, sent30",How do Liu et al. (2021b) enhance the alignment of PLMs with text-to-SQL tasks using an auxiliary concept prediction module?,"sent27, sent28, sent29, sent30",1,implement,"Liu et al. (2021b) enhance the alignment of PLMs with text-to-SQL tasks by training an auxiliary concept prediction module to predict which tables and columns correspond to the question.

They detect important question tokens by identifying the largest drop in the confidence score caused by erasing that token in the question.

Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns.

By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-and-predict technique.","Question:

How do Liu et al. (2021b) enhance the alignment of PLMs with text-to-SQL tasks using an auxiliary concept prediction module?

Answer:

Liu et al. (2021b) enhance the alignment of PLMs with text-to-SQL tasks by training an auxiliary concept prediction module to predict which tables and columns correspond to the question.

They detect important question tokens by identifying the largest drop in the confidence score caused by erasing that token in the question.

Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns.

By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-and-predict technique."
251719280,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,https://www.semanticscholar.org/paper/a814b76e589ef27e3f4af379d319d02d2110faa1,Graph-based Methods,12,"Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures. As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing. As a result, modeling DB schema receives little attention. Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas. Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure. In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation. RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.

Graphs have also been used to encode questions together with DB schema. Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema. Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently. SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking. Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance. To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.

Finally, graph-based techniques are also exploited in context-dependent text-to-SQL. For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.

Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.

RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns. DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.

Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task. PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas. The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021). Other methods adjust the embeddings by PLMs. On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings. Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.

HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on. After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction. Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question. They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question. Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns. By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.

Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task. For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT. Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered. GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.","sent1: Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures.
sent2: As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing.
sent3: As a result, modeling DB schema receives little attention.
sent4: Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas.
sent5: Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure.
sent6: In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation.
sent7: RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.
sent8: Graphs have also been used to encode questions together with DB schema.
sent9: Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema.
sent10: Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently.
sent11: SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking.
sent12: Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance.
sent13: To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.
sent14: Finally, graph-based techniques are also exploited in context-dependent text-to-SQL.
sent15: For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.
sent16: Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.
sent17: RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns.
sent18: DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.
sent19: Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task.
sent20: PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas.
sent21: The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021).
sent22: Other methods adjust the embeddings by PLMs.
sent23: On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings.
sent24: Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.
sent25: HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on.
sent26: After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction.
sent27: Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question.
sent28: They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question.
sent29: Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns.
sent30: By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.
sent31: Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task.
sent32: For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT.
sent33: Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered.
sent34: GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.","1. Why are graph-based methods used in modeling DB schemas? sent1
2. What was the focus of datasets prior to Spider, and how did it affect DB schema modeling? sent2, sent3
3. How does Spider differ from previous datasets, and what approach did Bogin et al. (2019a) propose for DB schemas? sent4
    3.1. How do Bogin et al. (2019a) represent and encode DB schemas using graphs? sent5
    3.2. What advancements did Bogin et al. (2019b) make in capturing DB structures? sent6
4. How does RAT-SQL enhance the encoding of DB schemas? sent7
5. How are graphs used to encode questions along with DB schemas? sent8
6. What types of graphs are used to capture semantics in natural language and facilitate linking with table schema? sent9
    6.1. How does Cao et al. (2021) utilize line graphs for multi-hop semantics? sent10
    6.2. How does SADGA use graph structures for question-schema linking? sent11
    6.3. How does S 2 SQL integrate syntax dependency into graphs? sent12
7. How does ShawdowGNN improve generalization for unseen domains? sent13
8. How are graph-based techniques applied in context-dependent text-to-SQL? sent14
    8.1. How does IGSQL utilize historical information of DB schemas? sent15
9. What role does self-attention play in graph-based methods for text-to-SQL? sent16
    9.1. How does RAT-SQL apply relation-aware self-attention? sent17
    9.2. How does DuoRAT utilize relation-aware self-attention? sent18
10. How are pre-trained language models (PLMs) adapted for text-to-SQL tasks? sent19
    10.1. How are PLMs like BERT used to encode questions and DB schemas? sent20, sent21
    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30
11. What are some pre-training objectives and data used to align transformer-based encoders with text-to-SQL tasks? sent31
    11.1. How does TaBERT use tabular data for pre-training? sent32
    11.2. How does Grappa synthesize question-SQL pairs for pre-training? sent33
    11.3. How does GAP pre-train BART for text-to-SQL tasks? sent34","6. What types of graphs are used to capture semantics in natural language and facilitate linking with table schema? sent9
    6.1. How does Cao et al. (2021) utilize line graphs for multi-hop semantics? sent10
    6.2. How does SADGA use graph structures for question-schema linking? sent11
    6.3. How does S 2 SQL integrate syntax dependency into graphs? sent12

10. How are pre-trained language models (PLMs) adapted for text-to-SQL tasks? sent19
    10.1. How are PLMs like BERT used to encode questions and DB schemas? sent20, sent21
    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

11. What are some pre-training objectives and data used to align transformer-based encoders with text-to-SQL tasks? sent31
    11.1. How does TaBERT use tabular data for pre-training? sent32
    11.2. How does Grappa synthesize question-SQL pairs for pre-training? sent33
    11.3. How does GAP pre-train BART for text-to-SQL tasks? sent34","    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30","Question: How do Liu et al. (2021b) enhance the alignment of pre-trained language models with text-to-SQL tasks?

Supporting sentence: sent27, sent28, sent29, sent30",How do Liu et al. (2021b) enhance the alignment of pre-trained language models with text-to-SQL tasks?,"sent27, sent28, sent29, sent30",1,implement,"Liu et al. (2021b) enhance the alignment of pre-trained language models with text-to-SQL tasks by training an auxiliary concept prediction module to predict which tables and columns correspond to the question.

They detect important question tokens by identifying the largest drop in the confidence score caused by erasing that token in the question.

Lastly, they train the pre-trained language model with a grounding module using the question tokens and the corresponding tables as well as columns.

By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from the pre-trained language model via this erase-and-predict technique.","Question:

How do Liu et al. (2021b) enhance the alignment of pre-trained language models with text-to-SQL tasks?

Answer:

Liu et al. (2021b) enhance the alignment of pre-trained language models with text-to-SQL tasks by training an auxiliary concept prediction module to predict which tables and columns correspond to the question.

They detect important question tokens by identifying the largest drop in the confidence score caused by erasing that token in the question.

Lastly, they train the pre-trained language model with a grounding module using the question tokens and the corresponding tables as well as columns.

By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from the pre-trained language model via this erase-and-predict technique."
251719280,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,https://www.semanticscholar.org/paper/a814b76e589ef27e3f4af379d319d02d2110faa1,Graph-based Methods,12,"Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures. As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing. As a result, modeling DB schema receives little attention. Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas. Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure. In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation. RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.

Graphs have also been used to encode questions together with DB schema. Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema. Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently. SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking. Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance. To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.

Finally, graph-based techniques are also exploited in context-dependent text-to-SQL. For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.

Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.

RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns. DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.

Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task. PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas. The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021). Other methods adjust the embeddings by PLMs. On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings. Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.

HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on. After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction. Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question. They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question. Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns. By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.

Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task. For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT. Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered. GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.","sent1: Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures.
sent2: As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing.
sent3: As a result, modeling DB schema receives little attention.
sent4: Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas.
sent5: Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure.
sent6: In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation.
sent7: RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.
sent8: Graphs have also been used to encode questions together with DB schema.
sent9: Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema.
sent10: Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently.
sent11: SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking.
sent12: Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance.
sent13: To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.
sent14: Finally, graph-based techniques are also exploited in context-dependent text-to-SQL.
sent15: For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.
sent16: Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.
sent17: RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns.
sent18: DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.
sent19: Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task.
sent20: PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas.
sent21: The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021).
sent22: Other methods adjust the embeddings by PLMs.
sent23: On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings.
sent24: Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.
sent25: HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on.
sent26: After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction.
sent27: Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question.
sent28: They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question.
sent29: Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns.
sent30: By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.
sent31: Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task.
sent32: For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT.
sent33: Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered.
sent34: GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.","1. Why are graph-based methods used in modeling DB schemas? sent1
2. What was the focus of datasets prior to Spider, and how did it affect DB schema modeling? sent2, sent3
3. How does Spider differ from previous datasets, and what approach did Bogin et al. (2019a) propose for DB schemas? sent4
    3.1. How do Bogin et al. (2019a) represent and encode DB schemas using graphs? sent5
    3.2. What advancements did Bogin et al. (2019b) make in capturing DB structures? sent6
4. How does RAT-SQL enhance the encoding of DB schemas? sent7
5. How are graphs used to encode questions along with DB schemas? sent8
6. What types of graphs are used to capture semantics in natural language and facilitate linking with table schema? sent9
    6.1. How does Cao et al. (2021) utilize line graphs for multi-hop semantics? sent10
    6.2. How does SADGA use graph structures for question-schema linking? sent11
    6.3. How does S 2 SQL integrate syntax dependency into graphs? sent12
7. How does ShawdowGNN improve generalization for unseen domains? sent13
8. How are graph-based techniques applied in context-dependent text-to-SQL? sent14
    8.1. How does IGSQL utilize historical information of DB schemas? sent15
9. What role does self-attention play in graph-based methods for text-to-SQL? sent16
    9.1. How does RAT-SQL apply relation-aware self-attention? sent17
    9.2. How does DuoRAT utilize relation-aware self-attention? sent18
10. How are pre-trained language models (PLMs) adapted for text-to-SQL tasks? sent19
    10.1. How are PLMs like BERT used to encode questions and DB schemas? sent20, sent21
    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30
11. What are some pre-training objectives and data used to align transformer-based encoders with text-to-SQL tasks? sent31
    11.1. How does TaBERT use tabular data for pre-training? sent32
    11.2. How does Grappa synthesize question-SQL pairs for pre-training? sent33
    11.3. How does GAP pre-train BART for text-to-SQL tasks? sent34","6. What types of graphs are used to capture semantics in natural language and facilitate linking with table schema? sent9
    6.1. How does Cao et al. (2021) utilize line graphs for multi-hop semantics? sent10
    6.2. How does SADGA use graph structures for question-schema linking? sent11
    6.3. How does S 2 SQL integrate syntax dependency into graphs? sent12

10. How are pre-trained language models (PLMs) adapted for text-to-SQL tasks? sent19
    10.1. How are PLMs like BERT used to encode questions and DB schemas? sent20, sent21
    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

11. What are some pre-training objectives and data used to align transformer-based encoders with text-to-SQL tasks? sent31
    11.1. How does TaBERT use tabular data for pre-training? sent32
    11.2. How does Grappa synthesize question-SQL pairs for pre-training? sent33
    11.3. How does GAP pre-train BART for text-to-SQL tasks? sent34","        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30","Question: How do Liu et al. (2021b) utilize an erase-and-predict technique to enhance PLM grounding in text-to-SQL tasks?

Supporting sentence: sent27, sent28, sent29, sent30",How do Liu et al. (2021b) utilize an erase-and-predict technique to enhance PLM grounding in text-to-SQL tasks?,"sent27, sent28, sent29, sent30",1,implement,"Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question.

They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question.

Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns.

By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-and-predict technique.","Question:

How do Liu et al. (2021b) utilize an erase-and-predict technique to enhance PLM grounding in text-to-SQL tasks?

Answer:

Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question.

They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question.

Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns.

By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-and-predict technique."
251719280,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,https://www.semanticscholar.org/paper/a814b76e589ef27e3f4af379d319d02d2110faa1,Graph-based Methods,12,"Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures. As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing. As a result, modeling DB schema receives little attention. Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas. Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure. In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation. RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.

Graphs have also been used to encode questions together with DB schema. Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema. Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently. SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking. Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance. To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.

Finally, graph-based techniques are also exploited in context-dependent text-to-SQL. For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.

Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.

RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns. DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.

Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task. PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas. The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021). Other methods adjust the embeddings by PLMs. On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings. Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.

HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on. After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction. Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question. They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question. Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns. By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.

Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task. For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT. Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered. GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.","sent1: Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures.
sent2: As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing.
sent3: As a result, modeling DB schema receives little attention.
sent4: Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas.
sent5: Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure.
sent6: In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation.
sent7: RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.
sent8: Graphs have also been used to encode questions together with DB schema.
sent9: Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema.
sent10: Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently.
sent11: SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking.
sent12: Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance.
sent13: To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.
sent14: Finally, graph-based techniques are also exploited in context-dependent text-to-SQL.
sent15: For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.
sent16: Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.
sent17: RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns.
sent18: DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.
sent19: Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task.
sent20: PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas.
sent21: The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021).
sent22: Other methods adjust the embeddings by PLMs.
sent23: On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings.
sent24: Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.
sent25: HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on.
sent26: After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction.
sent27: Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question.
sent28: They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question.
sent29: Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns.
sent30: By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.
sent31: Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task.
sent32: For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT.
sent33: Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered.
sent34: GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.","1. Why are graph-based methods used in modeling DB schemas? sent1
2. What was the focus of datasets prior to Spider, and how did it affect DB schema modeling? sent2, sent3
3. How does Spider differ from previous datasets, and what approach did Bogin et al. (2019a) propose for DB schemas? sent4
    3.1. How do Bogin et al. (2019a) represent and encode DB schemas using graphs? sent5
    3.2. What advancements did Bogin et al. (2019b) make in capturing DB structures? sent6
4. How does RAT-SQL enhance the encoding of DB schemas? sent7
5. How are graphs used to encode questions along with DB schemas? sent8
6. What types of graphs are used to capture semantics in natural language and facilitate linking with table schema? sent9
    6.1. How does Cao et al. (2021) utilize line graphs for multi-hop semantics? sent10
    6.2. How does SADGA use graph structures for question-schema linking? sent11
    6.3. How does S 2 SQL integrate syntax dependency into graphs? sent12
7. How does ShawdowGNN improve generalization for unseen domains? sent13
8. How are graph-based techniques applied in context-dependent text-to-SQL? sent14
    8.1. How does IGSQL utilize historical information of DB schemas? sent15
9. What role does self-attention play in graph-based methods for text-to-SQL? sent16
    9.1. How does RAT-SQL apply relation-aware self-attention? sent17
    9.2. How does DuoRAT utilize relation-aware self-attention? sent18
10. How are pre-trained language models (PLMs) adapted for text-to-SQL tasks? sent19
    10.1. How are PLMs like BERT used to encode questions and DB schemas? sent20, sent21
    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30
11. What are some pre-training objectives and data used to align transformer-based encoders with text-to-SQL tasks? sent31
    11.1. How does TaBERT use tabular data for pre-training? sent32
    11.2. How does Grappa synthesize question-SQL pairs for pre-training? sent33
    11.3. How does GAP pre-train BART for text-to-SQL tasks? sent34","6. What types of graphs are used to capture semantics in natural language and facilitate linking with table schema? sent9
    6.1. How does Cao et al. (2021) utilize line graphs for multi-hop semantics? sent10
    6.2. How does SADGA use graph structures for question-schema linking? sent11
    6.3. How does S 2 SQL integrate syntax dependency into graphs? sent12

10. How are pre-trained language models (PLMs) adapted for text-to-SQL tasks? sent19
    10.1. How are PLMs like BERT used to encode questions and DB schemas? sent20, sent21
    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

    10.2. How do other methods adjust embeddings by PLMs? sent22
        10.2.1. How does X-SQL adjust embeddings on WikiSQL? sent23
        10.2.2. How do Guo and Gao (2019) enhance matching between question tokens and table cells? sent24
        10.2.3. How does HydraNet align with BERT's pre-training tasks? sent25
        10.2.4. How do Lyu et al. (2020) select columns for SQL prediction? sent26
        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

        10.2.5. How do Liu et al. (2021b) train an auxiliary concept prediction module? sent27
            10.2.5.1. How do they detect important question tokens? sent28
            10.2.5.2. How do they train the PLM with a grounding module? sent29
            10.2.5.3. What claim do Liu et al. (2021b) make about their approach? sent30

11. What are some pre-training objectives and data used to align transformer-based encoders with text-to-SQL tasks? sent31
    11.1. How does TaBERT use tabular data for pre-training? sent32
    11.2. How does Grappa synthesize question-SQL pairs for pre-training? sent33
    11.3. How does GAP pre-train BART for text-to-SQL tasks? sent34","11. What are some pre-training objectives and data used to align transformer-based encoders with text-to-SQL tasks? sent31
    11.1. How does TaBERT use tabular data for pre-training? sent32
    11.2. How does Grappa synthesize question-SQL pairs for pre-training? sent33
    11.3. How does GAP pre-train BART for text-to-SQL tasks? sent34","Question: How do various pre-training methods align transformer-based encoders with text-to-SQL tasks?

Supporting sentence: sent31, sent32, sent33, sent34",How do various pre-training methods align transformer-based encoders with text-to-SQL tasks?,"sent31, sent32, sent33, sent34",4,implement,"Various pre-training methods align transformer-based encoders with text-to-SQL tasks by proposing different pre-training objectives and using different pre-training data.

For instance, TaBERT uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT.

Grappa synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered.

GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.","Question:

How do various pre-training methods align transformer-based encoders with text-to-SQL tasks?

Answer:

Various pre-training methods align transformer-based encoders with text-to-SQL tasks by proposing different pre-training objectives and using different pre-training data.

For instance, TaBERT uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT.

Grappa synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered.

GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation."
264451714,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,https://www.semanticscholar.org/paper/18d18d4ffdc070868ce06f216a2a8d040d42a4cb,Optimized Inputs,20,"Probing inputs contribute substantially to the probing procedure.PLMs are sensitive to the inputs (Petroni et al., 2019;Jiang et al., 2020b;Elazar et al., 2021), and even syntactical variations or distractors, that do not alter the meaning, cause the PLM's predictions to change (Heinzerling and Inui, 2021;Longpre et al., 2021;Pandia and Ettinger, 2021;Podkorytov et al., 2021;Li et al., 2022a).Therefore, depending on the probing inputs, the estimate on factual knowledge we obtain may vary significantly.Optimized inputs represent variations of the inputs, where the inputs are changed to account for the sensitivity of the probed PLMs.

Diversification and mining methods aim to diversify and optimize prompts by mining Wikipedia or other resources, and selecting the best performing prompts or a combination of them.For example, Jiang et al. (2020b) propose a mining-based and a paraphrasing-based approach to create alternative prompts that outperform manual ones.The final prompts are selected based on their performance on a training set, and can also be combined in an ensemble.Bouraoui et al. (2020) mine for prompts that contain the entities of interest, and filter these based on the ability of the probed PLMs to predict the masked objects.After the filtering step, the remaining prompts are utilized to create a dataset that consists of positive inputs, i.e., containing true subject-object pairs, and negative inputs, which contain false pairs.This dataset is then used for the final evaluation.

Direct optimization methods aim to directly optimize existing prompts.This optimization happens either in a discrete space, to keep the prompts in natural language, or in a continuous space where the prompts do not have to correspond to specific tokens from the vocabulary.Optimization could also target only the masked token or the order of the examples in the prompt, in case a few examples are provided in the prompt to better indicate the task.Shin et al. (2020)'s AUTOPROMPT extends manually created prompts by prompts with a pre-defined number of trigger tokens, and employs gradientbased search to sequentially replace the trigger tokens with concrete tokens.These tokens are chosen to increase the probability of predicting the correct object.OPTIPROMPT (Zhong et al., 2021) is sim-ilar to AUTOPROMPT, but allows for the trigger tokens to be replaced with vectors from a continuous embedding space.In a similar fashion, Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation.Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors.The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs.Newman et al. (2022) utilize adapters (Houlsby et al., 2019) to map the embedding vectors to continuous prompts in order to make the probed PLMs less sensitive to different phrasings of the same prompts.Saeed and Papotti (2022) augment the masked tokens with a special type of embeddings, called Type Embeddings.These embeddings are derived from several entities that share the same type, and are shown to help tie the probed PLM's predictions to the expected type of the masked entity.PERO (Kumar and Talukdar, 2021) depends on querying PLMs with prompts containing few training examples (or shots), which demonstrate the task to the queried PLMs.Since PLMs are quite sensitive to the order and the quality of the provided training examples in the prompt, PERO leverages a genetic algorithm to find an optimized prompt and a separator token to concatenate the examples in the prompts.(Li et al., 2022c) exploit the symmetry of the task, and optimize prompts in a continuous space so that the probability of predicting both the subject and the object is maximized using the resulting prompts.

Generation with PLM methods re-write prompts with the help of a secondary PLM.Haviv et al. (2021) re-write manual prompts using another version of the probed model.The re-writing model is trained to produce prompts that help extract more knowledge from the probed one, which is kept unchanged.Zhang et al. (2022) leverage a generative PLM to produce optimized prompts.","sent1: Probing inputs contribute substantially to the probing procedure.
sent2: PLMs are sensitive to the inputs (Petroni et al., 2019;Jiang et al., 2020b;Elazar et al., 2021), and even syntactical variations or distractors, that do not alter the meaning, cause the PLM's predictions to change (Heinzerling and Inui, 2021;Longpre et al., 2021;Pandia and Ettinger, 2021;Podkorytov et al., 2021;Li et al., 2022a).Therefore, depending on the probing inputs, the estimate on factual knowledge we obtain may vary significantly.
sent3: Optimized inputs represent variations of the inputs, where the inputs are changed to account for the sensitivity of the probed PLMs.
sent4: Diversification and mining methods aim to diversify and optimize prompts by mining Wikipedia or other resources, and selecting the best performing prompts or a combination of them.
sent5: For example, Jiang et al. (2020b) propose a mining-based and a paraphrasing-based approach to create alternative prompts that outperform manual ones.
sent6: The final prompts are selected based on their performance on a training set, and can also be combined in an ensemble.
sent7: Bouraoui et al. (2020) mine for prompts that contain the entities of interest, and filter these based on the ability of the probed PLMs to predict the masked objects.
sent8: After the filtering step, the remaining prompts are utilized to create a dataset that consists of positive inputs, i.e., containing true subject-object pairs, and negative inputs, which contain false pairs.
sent9: This dataset is then used for the final evaluation.
sent10: Direct optimization methods aim to directly optimize existing prompts.
sent11: This optimization happens either in a discrete space, to keep the prompts in natural language, or in a continuous space where the prompts do not have to correspond to specific tokens from the vocabulary.
sent12: Optimization could also target only the masked token or the order of the examples in the prompt, in case a few examples are provided in the prompt to better indicate the task.
sent13: Shin et al. (2020)'s AUTOPROMPT extends manually created prompts by prompts with a pre-defined number of trigger tokens, and employs gradientbased search to sequentially replace the trigger tokens with concrete tokens.
sent14: These tokens are chosen to increase the probability of predicting the correct object.
sent15: OPTIPROMPT (Zhong et al., 2021) is sim-ilar to AUTOPROMPT, but allows for the trigger tokens to be replaced with vectors from a continuous embedding space.
sent16: In a similar fashion, Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation.
sent17: Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors.
sent18: The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs.
sent19: Newman et al. (2022) utilize adapters (Houlsby et al., 2019) to map the embedding vectors to continuous prompts in order to make the probed PLMs less sensitive to different phrasings of the same prompts.
sent20: Saeed and Papotti (2022) augment the masked tokens with a special type of embeddings, called Type Embeddings.
sent21: These embeddings are derived from several entities that share the same type, and are shown to help tie the probed PLM's predictions to the expected type of the masked entity.
sent22: PERO (Kumar and Talukdar, 2021) depends on querying PLMs with prompts containing few training examples (or shots), which demonstrate the task to the queried PLMs.
sent23: Since PLMs are quite sensitive to the order and the quality of the provided training examples in the prompt, PERO leverages a genetic algorithm to find an optimized prompt and a separator token to concatenate the examples in the prompts.(Li et al., 2022c) exploit the symmetry of the task, and optimize prompts in a continuous space so that the probability of predicting both the subject and the object is maximized using the resulting prompts.
sent24: Generation with PLM methods re-write prompts with the help of a secondary PLM.Haviv et al. (2021) re-write manual prompts using another version of the probed model.
sent25: The re-writing model is trained to produce prompts that help extract more knowledge from the probed one, which is kept unchanged.
sent26: Zhang et al. (2022) leverage a generative PLM to produce optimized prompts.","1. How do probing inputs contribute to the probing procedure? sent1
2. Why are PLMs sensitive to inputs, and how does this affect factual knowledge estimation? sent2
3. What are optimized inputs, and why are they important for probing PLMs? sent3
4. What methods are used to diversify and optimize prompts? sent4
    4.1. Can you provide an example of a mining-based and paraphrasing-based approach for creating alternative prompts? sent5
    4.2. How are final prompts selected and utilized in the probing process? sent6, sent7, sent8, sent9
5. What are direct optimization methods, and how do they work? sent10
    5.1. How does optimization occur in discrete and continuous spaces? sent11
    5.2. What specific aspects of prompts can be targeted for optimization? sent12
    5.3. How does AUTOPROMPT extend manually created prompts? sent13, sent14
    5.4. How does OPTIPROMPT differ from AUTOPROMPT? sent15
    5.5. What approach do Qin and Eisner propose for learning continuous prompts? sent16, sent17, sent18
    5.6. How do Newman et al. utilize adapters in their optimization method? sent19
    5.7. What are Type Embeddings, and how do they help in probing PLMs? sent20, sent21
    5.8. How does PERO optimize prompts using training examples? sent22, sent23
6. What is the role of generation with PLM methods in re-writing prompts? sent24
    6.1. How do Haviv et al. re-write manual prompts using another version of the probed model? sent25
    6.2. How do Zhang et al. leverage a generative PLM for optimized prompts? sent26","4. What methods are used to diversify and optimize prompts? sent4
    4.1. Can you provide an example of a mining-based and paraphrasing-based approach for creating alternative prompts? sent5
    4.2. How are final prompts selected and utilized in the probing process? sent6, sent7, sent8, sent9

5. What are direct optimization methods, and how do they work? sent10
    5.1. How does optimization occur in discrete and continuous spaces? sent11
    5.2. What specific aspects of prompts can be targeted for optimization? sent12
    5.3. How does AUTOPROMPT extend manually created prompts? sent13, sent14
    5.4. How does OPTIPROMPT differ from AUTOPROMPT? sent15
    5.5. What approach do Qin and Eisner propose for learning continuous prompts? sent16, sent17, sent18
    5.6. How do Newman et al. utilize adapters in their optimization method? sent19
    5.7. What are Type Embeddings, and how do they help in probing PLMs? sent20, sent21
    5.8. How does PERO optimize prompts using training examples? sent22, sent23","4. What methods are used to diversify and optimize prompts? sent4
    4.1. Can you provide an example of a mining-based and paraphrasing-based approach for creating alternative prompts? sent5
    4.2. How are final prompts selected and utilized in the probing process? sent6, sent7, sent8, sent9","Question: How are final prompts selected and utilized in the probing process to evaluate factual knowledge in PLMs?

Supporting sentence: sent6, sent7, sent8, sent9",How are final prompts selected and utilized in the probing process to evaluate factual knowledge in PLMs?,"sent6, sent7, sent8, sent9",2,implement,"Probing inputs contribute substantially to the probing procedure.

Optimized inputs represent variations of the inputs, where the inputs are changed to account for the sensitivity of the probed PLMs.

Diversification and mining methods aim to diversify and optimize prompts by mining Wikipedia or other resources, and selecting the best performing prompts or a combination of them.

For example, Jiang et al. (2020b) propose a mining-based and a paraphrasing-based approach to create alternative prompts that outperform manual ones.

The final prompts are selected based on their performance on a training set, and can also be combined in an ensemble.

Bouraoui et al. (2020) mine for prompts that contain the entities of interest, and filter these based on the ability of the probed PLMs to predict the masked objects.

After the filtering step, the remaining prompts are utilized to create a dataset that consists of positive inputs, i.e., containing true subject-object pairs, and negative inputs, which contain false pairs.

This dataset is then used for the final evaluation.","Question:

How are final prompts selected and utilized in the probing process to evaluate factual knowledge in PLMs?

Answer:

Probing inputs contribute substantially to the probing procedure.

Optimized inputs represent variations of the inputs, where the inputs are changed to account for the sensitivity of the probed PLMs.

Diversification and mining methods aim to diversify and optimize prompts by mining Wikipedia or other resources, and selecting the best performing prompts or a combination of them.

For example, Jiang et al. (2020b) propose a mining-based and a paraphrasing-based approach to create alternative prompts that outperform manual ones.

The final prompts are selected based on their performance on a training set, and can also be combined in an ensemble.

Bouraoui et al. (2020) mine for prompts that contain the entities of interest, and filter these based on the ability of the probed PLMs to predict the masked objects.

After the filtering step, the remaining prompts are utilized to create a dataset that consists of positive inputs, i.e., containing true subject-object pairs, and negative inputs, which contain false pairs.

This dataset is then used for the final evaluation."
264451714,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,https://www.semanticscholar.org/paper/18d18d4ffdc070868ce06f216a2a8d040d42a4cb,Optimized Inputs,20,"Probing inputs contribute substantially to the probing procedure.PLMs are sensitive to the inputs (Petroni et al., 2019;Jiang et al., 2020b;Elazar et al., 2021), and even syntactical variations or distractors, that do not alter the meaning, cause the PLM's predictions to change (Heinzerling and Inui, 2021;Longpre et al., 2021;Pandia and Ettinger, 2021;Podkorytov et al., 2021;Li et al., 2022a).Therefore, depending on the probing inputs, the estimate on factual knowledge we obtain may vary significantly.Optimized inputs represent variations of the inputs, where the inputs are changed to account for the sensitivity of the probed PLMs.

Diversification and mining methods aim to diversify and optimize prompts by mining Wikipedia or other resources, and selecting the best performing prompts or a combination of them.For example, Jiang et al. (2020b) propose a mining-based and a paraphrasing-based approach to create alternative prompts that outperform manual ones.The final prompts are selected based on their performance on a training set, and can also be combined in an ensemble.Bouraoui et al. (2020) mine for prompts that contain the entities of interest, and filter these based on the ability of the probed PLMs to predict the masked objects.After the filtering step, the remaining prompts are utilized to create a dataset that consists of positive inputs, i.e., containing true subject-object pairs, and negative inputs, which contain false pairs.This dataset is then used for the final evaluation.

Direct optimization methods aim to directly optimize existing prompts.This optimization happens either in a discrete space, to keep the prompts in natural language, or in a continuous space where the prompts do not have to correspond to specific tokens from the vocabulary.Optimization could also target only the masked token or the order of the examples in the prompt, in case a few examples are provided in the prompt to better indicate the task.Shin et al. (2020)'s AUTOPROMPT extends manually created prompts by prompts with a pre-defined number of trigger tokens, and employs gradientbased search to sequentially replace the trigger tokens with concrete tokens.These tokens are chosen to increase the probability of predicting the correct object.OPTIPROMPT (Zhong et al., 2021) is sim-ilar to AUTOPROMPT, but allows for the trigger tokens to be replaced with vectors from a continuous embedding space.In a similar fashion, Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation.Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors.The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs.Newman et al. (2022) utilize adapters (Houlsby et al., 2019) to map the embedding vectors to continuous prompts in order to make the probed PLMs less sensitive to different phrasings of the same prompts.Saeed and Papotti (2022) augment the masked tokens with a special type of embeddings, called Type Embeddings.These embeddings are derived from several entities that share the same type, and are shown to help tie the probed PLM's predictions to the expected type of the masked entity.PERO (Kumar and Talukdar, 2021) depends on querying PLMs with prompts containing few training examples (or shots), which demonstrate the task to the queried PLMs.Since PLMs are quite sensitive to the order and the quality of the provided training examples in the prompt, PERO leverages a genetic algorithm to find an optimized prompt and a separator token to concatenate the examples in the prompts.(Li et al., 2022c) exploit the symmetry of the task, and optimize prompts in a continuous space so that the probability of predicting both the subject and the object is maximized using the resulting prompts.

Generation with PLM methods re-write prompts with the help of a secondary PLM.Haviv et al. (2021) re-write manual prompts using another version of the probed model.The re-writing model is trained to produce prompts that help extract more knowledge from the probed one, which is kept unchanged.Zhang et al. (2022) leverage a generative PLM to produce optimized prompts.","sent1: Probing inputs contribute substantially to the probing procedure.
sent2: PLMs are sensitive to the inputs (Petroni et al., 2019;Jiang et al., 2020b;Elazar et al., 2021), and even syntactical variations or distractors, that do not alter the meaning, cause the PLM's predictions to change (Heinzerling and Inui, 2021;Longpre et al., 2021;Pandia and Ettinger, 2021;Podkorytov et al., 2021;Li et al., 2022a).Therefore, depending on the probing inputs, the estimate on factual knowledge we obtain may vary significantly.
sent3: Optimized inputs represent variations of the inputs, where the inputs are changed to account for the sensitivity of the probed PLMs.
sent4: Diversification and mining methods aim to diversify and optimize prompts by mining Wikipedia or other resources, and selecting the best performing prompts or a combination of them.
sent5: For example, Jiang et al. (2020b) propose a mining-based and a paraphrasing-based approach to create alternative prompts that outperform manual ones.
sent6: The final prompts are selected based on their performance on a training set, and can also be combined in an ensemble.
sent7: Bouraoui et al. (2020) mine for prompts that contain the entities of interest, and filter these based on the ability of the probed PLMs to predict the masked objects.
sent8: After the filtering step, the remaining prompts are utilized to create a dataset that consists of positive inputs, i.e., containing true subject-object pairs, and negative inputs, which contain false pairs.
sent9: This dataset is then used for the final evaluation.
sent10: Direct optimization methods aim to directly optimize existing prompts.
sent11: This optimization happens either in a discrete space, to keep the prompts in natural language, or in a continuous space where the prompts do not have to correspond to specific tokens from the vocabulary.
sent12: Optimization could also target only the masked token or the order of the examples in the prompt, in case a few examples are provided in the prompt to better indicate the task.
sent13: Shin et al. (2020)'s AUTOPROMPT extends manually created prompts by prompts with a pre-defined number of trigger tokens, and employs gradientbased search to sequentially replace the trigger tokens with concrete tokens.
sent14: These tokens are chosen to increase the probability of predicting the correct object.
sent15: OPTIPROMPT (Zhong et al., 2021) is sim-ilar to AUTOPROMPT, but allows for the trigger tokens to be replaced with vectors from a continuous embedding space.
sent16: In a similar fashion, Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation.
sent17: Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors.
sent18: The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs.
sent19: Newman et al. (2022) utilize adapters (Houlsby et al., 2019) to map the embedding vectors to continuous prompts in order to make the probed PLMs less sensitive to different phrasings of the same prompts.
sent20: Saeed and Papotti (2022) augment the masked tokens with a special type of embeddings, called Type Embeddings.
sent21: These embeddings are derived from several entities that share the same type, and are shown to help tie the probed PLM's predictions to the expected type of the masked entity.
sent22: PERO (Kumar and Talukdar, 2021) depends on querying PLMs with prompts containing few training examples (or shots), which demonstrate the task to the queried PLMs.
sent23: Since PLMs are quite sensitive to the order and the quality of the provided training examples in the prompt, PERO leverages a genetic algorithm to find an optimized prompt and a separator token to concatenate the examples in the prompts.(Li et al., 2022c) exploit the symmetry of the task, and optimize prompts in a continuous space so that the probability of predicting both the subject and the object is maximized using the resulting prompts.
sent24: Generation with PLM methods re-write prompts with the help of a secondary PLM.Haviv et al. (2021) re-write manual prompts using another version of the probed model.
sent25: The re-writing model is trained to produce prompts that help extract more knowledge from the probed one, which is kept unchanged.
sent26: Zhang et al. (2022) leverage a generative PLM to produce optimized prompts.","1. How do probing inputs contribute to the probing procedure? sent1
2. Why are PLMs sensitive to inputs, and how does this affect factual knowledge estimation? sent2
3. What are optimized inputs, and why are they important for probing PLMs? sent3
4. What methods are used to diversify and optimize prompts? sent4
    4.1. Can you provide an example of a mining-based and paraphrasing-based approach for creating alternative prompts? sent5
    4.2. How are final prompts selected and utilized in the probing process? sent6, sent7, sent8, sent9
5. What are direct optimization methods, and how do they work? sent10
    5.1. How does optimization occur in discrete and continuous spaces? sent11
    5.2. What specific aspects of prompts can be targeted for optimization? sent12
    5.3. How does AUTOPROMPT extend manually created prompts? sent13, sent14
    5.4. How does OPTIPROMPT differ from AUTOPROMPT? sent15
    5.5. What approach do Qin and Eisner propose for learning continuous prompts? sent16, sent17, sent18
    5.6. How do Newman et al. utilize adapters in their optimization method? sent19
    5.7. What are Type Embeddings, and how do they help in probing PLMs? sent20, sent21
    5.8. How does PERO optimize prompts using training examples? sent22, sent23
6. What is the role of generation with PLM methods in re-writing prompts? sent24
    6.1. How do Haviv et al. re-write manual prompts using another version of the probed model? sent25
    6.2. How do Zhang et al. leverage a generative PLM for optimized prompts? sent26","4. What methods are used to diversify and optimize prompts? sent4
    4.1. Can you provide an example of a mining-based and paraphrasing-based approach for creating alternative prompts? sent5
    4.2. How are final prompts selected and utilized in the probing process? sent6, sent7, sent8, sent9

5. What are direct optimization methods, and how do they work? sent10
    5.1. How does optimization occur in discrete and continuous spaces? sent11
    5.2. What specific aspects of prompts can be targeted for optimization? sent12
    5.3. How does AUTOPROMPT extend manually created prompts? sent13, sent14
    5.4. How does OPTIPROMPT differ from AUTOPROMPT? sent15
    5.5. What approach do Qin and Eisner propose for learning continuous prompts? sent16, sent17, sent18
    5.6. How do Newman et al. utilize adapters in their optimization method? sent19
    5.7. What are Type Embeddings, and how do they help in probing PLMs? sent20, sent21
    5.8. How does PERO optimize prompts using training examples? sent22, sent23","5. What are direct optimization methods, and how do they work? sent10
    5.1. How does optimization occur in discrete and continuous spaces? sent11
    5.2. What specific aspects of prompts can be targeted for optimization? sent12
    5.3. How does AUTOPROMPT extend manually created prompts? sent13, sent14
    5.4. How does OPTIPROMPT differ from AUTOPROMPT? sent15
    5.5. What approach do Qin and Eisner propose for learning continuous prompts? sent16, sent17, sent18
    5.6. How do Newman et al. utilize adapters in their optimization method? sent19
    5.7. What are Type Embeddings, and how do they help in probing PLMs? sent20, sent21
    5.8. How does PERO optimize prompts using training examples? sent22, sent23","Question: How do Qin and Eisner propose to learn and optimize continuous prompts for probing pre-trained language models?

Supporting sentence: sent16, sent17, sent18",How do Qin and Eisner propose to learn and optimize continuous prompts for probing pre-trained language models?,"sent16, sent17, sent18",1,implement,"Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation.

Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors.

The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs.","Question:

How do Qin and Eisner propose to learn and optimize continuous prompts for probing pre-trained language models?

Answer:

Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation.

Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors.

The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs."
253736389,Transformers for Tabular Data Representation: A Survey of Models and Applications,https://www.semanticscholar.org/paper/18ff1542d5a2a4490c7b3f21522bf1343889f700,Vanilla Transformer,6,"The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules. The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Residual connections and layer-normalization modules are also used. Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions.

The transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017;Raffel et al., 2020), an encoder-only , or decoder-only (Radford et al., 2019;Brown et al., 2020) model. The choice of the architecture depends on the final task. Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data. In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input. The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).","sent1: The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules.
sent2: The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network.
sent3: Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.
sent4: Residual connections and layer-normalization modules are also used.
sent5: Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions.
sent6: The transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017;Raffel et al., 2020), an encoder-only , or decoder-only (Radford et al., 2019;Brown et al., 2020) model.
sent7: The choice of the architecture depends on the final task.
sent8: Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data.
sent9: In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input.
sent10: The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).","1. What is the vanilla transformer and what does it consist of? sent1
    1.1. What is the composition of the encoder block in a vanilla transformer? sent2
        1.1.1. What is the purpose of multi-head attention in the transformer model? sent3
        1.1.2. What additional modules are used in the encoder block? sent4
    1.2. What do decoder blocks consist of in a vanilla transformer? sent5
2. How can the transformer architecture be utilized? sent6
    2.1. What determines the choice of transformer architecture? sent7
    2.2. What are encoder-only models mainly used for, and what is their popular application? sent8
        2.2.1. How is pretraining done for encoder-only models? sent9
    2.3. For what type of tasks is the encoder-decoder architecture used? sent10","1. What is the vanilla transformer and what does it consist of? sent1
    1.1. What is the composition of the encoder block in a vanilla transformer? sent2
        1.1.1. What is the purpose of multi-head attention in the transformer model? sent3
        1.1.2. What additional modules are used in the encoder block? sent4
    1.2. What do decoder blocks consist of in a vanilla transformer? sent5

2. How can the transformer architecture be utilized? sent6
    2.1. What determines the choice of transformer architecture? sent7
    2.2. What are encoder-only models mainly used for, and what is their popular application? sent8
        2.2.1. How is pretraining done for encoder-only models? sent9
    2.3. For what type of tasks is the encoder-decoder architecture used? sent10","1. What is the vanilla transformer and what does it consist of? sent1
    1.1. What is the composition of the encoder block in a vanilla transformer? sent2
        1.1.1. What is the purpose of multi-head attention in the transformer model? sent3
        1.1.2. What additional modules are used in the encoder block? sent4
    1.2. What do decoder blocks consist of in a vanilla transformer? sent5","Question: How do decoder blocks function within the vanilla transformer architecture?

Supporting sentence: sent5",How do decoder blocks function within the vanilla transformer architecture?,sent5,2,implement,"Decoder blocks within the vanilla transformer architecture consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks.

Masking is used to prevent each position from attending to subsequent positions.","Question:

How do decoder blocks function within the vanilla transformer architecture?

Answer:

Decoder blocks within the vanilla transformer architecture consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks.

Masking is used to prevent each position from attending to subsequent positions."
253736389,Transformers for Tabular Data Representation: A Survey of Models and Applications,https://www.semanticscholar.org/paper/18ff1542d5a2a4490c7b3f21522bf1343889f700,Vanilla Transformer,6,"The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules. The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Residual connections and layer-normalization modules are also used. Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions.

The transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017;Raffel et al., 2020), an encoder-only , or decoder-only (Radford et al., 2019;Brown et al., 2020) model. The choice of the architecture depends on the final task. Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data. In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input. The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).","sent1: The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules.
sent2: The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network.
sent3: Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.
sent4: Residual connections and layer-normalization modules are also used.
sent5: Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions.
sent6: The transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017;Raffel et al., 2020), an encoder-only , or decoder-only (Radford et al., 2019;Brown et al., 2020) model.
sent7: The choice of the architecture depends on the final task.
sent8: Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data.
sent9: In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input.
sent10: The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).","1. What is the vanilla transformer and what does it consist of? sent1
    1.1. What is the composition of the encoder block in a vanilla transformer? sent2
        1.1.1. What is the purpose of multi-head attention in the transformer model? sent3
        1.1.2. What additional modules are used in the encoder block? sent4
    1.2. What do decoder blocks consist of in a vanilla transformer? sent5
2. How can the transformer architecture be utilized? sent6
    2.1. What determines the choice of transformer architecture? sent7
    2.2. What are encoder-only models mainly used for, and what is their popular application? sent8
        2.2.1. How is pretraining done for encoder-only models? sent9
    2.3. For what type of tasks is the encoder-decoder architecture used? sent10","1. What is the vanilla transformer and what does it consist of? sent1
    1.1. What is the composition of the encoder block in a vanilla transformer? sent2
        1.1.1. What is the purpose of multi-head attention in the transformer model? sent3
        1.1.2. What additional modules are used in the encoder block? sent4
    1.2. What do decoder blocks consist of in a vanilla transformer? sent5

2. How can the transformer architecture be utilized? sent6
    2.1. What determines the choice of transformer architecture? sent7
    2.2. What are encoder-only models mainly used for, and what is their popular application? sent8
        2.2.1. How is pretraining done for encoder-only models? sent9
    2.3. For what type of tasks is the encoder-decoder architecture used? sent10","2. How can the transformer architecture be utilized? sent6
    2.1. What determines the choice of transformer architecture? sent7
    2.2. What are encoder-only models mainly used for, and what is their popular application? sent8
        2.2.1. How is pretraining done for encoder-only models? sent9
    2.3. For what type of tasks is the encoder-decoder architecture used? sent10","Question: How is the transformer architecture adapted for different tasks in NLP?

Supporting sentence: sent6, sent7, sent8, sent9, sent10",How is the transformer architecture adapted for different tasks in NLP?,"sent6, sent7, sent8, sent9, sent10",4,implement,"The transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017; Raffel et al., 2020), an encoder-only, or decoder-only (Radford et al., 2019; Brown et al., 2020) model.

The choice of the architecture depends on the final task.

Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data.

In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input.

The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).","Question:

How is the transformer architecture adapted for different tasks in NLP?

Answer:

The transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017; Raffel et al., 2020), an encoder-only, or decoder-only (Radford et al., 2019; Brown et al., 2020) model.

The choice of the architecture depends on the final task.

Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data.

In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input.

The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX)."
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,Skimming,7,"Skimming techniques, as summarized in Table 1, skip some time steps or allocate different computation on them. Intuitively, skimming matches how human beings efficiently read text and extract information from it (Li et al., 2019). By em-phasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies. The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN, corresponding with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only.

Skipping and Early Stopping Skipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early. LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU). At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading. LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions. The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label. Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up. Skip RNN (Campos et al., 2018) introduces a binary gate to learn whether to skip a state update. If the gate decides to skip a time step, the hidden states will be directly copied without any update.

To stop reading early as needed, Rea-soNet (Shen et al., 2017) introduces a terminal state which decides whether to terminate early for machine reading comprehension on each time step at the token level. Jumper  first splits a paragraph to several sub-sentences and encodes them into sentence embeddings. They then apply early stopping at a sentence when the policy network decides to stop reading. Li et al. (2019) use eye-tracking devices and confirm that skipping and early stopping are common when humans read text. They propose Reading Inspired Model to mimic the behaviors of humans, which allows the model to decide whether to skip a single time step or stop reading early. Yu et al. (2018) add a rereading operation to LSTM-Jump (Yu et al., 2017) which allows the model to stay on the current time step, allocating more computation to important information.

The aforementioned techniques can only go forward, which makes it impossible to regret if hav-","sent1: Skimming techniques, as summarized in Table 1, skip some time steps or allocate different computation on them.
sent2: Intuitively, skimming matches how human beings efficiently read text and extract information from it (Li et al., 2019).
sent3: By em-phasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies.
sent4: The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN, corresponding with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only.
sent5: Skipping and Early Stopping Skipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early.
sent6: LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU).
sent7: At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading.
sent8: LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions.
sent9: The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label.
sent10: Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up.
sent11: Skip RNN (Campos et al., 2018) introduces a binary gate to learn whether to skip a state update.
sent12: If the gate decides to skip a time step, the hidden states will be directly copied without any update.
sent13: To stop reading early as needed, Rea-soNet (Shen et al., 2017) introduces a terminal state which decides whether to terminate early for machine reading comprehension on each time step at the token level.
sent14: Jumper  first splits a paragraph to several sub-sentences and encodes them into sentence embeddings.
sent15: They then apply early stopping at a sentence when the policy network decides to stop reading.
sent16: Li et al. (2019) use eye-tracking devices and confirm that skipping and early stopping are common when humans read text.
sent17: They propose Reading Inspired Model to mimic the behaviors of humans, which allows the model to decide whether to skip a single time step or stop reading early.
sent18: Yu et al. (2018) add a rereading operation to LSTM-Jump (Yu et al., 2017) which allows the model to stay on the current time step, allocating more computation to important information.
sent19: The aforementioned techniques can only go forward, which makes it impossible to regret if hav-","1. What are skimming techniques and how do they function? sent1
    1.1. How do skimming techniques relate to human reading behavior? sent2
    1.2. What benefits do skimming techniques provide to models? sent3
    1.3. What are the three categories of skimming techniques and their motivations? sent4
2. What is the purpose of skipping and early stopping in skimming techniques? sent5
    2.1. What is LSTM-Jump and how does it function? sent6, sent7, sent8
        2.1.1. What is the reward mechanism in LSTM-Jump? sent9
        2.1.2. How does LSTM-Jump compare to a standard LSTM? sent10
    2.2. What is Skip RNN and how does it function? sent11, sent12
    2.3. How does Rea-soNet implement early stopping? sent13
    2.4. How does Jumper apply early stopping? sent14, sent15
    2.5. What evidence supports the use of skipping and early stopping in human reading? sent16
    2.6. What is the Reading Inspired Model proposed by Li et al. (2019)? sent17
    2.7. How does Yu et al. (2018) enhance LSTM-Jump with a rereading operation? sent18","1. What are skimming techniques and how do they function? sent1
    1.1. How do skimming techniques relate to human reading behavior? sent2
    1.2. What benefits do skimming techniques provide to models? sent3
    1.3. What are the three categories of skimming techniques and their motivations? sent4

2. What is the purpose of skipping and early stopping in skimming techniques? sent5
    2.1. What is LSTM-Jump and how does it function? sent6, sent7, sent8
        2.1.1. What is the reward mechanism in LSTM-Jump? sent9
        2.1.2. How does LSTM-Jump compare to a standard LSTM? sent10
    2.2. What is Skip RNN and how does it function? sent11, sent12
    2.3. How does Rea-soNet implement early stopping? sent13
    2.4. How does Jumper apply early stopping? sent14, sent15
    2.5. What evidence supports the use of skipping and early stopping in human reading? sent16
    2.6. What is the Reading Inspired Model proposed by Li et al. (2019)? sent17
    2.7. How does Yu et al. (2018) enhance LSTM-Jump with a rereading operation? sent18","1. What are skimming techniques and how do they function? sent1
    1.1. How do skimming techniques relate to human reading behavior? sent2
    1.2. What benefits do skimming techniques provide to models? sent3
    1.3. What are the three categories of skimming techniques and their motivations? sent4","Question: What are the three categories of skimming techniques in NLP, and what motivates each category?

Supporting sentence: sent4","What are the three categories of skimming techniques in NLP, and what motivates each category?",sent4,2,implement,"Skimming techniques skip some time steps or allocate different computation on them.

The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN.

These categories correspond with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only.","Question:

What are the three categories of skimming techniques in NLP, and what motivates each category?

Answer:

Skimming techniques skip some time steps or allocate different computation on them.

The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN.

These categories correspond with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only."
246863418,A Survey on Dynamic Neural Networks for Natural Language Processing,https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568,Skimming,7,"Skimming techniques, as summarized in Table 1, skip some time steps or allocate different computation on them. Intuitively, skimming matches how human beings efficiently read text and extract information from it (Li et al., 2019). By em-phasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies. The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN, corresponding with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only.

Skipping and Early Stopping Skipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early. LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU). At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading. LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions. The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label. Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up. Skip RNN (Campos et al., 2018) introduces a binary gate to learn whether to skip a state update. If the gate decides to skip a time step, the hidden states will be directly copied without any update.

To stop reading early as needed, Rea-soNet (Shen et al., 2017) introduces a terminal state which decides whether to terminate early for machine reading comprehension on each time step at the token level. Jumper  first splits a paragraph to several sub-sentences and encodes them into sentence embeddings. They then apply early stopping at a sentence when the policy network decides to stop reading. Li et al. (2019) use eye-tracking devices and confirm that skipping and early stopping are common when humans read text. They propose Reading Inspired Model to mimic the behaviors of humans, which allows the model to decide whether to skip a single time step or stop reading early. Yu et al. (2018) add a rereading operation to LSTM-Jump (Yu et al., 2017) which allows the model to stay on the current time step, allocating more computation to important information.

The aforementioned techniques can only go forward, which makes it impossible to regret if hav-","sent1: Skimming techniques, as summarized in Table 1, skip some time steps or allocate different computation on them.
sent2: Intuitively, skimming matches how human beings efficiently read text and extract information from it (Li et al., 2019).
sent3: By em-phasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies.
sent4: The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN, corresponding with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only.
sent5: Skipping and Early Stopping Skipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early.
sent6: LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU).
sent7: At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading.
sent8: LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions.
sent9: The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label.
sent10: Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up.
sent11: Skip RNN (Campos et al., 2018) introduces a binary gate to learn whether to skip a state update.
sent12: If the gate decides to skip a time step, the hidden states will be directly copied without any update.
sent13: To stop reading early as needed, Rea-soNet (Shen et al., 2017) introduces a terminal state which decides whether to terminate early for machine reading comprehension on each time step at the token level.
sent14: Jumper  first splits a paragraph to several sub-sentences and encodes them into sentence embeddings.
sent15: They then apply early stopping at a sentence when the policy network decides to stop reading.
sent16: Li et al. (2019) use eye-tracking devices and confirm that skipping and early stopping are common when humans read text.
sent17: They propose Reading Inspired Model to mimic the behaviors of humans, which allows the model to decide whether to skip a single time step or stop reading early.
sent18: Yu et al. (2018) add a rereading operation to LSTM-Jump (Yu et al., 2017) which allows the model to stay on the current time step, allocating more computation to important information.
sent19: The aforementioned techniques can only go forward, which makes it impossible to regret if hav-","1. What are skimming techniques and how do they function? sent1
    1.1. How do skimming techniques relate to human reading behavior? sent2
    1.2. What benefits do skimming techniques provide to models? sent3
    1.3. What are the three categories of skimming techniques and their motivations? sent4
2. What is the purpose of skipping and early stopping in skimming techniques? sent5
    2.1. What is LSTM-Jump and how does it function? sent6, sent7, sent8
        2.1.1. What is the reward mechanism in LSTM-Jump? sent9
        2.1.2. How does LSTM-Jump compare to a standard LSTM? sent10
    2.2. What is Skip RNN and how does it function? sent11, sent12
    2.3. How does Rea-soNet implement early stopping? sent13
    2.4. How does Jumper apply early stopping? sent14, sent15
    2.5. What evidence supports the use of skipping and early stopping in human reading? sent16
    2.6. What is the Reading Inspired Model proposed by Li et al. (2019)? sent17
    2.7. How does Yu et al. (2018) enhance LSTM-Jump with a rereading operation? sent18","1. What are skimming techniques and how do they function? sent1
    1.1. How do skimming techniques relate to human reading behavior? sent2
    1.2. What benefits do skimming techniques provide to models? sent3
    1.3. What are the three categories of skimming techniques and their motivations? sent4

2. What is the purpose of skipping and early stopping in skimming techniques? sent5
    2.1. What is LSTM-Jump and how does it function? sent6, sent7, sent8
        2.1.1. What is the reward mechanism in LSTM-Jump? sent9
        2.1.2. How does LSTM-Jump compare to a standard LSTM? sent10
    2.2. What is Skip RNN and how does it function? sent11, sent12
    2.3. How does Rea-soNet implement early stopping? sent13
    2.4. How does Jumper apply early stopping? sent14, sent15
    2.5. What evidence supports the use of skipping and early stopping in human reading? sent16
    2.6. What is the Reading Inspired Model proposed by Li et al. (2019)? sent17
    2.7. How does Yu et al. (2018) enhance LSTM-Jump with a rereading operation? sent18","2. What is the purpose of skipping and early stopping in skimming techniques? sent5
    2.1. What is LSTM-Jump and how does it function? sent6, sent7, sent8
        2.1.1. What is the reward mechanism in LSTM-Jump? sent9
        2.1.2. How does LSTM-Jump compare to a standard LSTM? sent10
    2.2. What is Skip RNN and how does it function? sent11, sent12
    2.3. How does Rea-soNet implement early stopping? sent13
    2.4. How does Jumper apply early stopping? sent14, sent15
    2.5. What evidence supports the use of skipping and early stopping in human reading? sent16
    2.6. What is the Reading Inspired Model proposed by Li et al. (2019)? sent17
    2.7. How does Yu et al. (2018) enhance LSTM-Jump with a rereading operation? sent18","Question: How do skimming techniques like LSTM-Jump and Skip RNN improve efficiency in natural language processing tasks?

Supporting sentence: sent5, sent6, sent7, sent8, sent9, sent10, sent11, sent12",How do skimming techniques like LSTM-Jump and Skip RNN improve efficiency in natural language processing tasks?,"sent5, sent6, sent7, sent8, sent9, sent10, sent11, sent12",2,implement,"Skimming techniques skip some time steps or allocate different computation on them.

LSTM-Jump is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU).

At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading.

LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions.

The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label.

Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up.

Skip RNN introduces a binary gate to learn whether to skip a state update.

If the gate decides to skip a time step, the hidden states will be directly copied without any update.","Question:

How do skimming techniques like LSTM-Jump and Skip RNN improve efficiency in natural language processing tasks?

Answer:

Skimming techniques skip some time steps or allocate different computation on them.

LSTM-Jump is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU).

At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading.

LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions.

The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label.

Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up.

Skip RNN introduces a binary gate to learn whether to skip a state update.

If the gate decides to skip a time step, the hidden states will be directly copied without any update."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,B Additional Formulations of Stance as a Component for Fact-Checking,11,"Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline. Below, we describe some work that follows these formulations.

Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia. They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions. A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020). This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.

Media Profiling Stance detection has also been used for media profiling. Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics. They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020). This is an important step towards understanding media biases. Tweet: Wow, that is fascinating! I hope you never mock our proud Scandi heritage again.

(b) Examples from Qazvinian et al. (2011) andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992. ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.

(c) Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how do you know its an ISIS flag? Can you actually confirm that? ɳ u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table 3: Illustrative examples for different stance detection scenarios included in our survey. We annotate the expressed stance with (support, for), (deny, against), ɳ (query), and (comment).

The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.

More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information. In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling.

There is a well-known connection between factuality and bias. 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.

User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user. In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017).","sent1: Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline.
sent2: Below, we describe some work that follows these formulations.
sent3: Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia.
sent4: They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions.
sent5: A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020).
sent6: This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.
sent7: Media Profiling Stance detection has also been used for media profiling.
sent8: Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics.
sent9: They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020).
sent10: This is an important step towards understanding media biases.
sent11: Tweet: Wow, that is fascinating!
sent12: I hope you never mock our proud Scandi heritage again.
sent13: (b) Examples from Qazvinian et al. (2011)
sent14: andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992.
sent15: ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.(c)
sent16: Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ
sent17: Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ
sent18: Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how
sent19: do you know its an ISIS flag? Can you actually confirm that?
sent20: ɳ u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table
sent21: 3: Illustrative examples for different stance detection scenarios included in our survey.
sent22: We annotate the expressed stance with (support, for), (deny, against), ɳ (query), and (comment).
sent23: The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.
sent24: More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information.
sent25: In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling.
sent26: There is a well-known connection between factuality and bias.
sent27: 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.
sent28: User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user.
sent29: In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017).","1. How has stance been used beyond the approaches outlined in Section 3.2? sent1
    1.1. What are some works that follow these additional formulations of stance? sent2
        1.1.1. How has stance been used for detecting misconceptions? sent3
            1.1.1.1. How is the veracity of a tweet evaluated in the context of misconceptions? sent4
        1.1.2. What is a related formulation of the task of stance detection? sent5
            1.1.2.1. How does this formulation assess the veracity of dubious content? sent6
    1.2. How has stance detection been used for media profiling? sent7
        1.2.1. What approach did Stefanov et al. (2020) explore for media profiling? sent8
            1.2.1.1. How did they build clusters of users for media profiling? sent9
            1.2.1.2. Why is this step important for understanding media biases? sent10
2. How has the reliability of news media sources been rated based on stance? sent23
    2.1. What assumption is made about reliable and unreliable media in these methods? sent23
    2.2. What recent work has used gold labels from Media Bias/Fact Check for media profiling? sent24
        2.2.1. What did the follow-up work by Baly et al. (2019) aim to predict? sent25
3. What is the connection between factuality and bias? sent26
    3.1. How is hyper-partisanship linked to trustworthiness? sent27
4. Why is it important to model the trustworthiness of users in social media and community fora? sent28
    4.1. What research has been conducted on opinion manipulation and related activities? sent29","1. How has stance been used beyond the approaches outlined in Section 3.2? sent1
    1.1. What are some works that follow these additional formulations of stance? sent2
        1.1.1. How has stance been used for detecting misconceptions? sent3
            1.1.1.1. How is the veracity of a tweet evaluated in the context of misconceptions? sent4
        1.1.2. What is a related formulation of the task of stance detection? sent5
            1.1.2.1. How does this formulation assess the veracity of dubious content? sent6
    1.2. How has stance detection been used for media profiling? sent7
        1.2.1. What approach did Stefanov et al. (2020) explore for media profiling? sent8
            1.2.1.1. How did they build clusters of users for media profiling? sent9
            1.2.1.2. Why is this step important for understanding media biases? sent10

    1.1. What are some works that follow these additional formulations of stance? sent2
        1.1.1. How has stance been used for detecting misconceptions? sent3
            1.1.1.1. How is the veracity of a tweet evaluated in the context of misconceptions? sent4
        1.1.2. What is a related formulation of the task of stance detection? sent5
            1.1.2.1. How does this formulation assess the veracity of dubious content? sent6

    1.2. How has stance detection been used for media profiling? sent7
        1.2.1. What approach did Stefanov et al. (2020) explore for media profiling? sent8
            1.2.1.1. How did they build clusters of users for media profiling? sent9
            1.2.1.2. Why is this step important for understanding media biases? sent10","    1.1. What are some works that follow these additional formulations of stance? sent2
        1.1.1. How has stance been used for detecting misconceptions? sent3
            1.1.1.1. How is the veracity of a tweet evaluated in the context of misconceptions? sent4
        1.1.2. What is a related formulation of the task of stance detection? sent5
            1.1.2.1. How does this formulation assess the veracity of dubious content? sent6","Question: How is stance detection utilized for media profiling and what methodologies are employed in this context?

Supporting sentence: sent7, sent8, sent9, sent10",How is stance detection utilized for media profiling and what methodologies are employed in this context?,"sent7, sent8, sent9, sent10",4,implement,"Stance detection has been used for media profiling by exploring the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics.

Stefanov et al. (2020) built clusters of users around core vocal ones based on their behavior on Twitter, such as retweeting, using the procedure proposed by Darwish et al. (2020).

This approach is an important step towards understanding media biases.

The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015; Popat et al., 2017; Popat et al., 2018).

The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed.

More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information.

In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.

There is a well-known connection between factuality and bias, as hyper-partisanship is often linked to low trustworthiness, while center media tend to be generally more impartial and also more trustworthy.","Question:

How is stance detection utilized for media profiling and what methodologies are employed in this context?

Answer:

Stance detection has been used for media profiling by exploring the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics.

Stefanov et al. (2020) built clusters of users around core vocal ones based on their behavior on Twitter, such as retweeting, using the procedure proposed by Darwish et al. (2020).

This approach is an important step towards understanding media biases.

The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015; Popat et al., 2017; Popat et al., 2018).

The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed.

More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information.

In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.

There is a well-known connection between factuality and bias, as hyper-partisanship is often linked to low trustworthiness, while center media tend to be generally more impartial and also more trustworthy."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,B Additional Formulations of Stance as a Component for Fact-Checking,11,"Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline. Below, we describe some work that follows these formulations.

Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia. They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions. A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020). This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.

Media Profiling Stance detection has also been used for media profiling. Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics. They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020). This is an important step towards understanding media biases. Tweet: Wow, that is fascinating! I hope you never mock our proud Scandi heritage again.

(b) Examples from Qazvinian et al. (2011) andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992. ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.

(c) Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how do you know its an ISIS flag? Can you actually confirm that? ɳ u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table 3: Illustrative examples for different stance detection scenarios included in our survey. We annotate the expressed stance with (support, for), (deny, against), ɳ (query), and (comment).

The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.

More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information. In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling.

There is a well-known connection between factuality and bias. 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.

User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user. In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017).","sent1: Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline.
sent2: Below, we describe some work that follows these formulations.
sent3: Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia.
sent4: They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions.
sent5: A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020).
sent6: This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.
sent7: Media Profiling Stance detection has also been used for media profiling.
sent8: Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics.
sent9: They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020).
sent10: This is an important step towards understanding media biases.
sent11: Tweet: Wow, that is fascinating!
sent12: I hope you never mock our proud Scandi heritage again.
sent13: (b) Examples from Qazvinian et al. (2011)
sent14: andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992.
sent15: ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.(c)
sent16: Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ
sent17: Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ
sent18: Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how
sent19: do you know its an ISIS flag? Can you actually confirm that?
sent20: ɳ u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table
sent21: 3: Illustrative examples for different stance detection scenarios included in our survey.
sent22: We annotate the expressed stance with (support, for), (deny, against), ɳ (query), and (comment).
sent23: The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.
sent24: More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information.
sent25: In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling.
sent26: There is a well-known connection between factuality and bias.
sent27: 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.
sent28: User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user.
sent29: In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017).","1. How has stance been used beyond the approaches outlined in Section 3.2? sent1
    1.1. What are some works that follow these additional formulations of stance? sent2
        1.1.1. How has stance been used for detecting misconceptions? sent3
            1.1.1.1. How is the veracity of a tweet evaluated in the context of misconceptions? sent4
        1.1.2. What is a related formulation of the task of stance detection? sent5
            1.1.2.1. How does this formulation assess the veracity of dubious content? sent6
    1.2. How has stance detection been used for media profiling? sent7
        1.2.1. What approach did Stefanov et al. (2020) explore for media profiling? sent8
            1.2.1.1. How did they build clusters of users for media profiling? sent9
            1.2.1.2. Why is this step important for understanding media biases? sent10
2. How has the reliability of news media sources been rated based on stance? sent23
    2.1. What assumption is made about reliable and unreliable media in these methods? sent23
    2.2. What recent work has used gold labels from Media Bias/Fact Check for media profiling? sent24
        2.2.1. What did the follow-up work by Baly et al. (2019) aim to predict? sent25
3. What is the connection between factuality and bias? sent26
    3.1. How is hyper-partisanship linked to trustworthiness? sent27
4. Why is it important to model the trustworthiness of users in social media and community fora? sent28
    4.1. What research has been conducted on opinion manipulation and related activities? sent29","1. How has stance been used beyond the approaches outlined in Section 3.2? sent1
    1.1. What are some works that follow these additional formulations of stance? sent2
        1.1.1. How has stance been used for detecting misconceptions? sent3
            1.1.1.1. How is the veracity of a tweet evaluated in the context of misconceptions? sent4
        1.1.2. What is a related formulation of the task of stance detection? sent5
            1.1.2.1. How does this formulation assess the veracity of dubious content? sent6
    1.2. How has stance detection been used for media profiling? sent7
        1.2.1. What approach did Stefanov et al. (2020) explore for media profiling? sent8
            1.2.1.1. How did they build clusters of users for media profiling? sent9
            1.2.1.2. Why is this step important for understanding media biases? sent10

    1.1. What are some works that follow these additional formulations of stance? sent2
        1.1.1. How has stance been used for detecting misconceptions? sent3
            1.1.1.1. How is the veracity of a tweet evaluated in the context of misconceptions? sent4
        1.1.2. What is a related formulation of the task of stance detection? sent5
            1.1.2.1. How does this formulation assess the veracity of dubious content? sent6

    1.2. How has stance detection been used for media profiling? sent7
        1.2.1. What approach did Stefanov et al. (2020) explore for media profiling? sent8
            1.2.1.1. How did they build clusters of users for media profiling? sent9
            1.2.1.2. Why is this step important for understanding media biases? sent10","    1.2. How has stance detection been used for media profiling? sent7
        1.2.1. What approach did Stefanov et al. (2020) explore for media profiling? sent8
            1.2.1.1. How did they build clusters of users for media profiling? sent9
            1.2.1.2. Why is this step important for understanding media biases? sent10","Question: How is stance detection utilized to identify political leanings of media outlets on Twitter?

Supporting sentence: sent7, sent8, sent9, sent10",How is stance detection utilized to identify political leanings of media outlets on Twitter?,"sent7, sent8, sent9, sent10",4,implement,"Stance detection has been used for media profiling.

Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics.

They built clusters of users around core vocal ones based on their behavior on Twitter, such as retweeting, using the procedure proposed by Darwish et al. (2020).

This is an important step towards understanding media biases.","Question:

How is stance detection utilized to identify political leanings of media outlets on Twitter?

Answer:

Stance detection has been used for media profiling.

Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics.

They built clusters of users around core vocal ones based on their behavior on Twitter, such as retweeting, using the procedure proposed by Darwish et al. (2020).

This is an important step towards understanding media biases."
256662721,"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems",https://www.semanticscholar.org/paper/c589a3420ba335a05c248f525ea3c6e90215e42b,Adaptive objectives to recommendation,7,"Numerous pre-training or fine-tuning objectives draw inspiration from LM objectives and have been effectively applied to specific downstream tasks based on the input data types and recommendation goals. In sequential recommendations, there is a common interest in modelling an ordered input sequence in an auto-regressive manner from left to right.

Analogous to text sentences,  and Xiao et al. (2022) treated the user's clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction. However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly. Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training. Autoregressive learning tasks can also be adapted to other types of data. Geng et al. (2022b) modeled a series of paths sampled from a knowledge graph in an auto-regressive manner for recommendation by generating the end node from the pretrained model. Zhao (2022) proposed pre-training the Rearrange Sequence Prediction task to learn the sequence-level information of the user's entire interaction history by predicting whether the user interaction history had been rearranged, which is similar to Permuted Language Modelling (PerLM) (Yang et al., 2019).

MLM, also known as Cloze Prediction, can be adapted to learn graph representations for different recommendation purposes.  proposed pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives. Specifically, MNP was performed by randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node. Similarly, MEP was used to recover the masked edge of two adjacent nodes based on the surrounding context. Apart from that, MLM can also be adapted to multi-modal data called Masked Multimodal Modelling (MMM) (Wu et al., 2022a). MMM was performed by predicting the semantics of masked news and news image regions given the unmasked inputs and indicating whether a news image and news content segment correspond to each other for news recommendation purposes.

The NSP/SOP can be adapted for CTR prediction as Next K Behaviors Prediction (NBP). NBP was proposed to learn user representations in the pre-training stage by inferring whether a candidate behavior is the next i-th behavior of the target user based on their past N behaviors. NBP can also capture the relatedness between past and multiple future behaviors.","sent1: Numerous pre-training or fine-tuning objectives draw inspiration from LM objectives and have been effectively applied to specific downstream tasks based on the input data types and recommendation goals.
sent2: In sequential recommendations, there is a common interest in modelling an ordered input sequence in an auto-regressive manner from left to right.
sent3: Analogous to text sentences,  and Xiao et al. (2022) treated the user's clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction.
sent4: However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly.
sent5: Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training.
sent6: Autoregressive learning tasks can also be adapted to other types of data.
sent7: Geng et al. (2022b) modeled a series of paths sampled from a knowledge graph in an auto-regressive manner for recommendation by generating the end node from the pretrained model.
sent8: Zhao (2022) proposed pre-training the Rearrange Sequence Prediction task to learn the sequence-level information of the user's entire interaction history by predicting whether the user interaction history had been rearranged, which is similar to Permuted Language Modelling (PerLM) (Yang et al., 2019).MLM, also known as Cloze Prediction, can be adapted to learn graph representations for different recommendation purposes.
sent9: proposed pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives.
sent10: Specifically, MNP was performed by randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node.
sent11: Similarly, MEP was used to recover the masked edge of two adjacent nodes based on the surrounding context.
sent12: Apart from that, MLM can also be adapted to multi-modal data called Masked Multimodal Modelling (MMM) (Wu et al., 2022a).
sent13: MMM was performed by predicting the semantics of masked news and news image regions given the unmasked inputs and indicating whether a news image and news content segment correspond to each other for news recommendation purposes.
sent14: The NSP/SOP can be adapted for CTR prediction as Next K Behaviors Prediction (NBP).
sent15: NBP was proposed to learn user representations in the pre-training stage by inferring whether a candidate behavior is the next i-th behavior of the target user based on their past N behaviors.
sent16: NBP can also capture the relatedness between past and multiple future behaviors.","1. What are some objectives that draw inspiration from LM objectives and are applied to specific downstream tasks in recommendation systems? sent1
2. How are sequential recommendations typically modeled? sent2
    2.1. How did Xiao et al. (2022) model user behavior for next-click prediction? sent3
    2.2. Why might sequential dependency not always hold in user preferences for recommendations, and how can MLM objectives be modified? sent4
        2.2.1. How did Yuan et al. (2020b) modify MLM objectives for recommendations? sent5
3. Can autoregressive learning tasks be adapted to other types of data? sent6
    3.1. How did Geng et al. (2022b) use autoregressive modeling for recommendations? sent7
    3.2. What task did Zhao (2022) propose for learning sequence-level information of user interaction history? sent8
4. How can MLM be adapted for graph representations in recommendations? sent9
    4.1. What is Masked Node Prediction (MNP) and how is it performed? sent10
    4.2. What is Masked Edge Prediction (MEP) and how is it performed? sent11
5. How can MLM be adapted to multi-modal data? sent12
    5.1. What is Masked Multimodal Modelling (MMM) and how is it performed? sent13
6. How can NSP/SOP be adapted for CTR prediction? sent14
    6.1. What is Next K Behaviors Prediction (NBP) and how does it function? sent15
    6.2. What additional capability does NBP have? sent16","2. How are sequential recommendations typically modeled? sent2
    2.1. How did Xiao et al. (2022) model user behavior for next-click prediction? sent3
    2.2. Why might sequential dependency not always hold in user preferences for recommendations, and how can MLM objectives be modified? sent4
        2.2.1. How did Yuan et al. (2020b) modify MLM objectives for recommendations? sent5","2. How are sequential recommendations typically modeled? sent2
    2.1. How did Xiao et al. (2022) model user behavior for next-click prediction? sent3
    2.2. Why might sequential dependency not always hold in user preferences for recommendations, and how can MLM objectives be modified? sent4
        2.2.1. How did Yuan et al. (2020b) modify MLM objectives for recommendations? sent5","Question: How can MLM objectives be adapted to account for non-sequential dependencies in user preferences for recommendations?

Supporting sentence: sent4, sent5",How can MLM objectives be adapted to account for non-sequential dependencies in user preferences for recommendations?,"sent4, sent5",4,implement,"MLM objectives can be adapted to account for non-sequential dependencies in user preferences for recommendations by modifying the sequential dependency approach.

Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training.

MLM, also known as Cloze Prediction, can be adapted to learn graph representations for different recommendation purposes.

This involves pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives.

Specifically, MNP is performed by randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node.

Similarly, MEP is used to recover the masked edge of two adjacent nodes based on the surrounding context.","Question:

How can MLM objectives be adapted to account for non-sequential dependencies in user preferences for recommendations?

Answer:

MLM objectives can be adapted to account for non-sequential dependencies in user preferences for recommendations by modifying the sequential dependency approach.

Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training.

MLM, also known as Cloze Prediction, can be adapted to learn graph representations for different recommendation purposes.

This involves pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives.

Specifically, MNP is performed by randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node.

Similarly, MEP is used to recover the masked edge of two adjacent nodes based on the surrounding context."
254877753,Towards Reasoning in Large Language Models: A Survey,https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,Fully Supervised Finetuning,5,"Before discussing reasoning in large language models, it is worth mentioning there is research working on eliciting/improving reasoning in small language models through fully supervised finetuning on specific datasets. For example, Rajani et al.

(2019) finetune a pretrained GPT model (Radford et al., 2018) to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks (Talmor et al., 2019). Talmor et al. (2020) train RoBERTa (Liu et al., 2019) to perform reasoning/inference based on both implicit pre-trained knowledge and explicit free-text statements. Hendrycks et al. (2021) finetune pretrained 2 It is important to note that the term ""reasoning"" in this paper does not necessarily imply that LLMs are truly capable of reasoning or that they are able to reason in the same way that humans do. We will discuss this issue in more detail in §6. language models to solve competition mathematics problems by generating full step-by-step solutions, though the accuracy is relatively low. Nye et al. (2022) train language models to do multi-step reasoning for program synthesis/execution by generating ""scratchpads"", i.e., intermediate computations, before producing the final answers. We refer the reader to Helwe et al. (2021); Bhargava and Ng (2022)'s survey for more studies in this line.

There are two major limitations of fully supervised finetuning. First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create. Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions.","sent1: Before discussing reasoning in large language models, it is worth mentioning there is research working on eliciting/improving reasoning in small language models through fully supervised finetuning on specific datasets.
sent2: For example, Rajani et al.(2019) finetune a pretrained GPT model (Radford et al., 2018) to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks (Talmor et al., 2019).
sent3: Talmor et al. (2020) train RoBERTa (Liu et al., 2019) to perform reasoning/inference based on both implicit pre-trained knowledge and explicit free-text statements.
sent4: Hendrycks et al. (2021) finetune pretrained 2
sent5: It is important to note that the term ""reasoning"" in this paper does not necessarily imply that LLMs are truly capable of reasoning or that they are able to reason in the same way that humans do.
sent6: We will discuss this issue in more detail in §6.
sent7: language models to solve competition mathematics problems by generating full step-by-step solutions, though the accuracy is relatively low.
sent8: Nye et al. (2022) train language models to do multi-step reasoning for program synthesis/execution by generating ""scratchpads"", i.e., intermediate computations, before producing the final answers.
sent9: We refer the reader to Helwe et al. (2021); Bhargava and Ng (2022)'s survey for more studies in this line.
sent10: There are two major limitations of fully supervised finetuning.
sent11: First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create.
sent12: Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions.","1. What is the focus of research before discussing reasoning in large language models? sent1
    1.1. Can you provide an example of finetuning a pretrained model to improve reasoning? sent2
    1.2. How is RoBERTa trained to perform reasoning/inference? sent3
    1.3. What is the approach used by Hendrycks et al. (2021) for solving competition mathematics problems? sent7
    1.4. How do Nye et al. (2022) train language models for multi-step reasoning? sent8
    1.5. Where can more studies on this line of research be found? sent9
2. What is important to note about the term ""reasoning"" in the context of this paper? sent5
    2.1. Where will this issue be discussed in more detail? sent6
3. What are the two major limitations of fully supervised finetuning? sent10
    3.1. What is the first limitation of fully supervised finetuning? sent11
    3.2. What is the second limitation of fully supervised finetuning? sent12","1. What is the focus of research before discussing reasoning in large language models? sent1
    1.1. Can you provide an example of finetuning a pretrained model to improve reasoning? sent2
    1.2. How is RoBERTa trained to perform reasoning/inference? sent3
    1.3. What is the approach used by Hendrycks et al. (2021) for solving competition mathematics problems? sent7
    1.4. How do Nye et al. (2022) train language models for multi-step reasoning? sent8
    1.5. Where can more studies on this line of research be found? sent9","1. What is the focus of research before discussing reasoning in large language models? sent1
    1.1. Can you provide an example of finetuning a pretrained model to improve reasoning? sent2
    1.2. How is RoBERTa trained to perform reasoning/inference? sent3
    1.3. What is the approach used by Hendrycks et al. (2021) for solving competition mathematics problems? sent7
    1.4. How do Nye et al. (2022) train language models for multi-step reasoning? sent8
    1.5. Where can more studies on this line of research be found? sent9","Question: What are the limitations of fully supervised finetuning for reasoning in language models?

Supporting sentence: sent10, sent11, sent12",What are the limitations of fully supervised finetuning for reasoning in language models?,"sent10, sent11, sent12",4,implement,"There are two major limitations of fully supervised finetuning.

First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create.

Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions.","Question:

What are the limitations of fully supervised finetuning for reasoning in language models?

Answer:

There are two major limitations of fully supervised finetuning.

First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create.

Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions."
256231532,Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,https://www.semanticscholar.org/paper/a1275e92f4830e5bbd53bc1b1fa44a9a2f024f88,Strategy Representation,4,"Implicit: The most obvious way to represent the system strategy is implicitly, without any intended decoupling between system strategy and response realization. This corresponds to the usual sequenceto-sequence framework that has been a standard baseline for the methods developed in this space. An important example is the work by Lewis et al. (2017), who were one of the first works to train endto-end dialogue models that exhibit social influence. The authors employed a neural network based on GRUs, one for encoding the negotiation context, one to encode the dialogue utterances, and two recurrent units to generate the output agreement in a bidirectional manner. Latent vectors: Yarats and Lewis (2018) explored latent vectors to decouple utterance semantics from its linguistic aspects. Their hierarchical approach first constructs a latent vector from the input message, which is then used for response generation and planning. These latent vectors are trained to maximize the likelihood of future dialogue messages and actions, which enables the decoupling between semantics and realization. Dialogue Acts (DAs): Dialogue Acts, such as greeting, offer propose, agreement, or disagreement, are effective at capturing a high-level structure of the dialogue flow in social influence settings, reducing the model strategy to first predicting the dialogue act for the next response. The use of DAs makes it convenient to apply reinforcement learning approaches (Zhang et al., 2020b;Yang et al., 2021), while also aiding in developing a modular dialogue system design (He et al., 2018). Semantic Strategies: The structural properties ex-  ","sent1: Implicit: The most obvious way to represent the system strategy is implicitly, without any intended decoupling between system strategy and response realization.
sent2: This corresponds to the usual sequenceto-sequence framework that has been a standard baseline for the methods developed in this space.
sent3: An important example is the work by Lewis et al. (2017), who were one of the first works to train endto-end dialogue models that exhibit social influence.
sent4: The authors employed a neural network based on GRUs, one for encoding the negotiation context, one to encode the dialogue utterances, and two recurrent units to generate the output agreement in a bidirectional manner.
sent5: Latent vectors: Yarats and Lewis (2018) explored latent vectors to decouple utterance semantics from its linguistic aspects.
sent6: Their hierarchical approach first constructs a latent vector from the input message, which is then used for response generation and planning.
sent7: These latent vectors are trained to maximize the likelihood of future dialogue messages and actions, which enables the decoupling between semantics and realization.
sent8: Dialogue Acts (DAs): Dialogue Acts, such as greeting, offer propose, agreement, or disagreement, are effective at capturing a high-level structure of the dialogue flow in social influence settings, reducing the model strategy to first predicting the dialogue act for the next response.
sent9: The use of DAs makes it convenient to apply reinforcement learning approaches (Zhang et al., 2020b;Yang et al., 2021), while also aiding in developing a modular dialogue system design (He et al., 2018).
sent10: Semantic Strategies: The structural properties ex-","1. How can the system strategy be represented implicitly in dialogue systems? sent1
    1.1. What framework corresponds to the implicit representation of system strategy? sent2
    1.2. Can you provide an example of a work that uses implicit strategy representation in dialogue models? sent3
        1.2.1. What neural network architecture was employed by Lewis et al. (2017) for their dialogue model? sent4
2. How do latent vectors contribute to strategy representation in dialogue systems? sent5
    2.1. What is the hierarchical approach used by Yarats and Lewis (2018) for latent vectors? sent6
    2.2. How are latent vectors trained to decouple semantics from realization? sent7
3. What role do Dialogue Acts (DAs) play in strategy representation for dialogue systems? sent8
    3.1. How do Dialogue Acts facilitate the use of reinforcement learning and modular design in dialogue systems? sent9","1. How can the system strategy be represented implicitly in dialogue systems? sent1
    1.1. What framework corresponds to the implicit representation of system strategy? sent2
    1.2. Can you provide an example of a work that uses implicit strategy representation in dialogue models? sent3
        1.2.1. What neural network architecture was employed by Lewis et al. (2017) for their dialogue model? sent4","1. How can the system strategy be represented implicitly in dialogue systems? sent1
    1.1. What framework corresponds to the implicit representation of system strategy? sent2
    1.2. Can you provide an example of a work that uses implicit strategy representation in dialogue models? sent3
        1.2.1. What neural network architecture was employed by Lewis et al. (2017) for their dialogue model? sent4","Question: How can latent vectors be used to decouple semantics from linguistic aspects in dialogue systems?

Supporting sentence: sent5, sent6, sent7",How can latent vectors be used to decouple semantics from linguistic aspects in dialogue systems?,"sent5, sent6, sent7",4,implement,"Yarats and Lewis (2018) explored latent vectors to decouple utterance semantics from its linguistic aspects.

Their hierarchical approach first constructs a latent vector from the input message, which is then used for response generation and planning.

These latent vectors are trained to maximize the likelihood of future dialogue messages and actions, which enables the decoupling between semantics and realization.","Question:

How can latent vectors be used to decouple semantics from linguistic aspects in dialogue systems?

Answer:

Yarats and Lewis (2018) explored latent vectors to decouple utterance semantics from its linguistic aspects.

Their hierarchical approach first constructs a latent vector from the input message, which is then used for response generation and planning.

These latent vectors are trained to maximize the likelihood of future dialogue messages and actions, which enables the decoupling between semantics and realization."
254877175,A Survey of Deep Learning for Mathematical Reasoning,https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,High-quality Reasoning Chains,14,"Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

6 Discussion and Findings

Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

6 Discussion and Findings","sent1: Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt.
sent2: However, manually creating reasoning chains has two disadvantages.
sent3: First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks.
sent4: Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer.
sent5: To address this limitation, recent studies mainly fo-
sent6: Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex
sent7: (175B) Clustering Language Auto-generated -Complexity-
sent8: CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT
sent9: (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks.
sent10: In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.
sent11: Similarly , an alternative solution that uses large language models to express the reasoning process as a program.
sent12: The computation is then relegated to an external computer, which executes the generated programs to derive the answer.
sent13: A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a).
sent14: Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer.
sent15: In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.
sent16: 6 Discussion and FindingsEarly chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt.
sent17: However, manually creating reasoning chains has two disadvantages.
sent18: First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks.
sent19: Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer.
sent20: To address this limitation, recent studies mainly fo-
sent21: Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex
sent22: (175B) Clustering Language Auto-generated -Complexity-
sent23: CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT
sent24: (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks.
sent25: In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.
sent26: Similarly , an alternative solution that uses large language models to express the reasoning process as a program.
sent27: The computation is then relegated to an external computer, which executes the generated programs to derive the answer.
sent28: A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a).
sent29: Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer.
sent30: In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.
sent31: 6 Discussion and Findings","1. What does early chain of thought work rely on? sent1
2. What are the disadvantages of manually creating reasoning chains? sent2
    2.1. Why might current models struggle with complex tasks? sent3
    2.2. What is a vulnerability of a single decoding process? sent4
3. How do recent studies address the limitations of manually created reasoning chains? sent5
4. What are process-based approaches in improving chain-of-thought reasoning? sent9
    4.1. How is least-to-most prompting implemented? sent10
    4.2. What is an alternative solution using large language models? sent11, sent12
    4.3. What does the Chameleon work integrate to enhance LLMs? sent13
5. What are outcome-based approaches in reasoning? sent13
    5.1. How does self-consistency work in generating reasoning paths? sent14
    5.2. What does Li et al. (2022a) propose to increase diversity in reasoning paths? sent15","4. What are process-based approaches in improving chain-of-thought reasoning? sent9
    4.1. How is least-to-most prompting implemented? sent10
    4.2. What is an alternative solution using large language models? sent11, sent12
    4.3. What does the Chameleon work integrate to enhance LLMs? sent13","4. What are process-based approaches in improving chain-of-thought reasoning? sent9
    4.1. How is least-to-most prompting implemented? sent10
    4.2. What is an alternative solution using large language models? sent11, sent12
    4.3. What does the Chameleon work integrate to enhance LLMs? sent13","Question: How do process-based approaches aim to improve chain-of-thought reasoning quality for complex tasks?

Supporting sentence: sent9, sent10",How do process-based approaches aim to improve chain-of-thought reasoning quality for complex tasks?,"sent9, sent10",4,implement,"Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks.

In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.

Similarly, an alternative solution uses large language models to express the reasoning process as a program.

The computation is then relegated to an external computer, which executes the generated programs to derive the answer.

A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.","Question:

How do process-based approaches aim to improve chain-of-thought reasoning quality for complex tasks?

Answer:

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks.

In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.

Similarly, an alternative solution uses large language models to express the reasoning process as a program.

The computation is then relegated to an external computer, which executes the generated programs to derive the answer.

A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning."
259108815,Mapping Brains with Language Models: A Survey,https://www.semanticscholar.org/paper/a92c82231c0ea7027ab20b25fe5f82565047aad7,Performance Metrics,30,"We present the evaluation metrics used in the above studies and discuss how they relate. See Table 2 for a summary of metrics and corresponding studies. Mitchell et al. (2008) introduce pairwise matching accuracy. Because of their small sample size, they use a leave-2-out cross-validation, which later work also adopted. The metric is a binary classification accuracy metric on a balanced dataset, so a random baseline converges toward 0.5. Many studies have relied on this metric, both in encoding and decoding (see Table 2). 4 Pearson correlationPearson correlation is another widely used metric in the studies surveyed above, measuring the linear relationship between variables, and providing insight into the strength and direction of their association. Huth et al. (2016), compute Pearson correlation between predicted and actual brain responses using Gaussian random vectors to test statistical significance. Resulting p-values are corrected for multiple comparisons within each subject using false discovery rate (FDR) (Benjamini and Hochberg, 1995). Others have used Bonferroni correction (Huth et al., 2016) or block-wise permutation test (Adolf et al., 2014) to evaluate the statistical significance of the correlation (Zhang et al., 2020). Some report R 2 (explained variance) instead of or in addition to correlation coefficients (Minnema and Herbelot, 2019;Reddy and Wehbe, 2021). Others have adopted a more elaborate extension of Pearson correlation, namely BrainScore (Schrimpf et al., 2018). Brain-Score is estimated on held-out test data, calculating Pearson's correlation between model predictions and neural recordings divided by the estimated ceiling and averaged across voxels and participants.

Percentile rank was first used for encoding (Mitchell et al., 2008), but can also be used for decoding (Pereira et al., 2018;Gauthier and Levy, 2019;Minnema and Herbelot, 2019). In encoding, the predicted brain image for w is ranked along the predicted images for a set of candidate words w ′ by their similarity to the real (ground truth) image for w. The average rank is then reported. For decoding, they rank word vectors rather than neural response images. Note the similarity metric is unspecified, but typically cosine distance is used.

Mean squared error, the average of the squared differences between word vectors and neural responses, was first used for encoding in Søgaard (2016) on a held-out test split. It was also used by Gauthier and Levy (2019).

Representational similarity analysis (RSA) was introduced in Kriegeskorte et al. (2008) as a non-parametric way to characterize structural alignment between the geometries of representations derived from disparate modalities. RSA abstracts away from activity patterns themselves and instead computes representational similarity matrices (RSMs), which characterize the information carried by a given representation method through global similarity structure. A rank correlation coefficient is computed between RSMs derived from the two spaces, providing a summary statistic indicative of the overall representational alignment between them. Being non-parametric, RSA circumvents many of the various methodological weaknesses (such as over fitting, etc.). Gauthier and Levy (2019), Minnema and Herbelot (2019), and  apply (variations of) RSA to investigate the relations between different model components, and then to study the alignment of these components with brain response.

Cosine similarity was used in Mitchell et al. (2008) to select between the candidate images in pairwise matching accuracy, as well as in percentile rank and RSA, but the raw cosine similarities between predicted and real images or embeddings can also be used as a metric. Minnema and Herbelot (2019) use this metric to quantify how close the predicted word vectors are to the target. Finally, Zou et al. (2022) use precision@k, a standard metric in other mapping problems, e.g., cross-lingual word embeddings (Søgaard et al., 2019).

Comparisons Most metrics are used to evaluate both encoding and decoding models (pairwise matching accuracy, Pearson correlation, percentile rank, MSE, RSA, cosine distance). Results for two of the most widely used metrics -pairwise matching accuracy 5 and percentile rank -tend to be around 0.7-0.8 with generally better results for more recent architectures and larger LMs. To draw conclusions across studies relying on different metrics, we need to investigate which metrics are more conservative, and how different metrics relate.

Pairwise matching accuracy vs. Pearson correlation It seems that pairwise matching accuracy tends to increase monotonically with Pearson correlation. Consider three sets of distances over corresponding point sets, A, B, and C. If A and B are more strongly linearly correlated than A and C, under an optimal linear mapping Ω (minimizing point-wise squared error distance), E[(a − bΩ) 2 ] > E[(a − cΩ) 2 ]. Even in this conservative setting in our synthetic experiments in Appendix A.1, the correlation between matching accuracy and percentile rank was very high,~0.9.

Pairwise matching accuracy vs. percentile rank Both metrics have random baseline scores of 0.5, and they will converge in the limit. If a has a percentile rank of p in a list A, it will be higher than a random member of A p percent of the time. In our experiments in Appendix A.1, the correlation converges toward 1.0, with values consistently higher than 0.8 for N = 100.

Pairwise matching accuracy vs. precision@k are also positively correlated. Perfect score in one entails perfect score in the other, but precision@k can of course be very small for very high values of pairwise matching accuracy (especially if the set of candidate words is big). Conversely, we can have 5 When discriminating averages over 20 images (Wehbe et al., 2014b), scores are naturally lower. saturation for high values of k, because matching accuracies higher than n−k n will mean near-perfect precision@k scores. In practice, precision@k (for low values of k) will be much more conservative, however. The correlation coefficient for N = 100 (see Appendix A.1) tends to lie around 0.7.

Relative strength Pairwise Matching Accuracy is a relatively permissible performance metric. To see this, consider the scenario in which all target words can be divided into two equal-sized buckets based on word length (number of characters). Say the neural responses capture nothing but this binary distinction between long and short words, but do so perfectly. Moreover, our mapping method, e.g., linear regression, learns this from training data. Now, from this alone, the pairwise matching accuracy will converge toward µ = 0.75, since our model will do perfectly (1.0) on half of the data, and exhibit random performance (0.5) on the other half. If the neural responses tracked word length (and not just the distinction between short and long words), performance would be even better. In other words, Pairwise Matching Accuracy scores around 0.7-0.8 (observed in the studies above) may only reflect very shallow processing characteristics. The fact that Minnema and Herbelot (2019) only observed good results with this metric, led them to adopt a rather critical stance, for good reasons.

Other metrics are clearly more conservative. For a set of n candidate words, a random mapping will induce a precision@1-score of 1 n . While hubs may inflate scores for larger values, the metric is extremely conservative for small values of k. However, only Zou et al. (2022) use this metric, and they modify the experimental protocol substantially, making the task much easier by providing additional input to a non-linear model. The small improvement from adding neural response input is interesting, but could potentially be explained by shallow processing characteristics.

They argue that analogy testing would provide a better evaluation protocol: one would ideally use standard metrics such as semantic relatedness judgment tasks, analogy tasks, etc.

[but] this is not possible due to the limited vocabulary sizes of the available brain datasets Such evaluation is possible on small scale, though, and increasingly larger fMRI datasets are becoming available (see above). Zhang et al. (2020) have identified analogical reasoning in fMRI brain activation spaces. The analogies are computed using vector offset and probe the systematicity of how semantic relations are encoded. If a model encodes the capital-of relation systematically, we can retrieve the capital of Germany by subtracting the fMRI vector for 'Paris' from the sum of our the fMRI vectors for Germany and France. This is the same kind of analogical reasoning found in language models (Mikolov et al., 2013). Garneau et al. (2021) show that the more language models satisfy analogies, the more isomorphic they are.

So far, it seems that, with the possible exception of Zhang et al. (2020), there is little evidence for structural similarities, beyond what could be induced by shallow processing characteristics, but what about all the studies that report strong Pearson correlations? Per-voxel correlation coefficients are low on average, but across the above studies, typically only around 4-40% of the voxels exhibit significant correlations (Huth et al., 2016;. Since these correlations have been replicated across different datasets, they are generally not disputed, but could still reflect rather shallow processing characteristics.

On a more positive note, several studies show that larger (and better) language models align better with neural response measurements (Schrimpf et al., 2021;. This suggests that language models in the future may align even better with such measurements, possibly reflecting properties of deep processing. Such correlations with model quality and size are positive, making the results reported above more credible.

Generally, the conclusions we can draw from the above studies are somewhat vague. There are two reasons for this: (i) Past studies have relied on permissible (pairwise matching accuracy) and ambiguous (Pearson correlation) performance metrics; and (ii) past studies have relied on small-sized datasets. We believe that this calls for a meta-analysis of the above studies. To provide grounds for such a meta-analysis, we have in this section taken steps to compare the metrics used in these studies. We leave it for future work to explore various ways effect sizes can be computed across these studies.","sent1: We present the evaluation metrics used in the above studies and discuss how they relate.
sent2: See Table 2 for a summary of metrics and corresponding studies.
sent3: Mitchell et al. (2008) introduce pairwise matching accuracy.
sent4: Because of their small sample size, they use a leave-2-out cross-validation, which later work also adopted.
sent5: The metric is a binary classification accuracy metric on a balanced dataset, so a random baseline converges toward 0.5.
sent6: Many studies have relied on this metric, both in encoding and decoding (see Table 2).
sent7: 4 Pearson correlationPearson correlation is another widely used metric in the studies surveyed above, measuring the linear relationship between variables, and providing insight into the strength and direction of their association.
sent8: Huth et al. (2016), compute Pearson correlation between predicted and actual brain responses using Gaussian random vectors to test statistical significance.
sent9: Resulting p-values are corrected for multiple comparisons within each subject using false discovery rate (FDR) (Benjamini and Hochberg, 1995).
sent10: Others have used Bonferroni correction (Huth et al., 2016) or block-wise permutation test (Adolf et al., 2014) to evaluate the statistical significance of the correlation (Zhang et al., 2020).
sent11: Some report R 2 (explained variance) instead of or in addition to correlation coefficients (Minnema and Herbelot, 2019;Reddy and Wehbe, 2021).
sent12: Others have adopted a more elaborate extension of Pearson correlation, namely BrainScore (Schrimpf et al., 2018).
sent13: Brain-Score is estimated on held-out test data, calculating Pearson's correlation between model predictions and neural recordings divided by the estimated ceiling and averaged across voxels and participants.
sent14: Percentile rank was first used for encoding (Mitchell et al., 2008), but can also be used for decoding (Pereira et al., 2018;Gauthier and Levy, 2019;Minnema and Herbelot, 2019).
sent15: In encoding, the predicted brain image for w is ranked along the predicted images for a set of candidate words w ′ by their similarity to the real (ground truth) image for w.
sent16: The average rank is then reported.
sent17: For decoding, they rank word vectors rather than neural response images.
sent18: Note the similarity metric is unspecified, but typically cosine distance is used.
sent19: Mean squared error, the average of the squared differences between word vectors and neural responses, was first used for encoding in Søgaard (2016) on a held-out test split.
sent20: It was also used by Gauthier and Levy (2019).
sent21: Representational similarity analysis (RSA) was introduced in Kriegeskorte et al. (2008) as a non-parametric way to characterize structural alignment between the geometries of representations derived from disparate modalities.
sent22: RSA abstracts away from activity patterns themselves and instead computes representational similarity matrices (RSMs), which characterize the information carried by a given representation method through global similarity structure.
sent23: A rank correlation coefficient is computed between RSMs derived from the two spaces, providing a summary statistic indicative of the overall representational alignment between them.
sent24: Being non-parametric, RSA circumvents many of the various methodological weaknesses (such as over fitting, etc.).
sent25: Gauthier and Levy (2019), Minnema and Herbelot (2019), and  apply (variations of) RSA to investigate the relations between different model components, and then to study the alignment of these components with brain response.
sent26: Cosine similarity was used in Mitchell et al. (2008) to select between the candidate images in pairwise matching accuracy, as well as in percentile rank and RSA, but the raw cosine similarities between predicted and real images or embeddings can also be used as a metric.
sent27: Minnema and Herbelot (2019) use this metric to quantify how close the predicted word vectors are to the target.
sent28: Finally, Zou et al. (2022) use precision@k, a standard metric in other mapping problems, e.g., cross-lingual word embeddings (Søgaard et al., 2019).
sent29: Comparisons Most metrics are used to evaluate both encoding and decoding models (pairwise matching accuracy, Pearson correlation, percentile rank, MSE, RSA, cosine distance).
sent30: Results for two of the most widely used metrics -pairwise matching accuracy 5 and percentile rank -tend to be around 0.7-0.8 with generally better results for more recent architectures and larger LMs.
sent31: To draw conclusions across studies relying on different metrics, we need to investigate which metrics are more conservative, and how different metrics relate.
sent32: Pairwise matching accuracy vs. Pearson correlation It seems that pairwise matching accuracy tends to increase monotonically with Pearson correlation.
sent33: Consider three sets of distances over corresponding point sets, A, B, and C. If A and B are more strongly linearly correlated than A and C, under an optimal linear mapping Ω (minimizing point-wise squared error distance), E[(a − bΩ) 2 ] > E[(a − cΩ) 2 ].
sent34: Even in this conservative setting in our synthetic experiments in Appendix A.1, the correlation between matching accuracy and percentile rank was very high,~0.9.
sent35: Pairwise matching accuracy vs. percentile rank Both metrics have random baseline scores of 0.5, and they will converge in the limit.
sent36: If a has a percentile rank of p in a list A, it will be higher than a random member of A p percent of the time.
sent37: In our experiments in Appendix A.1, the correlation converges toward 1.0, with values consistently higher than 0.8 for N = 100.
sent38: Pairwise matching accuracy vs. precision@k are also positively correlated.
sent39: Perfect score in one entails perfect score in the other, but precision@k can of course be very small for very high values of pairwise matching accuracy (especially if the set of candidate words is big).
sent40: Conversely, we can have 5 When discriminating averages over 20 images (Wehbe et al., 2014b), scores are naturally lower.
sent41: saturation for high values of k, because matching accuracies higher than n−k n will mean near-perfect precision@k scores.
sent42: In practice, precision@k (for low values of k) will be much more conservative, however.
sent43: The correlation coefficient for N = 100 (see Appendix A.1) tends to lie around 0.7.
sent44: Relative strength Pairwise Matching Accuracy is a relatively permissible performance metric.
sent45: To see this, consider the scenario in which all target words can be divided into two equal-sized buckets based on word length (number of characters).
sent46: Say the neural responses capture nothing but this binary distinction between long and short words, but do so perfectly.
sent47: Moreover, our mapping method, e.g., linear regression, learns this from training data.
sent48: Now, from this alone, the pairwise matching accuracy will converge toward µ = 0.75, since our model will do perfectly (1.0) on half of the data, and exhibit random performance (0.5) on the other half.
sent49: If the neural responses tracked word length (and not just the distinction between short and long words), performance would be even better.
sent50: In other words, Pairwise Matching Accuracy scores around 0.7-0.8 (observed in the studies above) may only reflect very shallow processing characteristics.
sent51: The fact that Minnema and Herbelot (2019) only observed good results with this metric, led them to adopt a rather critical stance, for good reasons.
sent52: Other metrics are clearly more conservative.
sent53: For a set of n candidate words, a random mapping will induce a precision@1-score of 1 n .
sent54: While hubs may inflate scores for larger values, the metric is extremely conservative for small values of k.
sent55: However, only Zou et al. (2022) use this metric, and they modify the experimental protocol substantially, making the task much easier by providing additional input to a non-linear model.
sent56: The small improvement from adding neural response input is interesting, but could potentially be explained by shallow processing characteristics.
sent57: They argue that analogy testing would provide a better evaluation protocol: one would ideally use standard metrics such as semantic relatedness judgment tasks, analogy tasks, etc.
sent58: [but] this is not possible due to the limited vocabulary sizes of the available brain datasets Such evaluation is possible on small scale, though, and increasingly larger fMRI datasets are becoming available (see above).
sent59: Zhang et al. (2020) have identified analogical reasoning in fMRI brain activation spaces.
sent60: The analogies are computed using vector offset and probe the systematicity of how semantic relations are encoded.
sent61: If a model encodes the capital-of relation systematically, we can retrieve the capital of Germany by subtracting the fMRI vector for 'Paris' from the sum of our the fMRI vectors for Germany and France.
sent62: This is the same kind of analogical reasoning found in language models (Mikolov et al., 2013).
sent63: Garneau et al. (2021) show that the more language models satisfy analogies, the more isomorphic they are.
sent64: So far, it seems that, with the possible exception of Zhang et al. (2020), there is little evidence for structural similarities, beyond what could be induced by shallow processing characteristics, but what about all the studies that report strong Pearson correlations?
sent65: Per-voxel correlation coefficients are low on average, but across the above studies, typically only around 4-40% of the voxels exhibit significant correlations (Huth et al., 2016;.
sent66: Since these correlations have been replicated across different datasets, they are generally not disputed, but could still reflect rather shallow processing characteristics.
sent67: On a more positive note, several studies show that larger (and better) language models align better with neural response measurements (Schrimpf et al., 2021;.
sent68: This suggests that language models in the future may align even better with such measurements, possibly reflecting properties of deep processing.
sent69: Such correlations with model quality and size are positive, making the results reported above more credible.
sent70: Generally, the conclusions we can draw from the above studies are somewhat vague.
sent71: There are two reasons for this: (i) Past studies have relied on permissible (pairwise matching accuracy) and ambiguous (Pearson correlation) performance metrics; and (ii) past studies have relied on small-sized datasets.
sent72: We believe that this calls for a meta-analysis of the above studies.
sent73: To provide grounds for such a meta-analysis, we have in this section taken steps to compare the metrics used in these studies.
sent74: We leave it for future work to explore various ways effect sizes can be computed across these studies.","1. What is the focus of the section on performance metrics? sent1
    1.1. Where can a summary of metrics and corresponding studies be found? sent2
2. What metric did Mitchell et al. (2008) introduce, and what is its significance? sent3
    2.1. Why did they use leave-2-out cross-validation? sent4
    2.2. What is the baseline for this metric? sent5
    2.3. How widely has this metric been used? sent6
3. What is Pearson correlation, and how is it used in the studies? sent7
    3.1. How did Huth et al. (2016) use Pearson correlation? sent8
    3.2. What methods are used to evaluate the statistical significance of the correlation? sent9, sent10
4. What is R², and how is it used in the studies? sent11
5. What is Brain-Score, and how is it calculated? sent12, sent13
6. How is percentile rank used in encoding and decoding? sent14, sent15, sent16, sent17
    6.1. What similarity metric is typically used with percentile rank? sent18
7. What is mean squared error, and how has it been used in the studies? sent19, sent20
8. What is Representational Similarity Analysis (RSA), and how is it applied? sent21, sent22, sent23, sent24
    8.1. How have Gauthier and Levy (2019) and Minnema and Herbelot (2019) used RSA? sent25
9. How is cosine similarity used in the studies? sent26, sent27
10. What is precision@k, and how is it used in the studies? sent28
11. How are different metrics compared in terms of their use in encoding and decoding models? sent29
    11.1. What are the typical results for pairwise matching accuracy and percentile rank? sent30
    11.2. What is needed to draw conclusions across studies using different metrics? sent31
12. How do pairwise matching accuracy and Pearson correlation relate? sent32, sent33
13. How do pairwise matching accuracy and percentile rank relate? sent34, sent35, sent36, sent37
14. How do pairwise matching accuracy and precision@k relate? sent38, sent39, sent40, sent41, sent42, sent43
15. What is the relative strength of Pairwise Matching Accuracy as a performance metric? sent44, sent45, sent46, sent47, sent48, sent49, sent50, sent51
16. How do other metrics compare in terms of conservativeness? sent52, sent53, sent54, sent55, sent56
17. What alternative evaluation protocol is suggested for better evaluation? sent57, sent58
18. What evidence is there for analogical reasoning in fMRI brain activation spaces? sent59, sent60, sent61, sent62
    18.1. How do language models relate to analogical reasoning? sent63
19. What is the current evidence for structural similarities in language models? sent64, sent65, sent66
20. How do larger language models align with neural response measurements? sent67, sent68, sent69
21. What are the general conclusions drawn from the studies, and what are the reasons for their vagueness? sent70, sent71
22. What is suggested for future work in this area? sent72, sent73, sent74","2. What metric did Mitchell et al. (2008) introduce, and what is its significance? sent3
    2.1. Why did they use leave-2-out cross-validation? sent4
    2.2. What is the baseline for this metric? sent5
    2.3. How widely has this metric been used? sent6

3. What is Pearson correlation, and how is it used in the studies? sent7
    3.1. How did Huth et al. (2016) use Pearson correlation? sent8
    3.2. What methods are used to evaluate the statistical significance of the correlation? sent9, sent10","2. What metric did Mitchell et al. (2008) introduce, and what is its significance? sent3
    2.1. Why did they use leave-2-out cross-validation? sent4
    2.2. What is the baseline for this metric? sent5
    2.3. How widely has this metric been used? sent6","Question: How is pairwise matching accuracy used and evaluated in brain-language model studies?

Supporting sentence: sent3, sent4, sent5, sent6",How is pairwise matching accuracy used and evaluated in brain-language model studies?,"sent3, sent4, sent5, sent6",2,implement,"Pairwise matching accuracy is introduced by Mitchell et al. (2008) as a binary classification accuracy metric on a balanced dataset, where a random baseline converges toward 0.5.

Because of their small sample size, they use a leave-2-out cross-validation, which later work also adopted.

Many studies have relied on this metric, both in encoding and decoding.

Pairwise matching accuracy tends to increase monotonically with Pearson correlation.

Both pairwise matching accuracy and percentile rank have random baseline scores of 0.5, and they will converge in the limit.

Pairwise matching accuracy is a relatively permissible performance metric, as scores around 0.7-0.8 may only reflect very shallow processing characteristics.

The fact that Minnema and Herbelot (2019) only observed good results with this metric led them to adopt a rather critical stance.","Question:

How is pairwise matching accuracy used and evaluated in brain-language model studies?

Answer:

Pairwise matching accuracy is introduced by Mitchell et al. (2008) as a binary classification accuracy metric on a balanced dataset, where a random baseline converges toward 0.5.

Because of their small sample size, they use a leave-2-out cross-validation, which later work also adopted.

Many studies have relied on this metric, both in encoding and decoding.

Pairwise matching accuracy tends to increase monotonically with Pearson correlation.

Both pairwise matching accuracy and percentile rank have random baseline scores of 0.5, and they will converge in the limit.

Pairwise matching accuracy is a relatively permissible performance metric, as scores around 0.7-0.8 may only reflect very shallow processing characteristics.

The fact that Minnema and Herbelot (2019) only observed good results with this metric led them to adopt a rather critical stance."
259108815,Mapping Brains with Language Models: A Survey,https://www.semanticscholar.org/paper/a92c82231c0ea7027ab20b25fe5f82565047aad7,Performance Metrics,30,"We present the evaluation metrics used in the above studies and discuss how they relate. See Table 2 for a summary of metrics and corresponding studies. Mitchell et al. (2008) introduce pairwise matching accuracy. Because of their small sample size, they use a leave-2-out cross-validation, which later work also adopted. The metric is a binary classification accuracy metric on a balanced dataset, so a random baseline converges toward 0.5. Many studies have relied on this metric, both in encoding and decoding (see Table 2). 4 Pearson correlationPearson correlation is another widely used metric in the studies surveyed above, measuring the linear relationship between variables, and providing insight into the strength and direction of their association. Huth et al. (2016), compute Pearson correlation between predicted and actual brain responses using Gaussian random vectors to test statistical significance. Resulting p-values are corrected for multiple comparisons within each subject using false discovery rate (FDR) (Benjamini and Hochberg, 1995). Others have used Bonferroni correction (Huth et al., 2016) or block-wise permutation test (Adolf et al., 2014) to evaluate the statistical significance of the correlation (Zhang et al., 2020). Some report R 2 (explained variance) instead of or in addition to correlation coefficients (Minnema and Herbelot, 2019;Reddy and Wehbe, 2021). Others have adopted a more elaborate extension of Pearson correlation, namely BrainScore (Schrimpf et al., 2018). Brain-Score is estimated on held-out test data, calculating Pearson's correlation between model predictions and neural recordings divided by the estimated ceiling and averaged across voxels and participants.

Percentile rank was first used for encoding (Mitchell et al., 2008), but can also be used for decoding (Pereira et al., 2018;Gauthier and Levy, 2019;Minnema and Herbelot, 2019). In encoding, the predicted brain image for w is ranked along the predicted images for a set of candidate words w ′ by their similarity to the real (ground truth) image for w. The average rank is then reported. For decoding, they rank word vectors rather than neural response images. Note the similarity metric is unspecified, but typically cosine distance is used.

Mean squared error, the average of the squared differences between word vectors and neural responses, was first used for encoding in Søgaard (2016) on a held-out test split. It was also used by Gauthier and Levy (2019).

Representational similarity analysis (RSA) was introduced in Kriegeskorte et al. (2008) as a non-parametric way to characterize structural alignment between the geometries of representations derived from disparate modalities. RSA abstracts away from activity patterns themselves and instead computes representational similarity matrices (RSMs), which characterize the information carried by a given representation method through global similarity structure. A rank correlation coefficient is computed between RSMs derived from the two spaces, providing a summary statistic indicative of the overall representational alignment between them. Being non-parametric, RSA circumvents many of the various methodological weaknesses (such as over fitting, etc.). Gauthier and Levy (2019), Minnema and Herbelot (2019), and  apply (variations of) RSA to investigate the relations between different model components, and then to study the alignment of these components with brain response.

Cosine similarity was used in Mitchell et al. (2008) to select between the candidate images in pairwise matching accuracy, as well as in percentile rank and RSA, but the raw cosine similarities between predicted and real images or embeddings can also be used as a metric. Minnema and Herbelot (2019) use this metric to quantify how close the predicted word vectors are to the target. Finally, Zou et al. (2022) use precision@k, a standard metric in other mapping problems, e.g., cross-lingual word embeddings (Søgaard et al., 2019).

Comparisons Most metrics are used to evaluate both encoding and decoding models (pairwise matching accuracy, Pearson correlation, percentile rank, MSE, RSA, cosine distance). Results for two of the most widely used metrics -pairwise matching accuracy 5 and percentile rank -tend to be around 0.7-0.8 with generally better results for more recent architectures and larger LMs. To draw conclusions across studies relying on different metrics, we need to investigate which metrics are more conservative, and how different metrics relate.

Pairwise matching accuracy vs. Pearson correlation It seems that pairwise matching accuracy tends to increase monotonically with Pearson correlation. Consider three sets of distances over corresponding point sets, A, B, and C. If A and B are more strongly linearly correlated than A and C, under an optimal linear mapping Ω (minimizing point-wise squared error distance), E[(a − bΩ) 2 ] > E[(a − cΩ) 2 ]. Even in this conservative setting in our synthetic experiments in Appendix A.1, the correlation between matching accuracy and percentile rank was very high,~0.9.

Pairwise matching accuracy vs. percentile rank Both metrics have random baseline scores of 0.5, and they will converge in the limit. If a has a percentile rank of p in a list A, it will be higher than a random member of A p percent of the time. In our experiments in Appendix A.1, the correlation converges toward 1.0, with values consistently higher than 0.8 for N = 100.

Pairwise matching accuracy vs. precision@k are also positively correlated. Perfect score in one entails perfect score in the other, but precision@k can of course be very small for very high values of pairwise matching accuracy (especially if the set of candidate words is big). Conversely, we can have 5 When discriminating averages over 20 images (Wehbe et al., 2014b), scores are naturally lower. saturation for high values of k, because matching accuracies higher than n−k n will mean near-perfect precision@k scores. In practice, precision@k (for low values of k) will be much more conservative, however. The correlation coefficient for N = 100 (see Appendix A.1) tends to lie around 0.7.

Relative strength Pairwise Matching Accuracy is a relatively permissible performance metric. To see this, consider the scenario in which all target words can be divided into two equal-sized buckets based on word length (number of characters). Say the neural responses capture nothing but this binary distinction between long and short words, but do so perfectly. Moreover, our mapping method, e.g., linear regression, learns this from training data. Now, from this alone, the pairwise matching accuracy will converge toward µ = 0.75, since our model will do perfectly (1.0) on half of the data, and exhibit random performance (0.5) on the other half. If the neural responses tracked word length (and not just the distinction between short and long words), performance would be even better. In other words, Pairwise Matching Accuracy scores around 0.7-0.8 (observed in the studies above) may only reflect very shallow processing characteristics. The fact that Minnema and Herbelot (2019) only observed good results with this metric, led them to adopt a rather critical stance, for good reasons.

Other metrics are clearly more conservative. For a set of n candidate words, a random mapping will induce a precision@1-score of 1 n . While hubs may inflate scores for larger values, the metric is extremely conservative for small values of k. However, only Zou et al. (2022) use this metric, and they modify the experimental protocol substantially, making the task much easier by providing additional input to a non-linear model. The small improvement from adding neural response input is interesting, but could potentially be explained by shallow processing characteristics.

They argue that analogy testing would provide a better evaluation protocol: one would ideally use standard metrics such as semantic relatedness judgment tasks, analogy tasks, etc.

[but] this is not possible due to the limited vocabulary sizes of the available brain datasets Such evaluation is possible on small scale, though, and increasingly larger fMRI datasets are becoming available (see above). Zhang et al. (2020) have identified analogical reasoning in fMRI brain activation spaces. The analogies are computed using vector offset and probe the systematicity of how semantic relations are encoded. If a model encodes the capital-of relation systematically, we can retrieve the capital of Germany by subtracting the fMRI vector for 'Paris' from the sum of our the fMRI vectors for Germany and France. This is the same kind of analogical reasoning found in language models (Mikolov et al., 2013). Garneau et al. (2021) show that the more language models satisfy analogies, the more isomorphic they are.

So far, it seems that, with the possible exception of Zhang et al. (2020), there is little evidence for structural similarities, beyond what could be induced by shallow processing characteristics, but what about all the studies that report strong Pearson correlations? Per-voxel correlation coefficients are low on average, but across the above studies, typically only around 4-40% of the voxels exhibit significant correlations (Huth et al., 2016;. Since these correlations have been replicated across different datasets, they are generally not disputed, but could still reflect rather shallow processing characteristics.

On a more positive note, several studies show that larger (and better) language models align better with neural response measurements (Schrimpf et al., 2021;. This suggests that language models in the future may align even better with such measurements, possibly reflecting properties of deep processing. Such correlations with model quality and size are positive, making the results reported above more credible.

Generally, the conclusions we can draw from the above studies are somewhat vague. There are two reasons for this: (i) Past studies have relied on permissible (pairwise matching accuracy) and ambiguous (Pearson correlation) performance metrics; and (ii) past studies have relied on small-sized datasets. We believe that this calls for a meta-analysis of the above studies. To provide grounds for such a meta-analysis, we have in this section taken steps to compare the metrics used in these studies. We leave it for future work to explore various ways effect sizes can be computed across these studies.","sent1: We present the evaluation metrics used in the above studies and discuss how they relate.
sent2: See Table 2 for a summary of metrics and corresponding studies.
sent3: Mitchell et al. (2008) introduce pairwise matching accuracy.
sent4: Because of their small sample size, they use a leave-2-out cross-validation, which later work also adopted.
sent5: The metric is a binary classification accuracy metric on a balanced dataset, so a random baseline converges toward 0.5.
sent6: Many studies have relied on this metric, both in encoding and decoding (see Table 2).
sent7: 4 Pearson correlationPearson correlation is another widely used metric in the studies surveyed above, measuring the linear relationship between variables, and providing insight into the strength and direction of their association.
sent8: Huth et al. (2016), compute Pearson correlation between predicted and actual brain responses using Gaussian random vectors to test statistical significance.
sent9: Resulting p-values are corrected for multiple comparisons within each subject using false discovery rate (FDR) (Benjamini and Hochberg, 1995).
sent10: Others have used Bonferroni correction (Huth et al., 2016) or block-wise permutation test (Adolf et al., 2014) to evaluate the statistical significance of the correlation (Zhang et al., 2020).
sent11: Some report R 2 (explained variance) instead of or in addition to correlation coefficients (Minnema and Herbelot, 2019;Reddy and Wehbe, 2021).
sent12: Others have adopted a more elaborate extension of Pearson correlation, namely BrainScore (Schrimpf et al., 2018).
sent13: Brain-Score is estimated on held-out test data, calculating Pearson's correlation between model predictions and neural recordings divided by the estimated ceiling and averaged across voxels and participants.
sent14: Percentile rank was first used for encoding (Mitchell et al., 2008), but can also be used for decoding (Pereira et al., 2018;Gauthier and Levy, 2019;Minnema and Herbelot, 2019).
sent15: In encoding, the predicted brain image for w is ranked along the predicted images for a set of candidate words w ′ by their similarity to the real (ground truth) image for w.
sent16: The average rank is then reported.
sent17: For decoding, they rank word vectors rather than neural response images.
sent18: Note the similarity metric is unspecified, but typically cosine distance is used.
sent19: Mean squared error, the average of the squared differences between word vectors and neural responses, was first used for encoding in Søgaard (2016) on a held-out test split.
sent20: It was also used by Gauthier and Levy (2019).
sent21: Representational similarity analysis (RSA) was introduced in Kriegeskorte et al. (2008) as a non-parametric way to characterize structural alignment between the geometries of representations derived from disparate modalities.
sent22: RSA abstracts away from activity patterns themselves and instead computes representational similarity matrices (RSMs), which characterize the information carried by a given representation method through global similarity structure.
sent23: A rank correlation coefficient is computed between RSMs derived from the two spaces, providing a summary statistic indicative of the overall representational alignment between them.
sent24: Being non-parametric, RSA circumvents many of the various methodological weaknesses (such as over fitting, etc.).
sent25: Gauthier and Levy (2019), Minnema and Herbelot (2019), and  apply (variations of) RSA to investigate the relations between different model components, and then to study the alignment of these components with brain response.
sent26: Cosine similarity was used in Mitchell et al. (2008) to select between the candidate images in pairwise matching accuracy, as well as in percentile rank and RSA, but the raw cosine similarities between predicted and real images or embeddings can also be used as a metric.
sent27: Minnema and Herbelot (2019) use this metric to quantify how close the predicted word vectors are to the target.
sent28: Finally, Zou et al. (2022) use precision@k, a standard metric in other mapping problems, e.g., cross-lingual word embeddings (Søgaard et al., 2019).
sent29: Comparisons Most metrics are used to evaluate both encoding and decoding models (pairwise matching accuracy, Pearson correlation, percentile rank, MSE, RSA, cosine distance).
sent30: Results for two of the most widely used metrics -pairwise matching accuracy 5 and percentile rank -tend to be around 0.7-0.8 with generally better results for more recent architectures and larger LMs.
sent31: To draw conclusions across studies relying on different metrics, we need to investigate which metrics are more conservative, and how different metrics relate.
sent32: Pairwise matching accuracy vs. Pearson correlation It seems that pairwise matching accuracy tends to increase monotonically with Pearson correlation.
sent33: Consider three sets of distances over corresponding point sets, A, B, and C. If A and B are more strongly linearly correlated than A and C, under an optimal linear mapping Ω (minimizing point-wise squared error distance), E[(a − bΩ) 2 ] > E[(a − cΩ) 2 ].
sent34: Even in this conservative setting in our synthetic experiments in Appendix A.1, the correlation between matching accuracy and percentile rank was very high,~0.9.
sent35: Pairwise matching accuracy vs. percentile rank Both metrics have random baseline scores of 0.5, and they will converge in the limit.
sent36: If a has a percentile rank of p in a list A, it will be higher than a random member of A p percent of the time.
sent37: In our experiments in Appendix A.1, the correlation converges toward 1.0, with values consistently higher than 0.8 for N = 100.
sent38: Pairwise matching accuracy vs. precision@k are also positively correlated.
sent39: Perfect score in one entails perfect score in the other, but precision@k can of course be very small for very high values of pairwise matching accuracy (especially if the set of candidate words is big).
sent40: Conversely, we can have 5 When discriminating averages over 20 images (Wehbe et al., 2014b), scores are naturally lower.
sent41: saturation for high values of k, because matching accuracies higher than n−k n will mean near-perfect precision@k scores.
sent42: In practice, precision@k (for low values of k) will be much more conservative, however.
sent43: The correlation coefficient for N = 100 (see Appendix A.1) tends to lie around 0.7.
sent44: Relative strength Pairwise Matching Accuracy is a relatively permissible performance metric.
sent45: To see this, consider the scenario in which all target words can be divided into two equal-sized buckets based on word length (number of characters).
sent46: Say the neural responses capture nothing but this binary distinction between long and short words, but do so perfectly.
sent47: Moreover, our mapping method, e.g., linear regression, learns this from training data.
sent48: Now, from this alone, the pairwise matching accuracy will converge toward µ = 0.75, since our model will do perfectly (1.0) on half of the data, and exhibit random performance (0.5) on the other half.
sent49: If the neural responses tracked word length (and not just the distinction between short and long words), performance would be even better.
sent50: In other words, Pairwise Matching Accuracy scores around 0.7-0.8 (observed in the studies above) may only reflect very shallow processing characteristics.
sent51: The fact that Minnema and Herbelot (2019) only observed good results with this metric, led them to adopt a rather critical stance, for good reasons.
sent52: Other metrics are clearly more conservative.
sent53: For a set of n candidate words, a random mapping will induce a precision@1-score of 1 n .
sent54: While hubs may inflate scores for larger values, the metric is extremely conservative for small values of k.
sent55: However, only Zou et al. (2022) use this metric, and they modify the experimental protocol substantially, making the task much easier by providing additional input to a non-linear model.
sent56: The small improvement from adding neural response input is interesting, but could potentially be explained by shallow processing characteristics.
sent57: They argue that analogy testing would provide a better evaluation protocol: one would ideally use standard metrics such as semantic relatedness judgment tasks, analogy tasks, etc.
sent58: [but] this is not possible due to the limited vocabulary sizes of the available brain datasets Such evaluation is possible on small scale, though, and increasingly larger fMRI datasets are becoming available (see above).
sent59: Zhang et al. (2020) have identified analogical reasoning in fMRI brain activation spaces.
sent60: The analogies are computed using vector offset and probe the systematicity of how semantic relations are encoded.
sent61: If a model encodes the capital-of relation systematically, we can retrieve the capital of Germany by subtracting the fMRI vector for 'Paris' from the sum of our the fMRI vectors for Germany and France.
sent62: This is the same kind of analogical reasoning found in language models (Mikolov et al., 2013).
sent63: Garneau et al. (2021) show that the more language models satisfy analogies, the more isomorphic they are.
sent64: So far, it seems that, with the possible exception of Zhang et al. (2020), there is little evidence for structural similarities, beyond what could be induced by shallow processing characteristics, but what about all the studies that report strong Pearson correlations?
sent65: Per-voxel correlation coefficients are low on average, but across the above studies, typically only around 4-40% of the voxels exhibit significant correlations (Huth et al., 2016;.
sent66: Since these correlations have been replicated across different datasets, they are generally not disputed, but could still reflect rather shallow processing characteristics.
sent67: On a more positive note, several studies show that larger (and better) language models align better with neural response measurements (Schrimpf et al., 2021;.
sent68: This suggests that language models in the future may align even better with such measurements, possibly reflecting properties of deep processing.
sent69: Such correlations with model quality and size are positive, making the results reported above more credible.
sent70: Generally, the conclusions we can draw from the above studies are somewhat vague.
sent71: There are two reasons for this: (i) Past studies have relied on permissible (pairwise matching accuracy) and ambiguous (Pearson correlation) performance metrics; and (ii) past studies have relied on small-sized datasets.
sent72: We believe that this calls for a meta-analysis of the above studies.
sent73: To provide grounds for such a meta-analysis, we have in this section taken steps to compare the metrics used in these studies.
sent74: We leave it for future work to explore various ways effect sizes can be computed across these studies.","1. What is the focus of the section on performance metrics? sent1
    1.1. Where can a summary of metrics and corresponding studies be found? sent2
2. What metric did Mitchell et al. (2008) introduce, and what is its significance? sent3
    2.1. Why did they use leave-2-out cross-validation? sent4
    2.2. What is the baseline for this metric? sent5
    2.3. How widely has this metric been used? sent6
3. What is Pearson correlation, and how is it used in the studies? sent7
    3.1. How did Huth et al. (2016) use Pearson correlation? sent8
    3.2. What methods are used to evaluate the statistical significance of the correlation? sent9, sent10
4. What is R², and how is it used in the studies? sent11
5. What is Brain-Score, and how is it calculated? sent12, sent13
6. How is percentile rank used in encoding and decoding? sent14, sent15, sent16, sent17
    6.1. What similarity metric is typically used with percentile rank? sent18
7. What is mean squared error, and how has it been used in the studies? sent19, sent20
8. What is Representational Similarity Analysis (RSA), and how is it applied? sent21, sent22, sent23, sent24
    8.1. How have Gauthier and Levy (2019) and Minnema and Herbelot (2019) used RSA? sent25
9. How is cosine similarity used in the studies? sent26, sent27
10. What is precision@k, and how is it used in the studies? sent28
11. How are different metrics compared in terms of their use in encoding and decoding models? sent29
    11.1. What are the typical results for pairwise matching accuracy and percentile rank? sent30
    11.2. What is needed to draw conclusions across studies using different metrics? sent31
12. How do pairwise matching accuracy and Pearson correlation relate? sent32, sent33
13. How do pairwise matching accuracy and percentile rank relate? sent34, sent35, sent36, sent37
14. How do pairwise matching accuracy and precision@k relate? sent38, sent39, sent40, sent41, sent42, sent43
15. What is the relative strength of Pairwise Matching Accuracy as a performance metric? sent44, sent45, sent46, sent47, sent48, sent49, sent50, sent51
16. How do other metrics compare in terms of conservativeness? sent52, sent53, sent54, sent55, sent56
17. What alternative evaluation protocol is suggested for better evaluation? sent57, sent58
18. What evidence is there for analogical reasoning in fMRI brain activation spaces? sent59, sent60, sent61, sent62
    18.1. How do language models relate to analogical reasoning? sent63
19. What is the current evidence for structural similarities in language models? sent64, sent65, sent66
20. How do larger language models align with neural response measurements? sent67, sent68, sent69
21. What are the general conclusions drawn from the studies, and what are the reasons for their vagueness? sent70, sent71
22. What is suggested for future work in this area? sent72, sent73, sent74","2. What metric did Mitchell et al. (2008) introduce, and what is its significance? sent3
    2.1. Why did they use leave-2-out cross-validation? sent4
    2.2. What is the baseline for this metric? sent5
    2.3. How widely has this metric been used? sent6

3. What is Pearson correlation, and how is it used in the studies? sent7
    3.1. How did Huth et al. (2016) use Pearson correlation? sent8
    3.2. What methods are used to evaluate the statistical significance of the correlation? sent9, sent10","3. What is Pearson correlation, and how is it used in the studies? sent7
    3.1. How did Huth et al. (2016) use Pearson correlation? sent8
    3.2. What methods are used to evaluate the statistical significance of the correlation? sent9, sent10","Question: How is statistical significance evaluated for Pearson correlation in brain response prediction studies?

Supporting sentence: sent9, sent10",How is statistical significance evaluated for Pearson correlation in brain response prediction studies?,"sent9, sent10",2,implement,"Huth et al. (2016) compute Pearson correlation between predicted and actual brain responses using Gaussian random vectors to test statistical significance.

Resulting p-values are corrected for multiple comparisons within each subject using false discovery rate (FDR) (Benjamini and Hochberg, 1995).

Others have used Bonferroni correction (Huth et al., 2016) or block-wise permutation test (Adolf et al., 2014) to evaluate the statistical significance of the correlation (Zhang et al., 2020).","Question:

How is statistical significance evaluated for Pearson correlation in brain response prediction studies?

Answer:

Huth et al. (2016) compute Pearson correlation between predicted and actual brain responses using Gaussian random vectors to test statistical significance.

Resulting p-values are corrected for multiple comparisons within each subject using false discovery rate (FDR) (Benjamini and Hochberg, 1995).

Others have used Bonferroni correction (Huth et al., 2016) or block-wise permutation test (Adolf et al., 2014) to evaluate the statistical significance of the correlation (Zhang et al., 2020)."
252992688,A Survey of Active Learning for Natural Language Processing,https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,Performance Prediction,8,"Predicting performance can be another indicator for querying. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model. This strategy can be computationally costly since retraining is needed for each candidate.

Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b). This learning-to-select strategy may have some constraints. First, it requires labeled data (maybe from another domain) to train the policy. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.

A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).","sent1: Predicting performance can be another indicator for querying.
sent2: Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set.
sent3: This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model.
sent4: This strategy can be computationally costly since retraining is needed for each candidate.
sent5: Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set.
sent6: Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b).
sent7: This learning-to-select strategy may have some constraints.
sent8: First, it requires labeled data (maybe from another domain) to train the policy.
sent9: To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations.
sent10: Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.
sent11: A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model.
sent12: Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling.
sent13: Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold.
sent14: For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).","1. What is one indicator for querying in active learning? sent1
2. What is the ideal outcome when selecting instances for labeling and adding to the training set? sent2
3. What strategy is motivated by the goal of reducing future errors in active learning? sent3
    3.1. What is a potential drawback of the expected error reduction strategy? sent4
4. What recent methods have been proposed to select instances that lead to fewer errors? sent5
    4.1. What techniques have been used to train policy models for instance selection? sent6
    4.2. What are some constraints of the learning-to-select strategy? sent7
        4.2.1. What is the first constraint of the learning-to-select strategy? sent8
        4.2.2. How can the reliance on labeled data be mitigated in the learning-to-select strategy? sent9
        4.2.3. What issue may arise with learning signals for complex tasks? sent10
5. What is a simpler idea for selecting instances in active learning? sent11
6. How have smaller models been used in performance prediction for NLP tasks? sent12
7. How do Zhang and Plank (2021) select ambiguous instances in their approach? sent13
8. What special techniques are used in machine translation to identify erroneous instances? sent14","4. What recent methods have been proposed to select instances that lead to fewer errors? sent5
    4.1. What techniques have been used to train policy models for instance selection? sent6
    4.2. What are some constraints of the learning-to-select strategy? sent7
        4.2.1. What is the first constraint of the learning-to-select strategy? sent8
        4.2.2. How can the reliance on labeled data be mitigated in the learning-to-select strategy? sent9
        4.2.3. What issue may arise with learning signals for complex tasks? sent10

    4.2. What are some constraints of the learning-to-select strategy? sent7
        4.2.1. What is the first constraint of the learning-to-select strategy? sent8
        4.2.2. How can the reliance on labeled data be mitigated in the learning-to-select strategy? sent9
        4.2.3. What issue may arise with learning signals for complex tasks? sent10","4. What recent methods have been proposed to select instances that lead to fewer errors? sent5
    4.1. What techniques have been used to train policy models for instance selection? sent6
    4.2. What are some constraints of the learning-to-select strategy? sent7
        4.2.1. What is the first constraint of the learning-to-select strategy? sent8
        4.2.2. How can the reliance on labeled data be mitigated in the learning-to-select strategy? sent9
        4.2.3. What issue may arise with learning signals for complex tasks? sent10","Question: What are the constraints and potential solutions in the learning-to-select strategy for instance selection in active learning?

Supporting sentence: sent7, sent8, sent9, sent10",What are the constraints and potential solutions in the learning-to-select strategy for instance selection in active learning?,"sent7, sent8, sent9, sent10",4,implement,"The learning-to-select strategy may have some constraints.

First, it requires labeled data (maybe from another domain) to train the policy.

To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations.

Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.","Question:

What are the constraints and potential solutions in the learning-to-select strategy for instance selection in active learning?

Answer:

The learning-to-select strategy may have some constraints.

First, it requires labeled data (maybe from another domain) to train the policy.

To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations.

Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks."
252992688,A Survey of Active Learning for Natural Language Processing,https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,Performance Prediction,8,"Predicting performance can be another indicator for querying. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model. This strategy can be computationally costly since retraining is needed for each candidate.

Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b). This learning-to-select strategy may have some constraints. First, it requires labeled data (maybe from another domain) to train the policy. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.

A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).","sent1: Predicting performance can be another indicator for querying.
sent2: Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set.
sent3: This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model.
sent4: This strategy can be computationally costly since retraining is needed for each candidate.
sent5: Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set.
sent6: Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b).
sent7: This learning-to-select strategy may have some constraints.
sent8: First, it requires labeled data (maybe from another domain) to train the policy.
sent9: To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations.
sent10: Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.
sent11: A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model.
sent12: Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling.
sent13: Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold.
sent14: For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).","1. What is one indicator for querying in active learning? sent1
2. What is the ideal outcome when selecting instances for labeling and adding to the training set? sent2
3. What strategy is motivated by the goal of reducing future errors in active learning? sent3
    3.1. What is a potential drawback of the expected error reduction strategy? sent4
4. What recent methods have been proposed to select instances that lead to fewer errors? sent5
    4.1. What techniques have been used to train policy models for instance selection? sent6
    4.2. What are some constraints of the learning-to-select strategy? sent7
        4.2.1. What is the first constraint of the learning-to-select strategy? sent8
        4.2.2. How can the reliance on labeled data be mitigated in the learning-to-select strategy? sent9
        4.2.3. What issue may arise with learning signals for complex tasks? sent10
5. What is a simpler idea for selecting instances in active learning? sent11
6. How have smaller models been used in performance prediction for NLP tasks? sent12
7. How do Zhang and Plank (2021) select ambiguous instances in their approach? sent13
8. What special techniques are used in machine translation to identify erroneous instances? sent14","4. What recent methods have been proposed to select instances that lead to fewer errors? sent5
    4.1. What techniques have been used to train policy models for instance selection? sent6
    4.2. What are some constraints of the learning-to-select strategy? sent7
        4.2.1. What is the first constraint of the learning-to-select strategy? sent8
        4.2.2. How can the reliance on labeled data be mitigated in the learning-to-select strategy? sent9
        4.2.3. What issue may arise with learning signals for complex tasks? sent10

    4.2. What are some constraints of the learning-to-select strategy? sent7
        4.2.1. What is the first constraint of the learning-to-select strategy? sent8
        4.2.2. How can the reliance on labeled data be mitigated in the learning-to-select strategy? sent9
        4.2.3. What issue may arise with learning signals for complex tasks? sent10","    4.2. What are some constraints of the learning-to-select strategy? sent7
        4.2.1. What is the first constraint of the learning-to-select strategy? sent8
        4.2.2. How can the reliance on labeled data be mitigated in the learning-to-select strategy? sent9
        4.2.3. What issue may arise with learning signals for complex tasks? sent10","Question: What are the constraints and potential issues of the learning-to-select strategy in performance prediction for NLP?

Supporting sentence: sent7, sent8, sent9, sent10",What are the constraints and potential issues of the learning-to-select strategy in performance prediction for NLP?,"sent7, sent8, sent9, sent10",4,implement,"The learning-to-select strategy may have some constraints.

First, it requires labeled data (maybe from another domain) to train the policy.

To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations.

Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.","Question:

What are the constraints and potential issues of the learning-to-select strategy in performance prediction for NLP?

Answer:

The learning-to-select strategy may have some constraints.

First, it requires labeled data (maybe from another domain) to train the policy.

To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations.

Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks."
245144787,Measure and Improve Robustness in NLP Models: A Survey,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,Inductive-prior-based Approaches,9,"Another thread is to introduce inductive bias (i.e., to regularize the hypothesis space) to force the model to discard some spurious features. This is closely connected to the human-prior-based identification approaches in Section 4.1 as those human-priors can often be used to re-formulate the training objective with additional regularizers. To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component. The construction of this side component usually relies on prior knowledge of what the misaligned features are. Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020). Similarly, Clark et al. (2019Utama et al. (2020a,b) propose to ensemble with a model explicitly capturing bias, where the main model is trained together with this ""bias-only"" model such that the main model is discouraged from using biases. More recent work (Xiong et al., 2021) shows the ensemble-based approaches can be further improved via better calibrating the bias-only model. Furthermore, additional regularizers have been introduced for robust fine-tuning over pre-trained models, e.g., mutual-information-based regularizers (Wang et al., 2021a) and smoothness-inducing adversarial regularization (Jiang et al., 2020).

In a broader scope, given that one of the main challenges of domain adaptation is to counter the model's tendency in learning domain-specific spurious features (Ganin et al., 2016), some methods contributing to domain adaption may have also progressed along the line of our interest, e.g., domain adversarial neural network (Ganin et al., 2016). This line of work also inspires a family of methods forcing the model to learn auxiliary-annotationinvariant representations with a side component (Ghifary et al., 2016;Wang et al., 2017;Rozantsev et al., 2018;Motiian et al., 2017;Li et al., 2018;Vernikos et al., 2020).

Despite the diverse concrete ideas introduced, the above is mainly training for small empirical loss across different domains or distributions in addition to forcing the model to be invariant to domain-specific spurious features. As an extension along this direction, invariant risk minimization (IRM) (Arjovsky et al., 2019) introduces the idea of invariant predictors across multiple environments, which was later followed and discussed by a variety of extensions (Choe et al., 2020;Ahmed et al., 2020;Rosenfeld et al., 2021). More recently, Dranker et al. (2021) applied IRM in natural language inference and found that a more naturalistic characterization of the problem setup is needed.","sent1: Another thread is to introduce inductive bias (i.e., to regularize the hypothesis space) to force the model to discard some spurious features.
sent2: This is closely connected to the human-prior-based identification approaches in Section 4.1 as those human-priors can often be used to re-formulate the training objective with additional regularizers.
sent3: To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component.
sent4: The construction of this side component usually relies on prior knowledge of what the misaligned features are.
sent5: Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020).
sent6: Similarly, Clark et al. (2019Utama et al. (2020a,b) propose to ensemble with a model explicitly capturing bias, where the main model is trained together with this ""bias-only"" model such that the main model is discouraged from using biases.
sent7: More recent work (Xiong et al., 2021) shows the ensemble-based approaches can be further improved via better calibrating the bias-only model.
sent8: Furthermore, additional regularizers have been introduced for robust fine-tuning over pre-trained models, e.g., mutual-information-based regularizers (Wang et al., 2021a) and smoothness-inducing adversarial regularization (Jiang et al., 2020).
sent9: In a broader scope, given that one of the main challenges of domain adaptation is to counter the model's tendency in learning domain-specific spurious features (Ganin et al., 2016), some methods contributing to domain adaption may have also progressed along the line of our interest, e.g., domain adversarial neural network (Ganin et al., 2016).
sent10: This line of work also inspires a family of methods forcing the model to learn auxiliary-annotationinvariant representations with a side component (Ghifary et al., 2016;Wang et al., 2017;Rozantsev et al., 2018;Motiian et al., 2017;Li et al., 2018;Vernikos et al., 2020).
sent11: Despite the diverse concrete ideas introduced, the above is mainly training for small empirical loss across different domains or distributions in addition to forcing the model to be invariant to domain-specific spurious features.
sent12: As an extension along this direction, invariant risk minimization (IRM) (Arjovsky et al., 2019) introduces the idea of invariant predictors across multiple environments, which was later followed and discussed by a variety of extensions (Choe et al., 2020;Ahmed et al., 2020;Rosenfeld et al., 2021).
sent13: More recently, Dranker et al. (2021) applied IRM in natural language inference and found that a more naturalistic characterization of the problem setup is needed.","1. What is the purpose of introducing inductive bias in NLP models? sent1
    1.1. How is this approach connected to human-prior-based identification approaches? sent2
    1.2. What is the typical process to achieve the goal of introducing inductive bias? sent3
        1.2.1. What does the construction of the side component rely on? sent4
        1.2.2. What methods can be built to counter misaligned features? sent5
    1.3. How do ensemble-based approaches work in the context of bias reduction? sent6
        1.3.1. How can ensemble-based approaches be improved? sent7
    1.4. What additional regularizers have been introduced for robust fine-tuning? sent8
2. How does domain adaptation relate to the challenge of countering domain-specific spurious features? sent9
    2.1. What methods have been inspired by this line of work? sent10
3. What is the main focus of the diverse ideas introduced in this line of work? sent11
4. What is invariant risk minimization (IRM) and how has it been extended? sent12
    4.1. How has IRM been applied in natural language inference? sent13","1. What is the purpose of introducing inductive bias in NLP models? sent1
    1.1. How is this approach connected to human-prior-based identification approaches? sent2
    1.2. What is the typical process to achieve the goal of introducing inductive bias? sent3
        1.2.1. What does the construction of the side component rely on? sent4
        1.2.2. What methods can be built to counter misaligned features? sent5
    1.3. How do ensemble-based approaches work in the context of bias reduction? sent6
        1.3.1. How can ensemble-based approaches be improved? sent7
    1.4. What additional regularizers have been introduced for robust fine-tuning? sent8","1. What is the purpose of introducing inductive bias in NLP models? sent1
    1.1. How is this approach connected to human-prior-based identification approaches? sent2
    1.2. What is the typical process to achieve the goal of introducing inductive bias? sent3
        1.2.1. What does the construction of the side component rely on? sent4
        1.2.2. What methods can be built to counter misaligned features? sent5
    1.3. How do ensemble-based approaches work in the context of bias reduction? sent6
        1.3.1. How can ensemble-based approaches be improved? sent7
    1.4. What additional regularizers have been introduced for robust fine-tuning? sent8","Question: What methods and regularizers are used to counter misaligned features in NLP models?

Supporting sentence: sent3, sent4, sent5, sent6, sent7, sent8",What methods and regularizers are used to counter misaligned features in NLP models?,"sent3, sent4, sent5, sent6, sent7, sent8",3,implement,"Another thread is to introduce inductive bias (i.e., to regularize the hypothesis space) to force the model to discard some spurious features.

To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component.

The construction of this side component usually relies on prior knowledge of what the misaligned features are.

Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020).

Similarly, Clark et al. (2019) and Utama et al. (2020a,b) propose to ensemble with a model explicitly capturing bias, where the main model is trained together with this ""bias-only"" model such that the main model is discouraged from using biases.

More recent work (Xiong et al., 2021)

shows the ensemble-based approaches can be further improved via better calibrating the bias-only model.

Furthermore, additional regularizers have been introduced for robust fine-tuning over pre-trained models, e.g., mutual-information-based regularizers (Wang et al., 2021a) and smoothness-inducing adversarial regularization (Jiang et al., 2020).","Question:

What methods and regularizers are used to counter misaligned features in NLP models?

Answer:

Another thread is to introduce inductive bias (i.e., to regularize the hypothesis space) to force the model to discard some spurious features.

To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component.

The construction of this side component usually relies on prior knowledge of what the misaligned features are.

Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020).

Similarly, Clark et al. (2019) and Utama et al. (2020a,b) propose to ensemble with a model explicitly capturing bias, where the main model is trained together with this ""bias-only"" model such that the main model is discouraged from using biases.

More recent work (Xiong et al., 2021)

shows the ensemble-based approaches can be further improved via better calibrating the bias-only model.

Furthermore, additional regularizers have been introduced for robust fine-tuning over pre-trained models, e.g., mutual-information-based regularizers (Wang et al., 2021a) and smoothness-inducing adversarial regularization (Jiang et al., 2020)."
256662721,"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems",https://www.semanticscholar.org/paper/c589a3420ba335a05c248f525ea3c6e90215e42b,Evaluation metrics,4,"As an essential aspect of recommendation design, evaluation can provide insights on recommendation quality from multiple dimensions. Apart from well-known metrics such as RMSE, MAP, AUC, MAE, Recall, Precision, MRR, NDCG, F1-score and HitRate in offline mode, some works define Group AUC  or User Group AUC  to evaluate the utility of group recommendations. JIANG et al. (2022) and  conducted A/B testing to evaluate performance with online users using Conversion rate or CTR.

The integration of generative modules such as GPT and T5 into existing recommender systems offers additional possibilities for recommender systems, such as generating free-form textual explanations for recommendation results or simulating more realistic real-life dialogue scenarios during conversational recommendations to enhance users' experience. In such cases, BLEU and ROUGE are commonly adopted to automatically evaluate the relevance of generated text based on lexicon overlap. Besides, Perplexity (PPL), Distinct-n, and Unique Sentence Ratio (USR) are also widely used metrics to measure fluency, diversity, and informativeness of generated texts. Other evaluation metrics are leveraged with respect to special requests in LMRSs. For instance, Xie et al. (2023) adopted Entailment Ratio and MAUVE to measure if the generated explanations are factually correct and how close the generated contents are to the ground truth corpus, respectively. Geng et al. (2022a) adopted Feature Diversity (DIV) and CLIPScore (CS) to measure the generated explanations and text-image alignment. Besides, to assess the system's capability to provide item recommendations during conversations, Wang et al. (2022a) computed the Item Ratio within the final generated responses. They evaluated the recommendation performance in an end-to-end manner to prevent the inappropriate insertion of recommended items into dialogues.

Human evaluation complements objective evaluation, as automatic metrics may not match sub-jective feedback from users. Liu et al. (2023a) pointed out that human subjective and automatic objective evaluation measurements may yield opposite results, which underscores the limitations of existing automatic metrics for evaluating generated explanations and dialogues in LMRSs. Figure  3 displays usage frequency statistics for different evaluation metrics in their respective tasks.","sent1: As an essential aspect of recommendation design, evaluation can provide insights on recommendation quality from multiple dimensions.
sent2: Apart from well-known metrics such as RMSE, MAP, AUC, MAE, Recall, Precision, MRR, NDCG, F1-score and HitRate in offline mode, some works define Group AUC  or User Group AUC  to evaluate the utility of group recommendations.
sent3: JIANG et al. (2022) and  conducted A/B testing to evaluate performance with online users using Conversion rate or CTR.
sent4: The integration of generative modules such as GPT and T5 into existing recommender systems offers additional possibilities for recommender systems, such as generating free-form textual explanations for recommendation results or simulating more realistic real-life dialogue scenarios during conversational recommendations to enhance users' experience.
sent5: In such cases, BLEU and ROUGE are commonly adopted to automatically evaluate the relevance of generated text based on lexicon overlap.
sent6: Besides, Perplexity (PPL), Distinct-n, and Unique Sentence Ratio (USR) are also widely used metrics to measure fluency, diversity, and informativeness of generated texts.
sent7: Other evaluation metrics are leveraged with respect to special requests in LMRSs.
sent8: For instance, Xie et al. (2023) adopted Entailment Ratio and MAUVE to measure if the generated explanations are factually correct and how close the generated contents are to the ground truth corpus, respectively.
sent9: Geng et al. (2022a) adopted Feature Diversity (DIV) and CLIPScore (CS) to measure the generated explanations and text-image alignment.
sent10: Besides, to assess the system's capability to provide item recommendations during conversations, Wang et al. (2022a) computed the Item Ratio within the final generated responses.
sent11: They evaluated the recommendation performance in an end-to-end manner to prevent the inappropriate insertion of recommended items into dialogues.
sent12: Human evaluation complements objective evaluation, as automatic metrics may not match sub-jective feedback from users.
sent13: Liu et al. (2023a) pointed out that human subjective and automatic objective evaluation measurements may yield opposite results, which underscores the limitations of existing automatic metrics for evaluating generated explanations and dialogues in LMRSs.
sent14: Figure  3 displays usage frequency statistics for different evaluation metrics in their respective tasks.","1. What role does evaluation play in recommendation design? sent1
2. What are some well-known metrics used in offline mode for evaluating recommendations? sent2
    2.1. What specific metrics are used to evaluate group recommendations? sent2
3. How is performance evaluated with online users? sent3
4. What additional possibilities do generative modules like GPT and T5 offer in recommender systems? sent4
    4.1. What metrics are commonly used to evaluate the relevance of generated text in these systems? sent5
    4.2. What other metrics are used to measure fluency, diversity, and informativeness of generated texts? sent6
5. Are there evaluation metrics specific to special requests in LMRSs? sent7
    5.1. What metrics did Xie et al. (2023) adopt for evaluating generated explanations? sent8
    5.2. What metrics did Geng et al. (2022a) use for evaluating text-image alignment? sent9
    5.3. How did Wang et al. (2022a) assess the system's capability to provide item recommendations during conversations? sent10
        5.3.1. How was recommendation performance evaluated to prevent inappropriate insertion of items into dialogues? sent11
6. How does human evaluation complement objective evaluation in LMRSs? sent12
    6.1. What did Liu et al. (2023a) point out about the limitations of existing automatic metrics? sent13
7. What does Figure 3 display regarding evaluation metrics? sent14","5. Are there evaluation metrics specific to special requests in LMRSs? sent7
    5.1. What metrics did Xie et al. (2023) adopt for evaluating generated explanations? sent8
    5.2. What metrics did Geng et al. (2022a) use for evaluating text-image alignment? sent9
    5.3. How did Wang et al. (2022a) assess the system's capability to provide item recommendations during conversations? sent10
        5.3.1. How was recommendation performance evaluated to prevent inappropriate insertion of items into dialogues? sent11","5. Are there evaluation metrics specific to special requests in LMRSs? sent7
    5.1. What metrics did Xie et al. (2023) adopt for evaluating generated explanations? sent8
    5.2. What metrics did Geng et al. (2022a) use for evaluating text-image alignment? sent9
    5.3. How did Wang et al. (2022a) assess the system's capability to provide item recommendations during conversations? sent10
        5.3.1. How was recommendation performance evaluated to prevent inappropriate insertion of items into dialogues? sent11","Question: How is the recommendation performance evaluated to ensure appropriate item insertion in conversational systems?

Supporting sentence: sent10, sent11",How is the recommendation performance evaluated to ensure appropriate item insertion in conversational systems?,"sent10, sent11",4,implement,"To assess the system's capability to provide item recommendations during conversations, Wang et al. (2022a) computed the Item Ratio within the final generated responses.

They evaluated the recommendation performance in an end-to-end manner to prevent the inappropriate insertion of recommended items into dialogues.","Question:

How is the recommendation performance evaluated to ensure appropriate item insertion in conversational systems?

Answer:

To assess the system's capability to provide item recommendations during conversations, Wang et al. (2022a) computed the Item Ratio within the final generated responses.

They evaluated the recommendation performance in an end-to-end manner to prevent the inappropriate insertion of recommended items into dialogues."
258378191,A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,https://www.semanticscholar.org/paper/0d94850737bcf0df409ac7f2a6610f6ec69a6809,Pre-trained Language Models,6,"Recently, pre-trained language models have advanced the state-of-the-art in many NLP tasks ranging from textual similarity to text summarization (Zhang et al., 2019;Liu and Lapata, 2019;Zhong et al., 2020) and named entity recognition (Zhou et al., 2021). State-of-the-art pre-trained models include LSTM-based language models (e.g., ELMo (Peters et al., 2018)) and Transformer-based language models (e.g., BERT 2 (Devlin et al., 2019) and RoBERTa ). Specifically, the transformer-based models learn bidirectional representations for words based on a masked language model and sentence adjacency training objective (Devlin et al., 2019). Simply using contextualized embeddings obtained from the transformerbased pre-trained language models in place of traditional embeddings has resulted in state-of-the-art performance on a range of NLP tasks. Therefore, pre-trained language models have been employed as encoders for obtaining word-, sentence-, and document-level representations to assist the downstream tasks.","sent1: Recently, pre-trained language models have advanced the state-of-the-art in many NLP tasks ranging from textual similarity to text summarization (Zhang et al., 2019;Liu and Lapata, 2019;Zhong et al., 2020) and named entity recognition (Zhou et al., 2021).
sent2: State-of-the-art pre-trained models include LSTM-based language models (e.g., ELMo (Peters et al., 2018)) and Transformer-based language models (e.g., BERT 2 (Devlin et al., 2019) and RoBERTa ).
sent3: Specifically, the transformer-based models learn bidirectional representations for words based on a masked language model and sentence adjacency training objective (Devlin et al., 2019).
sent4: Simply using contextualized embeddings obtained from the transformerbased pre-trained language models in place of traditional embeddings has resulted in state-of-the-art performance on a range of NLP tasks.
sent5: Therefore, pre-trained language models have been employed as encoders for obtaining word-, sentence-, and document-level representations to assist the downstream tasks.","1. What advancements have pre-trained language models brought to NLP tasks? sent1
    1.1. What are some examples of state-of-the-art pre-trained language models? sent2
    1.2. How do transformer-based models learn representations for words? sent3
    1.3. What impact has the use of contextualized embeddings from transformer-based models had on NLP tasks? sent4
    1.4. How are pre-trained language models used in relation to downstream tasks? sent5","1. What advancements have pre-trained language models brought to NLP tasks? sent1
    1.1. What are some examples of state-of-the-art pre-trained language models? sent2
    1.2. How do transformer-based models learn representations for words? sent3
    1.3. What impact has the use of contextualized embeddings from transformer-based models had on NLP tasks? sent4
    1.4. How are pre-trained language models used in relation to downstream tasks? sent5","1. What advancements have pre-trained language models brought to NLP tasks? sent1
    1.1. What are some examples of state-of-the-art pre-trained language models? sent2
    1.2. How do transformer-based models learn representations for words? sent3
    1.3. What impact has the use of contextualized embeddings from transformer-based models had on NLP tasks? sent4
    1.4. How are pre-trained language models used in relation to downstream tasks? sent5","Question: How do pre-trained language models enhance NLP tasks through contextualized embeddings and representation learning?

Supporting sentence: sent1, sent2, sent3, sent4, sent5",How do pre-trained language models enhance NLP tasks through contextualized embeddings and representation learning?,"sent1, sent2, sent3, sent4, sent5",4,implement,"Recently, pre-trained language models have advanced the state-of-the-art in many NLP tasks ranging from textual similarity to text summarization (Zhang et al., 2019; Liu and Lapata, 2019; Zhong et al., 2020) and named entity recognition (Zhou et al., 2021).

State-of-the-art pre-trained models include LSTM-based language models (e.g., ELMo (Peters et al., 2018)) and Transformer-based language models (e.g., BERT (Devlin et al., 2019) and RoBERTa).

Specifically, the transformer-based models learn bidirectional representations for words based on a masked language model and sentence adjacency training objective (Devlin et al., 2019).

Simply using contextualized embeddings obtained from the transformer-based pre-trained language models in place of traditional embeddings has resulted in state-of-the-art performance on a range of NLP tasks.

Therefore, pre-trained language models have been employed as encoders for obtaining word-, sentence-, and document-level representations to assist the downstream tasks.","Question:

How do pre-trained language models enhance NLP tasks through contextualized embeddings and representation learning?

Answer:

Recently, pre-trained language models have advanced the state-of-the-art in many NLP tasks ranging from textual similarity to text summarization (Zhang et al., 2019; Liu and Lapata, 2019; Zhong et al., 2020) and named entity recognition (Zhou et al., 2021).

State-of-the-art pre-trained models include LSTM-based language models (e.g., ELMo (Peters et al., 2018)) and Transformer-based language models (e.g., BERT (Devlin et al., 2019) and RoBERTa).

Specifically, the transformer-based models learn bidirectional representations for words based on a masked language model and sentence adjacency training objective (Devlin et al., 2019).

Simply using contextualized embeddings obtained from the transformer-based pre-trained language models in place of traditional embeddings has resulted in state-of-the-art performance on a range of NLP tasks.

Therefore, pre-trained language models have been employed as encoders for obtaining word-, sentence-, and document-level representations to assist the downstream tasks."
234093015,A Survey of Data Augmentation Approaches for NLP,https://www.semanticscholar.org/paper/63d8426ba1f51a8525dd19fd8ec92934ec71aea5,Background,6,"What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","sent1: What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.
sent2: Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020).
sent3: DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training.
sent4: In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.
sent5: What are the goals and trade-offs?
sent6: Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).
sent7: As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance.
sent8: Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).
sent9: Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.
sent10: Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.
sent11: Further, the distribution of augmented data should neither be too similar nor too different from the original.
sent12: This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.
sent13: Effective DA approaches should aim for a balance.
sent14: Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.
sent15: Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.
sent16: Overall, there indeed appears to be a lack of research on why exactly DA works.
sent17: Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles.
sent18: We discuss this challenge more in §6, and highlight some of the existing work below.
sent19: Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2).
sent20: Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.
sent21: Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization.
sent22: Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","1. What is data augmentation (DA) in the context of NLP? sent1
    1.1. How do most data augmentation strategies work? sent2
    1.2. Where has data augmentation been commonly used, and what techniques are standard? sent3
    1.3. What challenges exist for data augmentation in NLP compared to CV? sent4
2. What are the goals and trade-offs of data augmentation? sent5
    2.1. What range of DA techniques have been proposed for NLP? sent6
    2.2. What should an ideal DA technique achieve? sent7
    2.3. What are the trade-offs between rule-based and model-based DA techniques? sent8, sent9
    2.4. How do model-based techniques customized for downstream tasks affect performance? sent10
    2.5. What should be considered regarding the distribution of augmented data? sent11, sent12
    2.6. What balance should effective DA approaches aim for? sent13
    2.7. How do Kashefi and Hwa (2020) propose to choose among DA heuristics? sent14
3. How is data augmentation typically interpreted, and what challenges exist in understanding its principles? sent15
    3.1. What is the current state of research on why DA works? sent16, sent17
    3.2. Where is this challenge discussed further in the paper? sent18
4. What are some theoretical insights into data augmentation? sent19
    4.1. How do Bishop (1995) relate training with noised examples to regularization? sent19
    4.2. What do Rajput et al. (2019) show about DA and positive margin for classifiers? sent20
    4.3. How do Dao et al. (2019) conceptualize DA transformations, and what benefits do they find? sent21
    4.4. What do Chen et al. (2020d) demonstrate about DA and variance reduction? sent22","1. What is data augmentation (DA) in the context of NLP? sent1
    1.1. How do most data augmentation strategies work? sent2
    1.2. Where has data augmentation been commonly used, and what techniques are standard? sent3
    1.3. What challenges exist for data augmentation in NLP compared to CV? sent4

2. What are the goals and trade-offs of data augmentation? sent5
    2.1. What range of DA techniques have been proposed for NLP? sent6
    2.2. What should an ideal DA technique achieve? sent7
    2.3. What are the trade-offs between rule-based and model-based DA techniques? sent8, sent9
    2.4. How do model-based techniques customized for downstream tasks affect performance? sent10
    2.5. What should be considered regarding the distribution of augmented data? sent11, sent12
    2.6. What balance should effective DA approaches aim for? sent13
    2.7. How do Kashefi and Hwa (2020) propose to choose among DA heuristics? sent14

3. How is data augmentation typically interpreted, and what challenges exist in understanding its principles? sent15
    3.1. What is the current state of research on why DA works? sent16, sent17
    3.2. Where is this challenge discussed further in the paper? sent18","1. What is data augmentation (DA) in the context of NLP? sent1
    1.1. How do most data augmentation strategies work? sent2
    1.2. Where has data augmentation been commonly used, and what techniques are standard? sent3
    1.3. What challenges exist for data augmentation in NLP compared to CV? sent4","Question: What challenges are associated with implementing data augmentation techniques in NLP compared to computer vision?

Supporting sentence: sent4",What challenges are associated with implementing data augmentation techniques in NLP compared to computer vision?,sent4,4,implement,"In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance.","Question:

What challenges are associated with implementing data augmentation techniques in NLP compared to computer vision?

Answer:

In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance."
234093015,A Survey of Data Augmentation Approaches for NLP,https://www.semanticscholar.org/paper/63d8426ba1f51a8525dd19fd8ec92934ec71aea5,Background,6,"What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","sent1: What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.
sent2: Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020).
sent3: DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training.
sent4: In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.
sent5: What are the goals and trade-offs?
sent6: Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).
sent7: As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance.
sent8: Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).
sent9: Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.
sent10: Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.
sent11: Further, the distribution of augmented data should neither be too similar nor too different from the original.
sent12: This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.
sent13: Effective DA approaches should aim for a balance.
sent14: Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.
sent15: Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.
sent16: Overall, there indeed appears to be a lack of research on why exactly DA works.
sent17: Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles.
sent18: We discuss this challenge more in §6, and highlight some of the existing work below.
sent19: Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2).
sent20: Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.
sent21: Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization.
sent22: Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","1. What is data augmentation (DA) in the context of NLP? sent1
    1.1. How do most data augmentation strategies work? sent2
    1.2. Where has data augmentation been commonly used, and what techniques are standard? sent3
    1.3. What challenges exist for data augmentation in NLP compared to CV? sent4
2. What are the goals and trade-offs of data augmentation? sent5
    2.1. What range of DA techniques have been proposed for NLP? sent6
    2.2. What should an ideal DA technique achieve? sent7
    2.3. What are the trade-offs between rule-based and model-based DA techniques? sent8, sent9
    2.4. How do model-based techniques customized for downstream tasks affect performance? sent10
    2.5. What should be considered regarding the distribution of augmented data? sent11, sent12
    2.6. What balance should effective DA approaches aim for? sent13
    2.7. How do Kashefi and Hwa (2020) propose to choose among DA heuristics? sent14
3. How is data augmentation typically interpreted, and what challenges exist in understanding its principles? sent15
    3.1. What is the current state of research on why DA works? sent16, sent17
    3.2. Where is this challenge discussed further in the paper? sent18
4. What are some theoretical insights into data augmentation? sent19
    4.1. How do Bishop (1995) relate training with noised examples to regularization? sent19
    4.2. What do Rajput et al. (2019) show about DA and positive margin for classifiers? sent20
    4.3. How do Dao et al. (2019) conceptualize DA transformations, and what benefits do they find? sent21
    4.4. What do Chen et al. (2020d) demonstrate about DA and variance reduction? sent22","1. What is data augmentation (DA) in the context of NLP? sent1
    1.1. How do most data augmentation strategies work? sent2
    1.2. Where has data augmentation been commonly used, and what techniques are standard? sent3
    1.3. What challenges exist for data augmentation in NLP compared to CV? sent4

2. What are the goals and trade-offs of data augmentation? sent5
    2.1. What range of DA techniques have been proposed for NLP? sent6
    2.2. What should an ideal DA technique achieve? sent7
    2.3. What are the trade-offs between rule-based and model-based DA techniques? sent8, sent9
    2.4. How do model-based techniques customized for downstream tasks affect performance? sent10
    2.5. What should be considered regarding the distribution of augmented data? sent11, sent12
    2.6. What balance should effective DA approaches aim for? sent13
    2.7. How do Kashefi and Hwa (2020) propose to choose among DA heuristics? sent14

3. How is data augmentation typically interpreted, and what challenges exist in understanding its principles? sent15
    3.1. What is the current state of research on why DA works? sent16, sent17
    3.2. Where is this challenge discussed further in the paper? sent18","2. What are the goals and trade-offs of data augmentation? sent5
    2.1. What range of DA techniques have been proposed for NLP? sent6
    2.2. What should an ideal DA technique achieve? sent7
    2.3. What are the trade-offs between rule-based and model-based DA techniques? sent8, sent9
    2.4. How do model-based techniques customized for downstream tasks affect performance? sent10
    2.5. What should be considered regarding the distribution of augmented data? sent11, sent12
    2.6. What balance should effective DA approaches aim for? sent13
    2.7. How do Kashefi and Hwa (2020) propose to choose among DA heuristics? sent14","Question: What are the trade-offs between rule-based and model-based data augmentation techniques in NLP?

Supporting sentence: sent8, sent9",What are the trade-offs between rule-based and model-based data augmentation techniques in NLP?,"sent8, sent9",2,implement,"Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance.","Question:

What are the trade-offs between rule-based and model-based data augmentation techniques in NLP?

Answer:

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance."
