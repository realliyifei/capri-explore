# MIXED-PRECISION NEURAL NETWORKS: A SURVEY A PREPRINT

CorpusID: 251554723 - [https://www.semanticscholar.org/paper/30fa61a9c1db9527987fc25a91f26b9f23aa5152](https://www.semanticscholar.org/paper/30fa61a9c1db9527987fc25a91f26b9f23aa5152)

Fields: Computer Science

## (s25) APQ: Joint Search for Network Architecture, Pruning and Quantization Policy (APQ) 5
Number of References: 63

(p25.0) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p25.1) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p25.2) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.

(p25.3) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p25.4) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p25.5) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.

(p25.6) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p25.7) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p25.8) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.
## (s39) Comparison Against Binary Neural Networks
Number of References: 9

(p39.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p39.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p39.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s74) APQ: Joint Search for Network Architecture, Pruning and Quantization Policy (APQ) 5
Number of References: 63

(p74.0) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p74.1) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p74.2) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.

(p74.3) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p74.4) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p74.5) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.

(p74.6) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p74.7) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p74.8) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.
## (s88) Comparison Against Binary Neural Networks
Number of References: 9

(p88.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p88.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p88.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s123) APQ: Joint Search for Network Architecture, Pruning and Quantization Policy (APQ) 5
Number of References: 63

(p123.0) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p123.1) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p123.2) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.

(p123.3) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p123.4) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p123.5) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.

(p123.6) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p123.7) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p123.8) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.
## (s137) Comparison Against Binary Neural Networks
Number of References: 9

(p137.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p137.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p137.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
