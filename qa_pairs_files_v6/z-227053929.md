# Challenges in Deploying Machine Learning: a Survey of Case Studies

CorpusID: 227053929 - [https://www.semanticscholar.org/paper/a178a0bdee7549d87402b6c6128c569109128458](https://www.semanticscholar.org/paper/a178a0bdee7549d87402b6c6128c569109128458)

Fields: Computer Science, Business

## (s10) Hyper-parameter selection
Number of References: 3

(p10.0) In addition to parameters that are learned during the training process, many ML models also require hyper-parameters. Examples of such hyper-parameters are the depth of a decision tree, the number of hidden layers in a neural network or the number of neighbors in k-Nearest Neighbors classifier. Hyper-parameter optimization (HPO) is the process of choosing the optimal setting of these hyper-parameters. Most HPO techniques involve multiple training cycles of the ML model. This is computationally challenging because in the worst case the size of the HPO task grows exponentially: each new hyper-parameter adds a new dimension to the search space. As discussed by Yang and Shami [57], these considerations make HPO techniques very expensive and resourceheavy in practice, especially for applications of deep learning. Even approaches like Hyperband [58] or Bayesian optimization [59], that are specifically designed to minimize the number of training cycles needed, are not yet able to deal with the high dimensional searches that emerge when many hyper-parameters are involved. Large datasets complicate matters by leading to long training times for each search.
## (s22) End users' trust
Number of References: 8

(p22.0) ML is often met cautiously by the end users [106], [3], [107]. On their own accord, models provide minimal explanations, which makes it difficult to persuade end users of their utility [98]. In order to convince users to trust ML based solutions, the time has to be invested to build that trust. In this section, we explore ways in which that is done in practice.

(p22.1) If an application has a well-defined accessible audience, getting that audience involved early in the project is an efficient way to foster their confidence in the end product. This approach is very common in medicine, because the end product is often targeted at a well defined group of healthcare workers and/or patients. One example is the project called Sepsis Watch [108]. In this project the goal was to build a model that estimates a patient's risk of developing sepsis. It was not the first attempt at automating this prediction, and since previous attempts were considered failures, medical personnel were skeptical about the eventual success of Sepsis Watch. To overcome this skepticism, the team prioritized building trust, with strong communication channels, early engagement of stakeholders, front-line clinicians and decision makers, and established accountability mechanisms. One of the key messages of this work is that model interpretability has limits as a trust-building tool, and other ways to achieve high credibility with the end users should be considered. This aligns with conclusions made by "Project explAIn" which found that the relative importance of explanations of AI decisions varies by context [109]. A similar argument is made by Soden et al. [90], who explore the impact ML has on disaster risk management (DRM). Due to the growing complexity of the ML solutions deployed, it is becoming harder for the public to participate and consequently to trust the ML-based DRM services, such as flooding area estimates or prediction of damage from a hurricane. As a mitigation measure the authors recommend making the development of these solutions as transparent as possible, by taking into account the voice of residents in the areas portrayed by models as "at risk" and relying on open software and data whenever possible. The importance of strong communication and engagement with early adopters is also emphasized by Mutembesa et al. [110] as they analyzed their experience of deploying a nation-wide cassava disease surveillance system in Uganda.
## (s23) Security
Number of References: 4

(p23.0) Machine Learning opens up opportunities for new types of security attacks across the whole ML deployment workflow [114]. Specialised adversarial attacks for ML can occur on the model itself, the data used for training and also the resulting predictions. The field of adversarial machine learning studies the effect of such attacks against ML models and how to protect against them [115,116]. Recent work from Siva et al. found that industry practitioners are not equipped to protect, detect and respond to attacks on their ML systems [117]. In this section, we describe the three most common attacks reported in practice which affect deployed ML models: data poisoning, model stealing and model inversion. We focus specifically on adversarial machine learning and consider other related general security concerns in deploying systems such as access control and code vulnerabilities beyond the scope of our work.
