# Article title: Monitoring bias and fairness in machine learning models: A review Monitoring bias and fairness in machine learning models: A review

CorpusID: 235510937 - [https://www.semanticscholar.org/paper/7c05b1777a44fb6403636ebeff0a631794f0bcfd](https://www.semanticscholar.org/paper/7c05b1777a44fb6403636ebeff0a631794f0bcfd)

Fields: Psychology

## (s4) c) Sample bias
Number of References: 4

(p4.0) Another issue is sample bias, which happens when the data sample used to train the algorithm is not representative of the entire population due to systemic data collection errors. Any decisionmaking system based on this sample will be biased in either direction, favoring or opposing the over-or underrepresented party [12]. There are numerous explanations for a group's over-or underrepresentation in the results.

(p4.1) Another issue is sample bias, which happens when the data sample used to train the algorithm is not representative of the entire population due to systemic data collection errors. Any decisionmaking system based on this sample will be biased in either direction, favoring or opposing the over-or underrepresented party [12]. There are numerous explanations for a group's over-or underrepresentation in the results.
## (s5) d) Algorithm development bias
Number of References: 2

(p5.0) Apart from data bias as a source of unfairness, there are prejudices introduced during the algorithm's preparation. Throughout the process of developing an algorithm, researchers are confronted with a plethora of decisions that can steer the outcome in a variety of directions, including the selection of the dataset, the selection and encoding of features extracted from the dataset, the selection and encoding of the outcome variable, the rigor in which sources of bias in the data are identified, and the selection and specification of specific auxiliary variables [13]. Each decision is based on an implicit assumption. These assumptions are silent because they appear to be concealed behind an emphasis on mathematical precision and procedural rigor during the algorithm's development. Additionally, in many situations, the underlying assumptions are normative in nature.
## (s11) d) Approaches to Binary Classification
Number of References: 2

(p11.0) The majority of strategies for mitigating unfairness, bias, or discrimination are focused on the concept of protected or sensitive variables (we will use the words interchangeably) and (un)privileged classes: groups (often identified by one or more sensitive variables) that are statistically (less) more likely to be rated positively. Before delving into the critical components of the fairness system, it is necessary to address the essence of protected variables. Protected variables identify the characteristics of data that are socio-culturally precarious for machine learning applications. Sex, ethnic origin, and age are often used examples (as well as their synonyms) [17]. However, the term "protected variable" may refer to any aspect of data that includes or concerns humans.
## (s12) e) Debilitating/ Blinding
Number of References: 8

(p12.0) Blinding is a technique that makes a classifier "immunize" itself against one or more sensitive variables. For example, if there is no discernible outcome distinction based on the variable gender, a classifier is gender blind. For instance, suppose we want to train a gender blind classifier so that the loan rate given to each of the two gender groups is equivalent for both genders. Blinding has been used in other works to refer to the exclusion of critical variables from training results. It has been shown that omission reduces model accuracy and does not improve model discriminatio n. Both omission and immunity ignore relationships with proxy variables, which may result in an increase in rather than a decrease in bias and discrimination, or in concealment of discrimina tio n indirectly [18]. Additionally, it ignores the fact that prejudice may not be caused by a single variable, but rather by a combination of several variables; as such, evaluating which combination(s) of variables to blind is not easy, and the phenomenon of omitted variable bias should not be minimized. Similarly, researchers continue to use omission in their assessment methodologies in order to equate their findings to prior work and serve as a benchmark. Additionally, omission can be used in particular components of the fairness approach, such as temporarily omitting sensitive variables prior to transforming the training data. Blinding (or partial blinding) has also been used as a tool for conducting fairness audits [19]. More precisely, these approaches investigate the effect of partially blinding features (sensitive or otherwise) on model results. This is analogous to causal models and can assist in identifying problematic sensitive or proxy variables through black-box-like analysis of a machine learning model.

(p12.1) Blinding is a technique that makes a classifier "immunize" itself against one or more sensitive variables. For example, if there is no discernible outcome distinction based on the variable gender, a classifier is gender blind. For instance, suppose we want to train a gender blind classifier so that the loan rate given to each of the two gender groups is equivalent for both genders. Blinding has been used in other works to refer to the exclusion of critical variables from training results. It has been shown that omission reduces model accuracy and does not improve model discriminatio n. Both omission and immunity ignore relationships with proxy variables, which may result in an increase in rather than a decrease in bias and discrimination, or in concealment of discrimina tio n indirectly [18]. Additionally, it ignores the fact that prejudice may not be caused by a single variable, but rather by a combination of several variables; as such, evaluating which combination(s) of variables to blind is not easy, and the phenomenon of omitted variable bias should not be minimized. Similarly, researchers continue to use omission in their assessment methodologies in order to equate their findings to prior work and serve as a benchmark. Additionally, omission can be used in particular components of the fairness approach, such as temporarily omitting sensitive variables prior to transforming the training data. Blinding (or partial blinding) has also been used as a tool for conducting fairness audits [19]. More precisely, these approaches investigate the effect of partially blinding features (sensitive or otherwise) on model results. This is analogous to causal models and can assist in identifying problematic sensitive or proxy variables through black-box-like analysis of a machine learning model.
## (s16) g) Regularization and Optimization of Constraints
Number of References: 2

(p16.0) Regularization has traditionally been used in machine learning to penalize the difficulty of the learned hypothesis in order to prevent over-fitting [22]. Regularization techniques, when applied to fairness, apply one or more penalty words that penalize the classifier for discriminator y practices. Thus, it is not hypothesis-driven (or learned model-driven), but data-driven and founded on the considered notion(s) of justice. Most of the literature extends or augments the classifier's (convex) loss function with fairness terms, usually in an attempt to strike a balance between fairness and accuracy. Frequently, approaches to fair machine learning are not stable, i.e., small changes in the training data have a major effect on results (comparatively high standard deviatio n). During model training, constraint optimization approaches often incorporate notions of fairness into the classifier loss function operating on the confusion matrix.
## (s27) c) Sample bias
Number of References: 4

(p27.0) Another issue is sample bias, which happens when the data sample used to train the algorithm is not representative of the entire population due to systemic data collection errors. Any decisionmaking system based on this sample will be biased in either direction, favoring or opposing the over-or underrepresented party [12]. There are numerous explanations for a group's over-or underrepresentation in the results.

(p27.1) Another issue is sample bias, which happens when the data sample used to train the algorithm is not representative of the entire population due to systemic data collection errors. Any decisionmaking system based on this sample will be biased in either direction, favoring or opposing the over-or underrepresented party [12]. There are numerous explanations for a group's over-or underrepresentation in the results.
## (s28) d) Algorithm development bias
Number of References: 2

(p28.0) Apart from data bias as a source of unfairness, there are prejudices introduced during the algorithm's preparation. Throughout the process of developing an algorithm, researchers are confronted with a plethora of decisions that can steer the outcome in a variety of directions, including the selection of the dataset, the selection and encoding of features extracted from the dataset, the selection and encoding of the outcome variable, the rigor in which sources of bias in the data are identified, and the selection and specification of specific auxiliary variables [13]. Each decision is based on an implicit assumption. These assumptions are silent because they appear to be concealed behind an emphasis on mathematical precision and procedural rigor during the algorithm's development. Additionally, in many situations, the underlying assumptions are normative in nature.
## (s34) d) Approaches to Binary Classification
Number of References: 2

(p34.0) The majority of strategies for mitigating unfairness, bias, or discrimination are focused on the concept of protected or sensitive variables (we will use the words interchangeably) and (un)privileged classes: groups (often identified by one or more sensitive variables) that are statistically (less) more likely to be rated positively. Before delving into the critical components of the fairness system, it is necessary to address the essence of protected variables. Protected variables identify the characteristics of data that are socio-culturally precarious for machine learning applications. Sex, ethnic origin, and age are often used examples (as well as their synonyms) [17]. However, the term "protected variable" may refer to any aspect of data that includes or concerns humans.
## (s35) e) Debilitating/ Blinding
Number of References: 8

(p35.0) Blinding is a technique that makes a classifier "immunize" itself against one or more sensitive variables. For example, if there is no discernible outcome distinction based on the variable gender, a classifier is gender blind. For instance, suppose we want to train a gender blind classifier so that the loan rate given to each of the two gender groups is equivalent for both genders. Blinding has been used in other works to refer to the exclusion of critical variables from training results. It has been shown that omission reduces model accuracy and does not improve model discriminatio n. Both omission and immunity ignore relationships with proxy variables, which may result in an increase in rather than a decrease in bias and discrimination, or in concealment of discrimina tio n indirectly [18]. Additionally, it ignores the fact that prejudice may not be caused by a single variable, but rather by a combination of several variables; as such, evaluating which combination(s) of variables to blind is not easy, and the phenomenon of omitted variable bias should not be minimized. Similarly, researchers continue to use omission in their assessment methodologies in order to equate their findings to prior work and serve as a benchmark. Additionally, omission can be used in particular components of the fairness approach, such as temporarily omitting sensitive variables prior to transforming the training data. Blinding (or partial blinding) has also been used as a tool for conducting fairness audits [19]. More precisely, these approaches investigate the effect of partially blinding features (sensitive or otherwise) on model results. This is analogous to causal models and can assist in identifying problematic sensitive or proxy variables through black-box-like analysis of a machine learning model.

(p35.1) Blinding is a technique that makes a classifier "immunize" itself against one or more sensitive variables. For example, if there is no discernible outcome distinction based on the variable gender, a classifier is gender blind. For instance, suppose we want to train a gender blind classifier so that the loan rate given to each of the two gender groups is equivalent for both genders. Blinding has been used in other works to refer to the exclusion of critical variables from training results. It has been shown that omission reduces model accuracy and does not improve model discriminatio n. Both omission and immunity ignore relationships with proxy variables, which may result in an increase in rather than a decrease in bias and discrimination, or in concealment of discrimina tio n indirectly [18]. Additionally, it ignores the fact that prejudice may not be caused by a single variable, but rather by a combination of several variables; as such, evaluating which combination(s) of variables to blind is not easy, and the phenomenon of omitted variable bias should not be minimized. Similarly, researchers continue to use omission in their assessment methodologies in order to equate their findings to prior work and serve as a benchmark. Additionally, omission can be used in particular components of the fairness approach, such as temporarily omitting sensitive variables prior to transforming the training data. Blinding (or partial blinding) has also been used as a tool for conducting fairness audits [19]. More precisely, these approaches investigate the effect of partially blinding features (sensitive or otherwise) on model results. This is analogous to causal models and can assist in identifying problematic sensitive or proxy variables through black-box-like analysis of a machine learning model.
## (s39) g) Regularization and Optimization of Constraints
Number of References: 2

(p39.0) Regularization has traditionally been used in machine learning to penalize the difficulty of the learned hypothesis in order to prevent over-fitting [22]. Regularization techniques, when applied to fairness, apply one or more penalty words that penalize the classifier for discriminator y practices. Thus, it is not hypothesis-driven (or learned model-driven), but data-driven and founded on the considered notion(s) of justice. Most of the literature extends or augments the classifier's (convex) loss function with fairness terms, usually in an attempt to strike a balance between fairness and accuracy. Frequently, approaches to fair machine learning are not stable, i.e., small changes in the training data have a major effect on results (comparatively high standard deviatio n). During model training, constraint optimization approaches often incorporate notions of fairness into the classifier loss function operating on the confusion matrix.
