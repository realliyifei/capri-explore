# Siamese Learning Visual Tracking: A Survey

CorpusID: 11786999 - [https://www.semanticscholar.org/paper/754fa133a250d824c50b4c3b9c73975059954f41](https://www.semanticscholar.org/paper/754fa133a250d824c50b4c3b9c73975059954f41)

Fields: Computer Science, Mathematics

## (s18) CFNet
Number of References: 3

(p18.0) [54] adds a correlation filter and crop layers to the branch that concerns the template. These layers follow directly the convolutional network. The input is a larger region of the frame including the template, hence resolution of feature maps in the branches and prediction map is larger. Feature maps are further multiplied by a cosine window and cropped after correlation to remove the effect of circular boundaries. CFNet inherits the basic ability from SiamFC to discriminate spatial features with triplets of template region, search region and corresponding prediction map. Instead of unconstrained features, CFNet learns features that especially discriminate and solve the underlying ridge regression of the correlation layer by exploiting background samples in the surrounding region of the template. The learnt parameters of the correlation layer remain fixed during tracking, no online learning happens as shown by [60]. Training is done as with SiamFC by using the same algorithms on videos of objects from ImageNet. To make training end-to-end, emphasis has been on a differential correlation layer and on back-propagation of the parameters. Correlation is formulated in the Fourier space to preserve efficiency of computation. Tracking is similar simple as in SiamFC and computes position and scale by a single feed forward pass. The prediction map is multiplied by a spatial cosine window to penalise larger displacements. Instead of handling five different scale variations, scale is handled as by [61]. To fully exploit the correlation filter, the initial template is updated in each frame by a moving average. [52] proposes two convolutional branches inherited from AlexNet up to pool5. Both branches share the same parameters. These pool5 features of both branches are connected to a single vector and fed to three fc layers. ReLUs are used after each fc layer. The final fc layer links to an output layer with four nodes describing the bounding box. The output is scaled by a validated constant factor. GOTURN learns simultaneously the hierarchy of spatial features in the branches as well as spatiotemporal features and the similarity function in the fc layers to discriminate between template and search region with corresponding bounding boxes. Training is done in two stages on augmented images of objects from ImageNet and on videos from ALOV by using standard back-propagation of CaffeNet. Augmentation assumes linear translation and constant scale with parameters sampled from a Laplace distribution, hence small motion is assumed to occurs more frequently than larger displacements. Training minimises a L 1 loss between predicted and ground truth bounding box by using mini-batches, dropout and pre-training of the branches on ImageNet without fine-tuning these parameters to prevent overfitting. Tracking initialises the template in the first frame and updates the template with the predicted bounding box for each frame. Crops of the current and next frame yield template and search region. These crops are not exact but padded to add context.
## (s22) Connection of Branches
Number of References: 2

(p22.0) All proposed methods except SINT connect the branches, SiamFC and CFNet with a single crosscorrelation layer, YCNN and GOTURN with three fc layers. SINT omits the concatenating layer by using normalisation layers at the end of both branches. Fig. 2 illustrates these three variations of network architecture. This Siamese network architecture of SiamFC, CFNet and SINT in combination with parameter sharing limit the feature selection to the spatial image domain. Instead, YCNN and GO-TURN enable additional learning of spatiotemporal features in the concatenating layers, as argued by [52] such as "relationships between an object's appearance change and its motion" which seems very general for different categories of objects. Parameter sharing has the consequence that all methods require appearance constancy between template and search region, hence [51]'s argument that YCNN's deep features show "superiority of recognising an object with varying appearance" is questionable. SiamFC, Theoretically, the network of GOTURN generalises over the network of SiamFC which allows capturing features beyond sole visual features of the exemplar image and which allows regression of the bounding box instead of convoluting a final score map capturing potential positions of the exemplar image within the search image. The author's argue that GOTURN learns a generic relationship between arbitrary motion and visual features, however this is not clear yet. Due to the more general Y-shaped architecture it might learn features beyond pure visual such as motion and their relationship in the fully-connected layers, however the network might also be able to learn context features as well.
## (s23) Network Training
Number of References: 2

(p23.0) Training is a crucial for sufficient performance. All methods describe basically two training phases, (i) a pre-training phase to transfer-learn generic features of objects from labeled datasets and (ii) a fine-tuning phase to adapt features to given video sets. The cross-correlation layer has here advantages as crosscorrelation preserves the convolutional property of the whole network which introduces invariance to object translation. Therefore training samples must not contain translated versions which reduces significantly the effort for training. Less augmentation of training data is needed. The training loss and its choice has significant influence on the training result. [52] argue that L 1 is superior to L 2 as it penalises more harshly small errors near zero which increases substantially accuracy of the predicted bounding box. This argument is an exception, as none of the other studies show some insights into this important problem. [52] chose also different inputs for training and studied their influence on the mean error derived from VOT accuracy and robustness measures. They show that feeding the network with whole frames instead of template and search region pairs, the frames' contexts are exploited which reduces significantly the mean error, especially in cases of occlusion. SINT is the only method following this insight but without any hints of their awareness. The reason is that their motivation comes from image retrieval where processing of frames is common.
## (s25) Tracker Results
Number of References: 5

(p25.0) CFNet seems a promising method as it has stateof-the-art performance with 75 fps 5 with less than 5. GOTURN runs best at 100 fps [52]. 4 % of the parameters of other five layer methods (in total 600 kB) such as SiamFC. This makes CFNet applicable to embedded applications. CFNet, SiamFC and SINT show comparable performance by reaching IoU/prec. 60/80 % on OTB-2013 and one pass evaluation (OPE) 6 . [51] reports significant lower IoU/prec. of 60/70 %. [52] did not report results on OTB-2013.
