# Convergence of Edge Computing and Deep Learning: A Comprehensive Survey

CorpusID: 197935335 - [https://www.semanticscholar.org/paper/4cd03cd34e7e94d1b1ee293d5dead8efc24c1a6d](https://www.semanticscholar.org/paper/4cd03cd34e7e94d1b1ee293d5dead8efc24c1a6d)

Fields: Engineering, Computer Science

## (s15) A. Optimization of DL Models in Edge
Number of References: 14

(p15.0) DL tasks are usually computationally intensive and requires large memory footprints. But in the edge, there are not enough resources to support raw large-scale DL models. Optimizing DL models and quantize their weights can reduce resource costs. In fact, model redundancies are common in DNNs [109], [110] and can be utilized to make model optimization possible. The most important challenge is how to ensure that there is no significant loss in model accuracy after being optimized. In other words, the optimization approach should transform or re-design DL models and make them fit in edge devices, with as little loss of model performance as possible. In this section, optimization methods for different scenarios are discussed: 1) general optimization methods for edge nodes with relatively sufficient resources; 2) fine-grained optimization methods for end devices with tight resource budgets.

(p15.1) 1) General Methods for Model Optimization: On one hand, increasing the depth and width of DL models with nearly constant computation overhead is one direction of optimization, such as inception [111] and deep residual networks [112] for CNNs. On the other hand, for more general neural network structures, existing optimization methods can be divided into four categories [113]: 1) parameter pruning and sharing [114], [115], including also weights quantization [116]- [118]; 2) low-rank factorization [109]; 3) transferred/compact convolution filters [94], [119], [120]; 4) knowledge distillation [121]. These approaches can be applied to different kinds of DNNs or be composed to optimize a complex DL model for the edge.
## (s27) B. Task Offloading Optimization
Number of References: 6

(p27.0) Edge computing allows edge devices offload part of their computing tasks to the edge node [194], under constraints of energy, delay, computing capability, etc. As shown in Fig.  19, these constraints put forward challenges of identifying 1) which edge nodes should receive tasks, 2) what ratio of tasks edge devices should offload and 3) how many resources should be allocated to these tasks. To solve this kind of task offloading problem is NP-hard [195], since at least combination optimization of communication and computing resources along with the contention of edge devices is required. Particularly, the optimization should concern both the time-varying wireless environments (such as the varying channel quality) and requests of task offloading, hence drawing the attention of using learning methods [196]- [205]. Among all these works related to learning-based optimization methods, DL-based approaches have advantages over others when multiple edge nodes and radio channels are available for computation offloading. At this background, large state and action spaces in the whole offloading problem make the conventional learning algorithms [196] [206] [198] infeasible actually.
