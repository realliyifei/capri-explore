# Deep Learning for Visual Speech Analysis: A Survey

CorpusID: 248987614 - [https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785](https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785)

Fields: Computer Science

## (s16) Visual Quality.
Number of References: 5

(p16.0) To evaluate the quality of the synthesized video frames, reconstruction error measurement (e.g., Mean Squared Error) is a natural evaluation way. However, reconstruction error only focuses on pixel-wise alignments and ignores global visual quality. Therefore, existing works usually adopt Peak Signal-to-Noise Ratio (PSNR) and Structure Similarity Index Measure (SSIM) to evaluate the global visual quality of generated frames. More recently, Prajwal et al. [38] introduced Fr√©chet Inception Distance (FID) to measure the distance between synthetic and real data distributions, as FID is more consistent with human perception evaluation. Besides, Cumulative Probability Blur Detection (CPBD) [115], a nonreference measure, is also widely used to evaluate the loss of sharpness during video generation.

(p16.1) Audio-visual Semantic Consistency. Semantic consistency of the generated video and the driving source mainly contains audio-visual synchronization and speech consistency. For, audio-visual synchronization, Landmark Distance (LMD) [116] computes the Euclidean distance of the lip region landmarks between the synthesized video frames and ground truth frames. The other synchronization evaluation metric is to use a pre-trained audio-to-video synchronisation network [48] to predict the offset of generated frames and the ground truth. For the speech consistency, Chen et al. [42] proposed a lipsynchronization evaluation metric, i.e., Lip-Reading Similarity Distance (LRSD), which measures the Euclidean distance of semantic-level speech embeddings obtained by lip reading networks. For better evaluation of speech consistency, lip reading results (accuracy, CER, or WER) comparisons of the generated frames and ground truth are also used as consistency evaluation metrics.
## (s31) Landmark based Methods
Number of References: 20

(p31.0) Facial landmark points around facial components capture the rigid and non-rigid facial deformations due to head movements and facial expressions [160]. Facial landmark points are widely used in various facial analysis tasks, including VSG. As a pioneering work, Suwajanakorn et al. [87] adopted a simple single-layer LSTM with the time delay mechanism to learn a nonlinear mapping from audio coefficients to lip landmark points. As shown in Fig. 8(b), the model outputs the synthesized talking face video of former US President Barack Obama, following the pipeline of facial texture synthesis, video re-timing, and target video compositing. Beyond computer graphic video generation methods, as shown in Fig. 8(c), Kumar et al. [64] proposed the LSTM + UNet architecture, improving the model by replacing the complex video generation pipeline with a pix2pix framework [161]. In this way, there is no need to get    involved with details of a face, e.g., synthesizing realistic teeth. However, as the above methods are trained on only Barack Obama with many hours of his weekly address footage, they cannot generalize to new identities or voices. The LSTM + UNet VSG backbone architecture is widely adopted in many subsequent works [15,97,162,163]. Unlike previous methods using audio MFCC features as input, Sinha et al. [162,163] introduced DeepSpeech [164] features instead, as DeepSpeech features are more robust for the variation of speakers. In 2018, Jalalifar et al. [165] proposed LSTM + C-GAN VSG backbone architecture, using the basic conditional generative adversarial network (C-GAN) [166] for generating talking faces given the learned landmarks. As the LSTM network and C-GAN network are mutual independence, this model can reanimate the target face with audio from another person. In 2019, Chen et al. [65] proposed a novel LSTM + Convolutional-RNN structure, further considering the correlation between adjacent video frames during generation. Besides, they also propose a novel dynamic pixel-wise Loss to solve the pixel jittering problem in correlated audio-visual regions. Wang et al. [167] proposed a three-stage VSG framework. Firstly, they use the 3D Hourglass Network as a motion field generator to predict landmark points based on the input audio, head motions, and the reference image. And then convert the predicted landmark points to dense motion fields. Finally, the synthesized talking video is obtained using a first-order motion model [168]. Recently, they further updated the motion field generator by replacing the 3D Hourglass Network with a self-attention architecture [41].

(p31.1) Besides 2D landmark based approaches, mapping driving source to 3D landmarks is also widely explored. Audio signals contain sich semantic-level information, including speech content, speaker's speaking style, emotion, etc. Zhou et al. [15] utilized a voice conversion neural network to learn disentangled speech content and identity features. Then, an LSTM-based network is introduced to predict 3D landmarks based on speech content features. Finally, the final synthesized talking face video is realized using a UNet-style generator network. The key insight is to predict 3D landmarks from disentangled audio content features and speaker-aware features, such that they capture controllable lip synchronization and head motion dynamics. As shown in Fig. 8(d), Lu et al. [169] introduced extracting the high-level speech information using an autoregressive predictive coding (APC) model [170] and manifold projection for better generalization. Then, an audio to lip-related motion module is designed to predict 3D lip landmarks. Finally, an image-to-image translation network (UNet) is introduced to synthesize video frames.
## (s37) Other Methods
Number of References: 3

(p37.0) In addition, some other one-stage VSG schemes have also been proposed. Inspired by the success of the neural radiance field (NeRF) [200], Guo et al. [73] proposed the audio-driven neural radiance fields (AD-NeRF) model for VSG. As shown in Fig. 8(k), AD-NeRF takes DeepSpeech audio features as conditional input, learning an implicit neural scene representation function to map audio features to dynamic neural radiance fields for talking face rendering. Furthermore, AD-NeRF models not only the head region but also the upper body via learning two individual neural radiance fields. However, AD-NeRF does not generalize well on mismatched driving audios and speakers. As shown in Fig. 8(l), unlike the previous concatenation-based feature fusion strategy, Ye et al. [74] presented a full convolutional neural network with dynamic convolution kernels (DCKs) for crossmodal feature fusion, which extracts features from audio and reshapes features as DCKs of the fully convolutional network. Due to the simple yet effective network architecture, the realtime performance of VSG is significantly improved.
