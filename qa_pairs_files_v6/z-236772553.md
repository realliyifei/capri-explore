# A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning

CorpusID: 236772553 - [https://www.semanticscholar.org/paper/c8ab8697afff4fd67714f4e7512a1a88c0a4167e](https://www.semanticscholar.org/paper/c8ab8697afff4fd67714f4e7512a1a88c0a4167e)

Fields: Computer Science, Mathematics

## (s11) Application scenarios
Number of References: 10

(p11.0) A brief description of practical scenarios where we must handle noisy and costly target evaluations is provided below. Namely, all the settings given below can be encompassed in the general framework described above. Pseudo-Marginal approach: Here, the unnormalized density can be expressed as a marginal distribution, i.e., p(θ) = V p(θ, v)dv where v is an auxiliary variable. Hence, p(θ) cannot be computed in closed-form. When the aim is to run a MH algorithm on p(θ), rather than on the joint p(θ, v), the evaluation of p(θ) at each θ can be estimated noisily by using IS [5,4]. ABC, likelihoods-free. In the likelihood-free (and/or synthetic likelihood) inference setting, it is assumed that the likelihood function is unknown or we cannot evaluate it, but we are able to generate independent data from it. In this scenario, substituting the intractable likelihood with an approximate likelihood is one possibility. This approximation is in turn approximated pointwise with Monte Carlo using pseudo-data sets [43,53]. See Section 5.1 below for more details. Doubly intractable posteriors. When only a part of the likelihood can be evaluated and another piece of the likelihood is unknown (typically a partition function Z(θ)), we are in doubly intractable posterior setting. Note that differently from the ABC case, here some part of the likelihood is available. In this case, the unknown part of the likelihood must be estimated, so that the evaluation of the complete likelihood will be noisy. Use of mini-batches (Big Data). The evaluation of the likelihood function can be prohibitively expensive when there are huge amounts of data. In this context, a subsampling strategy consists in computing the log-likelihood function using a random subset of data points, hence forming an unbiased estimator of the complete log-likelihood [10]. Reinforcement learning (RL). Direct policy search is an important branch of reinforcement learning, particularly in robotics [21,15]. In this context, θ is the parametrization of the policy of some agent, and p(θ) represents an expected return (i.e., a payoff function) for that policy. The expected return is approximated by the empirical return over an episode, i.e., the agent is run for a number of time steps and accumulates a payoff. More details are given in Section 5.2.

(p11.1) Other application scenarios. The topic of inference in noisy and costly settings is also of interest in the inverse problem literature, such as in the calibration of expensive computer codes [24,17,12]. Noisy likelihood evaluations are also considered for building surrogates, and then use them in order to obtain a variational approximations to the posterior [1].
