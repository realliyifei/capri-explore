# The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities

CorpusID: 264832783 - [https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825](https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825)

Fields: Computer Science, Linguistics

## (s12) Demonstration Order
Number of References: 7

(p12.0) The order of the demonstrations has a significant impact on downstream task performance.Lu et al. (2022) showed that it be the deciding factor between achieving near stateof-the-art and random guessing.They designed demonstrations containing four samples with a balanced label distribution and conducted experiments involving all 24 possible permutations of sample orders.The experimental results showed that the performance variations among different permutations exist across various model sizes, especially for smaller models.Besides, is was observed that effective prompts are not transferrable across models, indicating that the optimal order is model-dependent, and what works well for one model does not guarantee good results for another model.Zhao et al. (2021) identified a phenomenon that LLMs tend to repeat answers found at the end of demonstrations, which they termed "recency bias".Similarly, in multi-document question answering and key-value retrieval tasks, Liu et al. ( 2023) made analogous observations.These tasks involve identifying relevant information within lengthy input contexts.The results showed that LLMs performed best when the relevant information is located at the beginning or end of their input contexts.However, their performance degraded when they are forced to use information from the middle of their input.In addition, they noted that model performance declines as the input context length increases, suggesting that current models struggle to effectively reason over their entire context window.Although these studies offer insights into how demonstration order influences emergent abilities, they do not delve into the underlying reasons of these obesrvations.In an effort to investigate the impact of semantic similarity between ICL examples and test examples on downstream task, Liu et al. (2022) proposed retrieving examples semantically similar to a test example for creating its demonstration.They utilized the CLS embeddings from a pre-trained RoBERTa-large (Liu et al., 2019) model to represent sentences and assessed the semantic similarity between two sentences by computing the cosine similarity of their respective representations.For each test example, they identified the nearest K neighbors from the training set and concatenated them in descending order of semantic similarity to create the demonstration.Their experiments on Web Questions (Berant et al., 2013) and Trivia Question Answering (Joshi et al., 2017) benchmarks showed that the default order performed slightly better than the reverse order.However, the reverse order performed better on the Natural Questions (Kwiatkowski et al., 2019) benchmark.Consequently, the choice of order appears to be dependent on the specific dataset in use.
