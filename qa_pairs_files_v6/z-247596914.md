# Visuo-Haptic Object Perception for Robots: An Overview

CorpusID: 247596914 - [https://www.semanticscholar.org/paper/49579115424853a2e479678023ef592fddaca003](https://www.semanticscholar.org/paper/49579115424853a2e479678023ef592fddaca003)

Fields: Engineering, Computer Science

## (s13) Tactile Sensors
Number of References: 8

(p13.0) Tactile sensors are mostly designed to mimic mechanoreceptors, particularly to detect mechanical pressure. The main objectives of tactile sensors are to determine the location, shape and intensity of contacts. These properties are determined by measuring the instantaneous pressure or force applied to the sensor's surface on multiple contact points. Also, the contact's late effects, i.e., body-borne vibrations, may carry relevant information. Body-borne vibrations are not as commonly measured or exploited as part of haptic sensing; however, there are some examples, e.g., Syrymova et al (2020); Toprak et al (2018), including sensors that are inspired by hair follicle receptors or ciliary structure (Alfadhel and Kosel, 2015;Ribeiro et al, 2017;Kamat et al, 2019) and that have been proven very effective in obtaining information about the texture of objects (Ribeiro et al, 2020b,a).

(p13.1) Thermoceptors, although an integral part of human haptic perception, are typically not classified as tactile sensors within robotic applications. However, they are sometimes included because they might help compensate for thermal effects (Tomo et al, 2016), thus helping to obtain a more robust electronic signal related to pressure or vibrations, or because they might help to classify the material of the object in contact (Wade et al, 2017). In contrast, nociceptors have not yet been developed as part of haptic or tactile sensing per se but can be and have been implemented in software based on the limitations of robots (e.g., Navarro-Guerrero et al, 2017b,a).
## (s28) Transfer Learning
Number of References: 2

(p28.0) One of the challenges of transfer learning (colearning) is that machine learning models are based on the assumption that both training and test data are drawn from the same distribution. However, such an assumption does not hold when transferring knowledge between different robots or sensor modalities. A possible solution is domain adaptation, a.k.a. transfer learning, (e.g., Daum√© III and Marcu, 2006;Wang and Deng, 2018). Here, training samples from a source dataset are adapted to fit a target distribution.
## (s35) Contact-rich manipulation
Number of References: 4

(p35.0) While traditional robotic manipulation is all about avoiding physical contacts with the environment that surrounds the objects, human manipulation is to a large extent about exploiting those contacts, as noted by Deimel et al (2016). Inspired by this observation, and by the presence of several applied example in industry, such as peg-in-hole insertion tasks (Jiang et al, 2020), the robotics community is showing increased interest in the development of robotic solutions for contact-rich manipulation tasks, as summarised by Suomalainen et al (2022). Clearly, visual perception is not enough for these tasks, and visuo-haptic integration becomes crucial. As a most notable example, Lee et al (2019b) recently proposed a system in which a robotic manipulator learns by deep reinforcement learning a control policy that includes sensory feedback from visual (RGB camera), haptic (force/torque sensor) and proprioceptive (motor encoders) sensing. A shared and compact representation of the high-dimensional and heterogeneous multimodal data is learned with a neural network, which is trained to predict optical flow, presence of contact, and concurrency of visual and haptic data; the neural network is then used as sensory feedback to learn a control policy for a peg insertion task, directly on the real robot. The experiments compare four models: no sensory feedback, vision only, haptics only, vision and haptics. Interestingly, while the model with haptics only performs as bad as the one with no feedback, because the robot cannot even pick the peg in most trials, the model with vision only performs the insertion successfully only about 50% of the times, while the model with both vision and haptics brings the success rate to about 75%.
## (s39) Data Collection and Datasets
Number of References: 3

(p39.0) Collecting tactile data during grasping on a real robot or correctly simulating tactile sensors for synthetic data generation are resource-intensive tasks, which in turn is reflected in datasets' availability and size. While there are many large-scale visiononly datasets for grasping in real-world scenarios or simulation (e.g., Jiang et al, 2011;Levine et al, 2018;Depierre et al, 2018), only a few small-scale visuo-tactile datasets exist. Thus, large-scale multimodal datasets should be created, considering a variety of objects, grasping scenarios and different tactile sensor types. However, data acquisition from tactile sensors still lacks a unified theoretical framework. The challenges here stem from the fact that haptic perception is an intrinsically sequential process. Moreover, haptic perception is highly dependent on the robot's embodiment which makes the generalization to other robots or tasks difficult. In addition to a unified theoretical framework for data acquisition, solving other standing computational challenges such as representation learning, mapping and co-learning seem to be key enabling technologies that could help cope with the resource-intensive nature of data acquisition.
## (s40) Multimodal Signal Processing and Applications
Number of References: 11

(p40.0) With regards to signal processing and applications, even though multimodal visuo-haptic approaches for grasping show better results and have the potential to handle use-cases where visual information alone is insufficient, vision-only grasping approaches (e.g., Levine et al, 2018;Mahler et al, 2017;Bousmalis et al, 2018;James et al, 2019) are still more popular. Some reasons for this popularity are that the availability, durability and understanding of vision sensors are better than tactile ones. Moreover, the simulation of vision sensors is easier and more realistic, and the collection, processing and interpretation of visual information are easier than tactile sensor readings. On the other side of the spectrum, there are also recent grasping approaches (e.g., Murali et al, 2020;Hogan et al, 2018) that only use tactile information, but such approaches are usually only suitable for limited scenarios or parts of the grasping process. Thus, future efforts should be concentrated on multimodal approaches. However, as discussed by Xia et al (2022), the main challenge is ensuring safety during the physical contact between the object and the robot necessary for tactile sensing. To avoid the hardware dependencies and the safety risks, simulations are a promising alternative to real-world training and data collection for learningbased grasping approaches. However, due to the inaccurate nature of simulations, they cannot completely replace, but they can significantly reduce, the amount of real-world data needed. Finally, finetuning on the real system or sim2real techniques (e.g., Ding et al, 2020;Narang et al, 2021) can help to bridge the simulation-to-reality gap.

(p40.1) Another major problem of data-driven and endto-end learning grasping approaches is that they require a vast amount of training data, in contrast to humans, who learn and generalize from very few examples. In this regard, future work should concentrate on improving the sample efficiency of the algorithms. One option is to include priors in the learning process, e.g., meaningful relations between tactile sensing regions can be incorporated into the model through graph-like structures, e.g., Garcia-Garcia et al (2019). Another option is combining model-based and model-free techniques for grasping or developing hierarchical and multi-stage approaches. An added benefit of such approaches is that they provide better control over the grasping process and increased interpretability of the model's behaviour, which is crucial for applications in industrial or collaborative environments alongside humans. Safety is of utmost importance in such environments, and integrating tactile sensors like robotic skin (Pang et al, 2021) can help improve tasks like grasping, prevent injuries, and enable compliant robot control.
