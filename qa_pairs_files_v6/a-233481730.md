# AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models

CorpusID: 233481730 - [https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c](https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c)

Fields: Linguistics, Medicine, Computer Science

## (s2) FOUNDATIONS
Number of References: 7

(p2.0) In general, the core components of transformer-based PLMs like BERT and RoBERTa are embedding and transformer encoder layers (refer Figure 4). The embedding layer takes input tokens and returns a vector for each. The embedding layer has three or more sub-layers each of which provides a vector of specific embedding type for each of the input tokens. The final input vector for each token is obtained by summing all the vectors of each embedding type. The transformer encoder layer enhances each input token vector by encoding global contextual information using the self-attention mechanism. By applying a sequence of such transformer encoder layers, the model can encode complex language information in the input token vectors. Usually, the embedding layer consists of three sublayers with each sub-layer representing a particular embedding type. In some models, there are more than three also. For example, embedding layer of BERT-EHR [27] contains code, position, segment, age and gender embeddings. A detailed description of various embedding types is presented in Section 3.4. The first sublayer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation. The first sublayer can be char, sub-word, or code embedding based. For example, BioCharBERT [28] uses CharCNN [29] on the top of character embeddings, BERT uses WordPiece [30] embeddings while BEHRT [27], MedBERT [31] and BERT-EHR [32] models use code embeddings. Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes. The final input representation X for the given input tokens {x 1 , x 2 , . . . x n }is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types -refer Figure 5).

LLM judge: YES

What are the core components of transformer-based pre-trained language models like BERT and RoBERTa, and how do they process input tokens?

## (s16) Pretraining Tasks
Number of References: 3

(p16.0) During pretraining, the language models learn language representations based on the supervision provided by one or more pretraining tasks. A pretraining task is a pseudo-supervised task whose labels are generated automatically. A pretraining task can be main or auxiliary. The main pretraining tasks allow the model to learn language representations while auxiliary pretraining tasks allow the model to gain knowledge from human-curated sources like Ontology [34], [45]- [47]. The classification of pretraining tasks is given in Figure 12 and a brief summary of various pretraining tasks is presented in Table 1.

LLM judge: NO

The analysis fails criteria 1, 3, and 4:

1. The text suggests it was interspersed with non-textual elements (Figure 12 and Table 1) which are directly referenced but not included in the content provided.
3. The text relies on the understanding of figures and tables that are not presented, which are likely to include mathematical equations, chemical formulas, or other non-textual elements.
4. Although the content does not directly include unclear symbols indicative of errors from PDF extraction, the references to "[34], [45]- [47]" without accompanying text or context might confuse readers outside the context of the full paper.

Additionally, the content does not stand alone as a self-contained explanation of pretraining tasks without the inclusion of Figure 12 and Table 1, suggesting a lack of completeness and comprehensiveness.

## (s17) Main Pretraining Tasks
Number of References: 7

(p17.0) The main pretraining tasks allow the model to learn language representations. Some of the commonly used main pretraining tasks are masked language modelling (MLM) [2], replaced token detection (RTD) [50], sentence boundary objective (SBO) [49], next sentence prediction (NSP) [2] and sentence order prediction (SOP) [15].

(p17.1) Masked Language Modeling (MLM). It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2]. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens. However, the meaning of a word depends on both the left and right contexts. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI. Less challenging as it involves topic prediction which is a relatively easy task.

LLM judge: NO

The analysis:

1. The content is interrupted and suggests it was interspersed with non-textual elements or references in its original form. The abrupt ending of the sentence and reference to NSP without a complete explanation disrupt the flow and coherence.

2. It heavily relies on references for its claims, which violates the criterion that the text should offer comprehensive context beyond mere lists of references or topics.

3. The inclusion of acronyms and shorthand (e.g., MLM, NSP, NLI) without definitions (except for the initial expansion of MLM) makes the text less self-contained and could confuse readers unfamiliar with the subject.

4. The text seems to be missing parts, as indicated by the abrupt ending, undermining its ability to be self-contained and offer clarity without access to the entire paper.

5. The logical connection between paragraphs is weak due to the abrupt ending, and there's no comprehensive conclusion or summary tying together the discussed pretraining tasks' implications, benefits, or comparisons.

## (s19) Auxiliary Pretraining Tasks
Number of References: 4

(p19.0) Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS [54] into indomain models to further enhance them. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45]. This auxiliary task is used by Hao et al. [45] to inject UMLS relation knowledge into in-domain models. Yuan et al. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.

LLM judge: NO

The analysis shows the content:
1. Is largely cohesive but heavily integrates references within its narrative, potentially suggesting it was interspersed with non-textual elements (criteria 1 not fully met).
2. Provides a general context of how auxiliary pretraining tasks are used but relies on specific examples and references for a comprehensive understanding (criteria 2 marginally met).
3. It does not heavily rely on mathematical equations, chemical formulas, or similar non-textual elements, thus meeting criteria 3.
4. Is free from unclear symbols or significant errors from PDF extraction, meeting criteria 4.
5. While somewhat self-contained, the content references multiple studies to explain the use of auxiliary pretraining tasks, which might not provide clear understanding without access to further details from those studies, partially meeting criteria 5.
6. The excerpt is a single-paragraph piece and does not demonstrate multiple paragraph coherence, which makes criteria 6 not applicable.

Additional concerns:
The text relies significantly on external references to substantiate its claims, which could hinder a reader's understanding without access to those sources. This suggests the excerpt might not be entirely self-contained or offer a comprehensive explanation on its own.

## (s25) Auxiliary Embeddings
Number of References: 7

(p25.0) Main embeddings represent the given input sequence in low dimensional space. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].

(p25.1) Segment Embeddings -Segment embeddings help to Age Embeddings -In models like BEHRT [27] and BERT-EHR [32], age embeddings are used in addition to other embeddings. Age embeddings provide the age of the patient and help the model to leverage temporal information. Age embedding is the same for all the codes in a single patient visit.

(p25.2) Gender Embeddings -In models like BEHRT [27] and BERT-EHR [32], gender embeddings are used in addition to other embeddings. Gender embeddings provide the gender information of the patient to the model. Gender embedding is the same for all the codes in all the patient visits.

LLM judge: YES

What are the types and purposes of auxiliary embeddings in enhancing input representation for models, particularly in understanding token positions, patient age, and gender?

## (s31) Scientific Literature
Number of References: 7

(p31.0) In the last few decades, the amount of biomedical literature is growing at a rapid scale. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16]. However, biomedical text significantly differs from the general text with a lot of domain-specific words. As a result, the performance of general T-PLMs is limited in many of the tasks. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text. PubMed and PMC are the two popular sources of biomedical text. PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].

(p31.1) As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text. BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature. BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts). BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus. BioMedBERT outperformed BioBERT on biomedical question answering.

LLM judge: YES

What is the significance of developing in-domain Transformer-based Pre-trained Language Models (T-PLMs) for biomedical text, and how do PubMed and PMC contribute to this process?

## (s35) Ontology Enriched
Number of References: 10

(p35.0) T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks. These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text. Recent works [34], [45]- [47] showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be   [34], [45]- [47] . Clinical Kb-BERT and Clinical Kb-ALBERT [45] are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets. Here, pretraining involves three loss functions namely MLM, NSP, and triple classification. Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model. Umls-BERT [46] is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multilabel loss-based MLM and NSP. The novel multi-label loss function allows the model to connect all the words under the same CUI. CoderBERT [47] is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss. Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge. SapBERT [120] is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multisimilarity loss.  

LLM judge: NO

The analysis reveals that while the content provided does present a coherent overview of advancements in biomedical natural language processing (NLP) models, it does not fully meet the criteria necessary for inclusion in a scientific question-answering dataset. Criteria specifically not met include:

1. Cohesive, uninterrupted text: The content, although relatively cohesive, seems to integrate references in a manner that suggests it might have been extracted from a list or an overview section, possibly disrupting the flow for readers unfamiliar with the literature.
2. Comprehensive context: While the content does offer some context, the heavy reliance on acronyms and specific model names without explaining their significance (e.g., MLM, NSP, CUI) may leave readers without a specialized background puzzled, failing to provide clarity and understanding independently from the full paper.
3. Self-contained clarity: The mention of various models and their specific training enhancements demands a level of background knowledge or immediate access to external information, like what UMLS or MIMIC-III clinical notes entail, which the excerpt does not provide.

Furthermore, the content's listing nature of model developments, albeit informative, might challenge the distillation into a single question that encompasses its broad span without necessitating auxiliary explanations. The detailed focus on model specifics without sufficiently unpacking terms or the implications of these advancements restricts the text's standalone comprehensibility.

## (s36) Green Models
Number of References: 6

(p36.0) CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords. This kind of representation increases the overall length of the input as well as hinders the model learning. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text. However, both these approaches involve learning the model parameters from scratch which is highly expensive. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].

(p36.1) Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123]. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction. This approach is completely inexpensive as it requires only CPU. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive. Table 9 contains summary of Green T-BPLMs.

LLM judge: YES

What strategies are being explored by the biomedical research community to adapt general Transformer-based Pretrained Language Models (T-PLMs) for in-domain use in a cost-effective and environmentally friendly way?

## (s38) Multi-Modal Models
Number of References: 12

(p38.0) T-PLMs have achieved success in many of the NLP tasks including in specific domains like Biomedical. Recent research works have focused on developing pretrained models that can handle multi-modal data i.e., video + text [128], [129], image + text [130]- [134] etc. In Biomedical domain, models like BERTHop [134] and Medical-VLBERT [133] have been proposed recently to handle image + text data. BERTHop [134] is a multi-modal T-BPLM developed for Chest X-ray disease diagnosis. Like ViLBERT [131] and LXMERT [132], BERTHop uses separate encoder to encoder image and text inputs. BERTHop uses PixelHop++ [135] to encode image data and BlueBERT as text encoder. Medical-VLBERT is developed for automatic report generation from COVID-19 scans. Unlike BERTHop, Medical-VLBERT [133] uses shared encoder based on VL-BERT [130] to encode image and text data.  

LLM judge: YES

How do BERTHop and Medical-VLBERT differ in processing multi-modal data for biomedical applications?

## (s49) Low Cost Domain Adaptation
Number of References: 4

(p49.0) The two popular approaches for developing T-BPLMs are MDPT and DSPT. These approaches involve pretraining on large volumes of in-domain text using highend GPUs or TPUs for days. These two approaches are quite successful in developing BPLMs. However, these approaches are quite expensive requiring high computing resources with long pretraining durations [122]. For example, BioBERT -it took around ten days to adapt general BERT to the biomedical domain using eight GPUs [16]. Moreover, DSPT is more expensive compared to continual pretraining as it involves learning model weights from scratch [122], [123]. So, there is a need for lost cost domain adaptation methods to adapt general Approach Description Pros Cons

LLM judge: NO

The content does not meet several of the specified criteria:
1. It is not cohesive and suggests it was interspersed with non-textual elements, such as reference numbers ([122], [16], etc.) within the text.
2. It contains unclear symbols and notations indicative of errors from PDF extraction or referencing style not suitable for seamless reading (e.g., "[122]", "[123]").
3. The text seems to be cut off at the end, indicating it is not self-contained and lacks clarity and understanding without access to the full document.
4. There is an abrupt ending with "to adapt general Approach Description Pros Cons," implying the content is incomplete and lacks the coherence and logical connection between multiple paragraphs if any were intended to follow.

Additionally, the text presupposes knowledge of several acronyms without defining them (e.g., MDPT, DSPT, BPLMs, GPUs, TPUs), which further detracts from its suitability for a standalone question-answering format as per these guidelines.

## (s56) Quality In-Domain Word Representations
Number of References: 3

(p56.0) Continual pretraining allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of in-domain text. Though the models are adapted to in-domain, they still contain general vocabulary. As the vocabulary is learned over general text, it mostly includes subwords and words which are specific to the general domain. As a result, many of the in-domain words are not represented in a meaningful way. The two possible options to represent in-domain words in a meaningful way are a) in-domain vocabulary through DSPT [20] b) extending the general vocabulary with indomain vocabulary [122], [123].

LLM judge: NO

The content fails to meet several criteria:
1. It does not offer comprehensive context beyond a mere listing of strategies or references ([20], [122], [123]).
2. It fails to be self-contained, as it references specific strategies (DSPT, extending the general vocabulary with in-domain vocabulary) without explaining what they are or how they work.
3. The logical connection and coherence between the introduction of the problem and the presentation of solutions are weak without further elaboration on how these strategies effectively address the mentioned issue.
4. The presence of references within the text ([20], [122], [123]) suggests it is interspersed with non-textual elements in its original form, potentially indicating it's an excerpt rather than a cohesive, uninterrupted text. 

Additional concerns include the lack of explanation about what "T-PLMs" are and what specific in-domain challenges they face, making it challenging for someone without prior knowledge of the subject to understand the content fully.

## (s58) Quality Sequence Representation
Number of References: 8

(p58.0) For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space. Finally, a softmax is applied to convert it into a vector of probabilities. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.

LLM judge: YES

What technique did Devlin et al. recommend for representing the input sequence in text classification tasks, and what are some advanced alternatives?

## (s62) Domain Adaptation
Number of References: 5

(p62.0) In the beginning, the standard approach to develop BPLMs is to start with general PLMs and then further pretrain them on large volumes of biomedical text [16]. The main drawback of this approach is the lack of indomain vocabulary. Without domain-specific vocabulary, many of the in-domain are split into a number of subwords which hinders model learning during pretraining or fine-tuning. Moreover, continual pretraining is quite expensive as it involves pretraining on large volumes of unlabeled text. To overcome these drawbacks, there are low-cost domain adaptation approaches that extend the general domain vocabulary with in-domain vocabulary [122], [123]. The extra in-domain vocabulary is generated using Word2vec and then aligned [122] or generated directly using WordPiece [123] over biomedical text. The main drawback in these low-cost domain adaptation approaches is an increase in the size of the model with the addition of in-domain vocabulary. Further research on this topic can result in more novel methods for lowcost domain adaptation.

LLM judge: YES

What are the main drawbacks of both standard and low-cost domain adaptation approaches in developing Biomedical Pretrained Language Models (BPLMs), and how can they be addressed?

## (s64) Benchmarks
Number of References: 8

(p64.0) In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks. A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks. A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models. In general domain, we have a number of benchmarks like GLUE [207] and Super-GLUE [208] (general language understanding), XGLUE [209] (cross lingual language understanding) and LinCE [210] (code switching). In biomedical domain there are three benchmarks namely BLUE [18], BLURB [20] and ChineseBLUE [48]. BLUE introduced by Peng et al. [18] contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets. BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets. The semantics of EHR and medical social media texts are different from biomedical scientific literature. So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets.

LLM judge: YES

Why are benchmarks like GLUE, Super-GLUE, and BLUE essential for evaluating the performance of pre-trained language models across different NLP tasks?

## (s65) Intrinsic Probes
Number of References: 7

(p65.0) During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus [14]. Intrinsic probes through light on the knowledge learned by PLMs during pretraining. In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA [211], XLAMA [212], X-FACTR [213], MickeyProbe [214] to understand the knowledge encoded in pretrained models. For example, LAMA [211] probes the factual and common-sense knowledge of English pretrained models, while X-FACTR [213] probes the factual knowledge of multi-lingual pretrained models. However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining. This is an area which requires much attention from Biomedical NLP community.

LLM judge: NO

The content does not meet the criterion 3, as it relies heavily on references to illustrate points, which may not provide comprehensive context without access to these references. Additionally, criterion 5 is not fully met because, without specific examples or clearer explanations of the probes mentioned, the content lacks the self-contained clarity required for understanding without the full paper. Furthermore, the statement about the lack of intrinsic probes in the Biomedical domain hints at more information that might be provided in the rest of the paper to fully understand the significance and implications of this gap, thereby not fully making the text self-contained.

## (s66) Efficient Models
Number of References: 2

(p66.0) Pretraining provides BPLMs with necessary background knowledge which can be transferred to downstream tasks. However, pretraining is computationally very expensive and also requires large volumes of pretraining data. So, there is need of novel model architecture which reduces the pretraining time as well as the amount of pretraining corpus. In general NLP, recently efficient models like ConvBERT [215] and DeBERTa [216] are proposed which reduces the pretraining time and amount of pretraining corpus required respectively. DeBERTa with two novel improvements (Disentangled attention mechanism and enhanced masked decoder) achieves better compared to RoBERTa. DeBERTa is pretrained on just 78GB of data while RoBERTa is pretrained on 160GB of data. ConvBERT with mixed attention block outperforms ELECTRA while using just 1/4 th of its pretraining cost. Biomedical NLP research community must focus on developing pretrained models based on these novel model architectures.

LLM judge: YES

How do ConvBERT and DeBERTa improve the efficiency of pretraining in NLP, and what specific advancements do they bring to model pretraining?

