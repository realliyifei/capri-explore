# Generate FAIR Literature Surveys with Scholarly Knowledge Graphs

CorpusID: 219179714
 
tags: #Computer_Science

URL: [https://www.semanticscholar.org/paper/3e4de425e4a8db948b94cdc2fe14aed8d0cac6e9](https://www.semanticscholar.org/paper/3e4de425e4a8db948b94cdc2fe14aed8d0cac6e9)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Generate FAIR Literature Surveys with Scholarly Knowledge Graphs


Allard Oelen oelen@l3s.de 
L3S Research Center
Leibniz University Hannover
Germany

Mohamad Yaser Jaradeh jaradeh@l3s.de 
L3S Research Center
Leibniz University Hannover
Germany

Markus Stocker markus.stocker@tib.eu 
TIB Leibniz Information Centre for Science and Technology
Germany

Sören Auer auer@tib.eu 
TIB Leibniz Information Centre for Science and Technology & L3S Research Center
Germany

Generate FAIR Literature Surveys with Scholarly Knowledge Graphs
Scholarly Knowledge ComparisonScholarly Information SystemsComparison User InterfaceDigital LibrariesScholarly Communi- cationFAIR Data Principles
Reviewing scienti c literature is a cumbersome, time consuming but crucial activity in research. Leveraging a scholarly knowledge graph, we present a methodology and a system for comparing scholarly literature, in particular research contributions describing the addressed problem, utilized materials, employed methods and yielded results. The system can be used by researchers to quickly get familiar with existing work in a speci c research domain (e.g., a concrete research question or hypothesis). Additionally, it can be used to publish literature surveys following the FAIR Data Principles. The methodology to create a research contribution comparison consists of multiple tasks, speci cally: (a) nding similar contributions, (b) aligning contribution descriptions, (c) visualizing and nally (d) publishing the comparison. The methodology is implemented within the Open Research Knowledge Graph (ORKG), a scholarly infrastructure that enables researchers to collaboratively describe, nd and compare research contributions. We evaluate the implementation using data extracted from published review articles. The evaluation also addresses the FAIRness of comparisons published with the ORKG.

# INTRODUCTION

When conducting scienti c research, reviewing the existing literature is an essential activity [33]. Familiarity with the state-ofthe-art is required to e ectively contribute to advancing it and do relevant research. Mainly because published scholarly knowledge is unstructured [17], it is currently very tedious to review existing literature. Relevant literature has to be found among hundreds and increasingly thousands of PDF articles. This activity is supported by library catalogs and online search engines, such as Scopus or Google Scholar [18]. Because the search is keyword based, typically large numbers of articles are returned by search engines. Researchers have to manually identify the relevant papers. Having identi ed the relevant papers, the relevant pieces of information need to be extracted in order to obtain an overview of the literature. Overall, these are manual and time consuming steps. We argue that a key issue is that the scholarly knowledge communicated in the literature does not meet the FAIR Data Principles [40]. While PDF articles can be found and accessed (assuming Open Access or an institutional subscription), the scholarly literature is insu ciently interoperable and reusable, especially for machines. For units more granular than the PDF article, such as a speci c result, ndability and accessibility score low even for humans.

We present a methodology and its implementation integrated into the Open Research Knowledge Graph (ORKG) [15] that can be used to generate and publish literature surveys in form of machine actionable, comparable descriptions of research contributions. Machine actionability of research contributions relates to the ability of machines to access and interpret the contribution data. The bene ts for researchers of such an infrastructure are (at least) twofold. Firstly, it supports researchers in creating state-of-the-art overviews for speci c research problems e ciently. Secondly, it supports researchers in publishing literature surveys that adhere to the FAIR principles, thus contributing substantially to reuse of state-of-the-art overviews and therein contained information, for both humans and machines.

Literature reviews are articles that focus on analysing existing literature. Among other things, reviews can be used to gain understanding about a research problem or to identify further research directions [8,29]. Reviews can be used by authors to quickly obtain an overview of either emerging or mature research topics [36]. Review papers are important for research elds to develop. When review papers are lacking, the development of a research eld is weakened [38]. Compiling literature review papers is a complicated task [39] and is often more time consuming than performing original research [38]. The structure of such articles often consists of tables that compare published research contributions. Although in the literature the terms "literature review" and "literature survey" are sometimes used interchangeably, we make the following distinction. We refer to the tables in review articles as literature surveys. Together with a (textual) analysis and explanation, they form the literature review. The state-of-the-art (SoTA) analysis is a special kind of literature review with the objective of comparing the latest and most relevant papers in a speci c domain.

We implement the presented methodology in the ORKG. The ORKG is a scholarly infrastructure designed to acquire, publish and process structured scholarly knowledge published in the literature [14]. ORKG is part of a larger research agenda aiming at machine actionable scholarly knowledge that understands the ability to more e ciently compare literature as a key feature.

We tackle the following research questions:

(1) How to generate literature surveys using scholarly knowledge graphs? (2) How to ensure that published literature surveys comply with the FAIR principles? (3) How to e ectively specify and visualize literature surveys in a user interface?

In support of the rst research question, we present a methodology that describes the steps required to generate literature surveys. In support of the second research question, we describe how the FAIRness of the published literature review is ensured. Finally, in support of the third research question, we demonstrate how the methodology is implemented within the ORKG.

The paper is structured as follows. Section 2 motivates the work. Section 3 reviews related work. Section 4 presents the system design, the underlying methodology and its implementation. Section 5 explains how the knowledge graph is populated with data. Section 6 presents the evaluation of the system, speci cally system FAIRness and performance. Finally, Section 7 discusses the presented and future work.


# USE CASES

We motivate our work by means of two use cases that underscore the usefulness of a literature survey generation system. In the rst use case, a researcher wants to obtain an overview on state-of-theart research addressing a speci c problem. The second use case describes how a researcher can publish a FAIR compliant literature review with the ORKG.

Familiarize with the state-of-the-art. A state-of-the-art (SoTA) analysis reviews new and emerging research. They are useful for multiple reasons. Firstly, they provide a broad overview of a research problem and support understanding. Secondly, they juxtapose di erent approaches for a problem. Thirdly, they can support claims on why certain research is relevant by giving an overview of the breadth of research addressing a problem. The proposed approach enables automated generation of surveys to quickly obtain an overview of state-of-the-art research as well as sharing of surveys for others to reuse.

Publishing of literature reviews. Literature reviews typically consist of multiple (survey) tables in which di erent approaches from original papers are compared based on a set of properties. These tables can be seen as the main contribution and most informative part of the review paper, since the tables juxtapose and compare existing work. Comparison tables are published in review papers as static content in PDF documents. This presentational format is generated from datasets that typically contain more (structured) information than what is presented in the published table. However, the additional information is not published. It is "dark data" which is not stored or indexed and likely lost over time [12]. Furthermore, published tables are not machine actionable. Their overall low FAIRness hinders reusability of the published content. With the presented service, it is possible to publish a literature survey with high FAIRness, i.e. that is compliant with the FAIR principles to a high degree. Section 3 discusses this aspect in more details.

Summary of weaknesses of the current approach to literature review.

The weaknesses of the current approach to literature review can be summarized as follows:

• Static -reviews are static, since once published as PDF they are rarely updated and there are no possibilities or incentives for creating new or updated reviews for considerable time. • Lack of machine assistance -machine assistance is hardly possible, since the PDF representation of reviews is only human readable and relevant raw data is mostly not published along with the review. • Delay -reviews are produced and published with signicant delay (often years) after original research work was done. • Coverage -due to the amount of work required, reviews are often only performed for relatively popular research topics and are stale or missing for less popular topics. • Lacking collaboration -collaboration on reviews is not possible and reviews currently represent only the viewpoint of the few authors not the community. • Missing overarching systematic semantic representationthe overlap between di erent reviews and related work sections in individual original research papers is not explicit and cannot be exploited. We deem that these weaknesses of the current approach to scholarly literature review and synthesis signi cantly hinder scienti c progress.


# RELATED WORK

The task of comparing research contributions can be reviewed in light of the more general task of comparing resources (or entities) in a knowledge graph. While this is a well-known task in multiple domains (for instance in e-commerce systems [42]), not much work has focused on comparison in knowledge graphs, speci cally. One of the few works with this focus is by Petrova et al. [28] who created a framework for comparing entities in RDF graphs using SPARQL queries. In order to compare contributions, they rst have to be found. Finding is an information retrieval problem. As a well-known technique, TF-IDF [21] can be used for this task. More sophisticated techniques can be used to determine the structural similarity between graphs (e.g., [20]) and matching semantically similar predicates. This relates to dataset interlinking [1] or more generally ontology alignment [34]. For property alignment, techniques of interest include edit distance (e.g., Jaro-Winkler [41] or Levenshtein [19]) and vector distance. Gromann and Declerck [10] found that fastText [4] performs best for ontology alignment.

In light of the FAIR Data Principles [40], scholarly data should be Findable, Accessible, Interoperable and Reusable both for humans and machines. Due to the publication format, literature survey tables published in scholarly articles weakly adhere to the FAIR guidelines, particularly so for machines. Scholarly data should be considered rst-class objects [35], including data used to create literature surveys. Rodríguez-Iglesias et al. [30] describe the diculties of making data FAIR within the plant sciences. They argue that it is more complicated than reformatting data. On the other hand they suggest that most FAIR principles can be implemented relatively easily by using o -the-shelf technologies. Boeckhout et al. [3] argue that the FAIR principles alone are not su cient to lead to responsible data sharing. More applied principles are needed to ensure better scholarly data. This claim is supported by the ndings of Mons et al. [22] who suggest that there are very diverse interpretations of the guidelines. In their work, they try to clarify what is FAIR and what is not.

An e cient literature comparison relies on scholarly knowledge being represented in a structured way. There is substantial related work on representing scholarly knowledge in structured form [31]. Building on the work of numerous philosophers of science, Hars [11] proposed a comprehensive scienti c knowledge model that includes concepts such as theory, methodology and statement. More recently, ontologies were engineered to describe di erent aspects of the scholarly communication process. Semantic Publishing and Referencing (SPAR) 1 is a collection of ontologies that can be used to describe scholarly publishing and referencing of documents [5,9,26,27]. Ruiz Iniesta and Corcho [31] reviewed the state-of-the-art ontologies to describe scholarly articles. Sateli and Witte [32] use some of these scholarly ontologies to add semantic representations of scholarly articles to the Linked Open Data cloud. A literature survey comparing scholarly ontologies is available via the ORKG. 2 Most of these ontologies are designed to capture metadata about and structure of scholarly articles, not the content communicated in articles. Another literature survey is created to compare approaches for semantically representing scholarly communication. 3 An initial attempt for semantifying review articles was done in [7]. The work comprises a relatively rigid ontology for describing contributions (mainly centered around research problems, approaches, implementations and evaluations) and a prototypical implementation using Semantic MediaWiki. We relax this constraint, since we are not limited by a rigid ontology schema but rather allow arbitrary domain-speci c semantic structures for research contributions. The work by Vahdati et al. [37] focuses on semantic article representations for generating literature overviews. Their method is to use crowdsourcing to generate the overviews. Kohl et al. [16] present CADIMA, a system that supports systematic literature reviews. The tool supports the formal process of performing a literature review but does, for example, not publish data in machine actionable form for reuse.


# SYSTEM DESIGN

We now present the system design of the literature comparison service. It consists of a methodology that describes how to perform a comparison of research contributions. An early version of this methodology has been presented at the 3rd SciKnow workshop [24]. The methodology consists of ve steps: 1) nding comparison candidates, 2) selecting related statements, 3) aligning contribution descriptions, 4) visualizing comparisons and 5) publishing FAIR comparisons. The methodology is depicted in Figure 1  discuss the data structure of the ORKG, which forms the foundation of the comparison. Then, each step of the methodology is described in more detail. Finally, we discuss the implementation.


## ORKG ontology

In ORKG, each paper is typed as paper class. A paper consists of at least one research contribution, which addresses at least one research problem. Research contributions consist of contribution data that describe the contribution. For instance, a paper in Computer Science might have descriptions for materials, methods, implementation and results as contribution data. These prede ned core concepts can be easily extended with domain speci c research problems, methods, etc. in ORKG curation using crowdsourcing or other curation approaches. The underlying data structure uses the notion of statements. Statements are triples that consist of a subject, a predicate (also called a property) and an object. The granularity of a comparison is at the research contribution, meaning that contributions are compared rather than papers. For simplicity, we use the terms "paper comparison" and "contribution comparison" interchangeably. Because a comparison happens on contribution level, it is possible to compare speci c elements of a paper instead of the complete paper. The bene t of this is that a comparison does not contain data from irrelevant contributions. The ORKG OWL ontology is available online. 4 


## Select comparison candidates

To perform a comparison, a starting contribution is needed. This contribution is called main contribution and is always manually selected by a user. The main contribution is compared against other comparison contributions. There are two di erent approaches for selecting the comparison contributions. The rst approach automatically selects comparison contributions based on similarity. The second approach lets users manually select contributions.


### Find similar contributions.

Comparing contributions makes only sense when contributions can sensibly be compared. For example, it does not make (much) sense to compare a biology paper to a history paper. We thus argue that it makes only sense to compare contributions that are similar. More speci cally, contributions that share the same (or a similar set of) properties are good comparison candidates. For instance, a paper about question answering has the property orkg:disambiguationTask 5 and another paper is 4 https://gitlab.com/TIBHannover/orkg/orkg-ontology 5 orkg: denotes the ontology of the ORKG system described in Section 4.1  using the same property to describe what disambiguation tasks are performed. Since they share the same property it makes them likely candidates for comparison. Finding similar contributions is therefore based on nding contributions that share the same or similar informative description properties. To achieve this, each comparison contribution is converted into a string by concatenating all properties of the contribution. TF-IDF [21] is used to query these strings with the string of the main contribution as query. The search returns the most similar contributions by weighting the most informative properties higher due to TF-IDF. The top-k contributions are selected and form a set of contributions that are used in the next step. Figure 2 displays how the similar contribution selection is implemented. As depicted, three similar contributions are suggested to the user (with the corresponding similarity percentage being displayed next to paper title). These suggested contributions can be directly compared.


### Manual selection.

There are scenarios where comparison based on similarity computation is not suitable or desired. For example, a researcher wants to compare a speci c set of implementations to see which performs best. Therefore, the manual selection method is implemented in a similar fashion to an e-commerce shopping cart. When the "Add to comparison" checkbox is checked, a box appears listing the selected contributions ( Figure 3).


## Select related statements

This step selects the statements from the graph related to the set of contributions selected in the previous step. Statements are selected transitively to match contributions in subject or object position. This search is performed until a prede ned maximum transitive depth δ has been reached. The intuition is that the deeper a property is nested the less likely is its relevance for the comparison. The process of selecting statements is repeated until depth δ = 5 is reached. This number is chosen empirically to include statements that are not directly related to the contribution, but to exclude statements that are less relevant because they are nested too deep.


## Align contribution descriptions

As described in the rst step, comparisons are built using shared or similar properties of contributions. In case the same property has been used between contributions, these properties are grouped and form one comparison row. However, often di erent properties are used to describe the same concept. This occurs for various reasons. The most obvious reason is when two di erent ontologies are used to describe the same property. For example, for describing the population of a city, DBpedia uses dbo:populationTotal while WikiData uses WikiData:population (actually the property identi er is P1082; for the purpose here we use the label). When comparing contributions, these properties should be considered as equivalent. Especially for community-created knowledge graphs, di erently identi ed properties likely exist that are, in fact, equivalent.

To overcome this problem, we use pre-trained fastText [4] word embeddings to determine the similarity of properties. If the similarity is higher than a predetermined threshold τ , the properties are considered equivalent and are grouped. This happens when the similarity threshold τ ≥ 0.9 (also empirically determined). In the end, each group of properties will be visualized as one row in the comparison table. The result of this step is a list of statements for each contribution, where similar properties are grouped. Based on this similarity matrix γ is generated
γ p i = cos( − → p i , − → p j )(1)
with cos(.) as the cosine similarity of vector embeddings for property pairs (p i , p j ) ∈ P, whereby P is the set of all contributions.

Furthermore, we create a mask matrix Φ that selects properties of contributions c i ∈ C, whereby C is the set of contributions to be compared. Formally,
Φ i, j = 1 if p j ∈ c i 0 otherwise(2)
Next, for each selected property p we create the matrix φ that slices Φ to include only similar properties. Formally,
φ i, j = (Φ i, j ) c i ∈ C p j ∈sim(p)(3)
where sim(p) is the set of properties with similarity values γ [p] ≥ τ with property p. Finally, φ is used to e ciently compute the common set of properties [14]. This process is displayed in Algorithm 1. for each property p 1 ∈ properties do 3:

for each property p 2 ∈ properties do 4: similarit ← cos(Embb(p 1 ), Embb(p 2 )) 5: if similarit > threshold then 6: similarProps ← similarProps ∪ {p 1 , p 2 } return similarProps


## Visualize comparison

The next step of the work ow is to visualize the comparison and present the data in a human understandable format. Tabular format is often appropriate for visualizing comparisons since tables provide a good overview of data. Another aspect of the visualization is determining which properties should be displayed and which ones should be hidden. A property is displayed when it is shared among a predetermined amount α of contributions, where α mainly depends on comparison use and can be determined based on the total amount of contributions in the comparison. By default, only properties that are common to at least two contributions (α ≥ 2) are displayed.

Another aspect of comparison visualization is the possibility to customize the resulting table. This is needed because of the similarity-based matching of properties and the use of predetermined thresholds. For example, users should be able to enable or disable properties. They should also get feedback on property provenance (i.e., the property's path in the graph). Ultimately, this contributes to a better user experience, with the possibility to manually correct mistakes made by the system. Customization. Users can customize comparisons including transposing the table as well as hiding and rearranging the properties. Especially the option to hide properties is helpful when contributions with many statements are compared. Only properties considered relevant to the user can be selected to display. Customizing the comparison table can be useful before exporting or sharing the comparison.

Sharing and persistence. Comparisons can be shared using a persistent link. Especially when sharing the comparison for research purposes, it is important to refer to the original comparison. Since contribution descriptions may change over time comparisons may also change. To support persistency, the whole state of the comparison is stored in a document-oriented database and retrieved when the permalink is invoked.

Export. It is possible to export comparisons in di erent output formats such as PDF, CSV, RDF and L A T E X. The L A T E X export is useful for direct integration in research papers. Together with the L A T E X table, a BibTeX le containing the bibliographic information of the papers used in the comparison is also generated. Also, a 


## Publish comparison

Visualized and customized comparison tables can be stored. Storing tables is part of the publishing process and therefore only needed when a generated table is going to be used in a paper. In order to regenerate the table the whole state of the comparison should be saved. The knowledge graph from which the comparison was generated changes over time and thus storing just the URIs of the respective papers would not su ce. While saving a comparison, the user can provide additional metadata to ensure ndability, an aspect of the FAIR principles. Metadata include a comparison title, which would normally consist of a one sentence description of the comparison. Additionally, a longer textual description can be provided. This metadata is extended with machine generated data, such as the creation date and the creator of the comparison. The metadata is stored in the knowledge graph to support easy access and interoperability. In Figure 5, the structure of the metadata is displayed using the Dublin Core Metadata Terms 6 . The comparison data itself is stored in a document-oriented database. An RDF export of both the metadata and the comparison data can be generated. The comparison data is modeled with the RDF Data Cube Vocabulary 7 . A unique identi er is attached when the comparison is saved. This ID is used when the comparison is shared or when it is referenced in a paper. The literature comparison can also be performed without publishing. Although the work ow and the steps to create a comparison stay the same, the goal is di erent. Instead of creating a comparison that will be published and referenced in a paper, the comparison will be used by the researcher herself.


## Technical details

The user interface of the comparison feature is seamlessly integrated with the ORKG front end, which is written in JavaScript and is publicly available 8 . The back end of the comparison feature is a service separate from the ORKG back end written in Python and also available Open Source 9 . The comparison back end is responsible for step two and three of the comparison methodology. The 6 https://dublincore.org/speci cations/dublin-core/dcmi-terms 7 https://www.w3.org/TR/vocab-data-cube 8 https://gitlab.com/TIBHannover/orkg/orkg-frontend 9 https://gitlab.com/TIBHannover/orkg/orkg-similarity  


# DATA COLLECTION

In order to generate useful literature reviews it is crucial for the knowledge graph to contain su cient and relevant papers. Populating the knowledge graph with high quality paper descriptions it not straightforward. Structured descriptions of papers should be created in such a way that it is possible to compare papers based on shared properties. Both published papers and papers that will be published in the future should be added to the ORKG, retrospectively or prospectively. Although a comprehensive description on how to populate the ORKG is out-of-scope here, we now brie y describe how we envision populating the ORKG in a manner that would facilitate comparing contributions.

Prospectively, authors can become part of generating structured descriptions of their papers. This should be done in a crowdsourced manner and can become part of the paper submission process. Input templates that collect relevant properties can be used to ensure structured and comparable paper descriptions. Retrospectively, automated (machine learning) methods can be helpful ensure scalability of the process of adding a paper.


## Leverage legacy review paper tables

To populate the ORKG with comparable paper descriptions, we leverage the data published in review papers. Review papers consist of high quality, curated and often structured data that is collected from a set of papers that address the same (or a similar) research problem. Hence, using reviews to populate a scholarly knowledge graph is a relatively straightforward approach to obtain high quality structured paper descriptions. We now present a methodology to convert survey paper data into a knowledge graph structure. The steps are as follows:

(1) Survey paper selection. The rst step is the selection of survey papers that are suitable for building a knowledge graph. Firstly, the survey should compare peer-reviewed scienti c articles. For instance, a comparison of di erent systems without a reference to peer-reviewed work is not suitable for the scholarly knowledge graph. Secondly, the review should compare the papers' content in a structured way and should not merely list work in a eld. Especially reviews that present their results and literature comparisons in tabular format are suitable. The result of this step is a list of papers that will be added to the ORKG. (2)  Otherwise, at least the title, authors and publication date have to be collected. (5) Data ingestion. Finally, the paper data is ingestion into the knowledge graph. The paper data consists of both the paper's metadata and the extracted data from the comparison table. This does not result in a single description of the survey paper. Each paper referenced in the survey table is ingested individually. In order to speed up the process of adding papers, we developed a Python package 12 that has a function to add a paper to the knowledge graph. This methodology has been used to populate the ORKG with comparable paper data. The data is used to evaluate the presented literature review tool. The imported paper data is not only useful for the evaluation, but does also provide signi cant value to the ORKG itself.

In total, four review papers were selected for importing into the ORKG. The Python script for importing the table data is available online. 13 From those papers, 12 di erent tables were imported. Together, 169 papers were reviewed in those four survey papers. This resulted in a total amount of 3 750 statements being added to the knowledge graph. Table 1 lists the imported review papers and tables. The survey papers address di erent research problems. Figure 6 depicts an excerpt of the resulting graph for one particular paper. A set of comparison tables made with the imported data is available online. 14 This list includes some alternative comparison tables that were generated with the same data.


# EVALUATION

In this section, we present an evaluation of multiple aspects of the presented comparison methodology and implementation. Firstly, we evaluate information representation. Then, we evaluate the  Table 4 Author name disambiguation 5 https://orkg.org/orkg/c/vDxKdr No Hussain and Asghar [13] Table 5 Author name disambiguation 6 https://orkg.org/orkg/c/XXg8Wg No Hussain and Asghar [13] Table 6 Author name disambiguation 9 https://orkg.org/orkg/c/9rOwPV No Hussain and Asghar [13] Table 7 Author name disambiguation 6 https://orkg.org/orkg/c/mB7kIK No Naidu et al. [23] Figure 6: Partial graph structure of an imported paper. Orange colored resources indicate potentially interesting values for a paper comparison.

FAIRness of published reviews. Finally, we present a performance evaluation that tests the scalability.


## Information representation

This part of the evaluation focuses on the aspect of information representation. We use the data from the imported review papers, as described in Section 5. In order to build and publish useful and correct literature reviews, at a minimum our service should display the same information that was originally presented in the review tables. This means that there should not be information loss when review tables are published using our service. If there is no information loss, it means our service can be used as an alternative to the current way of publishing review tables. Apart from generating the same table, the added value comes from the ability to aggregate new (tabular) views using the same data as well as the increased FAIRness of the data published via our service. For each of the imported review tables, listed in Table 1, we can  evaluate whether the same table can be generated with our service. For this, we have compared the table from the review paper to the table generated by the ORKG comparison service. A collection of 169 paper with 9 distinct literature views/tables are part of this evaluation. These tables can be viewed online, the links are listed in the "ORKG representation" column. The results of this evaluation are displayed in the same table, in column "Information loss". As the results show, using our service it is possible to recreate the same tabular views as originally published in the review papers.


## FAIR data evaluation

As described before, with the presented service it is possible to publish a generated comparison that adheres with the FAIR principles. Because the service leverages a knowledge graph to generate and save comparisons, complying with the FAIR principles is more obvious for the ORKG comparison service than for tables in published PDF articles. In order to evaluate the FAIRness of a published comparison, we evaluate each of the four FAIR principles in detail. Wilkinson et al. [40] described each principle by assigning sub-principles. 15 We discuss the relevant sub-principles and explain how they are met. We use the term (meta)data to refer to both the actual comparison data (i.e., the data that is used to create the comparison table) and the associated metadata (e.g., the title, description and creator of a comparison). Table 2 presents an overview for the evaluation of the FAIR principles.

Findable. To make data ndable for both humans and machines (i.e., agents), a unique and persistent identi er should be attached to the data (F1). Additionally, metadata should describe the data (F2). In the metadata, the unique identi er of the data should be mentioned (F3). Also a search interface should be available to nd the data (F4). To ensure the ndability of comparisons, users can title and describe them. Furthermore, machine generated metadata is attached to a comparison (e.g., the number of papers and the creation date). A unique identi er is generated and attached to the data and included in the metadata. Finally, the ORKG search interface allows users to search the whole graph and has a dedicated lter to speci cally nd comparisons. Additionally, comparisons can be indexed and found by third-party search engines (such as Google or Bing).

Accessible. Having found data, agents need to know how to access it. This principle is primarily about using accessible standardised communication protocols (A1). Additionally, metadata should be available even when the data is not (A2). The metadata is part of the knowledge graph, which can be accessed via the HTTP protocol. The data can be accessed without authentication. To support A2, the metadata and the actual comparison data are stored separately. Therefore, it is possible to access only metadata when the original data is not available anymore (for example when data is retracted by the author).

Interoperable. To ensure the interoperability of data, it should use a formal language for knowledge representation (I1) and should use vocabularies that are FAIR (I2). Finally, references or links to other (meta)data should be made (I3). As argued before, thanks to highly structured data and the integration of shared vocabularies, interoperability is an inherent feature of knowledge graphs. Data is (partially) described using the ORKG core ontology and other ontologies we use to canonicalize the representation of relevant information content types. Links to other data are present in the knowledge graph. For example, if a comparison uses the "Web" resource to specify the domain of an application, this resource is generic, can be shared among paper descriptions and comparisons, and can be described in more detail, independently of a particular comparison.

Reusable. Finally, data should be reuseable. This can be accomplished by adding relevant (meta)data (R1). Required are an accessible data license (R1.1) and detailed provenance (R1.2) data. Finally, (meta)data should use community standards to describe data (R1.3). It is possible to add additional metadata to a comparison, e.g. metadata about the scope of the comparison, which could be a reference to the paper in which the comparison is being used. The metadata is complemented with the metadata that is already part of the Findability principle, e.g. provenance data about the creator of the comparison. The data license of the graph data is CC BY-SA 16 (Attribution-ShareAlike), which allows reuse of the data. There is currently no community standard to describe the comparison data. However, standard ontologies are used to describe metadata (e.g., Dublin Core). The evaluation of the FAIR principles shows that comparisons published with our service rank high in FAIRness, which can be even further increased with some e ort from users. Users are mainly responsible for adding the correct information to the comparison and reuse vocabularies. Otherwise, ndability, accessibility and to some extent also interoperability are largely handled by the service.


## Performance evaluation

In order to evaluate the performance of the overall comparison, we compared the implemented ORKG approach to a naive approach for comparing multiple resources. The naive approach compares each property against all other properties to perform the property alignment. Table 3 shows the time needed to generate comparisons, for both the naive and the ORKG approach. In total, eight papers are compared with on average ten properties per paper. In the naive approach, the "Align contribution descriptions" step is not scaling well, since each property is compared against all others. If multiple contributions are selected, the number of property similarity checks grows exponentially. Table 3 shows that the ORKG approach outperforms the naive approach. The total number of 16 https://creativecommons.org/licenses/by-sa/2.0  


# DISCUSSION & FUTURE WORK

One of the aims of the contribution comparison functionality is to support literature reviews and make this activity less cumbersome and time consuming for researchers. To live up to this aim, more structured contribution descriptions are needed. Existing scholarly knowledge graph initiatives focus primarily on scholarly metadata, while with ORKG we focus on making the actual research contributions machine readable. Currently, the ORKG does not yet contain su cient contribution descriptions in order for the comparison functionally to be practically useful for researchers. Furthermore, for an evaluation of the e ectiveness of certain components of the methodology (such as nding related papers or aligning similar properties), more contribution data is needed. Publishing surveys does not rely on data quantity and is therefore evaluated more extensively in this work. The performance evaluation results indicate that the comparison feature performs well. This means the technical infrastructure is in place for the literature survey service.

In the evaluation, we focused on the aspects of the system that are necessary for researchers to use the system in practice. The information representation evaluation is a straightforward evaluation to see if existing survey tables can be regenerated with the ORKG. This is a minimal requirement for researchers when using the system, since they should at least be able to recreate tables. This evaluation does not give insight to the usefulness and usability of the system, but still provides an indication that the service can be successfully used to publish literature surveys. One of the reasons for using the service is that also "dark data" in comparisons is published (as discussed in Section 2).

Another interesting aspect of the service is that published literature surveys rank high in FAIRness. Therefore, the second part of the evaluation focuses on how the FAIR principles are met. Merely publishing data as RDF is not su cient to fully meet the FAIR principles. Hence, we conducted a more detailed evaluation that describes how the service complies with each sub-principle. Since FAIR is not a standard, the principles are permissive and not prescriptive [22]. No technical requirements are speci ed. Both the implementation and evaluation of the guidelines are therefore subject to interpretation. With respect to data interoperability and reusability, certain aspects of the service can be improved. For example, to improve interoperability, the contribution data should be reusing existing vocabularies where possible. Additionally, although most of FAIRi cation is done by the system, the researcher is responsible for adding correct and relevant metadata while publishing a survey.

As indicated earlier, the usefulness of the presented tool depends on the number of papers present in the knowledge graph. Therefore, future work will focus on data collection, both in a crowdsourced and automated manner. We plan on extending the methodology presented in Section 5 with automated extraction of data and tables from literature review papers. With the extracted review data, the knowledge graph can be extended more quickly than the previously presented manual method. It could form the basis of a high quality scholarly knowledge graph that contains relevant and FAIR survey table data. Furthermore, in the future we will assign (DataCite) DOIs to published surveys. They will serve as a persistent identi er for the survey data [25].


# CONCLUSION

Reviewing existing literature is an important but cumbersome and time consuming activity. To address this problem, we presented a methodology and service that can be used to generate literature surveys from a scholarly knowledge graph. This service can be used by researchers in order to get familiar with existing literature. Additionally, the tool can be used to publish literature surveys in a way that they largely adhere to the FAIR data principles. The presented methodology addresses multiple aspects, including nding suitable contributions, aligning contribution descriptions, visualization and publishing. The methodology is implemented within the Open Research Knowledge Graph (ORKG). Since the comparison relies on structured scholarly knowledge, we discussed how to populate the ORKG with relevant data. This is done by extracting tabular survey data from existing literature reviews. In order to evaluate whether the proposed service can be used to publish literature surveys, the original survey table representations were compared with the ones generated by our service. As the results indicate, it is possible to use the service as an addition or potentially even replacement of the current publishing approach, since the same tables can be generated. The evaluation also showed how the published literature surveys largely adhere to the FAIR data principles. This is crucial for data reusability and machine actionability. To conclude, the proposed literature comparison service addresses multiple weaknesses of the current survey publishing approach and can be used by researchers to generate, publish and reuse literature surveys.

## Figure 1 :
1Research contribution comparison methodology.

## Figure 2 :
2Implementation of the rst step of the methodology: the selection of comparison candidates. Showing both the similarity-based and the manual selection approaches.

## Figure 3 :
3Box showing the manually selected contributions.

## Figure 4
4displays a comparison for research contributions related to visualization tools published in the literature. In this example, four properties are displayed. Literals are displayed as plain text while resources are displayed as links. When a resource link is selected, a popup is displayed showing the statements related to this resource. The UI implements some additional features that are particularly useful to compare research contributions.

## Figure 4 :
4Comparison of research contributions related to visualization tools. persistent link referring back to the comparison in ORKG is showed as table footnote.

## Figure 5 :
5The graph structure of the metadata for a published comparison. The dcterms: pre x denotes the Dublin Core Metadata Terms ontology.input in step two is the set of contribution IDs. The API selects the related statements and aligns the properties and returns the data needed to visualize the comparison. This data includes the list of papers, list of all properties and the values per property.


. First, we 1 http://purl.org/spar/{cito,c4o,fabio,biro,pro,pso,pwo,doco,deo} 2 https://www.orkg.org/orkg/comparison/R8342 3 https://www.orkg.org/orkg/comparison/R8364 • Find similar candidates to compare • Manually select contributions to compare • Select all statements from the comparison contributions • Do this until a predefined depth has been reached • Align contributions properties that are the same (i.e., same ID) • Use word embeddings to align properties that are similar • Hide properties that are not shared among contributions • Let users customize the comparison • Publish a FAIR comparison including relevant metadata • Ensure the persistency of the (meta)dataSelect comparison 
candidates 

Select related statements 

Align contribution 
descriptions 

Publish comparison 

Visualize comparison 



## Table selection .
selectionGiven the selected survey papers, tableshave to be selected. Some surveys contain only one table 
while in others multiple tables are presented. In some cases 
a collection of tables can be joined into one larger table. 
(3) Data modeling. Given the selected tables, a suitable graph 
structure has to be determined. The data structure has to 
be modeled. For instance, when implemented systems are 
compared, a suitable structure could be: [has implementation] 
-> System name. The referenced system can be described 
with a list of properties to be compared. Additionally, a 
research problem has to be de ned, which is typically the 
same for all papers that are part of the table. 
(4) Metadata collection. Next, the metadata for the papers 
that are referenced in the survey table is collected. In case 
a referenced paper has a DOI 10 , the metadata can be auto-
matically retrieved via a lookup service (e.g. Crossref 11 ). 


## Table 1 :
1List of imported survey tables in the ORKG.The paper and table reference can be used to identify the original table.Paper reference 
Table reference Research problem 
Papers ORKG representation 
Information loss 

Bikakis and Sellis [2] 
Table 1 
Generic visualizations 
11 https://orkg.org/orkg/c/pdLJDk 
No 
Bikakis and Sellis [2] 
Table 2 
Graph visualizations 
21 https://orkg.org/orkg/c/Rx476Z 
No 
Diefenbach et al. [6] 
Table 2 
Question answering evaluations 
33 https://orkg.org/orkg/c/gaVisD 
No 
Diefenbach et al. [6] 
Table 3,4,5,6 
Question answering systems 
26 https://orkg.org/orkg/c/IuEWl2 
No 
Hussain and Asghar [13] 

## Table 2 :
2Overview of FAIR principles compliance. For comparisons, the compared paper metadata is linked. More references are needed and can be created by users.Principle Level Explanation 

Findable 

F1 
3 
Unique IDs exist, DOI assignment for future work 
F2 
2 
Machine and user generated metadata is attached 
F3 
1 
Properties used to link data to metadata 
F4 
1 
Comparisons are ndable via a search interface 

Accessible 

A1 
2 
Data is accessed over HTTP (via REST or a user in-
terface), requires user e ort to integrate the ORKG 
API speci cation 
A1.1 
1 
The protocol is free and widely used 
A1.2 
1 
No authentication is required to access the data 
A2 
1 
Metadata is stored in a persistent way and available 
without the data itself 

Interoperable 

I1 
1 
RDF (with type assertions) and CSV export of com-
parisons 
I2 
2 
Reuse of ontologies where possible (ORKG core, 
Dublin core, RDF Data Cube Vocabulary). User re-
sponsible for other ontology reuse. 
I3 
3 
Reusable 

R1 
1 
Machine and user generated metadata is created 
while publishing 
R1.1 
1 
CC-BY SA license 
R1.2 
1 
If a registered user publishes a comparison, the user 
is associated with the published data 
R1.3 
2 
Users can describe contributions using domain-
relevant ontologies 

1=Yes; 2=Yes, requires user e ort; 3=Partially/future work 



## Table 3 :
3Time (in seconds) to perform comparisons with 2-8 contributions using the naive and ORKG approaches.papers used for the evaluation is limited to eight because the naive approach does not scale to larger sets.Number of compared research contributions 
2 
3 
4 
5 
6 
7 
8 
Naive 0.00026 0.1714 0.763 
4.99 112.74 1772.8 14421 
ORKG 0.0035 
0.0013 0.01158 0.02 0.0206 0.0189 0.0204 


Digital Object Identi er 11 https://www.crossref.org 12 https://gitlab.com/TIBHannover/orkg/orkg-pypi 13 https://gitlab.com/TIBHannover/orkg/orkg-papers 14 https://orkg.org/orkg/featured-comparisons
For a more detailed de nition of the FAIR principles, see: https://go-fair.org/ fair-principles
ACKNOWLEDGMENTSThis work was co-funded by the European Research Council for the project ScienceGRAPH (Grant agreement ID: 819536) and the TIB Leibniz Information Centre for Science and Technology. We want to thank Kheir Eddine Farfar for his contributions to this work.
SERIMI -Resource description similarity, RDF instance matching and interlinking. Samur Araujo, Jan Hidders, Daniel Schwabe, Arjen P. De Vries, CEUR Workshop Proceedings. 814Samur Araujo, Jan Hidders, Daniel Schwabe, and Arjen P. De Vries. 2011. SERIMI -Resource description similarity, RDF instance matching and interlinking. CEUR Workshop Proceedings 814 (2011), 246-247.

Exploration and visualization in the web of big linked data: A survey of the state of the art. Nikos Bikakis, Timos Sellis, CEUR Workshop Proceedings. 1558Nikos Bikakis and Timos Sellis. 2016. Exploration and visualization in the web of big linked data: A survey of the state of the art. CEUR Workshop Proceedings 1558 (2016).

The FAIR guiding principles for data stewardship: Fair enough?. Martin Boeckhout, Gerhard A Zielhuis, Annelien L Bredenoord, 10.1038/s41431-018-0160-0European Journal of Human Genetics. 26Martin Boeckhout, Gerhard A. Zielhuis, and Annelien L. Bredenoord. 2018. The FAIR guiding principles for data stewardship: Fair enough? European Journal of Human Genetics 26, 7 (2018), 931-936. https://doi.org/10.1038/s41431-018-0160-0

Enriching Word Vectors with Subword Information. Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov, 10.1162/tacl_a_00051Transactions of the Association for Computational Linguistics. 5Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. En- riching Word Vectors with Subword Information. Transactions of the Association for Computational Linguistics 5 (2017), 135-146. https://doi.org/10.1162/tacl_a_ 00051

The Document Components Ontology (DoCO). Alexandru Constantin, Silvio Peroni, Steve Pettifer, David Shotton, Fabio Vitali, 10.3233/SW-1501777Semantic WebAlexandru Constantin, Silvio Peroni, Steve Pettifer, David Shotton, and Fabio Vitali. 2016. The Document Components Ontology (DoCO). Semantic Web 7, 2 (2016), 167-181. https://doi.org/10.3233/SW-150177

Core techniques of question answering systems over knowledge bases: a survey. Dennis Diefenbach, Vanessa Lopez, Kamal Singh, Pierre Maret, 10.1007/s10115-017-1100-yKnowledge and Information Systems. 55Dennis Diefenbach, Vanessa Lopez, Kamal Singh, and Pierre Maret. 2018. Core techniques of question answering systems over knowledge bases: a survey. Knowledge and Information Systems 55, 3 (2018), 529-569. https://doi.org/10. 1007/s10115-017-1100-y

Towards a Knowledge Graph Representing Research Findings by Semantifying Survey Articles. Said Fathalla, Sahar Vahdati, Sören Auer, Christoph Lange, 10.1007/978-3-319-67008-9_25International Conference on Theory and Practice of Digital Libraries. SpringerSaid Fathalla, Sahar Vahdati, Sören Auer, and Christoph Lange. 2017. Towards a Knowledge Graph Representing Research Findings by Semantifying Survey Articles. In International Conference on Theory and Practice of Digital Libraries. Springer, 315-327. https://doi.org/10.1007/978-3-319-67008-9_25

D Meredith, Walter R Gall, Borg, Educational Research: An introduction. White Plains, NYLongman Publishers USAsixth editionMeredith D. Gall and Walter R. Borg. 1996. Educational Research: An introduction (sixth edition). White Plains, NY: Longman Publishers USA (1996).

The Publishing Work ow Ontology (PWO). Aldo Gangemi, Silvio Peroni, David Shotton, Fabio Vitali, 10.3233/SW-1602308Semantic WebAldo Gangemi, Silvio Peroni, David Shotton, and Fabio Vitali. 2017. The Publishing Work ow Ontology (PWO). Semantic Web 8, 5 (2017), 703-718. https://doi.org/10.3233/SW-160230

Comparing pretrained multilingual word embeddings on an ontology alignment task. Dagmar Gromann, Thierry Declerck, LREC 2018 -11th International Conference on Language Resources and Evaluation. Dagmar Gromann and Thierry Declerck. 2019. Comparing pretrained multi- lingual word embeddings on an ontology alignment task. LREC 2018 -11th International Conference on Language Resources and Evaluation (2019), 230-236.

Designing Scienti c Knowledge Infrastructures: The Contribution of Epistemology. Alexander Hars, 10.1023/A:1011401704862Information Systems Frontiers. 3Alexander Hars. 2001. Designing Scienti c Knowledge Infrastructures: The Contribution of Epistemology. Information Systems Frontiers 3, 1 (2001), 63-73. https://doi.org/10.1023/A:1011401704862

Shedding light on the dark data in the long tail of science. Patrick B Heidorn, 10.1353/lib.0.0036Library Trends. 57Patrick B. Heidorn. 2008. Shedding light on the dark data in the long tail of science. Library Trends 57, 2 (2008), 280-299. https://doi.org/10.1353/lib.0.0036

A survey of author name disambiguation techniques. Ijaz Hussain, Sohail Asghar, 10.1017/s0269888917000182The Knowledge Engineering Review. 32Ijaz Hussain and Sohail Asghar. 2017. A survey of author name disambiguation techniques: 2010âĂŞ2016. The Knowledge Engineering Review 32 (2017), 1-24. https://doi.org/10.1017/s0269888917000182

Open Research Knowledge Graph: Next Generation Infrastructure for Semantic Scholarly Knowledge. Mohamad Yaser Jaradeh, Allard Oelen, Manuel Prinz, D&apos; Jennifer, Gábor Souza, Markus Kismihók, Sören Stocker, Auer, 10.1145/3360901.3364435Proceedings of the 10th International Conference on Knowledge Capture (K-CAP '19). the 10th International Conference on Knowledge Capture (K-CAP '19)ACMMohamad Yaser Jaradeh, Allard Oelen, Manuel Prinz, Jennifer D'Souza, Gábor Kismihók, Markus Stocker, and Sören Auer. 2019. Open Research Knowledge Graph: Next Generation Infrastructure for Semantic Scholarly Knowledge. In In Proceedings of the 10th International Conference on Knowledge Capture (K-CAP '19). ACM. https://doi.org/10.1145/3360901.3364435

Open Research Knowledge Graph: A System Walkthrough. Mohamad Yaser Jaradeh, Allard Oelen, Manuel Prinz, Markus Stocker, Sören Auer, 10.1007/978-3-030-30760-8_31International Conference on Theory and Practice of Digital Libraries. SpringerMohamad Yaser Jaradeh, Allard Oelen, Manuel Prinz, Markus Stocker, and Sören Auer. 2019. Open Research Knowledge Graph: A System Walkthrough. In International Conference on Theory and Practice of Digital Libraries. Springer, 348-351. https://doi.org/10.1007/978-3-030-30760-8_31

Online tools supporting the conduct and reporting of systematic reviews and systematic maps: A case study on CADIMA and review of existing tools. Christian Kohl, Emma J Mcintosh, Stefan Unger, Neal R Haddaway, Joachim Ste En Kecke, Ralf Schiemann, Wilhelm, 10.1186/s13750-018-0115-5Environmental Evidence. 7Christian Kohl, Emma J. McIntosh, Stefan Unger, Neal R. Haddaway, Ste en Kecke, Joachim Schiemann, and Ralf Wilhelm. 2018. Online tools supporting the conduct and reporting of systematic reviews and systematic maps: A case study on CADIMA and review of existing tools. Environmental Evidence 7, 1 (2018), 1-17. https://doi.org/10.1186/s13750-018-0115-5

Decentralized provenance-aware publishing with nanopublications. Tobias Kuhn, Christine Chichester, Michael Krauthammer, Nãžria Queraltrosinach, Ruben Verborgh, George Giannakopoulos, 10.7717/peerj-cs.78PeerJ Computer Science. Tobias Kuhn, Christine Chichester, Michael Krauthammer, NÃžria Queralt- rosinach, Ruben Verborgh, and George Giannakopoulos. 2016. Decentralized provenance-aware publishing with nanopublications. PeerJ Computer Science (2016), 1-29. https://doi.org/10.7717/peerj-cs.78

Finding Citations to Social Work Literature: The Relative Bene ts of Using Web of Science, Scopus, or Google Scholar. Elaine M Lasda Bergman, 10.1016/j.acalib.2012.08.002Journal of Academic Librarianship. 38Elaine M. Lasda Bergman. 2012. Finding Citations to Social Work Literature: The Relative Bene ts of Using Web of Science, Scopus, or Google Scholar. Journal of Academic Librarianship 38, 6 (2012), 370-379. https://doi.org/10.1016/j.acalib. 2012.08.002

Binary codes capable of correcting deletions, insertions, and reversals. Vladimir I Levenshtein, Soviet physics doklady. 10Vladimir I Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet physics doklady, Vol. 10. 707-710.

Measuring structural similarity between RDF graphs. Pierre Maillot, Carlos Bobed, Pierre Maillot, Carlos Bobed, Proceedings of the 33rd Annual ACM Symposium on Applied Computing. the 33rd Annual ACM Symposium on Applied ComputingPierre Maillot, and Carlos BobedPierre Maillot, Carlos Bobed, Pierre Maillot, Carlos Bobed, Pierre Maillot, and Car- los Bobed. 2019. Measuring structural similarity between RDF graphs. Proceedings of the 33rd Annual ACM Symposium on Applied Computing (2019), 1960-1967.

Using TF-IDF to Determine Word Relevance in Document Queries Juan. Pinya Carme, Maria Rosa Rosselló Medina, Ramon, 10.15804/tner.2015.42.4.03New Educational Review. 42Carme Pinya Medina and Maria Rosa Rosselló Ramon. 2015. Using TF-IDF to Determine Word Relevance in Document Queries Juan. New Educational Review 42, 4 (2015), 40-51. https://doi.org/10.15804/tner.2015.42.4.03

Cloudy, increasingly FAIR; Revisiting the FAIR Data guiding principles for the European Open Science Cloud. Barend Mons, Cameron Neylon, Jan Velterop, Michel Dumontier, Luiz Olavo Bonino Da Silva Santos, Mark D Wilkinson, 10.3233/ISU-170824Information Services and Use. 37Barend Mons, Cameron Neylon, Jan Velterop, Michel Dumontier, Luiz Olavo Bonino Da Silva Santos, and Mark D. Wilkinson. 2017. Cloudy, in- creasingly FAIR; Revisiting the FAIR Data guiding principles for the Euro- pean Open Science Cloud. Information Services and Use 37, 1 (2017), 49-56. https://doi.org/10.3233/ISU-170824

Text Summarization with Automatic Keyword Extraction in Telugu e-Newspapers. Reddy Naidu, Korra Santosh Kumar Bharti, Ramesh Kumar Sathya Babu, Mohapatra, 10.1007/978-981-10-5544-7_54Smart Innovation, Systems and Technologies. 77Reddy Naidu, Santosh Kumar Bharti, Korra Sathya Babu, and Ramesh Kumar Mohapatra. 2018. Text Summarization with Automatic Keyword Extraction in Telugu e-Newspapers. In Smart Innovation, Systems and Technologies. Vol. 77. 555-564. https://doi.org/10.1007/978-981-10-5544-7_54

Comparing Research Contributions in a Scholarly Knowledge Graph. Allard Oelen, Mohamad Yaser Jaradeh, Kheir Eddine Farfar, Markus Stocker, Sãűren Auer, Proceedings of the Third International Workshop on Capturing Scienti c Knowledge. the Third International Workshop on Capturing Scienti c KnowledgeAllard Oelen, Mohamad Yaser Jaradeh, Kheir Eddine Farfar, Markus Stocker, and SÃűren Auer. 2019. Comparing Research Contributions in a Scholarly Knowledge Graph. In Proceedings of the Third International Workshop on Capturing Scienti c Knowledge (SciKnow19). 21-26.

Norman Paskin, 10.1081/E-ELIS3-120044418Digital object identi er (DOI®) system. Encyclopedia of library and information sciences. 3Norman Paskin. 2010. Digital object identi er (DOI®) system. Encyclopedia of library and information sciences 3 (2010), 1586-1592. https://doi.org/10.1081/ E-ELIS3-120044418

FaBiO and CiTO: Ontologies for describing bibliographic resources and citations. Silvio Peroni, David Shotton, 10.1016/j.websem.2012.08.001Journal of Web Semantics. 17Silvio Peroni and David Shotton. 2012. FaBiO and CiTO: Ontologies for describing bibliographic resources and citations. Journal of Web Semantics 17 (2012), 33-43. https://doi.org/10.1016/j.websem.2012.08.001

The SPAR ontologies. Silvio Peroni, David Shotton, 10.1007/978-3-030-00668-6_8International Semantic Web Conference. SpringerSilvio Peroni and David Shotton. 2018. The SPAR ontologies. In Interna- tional Semantic Web Conference. Springer, 119-136. https://doi.org/10.1007/ 978-3-030-00668-6_8

Entity Comparison in RDF Graphs. Alina Petrova, Evgeny Sherkhonov, Bernardo Cuenca Grau, Ian Horrocks, 10.1007/978-3-319-68288-4_31International Semantic Web Conference. 526-541. Alina Petrova, Evgeny Sherkhonov, Bernardo Cuenca Grau, and Ian Horrocks. 2017. Entity Comparison in RDF Graphs. In International Semantic Web Confer- ence. 526-541. https://doi.org/10.1007/978-3-319-68288-4_31

A guide to writing the dissertation literature review. J Justus, Randolph, 10.7275/b0az-8t74Practical Assessment, Research and Evaluation. 1413Justus J. Randolph. 2009. A guide to writing the dissertation literature review. Practical Assessment, Research and Evaluation 14, 13 (2009). https://doi.org/10. 7275/b0az-8t74

Publishing FAIR data: An exemplar methodology utilizing PHI-base. Alejandro Rodríguez-Iglesias, Alejandro Rodríguez-González, Alistair G Irvine, Ane Sesma, Martin Urban, Kim E Hammond-Kosack, Mark D Wilkinson, 10.3389/fpls.2016.00641Frontiers in Plant Science. 7Alejandro Rodríguez-Iglesias, Alejandro Rodríguez-González, Alistair G. Irvine, Ane Sesma, Martin Urban, Kim E. Hammond-Kosack, and Mark D. Wilkinson. 2016. Publishing FAIR data: An exemplar methodology utilizing PHI-base. Frontiers in Plant Science 7 (2016). https://doi.org/10.3389/fpls.2016.00641

A review of ontologies for describing scholarly and scienti c documents. Almudena Ruiz Iniesta, Oscar Corcho, 4 th Workshop on Semantic Publishing (SePublica) (CEUR Workshop Proceedings. Almudena Ruiz Iniesta and Oscar Corcho. 2014. A review of ontologies for describing scholarly and scienti c documents. In 4 th Workshop on Semantic Publishing (SePublica) (CEUR Workshop Proceedings).

Semantic representation of scienti c literature: bringing claims, contributions and named entities onto the Linked Open Data cloud. Bahar Sateli, René Witte, 10.7717/peerj-cs.37PeerJ Computer Science. 1Bahar Sateli and René Witte. 2015. Semantic representation of scienti c literature: bringing claims, contributions and named entities onto the Linked Open Data cloud. PeerJ Computer Science 1 (2015), e37. https://doi.org/10.7717/peerj-cs.37

Conducting a Literature Review -The Example of Sustainability in Supply Chains. Stefan Seuring, Martin Müller, Magnus Westhaus, Romy Morana, 10.1007/3-7908-1636-1_7Research Methodologies in Supply Chain Management. HeidelbergPhysica-Verlag HDStefan Seuring, Martin Müller, Magnus Westhaus, and Romy Morana. 2005. Conducting a Literature Review -The Example of Sustainability in Supply Chains. In Research Methodologies in Supply Chain Management. Physica-Verlag HD, Heidelberg, 91-106. https://doi.org/10.1007/3-7908-1636-1_7

Ontology matching: State of the art and future challenges. Pavel Shvaiko, Jérôme Euzenat, 10.1109/TKDE.2011.253IEEE Transactions on Knowledge and Data Engineering. 25Pavel Shvaiko and Jérôme Euzenat. 2013. Ontology matching: State of the art and future challenges. IEEE Transactions on Knowledge and Data Engineering 25, 1 (2013), 158-176. https://doi.org/10.1109/TKDE.2011.253

Achieving human and machine accessibility of cited data in scholarly publications. Joan Starr, Eleni Castro, Mercãĺ Crosas, Michel Dumontier, Robert R Downs, Ruth Duerr, Laurel L Haak, Melissa Haendel, Ivan Herman, Simon Hodson, Joe Hourclé, John Ernest Kratz, Jennifer Lin, 10.7717/peerj-cs.1PeerJ Computer Science. Lars Holm Nielsen, Amy Nurnberger, Stefan Proell, Andreas Rauber, Simone Sacchi, Arthur Smith, Mike Taylor, and Tim Clark5Joan Starr, Eleni Castro, MercÃĺ Crosas, Michel Dumontier, Robert R. Downs, Ruth Duerr, Laurel L. Haak, Melissa Haendel, Ivan Herman, Simon Hodson, Joe Hourclé, John Ernest Kratz, Jennifer Lin, Lars Holm Nielsen, Amy Nurnberger, Stefan Proell, Andreas Rauber, Simone Sacchi, Arthur Smith, Mike Taylor, and Tim Clark. 2015. Achieving human and machine accessibility of cited data in scholarly publications. PeerJ Computer Science 2015, 5 (2015), 1-22. https: //doi.org/10.7717/peerj-cs.1

Writing Integrative Literature Reviews: Guidelines and Examples. Richard J Torraco, 10.1177/1534484305278283Human Resource Development Review. 4Richard J. Torraco. 2005. Writing Integrative Literature Reviews: Guidelines and Examples. Human Resource Development Review 4, 3 (2005), 356-367. https: //doi.org/10.1177/1534484305278283

Semantic Representation of Scienti c Publications. Sahar Vahdati, Said Fathalla, Sören Auer, Christoph Lange, Maria-Esther Vidal, 10.1007/978-3-030-30760-8_37International Conference on Theory and Practice of Digital Libraries. Sahar Vahdati, Said Fathalla, Sören Auer, Christoph Lange, and Maria-Esther Vidal. 2019. Semantic Representation of Scienti c Publications. In International Conference on Theory and Practice of Digital Libraries. 375-379. https://doi.org/ 10.1007/978-3-030-30760-8_37

Analyzing the Past to Prepare for the Future: Writing a Literature Review. Jane Webster, Richard T Watson, MIS Quarterly. 26Jane Webster and Richard T. Watson. 2002. Analyzing the Past to Prepare for the Future: Writing a Literature Review. MIS Quarterly 26, 2 (2002), xiii -xxiii.

How to Write a Literature Review Paper?. Bert Van Wee, David Banister, 10.1080/01441647.2015.1065456Transport Reviews. 36Bert Van Wee and David Banister. 2016. How to Write a Literature Review Paper? Transport Reviews 36, 2 (2016), 278-288. https://doi.org/10.1080/01441647.2015. 1065456

Comment: The FAIR Guiding Principles for scienti c data management and stewardship. D Mark, Michel Wilkinson, Dumontier, Jan Ijsbrand, Gabrielle Aalbersberg, Myles Appleton, Luiz Axton ; Willem Boiten, Silva Bonino Da, Philip E Santos, Jildau Bourne, Anthony J Bouwman, Tim Brookes, Mercãĺ Clark, Ingrid Crosas, Olivier Dillo, Scott Dumon, Chris T Edmunds, Richard Evelo, Alejandra Finkers, Alasdair J G Gonzalez-Beltran, Paul Gray, Carole Groth, Je Goble, Jaap Rey S. Grethe, Heringa, A C Peter, Rob Hoen, Tobias Hooft, Ruben Kuhn, Joost Kok, Scott J Kok, Maryann E Lusher, 10.1038/sdata.2016.18Scienti c Data. 3MartoneMark D. Wilkinson, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Ap- pleton, Myles Axton, Arie Baak, Niklas Blomberg, Jan Willem Boiten, Luiz Bonino da Silva Santos, Philip E. Bourne, Jildau Bouwman, Anthony J. Brookes, Tim Clark, MercÃĺ Crosas, Ingrid Dillo, Olivier Dumon, Scott Edmunds, Chris T. Evelo, Richard Finkers, Alejandra Gonzalez-Beltran, Alasdair J.G. Gray, Paul Groth, Carole Goble, Je rey S. Grethe, Jaap Heringa, Peter A.C. t Hoen, Rob Hooft, Tobias Kuhn, Ruben Kok, Joost Kok, Scott J. Lusher, Maryann E. Mar- tone, Albert Mons, Abel L. Packer, Bengt Persson, Philippe Rocca-Serra, Marco Roos, Rene van Schaik, Susanna Assunta Sansone, Erik Schultes, Thierry Sen- gstag, Ted Slater, George Strawn, Morris A. Swertz, Mark Thompson, Johan Van Der Lei, Erik Van Mulligen, Jan Velterop, Andra Waagmeester, Peter Wittenburg, Katherine Wolstencroft, Jun Zhao, and Barend Mons. 2016. Comment: The FAIR Guiding Principles for scienti c data management and stewardship. Scienti c Data 3 (2016), 1-9. https://doi.org/10.1038/sdata.2016.18

String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage. William E Winkler, 10.1007/978-1-4612-2856-1_101Proceedings of the Section on Survey Research. the Section on Survey ResearchAmerican Statistical AssociationWilliam E. Winkler. 1990. String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage. Proceedings of the Section on Survey Research, American Statistical Association (1990), 354-359. https: //doi.org/10.1007/978-1-4612-2856-1_101

Online comparison system with certain and uncertain criteria based on multi-criteria decision analysis method. Paweł Ziemba, Jarosław Jankowski, Jarosław Wątróbski, 10.1007/978-3-319-67077-5_56International Conference on Computational Collective Intelligence. SpringerPaweł Ziemba, Jarosław Jankowski, and Jarosław Wątróbski. 2017. Online comparison system with certain and uncertain criteria based on multi-criteria decision analysis method. In International Conference on Computational Collective Intelligence. Springer, 579-589. https://doi.org/10.1007/978-3-319-67077-5_56