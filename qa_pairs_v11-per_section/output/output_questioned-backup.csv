corpusid,title,domain,url,section,section_title,paragraph,para_listed_answer,answer,indexed_answer,filtered_refids,filtered_refids_qualified,num_reference,segmented_answer,question,filtered_paragraph_indexs,filtered_answer,QA pair
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s1,BERT embeddings,"['p1.0', 'p1.1']","[""Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence."", ""In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).""]","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","[['b24', 'b22'], ['b21', 'b65']]","[['b24', 'b22'], ['b21', 'b65']]",4,"1. Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.
2. In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.
3. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
4. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
5. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
6. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
7. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).",How do BERT's contextualized embeddings differ from conventional static embeddings?,p1.0 p1.1,"Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","Question: How do BERT's contextualized embeddings differ from conventional static embeddings?

Answer:

Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic)."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s3,Syntactic knowledge,"['p3.0', 'p3.1', 'p3.2']","['As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.', '(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).', 'Regarding syntactic competence of BERT\'s MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT\'s encoding of syntactic structure does not indicate that it actually relies on that knowledge.']","As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","(p3.0) As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(p3.1) (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

(p3.2) Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","[[], ['b18', 'b5'], [None, 'b64', 'b59']]","[[], ['b18', 'b5'], [None, 'b64', 'b59']]",5,"1. As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it.
2. Htut et al.(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root.
3. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation.
4. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2).
5. Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).
6. Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task.
7. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences.
8. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations.
9. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input.
10. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019).
11. This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.",How does BERT encode and utilize syntactic information?,p3.0 p3.1 p3.2,"As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","Question: How does BERT encode and utilize syntactic information?

Answer:

As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s7,Self-attention heads,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4', 'p7.5', 'p7.6', 'p7.7', 'p7.8', 'p7.9']","['Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:', '• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);', '• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).', 'Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.', '[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.', 'Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.', 'Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.', ""(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis."", ""Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data."", ""Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.""]","Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","(p7.0) Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

(p7.1) • attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

(p7.2) • attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

(p7.3) Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

(p7.4) [SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

(p7.5) Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

(p7.6) Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(p7.7) (2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

(p7.8) Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

(p7.9) Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","[[], [], [None, 'b9'], [None], [], [], [], [None, 'b58'], [], [None, 'b9']]","[[], [], [None, 'b9'], [None], [], [], [], [None, 'b58'], [], [None, 'b9']]",7,"1. Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types: attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018); attending to previous/next tokens,    (Kovaleva et al., 2019)
2. According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"".
3. However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 .
4. Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019).
5. This apparent redundancy must be related to the overparametrization issue (see section 7).
6. Attention to (Kovaleva et al., 2019)5 is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to (Kovaleva et al., 2019)4 and punctuation.
7. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as (Kovaleva et al., 2019)5 and (Kovaleva et al., 2019)4, and the model learns to rely on them.
8. They suggest also that the function of (Kovaleva et al., 2019)4 might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.
9. (Kovaleva et al., 2019)4 gets increased attention starting in layer 5, but its importance for prediction drops.
10. If this hypothesis is correct, attention probing studies that excluded the (Kovaleva et al., 2019)4 and (Kovaleva et al., 2019)5 tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.
11. Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.
12. Some BERT heads seem to specialize in certain types of syntactic relations.
13. Htut et al.(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions.
14. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline.
15. The evidence for nsubj, advmod, and amod has some variation between these two studies.
16. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context.
17. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).
18. present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora.
19. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.
20. Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,.
21. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks.
22. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998).
23. Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.",What are the functions and limitations of BERT's self-attention heads?,p7.2 p7.3 p7.5 p7.6 p7.7 p7.8 p7.9,,"Question: What are the functions and limitations of BERT's self-attention heads?

Answer:

"
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s8,BERT layers,"['p8.0', 'p8.1', 'p8.2', 'p8.3', 'p8.4']","['The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.', 'There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.', 'The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.', ""The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT."", 'The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.']","The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","(p8.0) The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

(p8.1) There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

(p8.2) The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

(p8.3) The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

(p8.4) The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","[[], ['b5'], ['b57', 'b5', 'b50'], [None, 'b9', 'b50'], ['b5']]","[[], ['b5'], ['b57', 'b5', 'b50'], [None, 'b9', 'b50'], ['b5']]",8,"1. The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings.
2. It stands to reason that the lower layers have the most linear word order information.
3. report a decrease in the knowledge of linear word order around layer 4 in BERT-base.
4. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.
5. There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers.
6. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large).
7. Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.
8. The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5).
9. There is conflicting evidence about syntactic chunks.
10. Tenney et al. (6-9 for base-BERT, 14-19 for BERT-large)0 conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling.
11. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing.
12. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.
13. The final layers of BERT are the most taskspecific.
14. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable .
15. In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019).
16. At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance.
17. Tenney et al. (6-9 for base-BERT, 14-19 for BERT-large)0 suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers.
18. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (6-9 for base-BERT, 14-19 for BERT-large)1.
19. But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial.
20. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.
21. The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"".
22. However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.",How does syntactic and semantic information distribution vary across BERT layers?,p8.0 p8.1 p8.2 p8.3 p8.4,"The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","Question: How does syntactic and semantic information distribution vary across BERT layers?

Answer:

The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s10,Pre-training BERT,"['p10.0', 'p10.1', 'p10.2', 'p10.3', 'p10.4', 'p10.5', 'p10.6', 'p10.7', 'p10.8', 'p10.9']","['The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.', '• Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).', '• Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).', ""• Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);"", '• Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;', '• Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).', '• Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).', '• Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.', 'Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .', 'Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).']","The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

• Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

• Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

• Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

• Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

• Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

• Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

• Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","(p10.0) The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

(p10.1) • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

(p10.2) • Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

(p10.3) • Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

(p10.4) • Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

(p10.5) • Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

(p10.6) • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

(p10.7) • Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

(p10.8) Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

(p10.9) Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","[[], ['b8', None, 'b62', 'b23', 'b16'], ['b75', 'b62'], ['b8'], [], ['b47'], [], [], ['b17'], ['b9', 'b31']]","[[], ['b8', None, 'b62', 'b23', 'b16'], ['b75', 'b62'], ['b8'], [], ['b47'], [], [], ['b17'], ['b9', 'b31']]",12,"1. The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM).
2. Multiple studies have come up with alternative training objectives to improve on BERT.
3. Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting .
4. Wang et al. (2019a) Mikolov et al. (2013b).
5. Permutation language modeling. Yang et al. (MLM)6 replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order.
6. See also the n-gram word order reconstruction task (Wang et al., 2019a).
7. Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020); Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words; Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).
8. Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (MLM)0.
9. Clinchant et al. (MLM)6 propose replacing the MASK token with (MLM)2 token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.
10. Another obvious source of improvement is pretraining data.
11. Liu et al. (MLM)3 explore the benefits of increasing the corpus volume and longer training.
12. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (MLM)4, there are ongoing efforts to incorporate structured knowledge resources .
13. Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (MLM)5 and ERNIE .
14. Alternatively, SemBERT  integrates semantic role information with BERT representations.
15. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides.
16. Hao et al. (MLM)6 conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (MLM)7.
17. However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (MLM)8.",What are the alternative training objectives proposed to improve BERT's performance?,p10.2 p10.3 p10.4 p10.5 p10.6 p10.7,,"Question: What are the alternative training objectives proposed to improve BERT's performance?

Answer:

"
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s11,Model architecture choices,"['p11.0', 'p11.1']","[""To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks."", 'Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.']","To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","(p11.0) To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

(p11.1) Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","[['b20', 'b75', 'b58', 'b17'], []]","[['b20', 'b75', 'b58', 'b17'], []]",4,"1. To date, the most systematic study of BERT architecture was performed by .
2. They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others.
3. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable.
4. Larger hidden representation size was consistently better, but the gains varied by setting.
5. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance.
6. They also publish their recommendations for other model parameters.
7. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.
8. observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.
9. Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers.
10. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.",What are the key findings on optimizing BERT's architecture and training?,p11.0 p11.1,"To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","Question: What are the key findings on optimizing BERT's architecture and training?

Answer:

To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s12,Fine-tuning BERT,"['p12.0', 'p12.1', 'p12.2', 'p12.3', 'p12.4', 'p12.5', 'p12.6']","['Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).', '• Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).', 'With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.', '(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.', 'An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).', 'Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.', '7 How big should BERT be?']","Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

• Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

7 How big should BERT be?","(p12.0) Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

(p12.1) • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

(p12.2) With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(p12.3) (2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

(p12.4) An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

(p12.5) Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

(p12.6) 7 How big should BERT be?","[[None, 'b9'], ['b83'], [], [], ['b26'], ['b28'], []]","[[None, 'b9'], ['b83'], [], [], ['b26'], ['b28'], []]",5,"1. Pre-training + fine-tuning workflow is a crucial part of BERT.
2. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand.
3. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns.
4. It is understandable why fine-tuning would increase the attention to  Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).
5. Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).
6. With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules.
7. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost.
8. Artetxe et al.(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.
9. An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).Initialization can have a dramatic effect on the training process (Petrov, 2010).
10. However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018).
11. Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation.
12. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.
13. 7 How big should BERT be?",What are the key aspects and findings on fine-tuning BERT models?,p12.0 p12.1 p12.2 p12.3 p12.4 p12.5,"Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

• Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.","Question: What are the key aspects and findings on fine-tuning BERT models?

Answer:

Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

• Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s13,Overparametrization,"['p13.0', 'p13.1', 'p13.2', 'p13.3', 'p13.4']","['Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.', 'Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).', 'Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .', 'Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.', '(2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.']","Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.","(p13.0) Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

(p13.1) Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

(p13.2) Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

(p13.3) Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(p13.4) (2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.","[['b70', 'b38', 'b43'], ['b58', 'b50', None, 'b46', 'b20', 'b9', 'b37'], [], [], []]","[['b70', 'b38', 'b43'], ['b58', 'b50', None, 'b46', 'b20', 'b9', 'b37'], [], [], []]",10,"1. Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT.
2. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.
3. Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have.
4. Voita et al. (Strubell et al., 2019;Schwartz et al., 2019)3 showed that all but a few Transformer heads could be pruned without significant losses in performance.
5. For BERT, Clark et al. (Strubell et al., 2019;Schwartz et al., 2019)3 observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (Strubell et al., 2019;Schwartz et al., 2019)3 were able to reduce most layers to a single head.
6. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance.
7. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019).
8. Additionally, Tenney et al. (Strubell et al., 2019;Schwartz et al., 2019)0 examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (Strubell et al., 2019;Schwartz et al., 2019)1.
9. Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case.
10. In particular, the opposite was observed for subjectverb agreement (Strubell et al., 2019;Schwartz et al., 2019)2 and sentence subject detection .
11. Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers.
12. Clark et al.(Strubell et al., 2019;Schwartz et al., 2019)3 suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.",What are the implications of overparametrization in Transformer-based models like BERT?,p13.0 p13.1 p13.2,"Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .","Question: What are the implications of overparametrization in Transformer-based models like BERT?

Answer:

Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection ."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s14,BERT compression,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4']","['Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.', 'Two main approaches include knowledge distillation and quantization.', 'The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).', ""The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware."", ""Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).""]","Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

Two main approaches include knowledge distillation and quantization.

The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","(p14.0) Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

(p14.1) Two main approaches include knowledge distillation and quantization.

(p14.2) The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

(p14.3) The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

(p14.4) Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","[[], [], ['b54', None, 'b46', 'b37'], ['b76', 'b40'], ['b72', 'b11']]","[[], [], ['b54', None, 'b46', 'b37'], ['b76', 'b40'], ['b72', 'b11']]",8,"1. Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss.
2. Such efforts to date are summarized in Table 1.
3. Two main approaches include knowledge distillation and quantization.
4. The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase).
5. This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).
6. The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019).
7. Note that this strategy often requires compatible hardware.
8. Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).",What are the main approaches to compressing BERT with minimal accuracy loss?,p14.1 p14.2 p14.3,,"Question: What are the main approaches to compressing BERT with minimal accuracy loss?

Answer:

"
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s15,Multilingual BERT,"['p15.0', 'p15.1', 'p15.2', 'p15.3', 'p15.4', 'p15.5', 'p15.6', 'p15.7']","['Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).', 'mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.', 'mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.', 'At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.', 'To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.', 'Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).', 'Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.', 'Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.']","Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.","(p15.0) Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

(p15.1) mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

(p15.2) mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

(p15.3) At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

(p15.4) To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

(p15.5) Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

(p15.6) Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

(p15.7) Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.","[['b36', 'b13', 'b68', 'b30', 'b64'], ['b13', 'b30'], ['b30', 'b42', 'b68', 'b13'], ['b36', 'b68', 'b13', 'b30'], ['b50'], ['b64', 'b2'], ['b58', 'b50', None, 'b2', 'b20', 'b9'], ['b56', 'b39', None, 'b66', 'b4']]","[['b36', 'b13', 'b68', 'b30', 'b64'], ['b13', 'b30'], ['b30', 'b42', 'b68', 'b13'], ['b36', 'b68', 'b13', 'b30'], ['b50'], ['b64', 'b2'], ['b58', 'b50', None, 'b2', 'b20', 'b9'], ['b56', 'b39', None, 'b66', 'b4']]",29,"1. Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary).
2. Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing.
3. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019).
4. The model seems to naturally learn high-quality cross-lingual word alignments (110K wordpiece vocabulary)9, with caveats for open-class parts of speech (Cao et al., 2019).
5. Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (Wu and Dredze, 2019;Pires et al., 2019)8 note that this task could be solvable by simple lexical matches.
6. Pires et al. (Wu and Dredze, 2019;Pires et al., 2019)8 conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.
7. mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (110K wordpiece vocabulary)0, and adding the IDs in pre-training was not beneficial .
8. It is also aware of at least some typological language features (110K wordpiece vocabulary)1, and transfer between structurally similar languages works better Pires et al., 2019).
9. Singh et al. (Wu and Dredze, 2019;Pires et al., 2019)8 argue that if typological features structure its representation space, it could not be considered as interlingua.
10. However, Artetxe et al. (Wu and Dredze, 2019;Pires et al., 2019)8 show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.
11. At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (110K wordpiece vocabulary)4, and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (Wu and Dredze, 2019;Pires et al., 2019)8.
12. Pires et al. (Wu and Dredze, 2019;Pires et al., 2019)8 and Wu and Dredze (Wu and Dredze, 2019;Pires et al., 2019)8 hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages.
13. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary.
14. Artetxe et al. (Wu and Dredze, 2019;Pires et al., 2019)8 also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (110K wordpiece vocabulary)9 vocabulary, without any shared word-pieces.
15. To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge.
16. However, as Tenney et al. (Wu and Dredze, 2019;Pires et al., 2019)0 aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"".
17. There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis .
18. A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.
19. Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (Wu and Dredze, 2019;Pires et al., 2019)1 would not be sufficient (Wu and Dredze, 2019;Pires et al., 2019)2.
20. Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Wu and Dredze, 2019;Pires et al., 2019)3.
21. Head and layer ablation studies (Wu and Dredze, 2019;Pires et al., 2019)4 inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Wu and Dredze, 2019;Pires et al., 2019)5, the absence of heads that would perform parsing ""in general"" (Wu and Dredze, 2019;Pires et al., 2019)6.
22. Ablations are also problematic if the same information was duplicated elsewhere in the network.
23. To mitigate that, Michel et al. (Wu and Dredze, 2019;Pires et al., 2019)8 prune heads in the order set by a proxy importance score, and Voita et al. (Wu and Dredze, 2019;Pires et al., 2019)8
24. fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.
25. Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Wu and Dredze, 2019;Pires et al., 2019)9.
26. However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Rönnqvist et al., 2019)0.
27. Also, visualization is typically limited to qualitative analysis (Rönnqvist et al., 2019)1, and should not be interpreted as definitive evidence.",What are the capabilities and limitations of Multilingual BERT (mBERT)?,p15.0 p15.1 p15.2 p15.3,"Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.","Question: What are the capabilities and limitations of Multilingual BERT (mBERT)?

Answer:

Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s16,Directions for further research,"['p16.0', 'p16.1', 'p16.2', 'p16.3', 'p16.4']","['BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works. In this section, we list what we believe to be the most promising directions for further research, together with the starting points that we already have.', 'Benchmarks that require verbal reasoning. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020). As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.', 'Developing methods to ""teach"" reasoning. While the community had success extracting knowledge from large pre-trained models, they often fail if any reasoning needs to be performed on top of the facts they possess (see subsection 4.3). For instance,  propose a method to ""teach"" BERT quantification, conditionals, comparatives, and boolean coordination.', 'Learning what happens at inference time.', 'Most of the BERT analysis papers focused on different probes of the model, but we know much less about what knowledge actually gets used. At the moment, we know that the knowledge represented in BERT does not necessarily get used in downstream tasks (Kovaleva et al., 2019). As starting points for work in this direction, we also have other head ablation studies (Voita et al., 2019;Michel et al., 2019) and studies of how BERT behaves in reading comprehension task (van Aken et al., 2019; Arkhangelskaia and Dutta, 2019).']","BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works. In this section, we list what we believe to be the most promising directions for further research, together with the starting points that we already have.

Benchmarks that require verbal reasoning. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020). As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.

Developing methods to ""teach"" reasoning. While the community had success extracting knowledge from large pre-trained models, they often fail if any reasoning needs to be performed on top of the facts they possess (see subsection 4.3). For instance,  propose a method to ""teach"" BERT quantification, conditionals, comparatives, and boolean coordination.

Learning what happens at inference time.

Most of the BERT analysis papers focused on different probes of the model, but we know much less about what knowledge actually gets used. At the moment, we know that the knowledge represented in BERT does not necessarily get used in downstream tasks (Kovaleva et al., 2019). As starting points for work in this direction, we also have other head ablation studies (Voita et al., 2019;Michel et al., 2019) and studies of how BERT behaves in reading comprehension task (van Aken et al., 2019; Arkhangelskaia and Dutta, 2019).","(p16.0) BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works. In this section, we list what we believe to be the most promising directions for further research, together with the starting points that we already have.

(p16.1) Benchmarks that require verbal reasoning. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020). As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.

(p16.2) Developing methods to ""teach"" reasoning. While the community had success extracting knowledge from large pre-trained models, they often fail if any reasoning needs to be performed on top of the facts they possess (see subsection 4.3). For instance,  propose a method to ""teach"" BERT quantification, conditionals, comparatives, and boolean coordination.

(p16.3) Learning what happens at inference time.

(p16.4) Most of the BERT analysis papers focused on different probes of the model, but we know much less about what knowledge actually gets used. At the moment, we know that the knowledge represented in BERT does not necessarily get used in downstream tasks (Kovaleva et al., 2019). As starting points for work in this direction, we also have other head ablation studies (Voita et al., 2019;Michel et al., 2019) and studies of how BERT behaves in reading comprehension task (van Aken et al., 2019; Arkhangelskaia and Dutta, 2019).","[[], ['b77', 'b41', 'b45', 'b35'], [], [], ['b20', 'b58', 'b9']]","[[], ['b77', 'b41', 'b45', 'b35'], [], [], ['b20', 'b58', 'b9']]",7,"1. BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works.
2. In this section, we list what we believe to be the most promising directions for further research, together with the starting points that we already have.
3. Benchmarks that require verbal reasoning.
4. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems.
5. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020).
6. As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it.
7. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.
8. Developing methods to ""teach"" reasoning.
9. While the community had success extracting knowledge from large pre-trained models, they often fail if any reasoning needs to be performed on top of the facts they possess (see subsection 4.3).
10. For instance,  propose a method to ""teach"" BERT quantification, conditionals, comparatives, and boolean coordination.
11. Learning what happens at inference time.
12. Most of the BERT analysis papers focused on different probes of the model, but we know much less about what knowledge actually gets used.
13. At the moment, we know that the knowledge represented in BERT does not necessarily get used in downstream tasks (Kovaleva et al., 2019).
14. As starting points for work in this direction, we also have other head ablation studies (Voita et al., 2019;Michel et al., 2019) and studies of how BERT behaves in reading comprehension task (van Aken et al., 2019; Arkhangelskaia and Dutta, 2019).",What are the most promising directions for future BERTology research?,p16.0 p16.1 p16.2 p16.4,"BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works. In this section, we list what we believe to be the most promising directions for further research, together with the starting points that we already have.

Benchmarks that require verbal reasoning. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020). As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.

Developing methods to ""teach"" reasoning. While the community had success extracting knowledge from large pre-trained models, they often fail if any reasoning needs to be performed on top of the facts they possess (see subsection 4.3). For instance,  propose a method to ""teach"" BERT quantification, conditionals, comparatives, and boolean coordination.","Question: What are the most promising directions for future BERTology research?

Answer:

BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works. In this section, we list what we believe to be the most promising directions for further research, together with the starting points that we already have.

Benchmarks that require verbal reasoning. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020). As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.

Developing methods to ""teach"" reasoning. While the community had success extracting knowledge from large pre-trained models, they often fail if any reasoning needs to be performed on top of the facts they possess (see subsection 4.3). For instance,  propose a method to ""teach"" BERT quantification, conditionals, comparatives, and boolean coordination."
