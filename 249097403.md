# ARTICLE TYPE Confidence Intervals for Prevalence Estimates from Complex Surveys with Imperfect Assays

CorpusID: 249097403
 
tags: #Medicine, #Mathematics

URL: [https://www.semanticscholar.org/paper/657c4c6f63d9249e0224052061b3851ac9e0eecf](https://www.semanticscholar.org/paper/657c4c6f63d9249e0224052061b3851ac9e0eecf)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

ARTICLE TYPE Confidence Intervals for Prevalence Estimates from Complex Surveys with Imperfect Assays


Damon M Bayer 
Biostatistics Research Branch
National Institute of Allergy and Infectious Diseases
BethesdaMaryland

Department of Statistics
University of California
Irvine, IrvineCalifornia

Michael P Fay mfay@niaid.nih.gov 
Biostatistics Research Branch
National Institute of Allergy and Infectious Diseases
BethesdaMaryland

Barry I Graubard 
Division of Cancer Epidemiology and Genetics
National Cancer Institute
Rockville

Maryland Correspondence *Michael P. Fay
2089BethesdaMD

ARTICLE TYPE Confidence Intervals for Prevalence Estimates from Complex Surveys with Imperfect Assays
between May and July 2020.Received: Added at production Revised: Added at production Accepted: Added at production DOI: xxx/xxxxconfidence distributionsseroprevalencesurvey samplingweighted sampling
We present several related methods for creating confidence intervals to assess disease prevalence in variety of survey sampling settings. These include simple random samples with imperfect tests, weighted sampling with perfect tests, and weighted sampling with imperfect tests, with the first two settings considered special cases of the third. Our methods use survey results and measurements of test sensitivity and specificity to construct melded confidence intervals. We demonstrate that our methods appear to guarantee coverage in simulated settings, while competing methods are shown to achieve much lower than nominal coverage. We apply our method to a seroprevalence survey of SARS-CoV-2 in undiagnosed adults in the United States

# INTRODUCTION

Estimating and quantifying uncertainty for disease prevalence is a standard task in epidemiology. For rare events, these estimates are highly sensitive to misclassification 1 , making adjustments for sensitivity and specificity critically important. While estimating prevalence (or any event proportion in a population) in complex surveys and adjusting estimates for misclassification have been well studied separately, performing both of these tasks simultaneously remains relatively unexplored. Recent overviews of methods for estimating prevalence in surveys without misclassification are provided by Dean and Pagano 2 and Franco, et al 3 . For simple random sample surveys with imperfect sensitivity and specificity, Lang and Reiczigel 4 proposed an approximate method that performed well in simulations. Recent work by DiCiccio, et al 5 and Cai et al 6 study both valid (i.e., exact) and approximate methods. Their valid methods use test inversion and the adjustment of Berger and Boos 7 , while their approximation methods use the bootstrap with the test inversion approach. Fewer methods are available for for constructing frequentist confidence intervals for prevalence estimates from complex surveys while adjusting for sensitivity and specificity. Kalish et al 8 developed one such method that is closely related to one of the methods presented here, but that method's properties were not studied. Cai et al 6 (see also discussion in DiCiccio et al 5 ) modify their approximation approach to allow sample weights, but it assumes that the number of counts of events within the strata are large (see their Remark 4). Thus, it would not apply to a weighted survey method where each individual their own weight. Another recent advancement is the method developed by Rosin et al 9 that makes use of asymptotic normal approximations which reduce to the Wald interval when sensitivity and specificity are perfect. This problem has also previously been addressed in Bayesian literature, recently by Gelman and Carpenter 10 . We work up to our ultimate goal in stages. First, in Section 2.2, we propose confidence intervals for simple random samples where prevalence is assessed with an assay with imperfect sensitivity and/or specificity. Next, in Section 2.3, we present confidence intervals for weighted samples where prevalence is assessed with an assay without misclassification. In Section 2.4, we combine these methods to create confidence intervals for weighted samples where prevalence is assessed with an assay with imperfect sensitivity and specificity. Because the combined method reduces to one of the first two methods as a special case, we can think of the first two stages as testing the combined method in those cases. Finally, in Section 2.5 we show how certain complex surveys may fit into the format for our new method. Our new method is designed to guarantee coverage in all situations.

In simulations we compare our method to established frequentist competitors. and show through simulations that it beats the best of those in each of the three stages with respect to guaranteeing coverage. We did not include in our simulations the new methods that have been developed in response to the COVID-19 pandemic and are not yet in print in peer reviewed journals 6,5, 9 . The exact method of DiCiccio, et al 5 would guarantee coverage, although applying it to a survey with a large number of strata would be "computationally expensive", and it has not been applied to surveys using post-stratification weighting. In contrast, our new method can very tractable in those situations.


# CONFIDENCE INTERVAL METHODS


## Notation and Problem Set-up

To introduce notation, consider first the stratified simple random sample. Suppose we have a population partitioned into strata, with 1 , 2 , … , individuals in the K strata of the population. We sample 1 , 2 , … , individuals via a simple random sample from each of the strata to have an assay performed to determine who has a disease. Let be the number of positive results from an assay performed on the individuals from stratum and assume ∼ Binomial( , ), where is the population frequency of positive results for assays performed on individuals from stratum . Similarly, let * be the unobserved true number of people with the disease among the individuals from stratum and assume * ∼ Binomial( , * ), where * is the population frequency of cases in stratum . In the case of a perfect assay, = * . Therefore, the population prevalence is * =
∑ =1 * ∑ =1 = ∑ =1 * ,(1)
and the apparent prevalence is
= ∑ =1 ∑ =1 = ∑ =1 ,(2)
where = ∕ ∑ =1 and, therefore, ∑ =1 = 1. This set-up will approximately work for other complex survey samples, where we can estimate survey weights such that the complex survey sample may be treated as a multinomial sample with probabilities proportional to those weights (see Section 2.5).

We can relate and * using the sensitivity ( ) and specificity (1-) of the assay, where and are the proportion of positive assays from a population of positive controls (i.e., individuals known to have the disease) and negative controls (i.e., individuals known to be without the disease), respectively. Then = * + (1 − * ), or equivalently, * = − − ,
and we have * = ∑ =1 * = ∑ =1 − − = ∑ =1 − − ∑ =1 − = ∑ =1 − − −(3)
Suppose the assay is measured on individuals known to not have the disease and on individuals known to have the disease. Let and be the number who test positive from the respective samples. Assume that the negative and positive controls act like simple random samples from their respective populations. Thus, ∼ Binomial( , ) where 1 − is the specificity of the assay, and ∼ Binomial( , ), where is the sensitivity of the assay. Let̂ = ,̂ = , and̂ = . Then a plug-in estimator for * iŝ * =
∑ =1 ̂ ̂ −̂ −̂ ̂ −̂ .(4)
This estimator serves as an important basis for developing confidence intervals in this work. Section 2.2 is concerned with confidence intervals for * in the case where = 1, > 0, < 1, i.e. estimating prevalence from a simple random sample with an imperfect assay. Section 2.3 is concerned with confidence intervals for * in the case where > 1, = 0, = 1, i.e. estimating prevalence from a weighted sample with a perfect assay. Section 2.4 is concerned with confidence intervals for * in the case where > 1, > 0, < 1, i.e. estimating prevalence from a weighted sample with an imperfect assay.


## Estimating Prevalence from a Simple Random Sample with an Imperfect Assay

First, we consider the scenario where = 1, > 0, and < 1. We develop a confidence interval for the population prevalence, * . When = 1, the estimand in Equation 3 becomes * = ( 1 − )∕( − ). We have > for any useful assay, and since the sample is a mixture of individuals with and without the disease of interest,
≥ 1 ≥ . The estimator of * iŝ * ≡ (̂ 1 ,̂ ,̂ ) ≡ ⎧ ⎪ ⎨ ⎪ ⎩ 1 if̂ <̂ <̂ 1 1 −̂ ̂ −̂ if̂ ≥̂ 1 ≥̂ 0 otherwise (5)
where we define 0∕0 = 0.

To create a confidence interval for̂ * , we use a generalization of the melding method 11 , which makes use of lower and upper confidence distributions on functions of independent estimators to account for variability in̂ 1 ,̂ , and̂ . Confidence distributions are like frequentist posterior distributions 12 . The lower and upper confidence distributions are used with discrete responses to ensure the validity of the resulting inferences, and for the binomial case they are equivalent to the posterior distributions that result from using well-calibrated null preference priors 13 .

Each estimated component in Equation 5 is a binomial probability parameter. For each of these, we use distributions associated with the exact binomial confidence interval. For a binomial experiment with successes out of trials, the lower confidence distribution is Beta( , − + 1) with associated random variable , and the upper confidence distribution is Beta( + 1, − ) with random variable , where for > 0 we let Beta(0, ) and Beta( , 0) be point masses at 0 and 1, respectively. Let ( , ) be the th quantile of a random variable . Then the exact 1 − % central confidence interval of Clopper-Pearson 14 for the binomial parameter is
2 , , 1 − 2 , , .(6)
Fay et al 11 proposed a method for obtaining confidence intervals for functions of two parameters that are monotonic within the allowable range for each parameter given the other is fixed. Here we generalize that to * , which is a function of 3 parameters. When 1 ≥ > 1 > ≥ 0 then * is monotonically increasing in 1 , monotonically decreasing in , and monotonically decreasing in . For an assessment of monotonicity in other scenarios see Appendix A. Then the 1 − % confidence interval for * is 2 ,
1 , , , 1 − 2 , 1 , , .(7)
where (⋅) is defined in equation 5. The quantiles of these melded distributions are calculated by Monte Carlo sampling from each of the component distributions. We compare this method to one described in Lang and Reiczigel 4 as implemented in prevSeSp function in 15 , which provides approximate confidence intervals for true prevalence when sensitivity and specificity are estimated from independent samples, as they are in this section. The Lang-Reiczigel interval is given by * ′
1 + ± 1 − 2 , ⋅ Var( * ′ 1 ) 1∕2(8)
where ≡ 1 − 2 , and ∼ (0, 1).
= 2 ⋅ 2 ⋅ * ′ 1 ⋅ ′ (1 − ′ ) ′ − (1 − * ′ 1 ) ⋅ (1 − ′ ) ′ ′ Var( * ′ 1 ) = * ′ 1 (1− * ′ 1 ) 1 + * ′ 1 2 ′ (1− ′ ) + 1 + * ′ 1 2 (1− ′ ) ′ ( ′ − ′ ) 2 ′ = + 2 ′ = + 2 ′ = ⋅̂ + 1 + 2 1 − ′ = ⋅ (1 −̂ ) + 1 + 2 * ′ 1 = ′ 1 − ′ ′ − ′ and ′ 1 = 1 ⋅̂ 1 + 2 ∕2 1 + 2 .

## Estimating Prevalence from a Weighted Sample with a Perfect Assay

Next, we present a confidence interval for the population prevalence, * , in the scenario > 1, = 0, = 1. Our method is a straightforward adaptation of the gamma confidence interval presented in Fay and Feuer 16 , which was developed to create confidence intervals for a population rate which is assumed to be a weighted sum of Poisson rate parameters. We note that for sufficiently large sample size and small rate , a Poisson( ) distribution is approximately equal in distribution to a Binomial( , ) distribution. Under this Poisson assumption, we suggest the 100(1 − )% gamma confidence interval for * :
2 , * , 1 − 2 , *(9)
where * ∼ Gamma
2 , * ∼ Gamma * 2 * , * * = ∑ =1 = ∑ =1 2 * = + max 1 1 , … , * = + max 1 1 , … , 2 .
We call this the wsPoison method, since it assumes a weighted sum of Poissons. We compare the wsPoisson confidence interval to two methods presented in Dean and Pagano 2 , which were recommended for scenarios with low prevalence. Dean and Pagano showed in simulations that the standard Wald interval had poor coverage with low prevalence (e.g., Fig. 1 of that paper showed 95% confidence intervals with coverage of less than 85% for prevalence values less than 2%). Since the confidence interval of Rosin, et al 9 reduces to the Wald interval with perfect assays, we will not include that method in the simulation comparisons.

The first recommended method of Dean and Pagano is an adaptation of the method of Agresti and Coull 17 for the survey setting. The interval for * is given by:
̃ ± √̃ (1 −̃ )∕̃ (10) wherẽ = ∑ =1 ̂ eff + ̃ = eff + 2 ̃ =̃ ∕̃ = 2 ∕2 eff = ∑ =1 ̂ 1 − ∑ =1 ̂ ∑ =1 2 ̂(11)
In the case where ∑ =1 2 ̂ = 0, we instead let eff = ∑ =1 . We also compare our suggested method to Dean and Pagano's modification of the method of Korn and Graubard 18,2 . This interval is given by
2 , , 1 − 2 ,(12)
where analogously to the Clopper-Pearson interval (see equation 6),
∼ Beta eff , eff − eff + 1 ∼ Beta eff + 1, eff − eff with eff = eff ∑ =1
̂ , and eff defined in Equation 11. Although Dean and Pagano 2 expressed this in terms of the F distributions, the beta distribution representation is equivalent.


## Estimating Prevalence from a Weighted Sample with an Imperfect Assay

Lastly, we develop a confidence interval for the population prevalence, * , in the case where > 1, > 0, < 1. The two methods we discuss are closely related to each other and the methods discussed in Sections 2.2 and 2.3. As in Section 2.2, we use the melding method 11 to create 1 − % confidence interval very similar to Equation 7. The confidence distributions for and are the same Beta distributions as in Section 2.2. The two methods differ in their confidence distributions for the apparent prevalence .

In the first case, we use the adaptation of the gamma confidence interval 16 presented in Section 2.3 to derive the 1 − % confidence interval for * :
2 , * , , , 1 − 2 , * , ,(13)
where * and * are defined in Section 2.3. We refer to this method as the WprevSeSp Poisson -weighted prevalence with sensitivity and specificity, where the prevalence confidence distribution is based on the weighted sum of Poissons.

The alternative method is very similar to that used in Kalish, et al 8 . We use the modification of 18 presented in 2 , as in Section 2.3, to derive the 100(1 − )% confidence interval for * :
2 , , , , 1 − 2 , , , .(14)
where and are as defined in Section 2.3. Although equivalent, this expression looks different than in Kalish, et al 8 because they used a parameter for specificity, rather than , which is 1 minus specificity. We refer to this as WprevSeSp Binomial -weighted prevalence with sensitivity and specificity, where the prevalence confidence distribution is based on a binomial variance assumption.


## Applications to More Complex Surveys


### When Can We Use These Methods

In Section 2.1, we derived methods assuming that the apparent prevalence was a weighted sum of binomial random variables, = ∑


## =1

, where ∼ Binomial( , ). We used the fact that for small and large the binomial can be approximated by the Poisson, giving ⋅ ∼ Poisson( ). Thus, whenever we can model a complex survey estimator of apparent prevalence as a weighted sum of Poisson variates, then we can apply the methods of this paper.

In the upcoming Section 2.5.2, we give a detailed review relating the multinomial sampling model to a weighted sum of Poisson variates model. The multinomial sampling model treats the survey sample as if it is a sampling with replacement from the entire population of individuals, where each of the individuals has a probability of of being sampled for each of the samples from the survey, with ∑ =1 = 1. Under this model the number of times each of the individuals is included in the sample is a multinomial with parameters and [ 1 , … , ]. The multinomial model describes sampling with replacement, but it is nevertheless used to approximate a sampling design where the th individual is sampled without replacement with probability , even though under that design (unlike the multinomial model) no individual is included in the sample more than once. The multinomial model is a common approximation for other complex survey designs; see e.g., 19, p. 14 . For example, in the Kalish, et al 8 analysis of Section 4 each individual in the sample is assigned a pseudo-weight approximating one over their sampling probability from a multinomial model. The actual sample was not a probability sample. In fact, it was a quota sample from a very large pool of self-selected volunteers, and the pseudo-weights were calculated using a different large survey that was a probability weighted survey. The pseudo-weights were calculated such that if they were analyzed under the multinomial model, they would adjust for selection bias due to self-selection of the volunteers and the imperfection of the quota sampling.


### Multinomial Sampling Model
Let 1 , … ,
be the binary indicators of event in the individuals in the population of interest, so the prevalence is = −1 ∑


## =1

. There are many ways to design a complex survey sample, and it is often useful to analyze them as if individuals were sampled with replacement with the sampling probability of the th individual equal to , with ∑ =1 = 1 In other words, we treat the sample as if it was independent multinomial samples each with one trial and selection probability vector [ 1 , … , ]. Let = 1 if the th draw for the sample is individual in the population, and 0 otherwise. Then let = and = when = 1. Here, following the tradition in the survey literature, we use capital letters for the population of interest (e.g., N,Y,P), and lower case letters for the sample (e.g., n,y,p). In this notation, both and are fixed, and only the variables representing the sampling (i.e., the variables) are random. Under this independent multinomial model, since ( ) = , an unbiased estimator of iŝ
= 1 ∑ =1 = 1 ∑ =1 ∑ =1(15)
and an unbiased estimator of var(̂ ) under the multinomial model iŝ
var (̂ ) = 1 ( − 1) ∑ =1 −̂ 2(16)
(see Korn and Graubard 19 Problem 2.2-10). We can writê as a weighted sum. Traditional survey weighting defines the weights so that the weight for the th sampled individual can be interpreted as the number of individuals in the population that the th sampled individual represents. Following that tradition, let ( ) = 1∕( ) and ) = 1∕( ), then the expected sum of the sampled weights is ,
∑ =1 ( ) = ∑ =1 ∑ =1 ( ) = ∑ =1 ∑ =1 ( ) = ∑ =1 ∑ =1 1 = .
Sometimes the weights are scaled after selection so that the scaled weights are ( ) = ( ) ∑ =1 ( ) and are forced to sum to . For example, in Kalish et al 8 rescaling (sometimes called post-stratification) was done in a more complicated manner to ensure that the weights summed to the US census population within age group, sex, race, ethnicity and region.

For this paper we define the weights differently, because we want to model our estimator as a weighted sum of Poisson random variables. Thus, we use = 1∕( ) and = 1∕( ) so that the sums have expectation 1. In the complex survey case, we start with the independent multinomial model as in equation 15, then we use the relationship between the multinomial and Poisson distributions. Using the "multinomial-Poisson transformation", the maximum likelihood estimates (MLE) for a multinomial random variable are equivalent to the MLEs for independent Poisson random variables, and the variances are asymptotically equivalent (see Baker 20 ). Even though we model̂ using multinomial random variables where there are many missing values (which occurs in our situation whenever = 0), that multinomial-Poisson relationship holds even when there are missing variables (see Baker 20 , Section 3). For both the Poisson and multinomial models, ( ) = , and̂ is unbiased under either model. For the Poisson model, all the are independent and each mean equals its variance, so that the variance of̂ under this model is
var ̂ = var 1 ∑ =1 ∑ =1 = ∑ =1 ∑ =1 var ( ) 2 2 2 2 = ∑ =1 ∑ =1 2 2 .
We estimate var ̂ by multiplying each term in the sum by ∕ , which has an expectation of 1 and eliminates terms of non-selected individuals, givingv
ar ̂ = ∑ =1 ∑ =1 2 2 2 = ∑ =1 2 2 2(17)
Under the Poisson modelvar ̂ is an unbiased estimator of var ̂ .


# SIMULATIONS


## Estimating Prevalence from a Simple Random Sample with an Imperfect Assay

We assess and compare our new method (Melding, i.e., equation 7) to that of Lang and Reiczigel (LR) in a variety of simulated settings. In each simulation, 100 subjects are tested to estimate prevalence, 60 are tested to estimate sensitivity, and 300 are tested to estimate specificity. Several combinations of prevalences (0.5%-2%), sensitivities (75%-100%) and specificities (75%-100%) are assessed. Each simulated scenario is replicated 10,000 times. Figure 1 compares the two methods based on coverage, while Figures 2 and 3 present the lower and upper error frequencies for these scenarios, respectively. Figure 1 shows that, when specificity is less than perfect, both methods achieve approximately nominal coverage, with the melding method being somewhat more conservative. When specificity is 100%, both methods are conservative. Figure 3 shows that both methods make upper errors with roughly the same frequency. Figure 2 demonstrates that while the melding procedure bounds the lower error frequency below 2.5%, the Lang-Reiczigel method generally has lower error above 2.5%, which is undesirable for applications for which there is a need to bound the lower errors.


## Estimating Prevalence from a Weighted Sample with a Perfect Assay

We compare the wsPoisson method to the more traditional Dean-Pagano modification of the Agresti-Coull (DPAC) method and the Korn-Graubard (KG) method for survey proportions in a variety of settings. Our simulations examine varying levels of disease prevalence (0.5% or 5%), different types survey designs (50 sampling strata with 200 subjects each or 8000 individuals, each with their own weight), distributions of weights among the sampling strata or individuals (coefficient of variation from  approximately 0% to nearly 600%), and the number and weights of sampling strata with non-zero prevalence. For each combination of prevalence , and group type, up to 500 sets of weights are simulated. These 500 sets of weights are designed to span a range of coefficients of variation. For a target coefficient of variation, , weights ( ) are simulated by generating samples from a Beta 1 2 − 1 2 − 1 , −1 2 − −1 2 − −1 distribution and normalizing so that ∑ =1 = 1. This assures that the coefficient of variation among these weights is approximately . Then, certain weights are chosen to have non-zero prevalence (5%, 25%, or 75% distributed either among the highest weights, lowest weights, or distributed uniformly). These weights with non-zero prevalence are given a prevalence such that ∑ =1 = . For each simulated set of parameters and weights, 10,000 data sets are simulated and assessed.

The coverage properties for these simulations are presented in Figures   From Figures 4-7, we note that the two competitor methods generally exhibit lower coverage as the coefficient of variation among the weights increases. In Figure 4, this coverage falls below 60% when the prevalence very low and is concentrated among the highest weights, and the coefficient of variation among the weights exceeds 4. Uniform distribution of prevalence among the weights, increased overall prevalence, and larger sample sizes among fewer groups all appear to lessen the severity of this problem. In contrast, the wsPoisson method appears to guarantee coverage in all scenarios. The wsPoisson method tends to become more conservative when the coefficient of variation among the weights increases, when the other methods can have problems guaranteeing coverage. In all cases, the wsPoisson method is more conservative than the competitor methods. This is similar to the behavior observed in Fay and Feuer 16 , where, in simulations, the overall error rate for the gamma intervals decreased as the variance of the weights increased. Because our methods appear to be very conservative, with coverage near 100% in some cases, we present the widths of the confidence intervals in Figures B9-B12. In scenarios where coefficient of variation among the survey weights is high, the wsPoisson intervals are often two or three times wider than intervals produced by competing methods.


## Estimating Prevalence from a Weighted Sample with an Imperfect Assay

We compare properties our melded confidence interval WprevSeSp Poisson, to another melded confidence interval method WprevSeSp Binomial, and one method, wsPoisson, which does not account for the imperfect assay. The methods are assessed in a several simulated scenarios with varying levels of disease prevalence (0.5% or 5%), types of groups surveyed (50 groups 200 subjects or 8000 groups of 1 subject), distributions of weights among the groups (coefficient of variation from approximately 0% to nearly 6%), the number of groups with non-zero prevalence, and the specificity of the assay (80% -100%). In each scenario, the assay has 95% sensitivity. Each scenario creates up to 500 new sets of weights and parameters (as in Section 3.2), and each of those is simulated 10,000 times, with new prevalence, sensitivity, and specificity surveys generated and 95% confidence intervals are created. Modelled after the study of Kalish, et al 8 , the simulated sensitivity is assessed based on 60 tests, while specificity is based on 300 tests.

The coverage properties for these simulations are presented in Figures 8-11. Additional properties for these simulations are presented in Figures B13-B24.

Based on Figures B13-B20, we note that, in most settings, the two melding procedures result in conservative confidence intervals, often nearing 100% coverage. With perfect specificity, the WprevSeSp Binomial method fails to maintain nominal coverage when the coefficient of variation among the weights is high and specificity is perfect. Only our proposed WprevSeSp Poisson method maintains or exceed the desired coverage in all scenarios. In these scenarios, we also assess properties of the wsPoisson procedure, which does not account for the the imperfect assay. This method results in approximately 0% coverage  in any scenarios where specificity is less than perfect. For this reason, results from method are omitted in the figures. Because our methods appear to be very conservative, with coverage near 100% in some cases, we present the widths of the confidence intervals in Figures B21-B24. The WprevSeSp Binomial and WprevSeSp Poisson methods typically produce wide intervals of approximately the same size -sometimes as wide as 12%, even when true prevalence is 0.05%. One notable exception to this is presented in Figure B23, which shows that for tests with perfect specificity, the WprevSeSP Binomial method produces much narrower confidence intervals than the other method.


# APPLICATIONS

We apply these two methods to a real data set from Kalish, et al 8 . This data set was collected to estimate seroprevalence of SARS-CoV-2 in undiagnosed adults in the United States between May and July 2020. The assay used in this data is estimated to have perfect sensitivity, based on 56 tests on individuals with confirmed SARS-CoV-2 and perfect specificity based on 300 tests on individuals confirmed to not have SARS-CoV-2. First we apply the methods to the full data set  ( = 8058, weight coefficient of variation = 252%). The seroprevalence in Kalish, et al was 4.6% with (95% CI: 2.6% to 6.5%), using a confidence interval method that was nearly the same as the WprevSeSp Binomial method (the method Kalish et al included a calculation of the variability of the weights due to the estimation of the weights, whereas in this paper we treat the weights as fixed constants). The Korn and Graubard type melded confidence interval with imperfect assay adjustments (Wpre-vSeSp Binomial) studied in this paper produced the 95% confidence interval for population prevalence nearly the equivalent, (2.53%, 6.68%). while the wsPoisson type melded confidence interval with imperfect assay adjustments (WprevSeSp Poisson) produced the 95% confidence interval (2.56%, 7.54%). We also apply the wsPoisson method from Section 2.3, which does not account for imperfections in the assay, resulting in a 95% confidence interval of (3.04%, 7.39%). While all three intervals overlap to a large degree, the WprevSeSp Poisson interval is the widest. Our simulations show that in this situation, the WprevSeSp Binomial interval may be the best, because with coefficient of variance about 250% (see Figures B15 and B20, top right panel) the error on both sides of the confidence interval is bounded at 2.5% and the width of the intervals are better ( Figure B23). We also apply the methods to the subset of only Hispanic participants ( = 1281, weight coefficient of variation = 306%), where Kalish et al estimated the undiagnosed adult seroprevalence estimate as 6.1% (95% CI: 2.4% to 11.5%). The WprevSeSp  Binomial method produces a 95% confidence interval for population prevalence (2.35%, 11.75%), while the WprevSeSp Poisson method produces a 95% confidence interval (2.40%, 20.02%). The wsPoisson method produces a 95% confidence interval of (2.80%, 19.63%). In this case, the two melded confidence intervals are much wider than the WprevSeSp Binomial interval, which is as expected since the melding method is designed to guarantee coverage, although the simulations show that the WprevSeSp Binomial interval may be reasonable (see e.g., Figure B20). The smaller WprevSeSp Binomial interval is also unsurprising and is similar to the results observed in our simulation study.  


# DISCUSSION

We presented several methods for creating confidence intervals to assess disease prevalence in variety of settings, including simple random samples with imperfect tests, weighted sampling with perfect tests, and weighted sampling with imperfect tests. One of the new methods was very similar to the method used by Kalish et al 8 , and in this paper we have explored its properties. These new confidence intervals appear to guarantee coverage in most simulated settings, and in general to demonstrate higher coverage than competitor methods. In the case of the simple random sample with an imperfect test, our new methods are able to bound the lower error rate for a 95% confidence interval at 2.5%, while the Lang-Reiczigel 4 method maintains 95% coverage by allowing a higher lower error rate. A big advantage of our method is that it may be applied with complex survey methods, where each individual has their own weight, such as in Kalish et al 8 . However, our method only studied fixed weights and not when the weights are estimated as in Kalish et al. Further worked is needed to address such cases. In addition, further worked is needed to consider other complex sample designs such as multistage cluster designs that are used in household and institutional surveys such hospital and medical practice surveys.  Our method was slightly different to that used in Kalish, et al 8 , in that thdatae latter method included estimates of the variability of the weights; however, recalculating the confidence intervals on the same data shows that in that case there was little difference.

Our methods' conservative properties are especially advantageous in settings where the competitor methods exhibit much lower than nominal coverage. For example, there is high variance among the sample weights and prevalence is concentrated among the highest-weighted samples competitor coverage of 95% confidence intervals can fall to 60% for competitor methods, while our method exhibits > 95% coverage. Thus, we suggest that our melding methods be employed when working with survey settings which involve high variance among the weights or lower errors are particularly undesirable. 


# DATA AVAILABILITY STATEMENT

Data sharing is not applicable to this article as no new data were created or analyzed in this study. 


## APPENDIX A MONOTONICITY OF

It is clear that is monotonic within each of its piecewise-defined functions. In the following sections we consider if monotonicity holds at the change points between piecewise functions.


## A.1 Monotonicity in̂

• Case 1:̂ <̂ 1 . Not monotonic.
(̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ 0̂ <̂ <̂ 1 0̂ =̂ <̂ 1 1̂ <̂ <̂ 1 1 −̂ ̂ −̂ = 1̂ <̂ =̂ 1 1 −̂ ̂ −̂ ̂ <̂ 1 <̂ (A1) • Case 2:̂ 1 <̂ . Monotonic. (̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0̂ <̂ 1 <̂ 0̂ 1 =̂ <̂ 0̂ 1 <̂ <̂ 0̂ 1 <̂ =̂ 0̂ 1 <̂ <̂ (A2)
• Case 3:̂ 1 =̂ . Monotonic.
(̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0̂ <̂ 1 =̂ ̂ 1 −̂ ̂ −̂ = 0 0 = 0̂ =̂ 1 =̂ ̂ 1 −̂ ̂ −̂ = 0 −̂ = 0̂ 1 =̂ <̂ (A3)

## A.2 Monotonicity in̂ 1

• Case 1:̂ <̂ . Monotonic.
(̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ 0̂ 1 <̂ <̂ ̂ 1 −̂ ̂ −̂ = 0̂ =̂ 1 <̂ ̂ 1 −̂ ̂ −̂ ̂ <̂ 1 <̂ ̂ 1 −̂ ̂ −̂ = 1̂ <̂ 1 =̂ 1̂ <̂ <̂ 1 (A4)
• Case 2:̂ <̂ . Monotonic.
(̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0̂ 1 <̂ <̂ 0̂ =̂ 1 <̂ 0̂ <̂ 1 <̂ 0̂ <̂ 1 =̂ 0̂ <̂ <̂ 1 (A5) • Case 3:̂ =̂ . Monotonic. (̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎨ ⎪ ⎩ 0̂ 1 <̂ =̂ ̂ 1 −̂ ̂ −̂ = 0 0 = 0̂ 1 =̂ =̂ 0̂ =̂ <̂ 1 (A6)

## A.3 Monotonicity in̂

• Case 1:̂ <̂ 1 . Monotonic. 
(̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 1̂ <̂ <̂ 1 0̂ =̂ <̂ 1 0̂ <̂ <̂ 1 0̂ <̂ =̂ 1 0̂ <̂ 1 <̂ (A7) • Case 2:̂ 1 <̂ . Monotonic. (̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩̂ 1 −̂ ̂ −̂ ̂ <̂ 1 <̂ ̂ 1 −̂ ̂ −̂ = 0 −̂ = 0̂ 1 =̂ <̂ 0̂ 1 <̂ <̂ 0̂ 1 <̂ =̂ 0̂ 1 <̂ <̂ (A8) • Case 3:̂ 1 =̂ . Monotonic. (̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩̂ 1 −̂ ̂ −̂ = 1̂ <̂ 1 =̂ ̂ 1 −̂ ̂ −̂ = 0 0 = 0̂ =̂ 1 =̂ 0̂ 1 =̂ <̂ (A9)

## B ADDITIONAL FIGURES


## FIGURE B24

Confidence interval width properties for the confidence interval procedures, WprevSeSp Binomial and WprevS-eSp Poisson. Each point represents 10,000 simulations of datasets from a population with 5% Prevalence where 8000 individuals are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. Colored dashed lines are estimates from a logistic regression model using quadratic splines.

## 0
Abbreviations: ANA, anti-nuclear antibodies; APC, antigen-presenting cells; IRF, interferon regulatory factor arXiv:2205.13494v1 [stat.ME] 26 May 2022

## FIGURE 2
2Lower error properties of 95% confidence intervals for our new method (Melding) and the Lang-Reiczigel method in a variety of settings, each simulated 10,000 times.

## FIGURE 3
3Upper error properties of 95% confidence intervals for our new method (Melding) and the Lang-Reiczigel method in a variety of settings, each simulated 10,000 times.

## FIGURE 4
4Coverage properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of the Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets from a population with 0.5% Prevalence where 50 groups of 200 people are sampled. The horizontal dashed line indicates the nominal coverage, 95%. Colored dashed lines are estimates from a logistic regression model using quadratic splines.

## FIGURE 5
5Coverage properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of the Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets from a population with 0.5% Prevalence where 8000 individuals are sampled. The horizontal dashed line indicates the nominal coverage, 95%. Colored dashed lines are estimates from a logistic regression model using quadratic splines.

## FIGURE 6
6Coverage properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of the Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets from a population with 5% Prevalence where 50 groups of 200 people are sampled. The horizontal dashed line indicates the nominal coverage, 95%. Colored dashed lines are estimates from a logistic regression model using quadratic splines.

## FIGURE 7
7Coverage properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of Agresti-Coull and Korn-Graubard Each point represents 10,000 simulations of datasets from a population with 5% Prevalence where 8000 individuals are sampled. The horizontal dashed line indicates the nominal coverage, 95%. Colored dashed lines are estimates from a logistic regression model using quadratic splines.

## FIGURE 8# INTRODUCTION

Estimating and quantifying uncertainty for disease prevalence is a standard task in epidemiology. For rare events, these estimates are highly sensitive to misclassification 1 , making adjustments for sensitivity and specificity critically important. While estimating prevalence (or any event proportion in a population) in complex surveys and adjusting estimates for misclassification have been well studied separately, performing both of these tasks simultaneously remains relatively unexplored. Recent overviews of methods for estimating prevalence in surveys without misclassification are provided by Dean and Pagano 2 and Franco, et al 3 . For simple random sample surveys with imperfect sensitivity and specificity, Lang and Reiczigel 4 proposed an approximate method that performed well in simulations. Recent work by DiCiccio, et al 5 and Cai et al 6 study both valid (i.e., exact) and approximate methods. Their valid methods use test inversion and the adjustment of Berger and Boos 7 , while their approximation methods use the bootstrap with the test inversion approach. Fewer methods are available for for constructing frequentist confidence intervals for prevalence estimates from complex surveys while adjusting for sensitivity and specificity. Kalish et al 8 developed one such method that is closely related to one of the methods presented here, but that method's properties were not studied. Cai et al 6 (see also discussion in DiCiccio et al 5 ) modify their approximation approach to allow sample weights, but it assumes that the number of counts of events within the strata are large (see their Remark 4). Thus, it would not apply to a weighted survey method where each individual their own weight. Another recent advancement is the method developed by Rosin et al 9 that makes use of asymptotic normal approximations which reduce to the Wald interval when sensitivity and specificity are perfect. This problem has also previously been addressed in Bayesian literature, recently by Gelman and Carpenter 10 . We work up to our ultimate goal in stages. First, in Section 2.2, we propose confidence intervals for simple random samples where prevalence is assessed with an assay with imperfect sensitivity and/or specificity. Next, in Section 2.3, we present confidence intervals for weighted samples where prevalence is assessed with an assay without misclassification. In Section 2.4, we combine these methods to create confidence intervals for weighted samples where prevalence is assessed with an assay with imperfect sensitivity and specificity. Because the combined method reduces to one of the first two methods as a special case, we can think of the first two stages as testing the combined method in those cases. Finally, in Section 2.5 we show how certain complex surveys may fit into the format for our new method. Our new method is designed to guarantee coverage in all situations.

In simulations we compare our method to established frequentist competitors. and show through simulations that it beats the best of those in each of the three stages with respect to guaranteeing coverage. We did not include in our simulations the new methods that have been developed in response to the COVID-19 pandemic and are not yet in print in peer reviewed journals 6,5, 9 . The exact method of DiCiccio, et al 5 would guarantee coverage, although applying it to a survey with a large number of strata would be "computationally expensive", and it has not been applied to surveys using post-stratification weighting. In contrast, our new method can very tractable in those situations.


# CONFIDENCE INTERVAL METHODS


## Notation and Problem Set-up

To introduce notation, consider first the stratified simple random sample. Suppose we have a population partitioned into strata, with 1 , 2 , … , individuals in the K strata of the population. We sample 1 , 2 , … , individuals via a simple random sample from each of the strata to have an assay performed to determine who has a disease. Let be the number of positive results from an assay performed on the individuals from stratum and assume ∼ Binomial( , ), where is the population frequency of positive results for assays performed on individuals from stratum . Similarly, let * be the unobserved true number of people with the disease among the individuals from stratum and assume * ∼ Binomial( , * ), where * is the population frequency of cases in stratum . In the case of a perfect assay, = * . Therefore, the population prevalence is * =
∑ =1 * ∑ =1 = ∑ =1 * ,(1)
and the apparent prevalence is
= ∑ =1 ∑ =1 = ∑ =1 ,(2)
where = ∕ ∑ =1 and, therefore, ∑ =1 = 1. This set-up will approximately work for other complex survey samples, where we can estimate survey weights such that the complex survey sample may be treated as a multinomial sample with probabilities proportional to those weights (see Section 2.5).

We can relate and * using the sensitivity ( ) and specificity (1-) of the assay, where and are the proportion of positive assays from a population of positive controls (i.e., individuals known to have the disease) and negative controls (i.e., individuals known to be without the disease), respectively. Then = * + (1 − * ), or equivalently, * = − − ,
and we have * = ∑ =1 * = ∑ =1 − − = ∑ =1 − − ∑ =1 − = ∑ =1 − − −(3)
Suppose the assay is measured on individuals known to not have the disease and on individuals known to have the disease. Let and be the number who test positive from the respective samples. Assume that the negative and positive controls act like simple random samples from their respective populations. Thus, ∼ Binomial( , ) where 1 − is the specificity of the assay, and ∼ Binomial( , ), where is the sensitivity of the assay. Let̂ = ,̂ = , and̂ = . Then a plug-in estimator for * iŝ * =
∑ =1 ̂ ̂ −̂ −̂ ̂ −̂ .(4)
This estimator serves as an important basis for developing confidence intervals in this work. Section 2.2 is concerned with confidence intervals for * in the case where = 1, > 0, < 1, i.e. estimating prevalence from a simple random sample with an imperfect assay. Section 2.3 is concerned with confidence intervals for * in the case where > 1, = 0, = 1, i.e. estimating prevalence from a weighted sample with a perfect assay. Section 2.4 is concerned with confidence intervals for * in the case where > 1, > 0, < 1, i.e. estimating prevalence from a weighted sample with an imperfect assay.


## Estimating Prevalence from a Simple Random Sample with an Imperfect Assay

First, we consider the scenario where = 1, > 0, and < 1. We develop a confidence interval for the population prevalence, * . When = 1, the estimand in Equation 3 becomes * = ( 1 − )∕( − ). We have > for any useful assay, and since the sample is a mixture of individuals with and without the disease of interest,
≥ 1 ≥ . The estimator of * iŝ * ≡ (̂ 1 ,̂ ,̂ ) ≡ ⎧ ⎪ ⎨ ⎪ ⎩ 1 if̂ <̂ <̂ 1 1 −̂ ̂ −̂ if̂ ≥̂ 1 ≥̂ 0 otherwise (5)
where we define 0∕0 = 0.

To create a confidence interval for̂ * , we use a generalization of the melding method 11 , which makes use of lower and upper confidence distributions on functions of independent estimators to account for variability in̂ 1 ,̂ , and̂ . Confidence distributions are like frequentist posterior distributions 12 . The lower and upper confidence distributions are used with discrete responses to ensure the validity of the resulting inferences, and for the binomial case they are equivalent to the posterior distributions that result from using well-calibrated null preference priors 13 .

Each estimated component in Equation 5 is a binomial probability parameter. For each of these, we use distributions associated with the exact binomial confidence interval. For a binomial experiment with successes out of trials, the lower confidence distribution is Beta( , − + 1) with associated random variable , and the upper confidence distribution is Beta( + 1, − ) with random variable , where for > 0 we let Beta(0, ) and Beta( , 0) be point masses at 0 and 1, respectively. Let ( , ) be the th quantile of a random variable . Then the exact 1 − % central confidence interval of Clopper-Pearson 14 for the binomial parameter is
2 , , 1 − 2 , , .(6)
Fay et al 11 proposed a method for obtaining confidence intervals for functions of two parameters that are monotonic within the allowable range for each parameter given the other is fixed. Here we generalize that to * , which is a function of 3 parameters. When 1 ≥ > 1 > ≥ 0 then * is monotonically increasing in 1 , monotonically decreasing in , and monotonically decreasing in . For an assessment of monotonicity in other scenarios see Appendix A. Then the 1 − % confidence interval for * is 2 ,
1 , , , 1 − 2 , 1 , , .(7)
where (⋅) is defined in equation 5. The quantiles of these melded distributions are calculated by Monte Carlo sampling from each of the component distributions. We compare this method to one described in Lang and Reiczigel 4 as implemented in prevSeSp function in 15 , which provides approximate confidence intervals for true prevalence when sensitivity and specificity are estimated from independent samples, as they are in this section. The Lang-Reiczigel interval is given by * ′
1 + ± 1 − 2 , ⋅ Var( * ′ 1 ) 1∕2(8)
where ≡ 1 − 2 , and ∼ (0, 1).
= 2 ⋅ 2 ⋅ * ′ 1 ⋅ ′ (1 − ′ ) ′ − (1 − * ′ 1 ) ⋅ (1 − ′ ) ′ ′ Var( * ′ 1 ) = * ′ 1 (1− * ′ 1 ) 1 + * ′ 1 2 ′ (1− ′ ) + 1 + * ′ 1 2 (1− ′ ) ′ ( ′ − ′ ) 2 ′ = + 2 ′ = + 2 ′ = ⋅̂ + 1 + 2 1 − ′ = ⋅ (1 −̂ ) + 1 + 2 * ′ 1 = ′ 1 − ′ ′ − ′ and ′ 1 = 1 ⋅̂ 1 + 2 ∕2 1 + 2 .

## Estimating Prevalence from a Weighted Sample with a Perfect Assay

Next, we present a confidence interval for the population prevalence, * , in the scenario > 1, = 0, = 1. Our method is a straightforward adaptation of the gamma confidence interval presented in Fay and Feuer 16 , which was developed to create confidence intervals for a population rate which is assumed to be a weighted sum of Poisson rate parameters. We note that for sufficiently large sample size and small rate , a Poisson( ) distribution is approximately equal in distribution to a Binomial( , ) distribution. Under this Poisson assumption, we suggest the 100(1 − )% gamma confidence interval for * :
2 , * , 1 − 2 , *(9)
where * ∼ Gamma
2 , * ∼ Gamma * 2 * , * * = ∑ =1 = ∑ =1 2 * = + max 1 1 , … , * = + max 1 1 , … , 2 .
We call this the wsPoison method, since it assumes a weighted sum of Poissons. We compare the wsPoisson confidence interval to two methods presented in Dean and Pagano 2 , which were recommended for scenarios with low prevalence. Dean and Pagano showed in simulations that the standard Wald interval had poor coverage with low prevalence (e.g., Fig. 1 of that paper showed 95% confidence intervals with coverage of less than 85% for prevalence values less than 2%). Since the confidence interval of Rosin, et al 9 reduces to the Wald interval with perfect assays, we will not include that method in the simulation comparisons.

The first recommended method of Dean and Pagano is an adaptation of the method of Agresti and Coull 17 for the survey setting. The interval for * is given by:
̃ ± √̃ (1 −̃ )∕̃ (10) wherẽ = ∑ =1 ̂ eff + ̃ = eff + 2 ̃ =̃ ∕̃ = 2 ∕2 eff = ∑ =1 ̂ 1 − ∑ =1 ̂ ∑ =1 2 ̂(11)
In the case where ∑ =1 2 ̂ = 0, we instead let eff = ∑ =1 . We also compare our suggested method to Dean and Pagano's modification of the method of Korn and Graubard 18,2 . This interval is given by
2 , , 1 − 2 ,(12)
where analogously to the Clopper-Pearson interval (see equation 6),
∼ Beta eff , eff − eff + 1 ∼ Beta eff + 1, eff − eff with eff = eff ∑ =1
̂ , and eff defined in Equation 11. Although Dean and Pagano 2 expressed this in terms of the F distributions, the beta distribution representation is equivalent.


## Estimating Prevalence from a Weighted Sample with an Imperfect Assay

Lastly, we develop a confidence interval for the population prevalence, * , in the case where > 1, > 0, < 1. The two methods we discuss are closely related to each other and the methods discussed in Sections 2.2 and 2.3. As in Section 2.2, we use the melding method 11 to create 1 − % confidence interval very similar to Equation 7. The confidence distributions for and are the same Beta distributions as in Section 2.2. The two methods differ in their confidence distributions for the apparent prevalence .

In the first case, we use the adaptation of the gamma confidence interval 16 presented in Section 2.3 to derive the 1 − % confidence interval for * :
2 , * , , , 1 − 2 , * , ,(13)
where * and * are defined in Section 2.3. We refer to this method as the WprevSeSp Poisson -weighted prevalence with sensitivity and specificity, where the prevalence confidence distribution is based on the weighted sum of Poissons.

The alternative method is very similar to that used in Kalish, et al 8 . We use the modification of 18 presented in 2 , as in Section 2.3, to derive the 100(1 − )% confidence interval for * :
2 , , , , 1 − 2 , , , .(14)
where and are as defined in Section 2.3. Although equivalent, this expression looks different than in Kalish, et al 8 because they used a parameter for specificity, rather than , which is 1 minus specificity. We refer to this as WprevSeSp Binomial -weighted prevalence with sensitivity and specificity, where the prevalence confidence distribution is based on a binomial variance assumption.


## Applications to More Complex Surveys


### When Can We Use These Methods

In Section 2.1, we derived methods assuming that the apparent prevalence was a weighted sum of binomial random variables, = ∑


## =1

, where ∼ Binomial( , ). We used the fact that for small and large the binomial can be approximated by the Poisson, giving ⋅ ∼ Poisson( ). Thus, whenever we can model a complex survey estimator of apparent prevalence as a weighted sum of Poisson variates, then we can apply the methods of this paper.

In the upcoming Section 2.5.2, we give a detailed review relating the multinomial sampling model to a weighted sum of Poisson variates model. The multinomial sampling model treats the survey sample as if it is a sampling with replacement from the entire population of individuals, where each of the individuals has a probability of of being sampled for each of the samples from the survey, with ∑ =1 = 1. Under this model the number of times each of the individuals is included in the sample is a multinomial with parameters and [ 1 , … , ]. The multinomial model describes sampling with replacement, but it is nevertheless used to approximate a sampling design where the th individual is sampled without replacement with probability , even though under that design (unlike the multinomial model) no individual is included in the sample more than once. The multinomial model is a common approximation for other complex survey designs; see e.g., 19, p. 14 . For example, in the Kalish, et al 8 analysis of Section 4 each individual in the sample is assigned a pseudo-weight approximating one over their sampling probability from a multinomial model. The actual sample was not a probability sample. In fact, it was a quota sample from a very large pool of self-selected volunteers, and the pseudo-weights were calculated using a different large survey that was a probability weighted survey. The pseudo-weights were calculated such that if they were analyzed under the multinomial model, they would adjust for selection bias due to self-selection of the volunteers and the imperfection of the quota sampling.


### Multinomial Sampling Model
Let 1 , … ,
be the binary indicators of event in the individuals in the population of interest, so the prevalence is = −1 ∑


## =1

. There are many ways to design a complex survey sample, and it is often useful to analyze them as if individuals were sampled with replacement with the sampling probability of the th individual equal to , with ∑ =1 = 1 In other words, we treat the sample as if it was independent multinomial samples each with one trial and selection probability vector [ 1 , … , ]. Let = 1 if the th draw for the sample is individual in the population, and 0 otherwise. Then let = and = when = 1. Here, following the tradition in the survey literature, we use capital letters for the population of interest (e.g., N,Y,P), and lower case letters for the sample (e.g., n,y,p). In this notation, both and are fixed, and only the variables representing the sampling (i.e., the variables) are random. Under this independent multinomial model, since ( ) = , an unbiased estimator of iŝ
= 1 ∑ =1 = 1 ∑ =1 ∑ =1(15)
and an unbiased estimator of var(̂ ) under the multinomial model iŝ
var (̂ ) = 1 ( − 1) ∑ =1 −̂ 2(16)
(see Korn and Graubard 19 Problem 2.2-10). We can writê as a weighted sum. Traditional survey weighting defines the weights so that the weight for the th sampled individual can be interpreted as the number of individuals in the population that the th sampled individual represents. Following that tradition, let ( ) = 1∕( ) and ) = 1∕( ), then the expected sum of the sampled weights is ,
∑ =1 ( ) = ∑ =1 ∑ =1 ( ) = ∑ =1 ∑ =1 ( ) = ∑ =1 ∑ =1 1 = .
Sometimes the weights are scaled after selection so that the scaled weights are ( ) = ( ) ∑ =1 ( ) and are forced to sum to . For example, in Kalish et al 8 rescaling (sometimes called post-stratification) was done in a more complicated manner to ensure that the weights summed to the US census population within age group, sex, race, ethnicity and region.

For this paper we define the weights differently, because we want to model our estimator as a weighted sum of Poisson random variables. Thus, we use = 1∕( ) and = 1∕( ) so that the sums have expectation 1. In the complex survey case, we start with the independent multinomial model as in equation 15, then we use the relationship between the multinomial and Poisson distributions. Using the "multinomial-Poisson transformation", the maximum likelihood estimates (MLE) for a multinomial random variable are equivalent to the MLEs for independent Poisson random variables, and the variances are asymptotically equivalent (see Baker 20 ). Even though we model̂ using multinomial random variables where there are many missing values (which occurs in our situation whenever = 0), that multinomial-Poisson relationship holds even when there are missing variables (see Baker 20 , Section 3). For both the Poisson and multinomial models, ( ) = , and̂ is unbiased under either model. For the Poisson model, all the are independent and each mean equals its variance, so that the variance of̂ under this model is
var ̂ = var 1 ∑ =1 ∑ =1 = ∑ =1 ∑ =1 var ( ) 2 2 2 2 = ∑ =1 ∑ =1 2 2 .
We estimate var ̂ by multiplying each term in the sum by ∕ , which has an expectation of 1 and eliminates terms of non-selected individuals, givingv
ar ̂ = ∑ =1 ∑ =1 2 2 2 = ∑ =1 2 2 2(17)
Under the Poisson modelvar ̂ is an unbiased estimator of var ̂ .


# SIMULATIONS


## Estimating Prevalence from a Simple Random Sample with an Imperfect Assay

We assess and compare our new method (Melding, i.e., equation 7) to that of Lang and Reiczigel (LR) in a variety of simulated settings. In each simulation, 100 subjects are tested to estimate prevalence, 60 are tested to estimate sensitivity, and 300 are tested to estimate specificity. Several combinations of prevalences (0.5%-2%), sensitivities (75%-100%) and specificities (75%-100%) are assessed. Each simulated scenario is replicated 10,000 times. Figure 1 compares the two methods based on coverage, while Figures 2 and 3 present the lower and upper error frequencies for these scenarios, respectively. Figure 1 shows that, when specificity is less than perfect, both methods achieve approximately nominal coverage, with the melding method being somewhat more conservative. When specificity is 100%, both methods are conservative. Figure 3 shows that both methods make upper errors with roughly the same frequency. Figure 2 demonstrates that while the melding procedure bounds the lower error frequency below 2.5%, the Lang-Reiczigel method generally has lower error above 2.5%, which is undesirable for applications for which there is a need to bound the lower errors.


## Estimating Prevalence from a Weighted Sample with a Perfect Assay

We compare the wsPoisson method to the more traditional Dean-Pagano modification of the Agresti-Coull (DPAC) method and the Korn-Graubard (KG) method for survey proportions in a variety of settings. Our simulations examine varying levels of disease prevalence (0.5% or 5%), different types survey designs (50 sampling strata with 200 subjects each or 8000 individuals, each with their own weight), distributions of weights among the sampling strata or individuals (coefficient of variation from  approximately 0% to nearly 600%), and the number and weights of sampling strata with non-zero prevalence. For each combination of prevalence , and group type, up to 500 sets of weights are simulated. These 500 sets of weights are designed to span a range of coefficients of variation. For a target coefficient of variation, , weights ( ) are simulated by generating samples from a Beta 1 2 − 1 2 − 1 , −1 2 − −1 2 − −1 distribution and normalizing so that ∑ =1 = 1. This assures that the coefficient of variation among these weights is approximately . Then, certain weights are chosen to have non-zero prevalence (5%, 25%, or 75% distributed either among the highest weights, lowest weights, or distributed uniformly). These weights with non-zero prevalence are given a prevalence such that ∑ =1 = . For each simulated set of parameters and weights, 10,000 data sets are simulated and assessed.

The coverage properties for these simulations are presented in Figures   From Figures 4-7, we note that the two competitor methods generally exhibit lower coverage as the coefficient of variation among the weights increases. In Figure 4, this coverage falls below 60% when the prevalence very low and is concentrated among the highest weights, and the coefficient of variation among the weights exceeds 4. Uniform distribution of prevalence among the weights, increased overall prevalence, and larger sample sizes among fewer groups all appear to lessen the severity of this problem. In contrast, the wsPoisson method appears to guarantee coverage in all scenarios. The wsPoisson method tends to become more conservative when the coefficient of variation among the weights increases, when the other methods can have problems guaranteeing coverage. In all cases, the wsPoisson method is more conservative than the competitor methods. This is similar to the behavior observed in Fay and Feuer 16 , where, in simulations, the overall error rate for the gamma intervals decreased as the variance of the weights increased. Because our methods appear to be very conservative, with coverage near 100% in some cases, we present the widths of the confidence intervals in Figures B9-B12. In scenarios where coefficient of variation among the survey weights is high, the wsPoisson intervals are often two or three times wider than intervals produced by competing methods.


## Estimating Prevalence from a Weighted Sample with an Imperfect Assay

We compare properties our melded confidence interval WprevSeSp Poisson, to another melded confidence interval method WprevSeSp Binomial, and one method, wsPoisson, which does not account for the imperfect assay. The methods are assessed in a several simulated scenarios with varying levels of disease prevalence (0.5% or 5%), types of groups surveyed (50 groups 200 subjects or 8000 groups of 1 subject), distributions of weights among the groups (coefficient of variation from approximately 0% to nearly 6%), the number of groups with non-zero prevalence, and the specificity of the assay (80% -100%). In each scenario, the assay has 95% sensitivity. Each scenario creates up to 500 new sets of weights and parameters (as in Section 3.2), and each of those is simulated 10,000 times, with new prevalence, sensitivity, and specificity surveys generated and 95% confidence intervals are created. Modelled after the study of Kalish, et al 8 , the simulated sensitivity is assessed based on 60 tests, while specificity is based on 300 tests.

The coverage properties for these simulations are presented in Figures 8-11. Additional properties for these simulations are presented in Figures B13-B24.

Based on Figures B13-B20, we note that, in most settings, the two melding procedures result in conservative confidence intervals, often nearing 100% coverage. With perfect specificity, the WprevSeSp Binomial method fails to maintain nominal coverage when the coefficient of variation among the weights is high and specificity is perfect. Only our proposed WprevSeSp Poisson method maintains or exceed the desired coverage in all scenarios. In these scenarios, we also assess properties of the wsPoisson procedure, which does not account for the the imperfect assay. This method results in approximately 0% coverage  in any scenarios where specificity is less than perfect. For this reason, results from method are omitted in the figures. Because our methods appear to be very conservative, with coverage near 100% in some cases, we present the widths of the confidence intervals in Figures B21-B24. The WprevSeSp Binomial and WprevSeSp Poisson methods typically produce wide intervals of approximately the same size -sometimes as wide as 12%, even when true prevalence is 0.05%. One notable exception to this is presented in Figure B23, which shows that for tests with perfect specificity, the WprevSeSP Binomial method produces much narrower confidence intervals than the other method.


# APPLICATIONS

We apply these two methods to a real data set from Kalish, et al 8 . This data set was collected to estimate seroprevalence of SARS-CoV-2 in undiagnosed adults in the United States between May and July 2020. The assay used in this data is estimated to have perfect sensitivity, based on 56 tests on individuals with confirmed SARS-CoV-2 and perfect specificity based on 300 tests on individuals confirmed to not have SARS-CoV-2. First we apply the methods to the full data set  ( = 8058, weight coefficient of variation = 252%). The seroprevalence in Kalish, et al was 4.6% with (95% CI: 2.6% to 6.5%), using a confidence interval method that was nearly the same as the WprevSeSp Binomial method (the method Kalish et al included a calculation of the variability of the weights due to the estimation of the weights, whereas in this paper we treat the weights as fixed constants). The Korn and Graubard type melded confidence interval with imperfect assay adjustments (Wpre-vSeSp Binomial) studied in this paper produced the 95% confidence interval for population prevalence nearly the equivalent, (2.53%, 6.68%). while the wsPoisson type melded confidence interval with imperfect assay adjustments (WprevSeSp Poisson) produced the 95% confidence interval (2.56%, 7.54%). We also apply the wsPoisson method from Section 2.3, which does not account for imperfections in the assay, resulting in a 95% confidence interval of (3.04%, 7.39%). While all three intervals overlap to a large degree, the WprevSeSp Poisson interval is the widest. Our simulations show that in this situation, the WprevSeSp Binomial interval may be the best, because with coefficient of variance about 250% (see Figures B15 and B20, top right panel) the error on both sides of the confidence interval is bounded at 2.5% and the width of the intervals are better ( Figure B23). We also apply the methods to the subset of only Hispanic participants ( = 1281, weight coefficient of variation = 306%), where Kalish et al estimated the undiagnosed adult seroprevalence estimate as 6.1% (95% CI: 2.4% to 11.5%). The WprevSeSp  Binomial method produces a 95% confidence interval for population prevalence (2.35%, 11.75%), while the WprevSeSp Poisson method produces a 95% confidence interval (2.40%, 20.02%). The wsPoisson method produces a 95% confidence interval of (2.80%, 19.63%). In this case, the two melded confidence intervals are much wider than the WprevSeSp Binomial interval, which is as expected since the melding method is designed to guarantee coverage, although the simulations show that the WprevSeSp Binomial interval may be reasonable (see e.g., Figure B20). The smaller WprevSeSp Binomial interval is also unsurprising and is similar to the results observed in our simulation study.  


# DISCUSSION

We presented several methods for creating confidence intervals to assess disease prevalence in variety of settings, including simple random samples with imperfect tests, weighted sampling with perfect tests, and weighted sampling with imperfect tests. One of the new methods was very similar to the method used by Kalish et al 8 , and in this paper we have explored its properties. These new confidence intervals appear to guarantee coverage in most simulated settings, and in general to demonstrate higher coverage than competitor methods. In the case of the simple random sample with an imperfect test, our new methods are able to bound the lower error rate for a 95% confidence interval at 2.5%, while the Lang-Reiczigel 4 method maintains 95% coverage by allowing a higher lower error rate. A big advantage of our method is that it may be applied with complex survey methods, where each individual has their own weight, such as in Kalish et al 8 . However, our method only studied fixed weights and not when the weights are estimated as in Kalish et al. Further worked is needed to address such cases. In addition, further worked is needed to consider other complex sample designs such as multistage cluster designs that are used in household and institutional surveys such hospital and medical practice surveys.  Our method was slightly different to that used in Kalish, et al 8 , in that thdatae latter method included estimates of the variability of the weights; however, recalculating the confidence intervals on the same data shows that in that case there was little difference.

Our methods' conservative properties are especially advantageous in settings where the competitor methods exhibit much lower than nominal coverage. For example, there is high variance among the sample weights and prevalence is concentrated among the highest-weighted samples competitor coverage of 95% confidence intervals can fall to 60% for competitor methods, while our method exhibits > 95% coverage. Thus, we suggest that our melding methods be employed when working with survey settings which involve high variance among the weights or lower errors are particularly undesirable. 


# DATA AVAILABILITY STATEMENT

Data sharing is not applicable to this article as no new data were created or analyzed in this study. 


## APPENDIX A MONOTONICITY OF

It is clear that is monotonic within each of its piecewise-defined functions. In the following sections we consider if monotonicity holds at the change points between piecewise functions.


## A.1 Monotonicity in̂

• Case 1:̂ <̂ 1 . Not monotonic.
(̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ 0̂ <̂ <̂ 1 0̂ =̂ <̂ 1 1̂ <̂ <̂ 1 1 −̂ ̂ −̂ = 1̂ <̂ =̂ 1 1 −̂ ̂ −̂ ̂ <̂ 1 <̂ (A1) • Case 2:̂ 1 <̂ . Monotonic. (̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0̂ <̂ 1 <̂ 0̂ 1 =̂ <̂ 0̂ 1 <̂ <̂ 0̂ 1 <̂ =̂ 0̂ 1 <̂ <̂ (A2)
• Case 3:̂ 1 =̂ . Monotonic.
(̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0̂ <̂ 1 =̂ ̂ 1 −̂ ̂ −̂ = 0 0 = 0̂ =̂ 1 =̂ ̂ 1 −̂ ̂ −̂ = 0 −̂ = 0̂ 1 =̂ <̂ (A3)

## A.2 Monotonicity in̂ 1

• Case 1:̂ <̂ . Monotonic.
(̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ 0̂ 1 <̂ <̂ ̂ 1 −̂ ̂ −̂ = 0̂ =̂ 1 <̂ ̂ 1 −̂ ̂ −̂ ̂ <̂ 1 <̂ ̂ 1 −̂ ̂ −̂ = 1̂ <̂ 1 =̂ 1̂ <̂ <̂ 1 (A4)
• Case 2:̂ <̂ . Monotonic.
(̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0̂ 1 <̂ <̂ 0̂ =̂ 1 <̂ 0̂ <̂ 1 <̂ 0̂ <̂ 1 =̂ 0̂ <̂ <̂ 1 (A5) • Case 3:̂ =̂ . Monotonic. (̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎨ ⎪ ⎩ 0̂ 1 <̂ =̂ ̂ 1 −̂ ̂ −̂ = 0 0 = 0̂ 1 =̂ =̂ 0̂ =̂ <̂ 1 (A6)

## A.3 Monotonicity in̂

• Case 1:̂ <̂ 1 . Monotonic. 
(̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 1̂ <̂ <̂ 1 0̂ =̂ <̂ 1 0̂ <̂ <̂ 1 0̂ <̂ =̂ 1 0̂ <̂ 1 <̂ (A7) • Case 2:̂ 1 <̂ . Monotonic. (̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩̂ 1 −̂ ̂ −̂ ̂ <̂ 1 <̂ ̂ 1 −̂ ̂ −̂ = 0 −̂ = 0̂ 1 =̂ <̂ 0̂ 1 <̂ <̂ 0̂ 1 <̂ =̂ 0̂ 1 <̂ <̂ (A8) • Case 3:̂ 1 =̂ . Monotonic. (̂ 1 ,̂ ,̂ ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩̂ 1 −̂ ̂ −̂ = 1̂ <̂ 1 =̂ ̂ 1 −̂ ̂ −̂ = 0 0 = 0̂ =̂ 1 =̂ 0̂ 1 =̂ <̂ (A9)

## B ADDITIONAL FIGURES


## FIGURE B24

Confidence interval width properties for the confidence interval procedures, WprevSeSp Binomial and WprevS-eSp Poisson. Each point represents 10,000 simulations of datasets from a population with 5% Prevalence where 8000 individuals are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. Colored dashed lines are estimates from a logistic regression model using quadratic splines.

## 0
Abbreviations: ANA, anti-nuclear antibodies; APC, antigen-presenting cells; IRF, interferon regulatory factor arXiv:2205.13494v1 [stat.ME] 26 May 2022

## FIGURE 2
2Lower error properties of 95% confidence intervals for our new method (Melding) and the Lang-Reiczigel method in a variety of settings, each simulated 10,000 times.

## FIGURE 3
3Upper error properties of 95% confidence intervals for our new method (Melding) and the Lang-Reiczigel method in a variety of settings, each simulated 10,000 times.

## FIGURE 4
4Coverage properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of the Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets from a population with 0.5% Prevalence where 50 groups of 200 people are sampled. The horizontal dashed line indicates the nominal coverage, 95%. Colored dashed lines are estimates from a logistic regression model using quadratic splines.

## FIGURE 5
5Coverage properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of the Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets from a population with 0.5% Prevalence where 8000 individuals are sampled. The horizontal dashed line indicates the nominal coverage, 95%. Colored dashed lines are estimates from a logistic regression model using quadratic splines.

## FIGURE 6
6Coverage properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of the Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets from a population with 5% Prevalence where 50 groups of 200 people are sampled. The horizontal dashed line indicates the nominal coverage, 95%. Colored dashed lines are estimates from a logistic regression model using quadratic splines.

## FIGURE 7
7Coverage properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of Agresti-Coull and Korn-Graubard Each point represents 10,000 simulations of datasets from a population with 5% Prevalence where 8000 individuals are sampled. The horizontal dashed line indicates the nominal coverage, 95%. Colored dashed lines are estimates from a logistic regression model using quadratic splines.

## FIGURE 8
8Coverage properties for the confidence interval procedures, WprevSeSp Binomial and WprevSeSp Poisson. Each point represents 10,000 simulations of datasets from a population with 0.5% Prevalence where 50 groups of 200 people are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. The horizontal dashed line indicates the nominal coverage, 95%. Colored dashed lines are estimates from a logistic regression model using quadratic splines.


Weights with Non−0 Prev. 5%Weights with Non−0 Prev. 
25% 

Weights with Non−0 Prev. 
75% 

Weights with Prev. 
High Weights 

Weights with Prev. 
Uniform Weights 

Weights with Prev. 
Low Weights 

0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 

60% 

70% 

80% 

90% 

100% 

85.0% 

90.0% 

95.0% 

100.0% 

96.0% 

98.0% 

100.0% 

Weight Coefficient of Variation 

Coverage 

Method 

Agresti−Coull 
Korn−Graubard 
wsPoisson 

Each Point = 10,000 Replications 

Coverage Properties for Simulations with 0.5% Prevalence Among 50 Groups of 200 




FIGURE 9Coverage properties for the confidence interval procedures, WprevSeSp Binomial and WprevSeSp Poisson. Each point represents 10,000 simulations of datasets from a population with 0.5% Prevalence where 8000 individuals are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. The horizontal dashed line indicates the nominal coverage, 95%. Colored dashed lines are estimates from a logistic regression model using quadratic splines.For sharing the data from the Kalish, et al study, we thank Matthew Memoli and the LID Clinical Studies Unit of the National Institute of Allergy and Infectious Diseases, NIH, Kaitlyn Sadtler from the National Institute of Biomedical Imaging and Bioengineering, NIH, Matthew Hall from the National Center for Advancing Translational Sciences, NIH, and Dominic Esposito from the Fredrick National Laboratory for Cancer Research, NCI, NIH.Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
0% 
200% 
400% 
0% 
200% 
400% 

80.0% 

85.0% 

90.0% 

95.0% 

100.0% 

80.0% 

85.0% 

90.0% 

95.0% 

100.0% 

95.00% 

96.00% 

97.00% 

98.00% 

99.00% 

100.00% 

Weight Coefficient of Variation 

Coverage 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 

Each Facet = 95% Sensitivity, Each Point = 10,000 Replications 

Coverage Properties for Simulations with 0.5% Prevalence Among 8,000 Groups of 1 

6 ACKNOWLEDGEMENTS 




FIGURE 10Coverage properties for the confidence interval procedures, WprevSeSp Binomial and WprevSeSp Poisson. Each point represents 10,000 simulations of datasets from a population with 5% Prevalence where 50 groups of 200 people are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. The horizontal dashed line indicates the nominal coverage, 95%. Colored dashed lines are estimates from a logistic regression model using quadratic splines.FIGURE 11Coverage properties for the confidence interval procedures, WprevSeSp Binomial and WprevSeSp Poisson. Each point represents 10,000 simulations of datasets from a population with 5% Prevalence where 8000 individuals are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. The horizontal dashed line indicates the nominal coverage, 95%. Colored dashed lines are estimates from a logistic regression model using quadratic splines. Cai B, Ioannidis JPA, Bendavid E, Tian L. Exact inference for disease prevalence based on a test with unknown specificity and sensitivity.Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 

95.0% 

96.0% 

97.0% 

98.0% 

99.0% 

95.00% 

96.00% 

97.00% 

98.00% 

99.00% 

100.00% 

95.00% 

96.00% 

97.00% 

98.00% 

99.00% 

100.00% 

Weight Coefficient of Variation 

Coverage 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 

Each Facet = 95% Sensitivity, Each Point = 10,000 Replications 

Coverage Properties for Simulations with 5.0% Prevalence Among 50 Groups of 200 

8 BIBLIOGRAPHY 
Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
0% 
200% 
400% 
0% 
200% 
400% 

94.0% 

96.0% 

98.0% 

100.0% 

95.00% 

96.00% 

97.00% 

98.00% 

99.00% 

100.00% 

95.00% 

96.00% 

97.00% 

98.00% 

99.00% 

100.00% 

Weight Coefficient of Variation 

Coverage 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 

Each Facet = 95% Sensitivity, Each Point = 10,000 Replications 

Coverage Properties for Simulations with 5.0% Prevalence Among 8,000 Groups of 1 

4. Lang Z, Reiczigel J. Confidence limits for prevalence of disease adjusted for estimated sensitivity and specificity. Preventive 
Veterinary Medicine 2014; 113(1): 13-22. doi: https://doi.org/10.1016/j.prevetmed.2013.09.015 

5. DiCiccio TJ, Ritzwoller DM, Romano JP, Shaikh AM. Confidence Intervals for Seroprevalence. 2021. 
doi: 
10.48550/ARXIV.2103.15018 

6. Journal of Applied Statistics 2022; 0(0): 1-25. doi: 10.1080/02664763.2021.2019687 

7. Berger RL, Boos DD. P Values Maximized Over a Confidence Set for the Nuisance Parameter. Journal of the American 
Statistical Association 1994; 89(427): 1012-1016. doi: 10.1080/01621459.1994.10476836 

8. Kalish H, Klumpp-Thomas C, Hunsberger S, et al. Undiagnosed SARS-CoV-2 Seropositivity During the First Six Months 
of the COVID-19 Pandemic in the United States. Science Translational Medicine 2021. 



FIGURE B1Lower error properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of the Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets from a population with 0.5% Prevalence where 50 groups of 200 people are sampled. The horizontal dashed line indicates the nominal lower error rate, 2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines.FIGURE B2Lower error properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of the Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets from a population with 0.5% Prevalence where 8000 individuals are sampled. The horizontal dashed line indicates the nominal lower error rate, 2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines.FIGURE B14Lower error properties for the confidence interval procedures, WprevSeSp Binomial and WprevSeSp Poisson. Each point represents 10,000 simulations of datasets from a population with 0.5% Prevalence where 8000 individuals are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. The horizontal dashed line indicates the nominal lower error rate, 2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines.Each Facet = 95% Sensitivity, Each Point = 10,000 Replications Lower Error Frequency Properties for Simulations with 5.0% Prevalence Among 8,000 Groups of 1 FIGURE B15 Lower error properties for the confidence interval procedures, WprevSeSp Binomial and WprevSeSp Poisson. Each point represents 10,000 simulations of datasets from a population with 5% Prevalence where 50 groups of 200 people are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. The horizontal dashed line indicates the nominal lower error rate, 2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines. Each Facet = 95% Sensitivity, Each Point = 10,000 Replications Lower Error Frequency Properties for Simulations with 0.5% Prevalence Among 8,000 Groups of 1 FIGURE B16 Lower error properties for the confidence interval procedures, WprevSeSp Binomial and WprevSeSp Poisson. Each point represents 10,000 simulations of datasets from a population with 5% Prevalence where 8000 individuals are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. The horizontal dashed line indicates the nominal lower error rate, 2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines. Each Facet = 95% Sensitivity, Each Point = 10,000 Replications Upper Error Frequency Properties for Simulations with 0.5% Prevalence Among 50 Groups of 200 FIGURE B17 Upper error properties for the confidence interval procedures, WprevSeSp Binomial and WprevSeSp Poisson. Each point represents 10,000 simulations of datasets from a population with 0.5% Prevalence where 50 groups of 200 people are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. The horizontal dashed line indicates the nominal upper error rate, 2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines. Each Facet = 95% Sensitivity, Each Point = 10,000 Replications Upper Error Frequency Properties for Simulations with 0.5% Prevalence Among 8,000 Groups of 1 FIGURE B18 Upper error properties for the confidence interval procedures, WprevSeSp Binomial and WprevSeSp Poisson. Each point represents 10,000 simulations of datasets from a population with 0.5% Prevalence where 8000 individuals are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. The horizontal dashed line indicates the nominal upper error rate, 2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines. Each Facet = 95% Sensitivity, Each Point = 10,000 Replications Upper Error Frequency Properties for Simulations with 5.0% Prevalence Among 50 Groups of 200 FIGURE B19 Upper error properties for the confidence interval procedures, WprevSeSp Binomial and WprevSeSp Poisson. Each point represents 10,000 simulations of datasets from a population with 5% Prevalence where 50 groups of 200 people are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. The horizontal dashed line indicates the nominal upper error rate, 2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines. Each Facet = 95% Sensitivity, Each Point = 10,000 Replications Upper Error Frequency Properties for Simulations with 5.0% Prevalence Among 8,000 Groups of 1 FIGURE B20 Upper error properties for the confidence interval procedures, WprevSeSp Binomial and WprevSeSp Poisson. Each point represents 10,000 simulations of datasets from a population with 5% Prevalence where 8000 individuals are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. The horizontal dashed line indicates the nominal upper error rate, 2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines. Each Facet = 95% Sensitivity, Each Point = 10,000 Replications Confidence Interval Width Properties for Simulations with 0.5% Prevalence Among 50 Groups of 200 FIGURE B21 Confidence interval width properties for the confidence interval procedures, WprevSeSp Binomial and WprevS-eSp Poisson. Each point represents 10,000 simulations of datasets from a population with 0.5% Prevalence where 50 groups of 200 people are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. Colored dashed lines are estimates from a logistic regression model using quadratic splines. Each Facet = 95% Sensitivity, Each Point = 10,000 Replications Confidence Interval Width Properties for Simulations with 5.0% Prevalence Among 50 Groups of 200 FIGURE B22 Confidence interval width properties for the confidence interval procedures, WprevSeSp Binomial and WprevS-eSp Poisson. Each point represents 10,000 simulations of datasets from a population with 5% Prevalence where 50 groups of 200 people are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. Colored dashed lines are estimates from a logistic regression model using quadratic splines. Each Facet = 95% Sensitivity, Each Point = 10,000 Replications Confidence Interval Width Properties for Simulations with 0.5% Prevalence Among 8,000 Groups of 1FIGURE B23Confidence interval width properties for the confidence interval procedures, WprevSeSp Binomial and Wpre-vSeSp Poisson. Each point represents 10,000 simulations of datasets from a population with 0.5% Prevalence where 8000 individuals are sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed on 60 and 300 individuals, respectively. Colored dashed lines are estimates from a logistic regression model using quadratic splines.Each Facet = 95% Sensitivity, Each Point = 10,000 Replications Confidence Interval Width Properties for Simulations with 5.0% Prevalence Among 8,000 Groups of 1Weights with Non−0 Prev. 
5% 

Weights with Non−0 Prev. 
25% 

Weights with Non−0 Prev. 
75% 

Weights with Prev. 
High Weights 

Weights with Prev. 
Uniform Weights 

Weights with Prev. 
Low Weights 

0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 

1.0% 

2.0% 

3.0% 

0.0% 

1.0% 

2.0% 

3.0% 

0.0% 

1.0% 

2.0% 

3.0% 

Weight Coefficient of Variation 

Lower Error Frequency 

Method 

Agresti−Coull 
Korn−Graubard 
wsPoisson 

Each Point = 10,000 Replications 

Lower Error Frequency Properties for Simulations with 0.5% Prevalence Among 50 Groups of 200 

Weights with Non−0 Prev. 
5% 

Weights with Non−0 Prev. 
25% 

Weights with Non−0 Prev. 
75% 

Weights with Prev. 
High Weights 

Weights with Prev. 
Uniform Weights 

Weights with Prev. 
Low Weights 

0% 
200% 
400% 
0% 
200% 
400% 
0% 
200% 
400% 

1.0% 

2.0% 

3.0% 

1.0% 

2.0% 

3.0% 

0.0% 

1.0% 

2.0% 

3.0% 

Weight Coefficient of Variation 

Lower Error Frequency 

Method 

Agresti−Coull 
Korn−Graubard 
wsPoisson 

Each Point = 10,000 Replications 

Lower Error Frequency Properties for Simulations with 0.5% Prevalence Among 8,000 Groups of 1 

Weights with Non−0 Prev. 
5% 

Weights with Non−0 Prev. 
25% 

Weights with Non−0 Prev. 
75% 

Weights with Prev. 
High Weights 

Weights with Prev. 
Uniform Weights 

Weights with Prev. 
Low Weights 

0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 

0.0% 

1.0% 

2.0% 

3.0% 

4.0% 

0.0% 

1.0% 

2.0% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

Weight Coefficient of Variation 

Lower Error Frequency 

Method 

Agresti−Coull 
Korn−Graubard 
wsPoisson 

Each Point = 10,000 Replications 

Lower Error Frequency Properties for Simulations with 5.0% Prevalence Among 50 Groups of 200 

FIGURE B3 Lower error properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of the 
Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets from a population 
with 5% Prevalence where 50 groups of 200 people are sampled. The horizontal dashed line indicates the nominal lower error 
rate, 2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines. 
Weights with Non−0 Prev. 
5% 

Weights with Non−0 Prev. 
25% 

Weights with Non−0 Prev. 
75% 

Weights with Prev. 
High Weights 

Weights with Prev. 
Uniform Weights 

Weights with Prev. 
Low Weights 

0% 
200% 
400% 
0% 
200% 
400% 
0% 
200% 
400% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

Weight Coefficient of Variation 

Lower Error Frequency 

Method 

Agresti−Coull 
Korn−Graubard 
wsPoisson 

Each Point = 10,000 Replications 

Lower Error Frequency Properties for Simulations with 5.0% Prevalence Among 8,000 Groups of 1 

FIGURE B4 Lower error properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of the 
Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets from a population 
with 5% Prevalence where 8000 individuals are sampled. The horizontal dashed line indicates the nominal lower error rate, 
2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines. 
Weights with Non−0 Prev. 
5% 

Weights with Non−0 Prev. 
25% 

Weights with Non−0 Prev. 
75% 

Weights with Prev. 
High Weights 

Weights with Prev. 
Uniform Weights 

Weights with Prev. 
Low Weights 

0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 

0.0% 

10.0% 

20.0% 

30.0% 

40.0% 

0.0% 

5.0% 

10.0% 

0.0% 

1.0% 

2.0% 

3.0% 

Weight Coefficient of Variation 

Upper Error Frequency 

Method 

Agresti−Coull 
Korn−Graubard 
wsPoisson 

Each Point = 10,000 Replications 

Upper Error Frequency Properties for Simulations with 0.5% Prevalence Among 50 Groups of 200 

FIGURE B5 Upper error properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of the 
Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets from a population 
with 0.5% Prevalence where 50 groups of 200 people are sampled. The horizontal dashed line indicates the nominal upper error 
rate, 2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines. 
Weights with Non−0 Prev. 
5% 

Weights with Non−0 Prev. 
25% 

Weights with Non−0 Prev. 
75% 

Weights with Prev. 
High Weights 

Weights with Prev. 
Uniform Weights 

Weights with Prev. 
Low Weights 

0% 
200% 
400% 
0% 
200% 
400% 
0% 
200% 
400% 

0.0% 

5.0% 

10.0% 

15.0% 

20.0% 

0.0% 

5.0% 

10.0% 

15.0% 

20.0% 

0.0% 

1.0% 

2.0% 

Weight Coefficient of Variation 

Upper Error Frequency 

Method 

Agresti−Coull 
Korn−Graubard 
wsPoisson 

Each Point = 10,000 Replications 

Upper Error Frequency Properties for Simulations with 0.5% Prevalence Among 8,000 Groups of 1 

FIGURE B6 Upper error properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of the 
Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets from a population 
with 0.5% Prevalence where 8000 individuals are sampled. The horizontal dashed line indicates the nominal upper error rate, 
2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines. 
Weights with Non−0 Prev. 
5% 

Weights with Non−0 Prev. 
25% 

Weights with Non−0 Prev. 
75% 

Weights with Prev. 
High Weights 

Weights with Prev. 
Uniform Weights 

Weights with Prev. 
Low Weights 

0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 

0.0% 

1.0% 

2.0% 

3.0% 

0.0% 

1.0% 

2.0% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

Weight Coefficient of Variation 

Upper Error Frequency 

Method 

Agresti−Coull 
Korn−Graubard 
wsPoisson 

Each Point = 10,000 Replications 

Upper Error Frequency Properties for Simulations with 5.0% Prevalence Among 50 Groups of 200 

FIGURE B7 Upper error properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of the 
Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets from a population 
with 5% Prevalence where 50 groups of 200 people are sampled. The horizontal dashed line indicates the nominal upper error 
rate, 2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines. 
Weights with Non−0 Prev. 
5% 

Weights with Non−0 Prev. 
25% 

Weights with Non−0 Prev. 
75% 

Weights with Prev. 
High Weights 

Weights with Prev. 
Uniform Weights 

Weights with Prev. 
Low Weights 

0% 
200% 
400% 
0% 
200% 
400% 
0% 
200% 
400% 

0.0% 

2.0% 

4.0% 

6.0% 

0.0% 

2.0% 

4.0% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

Weight Coefficient of Variation 

Upper Error Frequency 

Method 

Agresti−Coull 
Korn−Graubard 
wsPoisson 

Each Point = 10,000 Replications 

Upper Error Frequency Properties for Simulations with 5.0% Prevalence Among 8,000 Groups of 1 

FIGURE B8 Upper error properties for the wsPoisson model and two standard methods, the Dean-Pagano modification of the 
Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets from a population 
with 5% Prevalence where 8000 individuals are sampled. The horizontal dashed line indicates the nominal upper error rate, 
2.5%. Colored dashed lines are estimates from a logistic regression model using quadratic splines. 
Weights with Non−0 Prev. 
5% 

Weights with Non−0 Prev. 
25% 

Weights with Non−0 Prev. 
75% 

Weights with Prev. 
High Weights 

Weights with Prev. 
Uniform Weights 

Weights with Prev. 
Low Weights 

0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 

1.0% 

2.0% 

0.50% 

1.00% 

1.50% 

0.00% 

0.30% 

0.60% 

0.90% 

Weight Coefficient of Variation 

Confidence Interval Width 

Method 

Agresti−Coull 
Korn−Graubard 
wsPoisson 

Each Point = 10,000 Replications 

Confidence Interval Width Properties for Simulations with 0.5% Prevalence Among 50 Groups of 200 

FIGURE B9 Confidence interval width properties for the wsPoisson model and two standard methods, the Dean-Pagano mod-
ification of the Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets 
from a population with 0.5% Prevalence where 50 groups of 200 people are sampled. Colored dashed lines are estimates from 
a logistic regression model using quadratic splines. 
Weights with Non−0 Prev. 
5% 

Weights with Non−0 Prev. 
25% 

Weights with Non−0 Prev. 
75% 

Weights with Prev. 
High Weights 

Weights with Prev. 
Uniform Weights 

Weights with Prev. 
Low Weights 

0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 

2.0% 

4.0% 

6.0% 

1.0% 

2.0% 

3.0% 

4.0% 

0.40% 

0.60% 

0.80% 

1.00% 

Weight Coefficient of Variation 

Confidence Interval Width 

Method 

Agresti−Coull 
Korn−Graubard 
wsPoisson 

Each Point = 10,000 Replications 

Confidence Interval Width Properties for Simulations with 5.0% Prevalence Among 50 Groups of 200 

FIGURE B10 Confidence interval width properties for the wsPoisson model and two standard methods, the Dean-Pagano mod-
ification of the Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets 
from a population with 5% Prevalence where 50 groups of 200 people are sampled. Colored dashed lines are estimates from a 
logistic regression model using quadratic splines. 
Weights with Non−0 Prev. 
5% 

Weights with Non−0 Prev. 
25% 

Weights with Non−0 Prev. 
75% 

Weights with Prev. 
High Weights 

Weights with Prev. 
Uniform Weights 

Weights with Prev. 
Low Weights 

0% 
200% 
400% 
0% 
200% 
400% 
0% 
200% 
400% 

0.0% 

3.0% 

6.0% 

9.0% 

0.0% 

3.0% 

6.0% 

9.0% 

0.0% 

1.0% 

2.0% 

3.0% 

4.0% 

5.0% 

Weight Coefficient of Variation 

Confidence Interval Width 

Method 

Agresti−Coull 
Korn−Graubard 
wsPoisson 

Each Point = 10,000 Replications 

Confidence Interval Width Properties for Simulations with 0.5% Prevalence Among 8,000 Groups of 1 

FIGURE B11 Confidence interval width properties for the wsPoisson model and two standard methods, the Dean-Pagano mod-
ification of the Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets 
from a population with 0.5% Prevalence where 8000 individuals are sampled. Colored dashed lines are estimates from a logistic 
regression model using quadratic splines. 
Weights with Non−0 Prev. 
5% 

Weights with Non−0 Prev. 
25% 

Weights with Non−0 Prev. 
75% 

Weights with Prev. 
High Weights 

Weights with Prev. 
Uniform Weights 

Weights with Prev. 
Low Weights 

0% 
200% 
400% 
0% 
200% 
400% 
0% 
200% 
400% 

2.5% 

5.0% 

7.5% 

10.0% 

12.5% 

2.5% 

5.0% 

7.5% 

10.0% 

1.0% 

2.0% 

Weight Coefficient of Variation 

Confidence Interval Width 

Method 

Agresti−Coull 
Korn−Graubard 
wsPoisson 

Each Point = 10,000 Replications 

Confidence Interval Width Properties for Simulations with 5.0% Prevalence Among 8,000 Groups of 1 

FIGURE B12 Confidence interval width properties for the wsPoisson model and two standard methods, the Dean-Pagano mod-
ification of the Agresti-Coull method and of the Korn-Graubard method. Each point represents 10,000 simulations of datasets 
from a population with 5% Prevalence where 8000 individuals are sampled. Colored dashed lines are estimates from a logistic 
regression model using quadratic splines. 
Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

Weight Coefficient of Variation 

Lower Error Frequency 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 

Each Facet = 95% Sensitivity, Each Point = 10,000 Replications 

Lower Error Frequency Properties for Simulations with 5.0% Prevalence Among 50 Groups of 200 

FIGURE B13 Lower error properties for the confidence interval procedures, WprevSeSp Binomial and WprevSeSp Poisson. 
Each point represents 10,000 simulations of datasets from a population with 0.5% Prevalence where 50 groups of 200 people are 
sampled. Each datasets also includes simulated results of tests to evaluate the sensitivity and specificity of the assay performed 
on 60 and 300 individuals, respectively. The horizontal dashed line indicates the nominal lower error rate, 2.5%. Colored dashed 
lines are estimates from a logistic regression model using quadratic splines. If the WprevSeSp Binomial line is not visible, then 
it is covered by the WprevSeSp Poisson line. 
Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

Weight Coefficient of Variation 

Lower Error Frequency 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 

Each Facet = 95% Sensitivity, Each Point = 10,000 Replications 

Lower Error Frequency Properties for Simulations with 0.5% Prevalence Among 50 Groups of 200 

Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
0% 
200% 
400% 
0% 
200% 
400% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

Weight Coefficient of Variation 

Lower Error Frequency 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 

Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
0% 
200% 
400% 
0% 
200% 
400% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

Weight Coefficient of Variation 

Lower Error Frequency 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 

Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 

0.0% 

10.0% 

20.0% 

30.0% 

40.0% 

0.0% 

5.0% 

10.0% 

15.0% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

Weight Coefficient of Variation 

Upper Error Frequency 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 

Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
0% 
200% 
400% 
0% 
200% 
400% 

0.0% 

5.0% 

10.0% 

15.0% 

20.0% 

0.0% 

5.0% 

10.0% 

15.0% 

20.0% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

Weight Coefficient of Variation 

Upper Error Frequency 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 

Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 

1.00% 

1.50% 

2.00% 

2.50% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

Weight Coefficient of Variation 

Upper Error Frequency 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 

Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
0% 
200% 
400% 
0% 
200% 
400% 

0.0% 

2.0% 

4.0% 

6.0% 

0.0% 

1.0% 

2.0% 

3.0% 

4.0% 

5.0% 

0.00% 

0.50% 

1.00% 

1.50% 

2.00% 

2.50% 

Weight Coefficient of Variation 

Upper Error Frequency 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 

Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 

3.0% 

6.0% 

9.0% 

12.0% 

3.0% 

6.0% 

9.0% 

12.0% 

2.5% 

5.0% 

7.5% 

Weight Coefficient of Variation 

Confidence Interval Width 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 

Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 
0% 
200% 
400% 
600% 

4.0% 

8.0% 

12.0% 

16.0% 

4.0% 

8.0% 

12.0% 

16.0% 

3.0% 

6.0% 

9.0% 

12.0% 

Weight Coefficient of Variation 

Confidence Interval Width 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 

Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
0% 
200% 
400% 
0% 
200% 
400% 

0.0% 

5.0% 

10.0% 

15.0% 

0.0% 

5.0% 

10.0% 

15.0% 

3.0% 

6.0% 

9.0% 

Weight Coefficient of Variation 

Confidence Interval Width 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 

Specificity 
80% 

Specificity 
90% 

Specificity 
100% 

Groups with Prev. 
High Weights 

Groups with Prev. 
Uniform Weights 

Groups with Prev. 
Low Weights 

0% 
200% 
400% 
0% 
200% 
400% 
0% 
200% 
400% 

5.0% 

10.0% 

15.0% 

20.0% 

5.0% 

10.0% 

15.0% 

20.0% 

2.5% 

5.0% 

7.5% 

10.0% 

12.5% 

Weight Coefficient of Variation 

Confidence Interval Width 

Method 

WprevSeSp Binomial 
WprevSeSp Poisson 



The Myth of Millions of Annual Self-Defense Gun Uses: A Case Study of Survey Overestimates of Rare Events. D Hemenway, 10.1080/09332480.1997.10542033CHANCE. 103Hemenway D. The Myth of Millions of Annual Self-Defense Gun Uses: A Case Study of Survey Overestimates of Rare Events. CHANCE 1997; 10(3): 6-10. doi: 10.1080/09332480.1997.10542033

Evaluating confidence interval methods for binomial proportions in clustered surveys. N Dean, M Pagano, Journal of Survey Statistics and Methodology. 34Dean N, Pagano M. Evaluating confidence interval methods for binomial proportions in clustered surveys. Journal of Survey Statistics and Methodology 2015; 3(4): 484-503.

Comparative study of confidence intervals for proportions in complex sample surveys. C Franco, R J Little, T A Louis, E V Slud, Journal of survey statistics and methodology. 73Franco C, Little RJ, Louis TA, Slud EV. Comparative study of confidence intervals for proportions in complex sample surveys. Journal of survey statistics and methodology 2019; 7(3): 334-364.

Estimating SARS-CoV-2 Seroprevalence. S Rosin, B E Shook-Sa, S R Cole, M G Hudgens, Rosin S, Shook-Sa BE, Cole SR, Hudgens MG. Estimating SARS-CoV-2 Seroprevalence.; 2021.

Bayesian analysis of tests with unknown specificity and sensitivity. A Gelman, B Carpenter, 10.1111/rssc.12435Journal of the Royal Statistical Society: Series C (Applied Statistics). 695Gelman A, Carpenter B. Bayesian analysis of tests with unknown specificity and sensitivity. Journal of the Royal Statistical Society: Series C (Applied Statistics) 2020; 69(5): 1269-1283. doi: https://doi.org/10.1111/rssc.12435

Combining one-sample confidence procedures for inference in the two-sample case. M P Fay, M A Proschan, E Brittain, Biometrics. 711Fay MP, Proschan MA, Brittain E. Combining one-sample confidence procedures for inference in the two-sample case. Biometrics 2015; 71(1): 146-156.

Confidence Distribution, the Frequentist Distribution Estimator of a Parameter: A Review. Xie Mg, K Singh, 10.1111/insr.12000International Statistical Review. 811Xie Mg, Singh K. Confidence Distribution, the Frequentist Distribution Estimator of a Parameter: A Review. International Statistical Review 2013; 81(1): 3-39. doi: https://doi.org/10.1111/insr.12000

Interpreting P-values and Confidence Intervals using Well-Calibrated Null Preference Priors. M P Fay, M A Proschan, E Brittain, R Tiwari, Statistical Science: to appearFay MP, Proschan MA, Brittain E, Tiwari R. Interpreting P-values and Confidence Intervals using Well-Calibrated Null Preference Priors. Statistical Science: to appear.

The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial. C J Clopper, E S Pearson, 10.1093/biomet/26.4.404Biometrika. 264Clopper CJ, Pearson ES. The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial. Biometrika 1934; 26(4): 404-413. doi: 10.1093/biomet/26.4.404

. M P Fay, asht: Applied Statistical Hypothesis Tests. 2020. R package version 0.9.6Fay MP. asht: Applied Statistical Hypothesis Tests. ; : 2020. R package version 0.9.6.

Confidence intervals for directly standardized rates: a method based on the gamma distribution. M P Fay, E J Feuer, Statistics in medicine. 167Fay MP, Feuer EJ. Confidence intervals for directly standardized rates: a method based on the gamma distribution. Statistics in medicine 1997; 16(7): 791-801.

Approximate is Better than "Exact" for Interval Estimation of Binomial Proportions. A Agresti, B A Coull, 10.1080/00031305.1998.10480550The American Statistician. 522Agresti A, Coull BA. Approximate is Better than "Exact" for Interval Estimation of Binomial Proportions. The American Statistician 1998; 52(2): 119-126. doi: 10.1080/00031305.1998.10480550

Confidence intervals for proportions with small expected number of positive counts estimated from survey data. E L Korn, B I Graubard, Survey Methodology. 24Korn EL, Graubard BI. Confidence intervals for proportions with small expected number of positive counts estimated from survey data. Survey Methodology 1998; 24: 193-201.

Analysis of health surveys. E L Korn, B I Graubard, John Wiley & SonsKorn EL, Graubard BI. Analysis of health surveys. John Wiley & Sons . 1999.

The multinomial-Poisson transformation. S G Baker, Journal of the Royal Statistical Society: Series D (The Statistician). 434Baker SG. The multinomial-Poisson transformation. Journal of the Royal Statistical Society: Series D (The Statistician) 1994; 43(4): 495-504.

Confidence intervals for directly standardized rates using mid-p gamma intervals. M P Fay, S Kim, Biometrical Journal. 592Fay MP, Kim S. Confidence intervals for directly standardized rates using mid-p gamma intervals. Biometrical Journal 2017; 59(2): 377-387.

Estimating prevalence from the results of a screening test. W J Rogan, B Gladen, American journal of epidemiology. 1071Rogan WJ, Gladen B. Estimating prevalence from the results of a screening test. American journal of epidemiology 1978; 107(1): 71-76.

Surveys to measure programme coverage and impact: a review of the methodology used by the expanded programme on immunization. S Lemeshow, D Robinson, 38World health statistics quarterly. Rapport trimestriel de statistiques sanitaires mondialesLemeshow S, Robinson D. Surveys to measure programme coverage and impact: a review of the methodology used by the expanded programme on immunization.. World health statistics quarterly. Rapport trimestriel de statistiques sanitaires mondiales 1985; 38(1): 65-75.

The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial. C J Clopper, E S Pearson, 10.1093/biomet/26.4.404Biometrika. 264Clopper CJ, Pearson ES. The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial. Biometrika 1934; 26(4): 404-413. doi: 10.1093/biomet/26.4.404

Measurement error and misclassification in statistics and epidemiology: impacts and Bayesian adjustments. P Gustafson, CRC PressGustafson P. Measurement error and misclassification in statistics and epidemiology: impacts and Bayesian adjustments. CRC Press . 2003.

Comparison of Bayesian and frequentist methods for prevalence estimation under misclassification. M Flor, M Weiß, T Selhorst, C Müller-Graf, M Greiner, 10.1186/s12889-020-09177-4BMC Public Health. 2011135Flor M, Weiß M, Selhorst T, Müller-Graf C, Greiner M. Comparison of Bayesian and frequentist methods for prevalence estimation under misclassification. BMC Public Health 2020; 20(1): 1135. doi: 10.1186/s12889-020-09177-4