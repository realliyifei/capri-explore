# Self-supervised Learning in Remote Sensing: A Review

CorpusID: 250072803
 
tags: #Environmental_Science, #Computer_Science

URL: [https://www.semanticscholar.org/paper/cfd94ae8dd48c695cd0d0d63cd67573bd5310f87](https://www.semanticscholar.org/paper/cfd94ae8dd48c695cd0d0d63cd67573bd5310f87)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Self-supervised Learning in Remote Sensing: A Review


Student Member, IEEEYi Wang 
Member, IEEEConrad M Albrecht 
Nassim Ait 
Ali Braham 
Lichao Mou 
Fellow, IEEEXiao Xiang Zhu 
Self-supervised Learning in Remote Sensing: A Review
Index Terms-Deep learningself-supervised learningcom- puter visionremote sensingEarth observation
SSL SSL in RSA GALLERY OF REPRESENTATIVE SELF-SUPERVISED METHODS. Category Sub-category Representative methods Generative Autoencoder Autoencoder (AE) [54]: encode and reconstruct the input. Sparse AE [55]: sparsity constraints on the hidden units. Denoising AE [56]: reconstruct clear image from noisy input. VAE [57]: encode the input to a normal distribution. MAE [58]: reconstruct randomly masked patches with vision transformer. GAN GAN [59]: adversarial training with a generator and a discriminator. AAE [60]: adversarial variational autoencoder. BiGAN [61]: an additional encoder to map samples to representations. Predictive Spatial Relative position [62]: predict relative position of random patch pairs. Jigsaw [63]: predict the correct order of a jigsaw puzzle. Rotation [49]: predict the rotation angle of rotated images. Inpainting [64]: recover a missing patch of the input image. Spectral Colorization [65]: predict colorful image from gray-scale input. Temporal Frame order [66]: predict the order of frame sequences. Others Counting [67]: count the visual primitives within the input image. Artifact [68]: spot and predict artifacts. Audio [69]: predict a statistical summary of the sound associated with a video. Contrastive Negative sampling Triplet loss [70]: learn a similarity metric discriminatively with triplet loss. CPC [71]: contrastive predictive coding with InfoNCE loss. DIM [72]: maximize mututal information between global-local feature pairs. InstDisc [73]: maximally scatter the features of different images over a unit sphere. PIRL [74]: pretext tasks as data augmentation. MoCo [75]: store negative samples in a queue and momentum-update the key encoder. SimCLR [76]: end-to-end contrastive structure with large batchsize and strong data augmentation. MoCo-v3 [77]: introduce vision transformer as the encoder backbone for MoCo-v2 [78]. Clustering DeepCluster [51]: iteratively leverage k-means clustering to yield pseudo labels for prediction. LocalAgg [79]: optimize a local soft-clustering metric. SeLa [80]: solve an optimal transport problem to obtain the pseudo-labels. SwAV [81]: contrastive learning with online clustering. PCL [82]: perform iterative clustering and representation learning in an EM-based framework. Knowledge distillation BYOL [83]: mean-teacher network with a predictor on top of the teacher encoder. SimSiam [84]: explore the simplest design of contrastive self-supervised learning. DINO [52]: explores the self-distillation scheme with vision transformer backbones. EsViT [85]: improve DINO with additional region-level contrastive task. iBOT [86]: cross-view and in-view self-distillation with masked image modeling. Redundancy reduction Barlow Twins [87]: redundancy reduction on the cross-correlation matrix between features. VICReg [88]: variance-invariance-covariance regularization.

Abstract-In deep learning research, self-supervised learning (SSL) has received great attention triggering interest within both the computer vision and remote sensing communities. While there has been a big success in computer vision, most of the potential of SSL in the domain of earth observation remains locked. In this paper, we provide an introduction to, and a review of the concepts and latest developments in SSL for computer vision in the context of remote sensing. Further, we provide a preliminary benchmark of modern SSL algorithms on popular remote sensing datasets, verifying the potential of SSL in remote sensing and providing an extended study on data augmentations. Finally, we identify a list of promising directions of future research in SSL for earth observation (SSL4EO) to pave the way for fruitful interaction of both domains.

Index Terms-Deep learning, self-supervised learning, computer vision, remote sensing, Earth observation.


## I. INTRODUCTION

A DVANCES of deep neural networks to model the rich structure of large amounts of data has led to major breakthroughs in computer vision, natural language processing, automatic speech recognition, and time series analysis [1]. However, the performance of deep neural networks is very sensitive to the size and quality of the training data. Thus, a plurality of annotated datasets (e.g. ImageNet [2]) have been generated for supervised training in the last decade, driving progress in many fields. Unfortunately, annotating large-scale datasets is an extremely laborious, time-consuming, and expensive procedure. This limitation of the supervised learning paradigm strongly impedes the applicability of deep learning in real-world scenarios.

A lot of research efforts have been deployed to tackle the challenge of data annotation in the machine learning literature.

The work is jointly supported by the Helmholtz Association through the Framework of Helmholtz AI (grant number: ZT-I-PF-5-01) -Local Unit "Munich Unit @Aeronautics, Space and Transport (MASTr)" and Helmholtz Excellent Professorship "Data Science in Earth Observation -Big Data Fusion for Urban Research"(grant number: W2-W3-100), by the German Federal Ministry of Education and Research (BMBF) in the framework of the international future AI lab "AI4EO -Artificial Intelligence for Earth Observation: Reasoning, Uncertainties, Ethics and Beyond" (grant number: 01DD20001) and by German Federal Ministry for Economic Affairs and Climate Action in the framework of the "national center of excellence ML4Earth" (grant number: 50EE2201C). In fact, numerous alternatives to vanilla supervised learning have been studied, such as unsupervised learning [3], semisupervised learning [4], weakly supervised learning [5] and meta-learning [6]. Recently, self-supervised learning (SSL) did raise considerable attention in computer vision (see Fig.  1) and achieved significant milestones towards the reduction of human supervision. Indeed, by distilling representative features from unlabeled data, SSL algorithms are already outperforming supervised pre-training on many problems [7].

Meanwhile, the success of deep learning did also spark remarkable progress in remote sensing [8]. Yet, data annotation remains a major challenge for earth observation as well. In that regard, self-supervised learning is a promising avenue of research for remote sensing. While there exist comprehensive surveys of SSL in computer vision [9,10,11,12], a didactic and up-to-date introduction to the remote sensing community is missing. This review aims at bridging this gap, documenting progress in self-supervised visual representation learning, and how it did get applied in remote sensing to date. Specifically:

• To the remote sensing community, we provide a comprehensive introduction and literature review on selfsupervised visual representation learning. • We summarize a taxonomy of self-supervised methods covering existing works from both computer vision and remote sensing communities. • We provide quantification of performance benchmarks on established SSL methodologies utilizing the ImageNet dataset [2], and extend the analysis to three multispectral satellite imagery datasets: BigEarthNet [13], SEN12MS [14], and So2Sat-LCZ42 [15]. • We discuss the link of natural imagery and remotely sensed data, providing insights for future work in selfsupervised learning for remote sensing and earth observation.


## A. Supervision: Data vs. Labels

A significant fraction of real-world applications of machine/ deep learning methodologies calls for massive amounts of training data. Fortunately, the increasing adoption of open access in earth observation provides remote sensing researchers with a plethora of such. However, annotation of such vast amounts of earth observation data implies significant human interaction with frequent updates needed every single day. For example, apart from the large amounts of sampled data to label, a single large-scale dataset may require separate annotation depending on downstream applications. Moreover, Fig. 1. The number of recent publications related to self-supervised learning (SSL). While a clear trend of increased efforts to advance SSL is observed, activity in remote sensing lags behind. datasets sensing the physical world get outdated over time due to both natural and man-made changes. Both aspects significantly amplify the efforts in annotating remotely sensed data of the Earth.

Another big challenge of machine learning in remote sensing is label noise. Due to the quality of remotely sensed images and the complexity of various specific applications, it is very difficult to generate perfect labels during large-scale data annotation. Therefore, there always exists a trade-off between the number of annotations and their quality: large but noisy labeled datasets can bias the model, whereas small amounts of good-quality labels usually lead to overfitting.

In addition, it has become popular to execute pre-training on established benchmark datasets before fine-tuning deeply learned models for downstream tasks that do not have enough labels. This procedure is commonly referred to as transfer learning [16]. However, the performance of supervised pretraining depends on the domain difference between source and target data. A good pre-trained model renders well on a similar dataset but is not as useful on a very different one.

All the above challenges emphasize the growing gap between an increasing amount of remote sensing data and the shortage of good-quality labels, calling for techniques to exploit the corpus of unlabeled data to learn valuable information that easily transfers to multiple applications. Self-supervised learning offers a paradigm to approach this dilemma. B. Self-supervision: Learning Task-agnostic Representations of Data Fig. 2 schematically depicts the general underlying principle of self-supervised learning. Based on a certain self-produced objective (the so-called self-supervision), a large amount of unlabeled data is exploited to train a model f 1 by optimizing this objective without requiring any manual annotation. With a carefully designed self-supervised problem, the model f gets the ability to capture high-level representations of the input data. Afterward, the model f can be further transferred to supervised downstream tasks for real-world applications. The most common strategies to design such self-supervision typically exploit three types of objectives: (1) reconstructing input data x, f (x) → x, (2) predicting a self-produced label c which usually comes from contextual information and data augmentation (e.g., predicting the rotation angle of a rotated image), f (x) → c, (3) contrasting semantically similar inputs x 1 and x 2 (e.g., the encoded features of two augmented views of the same image should be identical), |f (x 1 ) − f (x 2 )| → 0.

Given self-supervised training succeeded, the pre-trained model f may get transferred to downstream tasks. As Opposed to supervised pre-training, models pre-trained by selfsupervision bear the potential to leverage more general representations and offer a paradigm to mitigate the shortcomings of supervised learning: (1) no human annotation is needed for pre-training, (2) small amounts of labels are sufficient for good performance on downstream tasks, (3) little domain gap between pre-training and downstream dataset can be ensured by collecting unlabeled data from the target application.

Link to semi-supervised learning. Self-supervised learning algorithms may be considered as part of semi-supervised learning-a branch of machine learning concerned with labeled and unlabelled data. Van Engelen et al. [4] proposed a comprehensive taxonomy of semi-supervised classification algorithms based on conceptual and methodological aspects for processing unlabelled data. Within that scheme, selfsupervised learning may get categorized as unsupervised preprocessing: in the first step, unlabeled data gets transformed to extract feature representations; labeled data is then utilized to adapt the model to specific tasks.

Within the remote sensing community, semi-supervised learning has been long studied and enjoys applications in, e.g., hyperspectral image recognition and processing [19,20,21,22,23,24,25,26,27,28], multi-spectral image segmentation [29,30,31,32,33,34,35] and SAR-optical data fusion [36].

Link to unsupervised learning. Self-supervised learning is also a discipline in the realm of unsupervised learningthe general set of machine learning methods that are independent of human annotation. However, it is very ambiguous in various communities the difference between self-supervised and unsupervised learning. In this paper, we separate them in terms of methodology. Traditional unsupervised algorithms tend to utilize statistics of the input data and generate groups by dimensionality reduction [37,38] or clustering [39]. On the other hand, self-supervised learning is a recent terminology that refers to approaches in which a model (e.g., a neural network) is trained to learn good data representations using supervision signals that are automatically generated from the data itself.

In remote sensing, unsupervised learning has also been actively employed in various applications, e.g., in scene classification [40,38,41], semantic segmentation [42,43,44], change detection [45,46] and multi-sensor data analysis [47,48].


## C. Performance Evaluation in Self-Supervised Learning

To evaluate the performance of self-supervised learning methods, downstream tasks are commonly defined in order to transfer the pre-trained model f to specific applications, such as scene classification, semantic segmentation, and object detection. The performance of transfer learning on these highlevel vision tasks estimates the generalizability of the model f . In practice, three common procedures are used for quantitative evaluation:

• Linear probing (or linear classification): refers to fixing the parameters of the learned model f (frozen encoder), and training a linear classifier g on top of the generated representations. This approach measures how linearly separable the embeddings produced by the pre-trained model f are. • K nearest neighbors (KNN): refers to applying weighted voting of the K nearest neighbors of the input test image x in the feature space. This approach is nonparametric and can be used for classification tasks. • Fine-tuning: refers to training a model on the downstream task by using the parameters of the pre-trained model f as an initialization. It is the most general procedure since it is not only limited to classification. In addition to quantitative evaluation, qualitative visualization methods can also be used to derive insights on the features learnt during the self-supervised training of the encoder f . Among these, two popular techniques are kernel visualization [49,50,51] for CNN and feature map visualization [49,52,53] for both CNN and ViT. Kernel visualization plots kernels of the first convolutional layer of f and compares it with kernels from corresponding fully supervised training. The similarity of kernels learned by supervised and self-supervised training sheds light on the efficacy of self-supervision. Along the lines, feature map visualization displays hidden layer feature maps of f in order to analyze the spatial attention of  f on the input x. Once again, the feature maps are compared with their counterparts of supervised training for reference.


## II. GENERATIVE, PREDICTIVE, AND CONTRASTIVE: A TAXONOMY OF SELF-SUPERVISED LEARNING

In this section, we review works in self-supervised learning following an extended taxonomy: generative, contrastive, and predictive methods, as displayed in Fig. 3. Generative methods learn to reconstruct or generate input data, predictive methods learn to predict self-generated labels, and contrastive methods learn to maximize the similarity between semantically identical inputs. While some existing categorizations solely classify self-supervised methods into generative and contrastive [10,11], we add the predictive category conceptually based on the processing level of supervision (i.e., the self-supervision for generative methods is raw input, while for predictive methods it is carefully designed labels with high-level semantic information that come after processing of the input). Adopting such categorization also provides a historical perspective to the development of self-supervised learning. In the following sub-sections, we introduce in detail the three types of selfsupervised methodologies in computer vision and link them to works in remote sensing for a side-by-side comparison.


## A. Generative Methods

Generative self-supervised methods learn representations by reconstructing or generating input data. A prominent set of methods in this category are autoencoders (AE), which train an encoder E to map input x to a latent vector z = E(x), and a decoder D to reconstruct x = D(z) from z. While E serves the purpose of f (the feature extractor), the joint function D•E contributes to a self-supervised loss: ||x − D(E(x))||. Another class of methods, generative adversarial networks (GAN), approach the data generation problem from a game theory perspective. In a nutshell, a GAN consists of two models: a generator G and a discriminator D. The generator takes as input a random vector z and outputs a synthetic sample x = G(z). At the same time, the discriminator is trained to distinguish between real samples {x} from the training dataset and synthetic samples generated by G. By doing so, the distribution P z of synthetic data gradually converges to the distribution P z of the training dataset, thereby leading to realistic data generation. The vanilla GAN was not designed for feature extraction, but since formally G −1 = f , there exist GAN-inspired methods to construct representations z.

1) Autoencoder (AE): The concept of autoencoders was introduced as early as in [54] to serve as pre-training of artificial neural networks. Conceptually, an autoencoder D•E is a feedforward Encoder-Decoder network trained to reproduce its input at the output layer. However, one can easily imagine an autoencoder to fail: should E = D = 1, the autoencoder will learn a trivial identity mapping, D • E = 1. Thus demanding the latent vector z to retain information on x, by itself, is not sufficient to yield expressive representations. Constraints are needed to prevent such scenarios. For example, in early practices, the encoder E typically resembles a bottlenecklike structure such that the dimension of the input data gets compressed down, i.e. dim x > dim z. This links autoencoders to dimensionality reduction. In fact, a loose relation between linear autoencoders and Principal Component Analysis (PCA) [89] has previously been studied in the literature [90].

Constraining a small dimension of z is not a must to prevent identity mapping. It is also an option to construct D • E such that the dimension of z is greater than the input's x dimension with additional sparsity constraints, building so-called sparse autoencoder [55]. Other popular approaches include: denoising autoencoder [56] and variational autoencoder (VAE) [57].

Denoising autoencoder is trained to enforce robustness against noise in data. Its reconstruction loss gets modified to ||x − D(E(x + ))||, with a noise modelling term.

VAE decouples the encoder and the decoder by encoding a latent distribution. For each input sample x i , instead of a single deterministic latent representation z i , the encoder E generates two latent vectors µ i and σ i representing the mean and variance of a latent Gaussian distribution. The decoder then randomly samples a representation vector z i sampled from the Gaussian distribution N ( Fig. 4).
µ i , σ i ) ∼ exp −(z i − µ i ) 2 /σ 2 i to reconstruct x i , (see
The required mathematical machinery to properly define the VAE training loss builds on variational inference [91]. However, in practice, it amounts to adding a loss term that forces the latent distribution to a unit Gaussian, i.e., N (µ i , σ i ) → N (0, 1) for all training samples x i . This can be realized by minimizing the KL divergence between the two distributions:
KL(N (µ, σ 2 ) N (0, 1)) = 1 2 (− log σ 2 + µ 2 + σ 2 − 1) (1)
Together with the reconstruction term, the total loss of the VAE reads:  [57]. Instead of encoding the input X to a fixed latent vector, VAE maps input (x 1 , x 2 , . . . ) to a multi-dimensional Gaussian distribution with non-zero mean µ = (µ 1 , µ 2 , . . . ) and diagonal covariance matrix Σ = diag(σ 2 1 , σ 2 2 , . . . ). Reconstruction works through decoding sampled latent vectors (z 1 , z 2 , . . . ) from this distribution.
L VAE = |x − D(E(x))| + λ(− log σ 2 + µ 2 + σ 2 − 1) (2)
where λ is a weighting parameter.

Most recently, masked autoencoder (MAE) [58] raised great attention in the computer vision community with a breakthrough in autoencoding self-supervised pre-training of vision transformers. Inspired by denoising autoencoder, MAE masks out random patches of the input image, sends visible patches to the encoder, and reconstructs the missing patches from the latent representation and masked tokens. This work proves the potential of transformer-based autoencoders for self-supervised visual representation learning.

Remote sensing. Autoencoders have been widely used to learn representation from various remote sensing data like multispectral images [92,93,94,95,96,97,98,99], hyperspectral images [100,101,102,103,104,105,106,107] and SAR images [108,109,110,111]. Lu et al. [92] proposed a combination of a shallowly weighted de-convolution network with a spatial pyramid model in order to learn multi-layer feature maps and filters for input images. In a subsequent step, these get classified by a support vector machine (SVM) for scene classification. Zhang et al. [93] utilized a stacked denoising autoencoder to learn image features for change detection of imagery with various spatial resolutions. In hyperspectral image analysis, autoencoders are either exploited for pretraining [101], or become ingredients in common downstream tasks, such as hyperspectral image classification [100], hyperspectral image denoising [112], hyperspectral image restoration [102,113], and hyperspectral image unmixing [103,99,107].

2) Generative Adversarial Networks (GAN): Generative adversarial networks [59,114] proposed the adversarial training framework-a strategy ruled by minimax optimization [115]. GAN training may be viewed as a two players game: the generator G generates fake samples x = G(z) from random latent vector z, and the discriminator D aims to distinguish x from real data samples x.  [61]. BiGAN includes an encoder E which maps data x to latent representations z. The BiGAN discriminator D jointly acts in data and latent space: x versus G(z)), and E(x) versus z, respectively.

When the discriminator's output is interpreted as probability distribution, i.e. D(x) ∈ [0, 1], a widely adopted, unified form of the GAN-loss reads
max D min G L GAN = log D(x) + log[1 − D(G(z))](3)
where x represents a real sample and G(z) represents a fake sample. During training, G is optimized to fool the discriminator by maximizing D(G(z)) for any z, while D is tuned to minimize D(x) for fake data x and maximize D(x) for real data x. Accordingly, the parameters of G and D get simultaneously tuned to minimize and maximize L GAN , respectively. Contrarily to autoencoders where z = E(x), a GAN's latent representation z is implicitly modeled through the inverse of the generator, G −1 . To approximate G −1 and obtain z for a given sample x, adversarial autoencoders (AAE) [60] can be used. For real samples x, G gets fed by z = E(x) of a (trained) autoencoder D • E. The discriminator contributes to the loss by distinguishing fakes x = G(z) from real samplesx = G(E(x)). Once G, D, and D•E are simultaneously optimized, E serves to encode data points x into learnt representations z.

To make GAN's training scheme more symmetric, Bi-GAN [61,116] and ALI [117] have been proposed. As depicted by Fig. 5, in addition to generating fake data x = G(z) through G, an encoder E generates latent space representations z = E(x). A discriminator D = D(x, z) is trained to distinguish tuples of fakes (x, z) from real ones, (x, z).

Remote sensing. While GAN-based methods in remote sensing are few for self-supervised pre-training, several works tend to integrate such methods to target applications. Zhu et al. [118] presented a first work to explore the potential of GAN for hyperspectral image classification. Jin et al. [119] proposed to use adversarial autoencoder for unsupervised hyperspectral unmixing. Hughes et al. [108] proposed a GANbased framework to generate similar, but novel samples from a given image. Subsequently, these are defined as hard-negative samples to match imagery derived from radar and optical sensors. Also, replacing the generator branch of the GAN with a variational auto-encoder improves the quality of negative samples. In another study, Alvarez et al. [120] employed the discriminator D of a GAN for binary change detection, while the generator G is optimized to induce the distribution of unaltered samples. Walter et al. [121] experimented with and evaluate the performance of the BiGAN approach for remote sensing image retrieval based on the similarity of image features. Cheng et al. [122] proposed perturbation-seeking GAN in a defense framework for remote sensing image scene classification. Ozkan et al. [123] proposed to use Wasserstein GAN loss to optimize the multinomial mixture model for hyperspectral unmixing.


## B. Predictive Methods

While most generative methods perform pixel-level reconstruction or generation, predictive self-supervised methods are based on auto-generated labels. In fact, one may argue that being able to generate very high dimensional data points, such as images, is not necessary to learn useful representations for many downstream tasks. Instead, one can focus on predicting specific properties of the data, which is the general idea behind predictive methods. To this end, the so-called pretext tasks 2 get utilized. A predictive method firstly designs a suitable pretext task for the dataset, prepares self-generated labels, and trains a model to predict such labels and learn data representations.

Predictive self-supervised learning targets two possible drawbacks associated with generative methods that perform pixel-level reconstruction: (1) pixel-level loss functions may overly focus on low-level details whereas in practice such details are irrelevant for a human to recognize the contents of an image, (2) pixel-based reconstruction typically do not involve pixel-to-pixel (long-range) correlations that can be important for image understanding. Based on the assumption that providing the network with relevant high-level pretext tasks, the network may learn high-level semantic information.

In general, the design of different pretext tasks harnesses various context information of the input data. According to distinct context attributes, we categorize pretext tasks as follows: spatial context, spectral context, temporal context, and other semantic contexts.

1) Spatial Context: Images contain rich spatial information for designing self-supervised pretext tasks. Doersch et al. [62] proposed the first example of such methods, predicting the relative position of pairs of randomly cropped image patches drawn from a given input sample. It is assumed that doing well on this task requires a global understanding of the scene and its objects contained. Accordingly, a valuable visual representation is expected to extract the composition of objects in order to reason about their relative spatial location.

Following this paradigm, the literature reveals a multitude of methods to learn image features solving an increasingly complex set of spatial puzzles [63,124,50,125,126]. For example, Noroozi et al. [63] built a jigsaw puzzle from randomly ordered tiles of an image. The network was then trained to predict the correct order of tiles. However, given 9 image patches, there are 9! = 362, 880 distinct permutations and a network is unlikely to recognize all of them due to visual ambiguity. To limit the number of permutations, the Hamming distance [127] was employed to pick a subset of permutations that are significantly diverse. Borrowed from information theory, here the Hamming distance essentially measures the minimum number of tile permutations required in order to recover the image. Further works building on this approach tried to improve the jigsaw puzzles baseline [124] by introducing tiles from other images [50,126], and by extending to larger-size puzzles [125]. Along this line, one important thing to be taken into account is that if carelessly designed, the model can "cheat" from low-level details like edges.

Another set of spatial-context-based pretext tasks exploit geometric transformations of input imagery. Among others, predicting rotation angles [49,128] and image inpainting [64,129] are popular approaches. In [49], the input image was transformed by four separate rotation angles, which then served as the label for the network to predict. [64] cut a patch from the input for the network to recover. We note that this set of image inpainting strategies have also close relation to generative self-supervised methods-as discussed in Fig. II
-A.
Remote sensing. Zhao et al. [130] proposed a multi-task framework to simultaneously learn from rotation pretext and scene classification to distill task-specific features adopting a semi-supervised perspective. Zhang et al. [131] proposed to predict a set of rotation angles from a sequence of rotated SAR-probed targets for object detection. Singh et al. [132] utilized image inpainting as a pretext task for semantic segmentation arguing for spatial correlation of the pretext task to the downstream task. In [133] the authors exercised both relative position and inpainting as pretext tasks for remote sensing scene classification. The study added a contrastive self-supervised component referred to as instance discrimination which we discuss in the following section. Ji et al. [134] tackled the few-shot scene classification problem by incorporating two pretext tasks into training: rotation prediction and contrastive prediction. An additional adversarial model perturbation term is also used for regularization.

Notably, jigsaw puzzles are rarely leveraged in remote sensing. Potentially, spatial correlation in overhead imagery is less dominant. Indeed, translational invariance is prominent in blocks of urban areas, across water surfaces, and many other kinds of natural scenes (desert, forest, mountain ranges, etc.).

2) Spectral Context: Spectral information is another basis for the design of pretext tasks which is inspired by image colorization in computer vision. Zhang et al. [65] proposed one of the first of such tasks: a self-supervised network learns channel-specific representations by predicting a spectral channel taking other channels as input in CIELAB color space [136]. It turned out that the effectiveness of the color space roots in encoding according to human perception: the distance of two points reflects the amount of visually perceived change of the corresponding colors. The work was further extended to hue/chroma color space, and per-pixel color histograms were applied for better representation learning [137]. While these two works learn the representation of a single channel (the gray-scale intensity), a cross-channel representation learning method was proposed in [138]. Two sub-encoders got utilized to extract distinctive color channel representations, which are then concatenated to provide full-channel represen- tations. Larsson et al. [139] proposed a systematic analysis on image colorization as a pretext task. Remote sensing. Given remote sensing imagery contains spectral bands beyond the standard RGB color space, there may not exist straightforward extensions to the multi/hyperspectral domain based on methods established in computer vision. Indeed, sensing data with increased spectral resolution imprints fine details of the physical properties of the earth's surface. Thus, designing pretext tasks related to spectral bands is a subtle exercise to be treated with scientific care. A recent work established by Vincenzi et al. [135] provided an initial attempt to leverage spectral context for self-supervised learning (Fig. 7). Close to an autoencoder, it predicts CIELABencoded RGB imagery of a scene from multi-spectral bands for extraction of representations z s . At the same time, a second encoder pre-trained on ImageNet generates representations z c from the RGB imagery. Downstream classification tasks fine-tune corresponding linear layers L s/c such that the final prediction reads [L c (z c )+L s (z s ))]/2. Wu et al. [140] perform hyperspectral dimensionality reduction using self-supervised learning by training a model to predict low-dimensional representations generated by classic nonlinear manifold embedding methods (e.g., LLE).

3) Temporal Context: Among others, temporal context is very important, for example, in video understanding. Without the cut, footage clearly carries temporal correlation from frame to frame. With a similarity to both spatial and spectral predictive methods, temporal pretext tasks can be designed in two ways: (1) shuffling timestamps of frames to let a neural network predict the correct sequence, (2) masking one or several frames for the network to predict the missing frame.

Missing frame prediction in self-supervised learning typically estimates future snapshots from a short clip of video recordings. Provided its superior performance to model temporal dynamics, LSTM [127] or LSTM variants dominate existing methods to encode temporal correlation in video data [141,142,143].

Frame order prediction in self-supervised learning can be further divided into two distinct approaches: (1) temporal order verification [144,145,146,147], and (2) temporal order recognition [148,149,66]. While the former amounts to binary classification whether or not a sequence of frames is in temporal order, the latter goes beyond by explicitly assigning timestamps to a set of given frames.

One important thing to note for video understanding is that efficient self-supervised training of footage data may involve various steps of preprocessing. For example, the frames of a video may have different importance for the understanding of the event. Along this line, frame sampling strategies play a big role to boost the performance of temporal pretext tasks. A representative method was proposed by Misraet al. [144] that sampled video recordings from frames with significant motion as indicated by the magnitude of the optical flow [150].

Remote sensing. Though having a promising future, satellite-based video recording is not yet common in remote sensing. However, temporal stamps of remote sensing data are very important for applications like change detection or crop type classification. Dong et al. [151] quantified temporal context by coherence in time, proposing a self-supervised representation learning technique for remote sensing change detection. The model gets optimized to identify sample patches in two snapshots of the same geospatial area. The identification network (snapshot one vs. two) is designed to imitate the discriminator D of generative adversarial networks with G generating the self-supervised data representation, as displayed in Section II-A. The method yields improved, robust differentiation for change detection. Yuan et al. [152] proposed a Transformer-based self-supervised methods for satellite time series classification. By predicting randomly contaminated observations given an entire time series of a pixel, the model is trained to leverage the inherent temporal structure of satellite time series to learn general-purpose spectral-temporal representations. The work was further improved in [153], where the network is asked to regress the central pixels of the masked patches for patch-based representation learning. 4) Other Semantic Contexts: Apart from the above three types of contexts, there are also other semantic contexts that can be seen using integrated information of the abovementioned contexts. Noroozi et al. [67] proposed an artificial supervision signal based on counting visual primitives from an input image and the sum of primitives from cropped tiles. Jenni et al. [68] proposed a self-supervised learning method based on spotting and predicting artifacts in an adversarial manner. To generate images with artifacts, the authors pre-train a high-capacity autoencoder and then implement a damage-andrepair strategy. A discriminator is finally trained to distinguish artifact images and predict what entries in the feature were dropped when damaging and repairing.

Multi-sensor or multi-modal self-supervised learning gathers another set of semantic contexts. Owens et al. [69] proposed to predict a statistical summary of the sound associated with a video frame. By defining explicit sound categories, the authors formulate this visual recognition problem as a classification task. Ren et al. [154] trained an encoder-decoder network to predict depth map, surface normal map, and instance contour map from a synthetic image, and a discriminator network is trained to distinguish between features from the synthetic image and the real image.

With various pretext tasks proven to be useful for selfsupervised representation learning, there's also a trend of gathering different pretext tasks together. A direct example can be seen in [155], where the authors investigated the combination of four self-supervised tasks: relative position [62], colorization [65], exemplar [156] (creating a pseudo-class from the augmented view of a single image and predicting the class, this is rather a contrastive method which will be discussed in the next section) and motion segmentation [157] (the network learns to classify which pixels of a single frame will move in subsequent frames). It was shown that combining self-supervised tasks will in general improve the performance and lead to faster training.

Remote sensing. Due to the difference between remote sensing data and common images studied in the computer vision community, there's also a large potential for designing remote sensing-specific pretext tasks. Hermann et al. [158] proposed to predict different views (pose, projection, and depth) as well as reconstruction of the input for monocular depth estimation. Gao et al. [159] proposed to train the model to realize envelope data extrapolation from a frequency band to its adjacent low frequency band. He et al. [160] proposed to reconstruct the corrupted Seismic data with consecutively missing traces, in which the pseudolabels are automatically created from the uncorrupted parts of the observed data. Ayush et al. [161] designed a pretext task based on predicting the geo-location of the input image. Li et al. [162] made use of GlobeLand30 [163], a global land cover product that divides the earth into different areas and includes ten different land cover types. By aligning the geo-location of the input image with the land cover map from GlobeLand30, the land cover class of the input image is then used as a pretext task for self-supervised learning. However, it has also to be noted that pretexts like geo-location itself are usually too simple to learn a good representation, thus they are often used as auxiliary tasks for self-supervised learning (as in the two examples above). In general, how to design or integrate a suitable pretext task is still an important research question to be explored.


## C. Contrastive Methods

The performance of predictive self-supervised learning depends largely on a good pretext task, which is often very difficult to design and may even lead to pretext-specific representation, decreasing the network's generalizability. To tackle this problem, contrastive methods come into play, giving the network more freedom to learn high-level representations that do not rely on a single pretext task. As the name implies, contrastive methods 3 train a model by contrasting semantically identical inputs (e.g., two augmented views of the same image) and pushing them to be close-by in the representation space. Therefore, by design, contrastive methods usually follow a common Siamese-like architecture design. A side-by-side comparison with generative and predictive methods is provided in Fig. 8.

However, only enforcing similarity between pairs of input can easily lead to a trivial solution. Indeed, a constant mapping would be a valid solution to the problem since every pair of input would have identical representations, and thus a maximum similarity. This phenomenon is often referred to as model collapse and many solutions have been proposed in the SSL literature to mitigate it. In fact, depending on how collapsing is handled, one can form a sub-taxonomy of contrastive selfsupervised learning: negative sampling, clustering, knowledge distillation, and redundancy reduction.

1) Negative Sampling: A basic strategy to avoid model collapse is to include and utilize dissimilar samples to have both positive and negative pairs. This strategy is the first one that has been used in contrastive representation learning. For any data point x (the anchor), the encoder f is trained such that: where x + is a data point similar to x (positive sample), x − is a data point dissimilar to x (negative sample), and sim represents a metric that measures the similarity between two pairs of features encoded by f . Positive samples need to be generated in a way that preserves the semantics of the anchor x (e.g., using data augmentation). Negative samples on the other hand come from other data points in the dataset. The intuition is that, in order to output similar representations for visually different, yet semantically similar inputs, while repulsing negative samples in the embedding space, the network has to learn useful high-level representations of the input data. Once pre-trained, the encoder f can be further transferred to extract representative features of downstream datasets.
sim f (x), f x + sim f (x), f x − (4) AR E AR E AR E AR E Anchor Positive −3 −2 −1 +1 E Negative * InfoNCE − exp exp + σ exp
A lot of methods have been developed based on the general objective of Eq. 4, of which the earliest works [70,164,165] proposed triplet losses using a max-margin approach to separate positive from negative examples:
L x, x + , x − = max 0, x − x + 2 − x − x − 2 + 1 (5) where
x, x + , x − represent the anchor, the positive sample and the negative sample, respectively. Noroozi et al. [67] extended this triplet scheme for better transfer learning performance by solving an additional pretext task: counting the visual primitives of tiled patches.

Inspired by noise-contrastive estimation (NCE) [166] and word2vec [167,168], Oord et al. [71] started another set of contrastive methods with negative sampling by introducing contrastive predictive coding (CPC) with InfoNCE loss:
L N = −E X log exp f (x) T f (x + ) N j=1 exp (f (x) T f (x j ))(6)
where N is the number of samples, and the denominator consists of one positive and N − 1 negative samples (see Fig.  9). This is indeed the cross-entropy loss for an N-way softmax classifier that classifies positive and negative samples, with the dot product as the score function in Eq. 4. Take a piece of time series as an example, a consecutive time stamp is a positive sample, while clips randomly sampled from other scenes are negative samples (see Fig. 9). This work was further improved specifically for image recognition in [169].  [73]. A backbone CNN is used to encode each image as a feature vector, which is projected to a 128-dimensional space and L2 normalized. The optimal feature embedding is learned via instance-level discrimination, which tries to maximally scatter the features of training samples over the 128-dimensional unit sphere. ©[2018] IEEE.

InfoNCE loss is also connected to mutual information, as minimizing the InfoNCE loss can be seen as maximizing a lower bound of the mutual information between f (x) and f (x + ) [170]. This bridges the connection to mutualinformation-based contrastive learning which comes out in parallel with CPC. Deep information maximization (DIM) [72] for example, learns image representations by classifying whether a pair of global features and local features are from the same image. The global features are the final output of a convolutional encoder and local features are the output of an intermediate layer in the encoder. This work was further improved by enhancing the association of positive pairs in augmented multi-scale DIM (AMDIM) [171], where a positive sample is sampled from an augmented view of the input image.

Some further empirical evidence [172] claimed that the success of the models mentioned above (maximizing mutual information) is only loosely connected to mutual information. Instead, an upper bound on mutual information estimator leads to ill-conditioned and lower performance representations. Therefore, more attention should be attributed to the encoder architecture and a negative sampling strategy related to metric learning [173]. Along this line, instance-instance contrastive learning discards mutual information and directly studies the relationships between different samples' instance-level representations. A prototype work is instance discrimination (InstDisc, see Fig. 10) [73], which extended the idea of exemplar [156] with non-parametric NCE loss (both utilize instance-level discrimination). In fact, both mutual information and instance discrimination are useful for representation learning, and further works tend to combine both of them implicitly.

There are mainly two questions to consider for a contrastive method with negative sampling: (1) what to compare; (2) how to choose positive/negative pairs. Though different methods vary slightly in the contrasting level (mainly context-level like CPC [71] and DIM [72] or instance level like InstDisc [73]), what to compare is commonly the encoded features f (x) of positive/negative samples. The choice of positive/negative samples then leads to a range of different methods. While CPC [71] and DIM [72] defined features from the same input image as positive pairs and those from other images as negative pairs, AMDIM [171] extended the choice of the positive sample by sampling from an augmented view of the input im-age. Contrastive multiview coding (CMC) [174] extended the idea to several different views (depth, luminance, luminance, chrominance, surface normal, and semantic labels) of one image and samples another irrelevant image as the negative. This work was further improved by InfoMin [175] studying how to choose best views. In addition, Misra et al. [74] proposed pretext-invariant representation learning (PIRL), using pretext tasks in predictive self-supervised learning to transform the input image and let the model learn the invariances.

Contrastive methods tend to work better with more negative examples, since a presumably larger number of negative examples may cover the underlying distribution more effectively. However, the number of negative samples is usually restricted to the size of the mini-batch as the gradients flow back through the encoders of both the positive and negative samples. Consequently, larger mini-batches are preferable, but this in turn is going to be limited by hardware memory constraints. A possible solution is to maintain a separate dictionary, called memory bank, which stores and updates the embeddings of samples with the most recent ones at regular intervals. The memory bank contains a feature representation m x for each sample x in dataset X. The representation m x is an exponentially moving average of feature representations that were computed in prior epochs. It enables replacing negative samples m x − by their memory bank representations without increasing the batch size. PIRL [74] and InstDisc [73] are two representative methods that use a memory bank to boost performance.

However, maintaining a memory bank during training can be a complicated task, as (1) it can be computationally expensive to update the representations in the memory bank as the representations get outdated quickly in a few passes; (2) the representations in the memory bank are always one step behind the current encoding, which might bring unnecessary mismatches. To address these issues, the memory bank got replaced by a separate module called momentum encoder in MoCo [75]. The momentum encoder generates a dictionary as a queue of encoded keys with the current mini-batch enqueued and the oldest mini-batch dequeued. The dictionary keys are defined on the fly by a set of data samples in the batch during training. MoCo also abandons the traditional end-toend training framework by updating the momentum encoder Fig. 11. Conceptual comparison of MoCo [75] with two previous contrastive mechanisms. Rather than end-to-end back-propagation or sampling from a memory bank, MoCo encodes the new keys (key representation) on the fly by a momentum-updated encoder and maintains a queue of keys (a dictionary) for storing negative samples. The size of the dictionary can be much larger than a typical mini-batch size, and the samples in the dictionary are progressively replaced. ©[2020] IEEE. Data augmentation operators studied in SimCLR [76]. The selected best group of data augmentations ("random cropping, color jittering, grayscaling, Gaussian blurring and horizontal flipping") are widely referenced in following self-supervised studies.

based on the query encoder (Fig. 11):
θ k ← mθ k + (1 − m)θ q(7)
where m is the momentum coefficient and k, q represent key encoder (momentum encoder, no back-propagation) and query encoder (back-propagation) respectively. As a result, it does not require training two separate models and there is no need to maintain a memory bank. Based on previous works, Chen et al. [76] proposed a milestone of contrastive learning: A simple framework for contrastive learning of visual representations (SimCLR). SimCLR follows the end-to-end training framework and chooses a batch size as large as 8196 to handle the performance bottleneck of the number of negative samples. The main contribution of Sim-CLR is that it illustrates the importance of a hard positive sampling strategy by introducing data augmentation in 10 forms, and provides a standard scheme of data augmentation for most further works (see Fig. 12). SimCLR also provides some other practical techniques for contrastive learning, including an additional learnable nonlinear transformation between the representation and the contrastive loss (the projection head), doubling negative samples and more training steps. Following the strategies of SimCLR, the authors of MoCo proposed an improved version MoCo-v2 [78] by improving the data augmentation, adding a projection head, and adapting cosine Anchor Positive Negative Fig. 13. Tile2Vec [177]. A triplet loss is optimized to move neighbour patches close and distant patches far away in the feature space. decay on the learning rate. The authors also proposed a transformer version MoCo-v3 [77] by replacing the ResNet [17] backbone of the encoder to a vision transformer [18]. Meanwhile, the authors of SimCLR improved their model to a second version SimCLR-v2 [176] for better performance on semi-supervised learning using distillation, with larger encoder networks, larger batch size, and deeper projection heads. Remote sensing. While contrastive self-supervised learning is relatively new, the wide use of contrastive loss in remote sensing [178,179,180,181,182,183] can date back to [184], where the authors imposed a supervised contrastive regularization term on the CNN features for remote sensing scene classification. The first self-supervised work making use of contrastive learning for remote sensing image representation learning is Tile2Vec proposed by Jean et al. [177]. Similar to CPC [71], this work was also inspired by word2vec [167,168]. The authors trained a convolutional neural network on triplets of tiles, where each triplet consists of an anchor tile t a , a neighbor tile t n that is close geographically, and a distant tile t d that is farther away. A triplet loss is used to move closer neighbour tiles and move further distant tiles in feature space. Jung et al. [185] later reformulated the triplet loss to binary classification loss and added no-updated fully connected layers to improve robustness. Leenstra et al. [186] proposed a combination of triplet loss and binary cross-entropy loss (positive pair as 1 and negative pair as 0) for self-supervised change detection. Inspired by DIM, Li et al. [187] proposed a deep mutual information subspace clustering (DMISC) network for hyperspectral subspace clustering. Hou et al. [188] rely on contrastive learning, following a design similar to SimCLR, to pre-train an HSI classification model on a single scene and reduce the need for dense annotations. Contrastive learning is used on the unlabeled pixels with spectral-spatial feature extraction followed by a fine-tuning stage. Similarly, Zhao et al. [189] also apply SimCLR for HSI classification showing promising results with very limited labels. In the same line of works, Zhu et al. [190] leverage SimCLR for HSI classification using a multi-scale feature extraction approach. PuHong et al. [191] utilized MoCo for oil spill detection in hyperspectral images. It is worth mentioning that all these hyperspectral data related works rely on simple spatial and spectral augmentations (e.g., random cropping, Gaussian noise, etc.) for view generation. We believe there is room for improvement in this direction.

Jung et al. [192] combined the sampling idea of tile2vec and contrastive architecture of SimCLR, utilizing smoothed representation of three neighbor tiles as a positive sample. Montanaro et al. [193] proposed to use SimCLR for representation learning of the encoder and perturbation invariant autoencoder for segmentation training of the decoder to perform land cover classification. Scheibenreif et al. [194] tackled land cover classification and segmentation problems using SimCLR with Swin Transformers and by contrasting optical Sentinel 2 and SAR Sentinel 1 patches. Stevenson et al. [195] proposed to use SimCLR for representation learning of LiDAR elevation data. Kang et al. [196] define spatial augmentation criteria to uncover semantic relationships among land cover tiles and use MoCo for contrastive self-supervised learning. Li et al. [197] added one local matching contrastive loss (between patches within an image) to commonly used global contrastive loss (between different images) to learn rich pixel-level information for semantic segmentation. All above-mentioned methods consider the spatial contexts of remote sensing images and based on that build positive/negative pairs. Apart from these methods, contrastive multi-view coding (CMC) is used in [198,199] and [200] for multispectral and hyperspectral representation learning, and seasonal contrast (SeCo) is proposed in [201] to utilize temporal (season) information to create different views. Heidler et al. [202] proposed a combination of triplet loss and all possible pairings (called batch triplet loss) for audio-visual multi-modal self-supervised learning, and Ayush et al. [161] combined contrastive learning and predictive learning, proposing a combination of MoCo-v2 [78] and geo-location as a pretext task for better representation learning of geo-related images. Guan et al. [203] proposed to integrate contrastive NCE loss and masked image modeling for cross-domain hyperspectral image classification.

2) Clustering: Referring back to one of the earliest unsupervised learning algorithms, the series of clustering-based self-supervised methods learn data representation by using a clustering algorithm to group similar features together in the embedding space. In supervised learning, this pullingnear process is accomplished via label supervision; in selfsupervised learning, however, we do not have such labels. To solve the problem, DeepCluster [51] proposed to iteratively leverage K-means clustering to yield pseudo labels and ask a discriminator to predict the labels. This is also the first work towards clustering-based self-supervised learning.

Self-labeling [80] and local aggregation [79] further pushed forward the boundary of clustering-based methods. Like Deep-Cluster [51], local aggregation and self-labeling also use an iterative training procedure, but the specific process within each iteration differs significantly. In self-labeling, instead of K-means clustering, an optimal transport problem is solved to obtain the pseudo-labels. In local aggregation, unlike the clustering step of DeepCluster where all examples are divided into mutually-exclusive clusters, the method identifies neighbors separately for each example, allowing for more flexible statistical structures. In addition, local aggregation employs  [81]. "Codes" are obtained by assigning features to learnable prototype vectors, a "swapped" prediction problem is then solved wherein the codes obtained from one data augmented view are predicted using the other view.

an objective function that directly optimizes a local softclustering metric, requiring no extra readout layer and only a small amount of additional computation on top of the feature representation training.

Despite the early success of clustering-based selfsupervised learning, the two-stage training paradigm is timeconsuming and poor-performing compared to later instancediscrimination-based methods such as SimCLR [76] and MoCo [75], which have gotten rid of the slow clustering stage and introduced efficient data augmentation strategies to boost the performance. In light of these problems, the authors of DeepCluster [51] brought the idea of online clustering and multi-view data augmentation into the clustering-based contrastive self-supervised approach, proposing to learn features by swapping assignments between multiple views of the same image (SwAV) [81]. The intuition is that, given some (clustered) prototypes, different views of the same images should be assigned to the same prototypes. Meanwhile, an online computing strategy was designed to accelerate code (cluster assignment) computing (see Fig. 14). Based on SwAV, SEER [7] trained a RegNetY [204] with 1.3B parameters on 1B random images, which for the first time surpassed the best supervised pre-trained model. In addition, some recent works presented other efforts towards bridging contrastive learning and clustering, such as Prototypical Contrastive Learning (PCL) [82], Jigsaw clustering [126] and Contrastive Clustering (CC) [205].

Remote sensing. Saha et al. [206] proposed to use Deep-Cluster and triplet contrastive learning to encode multi-modal multi-temporal remote sensing images for change detection. The authors further integrate DeepCluster, BYOL and MoCo-v2 in the pixel-level to produce segmentation maps [207]. Walter et al. [121] presented a comparison between Deep-Cluster, VAE, clolorization as pretext task and BiGAN for remote sensing image retrieval. Similarly, Cao et al. [208] presented a comparison between VAE, AAE and PCL for hyperspectral image classification. Hu et al. [209] proposed spatial-spectral subspace clustering for hyperspectral images based on contrastive clustering [205]. Liu et al. [210] proposed dual dynamic graph convolutional network (DDGCN), contributing a novel clustering-based contrastive loss to capture the structures of views and scenes. In addition to naive contrastive learning on views, scene structures are added into the loss term by clustering scene indexes within the minibatch. Proj. Pred.

Proj.

projector predictor online target ema Fig. 15. Bootstrap Your Own Latent (BYOL) [83]. A similarity loss is minimized between features from the online (student) and the target (teacher) branch. A stop-gradient (sg) operator is applied on the teacher whose parameters are updated with an exponential moving average (ema) of the student parameters.

3) Knowledge Distillation: Another set of methods for contrastive self-supervised learning is based on knowledge distillation [211]. These methods commonly use a teacherstudent network (still Siamese-like structure) and optimize a similarity metric of two augmented views of the same input image. Either asymmetric learning rules or asymmetric architectures are utilized to transfer knowledge between the student network and the teacher network, and thus no negative samples are necessary.

Grill et al. [83] proposed a first milestone called BYOL (bootstrap your own latent, see Fig. 15) for self-supervised learning based on knowledge distillation. The general architecture is similar to MoCo [75] but without the usage of negative samples. It was shown that, if a fixed randomly initialized network (which would not collapse because it is not trained) is used to serve as the key encoder, the representation produced by the query encoder would still be improved during training. If then the target encoder is set to be the trained query encoder and iterates this procedure, it will progressively achieve better performance. Therefore, BYOL proposed an architecture with an exponential moving average strategy to update the target encoder just as MoCo does, and used mean square error as the similarity measurement. Additionally, BYOL is also similar to mean teacher [212] (a semi-supervised method including also a classification loss), which would collapse if removing the classification loss. To prevent collapse, BYOL introduced an additional predictor on top of the online (teacher) network.

SimSiam [84] presented a systematic study on the importance of different tricks in avoiding collapse and proposed a simplified version of the previous self-supervised contrastive methods, arguing that the additional predictor of BYOL is helpful but not necessary to prevent model collapse. Instead, the stop gradient operation of the teacher (target) network is the most critical component to make target representation stable. In addition, the authors showed the relationship between SimSiam and other popular contrastive methods: (1) SimSiam can be thought of as "SimCLR without negatives";

(2) SimSiam is conceptually analogous to "SwAV without online clustering"; (3) SimSiam can be seen as "BYOL without the momentum encoder".

DINO [52] further explored the self-distillation scheme with vision transformer backbones, proposing a new state-of-theart self-supervised learning algorithm. Following a common teacher-student network architecture, the output of the teacher Feature dimension Target Cross-corr. Fig. 16. Barlow Twins [87]. The objective function measures the crosscorrelation matrix between the embeddings of two identical networks fed with distorted versions of a batch of samples and tries to make this matrix close to the identity. This causes the embedding vectors of distorted versions of a sample to be similar while minimizing the redundancy between the components of these vectors. network is centered with a mean computed over the batch. Each network outputs a K dimensional feature that is normalized with a temperature softmax over the feature dimension and their similarity is then measured with a cross-entropy loss. Stop gradient and momentum update are used to improve the performance. A following work EsViT [85] improved DINO by introducing additional region-level contrastive task apart from commonly used global view-level task. Recently, Zhou et al. [86] integrated masked image modeling into the contrastive self-distillation scheme with vision transforms, proposing image BERT pre-training with Online Tokenizer (iBOT).

Remote sensing. Guo et al. [213] proposed to combine GAN and BYOL for better discriminative representation learning. Following a normal GAN structure, a generator is used to generate fake images and a discriminator is used to distinguish them from real images. An additional similarity loss is combined with the discriminator loss by seeing the discriminator as a self-supervised encoder, which encodes both fake and real images as two input views to a BYOL-like Siamese network. In addition, gated self-attention and pyramidal convolution are proposed to be used in both the generator and the discriminator. Chen et al. [214,215] utilized Siamese ResUNet to distill related representations from different modalities (SAR and optical), and a shift transformation to learn pixel-wise feature representations. Hu et al. [216] utilized transformer as encoder backbone and BYOL as baseline structure for hyperspectral image classification. Dong et al. [217] employed SimSiam for ViT based PolSAR image classification. Wang et al. [218] leverage BYOL for SAR ship detection. Jain et al. [219] perform pre-training by contrasting SAR and optical images using BYOL. Wang et al. [220] proposed DINO-MM, a joint SAR-optical representation learning approach with vision transformers. Following the main self-supervised mechanism as DINO, the authors introduced RandomSensorDrop to let the model see all possible combinations of both modalities during training. Zhang et al. [221] introduced a self-supervised spectral-spatial attention-based vision transformer (SSVT), where global and local augmented views are contrasted based on self-distillation [222]. Muhtar et al. [223] proposed In-dexNet, a dense self-supervised method for remote sensing image segmentation. Their approach is built on BYOL and performs contrastive at image and pixel level to preserve spatial information. In a similar fashion, Chen et al. [ [2]. All are reported as unsupervised pre-training on the ImageNet-1M training set, followed by supervised linear classification trained on frozen features, evaluated on the validation set. The parameter counts are those of the feature extractors which are commonly ResNet [17] or ViT [18]. propose a pixel-level self supervised learning approach for change detection. Their method is based on SimSiam with the objective of enforcing point-level consistency across views. They also propose a background-swap augmentation to focus on the foreground.


## 4) Redundancy Reduction:

The idea of redundancy reduction for self-supervised learning comes from neuroscience. Indeed, H. Barlow [225] states that the goal of sensory processing is to record highly redundant sensory inputs into a factorial code (a code with statistically independent com-ponents). Inspired by this principle, Zbontar et al. proposed Barlow Twins [87], a method that uses redundancy reduction as a way to avoid trivial solution in contrastive learning without explicit (e.g. MoCo [75] and SimCLR [76]) or implicit (e.g. DeepCluster [51] and SwAV [81]) negative samples. With the main network architecture similar to previous contrastive learning, Barlow Twins' objective function measures the crosscorrelation matrix between the embeddings of two identical networks fed with distorted versions of a batch of samples, and tries to make this matrix close to the identity (Fig. 16). This causes the embedding vectors of distorted versions of a sample to be similar while minimizing the redundancy between the components of these vectors. The loss function of Barlow Twins is:
L BT i (1 − C ii ) 2 invariance term +λ i j =i C 2 ij
redundancy reduction term (8) where λ is a trade-off parameter and C is the cross-correlation matrix computed between the outputs of the two identical networks along the batch dimension:
C ij b z A b,i z B b,j b z A b,i 2 b z B b,j 2(9)
where b indexes batch samples, i, j index the vector dimension of the networks' outputs, and A, B represent two different views.

Bardes et al. [88] borrowed the decorrelation mechanism from Barlow Twins, proposing a new redundancy-reductionbased self-supervised method VICReg (variance-invariancecovariance regularization). VICReg follows common contrastive learning architecture, and avoids the trivial solution by introducing variance along the batch dimension (a hinge loss which constrains the standard deviation), invariance cross different views (mean-squared euclidean distance), and covariance along feature dimension (penalizing the off-diagonal coefficients of the covariance matrix of the embeddings).

Remote sensing. Since redundancy-reduction-based algorithms (Barlow Twins [87] and VICReg [88]) are relatively new, there are only works using these methods in the remote sensing community. Marsocci et al. [226] proposed to use Barlow-Twins in a continual learning setting to pre-train a model on a set of heterogeneous datasets while avoiding catastrophic forgetting. Their approach is evaluated on segmentation problems. Ait Ali Braham et al. [227] used Barlow-Twins for few-shot HSI classification in a two-stages approach. They propose two pair sampling strategies for pixel and patch level classification using spatial information from the scene to generate positive pairs, along with a set of data augmentations. Their results support the effectiveness of redundancy reduction methods on hyperspectral images. In the next section, we further discuss the conceptual idea of redundancy reduction and how it could be beneficial for remote sensing image understanding. We believe there is a large potential to be explored.


## III. FROM IMAGENET TO REMOTE SENSING IMAGERY:

SELF-SUPERVISED LEARNING ON GEOSPATIAL EARTH OBSERVATION DATA Self-supervised learning is a data-driven methodology that learns the representation of a dataset without human annotation. However, most existing self-supervised methods in the computer vision community deal with natural RGB images (e.g., ImageNet [2]) which are different from remote sensing imagery, and there are various types of data from various types Fig. 19. Geography-aware self-supervised learning [161]. Spatially aligned images over time are used to construct temporal positive pairs in contrastive learning, and geo-location is used as an additional pre-text task to boost representation learning for remote sensing images. ©[2021] IEEE. of sensors that require specific considerations. In addition, though intended to learn common representations for various downstream tasks, there's an unignorable gap between pixellevel, patch-level and image-level tasks that may benefit from different designs of self-supervision. In this section, we dig into the implementation of self-supervised methods on different types of remote sensing data and image-/patch-/pixel-level applications.


## A. Characterisics and Challenges of Remote Sensing Data

Remote-sensing data contains multiple modalities, e.g., from optical (multi-and hyperspectral), Lidar, and synthetic aperture radar (SAR) sensors, where the imaging geometries and contents are completely different [8]. Yet before we discuss those different modalities, there exist some characteristics that all data types share as a common remote sensing specific property compared to natural images.

First, remote sensing data are geolocated, which means each image pixel corresponds to a geospatial coordinate. This information provides additional chances for the design of selfsupervised pretext tasks (e.g., predicting the geo-location of an input image [161], see Fig. 19), as well as indirect cooperation with geographical resources (e.g., land cover database [162]).

Second, remote sensing data are geodetic measurements and usually correspond to specific physical meanings. If not well analyzed, this characteristic can raise issues with data augmentation. Specifically, data augmentation is very important especially for contrastive methods to help the network learn semantic invariance, yet a careless design of data augmentation might change certain physical properties which are indeed important to recognize the objects in a remote sensing image.

Third, remote sensing images usually contain many more objects than natural images (e.g., compared to an image from ImageNet which generally contains one main object, a remote sensing image usually includes multiple repeated objects). This difference influences both pretext tasks and data augmentation. For example, jigsaw puzzles can be expected to be difficult for representation learning on low-resolution satellite images, and rotation can affect the shadows of buildings which are important for the model to learn relative heights. On the other 


## Modality

Application Method

Multi-spectral


## Representation learning

Vincenzi et al. [135]: colorization as pretext task. tile2vec [177]: contrastive learning with triplet loss. Jung et al. [185]: replace triple loss of tile2vec to binary cross entropy loss. Stojnic et al. [198]: contrastive multiview coding. CSF [228]: contrastive multiview coding. Jung et al. [192]: SimCLR with smoothed view. GeoMoCo [161]: MoCo + geo-location as pretext task. SauMoCo [196]: MoCo with spatial augmentation. SeCo [201]: MoCo + seasonal contrast. GeoKR [162]: geo-supervision (landcover map) + teacher-student network.


## Scene classification

Lu et al. [92]: autoencoder. Zhao et al. [130]: rotation as pretext task + classification loss. Tao et al. [133]: inpainting, relative position and instance discrimination. SGSAGANs [213]: BYOL + GAN.


## Semantic segmentation

Li et al. [229]: inpainting/rotation as pretext tasks + contrastive learning.

Singh et al. [132]: GAN + inpainting as pretext task.

Li et al. [197]: global and local contrastive learning. hand, when image-level representation is important for natural images, patch-level and pixel-level representation can be equally important for remote sensing images (e.g., considering the spatial relationship of neighboring and distant patches to sample positive/negative pairs [177,196] as shown in Fig. 13 and 20). Last but not least, the time variable is becoming increasingly important despite the modality. This raises the interest of getting inspiration from video self-supervised learning, yet time stamps in remote sensing images are quite different from frames in videos, as well as the special objectives (e.g. change detection in remote sensing). Therefore, how to make the best of temporal information is a common question to be answered in all modalities. An early progress is shown in Fig. 21 where seasonal information is used for data augmentation [201].

Apart from these important common characteristics, below we discuss different modalities of remote sensing data. A list of recent self-supervised works classified w.r.t modality is shown in Table II. 1) Multispectral Images: Multispectral images are captured within specific wavelengths ranging across the electromagnetic spectrum, which allows for the extraction of information covering or beyond the perception range of human eyes. Compared to hyperspectral imaging, multispectral imaging measures light in a small number (typically 3 to 15) of spectral bands with relatively low spectral resolution but Fig. 20. Spatially augmented momentum contrast [196]. Following the underlying assumption of Tile2Vec [177], nearby cropped tiles are seen as spatial augmentation for the anchor tile, and a similarity loss is optimized based on momentum contrast [75]. ©[2020] IEEE.  [201]. Three embedding subspaces are trained to be invariant to temporal (seasonal), synthetic and both temporal and synthetic transforms T 1 , T 2 and T 0 , respectively. high spatial resolution. Multispectral images are the most commonly used remote sensing data, as they are relatively easy to acquire, and are close to natural images which enable easier human perception and convenient technology transfer from the computer vision community. Due to the popularity and the similarity of multispectral images to natural images, a lot of self-supervised methods have been directly transferred to the remote sensing field (see Fig. 23). Generally, all categories of self-supervised methods in CV can be extended to remote sensing multi-spectral image analysis, but there is some special attention to pay: (1) the common characteristics of remote sensing data for all modalities need to be considered as have been discussed above, (2) spectral contexts of predictive methods (e.g., colorization) now have possibly more channel information that can be used to design the pretext tasks, (3) color-related data augmentations (e.g., color distortions shown in Fig. 12) which are very important for contrastive selfsupervised learning need to be carefully modified to fit into multiple channels.

2) Hyperspectral Images: Hyperspectral imaging also captures images across the electromagnetic spectrum, but lights in a far bigger number (up to hundreds) of spectral bands than multispectral imaging with a trade-off on the spatial resolution.

This very high spectral resolution enables us to identify the materials contained in the pixel via spectroscopic analysis, yet, on the other hand, requires special care about band-wise analysis. In fact, with a basic objective of dimensionality reduction, PCA/ICA/clustering and manifold-learning-based methods have been widely studied in the field before the era of neural networks. Along the lines, deep-learning-based selfsupervised methods have been focusing on autoencoders and 3D convolution to compress spectral representations. Because of the large number of spectral channels, the network usually takes a single-pixel or a small patch as input. In both cases, spatial contexts for pretext tasks or data augmentations tend to consider intra-sample instead of inner-sample information (e.g., neighboring patches or pixels can be seen as positive samples in contrastive negative sampling). Meanwhile, spectral contexts have a large potential to be explored because of the high spectral dimensionality of hyperspectral data. For example, two spectrally divided parts of a hyperspectral image can be two augmented views.

3) SAR Images: Synthetic-aperture radar (SAR) is a form of radar that is used to create two-dimensional images or threedimensional reconstructions of objects. Due to the imaging principle, SAR images are substantially different from multispectral and hyperspectral images in characteristics like dynamic range, speckle statistics, imaging geometry and complex domain information. While self-supervised learning with SAR images is also a new research direction and has not been widely explored, there's one fact that makes it a valuable topic: the prevailing lack of ground truth for regression-type tasks. Though simulators can be used to provide training data for supervised learning, this bears the risk that the networks will learn models that are far too simplified. Therefore, it would be very beneficial if we can use self-supervised techniques to learn representations from real SAR data. In addition, the general self-supervised schemes can also be easily transferred to SAR images with specific considerations. For example, the data augmentation of color distortion is not suitable for SAR data, the speckle noise of SAR images has a similar effect as Gaussian blurring, and the ascending and descending order may serve as a natural augmentation strategy. Fig. 22. Shift invariance [214]. The two inputs have an offset but keep an overlap. A shift transformation is included in the one branch for aligning representations between two branches which are optimized by a contrastive loss.  


## 4) Multi-sensor Fusion:

As a result of the multi-modality of remote sensing data, multi-sensor fusion is always an important task for various objectives, which can benefit from multi-modal self-supervised learning. There are generally two ways of combining multi-modal inputs: (1) similar to various input channels, different modalities can be seen as augmented views of semantically the same input which can be used for designing pretext tasks or specific data augmentation in contrastive self-supervised learning; (2) two encoders are used separately to encode features from two different modalities and then concatenated together for a joint representation learning. Regarding remote sensing data, a common multi-modality is from optical and SAR images, leading to the important topic SAR-optical data fusion, which has been shown by some recent works that benefits from self-supervised representation learning [214] (see Fig. 22) and [220]. In addition, the fusion of other modalities like audio and social media data is also raising increasing interest [202].


## B. Applications

Though the general goal of self-supervised learning is to perform task-independent representation learning which can benefit various downstream tasks, it has to be noted that different self-supervision can have different influences on different tasks. Therefore, it is necessary to pre-consider the choice of self-supervised methods when we are targeting a specific application in the end. In fact, so far a large number of self-supervised works in the remote sensing field are based on a specific application. In general, the various applications can be separated into three categories: image-level tasks, pixellevel tasks and patch-level tasks. A list of representative recent self-supervised works classified w.r.t applications is shown in Table II. We refer the interested reader to the numerous surveys on machine learning and deep learning in remote sensing [8,239,240,241,242,243,244] for more details about common downstream tasks and available datasets.

1) Image-level Tasks: Image-level tasks correspond to the applications that expect the recognition of the whole image or image patches, focusing more on global knowledge. The most common image-level task is scene classification for multispectral [92,130,133,213], SAR [235] and other possible modalities. Scene classification is directly related to natural image classification and is usually the default downstream task for the evaluation of a self-supervised method. Like most of the above-mentioned predictive and contrastive selfsupervised methods, an image-level task requires focusing on a global representation that can be done by predicting high-level pretexts or contrasting the encoded features of two augmented image views. Other image-level tasks include time series classification [152] and image retrieval [121].

2) Pixel-level Tasks: Pixel-level tasks correspond to the applications that expect the recognition of each image pixel, focusing more on the local details. The most common pixellevel task is semantic segmentation [245,229,132,197], which requires finer representation in pixel level. Generative self-supervised methods like autoencoder and pretext tasks / data augmentations like image inpainting and pixel based contrastive learning can be helpful. Change detection [93,120,151,98,230,199,215,186,186] is usually also a pixellevel task which utilizes multitemporal information to detect changing pixels. In hyperspectral image analysis, most of the tasks are based on pixel level, including hyperspectral image classification 4 [106,100,101], image denoising [233], spectral unmixing [99], target detection [232], image restoration [102] and super-resolution [105,104]. Other pixel-level tasks include depth estimation [158,95] and SAR despeckling [110,109].

3) Patch-level Tasks: Patch-level tasks mainly correspond to object detection, lying in-between image-level and pixellevel applications. When requiring both the location and the label of the object in the image, patch-level tasks benefit from both global representation and local details. Hence for good performance in the application of object detection, an integrated representation learning pipeline deserves consideration [231]. In addition, patch-based change detection considers comparisons between tiles of images (change or no change) rather than pixels due to reasons like possible misalignment between time stamps [246], or the requirement of fast preprocessing to retrieve region of interest and limited bandwidth for data communication [247]. We note that patch-level change detection can also be classified into image-level tasks. However, we put them here as there is a stronger correlation between the small patches and the big scene.


## IV. A PRELIMINARY BENCHMARK

As a milestone along the road of large-scale unsupervised pre-training in the image domain, the past two years have  [13], SEN12MS [14] and So2Sat-LCZ42 [15] (culture-10 version) datasets. We use ResNet-18 as encoder backbones, follow the official settings of the models for self-supervised pre-training, and evaluate the performance by training a linear classifier on frozen features. (b) The "collapsing" training curve of SimSiam. The loss goes down quickly in the beginning 10 epochs and becomes very unstable in the following epochs. witnessed big advances with contrastive self-supervised learning. However, as most of the recent representative methods are developed and verified on natural images, there lacks a comprehensive study about their performance on remote sensing imagery. Therefore, with the hope to build a reference for the remote sensing community, we provide in this section a preliminary benchmark of recent contrastive self-supervised methods on several popular remote sensing datasets. Codes are available at https://github.com/zhu-xlab/SSL4EO-Review.

A. Methods, Datasets and Implementation Details 1) Methods and Datasets: We consider four representative contrastive self-supervised methods to provide the benchmark: MoCo-v2 [78], SwAV [81], SimSiam [84] and Barlow Twins [87], each representing one sub-category of modern contrastive self-supervised learning: negative sampling, clustering, knowledge distillation and redundancy reduction. We use three popular remote sensing datasets for pre-training and evaluation: BigEarthNet [13], SEN12MS [14] and So2Sat-LCZ42 [15]. BigEarthNet is a multi-label scene classification dataset, containing 590,326 non-overlapping Sentinel-2 image patches of size 128*128 covering 10 European countries. SEN12MS is a global dataset with 180,662 256*256 triplets of Sentinel-1, Sentinel-2 and MODIS land cover maps. So2Sat-LCZ42 is a dataset built for local climate zone classification, containing 400,673 32*32 Sentinel-1/2 pairs covering 42 cities in the world. They are all large-scale datasets with different geographical coverage and built for different tasks, which we believe provide solid diversity for evaluation of the generalizability of self-supervised methods. In addition, we use EuroSAT [248] for transfer learning evaluation, which is a small single-label land cover classification dataset containing 27,000 64*64 Sentinel-2 patches.

2) Implementation Details: We pre-train ResNet-18 encoders for 100 epochs using each of the four methods on each of the three pre-training datasets. Our experiments are based on Sentinel-2 only. In terms of pre-training, we use 311,667 patches for pre-training on BigEarthNet; for SEN12MS, all images are cropped to 128*128 to avoid the 50% overlap be-tween neighbouring patches of the original dataset; for So2Sat-LCZ42 (culture-10 version), we pre-train on the 352,366 training split. In terms of downstream evaluation, we do linear probing or fine-tuning on the training split and report testing split accuracy.

We mainly follow the default implementation in the official repositories of the four self-supervised methods. Importantly, the queue size is 65536 and the feature dimension is 128 for MoCo-v2; the number of prototypes is 60 for SwAV; the feature dimension is 2048 for the encoder and 512 for the projector in SimSiam; three projectors with dimension 2048 are used for Barlow Twins. The data augmentations include cropping, grayscaling, Gaussian blurring, horizontal flipping and channel dropping. For simplicity, we do not include color jitterring for the main benchmark (except for SimSiam due to collapse which will be discussed later) as it is designed for natural RGB images. 4 NVIDIA A100 GPUs are used for the experiments, on which the batch size is in total 256. Links to the four methods' official sites and details of all the hyperparameters we use can be found in our code repository. In downstream tasks, we transfer 100-epoch pretrained encoders for MoCo, SwAV and Barlow Twins, and 10epoch pre-trained encoders for SimSiam (the model collpases afterwards). In addition, we report the performance of fully supervised learning and training with randomly initialized encoders for more thorough comparison.


## B. Benchmark Results

1) General Comparsion of SSL Methods: The general benchmark results with linear probing evaluation are shown in Fig. 24 (a). A first conclusion is that with pure simple transfer from natural images to satellite images, all the selfsupervised methods provide meaningful representations for all datasets (compared to random initialization). With frozen features that come from label-independent pre-training, we can train much fewer parameters (e.g. a full ResNet-18 has 10M parameters while the last linear layer contains only 500K) with comparable performance towards fully supervised training.

The second observation is that MoCo-v2 [78], which represents contrastive negative sampling, generally outperforms the other methods across datasets. This proves the robustness and transferability of using negative samples for self-supervised learning in remote sensing, which is also consistent with the fact that most of the recent self-supervised works in the remote sensing community prefer MoCo [75] and MoCo-v2 [78] as a baseline structure. A third observation is that compared to the other methods, the training of SimSiam [84] is very unstable. As can be seen from the training curves in Fig. 24 (b), the training loss of SimSiam drops very quickly and significantly in the beginning epochs, and starts vibrating in the following epochs. This can be attributed to SimSiam's simplified design (as has been described in section II-C), which makes the similarity matching task easy to solve for the models. When utilizing checkpoints of the later epochs to serve as pre-trained weights, the model fails to beat even random initialization. That is to say, SimSiam gets collapsed after 10 epochs. Thus in real applications, it is better to add a momentum encoder or other tricks for better performance and more stable training.

2) Data Augmentation: In this subsection, we provide an extended study on the impact of data augmentations which have been proved very important for natural images [76]. Focusing on BigEarthNet, we do self-supervised pre-training considering a rich set of augmentations: "RandomResized-Crop", "RandomColorJitter" (a simplified version which only changes contrast and brightness), "RandomGrayscale", "Ran-domGaussianBlur", "RandomHorizontalFlip" and "Random-ChannelDrop" (replacing one or several channels by zero). As can be seen in Fig. 25, the results show both common and different trends compared to the computer vision community. When dropping only the "RandomResizedCrop" transform, SwAV and Barlow Twins collapse immediately and MoCo-v2 and SimSiam see a significant performance drop. All methods confirm the importance of cropping which is in line with natural images. Surprisingly, almost no visible performance drop is observed when using only cropping as the data augmentation for all four methods. This is a significant difference compared to natural images, where other augmentations (especially color distortion) bear big importance as well. Therefore, apart from cropping which is a key augmentation, further study is re- Fig. 26. BigEarthNet linear classification and fine-tuning results using different amounts of labels pre-trained with MoCo-v2. Self-supervision outperforms supervision when reducing the number of labels. quired on the design of other augmentations to work well on remote sensing multispectral images. In addition, to further analyze the impact of cropping, we explore different settings for the minimum cropping size. The results show that, the bigger the minimum size (less aggressive cropping), the worse the performance is. This is inline with the computer vision literature which emphasizes the importance of having stronger augmentations to make the training task hard enough for the network.

3) Regime of Limited Labels: Though Fig. 24 proves the capability of self-supervised pre-trained models, the linear probing results are still below the upper bound of fully supervised learning with full labels. Thus, we wonder how these pre-trained models behave when reducing the number of labels and turning on fine-tuning. We pre-train ResNet18 with MoCo-v2 for 100 epochs and evaluate on BigEarthNet. Fig. 26 shows the linear classification and fine tuning performance when reducing the amount of labeled samples to 50%, 10%, 1% and 0.1%. The general trend of the figure proves the potential of self-supervised pre-training when we do not have enough labels: linear classification with self-supervision outperforms vanilla supervised learning when reducing the amount of available labels to 1%. The advantage is more significant for fine-tuning, which outperforms supervised learning on all scenarios and provide huge improvements on tiny amount of labels. In general, the fewer labels we use, the bigger the advantage of self-supervised pre-training is. 4) Transfer Learning: In addition, we report transfer learning performance by linear classification and fine-tuning on EuroSat with a ResNet18 pre-trained on BigEarthNet. As is shown in Fig. 27, the performance plots are similar to Fig. 26, proving the pre-trained models transfer well the data representations across datasets. Promisingly, fine tuning on 50% of labels outperforms supervised learning with full labels. In fact, fine tuning on 10% of labels gives already very close performance compared to full-label supervised learning.

V. CHALLENGES AND FUTURE DIRECTIONS Self-supervised learning has been achieving great success in various vision tasks, yet it is still a relatively new branch of technologies in both computer vision and remote sensing communities. In this section, we summarize the challenges of self-supervised learning in remote sensing and discuss possible future directions.

Model collapse. Model collapse is one of the main challenges along with the development of self-supervised learning, especially for modern contrastive learning. Though it has been shown that contrastive self-supervised learning achieved great success in the field, the fundamental theory behind those contrastive methods (especially those using no negative samples) is still not well understood. There are already some works like SimSiam [84] and [249] that try to investigate the underline theory, yet they are not enough. In fact, as has been mentioned in section IV, SimSiam easily gets collapsed in Earth observation data. Future works need to go deeper into self-supervised representation learning and explore the theoretical foundations behind model collapse in remote sensing data.

Pretext tasks and data augmentations. Pretext tasks and data augmentations play a similar and very important role in self-supervised learning, as they correspond directly to what invariance the networks need to learn and what information is important for image understanding. There have been some early studies on the evaluation of different pretext tasks and data augmentations and possible improvements [250,251], yet they mainly focus on natural images and evaluate on imagelevel classification tasks. As has been proved in section IV, the findings from common computer vision benchmarks may not completely hold for multispectral imagery, not to mention other modalities. In section III, we discussed the characteristics of different remote sensing data, yet still experiments are needed. Therefore, in general more studies on pretext tasks and data augmentations are required to better understand which ones are useful for different types of remote sensing data.

Pre-training datasets. Most of existing remote sensing datasets are intended for supervised learning. Though they can also be used for self-supervised pre-training by discarding the labels, the amount of data is limited and the data is usually biased towards the tasks of the datasets. Therefore, it needs to be studied the necessity of self-supervised pretraining on large-scale uncurated datasets. There have been some early contributions following this thread: Leenstra et al. [186] proposed a Sentinel-2 multitemporal cities pairs dataset for change detection; Manas et al. [201] proposed a large-scale self-supervised Sentinel-2 dataset; Heidler et al. [202] proposed an audio-visual dataset for multi-modal self-supervised learning. However, there is still much space to explore and contribute (e.g., dataset size, image sampling strategies, and the availability of more modalities).

Multi-modal/temporal self-supervised learning. As one of the most important characteristics of remote sensing data, multi-modality is a significant aspect to be explored in selfsupervised representation learning [214,220,151]. On the other hand, multi-temporal image analysis is raising more interest because of the increasing frequency of data acquisition and transferring. Without the need for any human annotation, self-supervised representation learning has a big advantage to performing big data analysis which is usually a challenge for multi-modal and multi-temporal data. However, adding modalities and time stamps will also bring complexity to the model to be trained. Thus how to balance different modalities or time stamps such that the model can learn good representations from both modalities is a challenging task to be explored.

Computing efficient self-supervised learning. Selfsupervised pre-training usually requires a large amount of computing resources because of big pre-training data, various data augmentations, the requirement of large batch sizes and more training epochs than supervised learning, etc. Few efforts have been made towards reducing computational cost, which is however an important factor for practical usage. Therefore, efficient data compression [252], data loading, model design [58] and hardware acceleration are necessary to be explored.

Network backbones. Most of existing self-supervised methods utilize ResNet [17] as their model backbones, while there has been a hot trend towards vision transformers (ViT) [18]. Since ViT has shown promising results in selfsupervised learning [77,52,85], and recent advances are exploring the link between large-scale pretraining for both language and vision [58,86], it deserves more exploration in remote sensing images as well. In addition, the specific encoder architecture itself is also important to study, as most existing methods tend to keep it unchanged with a focus on the conceptual design of self-supervision.

Task-oriented weakly-supervised learning. Last but not least, in parallel to the progress of task-agnostic selfsupervised learning, there's also the need to integrate "few labels" or "weak labels" for better task-dependant weaklysupervised learning in practice. In other words, not only can self-supervised learning provide a pre-trained model for downstream tasks, but there's also the possibility of bringing representation learning online. For example, both data representations and the supervised task can be learned together, where the self-supervised branch helps the model capture more useful information. This scenario is especially important when moving to extreme applications that do not have a common large dataset for pre-training. In addition, this can also be a way to mitigate the generally high computational cost of training SSL algorihtms.


## VI. CONCLUSION

This article provides a systematic introduction and literature review of self-supervised learning for the remote sensing community. We summarize three main categories of selfsupervised methods, introduce representative works and link them from natural images to remote sensing data. Moreover, we provide a preliminary benchmark and extened analysis of four modern contrastive self-supervised learning on three popular remote sensing datasets. Finally, fundamental problems and future directions are listed and discussed. 


(Corresponding author: Xiao Xiang Zhu.) Y. Wang, N.A.A. Braham, L. Mou and X. Zhu are with the Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Germany and with the Chair of Data Science in Earth Observation, Technical University of Munich (TUM), Germany (e-mails: yi.wang@dlr.de; Nassim.aitalibraham@dlr.de; lichao.mou@dlr.de; xiaoxiang.zhu@dlr.de). C. M Albrecht is with the Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Germany (e-mail: conrad.albrecht@dlr.de).

## Fig. 2 .
2The general pipeline of self-supervised learning. The visual representation is learned through self-supervision that comes from the unlabeled data. The learned parameters serve as a pre-trained model and are transferred to supervised downstream tasks for fine-tuning.

## Fig. 3 .
3A taxonomy of self-supervised learning.

## Fig. 4 .
4Variational AutoEncoder (VAE)

## Fig. 5 .
5Bidirectional Generative Adversarial Networks(BiGAN)

## Fig. 6 .
6Examples of different predictive pretext tasks. (a),(b),(c), and (d) are based on spatial contexts; (e) is based on spectral context; (f) illustrates temporal context.

## Fig. 7 .
7The color out of space[135]. CIELAB-encoded RGB imagery of a scene is predicted from other spectral bands for the extraction of representations. ©[2021] IEEE.

## Fig. 8 .
8A comparison of the general structures between generative, predictive, and contrastive self-supervised learning. E and D refer to the encoder and decoder network. While generative and predictive methods calculate the loss in the output space, contrastive methods calculate the loss in the representation space.

## Fig. 9 .
9Contrastive Predictive Coding (CPC)[71]. Take time series as an example, the consecutive time stamp is a positive sample, while clips sampled from other scenes are negative samples.

## Fig. 10 .
10Instance Discrimination (InstDisc)


Fig. 12. Data augmentation operators studied in SimCLR [76]. The selected best group of data augmentations ("random cropping, color jittering, grayscaling, Gaussian blurring and horizontal flipping") are widely referenced in following self-supervised studies.

## Fig. 14 .
14Swapping Assignments between Views (SwAV)

## Fig. 21 .
21Seasonal Contrast (SeCo)

## Fig. 23 .
23Statistics of recent publications related to self-supervised learning in remote sensing.

## Fig. 24 .
24(a) A preliminary benchmark of four popular contrastive self-supervised methods (each representing one category in section II-C) on Sentinel-2 images of BigEarthNet

## Fig. 25 .
25A study of data augmentation on BigEarthNet. The baseline includes all augmentations, "no crop" means all augmentations except cropping, and "crop only" means only using cropping as the augmentation with minimum cropping size 0.2, 0.5 and 0.8 of the original size, respectively.

## Fig. 27 .
27EuroSAT linear classification and fine-tuning results pre-trained with MoCo-v2 on BigEarthNet. Self-supervision with 50% labels outperforms supervised learning with full labels.


ACKNOWLEDGMENTS This work was jointly supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. [ERC-2016-StG-714087], Acronym: So2Sat and grant agreement No. [957407], Acronym: DAPHNE), by the Helmholtz Association through the Framework of Helmholtz AI (grant number: ZT-I-PF-5-01) -Local Unit "Munich Unit @Aeronautics, Space and Transport (MASTr)", the Helmholtz Klimainitiative (HICAM) and Helmholtz Excellent Professorship "Data Science in Earth Observation -Big Data Fusion for Urban Research" (grant number: W2-W3-100)) and by the German Federal Ministry of Education and Research (BMBF) in the framework of the international future AI lab "AI4EO -Artificial Intelligence for Earth Observation: Reasoning, Uncertainties, Ethics and Beyond" (grant number: 01DD20001). The computing resources were supported by the Helmholtz Association's Initiative and Networking Fund on the HAICORE@FZJ partition.

## TABLE I
I


224] Fig. 18. Comparison of SSL methods under the linear classification protocol on ImageNetEncoder 
Encoder 

image 

contrastive 

grad 
grad 

Encoder 
Encoder 

image 

redundancy 

grad 
grad 

Encoder 
Encoder 

image 

similarity 

grad 

sinkhorn 

student 
teacher 

image 

similarity 

grad 

predictor 

(a) Contrasting negative samples 
(b) Clustering 
(c) Distillation 
(d) Redundancy reduction 

Fig. 17. A comparison of contrastive self-supervised methods. 

Method 
Architecture Params (M) Top-1 acc 
BigBiGAN [116] 
R50 
24 
56.6 
MAE [58] 
ViT-L/16 
307 
73.5 
RelativePosition [62] 
R50w2X 
94 
51.4 
Jigsaw [63] 
R50w2X 
94 
44.6 
Rotation [49] 
Rv50w4X 
86 
55.4 
Colorization [65] 
R101 
28 
39.6 
CPC-v1 [71] 
R101 
28 
48.7 
CPC-v2 [169] 
R161 
305 
71.5 
AMDIM [171] 
R-custom 
194 
63.5 
CMC [174] 
R50 
47 
66.2 
InstDisc [73] 
R50 
24 
54.0 
PIRL [74] 
R50 
24 
63.6 
MoCo [75] 
R50 
24 
60.6 
MoCo-2X 
R50w2X 
94 
65.4 
SimCLR [76] 
R50 
24 
69.3 
SimCLR-2X 
R50w2X 
94 
74.2 
MoCo-v2 [78] 
R50 
24 
71.1 
SimCLR-v2 [176] 
R50 
24 
71.7 
MoCo-v3 (ViT-B) [77] ViT-B/16 
86 
76.7 
MoCo-v3 (ViT-L) 
ViT-L/16 
304 
81.0 
DeepCluster [51] 
VGG 
15 
48.4 
LocalAgg [79] 
R50 
24 
60.2 
SwAV [81] 
R50 
24 
75.3 
SwAV-2X 
R50w2X 
94 
77.3 
BarlowTwins [87] 
R50 
24 
73.2 
VICReg [88] 
R50 
24 
73.2 
BYOL [83] 
R50 
24 
74.3 
SimSiam [84] 
R50 
24 
71.3 
DINO (RN) [52] 
R50 
24 
75.3 
DINO (ViT) 
ViT-B/16 
85 
78.2 
EsViT [85] 
Swin-B 
87 
81.3 
iBOT [86] 
ViT-L/16 
307 
81.7 
Supervised (ResNet) 
R50 
24 
76.5 
Supervised (ViT) 
ViT-B/16 
85 
79.9 



## TABLE II A
IIGALLERY OF RECENT SELF-SUPERVISED WORKS IN REMOTE SENSING.
Typically a convolutional neural network (CNN), e.g. ResNet[17]. Recent work also show promising results with vision transformers (ViT)[18].
We note that the term pretext task can also represent any generic objective associated with self-supervised methods. In this paper, we restrict such notion to predictive approaches (where the term originally came from) to make the taxonomy more clear.
In some literature, the term contrastive is only used to denote methods that include negative samples. Yet, we follow the more common terminology in which contrastive methods also include the recent works without negative samples.
Note that with the goal to classify the center pixel, hyperspectral image classification tends to work on a small patch to benefit from spatial relationships, linking towards image and patch level tasks.

Deep learning. Yann Lecun, Yoshua Bengio, Geoffrey Hinton, 5211Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. "Deep learning". In: nature 521.7553 (2015), pp. 436- 444 (cit. on p. 1).

Imagenet: A large-scale hierarchical image database. Jia Deng, 2009 IEEE conference on computer vision and pattern recognition. Ieeecit. on pp. 1, 14, 15Jia Deng et al. "Imagenet: A large-scale hierarchi- cal image database". In: 2009 IEEE conference on computer vision and pattern recognition. Ieee. 2009, pp. 248-255 (cit. on pp. 1, 14, 15).

Unsupervised learning based on artificial neural network: A review. Happiness Ugochi Dike, 2018 IEEE International Conference on Cyborg and Bionic Systems (CBS). IEEE1citHappiness Ugochi Dike et al. "Unsupervised learning based on artificial neural network: A review". In: 2018 IEEE International Conference on Cyborg and Bionic Systems (CBS). IEEE. 2018, pp. 322-327 (cit. on p. 1).

A survey on semi-supervised learning. Jesper E Van Engelen, H Holger, Hoos, Machine Learning. 109cit. on pp. 1, 2)Jesper E Van Engelen and Holger H Hoos. "A survey on semi-supervised learning". In: Machine Learning 109.2 (2020), pp. 373-440 (cit. on pp. 1, 2).

A brief introduction to weakly supervised learning. Zhi-Hua Zhou, National science review. 51Zhi-Hua Zhou. "A brief introduction to weakly super- vised learning". In: National science review 5.1 (2018), pp. 44-53 (cit. on p. 1).

Meta-learning. Joaquin Vanschoren, Automated Machine Learning. ChamSpringer1Joaquin Vanschoren. "Meta-learning". In: Automated Machine Learning. Springer, Cham, 2019, pp. 35-61 (cit. on p. 1).

Self-supervised pretraining of visual features in the wild. Priya Goyal, arXiv:2103.01988arXiv preprintcit. on pp. 1, 12Priya Goyal et al. "Self-supervised pretraining of visual features in the wild". In: arXiv preprint arXiv:2103.01988 (2021) (cit. on pp. 1, 12).

Deep learning in remote sensing: A comprehensive review and list of resources. Xiao Xiang Zhu, IEEE Geoscience and Remote Sensing Magazine. 5cit. on pp. 1, 15Xiao Xiang Zhu et al. "Deep learning in remote sens- ing: A comprehensive review and list of resources". In: IEEE Geoscience and Remote Sensing Magazine 5.4 (2017), pp. 8-36 (cit. on pp. 1, 15, 18).

Self-supervised visual feature learning with deep neural networks: A survey. Longlong Jing, Yingli Tian, IEEE transactions on pattern analysis and machine intelligence. 1Longlong Jing and Yingli Tian. "Self-supervised visual feature learning with deep neural networks: A survey". In: IEEE transactions on pattern analysis and machine intelligence (2020) (cit. on p. 1).

A survey on contrastive selfsupervised learning. Ashish Jaiswal, Technologies 9. 1cit. on pp. 1, 3Ashish Jaiswal et al. "A survey on contrastive self- supervised learning". In: Technologies 9.1 (2021), p. 2 (cit. on pp. 1, 3).

Self-supervised learning: Generative or contrastive. Xiao Liu, IEEE Transactions on Knowledge and Data Engineering. cit. on pp. 1, 3Xiao Liu et al. "Self-supervised learning: Generative or contrastive". In: IEEE Transactions on Knowledge and Data Engineering (2021) (cit. on pp. 1, 3).

Review on selfsupervised image recognition using deep neural networks. Kriti Ohri, Mukesh Kumar, Knowledge-Based Systems. 2241Kriti Ohri and Mukesh Kumar. "Review on self- supervised image recognition using deep neural net- works". In: Knowledge-Based Systems 224 (2021), p. 107090 (cit. on p. 1).

Bigearthnet: A large-scale benchmark archive for remote sensing image understanding. Gencer Sumbul, IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2019. cit. on pp. 1, 19Gencer Sumbul et al. "Bigearthnet: A large-scale benchmark archive for remote sensing image under- standing". In: IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2019, pp. 5901-5904 (cit. on pp. 1, 19).

SEN12MS-A Curated Dataset of Georeferenced Multi-Spectral Sentinel-1/2 Imagery for Deep Learning and Data Fusion. Michael Schmitt, arXiv:1906.07789arXiv preprintcit. on pp. 1, 19Michael Schmitt et al. "SEN12MS-A Curated Dataset of Georeferenced Multi-Spectral Sentinel-1/2 Imagery for Deep Learning and Data Fusion". In: arXiv preprint arXiv:1906.07789 (2019) (cit. on pp. 1, 19).

So2Sat LCZ42: A benchmark dataset for global local climate zones classification. Xiao Xiang Zhu, arXiv:1912.12171arXiv preprintcit. on pp. 1, 19Xiao Xiang Zhu et al. "So2Sat LCZ42: A benchmark dataset for global local climate zones classification". In: arXiv preprint arXiv:1912.12171 (2019) (cit. on pp. 1, 19).

A comprehensive survey on transfer learning. Fuzhen Zhuang, Proceedings of the IEEE 109.1 (2020). the IEEE 109.1 (2020)Fuzhen Zhuang et al. "A comprehensive survey on transfer learning". In: Proceedings of the IEEE 109.1 (2020), pp. 43-76 (cit. on p. 2).

Deep residual learning for image recognition. Kaiming He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitioncit. on pp. 2, 11, 14, 21Kaiming He et al. "Deep residual learning for image recognition". In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2016, pp. 770-778 (cit. on pp. 2, 11, 14, 21).

An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, arXiv:2010.11929arXiv preprintcit. on pp. 2, 11, 14, 21Alexey Dosovitskiy et al. "An image is worth 16x16 words: Transformers for image recognition at scale". In: arXiv preprint arXiv:2010.11929 (2020) (cit. on pp. 2, 11, 14, 21).

Semisupervised selflearning for hyperspectral image classification. Inmaculada Dópido, IEEE transactions on geoscience and remote sensing. 51citInmaculada Dópido et al. "Semisupervised self- learning for hyperspectral image classification". In: IEEE transactions on geoscience and remote sensing 51.7 (2013), pp. 4032-4044 (cit. on p. 2).

Semisupervised neural networks for efficient hyperspectral image classification. Frédéric Ratle, Gustavo Camps-Valls, Jason Weston, IEEE Transactions on Geoscience and Remote Sensing. 48citFrédéric Ratle, Gustavo Camps-Valls, and Jason We- ston. "Semisupervised neural networks for efficient hyperspectral image classification". In: IEEE Transac- tions on Geoscience and Remote Sensing 48.5 (2010), pp. 2271-2282 (cit. on p. 2).

Self-supervised low-rank representation (SSLRR) for hyperspectral image classification. Yuebin Wang, IEEE Transactions on Geoscience and Remote Sensing. 562citYuebin Wang et al. "Self-supervised low-rank rep- resentation (SSLRR) for hyperspectral image classi- fication". In: IEEE Transactions on Geoscience and Remote Sensing 56.10 (2018), pp. 5658-5672 (cit. on p. 2).

Wu-net: A weakly-supervised unmixing network for remotely sensed hyperspectral imagery. Danfeng Hong, IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium. IEEEcitDanfeng Hong et al. "Wu-net: A weakly-supervised unmixing network for remotely sensed hyperspectral imagery". In: IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2019, pp. 373-376 (cit. on p. 2).

Learning to propagate labels on graphs: An iterative multitask regression framework for semi-supervised hyperspectral dimensionality reduction. Danfeng Hong, ISPRS journal of photogrammetry and remote sensing. 158citDanfeng Hong et al. "Learning to propagate labels on graphs: An iterative multitask regression framework for semi-supervised hyperspectral dimensionality re- duction". In: ISPRS journal of photogrammetry and remote sensing 158 (2019), pp. 35-49 (cit. on p. 2).

Joint and progressive subspace analysis (JPSA) with spatial-spectral manifold alignment for semisupervised hyperspectral dimensionality reduction. Danfeng Hong, IEEE Transactions on Cybernetics. 2citDanfeng Hong et al. "Joint and progressive subspace analysis (JPSA) with spatial-spectral manifold align- ment for semisupervised hyperspectral dimensional- ity reduction". In: IEEE Transactions on Cybernetics (2020) (cit. on p. 2).

SDFL-FC: Semisupervised Deep Feature Learning With Feature Consistency for Hyperspectral Image Classification. Yun Cao, IEEE Transactions on Geoscience and Remote Sensing. 2citYun Cao et al. "SDFL-FC: Semisupervised Deep Fea- ture Learning With Feature Consistency for Hyper- spectral Image Classification". In: IEEE Transactions on Geoscience and Remote Sensing (2020) (cit. on p. 2).

3D convolutional siamese network for few-shot hyperspectral classification. Zeyu Cao, Journal of Applied Remote Sensing. 14248504Zeyu Cao et al. "3D convolutional siamese network for few-shot hyperspectral classification". In: Journal of Applied Remote Sensing 14.4 (2020), p. 048504 (cit. on p. 2).

Deep Self-Supervised Learning for Few-Shot Hyperspectral Image Classification. Yu Li, IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2020. citYu Li et al. "Deep Self-Supervised Learning for Few- Shot Hyperspectral Image Classification". In: IGARSS 2020-2020 IEEE International Geoscience and Re- mote Sensing Symposium. IEEE. 2020, pp. 501-504 (cit. on p. 2).

Self-Supervised Learning With Adaptive Distillation for Hyperspectral Image Classification. Jun Yue, IEEE Transactions on Geoscience and Remote Sensing. citJun Yue et al. "Self-Supervised Learning With Adap- tive Distillation for Hyperspectral Image Classifica- tion". In: IEEE Transactions on Geoscience and Re- mote Sensing (2021) (cit. on p. 2).

CoSpace: Common subspace learning from hyperspectral-multispectral correspondences. Danfeng Hong, IEEE Transactions on Geoscience and Remote Sensing. 573citDanfeng Hong et al. "CoSpace: Common subspace learning from hyperspectral-multispectral correspon- dences". In: IEEE Transactions on Geoscience and Remote Sensing 57.7 (2019), pp. 4349-4359 (cit. on p. 3).

Learnable manifold alignment (LeMA): A semi-supervised cross-modality learning framework for land cover and land use classification. Danfeng Hong, ISPRS journal of photogrammetry and remote sensing. 147cit. on p. 3Danfeng Hong et al. "Learnable manifold alignment (LeMA): A semi-supervised cross-modality learning framework for land cover and land use classification". In: ISPRS journal of photogrammetry and remote sensing 147 (2019), pp. 193-205 (cit. on p. 3).

X-ModalNet: A semi-supervised deep cross-modal network for classification of remote sensing data. Danfeng Hong, ISPRS Journal of Photogrammetry and Remote Sensing. 1673citDanfeng Hong et al. "X-ModalNet: A semi-supervised deep cross-modal network for classification of remote sensing data". In: ISPRS Journal of Photogrammetry and Remote Sensing 167 (2020), pp. 12-23 (cit. on p. 3).

Unsupervised Domain Adaptation Using a Teacher-Student Network for Cross-City Classification of SENTINEL-2 Images. Jingliang Hu, Lichao Mou, Xiao Xiang Zhu, The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences. 43cit. on p. 3Jingliang Hu, Lichao Mou, and Xiao Xiang Zhu. "Unsupervised Domain Adaptation Using a Teacher- Student Network for Cross-City Classification of SENTINEL-2 Images". In: The International Archives of Photogrammetry, Remote Sensing and Spatial In- formation Sciences 43 (2020), pp. 1569-1574 (cit. on p. 3).

Semantic segmentation of remote sensing images with sparse annotations. Yuansheng Hua, IEEE Geoscience and Remote Sensing Letters. 3Yuansheng Hua et al. "Semantic segmentation of re- mote sensing images with sparse annotations". In: IEEE Geoscience and Remote Sensing Letters (2021) (cit. on p. 3).

Aerial scene understanding in the wild: Multi-scene recognition via prototype-based memory networks. Yuansheng Hua, ISPRS Journal of Photogrammetry and Remote Sensing. 1773citYuansheng Hua et al. "Aerial scene understanding in the wild: Multi-scene recognition via prototype-based memory networks". In: ISPRS Journal of Photogram- metry and Remote Sensing 177 (2021), pp. 89-102 (cit. on p. 3).

Semisupervised change detection using graph convolutional network. Sudipan Saha, IEEE Geoscience and Remote Sensing Letters. 183citSudipan Saha et al. "Semisupervised change detec- tion using graph convolutional network". In: IEEE Geoscience and Remote Sensing Letters 18.4 (2020), pp. 607-611 (cit. on p. 3).

MIMA: MAPPER-induced manifold alignment for semi-supervised fusion of optical image and polarimetric SAR data. Jingliang Hu, Danfeng Hong, Xiao Xiang Zhu, IEEE Transactions on Geoscience and Remote Sensing. 57cit. on p. 3Jingliang Hu, Danfeng Hong, and Xiao Xiang Zhu. "MIMA: MAPPER-induced manifold alignment for semi-supervised fusion of optical image and polarimet- ric SAR data". In: IEEE Transactions on Geoscience and Remote Sensing 57.11 (2019), pp. 9025-9040 (cit. on p. 3).

Principal component analysis. Hervé Abdi, Lynne J Williams, Wiley interdisciplinary reviews: computational statistics. 23Hervé Abdi and Lynne J Williams. "Principal com- ponent analysis". In: Wiley interdisciplinary reviews: computational statistics 2.4 (2010), pp. 433-459 (cit. on p. 3).

Unsupervised feature learning for aerial scene classification. M Anil, Cheriyadat, IEEE Transactions on Geoscience and Remote Sensing. 523citAnil M Cheriyadat. "Unsupervised feature learning for aerial scene classification". In: IEEE Transactions on Geoscience and Remote Sensing 52.1 (2013), pp. 439- 451 (cit. on p. 3).

Data clustering: a review. Anil K Jain, Patrick J Narasimha Murty, Flynn, ACM computing surveys (CSUR). 313citAnil K Jain, M Narasimha Murty, and Patrick J Flynn. "Data clustering: a review". In: ACM computing sur- veys (CSUR) 31.3 (1999), pp. 264-323 (cit. on p. 3).

Unsupervised multilayer feature learning for satellite image scene classification. Yansheng Li, IEEE Geoscience and Remote Sensing Letters. 133citYansheng Li et al. "Unsupervised multilayer feature learning for satellite image scene classification". In: IEEE Geoscience and Remote Sensing Letters 13.2 (2016), pp. 157-161 (cit. on p. 3).

Unsupervised deep feature extraction for remote sensing image classification. Adriana Romero, Carlo Gatta, Gustau Camps-Valls, IEEE Transactions on Geoscience and Remote Sensing. 543citAdriana Romero, Carlo Gatta, and Gustau Camps- Valls. "Unsupervised deep feature extraction for re- mote sensing image classification". In: IEEE Transac- tions on Geoscience and Remote Sensing 54.3 (2015), pp. 1349-1362 (cit. on p. 3).

Multiobjective genetic clustering for pixel classification in remote sensing imagery. Sanghamitra Bandyopadhyay, Ujjwal Maulik, Anirban Mukhopadhyay, IEEE transactions on Geoscience and Remote Sensing. 453citSanghamitra Bandyopadhyay, Ujjwal Maulik, and Anirban Mukhopadhyay. "Multiobjective genetic clus- tering for pixel classification in remote sensing im- agery". In: IEEE transactions on Geoscience and Remote Sensing 45.5 (2007), pp. 1506-1511 (cit. on p. 3).

Single point iterative weighted fuzzy C-means clustering algorithm for remote sensing image segmentation. Jianchao Fan, Min Han, Jun Wang, Pattern Recognition. 423citJianchao Fan, Min Han, and Jun Wang. "Single point iterative weighted fuzzy C-means clustering algorithm for remote sensing image segmentation". In: Pattern Recognition 42.11 (2009), pp. 2527-2540 (cit. on p. 3).

Unsupervised deep joint segmentation of multitemporal high-resolution images. Sudipan Saha, IEEE Transactions on Geoscience and Remote Sensing. 583citSudipan Saha et al. "Unsupervised deep joint segmen- tation of multitemporal high-resolution images". In: IEEE Transactions on Geoscience and Remote Sensing 58.12 (2020), pp. 8780-8792 (cit. on p. 3).

Fuzzy clustering algorithms for unsupervised change detection in remote sensing images. Ashish Ghosh, Niladri Shekhar Mishra, Susmita Ghosh, Information Sciences. 1813citAshish Ghosh, Niladri Shekhar Mishra, and Susmita Ghosh. "Fuzzy clustering algorithms for unsupervised change detection in remote sensing images". In: In- formation Sciences 181.4 (2011), pp. 699-715 (cit. on p. 3).

Use of principal component analysis (PCA) of remote sensing images in wetland change detection on the Kafue Flats, Zambia. Christopher Munyati, Geocarto International. 193citChristopher Munyati. "Use of principal component analysis (PCA) of remote sensing images in wetland change detection on the Kafue Flats, Zambia". In: Geocarto International 19.3 (2004), pp. 11-22 (cit. on p. 3).

Novel folded-PCA for improved feature extraction and data reduction with hyperspectral imaging and SAR in remote sensing. Jaime Zabalza, ISPRS Journal of Photogrammetry and Remote Sensing. 93cit. on p. 3Jaime Zabalza et al. "Novel folded-PCA for improved feature extraction and data reduction with hyperspec- tral imaging and SAR in remote sensing". In: ISPRS Journal of Photogrammetry and Remote Sensing 93 (2014), pp. 112-122 (cit. on p. 3).

A remote sensing image fusion method based on PCA transform and wavelet packet transform. Wen Cao, Bicheng Li, Yong Zhang, International Conference on Neural Networks and Signal Processing. 2Proceedings of the 2003. cit. on p. 3Wen Cao, Bicheng Li, and Yong Zhang. "A remote sensing image fusion method based on PCA transform and wavelet packet transform". In: International Con- ference on Neural Networks and Signal Processing, 2003. Proceedings of the 2003. Vol. 2. IEEE. 2003, pp. 976-981 (cit. on p. 3).

Unsupervised representation learning by predicting image rotations. Spyros Gidaris, Praveer Singh, Nikos Komodakis, International Conference on Learning Representations (ICLR). cit. on pp. 3, 4, 6, 14Spyros Gidaris, Praveer Singh, and Nikos Komodakis. "Unsupervised representation learning by predicting image rotations". In: International Conference on Learning Representations (ICLR). 2018 (cit. on pp. 3, 4, 6, 14).

Boosting self-supervised learning via knowledge transfer. Mehdi Noroozi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognitioncit. on pp. 3, 6)Mehdi Noroozi et al. "Boosting self-supervised learn- ing via knowledge transfer". In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018, pp. 9359-9367 (cit. on pp. 3, 6).

Deep clustering for unsupervised learning of visual features. Mathilde Caron, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)cit. on pp. 3, 4, 12, 14Mathilde Caron et al. "Deep clustering for unsuper- vised learning of visual features". In: Proceedings of the European Conference on Computer Vision (ECCV). 2018, pp. 132-149 (cit. on pp. 3, 4, 12, 14).

Emerging properties in selfsupervised vision transformers. Mathilde Caron, arXiv:2104.14294arXiv preprintcit. on pp. 3, 4, 13, 14, 21Mathilde Caron et al. "Emerging properties in self- supervised vision transformers". In: arXiv preprint arXiv:2104.14294 (2021) (cit. on pp. 3, 4, 13, 14, 21).

XCiT: Cross-Covariance Image Transformers. Alaaeldin El-Nouby, arXiv:2106.096813arXiv preprintAlaaeldin El-Nouby et al. "XCiT: Cross- Covariance Image Transformers". In: arXiv preprint arXiv:2106.09681 (2021) (cit. on p. 3).

Modular learning in neural networks. H Dana, Ballard, In: AAAI. 647cit. on p. 4)Dana H Ballard. "Modular learning in neural net- works." In: AAAI. Vol. 647. 1987, pp. 279-284 (cit. on p. 4).

Sparse autoencoder. Andrew Ng, CS294A Lecture notes 72.2011cit. on p. 4)Andrew Ng et al. "Sparse autoencoder". In: CS294A Lecture notes 72.2011 (2011), pp. 1-19 (cit. on p. 4).

Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Pascal Vincent, In: Journal of machine learning research. 11cit. on p. 4)Pascal Vincent et al. "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion." In: Journal of machine learning research 11.12 (2010) (cit. on p. 4).

Auto-encoding variational bayes. P Diederik, Max Kingma, Welling, arXiv:1312.6114arXiv preprintcit. on pp. 4, 5Diederik P Kingma and Max Welling. "Auto-encoding variational bayes". In: arXiv preprint arXiv:1312.6114 (2013) (cit. on pp. 4, 5).

Masked Autoencoders Are Scalable Vision Learners. Kaiming He, arXiv:2111.06377arXiv preprintcit. on pp. 4, 5, 14, 21Kaiming He et al. "Masked Autoencoders Are Scalable Vision Learners". In: arXiv preprint arXiv:2111.06377 (2021) (cit. on pp. 4, 5, 14, 21).

Generative adversarial nets. Ian Goodfellow, Advances in neural information processing systems. 27cit. on pp. 4, 5Ian Goodfellow et al. "Generative adversarial nets". In: Advances in neural information processing systems 27 (2014) (cit. on pp. 4, 5).

Adversarial autoencoders. Alireza Makhzani, arXiv:1511.05644arXiv preprintcit. on pp. 4, 5Alireza Makhzani et al. "Adversarial autoencoders". In: arXiv preprint arXiv:1511.05644 (2015) (cit. on pp. 4, 5).

Adversarial feature learning. Jeff Donahue, Philipp Krähenbühl, Trevor Darrell, arXiv:1605.09782arXiv preprintcit. on pp. 4, 5Jeff Donahue, Philipp Krähenbühl, and Trevor Dar- rell. "Adversarial feature learning". In: arXiv preprint arXiv:1605.09782 (2016) (cit. on pp. 4, 5).

Unsupervised visual representation learning by context prediction. Carl Doersch, Abhinav Gupta, Alexei A Efros, Proceedings of the IEEE interna. the IEEE internacit. on pp. 4, 6, 8, 14Carl Doersch, Abhinav Gupta, and Alexei A Efros. "Unsupervised visual representation learning by con- text prediction". In: Proceedings of the IEEE interna- tional conference on computer vision. 2015, pp. 1422- 1430 (cit. on pp. 4, 6, 8, 14).

Unsupervised learning of visual representations by solving jigsaw puzzles. Mehdi Noroozi, Paolo Favaro, European conference on computer vision. Mehdi Noroozi and Paolo Favaro. "Unsupervised learning of visual representations by solving jigsaw puzzles". In: European conference on computer vision.

. Springer, cit. on pp. 4, 6, 14Springer. 2016, pp. 69-84 (cit. on pp. 4, 6, 14).

Context encoders: Feature learning by inpainting. Deepak Pathak, Proceedings of the IEEE. the IEEEcit. on pp. 4, 6Deepak Pathak et al. "Context encoders: Feature learn- ing by inpainting". In: Proceedings of the IEEE con- ference on computer vision and pattern recognition. 2016, pp. 2536-2544 (cit. on pp. 4, 6).

Colorful image colorization". In: European conference on computer vision. Richard Zhang, Phillip Isola, Alexei A Efros, Springercit. on pp. 4, 6, 8, 14Richard Zhang, Phillip Isola, and Alexei A Efros. "Colorful image colorization". In: European confer- ence on computer vision. Springer. 2016, pp. 649-666 (cit. on pp. 4, 6, 8, 14).

Selfsupervised video representation learning with spacetime cubic puzzles. Dahun Kim, Donghyeon Cho, In So Kweon, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33cit. on pp. 4, 7Dahun Kim, Donghyeon Cho, and In So Kweon. "Self- supervised video representation learning with space- time cubic puzzles". In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019, pp. 8545-8552 (cit. on pp. 4, 7).

Representation learning by learning to count. Mehdi Noroozi, Hamed Pirsiavash, Paolo Favaro, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Visioncit. on pp. 4, 8, 9Mehdi Noroozi, Hamed Pirsiavash, and Paolo Favaro. "Representation learning by learning to count". In: Proceedings of the IEEE International Conference on Computer Vision. 2017, pp. 5898-5906 (cit. on pp. 4, 8, 9).

Self-supervised feature learning by learning to spot artifacts. Simon Jenni, Paolo Favaro, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognitioncit. on pp. 4, 8Simon Jenni and Paolo Favaro. "Self-supervised fea- ture learning by learning to spot artifacts". In: Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018, pp. 2733-2742 (cit. on pp. 4, 8).

Ambient sound provides supervision for visual learning. Andrew Owens, European conference on computer vision. Springercit. on pp. 4, 8Andrew Owens et al. "Ambient sound provides super- vision for visual learning". In: European conference on computer vision. Springer. 2016, pp. 801-816 (cit. on pp. 4, 8).

Learning a similarity metric discriminatively, with application to face verification. Sumit Chopra, Raia Hadsell, Yann Lecun, 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). 1cit. on pp. 4, 9Sumit Chopra, Raia Hadsell, and Yann LeCun. "Learn- ing a similarity metric discriminatively, with applica- tion to face verification". In: 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). Vol. 1. IEEE. 2005, pp. 539- 546 (cit. on pp. 4, 9).

Representation learning with contrastive predictive coding. Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.0374814arXiv preprintcit. on pp. 4, 9-11Aaron van den Oord, Yazhe Li, and Oriol Vinyals. "Representation learning with contrastive predictive coding". In: arXiv preprint arXiv:1807.03748 (2018) (cit. on pp. 4, 9-11, 14).

Learning deep representations by mutual information estimation and maximization. R Devon Hjelm, arXiv:1808.06670arXiv preprintcit. on pp. 4, 10R Devon Hjelm et al. "Learning deep representations by mutual information estimation and maximization". In: arXiv preprint arXiv:1808.06670 (2018) (cit. on pp. 4, 10).

Unsupervised feature learning via non-parametric instance discrimination. Zhirong Wu, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitioncit. on pp. 4, 10, 14Zhirong Wu et al. "Unsupervised feature learning via non-parametric instance discrimination". In: Pro- ceedings of the IEEE conference on computer vision and pattern recognition. 2018, pp. 3733-3742 (cit. on pp. 4, 10, 14).

Selfsupervised learning of pretext-invariant representations. Ishan Misra, Laurens Van Der Maaten, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020cit. on pp. 4, 10, 14Ishan Misra and Laurens van der Maaten. "Self- supervised learning of pretext-invariant representa- tions". In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020, pp. 6707-6717 (cit. on pp. 4, 10, 14).

Momentum contrast for unsupervised visual representation learning. Kaiming He, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 20204cit. on pp.Kaiming He et al. "Momentum contrast for unsuper- vised visual representation learning". In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020, pp. 9729-9738 (cit. on pp. 4, 10-14, 17, 20).

A simple framework for contrastive learning of visual representations. Ting Chen, PMLR. 2020International conference on machine learning. cit. on pp. 4, 11, 12, 14, 20Ting Chen et al. "A simple framework for con- trastive learning of visual representations". In: Interna- tional conference on machine learning. PMLR. 2020, pp. 1597-1607 (cit. on pp. 4, 11, 12, 14, 20).

An empirical study of training self-supervised vision transformers. Xinlei Chen, Saining Xie, Kaiming He, arXiv:2104.02057arXiv preprintcit. on pp. 4, 11, 14, 21Xinlei Chen, Saining Xie, and Kaiming He. "An em- pirical study of training self-supervised vision trans- formers". In: arXiv preprint arXiv:2104.02057 (2021) (cit. on pp. 4, 11, 14, 21).

Improved baselines with momentum contrastive learning. Xinlei Chen, arXiv:2003.04297arXiv preprintcit. on pp. 4, 11, 12, 14, 19, 20Xinlei Chen et al. "Improved baselines with mo- mentum contrastive learning". In: arXiv preprint arXiv:2003.04297 (2020) (cit. on pp. 4, 11, 12, 14, 19, 20).

Local aggregation for unsupervised learning of visual embeddings. Chengxu Zhuang, Alex Lin Zhai, Daniel Yamins, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Visioncit. on pp. 4, 12, 14Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. "Local aggregation for unsupervised learning of vi- sual embeddings". In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019, pp. 6002-6012 (cit. on pp. 4, 12, 14).

Self-labelling via simultaneous clustering and representation learning. Yuki Markus Asano, Christian Rupprecht, Andrea Vedaldi, arXiv:1911.05371arXiv preprintcit. on pp. 4, 12Yuki Markus Asano, Christian Rupprecht, and An- drea Vedaldi. "Self-labelling via simultaneous clus- tering and representation learning". In: arXiv preprint arXiv:1911.05371 (2019) (cit. on pp. 4, 12).

Unsupervised learning of visual features by contrasting cluster assignments. Mathilde Caron, arXiv:2006.09882arXiv preprintcit. on pp. 4, 12, 14, 19Mathilde Caron et al. "Unsupervised learning of visual features by contrasting cluster assignments". In: arXiv preprint arXiv:2006.09882 (2020) (cit. on pp. 4, 12, 14, 19).

Prototypical contrastive learning of unsupervised representations. Junnan Li, arXiv:2005.04966arXiv preprintcit. on pp. 4, 12Junnan Li et al. "Prototypical contrastive learning of unsupervised representations". In: arXiv preprint arXiv:2005.04966 (2020) (cit. on pp. 4, 12).

Bootstrap your own latent: A new approach to self-supervised learning. Jean-Bastien Grill, arXiv:2006.07733arXiv preprintcit. on pp. 4, 13, 14Jean-Bastien Grill et al. "Bootstrap your own latent: A new approach to self-supervised learning". In: arXiv preprint arXiv:2006.07733 (2020) (cit. on pp. 4, 13, 14).

Exploring simple siamese representation learning. Xinlei Chen, Kaiming He, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021. the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 202114cit. on pp. 4, 13Xinlei Chen and Kaiming He. "Exploring simple siamese representation learning". In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021, pp. 15750-15758 (cit. on pp. 4, 13, 14, 19-21).

Efficient Self-supervised Vision Transformers for Representation Learning. Chunyuan Li, arXiv:2106.09785arXiv preprintcit. on pp. 4, 13, 14, 21Chunyuan Li et al. "Efficient Self-supervised Vision Transformers for Representation Learning". In: arXiv preprint arXiv:2106.09785 (2021) (cit. on pp. 4, 13, 14, 21).

Jinghao Zhou, arXiv:2111.07832Image BERT Pre-Training with Online Tokenizer. 2021. cs.CV] (cit. on pp. 4, 13, 14, 21Jinghao Zhou et al. iBOT: Image BERT Pre-Training with Online Tokenizer. 2021. arXiv: 2111 . 07832 [cs.CV] (cit. on pp. 4, 13, 14, 21).

Barlow twins: Self-supervised learning via redundancy reduction. Jure Zbontar, arXiv:2103.032304arXiv preprintcit. on pp.Jure Zbontar et al. "Barlow twins: Self-supervised learning via redundancy reduction". In: arXiv preprint arXiv:2103.03230 (2021) (cit. on pp. 4, 13-15, 19).

Vicreg: Variance-invariance-covariance regularization for self-supervised learning. Adrien Bardes, Jean Ponce, Yann Lecun, arXiv:2105.04906arXiv preprintcit. on pp. 4, 14, 15Adrien Bardes, Jean Ponce, and Yann LeCun. "Vicreg: Variance-invariance-covariance regularization for self-supervised learning". In: arXiv preprint arXiv:2105.04906 (2021) (cit. on pp. 4, 14, 15).

Principal component analysis. Svante Wold, Kim Esbensen, Paul Geladi, Chemometrics and intelligent laboratory systems. 2cit. on p. 4)Svante Wold, Kim Esbensen, and Paul Geladi. "Prin- cipal component analysis". In: Chemometrics and in- telligent laboratory systems 2.1-3 (1987), pp. 37-52 (cit. on p. 4).

From principal subspaces to principal components with linear autoencoders. Elad Plaut, arXiv:1804.102534arXiv preprintElad Plaut. "From principal subspaces to princi- pal components with linear autoencoders". In: arXiv preprint arXiv:1804.10253 (2018) (cit. on p. 4).

Variational inference: A review for statisticians. M David, Alp Blei, Jon D Kucukelbir, Mcauliffe, Journal of the American statistical Association. 112cit. on p. 4)David M Blei, Alp Kucukelbir, and Jon D McAuliffe. "Variational inference: A review for statisticians". In: Journal of the American statistical Association 112.518 (2017), pp. 859-877 (cit. on p. 4).

Remote sensing scene classification by unsupervised representation learning. Xiaoqiang Lu, Xiangtao Zheng, Yuan Yuan, IEEE Transactions on Geoscience and Remote Sensing. 55cit. on pp. 5, 16Xiaoqiang Lu, Xiangtao Zheng, and Yuan Yuan. "Re- mote sensing scene classification by unsupervised rep- resentation learning". In: IEEE Transactions on Geo- science and Remote Sensing 55.9 (2017), pp. 5148- 5157 (cit. on pp. 5, 16, 18).

Change detection based on deep feature representation and mapping transformation for multi-spatial-resolution remote sensing images. Puzhao Zhang, ISPRS Journal of Photogrammetry and Remote Sensing. 116cit. on pp. 5, 16Puzhao Zhang et al. "Change detection based on deep feature representation and mapping transformation for multi-spatial-resolution remote sensing images". In: ISPRS Journal of Photogrammetry and Remote Sens- ing 116 (2016), pp. 24-41 (cit. on pp. 5, 16, 18).

Self-Supervised Learning of Satellite-Derived Vegetation Indices for Clustering and Visualization of Vegetation Types. C Ram, Keitarou Sharma, Hara, Journal of Imaging. 725Ram C Sharma and Keitarou Hara. "Self-Supervised Learning of Satellite-Derived Vegetation Indices for Clustering and Visualization of Vegetation Types". In: Journal of Imaging 7.2 (2021), p. 30 (cit. on p. 5).

Self-supervised monocular depth estimation from oblique UAV videos. Logambal Madhuanand, Francesco Nex, Michael Ying Yang, ISPRS Journal of Photogrammetry and Remote Sensing. 176cit. on pp. 5, 16Logambal Madhuanand, Francesco Nex, and Michael Ying Yang. "Self-supervised monocular depth estima- tion from oblique UAV videos". In: ISPRS Journal of Photogrammetry and Remote Sensing 176 (2021), pp. 1-14 (cit. on pp. 5, 16, 18).

Urban Flood Mapping With Bitemporal Multispectral Imagery Via a Self-Supervised Learning Framework. Bo Peng, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 14cit. on p. 5Bo Peng et al. "Urban Flood Mapping With Bitem- poral Multispectral Imagery Via a Self-Supervised Learning Framework". In: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 14 (2020), pp. 2001-2016 (cit. on p. 5).

Self-Supervised Multi-Image Super-Resolution for Push-Frame Satellite Images. Ngoc Long Nguyen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognitioncit. on p. 5Ngoc Long Nguyen et al. "Self-Supervised Multi- Image Super-Resolution for Push-Frame Satellite Im- ages". In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021, pp. 1121-1131 (cit. on p. 5).

Heterogeneous change detection with self-supervised deep canonically correlated autoencoders. F Figari Tomenotti, IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2020. cit. on pp. 5, 18F Figari Tomenotti et al. "Heterogeneous change detection with self-supervised deep canonically cor- related autoencoders". In: IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Sympo- sium. IEEE. 2020, pp. 680-683 (cit. on pp. 5, 18).

Blind hyperspectral unmixing using autoencoders: A critical comparison. Burkni Palsson, Magnus O Johannes R Sveinsson, Ulfarsson, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 15cit. on pp. 5, 18Burkni Palsson, Johannes R Sveinsson, and Magnus O Ulfarsson. "Blind hyperspectral unmixing using au- toencoders: A critical comparison". In: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 15 (2022), pp. 1340-1372 (cit. on pp. 5, 18).

Self-taught feature learning for hyperspectral image classification. Ronald Kemker, Christopher Kanan, IEEE Transactions on Geoscience and Remote Sensing. 55cit. on pp. 5, 18Ronald Kemker and Christopher Kanan. "Self-taught feature learning for hyperspectral image classifica- tion". In: IEEE Transactions on Geoscience and Re- mote Sensing 55.5 (2017), pp. 2693-2705 (cit. on pp. 5, 18).

Unsupervised spectral-spatial feature learning via deep residual Conv-Deconv network for hyperspectral image classification. Lichao Mou, Pedram Ghamisi, Xiao Xiang Zhu, IEEE Transactions on Geoscience and Remote Sensing. 56cit. on pp. 5, 16Lichao Mou, Pedram Ghamisi, and Xiao Xiang Zhu. "Unsupervised spectral-spatial feature learning via deep residual Conv-Deconv network for hyperspectral image classification". In: IEEE Transactions on Geo- science and Remote Sensing 56.1 (2017), pp. 391-406 (cit. on pp. 5, 16, 18).

Self-supervised Hyperspectral Image Restoration using Separable Image Prior. Ryuji Imamura, Tatsuki Itasaka, Masahiro Okuda, arXiv:1907.0065118arXiv preprintcit. on pp. 5, 16Ryuji Imamura, Tatsuki Itasaka, and Masahiro Okuda. "Self-supervised Hyperspectral Image Restoration us- ing Separable Image Prior". In: arXiv preprint arXiv:1907.00651 (2019) (cit. on pp. 5, 16, 18).

Endmember-Guided Unmixing Network (EGU-Net): A General Deep Learning Framework for Self-Supervised Hyperspectral Unmixing. Danfeng Hong, IEEE Transactions on Neural Networks and Learning Systems. cit. on pp. 5, 16Danfeng Hong et al. "Endmember-Guided Unmix- ing Network (EGU-Net): A General Deep Learning Framework for Self-Supervised Hyperspectral Unmix- ing". In: IEEE Transactions on Neural Networks and Learning Systems (2021) (cit. on pp. 5, 16).

Cross-attention in coupled unmixing nets for unsupervised hyperspectral superresolution. Jing Yao, European Conference on Computer Vision. Springer18cit. on pp. 5, 16Jing Yao et al. "Cross-attention in coupled un- mixing nets for unsupervised hyperspectral super- resolution". In: European Conference on Computer Vision. Springer. 2020, pp. 208-224 (cit. on pp. 5, 16, 18).

Hyperspectral image super-resolution with selfsupervised spectral-spatial residual network. Wenjing Chen, Xiangtao Zheng, Xiaoqiang Lu, Remote Sensing. 131260cit. on pp. 5, 18Wenjing Chen, Xiangtao Zheng, and Xiaoqiang Lu. "Hyperspectral image super-resolution with self- supervised spectral-spatial residual network". In: Re- mote Sensing 13.7 (2021), p. 1260 (cit. on pp. 5, 18).

Self-Supervised Deep Subspace Clustering for Hyperspectral Images With Adaptive Self-Expressive Coefficient Matrix Initialization. Kun Li, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 14cit. on pp. 5, 16Kun Li et al. "Self-Supervised Deep Subspace Clus- tering for Hyperspectral Images With Adaptive Self- Expressive Coefficient Matrix Initialization". In: IEEE Journal of Selected Topics in Applied Earth Observa- tions and Remote Sensing 14 (2021), pp. 3215-3227 (cit. on pp. 5, 16, 18).

Endnet: Sparse autoencoder network for endmember extraction and hyperspectral unmixing. Savas Ozkan, Berk Kaya, Gozde Bozdagi Akar, IEEE Transactions on Geoscience and Remote Sensing. 57cit. on p. 5Savas Ozkan, Berk Kaya, and Gozde Bozdagi Akar. "Endnet: Sparse autoencoder network for endmem- ber extraction and hyperspectral unmixing". In: IEEE Transactions on Geoscience and Remote Sensing 57.1 (2018), pp. 482-496 (cit. on p. 5).

Mining hard negative samples for SARoptical image matching using generative adversarial networks. Lloyd Haydn, Michael Hughes, Xiao Xiang Schmitt, Zhu, Remote Sensing. 101552cit. on pp. 5, 16Lloyd Haydn Hughes, Michael Schmitt, and Xiao Xi- ang Zhu. "Mining hard negative samples for SAR- optical image matching using generative adversarial networks". In: Remote Sensing 10.10 (2018), p. 1552 (cit. on pp. 5, 16).

Speckle2Void: Deep self-supervised SAR despeckling with blind-spot convolutional neural networks. Andrea Bordone Molini, IEEE Transactions on Geoscience and Remote Sensing. 18cit. on pp. 5, 16Andrea Bordone Molini et al. "Speckle2Void: Deep self-supervised SAR despeckling with blind-spot con- volutional neural networks". In: IEEE Transactions on Geoscience and Remote Sensing (2021) (cit. on pp. 5, 16, 18).

Blind SAR image despeckling using self-supervised dense dilated convolutional neural network. Ye Yuan, Jian Guan, Jianguo Sun, arXiv:1908.0160818arXiv preprintcit. on pp. 5, 16Ye Yuan, Jian Guan, and Jianguo Sun. "Blind SAR image despeckling using self-supervised dense di- lated convolutional neural network". In: arXiv preprint arXiv:1908.01608 (2019) (cit. on pp. 5, 16, 18).

SAE-net: A deep neural network for SAR autofocus. Wei Pu, IEEE Transactions on Geoscience and Remote Sensing. 60cit. on p. 5Wei Pu. "SAE-net: A deep neural network for SAR autofocus". In: IEEE Transactions on Geoscience and Remote Sensing 60 (2022), pp. 1-14 (cit. on p. 5).

A Self-Supervised Denoising Network for Satellite-Airborne-Ground Hyperspectral Imagery. Xinyu Wang, 10.1109/TGRS.2021.3064429DOI: 10.1109/ TGRS.2021.3064429IEEE Transactions on Geoscience and Remote Sensing. 605Xinyu Wang et al. "A Self-Supervised Denoising Network for Satellite-Airborne-Ground Hyperspectral Imagery". In: IEEE Transactions on Geoscience and Remote Sensing 60 (2022), pp. 1-16. DOI: 10.1109/ TGRS.2021.3064429 (cit. on p. 5).

Hyperspectral Image Restoration With Self-Supervised Learning: A Two-Stage Training Approach. Yuntao Qian, IEEE Transactions on Geoscience and Remote Sensing. 60cit. on p. 5Yuntao Qian et al. "Hyperspectral Image Restoration With Self-Supervised Learning: A Two-Stage Training Approach". In: IEEE Transactions on Geoscience and Remote Sensing 60 (2021), pp. 1-17 (cit. on p. 5).

Unsupervised representation learning with deep convolutional generative adversarial networks. Alec Radford, Luke Metz, Soumith Chintala, arXiv:1511.06434arXiv preprintcit. on p. 5Alec Radford, Luke Metz, and Soumith Chintala. "Unsupervised representation learning with deep con- volutional generative adversarial networks". In: arXiv preprint arXiv:1511.06434 (2015) (cit. on p. 5).

Robust and adaptive network flows. Dimitris Bertsimas, Ebrahim Nasrabadi, Sebastian Stiller, Operations Research. 61cit. on p. 5Dimitris Bertsimas, Ebrahim Nasrabadi, and Sebastian Stiller. "Robust and adaptive network flows". In: Op- erations Research 61.5 (2013), pp. 1218-1242 (cit. on p. 5).

Large scale adversarial representation learning. Jeff Donahue, Karen Simonyan, arXiv:1907.02544arXiv preprintcit. on pp. 5, 14Jeff Donahue and Karen Simonyan. "Large scale ad- versarial representation learning". In: arXiv preprint arXiv:1907.02544 (2019) (cit. on pp. 5, 14).

Adversarially learned inference. Vincent Dumoulin, arXiv:1606.007045arXiv preprintVincent Dumoulin et al. "Adversarially learned in- ference". In: arXiv preprint arXiv:1606.00704 (2016) (cit. on p. 5).

Generative adversarial networks for hyperspectral image classification. Lin Zhu, IEEE Transactions on Geoscience and Remote Sensing. 56cit. on p. 5Lin Zhu et al. "Generative adversarial networks for hyperspectral image classification". In: IEEE Transac- tions on Geoscience and Remote Sensing 56.9 (2018), pp. 5046-5063 (cit. on p. 5).

Adversarial autoencoder network for hyperspectral unmixing. Qiwen Jin, IEEE Transactions on Neural Networks and Learning Systems. 5Qiwen Jin et al. "Adversarial autoencoder network for hyperspectral unmixing". In: IEEE Transactions on Neural Networks and Learning Systems (2021) (cit. on p. 5).

S2-cGAN: Self-supervised adversarial representation learning for binary change detection in multispectral images. Jose Luis Holgado Alvarez, Mahdyar Ravanbakhsh, Begüm Demir, IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2020. 18cit. on pp. 5, 16Jose Luis Holgado Alvarez, Mahdyar Ravanbakhsh, and Begüm Demir. "S2-cGAN: Self-supervised adver- sarial representation learning for binary change detec- tion in multispectral images". In: IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2020, pp. 2515-2518 (cit. on pp. 5, 16, 18).

Self-Supervised Remote Sensing Image Retrieval. Kane Walter, J Matthew, Arcot Gibson, Sowmya, IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2020. 1816cit. on pp. 5, 12Kane Walter, Matthew J Gibson, and Arcot Sowmya. "Self-Supervised Remote Sensing Image Retrieval". In: IGARSS 2020-2020 IEEE International Geo- science and Remote Sensing Symposium. IEEE. 2020, pp. 1683-1686 (cit. on pp. 5, 12, 16, 18).

Perturbation-seeking generative adversarial networks: A defense framework for remote sensing image scene classification. Gong Cheng, IEEE Transactions on Geoscience and Remote Sensing. 60cit. on p. 6Gong Cheng et al. "Perturbation-seeking generative adversarial networks: A defense framework for remote sensing image scene classification". In: IEEE Transac- tions on Geoscience and Remote Sensing 60 (2021), pp. 1-11 (cit. on p. 6).

Spectral unmixing with multinomial mixture Kernel and Wasserstein generative adversarial loss. Savas Ozkan, Gozde Bozdagi Akar, arXiv:2012.06859arXiv preprintcitSavas Ozkan and Gozde Bozdagi Akar. "Spectral un- mixing with multinomial mixture Kernel and Wasser- stein generative adversarial loss". In: arXiv preprint arXiv:2012.06859 (2020) (cit. on p. 6).

Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw puzzles for unsupervised representation learning. Chen Wei, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitioncitChen Wei et al. "Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw puzzles for unsupervised representation learning". In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019, pp. 1910-1919 (cit. on p. 6).

Visual permutation learning. Rodrigo Santa , Cruz , IEEE transactions on pattern analysis and machine intelligence. 41citRodrigo Santa Cruz et al. "Visual permutation learn- ing". In: IEEE transactions on pattern analysis and machine intelligence 41.12 (2018), pp. 3100-3114 (cit. on p. 6).

Jigsaw Clustering for Unsupervised Visual Representation Learning. Pengguang Chen, Shu Liu, Jiaya Jia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021cit. on pp. 6, 12Pengguang Chen, Shu Liu, and Jiaya Jia. "Jigsaw Clus- tering for Unsupervised Visual Representation Learn- ing". In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021, pp. 11526-11535 (cit. on pp. 6, 12).

Error detecting and error correcting codes. R W Hamming, 10.1002/j.1538-7305.1950.tb00463.xThe Bell System Technical Journal. 29tb00463.x (cit. on pp. 6, 7R. W. Hamming. "Error detecting and error correcting codes". In: The Bell System Technical Journal 29.2 (1950), pp. 147-160. DOI: 10.1002/j.1538-7305.1950. tb00463.x (cit. on pp. 6, 7).

Selfsupervised representation learning by rotation feature decoupling. Zeyu Feng, Chang Xu, Dacheng Tao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitioncitZeyu Feng, Chang Xu, and Dacheng Tao. "Self- supervised representation learning by rotation feature decoupling". In: Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition. 2019, pp. 10364-10374 (cit. on p. 6).

Globally and locally consistent image completion. Satoshi Iizuka, Edgar Simo-Serra, Hiroshi Ishikawa, ACM Transactions on Graphics (ToG). 364citSatoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. "Globally and locally consistent image com- pletion". In: ACM Transactions on Graphics (ToG) 36.4 (2017), pp. 1-14 (cit. on p. 6).

When self-supervised learning meets scene classification: Remote sensing scene classification based on a multitask learning framework. Zhicheng Zhao, Remote Sensing. 123276cit. on pp. 6, 16Zhicheng Zhao et al. "When self-supervised learning meets scene classification: Remote sensing scene clas- sification based on a multitask learning framework". In: Remote Sensing 12.20 (2020), p. 3276 (cit. on pp. 6, 16, 18).

Rotation Awareness Based Self-Supervised Learning for SAR Target Recognition. Shuai Zhang, IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2019. cit. on pp. 6, 16Shuai Zhang et al. "Rotation Awareness Based Self- Supervised Learning for SAR Target Recognition". In: IGARSS 2019-2019 IEEE International Geo- science and Remote Sensing Symposium. IEEE. 2019, pp. 1378-1381 (cit. on pp. 6, 16).

Self-Supervised Feature Learning for Semantic Segmentation of Overhead Imagery. Suriya Singh, BMVC. 116Suriya Singh et al. "Self-Supervised Feature Learning for Semantic Segmentation of Overhead Imagery". In: BMVC. Vol. 1. 2018, p. 4 (cit. on pp. 6, 16, 18).

Remote sensing image scene classification with self-supervised paradigm under limited labeled samples. Chao Tao, IEEE Geoscience and Remote Sensing Letters. 18cit. on pp. 6, 16Chao Tao et al. "Remote sensing image scene clas- sification with self-supervised paradigm under limited labeled samples". In: IEEE Geoscience and Remote Sensing Letters (2020) (cit. on pp. 6, 16, 18).

Few-shot Scene Classification of Optical Remote Sensing Images Leveraging Calibrated Pretext Tasks. Hong Ji, IEEE Transactions on Geoscience and Remote Sensing. 6citHong Ji et al. "Few-shot Scene Classification of Op- tical Remote Sensing Images Leveraging Calibrated Pretext Tasks". In: IEEE Transactions on Geoscience and Remote Sensing (2022) (cit. on p. 6).

The color out of space: learning self-supervised representations for Earth Observation imagery. Stefano Vincenzi, 2020 25th International Conference on Pattern Recognition (ICPR). IEEE. 2021. cit. on pp. 7, 8, 16Stefano Vincenzi et al. "The color out of space: learning self-supervised representations for Earth Ob- servation imagery". In: 2020 25th International Con- ference on Pattern Recognition (ICPR). IEEE. 2021, pp. 3034-3041 (cit. on pp. 7, 8, 16).

Comparative analysis of the quantization of color spaces on the basis of the CIELAB colordifference formula. Bernhard Hill, Th Roger, Friedrich Wilhelm Vorhagen, ACM Transactions on Graphics. 16TOG). cit. on p. 6Bernhard Hill, Th Roger, and Friedrich Wilhelm Vorhagen. "Comparative analysis of the quantization of color spaces on the basis of the CIELAB color- difference formula". In: ACM Transactions on Graph- ics (TOG) 16.2 (1997), pp. 109-154 (cit. on p. 6).

Learning representations for automatic colorization. Gustav Larsson, Michael Maire, Gregory Shakhnarovich, European conference on computer vision. SpringercitGustav Larsson, Michael Maire, and Gregory Shakhnarovich. "Learning representations for automatic colorization". In: European conference on computer vision. Springer. 2016, pp. 577-593 (cit. on p. 6).

Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction. Richard Zhang, Phillip Isola, Alexei A Efros, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017 (cit. the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017 (citRichard Zhang, Phillip Isola, and Alexei A. Efros. "Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction". In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017 (cit. on p. 6).

Colorization as a proxy task for visual understanding. Gustav Larsson, Michael Maire, Gregory Shakhnarovich, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition7citGustav Larsson, Michael Maire, and Gregory Shakhnarovich. "Colorization as a proxy task for visual understanding". In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017, pp. 6874-6883 (cit. on p. 7).

Hyper-Embedder: Learning a Deep Embedder for Self-Supervised Hyperspectral Dimensionality Reduction. Xin Wu, Danfeng Hong, Di Zhao, IEEE Geoscience and Remote Sensing Letters. 197citXin Wu, Danfeng Hong, and Di Zhao. "Hyper- Embedder: Learning a Deep Embedder for Self- Supervised Hyperspectral Dimensionality Reduction". In: IEEE Geoscience and Remote Sensing Letters 19 (2021), pp. 1-5 (cit. on p. 7).

Unsupervised learning of video representations using lstms. Nitish Srivastava, Elman Mansimov, Ruslan Salakhudinov, PMLR. 2015International conference on machine learning. 7citNitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. "Unsupervised learning of video rep- resentations using lstms". In: International conference on machine learning. PMLR. 2015, pp. 843-852 (cit. on p. 7).

Decomposing motion and content for natural video sequence prediction. Ruben Villegas, arXiv:1706.080337arXiv preprintRuben Villegas et al. "Decomposing motion and con- tent for natural video sequence prediction". In: arXiv preprint arXiv:1706.08033 (2017) (cit. on p. 7).

Unsupervised learning for physical interaction through video prediction. Chelsea Finn, Ian Goodfellow, Sergey Levine, Advances in neural information processing systems. 297citChelsea Finn, Ian Goodfellow, and Sergey Levine. "Unsupervised learning for physical interaction through video prediction". In: Advances in neural information processing systems 29 (2016), pp. 64-72 (cit. on p. 7).

Shuffle and learn: unsupervised learning using temporal order verification. Ishan Misra, Lawrence Zitnick, Martial Hebert, European Conference on Computer Vision. Springer7citIshan Misra, C Lawrence Zitnick, and Martial Hebert. "Shuffle and learn: unsupervised learning using tem- poral order verification". In: European Conference on Computer Vision. Springer. 2016, pp. 527-544 (cit. on p. 7).

Learning and using the arrow of time. Donglai Wei, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition7citDonglai Wei et al. "Learning and using the arrow of time". In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018, pp. 8052-8060 (cit. on p. 7).

Self-supervised video representation learning with odd-one-out networks. Basura Fernando, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition7citBasura Fernando et al. "Self-supervised video rep- resentation learning with odd-one-out networks". In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, pp. 3636-3645 (cit. on p. 7).

Unsupervised learning using sequential verification for action recognition. Ishan Misra, Lawrence Zitnick, Martial Hebert, arXiv:1603.085612.77arXiv preprintIshan Misra, C Lawrence Zitnick, and Martial Hebert. "Unsupervised learning using sequential ver- ification for action recognition". In: arXiv preprint arXiv:1603.08561 2.7 (2016), p. 8 (cit. on p. 7).

Unsupervised representation learning by sorting sequences. Hsin-Ying Lee, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision7citHsin-Ying Lee et al. "Unsupervised representation learning by sorting sequences". In: Proceedings of the IEEE International Conference on Computer Vision. 2017, pp. 667-676 (cit. on p. 7).

Self-supervised spatiotemporal learning via video clip order prediction. Dejing Xu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition7citDejing Xu et al. "Self-supervised spatiotemporal learn- ing via video clip order prediction". In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019, pp. 10334-10343 (cit. on p. 7).

Optical flow modeling and computation: A survey. Denis Fortun, Patrick Bouthemy, Charles Kervrann, Computer Vision and Image Understanding. 1347citDenis Fortun, Patrick Bouthemy, and Charles Kervrann. "Optical flow modeling and computation: A survey". In: Computer Vision and Image Understanding 134 (2015), pp. 1-21 (cit. on p. 7).

Self-supervised representation learning for remote sensing image change detection based on temporal prediction. Huihui Dong, Remote Sensing. 121868cit. on pp. 7, 16, 18, 21Huihui Dong et al. "Self-supervised representation learning for remote sensing image change detection based on temporal prediction". In: Remote Sensing 12.11 (2020), p. 1868 (cit. on pp. 7, 16, 18, 21).

Self-Supervised Pretraining of Transformers for Satellite Image Time Series Classification. Yuan Yuan, Lei Lin, IEEE Journal of Selected Topics. Yuan Yuan and Lei Lin. "Self-Supervised Pretrain- ing of Transformers for Satellite Image Time Series Classification". In: IEEE Journal of Selected Topics in

. Applied Earth Observations and Remote Sensing. 14cit. on pp. 8, 16Applied Earth Observations and Remote Sensing 14 (2020), pp. 474-487 (cit. on pp. 8, 16, 18).

SITS-Former: A pre-trained spatiospectral-temporal representation model for Sentinel-2 time series classification. Yuan Yuan, International Journal of Applied Earth Observation and Geoinformation. 106102651Yuan Yuan et al. "SITS-Former: A pre-trained spatio- spectral-temporal representation model for Sentinel-2 time series classification". In: International Journal of Applied Earth Observation and Geoinformation 106 (2022), p. 102651 (cit. on p. 8).

Cross-domain self-supervised multi-task feature learning using synthetic imagery. Zhongzheng Ren, Yong Jae Lee, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognitioncit. on p. 8)Zhongzheng Ren and Yong Jae Lee. "Cross-domain self-supervised multi-task feature learning using syn- thetic imagery". In: Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition. 2018, pp. 762-771 (cit. on p. 8).

Multi-task selfsupervised visual learning. Carl Doersch, Andrew Zisserman, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Visioncit. on p. 8)Carl Doersch and Andrew Zisserman. "Multi-task self- supervised visual learning". In: Proceedings of the IEEE International Conference on Computer Vision. 2017, pp. 2051-2060 (cit. on p. 8).

Discriminative unsupervised feature learning with convolutional neural networks. Alexey Dosovitskiy, Advances in neural information processing systems. 27cit. on pp. 8, 10Alexey Dosovitskiy et al. "Discriminative unsuper- vised feature learning with convolutional neural net- works". In: Advances in neural information processing systems 27 (2014), pp. 766-774 (cit. on pp. 8, 10).

Learning features by watching objects move. Deepak Pathak, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognitioncit. on p. 8)Deepak Pathak et al. "Learning features by watching objects move". In: Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition. 2017, pp. 2701-2710 (cit. on p. 8).

Self-Supervised Learning for Monocular Depth Estimation from Aerial Imagery. Max Hermann, arXiv:2008.07246arXiv preprintcit. on pp. 8, 18Max Hermann et al. "Self-Supervised Learning for Monocular Depth Estimation from Aerial Imagery". In: arXiv preprint arXiv:2008.07246 (2020) (cit. on pp. 8, 18).

Self-Supervised Deep Learning For Nonlinear Seismic Full Waveform Inversion. Zhaoqi Gao, IEEE Transactions on Geoscience and Remote Sensing. 8Zhaoqi Gao et al. "Self-Supervised Deep Learning For Nonlinear Seismic Full Waveform Inversion". In: IEEE Transactions on Geoscience and Remote Sensing (2022) (cit. on p. 8).

Self-Supervised Deep Learning to Reconstruct Seismic Data With Consecutively Missing Traces. He Huang, IEEE Transactions on Geoscience and Remote Sensing. 60cit. on p. 8)He Huang et al. "Self-Supervised Deep Learning to Reconstruct Seismic Data With Consecutively Missing Traces". In: IEEE Transactions on Geoscience and Remote Sensing 60 (2022), pp. 1-14 (cit. on p. 8).

Geography-aware self-supervised learning. Kumar Ayush, arXiv:2011.09980arXiv preprintcit. on pp. 8, 12, 15, 16)Kumar Ayush et al. "Geography-aware self-supervised learning". In: arXiv preprint arXiv:2011.09980 (2020) (cit. on pp. 8, 12, 15, 16).

Geographical Knowledge-driven Representation Learning for Remote Sensing Images. Wenyuan Li, arXiv:2107.05276arXiv preprintcit. on pp. 8, 15, 16)Wenyuan Li et al. "Geographical Knowledge-driven Representation Learning for Remote Sensing Images". In: arXiv preprint arXiv:2107.05276 (2021) (cit. on pp. 8, 15, 16).

Open access to Earth land-cover map. Chen Jun, Yifang Ban, Songnian Li, Nature. 514citChen Jun, Yifang Ban, and Songnian Li. "Open access to Earth land-cover map". In: Nature 514.7523 (2014), pp. 434-434 (cit. on p. 8).

Distance metric learning for large margin nearest neighbor classification. Q Kilian, Lawrence K Weinberger, Saul, In: Journal of machine learning research. 109Kilian Q Weinberger and Lawrence K Saul. "Distance metric learning for large margin nearest neighbor clas- sification." In: Journal of machine learning research 10.2 (2009) (cit. on p. 9).

Facenet: A unified embedding for face recognition and clustering. Florian Schroff, Dmitry Kalenichenko, James Philbin, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitioncit. on p. 9Florian Schroff, Dmitry Kalenichenko, and James Philbin. "Facenet: A unified embedding for face recog- nition and clustering". In: Proceedings of the IEEE conference on computer vision and pattern recogni- tion. 2015, pp. 815-823 (cit. on p. 9).

Noisecontrastive estimation: A new estimation principle for unnormalized statistical models. Michael Gutmann, Aapo Hyvärinen, Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings. the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedingscit. on p. 9Michael Gutmann and Aapo Hyvärinen. "Noise- contrastive estimation: A new estimation principle for unnormalized statistical models". In: Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Con- ference Proceedings. 2010, pp. 297-304 (cit. on p. 9).

Efficient estimation of word representations in vector space. Tomas Mikolov, arXiv:1301.3781119arXiv preprintTomas Mikolov et al. "Efficient estimation of word representations in vector space". In: arXiv preprint arXiv:1301.3781 (2013) (cit. on pp. 9, 11).

Distributed representations of words and phrases and their compositionality. Tomas Mikolov, Advances in neural information processing systems. 119cit. onTomas Mikolov et al. "Distributed representations of words and phrases and their compositionality". In: Advances in neural information processing systems. 2013, pp. 3111-3119 (cit. on pp. 9, 11).

Data-efficient image recognition with contrastive predictive coding. Olivier Henaff, PMLR. 2020International Conference on Machine Learning. 149cit. onOlivier Henaff. "Data-efficient image recognition with contrastive predictive coding". In: International Con- ference on Machine Learning. PMLR. 2020, pp. 4182- 4192 (cit. on pp. 9, 14).

On variational bounds of mutual information. Ben Poole, PMLR. 2019International Conference on Machine Learning. 10citBen Poole et al. "On variational bounds of mutual information". In: International Conference on Machine Learning. PMLR. 2019, pp. 5171-5180 (cit. on p. 10).

Learning representations by maximizing mutual information across views. Philip Bachman, Devon Hjelm, William Buchwalter, arXiv:1906.00910arXiv preprintcit. on pp. 10, 14Philip Bachman, R Devon Hjelm, and William Buch- walter. "Learning representations by maximizing mu- tual information across views". In: arXiv preprint arXiv:1906.00910 (2019) (cit. on pp. 10, 14).

On mutual information maximization for representation learning. Michael Tschannen, arXiv:1907.1362510arXiv preprintMichael Tschannen et al. "On mutual information maximization for representation learning". In: arXiv preprint arXiv:1907.13625 (2019) (cit. on p. 10).

Metric learning: A survey. Brian Kulis, Foundations and Trends® in Machine Learning. 510citBrian Kulis et al. "Metric learning: A survey". In: Foundations and Trends® in Machine Learning 5.4 (2013), pp. 287-364 (cit. on p. 10).

Contrastive multiview coding. Yonglong Tian, Dilip Krishnan, Phillip Isola, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKProceedings, Part XI 16Yonglong Tian, Dilip Krishnan, and Phillip Isola. "Contrastive multiview coding". In: Computer Vision- ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI 16.

. Springer, 1410citSpringer. 2020, pp. 776-794 (cit. on pp. 10, 14).

What makes for good views for contrastive learning. Yonglong Tian, arXiv:2005.1024310arXiv preprintYonglong Tian et al. "What makes for good views for contrastive learning?" In: arXiv preprint arXiv:2005.10243 (2020) (cit. on p. 10).

Big self-supervised models are strong semi-supervised learners. Ting Chen, arXiv:2006.100291411arXiv preprintTing Chen et al. "Big self-supervised models are strong semi-supervised learners". In: arXiv preprint arXiv:2006.10029 (2020) (cit. on pp. 11, 14).

Tile2vec: Unsupervised representation learning for spatially distributed data. Neal Jean, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligencecit. on pp. 11, 16, 17Neal Jean et al. "Tile2vec: Unsupervised representa- tion learning for spatially distributed data". In: Pro- ceedings of the AAAI Conference on Artificial Intelli- gence. 2019, pp. 3967-3974 (cit. on pp. 11, 16, 17).

A Semisupervised Convolution Neural Network for Partial Unlabeled Remote-Sensing Image Segmentation. Lili Zhang, IEEE Geoscience and Remote Sensing Letters. 1911citLili Zhang et al. "A Semisupervised Convolution Neu- ral Network for Partial Unlabeled Remote-Sensing Im- age Segmentation". In: IEEE Geoscience and Remote Sensing Letters 19 (2022), pp. 1-5 (cit. on p. 11).

A laboratory open-set Martian rock classification method based on spectral signatures. Juntao Yang, IEEE Transactions on Geoscience and Remote Sensing. 11Juntao Yang et al. "A laboratory open-set Martian rock classification method based on spectral signatures". In: IEEE Transactions on Geoscience and Remote Sensing (2022) (cit. on p. 11).

Transferable network with Siamese architecture for anomaly detection in hyperspectral images. Weiqiang Rao, International Journal of Applied Earth Observation and Geoinformation. 10611Weiqiang Rao et al. "Transferable network with Siamese architecture for anomaly detection in hyper- spectral images". In: International Journal of Applied Earth Observation and Geoinformation 106 (2022), p. 102669 (cit. on p. 11).

Radar Target Detection With Multi-Task Learning in Heterogeneous Environment. He Jing, IEEE Geoscience and Remote Sensing Letters. 1911citHe Jing et al. "Radar Target Detection With Multi-Task Learning in Heterogeneous Environment". In: IEEE Geoscience and Remote Sensing Letters 19 (2022), pp. 1-5 (cit. on p. 11).

Unsupervised deep representation learning and few-shot classification of PolSAR images. Lamei Zhang, IEEE Transactions on Geoscience and Remote Sensing. 6011citLamei Zhang et al. "Unsupervised deep representa- tion learning and few-shot classification of PolSAR images". In: IEEE Transactions on Geoscience and Remote Sensing 60 (2020), pp. 1-16 (cit. on p. 11).

Multi-View Urban Scene Classification with a Complementary-Information Learning Model. Wanxuan Geng, Weixun Zhou, Shuanggen Jin, Photogrammetric Engineering & Remote Sensing. 8811citWanxuan Geng, Weixun Zhou, and Shuanggen Jin. "Multi-View Urban Scene Classification with a Complementary-Information Learning Model". In: Photogrammetric Engineering & Remote Sensing 88.1 (2022), pp. 65-72 (cit. on p. 11).

When deep learning meets metric learning: Remote sensing image scene classification via learning discriminative CNNs. Gong Cheng, IEEE transactions on geoscience and remote sensing. 5611citGong Cheng et al. "When deep learning meets metric learning: Remote sensing image scene classification via learning discriminative CNNs". In: IEEE transac- tions on geoscience and remote sensing 56.5 (2018), pp. 2811-2821 (cit. on p. 11).

Self-supervised learning with randomised layers for remote sensing. Heechul Jung, Taegyun Jeon, Electronics Letters. 5711Heechul Jung and Taegyun Jeon. "Self-supervised learning with randomised layers for remote sensing". In: Electronics Letters 57.6 (2021), pp. 249-251 (cit. on pp. 11, 16).

Self-supervised pre-training enhances change detection in Sentinel-2 imagery. Marrit Leenstra, arXiv:2101.08122arXiv preprintcit. on pp. 11, 16, 18, 21Marrit Leenstra et al. "Self-supervised pre-training enhances change detection in Sentinel-2 imagery". In: arXiv preprint arXiv:2101.08122 (2021) (cit. on pp. 11, 16, 18, 21).

Deep Mutual Information Subspace Clustering Network for Hyperspectral Images. Tiancong Li, IEEE Geoscience and Remote Sensing Letters. 11Tiancong Li et al. "Deep Mutual Information Subspace Clustering Network for Hyperspectral Images". In: IEEE Geoscience and Remote Sensing Letters (2022) (cit. on p. 11).

Hyperspectral Imagery Classification Based on Contrastive Learning. Sikang Hou, IEEE Transactions on Geoscience and Remote Sensing. 6011citSikang Hou et al. "Hyperspectral Imagery Classi- fication Based on Contrastive Learning". In: IEEE Transactions on Geoscience and Remote Sensing 60 (2021), pp. 1-13 (cit. on p. 11).

Hyperspectral Image Classification With Contrastive Self-Supervised Learning Under Limited Labeled Samples. Lin Zhao, IEEE Geoscience and Remote Sensing Letters. 1911citLin Zhao et al. "Hyperspectral Image Classifica- tion With Contrastive Self-Supervised Learning Under Limited Labeled Samples". In: IEEE Geoscience and Remote Sensing Letters 19 (2022), pp. 1-5 (cit. on p. 11).

SC-EADNet: A Self-Supervised Contrastive Efficient Asymmetric Dilated Network for Hyperspectral Image Classification. Mingzhen Zhu, IEEE Transactions on Geoscience and Remote Sensing. 6011citMingzhen Zhu et al. "SC-EADNet: A Self-Supervised Contrastive Efficient Asymmetric Dilated Network for Hyperspectral Image Classification". In: IEEE Trans- actions on Geoscience and Remote Sensing 60 (2021), pp. 1-17 (cit. on p. 11).

Self-supervised learning-based oil spill detection of hyperspectral images. Puhong Duan, Science China Technological Sciences. 65411citPuHong Duan et al. "Self-supervised learning-based oil spill detection of hyperspectral images". In: Science China Technological Sciences 65.4 (2022), pp. 793- 801 (cit. on p. 11).

Contrastive Self-Supervised Learning With Smoothed Representation for Remote Sensing. Heechul Jung, IEEE Geoscience and Remote Sensing Letters. 1612Heechul Jung et al. "Contrastive Self-Supervised Learning With Smoothed Representation for Remote Sensing". In: IEEE Geoscience and Remote Sensing Letters (2021) (cit. on pp. 12, 16).

Self-supervised learning for joint SAR and multispectral land cover classification. Antonio Montanaro, arXiv:2108.090751612arXiv preprintAntonio Montanaro et al. "Self-supervised learning for joint SAR and multispectral land cover classification". In: arXiv preprint arXiv:2108.09075 (2021) (cit. on pp. 12, 16).

Self-Supervised Vision Transformers for Land-Cover Segmentation and Classification. Linus Scheibenreif, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 202212citLinus Scheibenreif et al. "Self-Supervised Vision Transformers for Land-Cover Segmentation and Clas- sification". In: Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition. 2022, pp. 1422-1431 (cit. on p. 12).

Deep residential representations: Using unsupervised learning to unlock elevation data for geodemographic prediction. Matthew Stevenson, Christophe Mues, Cristián Bravo, ISPRS Journal of Photogrammetry and Remote Sensing. 18712citMatthew Stevenson, Christophe Mues, and Cristián Bravo. "Deep residential representations: Using un- supervised learning to unlock elevation data for geo- demographic prediction". In: ISPRS Journal of Pho- togrammetry and Remote Sensing 187 (2022), pp. 378- 392 (cit. on p. 12).

Deep unsupervised embedding for remotely sensed images based on spatially augmented momentum contrast. Jian Kang, IEEE Transactions on Geoscience and Remote Sensing. 59cit. on pp. 12, 16Jian Kang et al. "Deep unsupervised embedding for remotely sensed images based on spatially augmented momentum contrast". In: IEEE Transactions on Geo- science and Remote Sensing 59.3 (2020), pp. 2598- 2610 (cit. on pp. 12, 16, 17).

Remote Sensing Images Semantic Segmentation with General Remote Sensing Vision Model via a Self-Supervised Contrastive Learning Method. Haifeng Li, arXiv:2106.1060518arXiv preprintcit. on pp. 12, 16Haifeng Li et al. "Remote Sensing Images Semantic Segmentation with General Remote Sensing Vision Model via a Self-Supervised Contrastive Learning Method". In: arXiv preprint arXiv:2106.10605 (2021) (cit. on pp. 12, 16, 18).

Self-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding. Vladan Stojnic, Vladimir Risojevic, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021. the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 20211612cit.Vladan Stojnic and Vladimir Risojevic. "Self- Supervised Learning of Remote Sensing Scene Repre- sentations Using Contrastive Multiview Coding". In: Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition. 2021, pp. 1182- 1191 (cit. on pp. 12, 16).

Self-supervised Change Detection in Multi-view Remote Sensing Images. Yuxing Chen, Lorenzo Bruzzone, arXiv:2103.0596918arXiv preprintcit. on pp. 12, 16Yuxing Chen and Lorenzo Bruzzone. "Self-supervised Change Detection in Multi-view Remote Sensing Im- ages". In: arXiv preprint arXiv:2103.05969 (2021) (cit. on pp. 12, 16, 18).

Deep multiview learning for hyperspectral image classification. Bing Liu, IEEE Transactions on Geoscience and Remote Sensing. 1612Bing Liu et al. "Deep multiview learning for hyper- spectral image classification". In: IEEE Transactions on Geoscience and Remote Sensing (2020) (cit. on pp. 12, 16).

Seasonal Contrast: Unsupervised Pre-Training from Uncurated Remote Sensing Data. Oscar Mañas, arXiv:2103.16607arXiv preprintcit. on pp. 12, 16, 17, 21Oscar Mañas et al. "Seasonal Contrast: Unsupervised Pre-Training from Uncurated Remote Sensing Data". In: arXiv preprint arXiv:2103.16607 (2021) (cit. on pp. 12, 16, 17, 21).

Self-supervised Audiovisual Representation Learning for Remote Sensing Data. Konrad Heidler, arXiv:2108.00688arXiv preprintcit. on pp. 12, 16, 18, 21Konrad Heidler et al. "Self-supervised Audiovisual Representation Learning for Remote Sensing Data". In: arXiv preprint arXiv:2108.00688 (2021) (cit. on pp. 12, 16, 18, 21).

Cross-domain Contrastive Learning for Hyperspectral Image Classification. Peiyan Guan, Y Edmund, Lam, IEEE Transactions on Geoscience and Remote Sensing. 12Peiyan Guan and Edmund Y Lam. "Cross-domain Contrastive Learning for Hyperspectral Image Clas- sification". In: IEEE Transactions on Geoscience and Remote Sensing (2022) (cit. on p. 12).

Designing network design spaces. Ilija Radosavovic, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 202012citIlija Radosavovic et al. "Designing network design spaces". In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020, pp. 10428-10436 (cit. on p. 12).

Contrastive clustering. Yunfan Li, 2021 AAAI Conference on Artificial Intelligence (AAAI). 2021 (cit. 12Yunfan Li et al. "Contrastive clustering". In: 2021 AAAI Conference on Artificial Intelligence (AAAI). 2021 (cit. on p. 12).

Self-supervised Multisensor Change Detection. Sudipan Saha, Patrick Ebel, Xiao Xiang Zhu, arXiv:2103.051021612arXiv preprintSudipan Saha, Patrick Ebel, and Xiao Xiang Zhu. "Self-supervised Multisensor Change Detection". In: arXiv preprint arXiv:2103.05102 (2021) (cit. on pp. 12, 16).

Unsupervised Single-Scene Semantic Segmentation for Earth Observation. Sudipan Saha, IEEE Transactions on Geoscience and Remote Sensing. 12Sudipan Saha et al. "Unsupervised Single-Scene Se- mantic Segmentation for Earth Observation". In: IEEE Transactions on Geoscience and Remote Sensing (2022) (cit. on p. 12).

ContrastNet: Unsupervised feature learning by autoencoder and prototypical contrastive learning for hyperspectral imagery classification. Zeyu Cao, Neurocomputing. 46012Zeyu Cao et al. "ContrastNet: Unsupervised feature learning by autoencoder and prototypical contrastive learning for hyperspectral imagery classification". In: Neurocomputing 460 (2021), pp. 71-83 (cit. on pp. 12, 16).

Deep Spatial-Spectral Subspace Clustering for Hyperspectral Images Based on Contrastive Learning. Xiang Hu, Remote Sensing. 1312Xiang Hu et al. "Deep Spatial-Spectral Subspace Clus- tering for Hyperspectral Images Based on Contrastive Learning". In: Remote Sensing 13.21 (2021), p. 4418 (cit. on pp. 12, 16).

Contrastive Learning-Based Dual Dynamic GCN for SAR Image Scene Classification. Fang Liu, IEEE Transactions on Neural Networks and Learning Systems. 12Fang Liu et al. "Contrastive Learning-Based Dual Dy- namic GCN for SAR Image Scene Classification". In: IEEE Transactions on Neural Networks and Learning Systems (2022) (cit. on p. 12).

Distilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.0253113arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. "Dis- tilling the knowledge in a neural network". In: arXiv preprint arXiv:1503.02531 (2015) (cit. on p. 13).

Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Antti Tarvainen, Harri Valpola, arXiv:1703.0178013arXiv preprintAntti Tarvainen and Harri Valpola. "Mean teachers are better role models: Weight-averaged consistency tar- gets improve semi-supervised deep learning results". In: arXiv preprint arXiv:1703.01780 (2017) (cit. on p. 13).

Self-Supervised GANs With Similarity Loss for Remote Sensing Image Scene Classification. Dongen Guo, Ying Xia, Xiaobo Luo, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 14cit. on pp. 13, 16Dongen Guo, Ying Xia, and Xiaobo Luo. "Self- Supervised GANs With Similarity Loss for Remote Sensing Image Scene Classification". In: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 14 (2021), pp. 2508-2521 (cit. on pp. 13, 16, 18).

Self-supervised SAR-optical Data Fusion of Sentinel-1/-2 Images. Yuxing Chen, Lorenzo Bruzzone, IEEE Transactions on Geoscience and Remote Sensing (2021) (cit. on pp. 13. 16Yuxing Chen and Lorenzo Bruzzone. "Self-supervised SAR-optical Data Fusion of Sentinel-1/-2 Images". In: IEEE Transactions on Geoscience and Remote Sensing (2021) (cit. on pp. 13, 16, 18, 21).

Self-supervised Remote Sensing Images Change Detection at Pixellevel. Yuxing Chen, Lorenzo Bruzzone, arXiv:2105.0850118arXiv preprintcit. on pp. 13, 16Yuxing Chen and Lorenzo Bruzzone. "Self-supervised Remote Sensing Images Change Detection at Pixel- level". In: arXiv preprint arXiv:2105.08501 (2021) (cit. on pp. 13, 16, 18).

Contrastive Learning Based on Transformer for Hyperspectral Image Classification. Xiang Hu, Applied Sciences. 1113Xiang Hu et al. "Contrastive Learning Based on Trans- former for Hyperspectral Image Classification". In: Applied Sciences 11.18 (2021), p. 8670 (cit. on pp. 13, 16).

Exploring vision transformers for polarimetric SAR image classification. Hongwei Dong, Lamei Zhang, Bin Zou, IEEE Transactions on Geoscience and Remote Sensing. 6013citHongwei Dong, Lamei Zhang, and Bin Zou. "Explor- ing vision transformers for polarimetric SAR image classification". In: IEEE Transactions on Geoscience and Remote Sensing 60 (2021), pp. 1-15 (cit. on p. 13).

FIAD net: a Fast SAR ship detection network based on feature integration attention and self-supervised learning. Deyi Wang, Chengkun Zhang, Min Han, International Journal of Remote Sensing. 4313citDeyi Wang, Chengkun Zhang, and Min Han. "FIAD net: a Fast SAR ship detection network based on feature integration attention and self-supervised learn- ing". In: International Journal of Remote Sensing 43.4 (2022), pp. 1485-1513 (cit. on p. 13).

Self-Supervised Learning for Invariant Representations from Multi-Spectral and SAR Images. Pallavi Jain, Bianca Schoen-Phelan, Robert Ross, arXiv:2205.0204913arXiv preprintPallavi Jain, Bianca Schoen-Phelan, and Robert Ross. "Self-Supervised Learning for Invariant Representa- tions from Multi-Spectral and SAR Images". In: arXiv preprint arXiv:2205.02049 (2022) (cit. on p. 13).

Self-supervised Vision Transformers for Joint SARoptical Representation Learning. Yi Wang, M Conrad, Xiao Xiang Albrecht, Zhu, arXiv:2204.05381arXiv preprintcit. on pp. 13, 18, 21Yi Wang, Conrad M Albrecht, and Xiao Xiang Zhu. "Self-supervised Vision Transformers for Joint SAR- optical Representation Learning". In: arXiv preprint arXiv:2204.05381 (2022) (cit. on pp. 13, 18, 21).

The Self-Supervised Spectral-Spatial Vision Transformer Network for Accurate Prediction of Wheat Nitrogen Status from UAV Imagery. Xin Zhang, Remote Sensing. 1413Xin Zhang et al. "The Self-Supervised Spectral- Spatial Vision Transformer Network for Accurate Pre- diction of Wheat Nitrogen Status from UAV Imagery". In: Remote Sensing 14.6 (2022), p. 1400 (cit. on p. 13).

Be your own teacher: Improve the performance of convolutional neural networks via self distillation. Linfeng Zhang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision13citLinfeng Zhang et al. "Be your own teacher: Improve the performance of convolutional neural networks via self distillation". In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019, pp. 3713-3722 (cit. on p. 13).

Index Your Position: A Novel Self-Supervised Learning Method for Remote Sensing Images Semantic Segmentation. Dilxat Muhtar, Xueliang Zhang, Pengfeng Xiao, IEEE Transactions on Geoscience and Remote Sensing. 13Dilxat Muhtar, Xueliang Zhang, and Pengfeng Xiao. "Index Your Position: A Novel Self-Supervised Learn- ing Method for Remote Sensing Images Semantic Segmentation". In: IEEE Transactions on Geoscience and Remote Sensing (2022) (cit. on p. 13).

Semantic-aware Dense Representation Learning for Remote Sensing Image Change Detection. Hao Chen, arXiv:2205.1376913arXiv preprintHao Chen et al. "Semantic-aware Dense Representa- tion Learning for Remote Sensing Image Change De- tection". In: arXiv preprint arXiv:2205.13769 (2022) (cit. on p. 13).

Possible principles underlying the transformation of sensory messages. B Horace, Barlow, 14Sensory communication 1.01Horace B Barlow et al. "Possible principles underlying the transformation of sensory messages". In: Sensory communication 1.01 (1961) (cit. on p. 14).

Continual Barlow Twins: continual self-supervised learning for remote sensing semantic segmentation. Valerio Marsocci, Simone Scardapane, arXiv:2205.1131915arXiv preprintValerio Marsocci and Simone Scardapane. "Contin- ual Barlow Twins: continual self-supervised learning for remote sensing semantic segmentation". In: arXiv preprint arXiv:2205.11319 (2022) (cit. on p. 15).

Self Supervised Learning For Few Shot Hyperspectral Image Classification. Nassim Ait Ali Braham, International Geoscience and Remote Sensing Symposium (IGARSS) 2022. IEEE. 2022 (cit. 15Nassim Ait Ali Braham et al. "Self Supervised Learn- ing For Few Shot Hyperspectral Image Classification". In: International Geoscience and Remote Sensing Sym- posium (IGARSS) 2022. IEEE. 2022 (cit. on p. 15).

Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach. M Aidan, Swope, H Xander, Kyle T Rudelis, Story, arXiv:2108.0509416arXiv preprintAidan M Swope, Xander H Rudelis, and Kyle T Story. "Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach". In: arXiv preprint arXiv:2108.05094 (2021) (cit. on p. 16).

Semantic Segmentation of Remote Sensing Images With Self-Supervised Multitask Representation Learning. Wenyuan Li, Hao Chen, Zhenwei Shi, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 1416citWenyuan Li, Hao Chen, and Zhenwei Shi. "Semantic Segmentation of Remote Sensing Images With Self- Supervised Multitask Representation Learning". In: IEEE Journal of Selected Topics in Applied Earth Ob- servations and Remote Sensing 14 (2021), pp. 6438- 6450 (cit. on pp. 16, 18).

Taskrelated self-supervised learning for remote sensing image change detection. Zhinan Cai, Zhiyu Jiang, Yuan Yuan, ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2021. 1816citZhinan Cai, Zhiyu Jiang, and Yuan Yuan. "Task- related self-supervised learning for remote sensing image change detection". In: ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2021, pp. 1535- 1539 (cit. on pp. 16, 18).

Unsupervised Pretraining for Object Detection by Patch Reidentification. Jian Ding, arXiv:2103.048141816arXiv preprintJian Ding et al. "Unsupervised Pretraining for Ob- ject Detection by Patch Reidentification". In: arXiv preprint arXiv:2103.04814 (2021) (cit. on pp. 16, 18).

Selfsupervised spectral matching network for hyperspectral target detection. Can Yao, Yuan Yuan, Zhiyu Jiang, arXiv:2105.040781816arXiv preprintCan Yao, Yuan Yuan, and Zhiyu Jiang. "Self- supervised spectral matching network for hyperspectral target detection". In: arXiv preprint arXiv:2105.04078 (2021) (cit. on pp. 16, 18).

A Self-Supervised Denoising Network for Satellite-Airborne-Ground Hyperspectral Imagery. Xinyu Wang, IEEE Transactions on Geoscience and Remote Sensing. 1816Xinyu Wang et al. "A Self-Supervised Denoising Network for Satellite-Airborne-Ground Hyperspectral Imagery". In: IEEE Transactions on Geoscience and Remote Sensing (2021) (cit. on pp. 16, 18).

SAR Image Classification Using Contrastive Learning and Pseudo-Labels With Limited Data. Chenchen Wang, Hong Gu, Weimin Su, IEEE Geoscience and Remote Sensing Letters. 16Chenchen Wang, Hong Gu, and Weimin Su. "SAR Image Classification Using Contrastive Learning and Pseudo-Labels With Limited Data". In: IEEE Geo- science and Remote Sensing Letters (2021) (cit. on p. 16).

A Mutual Information-Based Self-Supervised Learning Model for PolSAR Land Cover Classification. Bo Ren, IEEE Transactions on Geoscience and Remote Sensing. 1816Bo Ren et al. "A Mutual Information-Based Self- Supervised Learning Model for PolSAR Land Cover Classification". In: IEEE Transactions on Geoscience and Remote Sensing (2021) (cit. on pp. 16, 18).

Homography augumented momentum constrastive learning for SAR image retrieval. Seonho Park, arXiv:2109.1032916arXiv preprintSeonho Park et al. "Homography augumented momen- tum constrastive learning for SAR image retrieval". In: arXiv preprint arXiv:2109.10329 (2021) (cit. on p. 16).

Adversarial Self-Supervised Learning for Robust SAR Target Recognition. Yanjie Xu, Remote Sensing. 1316Yanjie Xu et al. "Adversarial Self-Supervised Learn- ing for Robust SAR Target Recognition". In: Remote Sensing 13.20 (2021), p. 4158 (cit. on p. 16).

Contrastive Multiview Coding With Electro-Optics for SAR Semantic Segmentation. Keumgang Cha, Junghoon Seo, Yeji Choi, IEEE Geoscience and Remote Sensing Letters. 16Keumgang Cha, Junghoon Seo, and Yeji Choi. "Con- trastive Multiview Coding With Electro-Optics for SAR Semantic Segmentation". In: IEEE Geoscience and Remote Sensing Letters (2021) (cit. on p. 16).

Deep learning in remote sensing applications: A meta-analysis and review. Lei Ma, ISPRS journal of photogrammetry and remote sensing. 152cit. on p. 18Lei Ma et al. "Deep learning in remote sensing ap- plications: A meta-analysis and review". In: ISPRS journal of photogrammetry and remote sensing 152 (2019), pp. 166-177 (cit. on p. 18).

Deep learning for remote sensing image classification: A survey. Ying Li, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery. 81264cit. on p. 18Ying Li et al. "Deep learning for remote sensing image classification: A survey". In: Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 8.6 (2018), e1264 (cit. on p. 18).

A review of deep learning methods for semantic segmentation of remote sensing imagery. Xiaohui Yuan, Jianfang Shi, Lichuan Gu, Expert Systems with Applications. 16918Xiaohui Yuan, Jianfang Shi, and Lichuan Gu. "A review of deep learning methods for semantic segmen- tation of remote sensing imagery". In: Expert Systems with Applications 169 (2021), p. 114417 (cit. on p. 18).

Deep learning for change detection in remote sensing images: Comprehensive review and meta-analysis. Lazhar Khelifi, Max Mignotte, IEEE Access. 8cit. on p. 18Lazhar Khelifi and Max Mignotte. "Deep learning for change detection in remote sensing images: Compre- hensive review and meta-analysis". In: IEEE Access 8 (2020), pp. 126385-126400 (cit. on p. 18).

Deep learning meets hyperspectral image analysis: A multidisciplinary review. Alberto Signoroni, Journal of Imaging. 518Alberto Signoroni et al. "Deep learning meets hyper- spectral image analysis: A multidisciplinary review". In: Journal of Imaging 5.5 (2019), p. 52 (cit. on p. 18).

Deep learning meets SAR: Concepts, models, pitfalls, and perspectives. Xiao Xiang Zhu, IEEE Geoscience and Remote Sensing Magazine. 9cit. on p. 18Xiao Xiang Zhu et al. "Deep learning meets SAR: Concepts, models, pitfalls, and perspectives". In: IEEE Geoscience and Remote Sensing Magazine 9.4 (2021), pp. 143-172 (cit. on p. 18).

H2O-Net: Self-Supervised Flood Segmentation via Adversarial Domain Adaptation and Label Refinement. Peri Akiva, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 2021. the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 2021cit. on p. 18Peri Akiva et al. "H2O-Net: Self-Supervised Flood Segmentation via Adversarial Domain Adaptation and Label Refinement". In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 2021, pp. 111-122 (cit. on p. 18).

Patch-level unsupervised planetary change detection. Sudipan Saha, Xiao Xiang Zhu, IEEE Geoscience and Remote Sensing Letters. 19cit. on p. 18Sudipan Saha and Xiao Xiang Zhu. "Patch-level unsu- pervised planetary change detection". In: IEEE Geo- science and Remote Sensing Letters 19 (2021), pp. 1-5 (cit. on p. 18).

Unsupervised Change Detection of Extreme Events Using ML On-Board. Vít Růžička, arXiv:2111.0299518arXiv preprintVít Růžička et al. "Unsupervised Change Detection of Extreme Events Using ML On-Board". In: arXiv preprint arXiv:2111.02995 (2021) (cit. on p. 18).

Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Patrick Helber, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 1219citPatrick Helber et al. "Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification". In: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 12.7 (2019), pp. 2217-2226 (cit. on p. 19).

Understanding self-supervised learning dynamics without contrastive pairs. Yuandong Tian, Xinlei Chen, Surya Ganguli, arXiv:2102.0681021arXiv preprintYuandong Tian, Xinlei Chen, and Surya Ganguli. "Un- derstanding self-supervised learning dynamics without contrastive pairs". In: arXiv preprint arXiv:2102.06810 (2021) (cit. on p. 21).

Improvements to context based self-supervised learning. Daniel T Nathan Mundhenk, Barry Y Ho, Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognitioncit. on p. 21T Nathan Mundhenk, Daniel Ho, and Barry Y Chen. "Improvements to context based self-supervised learn- ing". In: Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition. 2018, pp. 9339- 9348 (cit. on p. 21).

Leveraging background augmentations to encourage semantic focus in self-supervised contrastive learning. K Chaitanya, Ryali, J David, Ari S Schwab, Morcos, arXiv:2103.1271921arXiv preprintChaitanya K Ryali, David J Schwab, and Ari S Mor- cos. "Leveraging background augmentations to en- courage semantic focus in self-supervised contrastive learning". In: arXiv preprint arXiv:2103.12719 (2021) (cit. on p. 21).

Rethinking selfsupervised learning: Small is beautiful. Yun-Hao Cao, Jianxin Wu, arXiv:2103.1355921arXiv preprintYun-Hao Cao and Jianxin Wu. "Rethinking self- supervised learning: Small is beautiful". In: arXiv preprint arXiv:2103.13559 (2021) (cit. on p. 21).

He is pursuing his Ph.D. degree at the German Aerospace Center. 2018 and his M.Sc. degree in geomatics engineering from University of Stuttgart. Wuhan, China; Stuttgart, Germany; Wessling, 82234, Germany,; Munich, Germany; Stuttgart, Germany2021Yi Wang received his B.E. degree in remote sensing science and technology from Wuhan University ; at Technical University of Munichhe spent three months at perception system group. His research interests include remote sensing, deep learning, computer vision and self-supervised learningYi Wang received his B.E. degree in remote sensing science and technology from Wuhan University, Wuhan, China, in 2018 and his M.Sc. degree in geomatics engineering from University of Stuttgart, Stuttgart, Germany, in 2021. He is pursuing his Ph.D. degree at the German Aerospace Center, Wessling, 82234, Germany, and at Technical University of Munich, Munich, Germany. In 2020, he spent three months at perception system group, Sony Corporation, Stuttgart, Germany. His research interests include remote sensing, deep learning, computer vision and self-supervised learning.

Conrad's research agenda interconnect physical models and numerical analysis, employing Big Data technologies and machine learning. As part of the "Data Intensive Physical Analytics" team in IBM Research, he significantly contributed to industry-level solutions processing geospatial information with focus on machine-learning driven remote sensing applications. He co-organized workshops at the IEEE BigData conference and the AAAS annual meeting. M Conrad, Albrecht, M'17) received an undergraduate degree in physics from Technical University Dresden, Germany, in 2007 and a Ph.D. degree in physics with an extra certification in computer science from Heidelberg University. Germany; Germany; Yorktown Heights, NY, USA. Currently, since; Oberpfaffenhofen, GermanySpanning the fields of physics, mathematics and computer science, among others, he was a visiting scientist with CERN, Switzerland, in 2010, and with the Dresden Max Planck Institute for the Physics of Complex Systems. Home to the US and the EU, Conrad's scientific agenda aims to strengthen transatlantic collaboration of corporate research and academiaConrad M Albrecht (M'17) received an undergraduate degree in physics from Technical University Dresden, Germany, in 2007 and a Ph.D. degree in physics with an extra certification in computer science from Heidelberg University, Germany, in 2014. Spanning the fields of physics, mathematics and computer science, among others, he was a visiting scientist with CERN, Switzerland, in 2010, and with the Dresden Max Planck Institute for the Physics of Complex Systems, Germany, in 2007. In 2015 he became research scientist with the IBM T.J. Watson Research Center, Yorktown Heights, NY, USA. Currently, since April 2021, he leads a HelmholtzAI-funded team for "Large-Scale Data Mining in Earth Observation" at the German Aerospace Center, Oberpfaffenhofen, Germany. Conrad's research agenda interconnect physical models and numerical analysis, employing Big Data technologies and machine learning. As part of the "Data Intensive Physical Analytics" team in IBM Research, he significantly contributed to industry-level solutions processing geospatial information with focus on machine-learning driven remote sensing applications. He co-organized workshops at the IEEE BigData conference and the AAAS annual meeting. Home to the US and the EU, Conrad's scientific agenda aims to strengthen transatlantic collaboration of corporate research and academia.

he spent six months at the LIRIS-CNRS laboratory. 2019 and M.Sc. degree in Artificial Intelligence and Data Science from Université Paris Dauphine-PSL. Algiers, Algeria; Paris, France; Munich, Germany; Lyon, France; Paris, FranceNassim Ait Ali Braham received his M.Sc. degree in computer science from Ecole nationale Supérieure d'Informatique (ESI ; German Aerospace Center, Wessling, Germany, and at Technical University of Munich ; PSL Research University2020, he spent six months at the LAMSADE-CNRS laboratory. His research interests include deep learning, computer vision, selfsupervised learning and remote sensingNassim Ait Ali Braham received his M.Sc. degree in computer science from Ecole nationale Supérieure d'Informatique (ESI), Algiers, Algeria, in 2019 and M.Sc. degree in Artificial Intelligence and Data Science from Université Paris Dauphine-PSL, Paris, France, in 2020. He is pursuing his Ph.D. degree at the German Aerospace Center, Wessling, Germany, and at Technical University of Munich, Munich, Germany. In 2019, he spent six months at the LIRIS-CNRS laboratory, Lyon, France. In 2020, he spent six months at the LAMSADE-CNRS laboratory, PSL Research University, Paris, France. His research interests include deep learning, computer vision, self- supervised learning and remote sensing.

He is currently a Guest Professor at the Munich AI Future Lab AI4EO, TUM and the Head of Visual Learning and Reasoning team at the Department "EO Data Science. 2012, the Master's degree in signal and information processing from the University of Chinese Academy of Sciences (UCAS), China, in 2015, and the Dr.-Ing. degree from the Technical University of Munich (TUM). Xi'an, China; Munich, Germany; Wessling, Germany; University of Cambridge, UKGerman Aerospace Center2020Xi'an University of Posts and Telecommunications. He was the recipient of the first place in the 2016 IEEE GRSS Data Fusion Contest and finalists for the Best. Student Paper Award at the 2017 Joint Urban Remote Sensing Event and 2019 Joint Urban Remote Sensing EventLichao Mou received the Bachelor's degree in automation from the Xi'an University of Posts and Telecommunications, Xi'an, China, in 2012, the Master's degree in signal and information processing from the University of Chinese Academy of Sciences (UCAS), China, in 2015, and the Dr.-Ing. degree from the Technical University of Munich (TUM), Munich, Germany, in 2020. He is currently a Guest Professor at the Munich AI Future Lab AI4EO, TUM and the Head of Visual Learning and Reasoning team at the Department "EO Data Science", Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Wessling, Germany. Since 2019, he is a Research Scientist at DLR-IMF and an AI Consultant for the Helmholtz Artificial Intelligence Cooperation Unit (HAICU). In 2015 he spent six months at the Computer Vision Group at the University of Freiburg in Germany. In 2019 he was a Visiting Researcher with the Cambridge Image Analysis Group (CIA), University of Cambridge, UK. He was the recipient of the first place in the 2016 IEEE GRSS Data Fusion Contest and finalists for the Best Student Paper Award at the 2017 Joint Urban Remote Sensing Event and 2019 Joint Urban Remote Sensing Event.

She is currently a visiting AI professor at ESA's Phi-lab. Her main research interests are remote sensing and Earth observation, signal processing, machine learning and data science, with their applications in tackling societal grand challenges, e.g. Global Urbanization, UN's SDGs and Climate Change. Dr. Zhu is a member of young academy (Junge Akademie/Junges Kolleg) at the Berlin-Brandenburg Academy of Sciences and Humanities and the German National Academy of Sciences Leopoldina and the Bavarian Academy of Sciences and Humanities. She serves in the scientific advisory board in several research organizations, among others the German Research Center for Geosciences (GFZ) and Potsdam Institute for Climate Impact Research (PIK). Xiao Xiang Zhu (S'10-M'12-SM'14-F'21) received the Master (M.Sc.) degree, her doctor of engineering (Dr.-Ing.) degree and her "Habilitation" in the field of signal processing from Technical University of Munich (TUM). Munich, Germany; Munich, Germany; Naples, Italy, Fudan University, Shanghai, China, the University of Tokyo, Tokyo, Japan and University of California, Los Angeles, United StatesGerman Aerospace Center (DLRShe is the Chair Professor for Data Science in Earth Observation at Technical University of Munich (TUM) and the Head of the Department "EO Data Science" at the Remote Sensing Technology Institute. She is an associate Editor of IEEE Transactions on Geoscience and Remote Sensing and serves as the area editor responsible for special issues of IEEE Signal Processing Magazine. She is a Fellow of IEEEXiao Xiang Zhu (S'10-M'12-SM'14-F'21) received the Master (M.Sc.) degree, her doctor of engineering (Dr.-Ing.) degree and her "Habilitation" in the field of signal processing from Technical University of Munich (TUM), Munich, Germany, in 2008, 2011 and 2013, respectively. She is the Chair Professor for Data Science in Earth Observation at Technical University of Munich (TUM) and the Head of the Department "EO Data Science" at the Remote Sensing Technology Institute, German Aerospace Center (DLR). Since 2019, Zhu is a co-coordinator of the Munich Data Science Research School (www.mu-ds.de). Since 2019 She also heads the Helmholtz Artificial Intelligence -Research Field "Aeronautics, Space and Transport". Since May 2020, she is the PI and director of the international fu- ture AI lab "AI4EO -Artificial Intelligence for Earth Observation: Reasoning, Uncertainties, Ethics and Beyond", Munich, Germany. Since October 2020, she also serves as a co-director of the Munich Data Science Institute (MDSI), TUM. Prof. Zhu was a guest scientist or visiting professor at the Italian National Research Council (CNR-IREA), Naples, Italy, Fudan University, Shanghai, China, the University of Tokyo, Tokyo, Japan and University of California, Los Angeles, United States in 2009, 2014, 2015 and 2016, respectively. She is currently a visiting AI professor at ESA's Phi-lab. Her main research interests are remote sensing and Earth observation, signal processing, machine learning and data science, with their applications in tackling societal grand challenges, e.g. Global Urbanization, UN's SDGs and Climate Change. Dr. Zhu is a member of young academy (Junge Akademie/Junges Kolleg) at the Berlin-Brandenburg Academy of Sciences and Humanities and the German National Academy of Sciences Leopoldina and the Bavarian Academy of Sciences and Humanities. She serves in the scientific advisory board in several research organizations, among others the German Research Center for Geosciences (GFZ) and Potsdam Institute for Climate Impact Research (PIK). She is an associate Editor of IEEE Transactions on Geoscience and Remote Sensing and serves as the area editor responsible for special issues of IEEE Signal Processing Magazine. She is a Fellow of IEEE.