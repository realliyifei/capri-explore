# Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

What are shell nouns, and how do they relate to their shell content? Can you provide examples of these relationships?

## URLs

1. https://ar5iv.org/html/2101.10554. [2101.10554] 1 Introduction
2. https://ar5iv.org/html/1706.02256. [1706.02256] A Mention-Ranking Model for Abstract Anaphora Resolution
3. https://ar5iv.org/html/2101.09619. [2101.09619] Implementing WordNet Measures of Lexical Semantic Similarity in a Fuzzy Logic Programming SystemWork partially funded by the State Research Agency (AEI) of the Spanish Ministry of Science and Innovation under grant PID2019-104735RB-C42 (SAFER), by the Spanish Ministry of Economy and Competitiveness, under the grants TIN2016-76843-C4-2-R (MERINET), TIN2017-86217-R (CAVI-ART-2), and by the Comunidad de Madrid, under the grant S2018/TCS-4339 (BLOQUES-CM), co-funded by EIE Funds of the European Union.
4. https://ar5iv.org/html/2205.12825. [2205.12825] A Paradigm Change for Formal Syntax: Computational Algorithms in the Grammar of English
5. https://ar5iv.org/html/2406.00537. [2406.00537] Towards an ontology of portions of matter to support multi-scale analysis and provenance tracking
6. https://ar5iv.org/html/2206.14055. [2206.14055] Towards Lexical Gender Inference: A Scalable Methodology using Online Databases
7. https://ar5iv.org/html/2306.08832. [2306.08832] Contrasting intra-modal and ranking cross-modal hard negatives to enhance visio-linguistic compositional understanding
8. https://ar5iv.org/html/2402.16159. [2402.16159] DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem
9. https://ar5iv.org/html/1610.04533. [1610.04533] A Comprehensive Comparative Study of Word and Sentence Similarity Measures
10. https://ar5iv.org/html/2205.10192. [2205.10192] On the Trade-off between Redundancy and Local Coherence in Summarization

## Answer (I don't know?: ✗)

Shell nouns are abstract nouns that require the presence of an associated embedded proposition for their interpretation. They cannot stand alone and make sense without referencing their "shell content"—the clause or context that specifies their meaning. Examples of shell nouns include terms like "fact," "possibility," or "issue" [2].

The relationship between shell nouns and their shell content is inherently tied to anaphora resolution. Shell nouns are often referred to as anaphoric shell nouns, where the meaning of the noun is contingent on understanding the antecedent or embedded clause related to it. For instance, in a sentence such as "The possibility that he might win is surprising," the shell noun "possibility" cannot be fully understood without the clause "that he might win," which clarifies what the possibility refers to [2].

Research in this area, particularly by Kolhatkar et al. (2013b), shows that shell nouns can be identified through their syntactic structure and the context provided by their antecedents. Their work illustrates how, through a process of classification and the application of training datasets, one can resolve shell nouns by recognizing the structures and patterns within their surrounding contexts [4]. This classification relies on discerning the relationships between shell nouns and their antecedent content, thereby enhancing the understanding and processing of abstract anaphora resolution tasks.

The idea that shell nouns share linguistic properties with their embedded counterparts is underscored by KZH13, who suggest that understanding these relationships can significantly aid in resolving ambiguities in language processing [4]. They formulated the shell noun resolution based on an approach that leverages the syntactic and semantic connections between anaphoric shell nouns and their antecedents, proposing a machine learning framework to systematically address these associations [1] [4].

To summarize, shell nouns are crucial components of abstract anaphora that require a referential clause to derive their meaning. Their interpretation cannot exist in isolation and instead relies on the context provided by their shell content. Understanding this relationship enhances the effectiveness of NLP models tasked with resolving such linguistic constructs.

1. [1]:  https://ar5iv.org/html/1706.02256, [1706.02256] A Mention-Ranking Model for Abstract Anaphora Resolution
2. [2]:  https://ar5iv.org/html/1706.02256, [1706.02256] A Mention-Ranking Model for Abstract Anaphora Resolution
3. [3]:  https://ar5iv.org/html/1706.02256, [1706.02256] A Mention-Ranking Model for Abstract Anaphora Resolution
4. [4]:  https://ar5iv.org/html/1706.02256, [1706.02256] A Mention-Ranking Model for Abstract Anaphora Resolution
5. [5]:  https://ar5iv.org/html/2402.16159, [2402.16159] DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem
---
1. [1]:  Passage ID 1: model on two types of anaphora: (a) shell noun anaphora and (b) (pro)nominal abstractanaphors extracted from ARRAU.a. Shell noun resolution dataset.For comparability we train and evaluate our model forshell noun resolution, using theoriginal training (CSN) and test (ASN) corpus of Kolhatkar et al. (2013a, b).999We thank the authors for providing the available data. We follow the data preparation and evaluation protocol of Kolhatkar et al. (2013b)(KZH13).The CSN corpus was constructed from the NYT corpus using manually developed patterns to identify the antecedent of cataphoric shell nouns (CSNs). In KZH13, all syntactic constituents of the sentence that contains both the CSN and its antecedent were considered as candidates for training a ranking model. Candidates that differ from the antecedent in only one word or one word and punctuation were as well considered as antecedents101010We obtained this information from the authors directly.. To all other candidates we refer to
2. [2]:  Passage ID 2: based on its surrounding context. While related, their task is not comparable to the full-fledged abstract anaphora resolution task, since the events to be classified are known to be coreferent and chosen from a set of restricted abstract types.More related to our work is Anand and Hardt (2016) who present an antecedent ranking account for sluicing using classical machine learning based on a small training dataset. They employ features modeling distance, containment, discourse structure, and – less effectively – content and lexical correlates.444Their data set was not publicized.Closest to our work is Kolhatkar et al. (2013b) (KZH13) and Kolhatkar and Hirst (2014) (KH14) on shell noun resolution, using classical machine learning techniques.Shell nouns are abstract nouns, such as fact, possibility, or issue, which can only be interpretedjointlywith their shell content (their embedded clause as in (\ex1) or antecedent as in (\ex2)).KZH13 refer to shell nouns whose antecedent
3. [3]:  Passage ID 3: Zinsmeister, and Graeme Hirst. 2013a.Annotating anaphoric shell nouns with their antecedents.In Proceedings of the 7th Linguistic Annotation Workshop andInteroperability with Discourse, pages 112–121, Sofia, Bulgaria.Kolhatkar et al. (2013b)Varada Kolhatkar, Heike Zinsmeister, and Graeme Hirst. 2013b.Interpreting anaphoric shell nouns using antecedents of cataphoricshell nouns as training data.In Proceedings of the 2013 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 300–310, Seattle, Washington,USA.Lu and Ng (2016)Jing Lu and Vincent Ng. 2016.Event Coreference Resolution with Multi-Pass Sieves.In Proceedings of the Tenth International Conference onLanguage Resources and Evaluation (LREC), pages 3996–4003, Portoroz.v.d. Maaten and Hinton (2008)Laurens v.d. Maaten and Geoffrey Hinton. 2008.Visualizing data using t-sne.Journal of Machine Learning Research, 9(2579-2605):85.Marcus et al. (1993)Mitchell P
4. [4]:  Passage ID 4: push mowers […]].KZH13 presented an approach for resolving six typical shell nouns following the observation thatCSNs are easy to resolve based on their syntactic structure alone, and the assumption that ASNs share linguistic properties with their embedded (CSN) counterparts.They manually developed rules to identify the embedded clause (i.e. cataphoric antecedent) of CSNs and trained SVMrank Joachims (2002) on such instances.The trained SVMrank model is then used to resolve ASNs. KH14 generalized their method to be ableto create training data for any given shell noun,however, their method heavily exploits the specific properties of shell nouns and does not apply to other types of abstract anaphora.Stede and Grishina (2016) study a related phenomenon for German. They examine inherently anaphoric connectives(such as demzufolge – according to which) thatcould be used to access their abstract antecedent in the immediate context. Yet, such connectives are restricted in type,
5. [5]:  Passage ID 5: With their ability to understand complex patterns and generate human-like text, they can assist in identifying and classifying key entities such as specific coding languages, software tools, packages, peripherals, or developers mentioned in various sources like code, software documentation, and community discussions. However, LLMs can sometimes prove to be a bottleneck in identifying NER due to security issues, cost and lack of contextual knowledge. These limitations arise due to their broad contextual learning from vast corpora that often spans numerous domains, making them less specialized for a particular field like software development. These models might struggle to identify and classify domain-specific entities accurately, given their generic training Qin et al. (2023). Moreover, as LLMs learn from data available up to their last training checkpoint, they may not be aware of new terms or entities introduced in the domain post-training. Taking into account the aforementioned