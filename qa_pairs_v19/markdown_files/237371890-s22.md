# Survey of Low-Resource Machine Translation

## Question

How do factored models enhance translation quality in neural machine translation (NMT), and what factors affect their effectiveness?

## URLs

1. https://ar5iv.org/html/1706.05083. [1706.05083] Ensembling Factored Neural Machine Translation Models for Automatic Post-Editing and Quality Estimation
2. https://ar5iv.org/html/2109.12105. [2109.12105] Faithful Target Attribute Prediction in Neural Machine Translation
3. https://ar5iv.org/html/2311.02851. [2311.02851] Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding
4. https://ar5iv.org/html/2303.10966. [2303.10966] Towards Reliable Neural Machine Translation with Consistency-Aware Meta-Learning
5. https://ar5iv.org/html/1910.03912. [1910.03912] Novel Applications of Factored Neural Machine Translation
6. https://ar5iv.org/html/2301.00688. [2301.00688] Active Learning for Neural Machine Translation
7. https://ar5iv.org/html/2405.20089. [2405.20089] The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities
8. https://ar5iv.org/html/2405.18922. [2405.18922] Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective
9. https://ar5iv.org/html/2403.14118. [2403.14118] From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation

## Answer (I don't know?: ✓)

I don't know. The context provided does not contain specific information about factored models in neural machine translation (NMT) or details on how they enhance translation quality and what factors affect their effectiveness. The passages primarily discuss the evolution of machine translation, the advantages of NMT over traditional methods, and some challenges in preserving endangered languages through translation. However, they do not delve into the mechanics of factored models or their specific contributions to translation quality. As a result, I cannot provide a precise answer to your question based on the given context.

1. [1]:  https://ar5iv.org/html/2410.22335, No Title
2. [2]:  https://ar5iv.org/html/2107.04239, No Title
3. [3]:  https://ar5iv.org/html/2107.04239, No Title
4. [4]:  https://ar5iv.org/html/2408.12079, No Title
5. [5]:  https://ar5iv.org/html/2408.12079, No Title
---
1. [1]:  Passage ID 1: information. Furthermore, we explored key factors contributing to the model’s performance improvement, including but not limited to the design of the network architecture, optimization of training strategies, and adjustment of hyperparameters. These analyses not only deepen our understanding of the model’s internal working mechanisms but also provide valuable insights and guidance for future research, especially in terms of further enhancing the performance and application scope of machine translation systems.2 PreliminaryMachine translation, as an important branch of the field of Natural Language Processing (NLP), aims to achieve automatic conversion from one language to another. Early machine translation methods were primarily based on rules and dictionaries. Since the mid-20th century, machine translation has undergone a transition from rule-based translation to statistical methods Brown et al. (1993); Lopez (2008), and to the current Neural Machine Translation (NMT)
2. [2]:  Passage ID 2: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
3. [3]:  Passage ID 3: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
4. [4]:  Passage ID 4: Memory High-quality filtering1 IntroductionThere are many languages around the world that are on the brink of extinction.Machine translation plays a crucial role in preserving endangered languages.However, a common challenge in this endeavor is the scarcity of parallel corpora available online, which hinders the development of effective translation systems.In recent years, the rise of Neural Machine Translation (NMT) [18] make to a technological leap.At the beginning of the research of NMT, researchers used deep neural networks, especially Recurrent Neural Network (RNN) and Long Short-term Memory Networks (LSTM), to better capture language context and complex structures.Moreover, some researchers transfered Convolution Neural Network (CNN) in the classfication task of Natural Language Processing (NLP) [16].However, today, most of the researchers design their models based on transformer [19].The transformer model, based on attention mechanisms, allows more flexible focus
5. [5]:  Passage ID 5: Memory High-quality filtering1 IntroductionThere are many languages around the world that are on the brink of extinction.Machine translation plays a crucial role in preserving endangered languages.However, a common challenge in this endeavor is the scarcity of parallel corpora available online, which hinders the development of effective translation systems.In recent years, the rise of Neural Machine Translation (NMT) [18] make to a technological leap.At the beginning of the research of NMT, researchers used deep neural networks, especially Recurrent Neural Network (RNN) and Long Short-term Memory Networks (LSTM), to better capture language context and complex structures.Moreover, some researchers transfered Convolution Neural Network (CNN) in the classfication task of Natural Language Processing (NLP) [16].However, today, most of the researchers design their models based on transformer [19].The transformer model, based on attention mechanisms, allows more flexible focus