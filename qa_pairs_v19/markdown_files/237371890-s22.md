# Survey of Low-Resource Machine Translation

## Question

How do factored models enhance translation quality in neural machine translation (NMT), and what factors affect their effectiveness?

## URLs

1. https://ar5iv.org/html/1706.05083. [1706.05083] Ensembling Factored Neural Machine Translation Models for Automatic Post-Editing and Quality Estimation
2. https://ar5iv.org/html/2109.12105. [2109.12105] Faithful Target Attribute Prediction in Neural Machine Translation
3. https://ar5iv.org/html/2311.02851. [2311.02851] Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding
4. https://ar5iv.org/html/2303.10966. [2303.10966] Towards Reliable Neural Machine Translation with Consistency-Aware Meta-Learning
5. https://ar5iv.org/html/1910.03912. [1910.03912] Novel Applications of Factored Neural Machine Translation
6. https://ar5iv.org/html/2301.00688. [2301.00688] Active Learning for Neural Machine Translation
7. https://ar5iv.org/html/2405.20089. [2405.20089] The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities
8. https://ar5iv.org/html/2405.18922. [2405.18922] Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective
9. https://ar5iv.org/html/2403.14118. [2403.14118] From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation

## Answer (I don't know?: ‚úó)

Factored models enhance translation quality in neural machine translation (NMT) by addressing data sparsity and allowing the model to incorporate linguistic features that improve generalization capabilities. In factored models, a word is represented as a vector of factors such as stems, word classes, or other linguistic attributes, which helps to reduce the output space by decomposing words into their underlying components. This allows the model to explicitly model the probabilities of these factor sequences and recombine them to produce the surface words for translation [3] [5].

One of the critical enhancements offered by factored NMT is the incorporation of contextual and grammatical information into the translation process. For instance, simultaneous prediction of attributes like casing and gender has been shown to improve not only translation quality but also the faithfulness of the output to the training data distribution [4]. This is particularly advantageous when translating morphologically rich languages, where the linguistic features can significantly affect the quality and accuracy of translations [5].

However, the effectiveness of factored models is influenced by several factors. First, the quality and size of the bilingual corpus used for training significantly impact the performance of NMT systems. Factored models require substantial parallel data to train effectively, especially for low-resource languages where obtaining such data is challenging and expensive [2]. The lack of rich corpora can lead to poorer performance compared to more resource-abundant languages.

Moreover, specific linguistic features included as factors play a crucial role in determining the efficiency of the model. For example, the choice of attributes such as morphological tags or lemmas must align closely with the target language's grammatical structures to yield optimal results [3] [4]. If the model fails to capture these essential linguistic nuances, the translations may suffer in quality.

Additionally, the design of the factored NMT architecture itself can affect its performance. The original factored translation model implemented in SMT has been successfully re-applied in NMT by addressing issues like out-of-vocabulary terms and large vocabulary sizes through the generation of lemma and morphological tags [5]. Enhancements like these are vital in ensuring that the model can effectively handle the complexities of different languages during the translation process.

In conclusion, factored models significantly enhance translation quality in NMT by utilizing linguistic factors to improve generalization and contextual understanding. Their effectiveness is contingent on the availability of quality training data, the selection of appropriate linguistic features, and the architecture of the translation model. Ongoing research into these factors could further enhance the performance of NMT systems, especially for languages with limited resources.

1. [1]:  https://ar5iv.org/html/1910.03912, [1910.03912] Novel Applications of Factored Neural Machine Translation
2. [2]:  https://ar5iv.org/html/2301.00688, [2301.00688] Active Learning for Neural Machine Translation
3. [3]:  https://ar5iv.org/html/2109.12105, [2109.12105] Faithful Target Attribute Prediction in Neural Machine Translation
4. [4]:  https://ar5iv.org/html/2109.12105, [2109.12105] Faithful Target Attribute Prediction in Neural Machine Translation
5. [5]:  https://ar5iv.org/html/1910.03912, [1910.03912] Novel Applications of Factored Neural Machine Translation
---
1. [1]:  Passage ID 1: is able to produce reasonable translations, only the factored model is able to produce the complex unseen German compounds found in the reference. The positive effect however is not significant to automatic metric scores (Section 7.1) as it affects only a small fraction of the words in the test sets.8 ConclusionWe presented novel applications of factored NMT.We showed that word case information and joining of subword units can be predicted effectively by a target factor; this allows for a single representation of similar or even identical lexical items,so that more full forms can be kept for a given subword vocabulary limit. Experiments on two language pairs confirm that this can be done without loss in MT quality and without an increase in decoding time.We also showed that our factored OSNMT implementation significantly improves the original non-factored oneby reducing the number of decoding steps. At the same time,we showed that OSNMT on two WMT tasks exhibits
2. [2]:  Passage ID 2: learning problems for variable length source and target sentences and long-term dependency problems. The NMT system improves translation prediction and has excellent context-analyzing properties.The superiority of NMT over phrase-based SMT is undeniable, and neural networks are used in most online machine translation engines. However, in spite of the growth achieved in the domain of machine translation, the idea of NMT system development being data-hungry continues to be a vital issue in expanding the work for any low-resource languages. In order to train a high-quality translation model, NMT requires a large bilingual corpus. However, creating parallel corpora for most low-resource language pairs is costly and requires human effort. Nevertheless, the language and geographical coverage of NMT have yet to hit new heights due to resource accessibility concerns and a preference for well-established assessment benchmarks. This compels additional research into the present state of NMT
3. [3]:  Passage ID 3: NMTBilmes and Kirchhoff (2003) and Alexandrescu andKirchhoff (2006) introduced factored language models to deal with data sparsity and increase the generalization power of language models. They represent a word as a vector of factors {f1,‚Ä¶,fn}subscriptùëì1‚Ä¶subscriptùëìùëõ\{f_{1},\dots,f_{n}\} (e.g., stem, word class) and explicitly model the probabilities of the vector sequences. In MT, factored models have been used to enrich phrase-based MT or NMT with linguistic features (Koehn and Hoang, 2007; Garc√≠a-Mart√≠nezet¬†al., 2016). They reduce the output space by decomposing surface words yùë¶y on different dimensions, such as lemma and morphological tags, and maximize P‚Äã(yt|ùê±)=‚àèi=1nP‚Äã(fit|y<t,ùê±)Pconditionalsuperscriptùë¶ùë°ùê±superscriptsubscriptproductùëñ1ùëõPconditionalsuperscriptsubscriptùëìùëñùë°superscriptùë¶absentùë°ùê±\textrm{P}(y^{t}|\mathbf{x})=\prod_{i=1}^{n}\textrm{P}(f_{i}^{t}|y^{<t},\mathbf{x}). Factors are recombined to obtain the surface word. The term ‚Äúfactor" has also been applied more generally to
4. [4]:  Passage ID 4: that attribute in inference predictions. We show that factored NMT ‚Äì simultaneously predicting the target word and attributes ‚Äì is effective for increasing faithfulness of translations to the training data distribution. To the best of our knowledge, this is the first work connecting factored NMT with models‚Äô faithfulness to the training data.We study two token-level attributes ‚Äì casing and gender ‚Äì in two different tasks: uppercased text translation and gender prediction of professions. While modeling these two attributes with factored NMT has been shown to improve translation quality (Koehn and Hoang, 2007; Sennrich and Haddow, 2016; Berard et¬†al., 2019; Wilken and Matusov, 2019; Stafanoviƒçs et¬†al., 2020), we focus on evaluating models‚Äô faithfulness across various training data distributions, simulating the uncertainty of natural data. Results using simultaneous attribute prediction show that the translation output better mirrors the training distribution with respect to the
5. [5]:  Passage ID 5: the read head to reduce the number of decoding steps.2 Related WorkThe term factored translation first appeared in the context of statistical machine translation (SMT). The Moses system¬†[7] included implementation of source and target factors. They were used to either consume or generate different sets of label sequences: full word surface forms, lemmas, part-of-speech tags, morphological tags¬†[8]. The original motivation was to reduce the data sparseness and estimation problems for the phrase-level translation models and target language models. Factored models were most effectively used in SMT for translation into morphologically rich languages, as shown e.g. in the work of¬†[9].With the rapid emergence of NMT,a factored neural translation model was introduced by¬†[1]. In that work, the out-of-vocabulary and large vocabulary size problems are mitigated by generating lemma and morphological tags. Our work is a re-implementation of that model, but we apply it for a different