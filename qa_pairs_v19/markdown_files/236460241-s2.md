# A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies

## Question

What are the differences between loan translation and transfer in linguistics, and can you provide examples of these phenomena?

## URLs

1. https://ar5iv.org/html/2404.08661
2. https://ar5iv.org/html/2311.12395
3. https://ar5iv.org/html/2203.16169
4. https://ar5iv.org/html/2408.11749
5. https://ar5iv.org/html/2403.19142
6. https://ar5iv.org/html/1704.07146
7. https://ar5iv.org/html/2008.01391
8. https://ar5iv.org/html/2309.06085
9. https://ar5iv.org/html/2302.14220

## Answer

I don't know. The context provided does not include any specific information regarding the concepts of loan translation and transfer in linguistics, nor does it provide relevant examples of these phenomena. The passages focus on topics related to natural language processing (NLP), machine learning, and multilingual models, but they do not define or elaborate on linguistic strategies such as loan translation or transfer. Therefore, I am unable to answer your question based on the available passages.

[1]: https://ar5iv.org/html/2404.02512, No Title
[2]: https://ar5iv.org/html/2405.05966, No Title
[3]: https://ar5iv.org/html/2403.20056, No Title
[4]: https://ar5iv.org/html/2403.20056, No Title
[5]: https://ar5iv.org/html/2105.05975, No Title

[1]: Passage ID 1: including open/close question answering, summarization (Chang et¬†al., 2023; Min et¬†al., 2023), code completion, and code debugging (Wang et¬†al., 2023; Zan et¬†al., 2023; Surameery and Shakor, 2023), etc. These models have significantly impacted NLP applications by enhancing various aspects of language understanding, generation, and analysis (Zhao et¬†al., 2023). As LLMs continue to evolve and improve, they hold immense potential for further advancements in NLP, paving the way for more sophisticated and intelligent solutions (Hadi et¬†al., 2023). Translation, on the other hand, plays a vital role in bridging the gap between different languages, enabling effective communication. With the advancement of machine translation systems (Ranathunga et¬†al., 2023; Gala et¬†al., 2023), there has been a significant shift in how translations are produced (Arivazhagan et¬†al., 2019). Translation engines have made great progress in understanding languages and generating translations, but they still
[2]: Passage ID 2: the past couple of years have seen LLMs with billions of parameters that can be accessed by a wide range of users through a natural language interface (‚Äòprompting‚Äô).Do these developments prove the ‚Äòbitter lesson‚Äô Sutton (2019) that AI for language is best achieved by funneling data on a massive scale into rather general machine learning architectures? What role does linguistics play nowadays in NLP? Triggered by these and similar questions, we analyze the role of linguistics in the current LLM regime of NLP.We are not the only researchers that have studied these apparently tectonic shifts. Relatedly, Ignat et¬†al. (2024) collect interesting topics for PhD students, particularly evaluation, and Saphra et¬†al. (2023) hypothesize a cyclical historic model that then would suggest that familiar problems will resurface.The relationship between Linguistics and NLP (as well as the relationship between Linguistics and AI) has been a topic of discussion for some time (e.g., Lakoff, 1978;
[3]: Passage ID 3: this is the case on specific tasks remains challenging due to linguistic variations and domain-specific differences.Recent research highlights sensitivity of NLP tasks to minor input changes, in contrast to evaluating against fixed gold standard Gardner et¬†al. (2020).In this paper, we evaluate specifically the extent to which MLLM performance on a high-resource language (HRL) can be expected to transfer to a LRL that shares vocabulary similarity due to areal or genetic proximity, or linguistic borrowing, and the extent to which this assistance in performance is robust to input changes that are driven either by particulars of the task, or by semantic similarity.Hence, we explore the following questions in this paper:‚Ä¢How does the accuracy of zero-shot learning change when introducing minor variations to the original test input?‚Ä¢What impact do language features, such as vocabulary overlapping, have on zero-shot learning?Our novel contributions are as
[4]: Passage ID 4: this is the case on specific tasks remains challenging due to linguistic variations and domain-specific differences.Recent research highlights sensitivity of NLP tasks to minor input changes, in contrast to evaluating against fixed gold standard Gardner et¬†al. (2020).In this paper, we evaluate specifically the extent to which MLLM performance on a high-resource language (HRL) can be expected to transfer to a LRL that shares vocabulary similarity due to areal or genetic proximity, or linguistic borrowing, and the extent to which this assistance in performance is robust to input changes that are driven either by particulars of the task, or by semantic similarity.Hence, we explore the following questions in this paper:‚Ä¢How does the accuracy of zero-shot learning change when introducing minor variations to the original test input?‚Ä¢What impact do language features, such as vocabulary overlapping, have on zero-shot learning?Our novel contributions are as
[5]: Passage ID 5: transfer quality using statistical and machine learning methods.1 IntroductionA vast majority of currently available NLP datasets is in English. However, in many applications around the world it is crucial to use the local language. At the same time, creating new datasets requires a lot of resources and time. This is why recently multilingual language models are gaining more and more attention in academia and industry. Using them allows to train well-performing models with little or no data in the target language (usually low-resourced) given a corresponding dataset in the source language (usually high-resourced). This technique is called zero-shot learning.We explore how differences in linguistic features correlate with or potentially affect performance on various kinds of downstream tasks. To present how such analysis can be helpful, let us consider the following scenario: We want to train a model in a task TùëáT in language L1subscriptùêø1L_{1}, but we do not have access to a