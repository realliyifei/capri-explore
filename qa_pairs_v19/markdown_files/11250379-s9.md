# A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin

## Question

What are the advantages and difficulties associated with using hidden Markov models in speech recognition systems?

## URLs

1. https://ar5iv.org/html/1501.05530. [1501.05530] Belief Hidden Markov Model for Speech Recognition
2. https://ar5iv.org/html/2105.11490. [2105.11490] Hidden Markov and semi-Markov models When and why are these models useful for classifying states in time series data?
3. https://ar5iv.org/html/2102.07112. [2102.07112] A New Algorithm for Hidden Markov Models Learning Problem
4. https://ar5iv.org/html/2310.02724. [2310.02724] End-to-End Training of a Neural HMM with Label and Transition Probabilities
5. https://ar5iv.org/html/1904.04100. [1904.04100] Completely Unsupervised Speech Recognition By A Generative Adversarial Network Harmonized With Iteratively Refined Hidden Markov Models
6. https://ar5iv.org/html/2403.09298. [2403.09298] More than words: Advancements and Challenges in Speech Recognition for Singing
7. https://ar5iv.org/html/2105.06285. [2105.06285] Memory compression and thermal efficiency of quantum implementations of non-deterministic hidden Markov models
8. https://ar5iv.org/html/1508.04928. [1508.04928] Duration and Interval Hidden Markov Model for Sequential Data Analysis
9. https://ar5iv.org/html/2403.01255. [2403.01255] Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey

## Answer (I don't know?: ✗)

Hidden Markov Models (HMMs) have been foundational tools in the field of automatic speech recognition (ASR), particularly for their ability to model sequential data. However, while they have notable advantages, they also present various difficulties.

**Advantages:**

1. **Modeling Temporal Sequences**: HMMs are particularly effective at modeling time-series data or sequential observations, which is essential in speech recognition where phonemes and words are processed over time. This makes them well-suited for capturing the temporal dynamics of speech signals [1].

2. **Integration with Other Techniques**: Recent advancements have led to enhancements in HMMs, such as the incorporation of belief functions, which can address some of the challenges in traditional probabilistic models. This integration allows for better management of uncertainty and imprecision in the data, thereby improving the overall decision-making process in ASR systems [1] [5].

3. **Cost Efficiency**: Utilizing a Belief HMM can reduce the costs associated with creating ASR systems. This is particularly important as traditional HMMs often require extensive training with large amounts of annotated data, making them expensive and resource-intensive [5].

**Difficulties:**

1. **Data Requirements**: HMMs tend to require significant amounts of labeled data for training. This poses a challenge especially for under-resourced languages, where obtaining large annotated corpora is often difficult. Consequently, while collecting unlabeled data may be more feasible, the reliance on labeled cases restricts the applicability of HMMs in many scenarios [3].

2. **Robustness to Noisy Environments**: Conventional HMMs may struggle when dealing with noisy signals typical in real-world environments. Research indicates that while some progress has been made, the ability to accurately recognize speech in the presence of background noise remains a significant challenge for standard HMM implementations [5].

3. **Fixed Model Framework**: The probabilistic framework of HMMs can be limiting since it assumes independent observations given the hidden states. This can lead to difficulties in accurately modeling complex relationships and dependencies within speech data. Newer approaches, including deep learning methods, are beginning to offer more flexibility and better performance for certain tasks, but they typically involve a steep learning curve and require substantial computational resources [4].

In conclusion, while HMMs present valuable advantages such as effective temporal modeling and cost reduction, they also face significant challenges relating to data requirements, noise robustness, and model constraints. Therefore, updates like Belief HMM and the potential incorporation of deep learning approaches are seen as promising directions for overcoming these difficulties [1] [5].

1. [1]:  https://ar5iv.org/html/1501.05530, [1501.05530] Belief Hidden Markov Model for Speech Recognition
2. [2]:  https://ar5iv.org/html/1904.04100, [1904.04100] Completely Unsupervised Speech Recognition By A Generative Adversarial Network Harmonized With Iteratively Refined Hidden Markov Models
3. [3]:  https://ar5iv.org/html/1904.04100, [1904.04100] Completely Unsupervised Speech Recognition By A Generative Adversarial Network Harmonized With Iteratively Refined Hidden Markov Models
4. [4]:  https://ar5iv.org/html/2403.09298, [2403.09298] More than words: Advancements and Challenges in Speech Recognition for Singing
5. [5]:  https://ar5iv.org/html/1501.05530, [1501.05530] Belief Hidden Markov Model for Speech Recognition
---
1. [1]:  Passage ID 1: recognize about 80%of a given speech signal, but this recognition rate still not yetsatisfying. Also, this method needs many hours of speech for trainingwhich makes the automatic speech recognition task very expensive.Recently, [7, 6] extend the Hidden Markov Modelto the theory of belief functions. The belief HMM will avoid disadvantagesof probabilistic HMM which are, generally, due to the use of probabilitytheory. Belief functions are used in several domains of research whereincertitude and imprecision dominate. They provide many tools formanaging and processing the existent pieces of evidence in order toextract knowledge and make better decision. They allow experts tohave a more clear vision about their problems, which is helpful forfinding better solutions. What’s more, belief functions theories presenta more flexible ways to model uncertainty and imprecise data thanprobability functions. Finally, it offers many tools with a higherability to combine a great number
2. [2]:  Passage ID 2: models for speech recognition,” in Advances inneural information processing systems, 2015, pp. 577–585.[2]C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen,A. Kannan, R. J. Weiss, K. Rao, E. Gonina et al., “State-of-the-artspeech recognition with sequence-to-sequence models,” in 2018 IEEEInternational Conference on Acoustics, Speech and Signal Processing(ICASSP).   IEEE, 2018, pp. 4774–4778.[3]N. Dehak, R. Dehak, P. Kenny, N. Brümmer, P. Ouellet, and P. Dumouchel,“Support vector machines versus fast scoring in the low-dimensional totalvariability space for speaker verification,” in Tenth Annualconference of the international speech communication association, 2009.[4]H.-y. Lee and L.-s. Lee, “Enhanced spoken term detection using support vectormachines and weighted pseudo examples,” IEEE Transactions on Audio,Speech, and Language Processing, vol. 21, no. 6, pp. 1272–1284, 2013.[5]I.-F. Chen and C.-H. Lee, “A hybrid hmm/dnn
3. [3]:  Passage ID 3: develop a Generative Adversarial Network (GAN) to achieve this purpose, in which a Generator and a Discriminator learn from each other iteratively to improve the performance. We further use a set of Hidden Markov Models (HMMs) iteratively refined from the machine generated labels to work in harmony with the GAN. The initial experiments on TIMIT data set achieve an phone error rate of 33.1%, which is 8.5% lower than the previous state-of-the-art.Index Terms: Speech Recognition, Unsupervised Learning, Generative Adversarial Network, Hidden Markov Models.1 IntroductionAutomatic speech recognition (ASR) has achieved remarkable performance and been widely used.However, the state-of-the-art ASR systems [1, 2] have to learn from massive annotated data which is difficult to obtain for at least 95% of the languages over the world which are low-resourced. Conversely, collecting a relatively big unlabeled corpora for such languages is more achievable in the big data era. This is why
4. [4]:  Passage ID 4: system, followed by post-processing using ChatGPT, leading to further reductions in error rates [58]. It is anticipated that future research will increasingly leverage such versatile, pre-existing systems or adapt large language models (LLMs) to this domain.9 Conclusion and future workThis paper has examined the challenges and recent progress in speech recognition for singing. We have seen how singing’s unique characteristics, like varying pitches and durations, complex pronunciations, and background music, present different challenges from regular speech recognition. Significant advances have been made in key areas such as phoneme recognition, sung language identification, keyword spotting, and full song transcription. In recent years, improvements are largely due to advancements in deep learning and the availability of diverse, large datasets.The transition from feature-based methods to deep learning signifies a major change in this field. These advanced models are better
5. [5]:  Passage ID 5: HMM recognizer can greatly minimize the cost of these systems.VIII ConclusionIn this paper, we proposed the Belief HMM recognizer. We showed thatincorporating belief functions theory in the speech recognition processis very beneficial, in fact, it reduces considerably the cost of thespeech recognition system. Future works will be focuced on the caseof the noisy speech signal. Indeed, existent speech recognizer stillnot yet good if we have a noisy signal to be decoded.References[1]F. Brugnara, D. Falavigna, and M. Omologo.Automatic segmentation and labeling of speech based on hiddenmarkovmodels.Speech Communication, 12:370–375, 1993.[2]P. Carvalho, L. C. Oliveira, I. M. Trancoso, and M. C. Viana.Concatenative speech synthesis for european portuguese.In 3rd ESCA/COCOSDA Worshop on Speech Synthesis, pages159–163, 1998.[3]S. Cox, R. Brady, and P. Jackson.Techniques for accurate automatic annotation of speech waveforms.In Proc. ICASSP,