# A Survey of Unsupervised Dependency Parsing

## Question

How can models utilize the similarities between different languages to enhance unsupervised multilingual dependency parsing?

## URLs

1. https://ar5iv.org/html/2004.05991. [2004.05991] A Simple Approach to Learning Unsupervised Multilingual Embeddings
2. https://ar5iv.org/html/2405.10936. [2405.10936] A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers
3. https://ar5iv.org/html/2404.16627. [2404.16627] Incorporating Lexical and Syntactic Knowledge for Unsupervised Cross-Lingual Transfer
4. https://ar5iv.org/html/1904.02099. [1904.02099] 75 Languages, 1 Model: Parsing Universal Dependencies Universally
5. https://ar5iv.org/html/2411.04986. [2411.04986] The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities
6. https://ar5iv.org/html/2403.19183. [2403.19183] Empirical Analysis for Unsupervised Universal Dependency Parse Tree Aggregation
7. https://ar5iv.org/html/2408.15040. [2408.15040] A Survey of Large Language Models for European Languages
8. https://ar5iv.org/html/2011.05037. [2011.05037] Translating Similar Languages: Role of Mutual Intelligibility in Multilingual Transformers
9. https://ar5iv.org/html/2010.01535. [2010.01535] A Survey of Unsupervised Dependency Parsing
10. https://ar5iv.org/html/2407.15489. [2407.15489] Two Stacks Are Better Than One: A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives

## Answer (I don't know?: ✓)

I don't know. The context provided does not specifically mention how models can utilize the similarities between different languages to enhance unsupervised multilingual dependency parsing. While there are mentions of cross-lingual model transfer for syntactic dependency parsing [4], and the challenges facing low-resource languages [4], it does not detail methods or techniques that leverage language similarities in unsupervised multilingual dependency parsing specifically. Thus, the information is insufficient to answer the question directly.


1. [1]:  https://ar5iv.org/html/2401.14559, No Title
2. [2]:  https://ar5iv.org/html/2401.14559, No Title
3. [3]:  https://ar5iv.org/html/2407.10554, No Title
4. [4]:  https://ar5iv.org/html/2101.11216, No Title
5. [5]:  https://ar5iv.org/html/1807.10854, No Title
---
1. [1]:  Passage ID 1: I examined how to leverage language modelling techniques in general and LLMs in particular to improve translation features that involve human interaction and continuous feedback, such as adaptive MT, terminology-constrained MT, domain-aware automatic post-editing, auto-suggestion and auto-completion. In the second research question, I addressed a common scenario in the translation industry, namely receiving highly specialised projects, where there is hardly any parallel in-domain data. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. One way to address this question is through domain-specific text generation with LLMs and then fine-tuning an MT model using the generated bilingual in-domain synthetic data. Furthermore, this question overlaps with the first question, since real-time adaptive MT with LLMs can serve as an alternative approach to address in-domain data
2. [2]:  Passage ID 2: common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations (Axelrod et al.,, 2011; Haddow and Koehn,, 2012).Recent advances in language modelling techniques in general and large-scale language models (LLMs) in particular have shown significant potential in improving a wide range of NLP tasks. Inspired by this idea, this research aims to answer two major Research Questions (RQ):RQ1In scenarios involving human interaction and continuous feedback, can we employ language models to improve the quality of adaptive MT at inference time? In the subsequent sections, I will be referring to this question as “Adaptive and Interactive MT”.RQ2In the absence of sufficient in-domain data, can we use pre-trained LLMs to improve the process of NMT domain adaptation? In the following sections, I will be referring to this question as “Domain-specific Text Generation for MT”.Figure 1.1:
3. [3]:  Passage ID 3: LLMs (as machine translation, text classification, and text generation). The progress has been so great that some of these tasks can now be considered solved. The question arises as to how this will impact NLP and NLG going forward and how will their role shift in the face of recent advances in LLMs. Languages are, however, more complex and ultimately LLMs are only specific models based mainly on contextual relationships between words. Indeed, new tasks or new NLU and NLG research lines are emerging, and others remain unsolved. Papers as [2] indicate some of the unsolved topics, such as syntactic parsing with Universal Dependencies, semantic compositionality or causality relationships.The overall goal of this survey is to provide an analysis of several NLG survey papers published recently, exploring the emerging and unsolved research topics in NLG. Our work is presented as a NLG roadmap, detecting the areas requiring improvement and looking beyond the recent successes of
4. [4]:  Passage ID 4: of labelled datasets. The majorityof the world’s languages, however, are low-resource, with little to nolabelled data available (Joshi et al., 2020). Predicting linguistic labels, suchas syntactic dependencies, underlies many downstream NLP applications, and themost effective systems rely on labelled data. Their lack hinders the access toNLP technology in many languages. One solution is cross-lingual model transfer,which adapts models trained on high-resource languages to low-resourceones. This paper presents a flexible framework for cross-lingual transferof syntactic dependency parsers which can leverage any pre-trainedarc-factored dependency parser, and assumes no access to labelled target languagedata.One straightforward method of cross-lingual parsing is direct transfer. Itworks by training a parser on the source language labelled data andsubsequently using it to parse the target language directly. Direct transfer isattractive as it does not require labelled target
5. [5]:  Passage ID 5: the same) languages.This task has gained traction in large part due to the fact that it has been a CoNLL shared task for the past two years. [126] A number of approaches from the 2018 task included using deep transition parsing [127], graph-based neural parsing [128], and a competitive model which used only a single neural model, rather than an ensemble [129].The task has begun to be examined outside of CoNLL, with Liu et al. [130] applying universal dependencies to the parsing of tweets, using an ensemble of bidirectional LSTM.III-C5 Remaining ChallengesOutside of universal parsing, a parsing challenge that needs to be further investigated is the building of syntactic structures without the use of treebanks for training. Attempts have been made using attention scores and Tree-LSTMs, as well as “outside-inside” auto-encoders. If such approaches are successful, they have potential use in many environments, including in the context of low-resource languages and out-of-domain