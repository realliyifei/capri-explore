# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How can high reliability be achieved in annotation practices, and what are the potential drawbacks?

## URLs

1. https://ar5iv.org/html/2408.00880. [2408.00880] Annotator in the Loop: A Case Study of In-Depth Rater Engagement to Create a Bridging Benchmark Dataset
2. https://ar5iv.org/html/2410.14515. [2410.14515] Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media
3. https://ar5iv.org/html/2009.11654. [2009.11654] Best Practices for Managing Data Annotation Projects
4. https://ar5iv.org/html/2409.12218. [2409.12218] ARTICLE: Annotator Reliability Through In-Context Learning
5. https://ar5iv.org/html/2402.05129. [2402.05129] Best Practices for Text Annotation with Large Language Models
6. https://ar5iv.org/html/2306.12189. [2306.12189] Annotating Ambiguous Images: General Annotation Strategy for Image Classification with Real-World Biomedical Validation on Vertebral Fracture Diagnosis
7. https://ar5iv.org/html/2412.04363. [2412.04363] Challenges in Trustworthy Human Evaluation of Chatbots
8. https://ar5iv.org/html/2307.08153. [2307.08153] Analyzing Dataset Annotation Quality Management in the Wild
9. https://ar5iv.org/html/2409.10272. [2409.10272] Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data
10. https://ar5iv.org/html/2410.13313. [2410.13313] Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language

## Answer (I don't know?: ✗)

Achieving high reliability in annotation practices, particularly in complex fields like education and Natural Language Processing (NLP), involves several strategies, but it also presents notable challenges and potential drawbacks.

One effective method for improving the reliability of annotations is to employ expert annotators who can establish precise guidelines and perform initial annotations. Experts bring the necessary subject matter knowledge, which is crucial for writing effective annotation guidelines and conducting error analysis [3]. However, even expert annotators are not infallible: research indicates that they may misinterpret implicit data, which can lead to inconsistent quality of annotations compared to non-experts [4]. This nuanced perspective challenges the notion that experts inherently produce better annotations.

Incorporating multiple annotators and aggregating their input can enhance reliability. By utilizing a framework that allows for a diverse set of annotators, a study found that performance disparities among annotators were reduced, leading to more equitable models [5]. This cost-efficient approach enables the scaling of annotation efforts even with limited original budgets, which can also help in capturing a broader range of perspectives [5]. 

However, while these methods can contribute to higher reliability, there are potential drawbacks. The requirement for expert inputs can lead to increased costs associated with expert involvement, which may not be sustainable given resource constraints [3]. Additionally, the reliance on human annotators can introduce variability and subjectivity, undermining the reliability of annotations if not managed carefully. The complexities inherent in data annotation, especially in subjective tasks where a single ground truth is often unavailable, further complicate these efforts. Disagreements in label assignments should not simply be dismissed as noise but rather understood as integral to the annotation process [5].

Moreover, while advanced models like GPT are capable of achieving competitive performance with fewer annotated examples, they still necessitate expert validation to ensure model reliability and accuracy [3]. Balancing the use of fewer annotated examples against the need for thorough and accurate guidelines is a critical consideration. In essence, while employing expert annotators and diverse annotator groups can enhance annotation reliability, the associated costs and complexities must be diligently managed to prevent introducing inconsistencies into the dataset.

In conclusion, achieving high reliability in annotation practices involves leveraging both expert insights and diverse crowdsourcing strategies, coupled with a careful consideration of the implications regarding cost and potential biases. The pursuit of reliability must weigh these factors to establish a robust and actionable annotation strategy.

1. [1]:  https://ar5iv.org/html/2411.15634, No Title
2. [2]:  https://ar5iv.org/html/2411.15634, No Title
3. [3]:  https://ar5iv.org/html/2303.16416, No Title
4. [4]:  https://ar5iv.org/html/2409.10272, [2409.10272] Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data
5. [5]:  https://ar5iv.org/html/2402.14101, No Title
---
1. [1]:  Passage ID 1: from not using tools expressive enough to interpret labels in low reliability. To that end, this work demonstrates methods for working with low/unknown reliability annotations, often found in tasks requiring complex expert judgment.The field of education has many complex tasks that often yield low reliabilities in labels (Jurenka et al., 2024; Kane and Staiger, 2012) which make edtech NLP models and research particularly vulnerable to the effects of inexpert annotations Belz et al. (2020); van der Lee et al. (2019); Zhou et al. (2023). The case study used to illustrate more expressive methods for working with unreliable labels will be from K12 education. Specifically, this study examines a use case where expert annotations are highly unreliable and yet used in high-stakes decisions: automated rating of the quality of classroom teaching. Methods used in this paper answer the call from others to evaluate the psychometric properties of models that perform this task (Casabianca et al.,
2. [2]:  Passage ID 2: from not using tools expressive enough to interpret labels in low reliability. To that end, this work demonstrates methods for working with low/unknown reliability annotations, often found in tasks requiring complex expert judgment.The field of education has many complex tasks that often yield low reliabilities in labels (Jurenka et al., 2024; Kane and Staiger, 2012) which make edtech NLP models and research particularly vulnerable to the effects of inexpert annotations Belz et al. (2020); van der Lee et al. (2019); Zhou et al. (2023). The case study used to illustrate more expressive methods for working with unreliable labels will be from K12 education. Specifically, this study examines a use case where expert annotations are highly unreliable and yet used in high-stakes decisions: automated rating of the quality of classroom teaching. Methods used in this paper answer the call from others to evaluate the psychometric properties of models that perform this task (Casabianca et al.,
3. [3]:  Passage ID 3: particularly in the areas of data annotation. However, it is important to note that this does not eliminate the need for expert input in creating annotation guidelines and in the initial phases of model training. While our study demonstrates that GPT models can achieve competitive performance with fewer annotated examples compared to traditional NLP systems, the role of subject matter experts remains crucial. Experts are needed to write precise annotation guidelines, perform initial annotations for error analysis and example generation, and validate the model’s performance. Although the GPT models require fewer annotated instances, the costs associated with expert involvement, API usage, and running an LLM service should not be overlooked. A comprehensive comparison of resource requirements and costs between traditional NLP systems, word embedding models, and LLM-based systems would be valuable for future studies. This will provide a clearer understanding of the practical implications
4. [4]:  Passage ID 4: employ? Additionally, the differences between these two groups affect how crowdsourced data should be managed (see the section above on annotation strategy). A critical question arises regarding how this practice impacts the quality of training datasets created by non-experts. This conundrum was also observed in NLP annotations [53].For example, the study by [9] demonstrated that contrary to the common belief that ”experts are better”, experts did not consistently produced higher-quality annotations compared with non-experts. While lay users might make mistakes due to a lack of knowledge, experts possessing sufficient knowledge may sometimes misinterpret implicit data, a tendency not observed in lay users. Conversely, research in RS indicates that generating high-quality labels for RS data is a complex task that demands previous knowledge [26]. Additionally, [30] found that expert annotators are often necessary for accurately identifying pixels at object boundaries and in ambiguous
5. [5]:  Passage ID 5: on two subjective datasets revealed that our framework consistently surpasses previous state-of-the-art models with access to as little as 25% of the original annotation budget. In addition, our framework produced more equitable models with reduced performance disparities among the annotators.By minimizing data requirements, our cost-efficient framework for subjective tasks enables us to scale the number of included annotators and, hence, improve the diversity of captured perspectives. Furthermore, the two-stage design of our framework facilitates the integration of new annotators into pre-existing datasets.2 Related WorkSubjective Tasks in NLP:In recent years, the variety of tasks for which NLP is used has significantly expanded. In many of these tasks, a single ground truth does not exist, making them inherently subjective in nature. In subjective tasks, researchers have argued that disagreements in particular labels should not be treated as statistical noise (Larimore