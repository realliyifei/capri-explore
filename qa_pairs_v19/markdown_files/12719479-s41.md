# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How can high reliability be achieved in annotation practices, and what are the potential drawbacks?

## URLs

1. https://ar5iv.org/html/2408.00880
2. https://ar5iv.org/html/2410.14515
3. https://ar5iv.org/html/2009.11654
4. https://ar5iv.org/html/2409.12218
5. https://ar5iv.org/html/2402.05129
6. https://ar5iv.org/html/2306.12189
7. https://ar5iv.org/html/2412.04363
8. https://ar5iv.org/html/2307.08153
9. https://ar5iv.org/html/2409.10272
10. https://ar5iv.org/html/2410.13313

## Answer

## I don't know?: False

High reliability in annotation practices can be achieved through a combination of expert involvement, strategic management of crowdsourced data, and careful design of annotation frameworks. However, there are also potential drawbacks associated with these approaches.

Firstly, the role of experts is crucial for establishing high-quality annotation practices. Experts are integral in writing precise annotation guidelines, conducting initial analyses for error identification, and validating model performance [1]. Although utilizing large language models (LLMs) like GPT-3 for data annotation offers the potential for improved efficiency and costs associated with fewer human-labeled examples, the quality of the annotations can vary. Research indicates that while LLM-based annotations may achieve promising accuracy in tasks like sentiment classification and question answering, they can also be noisy, making direct deployment of these models impractical without further refinement [2]. This highlights the necessity of human oversight to ensure the annotations are reliable.

Moreover, the management of crowdsourced data must factor in the differences between expert and non-expert annotations. Surprisingly, studies have suggested that there might not always be a substantial difference in quality between expert and lay user annotations; in some cases, experts can misinterpret data, while lay users make mistakes for lack of knowledge [3]. This emphasizes the importance of careful integration of both types of annotators and the acknowledgment that simply employing experts does not guarantee higher quality annotations.

Implementing a cost-efficient annotation framework can enhance the reliability of outcomes by reducing performance disparities among annotators and allowing for a more diverse range of perspectives [4]. For example, frameworks that minimize data requirements can enable the inclusion of more annotators, which fosters diversity in captured insights. However, such frameworks may also come with challenges, particularly in ensuring consistency and managing the possibility of increased noise from a larger variety of data sources.

Another aspect to consider is the balance between resource requirements and costs. While LLMs and expert involvement can collectively improve reliability, they also introduce costs associated with API usage and hiring qualified annotators. Therefore, a comprehensive understanding of resource allocation is essential for future studies to accurately assess practicality [1].

In conclusion, achieving high reliability in annotation practices relies on the essential role of experts, strategic integration of crowdsourced data, and the use of effective annotation frameworks. Yet, potential drawbacks such as the variability in annotation quality among different annotator types, the costs associated with expert involvement, and the risk of increased noise from diverse sources must also be addressed to maintain effectiveness and reliability in NLP annotation.

[1]: https://ar5iv.org/html/2303.16416, No Title
[2]: https://ar5iv.org/html/2306.15766, No Title
[3]: https://ar5iv.org/html/2409.10272, [2409.10272] Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data
[4]: https://ar5iv.org/html/2402.14101, No Title
[5]: https://ar5iv.org/html/2211.14880, No Title

[1]: Passage ID 1: particularly in the areas of data annotation. However, it is important to note that this does not eliminate the need for expert input in creating annotation guidelines and in the initial phases of model training. While our study demonstrates that GPT models can achieve competitive performance with fewer annotated examples compared to traditional NLP systems, the role of subject matter experts remains crucial. Experts are needed to write precise annotation guidelines, perform initial annotations for error analysis and example generation, and validate the model’s performance. Although the GPT models require fewer annotated instances, the costs associated with expert involvement, API usage, and running an LLM service should not be overlooked. A comprehensive comparison of resource requirements and costs between traditional NLP systems, word embedding models, and LLM-based systems would be valuable for future studies. This will provide a clearer understanding of the practical implications
[2]: Passage ID 2: 1).A common solution in all these cases is to collect more labelled data distinct from the distribution of training data, but labelling (or annotating) data is an expensive and manual process. To address this issue, prior work suggests using large language models (LLMs, Ouyang et al. (2022); Brown et al. (2020)) to annotate data. LLMs like GPT-3 obtain promising accuracy for annotating data for a variety of NLP tasks including sentiment classification Ding et al. (2022), keyword relevance Choi et al. (2023); Gilardi et al. (2023) and question answering Gilardi et al. (2023). However, LLM-based annotations can be noisy and due to efficiency reasons, we cannot deploy LLM models directly.In this paper, we take the natural next step and ask whether annotations from LLMs can be used to enhance generalization of existing NLP models. Given a corpus of unlabelled data, we find that a naive application of LLMs (annotating inputs at random) provides only marginal gains on total accuracy
[3]: Passage ID 3: employ? Additionally, the differences between these two groups affect how crowdsourced data should be managed (see the section above on annotation strategy). A critical question arises regarding how this practice impacts the quality of training datasets created by non-experts. This conundrum was also observed in NLP annotations [53].For example, the study by [9] demonstrated that contrary to the common belief that ”experts are better”, experts did not consistently produced higher-quality annotations compared with non-experts. While lay users might make mistakes due to a lack of knowledge, experts possessing sufficient knowledge may sometimes misinterpret implicit data, a tendency not observed in lay users. Conversely, research in RS indicates that generating high-quality labels for RS data is a complex task that demands previous knowledge [26]. Additionally, [30] found that expert annotators are often necessary for accurately identifying pixels at object boundaries and in ambiguous
[4]: Passage ID 4: on two subjective datasets revealed that our framework consistently surpasses previous state-of-the-art models with access to as little as 25% of the original annotation budget. In addition, our framework produced more equitable models with reduced performance disparities among the annotators.By minimizing data requirements, our cost-efficient framework for subjective tasks enables us to scale the number of included annotators and, hence, improve the diversity of captured perspectives. Furthermore, the two-stage design of our framework facilitates the integration of new annotators into pre-existing datasets.2 Related WorkSubjective Tasks in NLP:In recent years, the variety of tasks for which NLP is used has significantly expanded. In many of these tasks, a single ground truth does not exist, making them inherently subjective in nature. In subjective tasks, researchers have argued that disagreements in particular labels should not be treated as statistical noise (Larimore
[5]: Passage ID 5: experts.Our findings show that our novel approach, where humans are incorporated as early as possible in the process, boosts performance in the low-resource, domain-specific setting, allowing for low-labeling-effort question answering systems in new, specialized domains.They further demonstrate how human annotation affects the performance of QA depending on the stage it is performed.1 IntroductionMachine Reading Question Answering (MRQA) is a challenging and important problem.Facilitating targeted information extraction from documents, it allows users to get fast, easy access to a vast amount of documents available. MRQA models generally need plenty of annotations, therefore several methods have been devised for augmenting data by generating new annotated samples, with the ultimate goal of improving quality of predictions.Some of these approaches show a real benefit in the downstream MRQA task; however, there is no work focusing on employing Language Models (LM) fine-tuned