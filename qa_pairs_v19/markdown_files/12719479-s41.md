# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How can high reliability be achieved in annotation practices, and what are the potential drawbacks?

## URLs

1. https://ar5iv.org/html/2408.00880. [2408.00880] Annotator in the Loop: A Case Study of In-Depth Rater Engagement to Create a Bridging Benchmark Dataset
2. https://ar5iv.org/html/2410.14515. [2410.14515] Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media
3. https://ar5iv.org/html/2009.11654. [2009.11654] Best Practices for Managing Data Annotation Projects
4. https://ar5iv.org/html/2409.12218. [2409.12218] ARTICLE: Annotator Reliability Through In-Context Learning
5. https://ar5iv.org/html/2402.05129. [2402.05129] Best Practices for Text Annotation with Large Language Models
6. https://ar5iv.org/html/2306.12189. [2306.12189] Annotating Ambiguous Images: General Annotation Strategy for Image Classification with Real-World Biomedical Validation on Vertebral Fracture Diagnosis
7. https://ar5iv.org/html/2412.04363. [2412.04363] Challenges in Trustworthy Human Evaluation of Chatbots
8. https://ar5iv.org/html/2307.08153. [2307.08153] Analyzing Dataset Annotation Quality Management in the Wild
9. https://ar5iv.org/html/2409.10272. [2409.10272] Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data
10. https://ar5iv.org/html/2410.13313. [2410.13313] Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language

## Answer (I don't know?: ✗)

High reliability in annotation practices can be achieved through a combination of trained annotators, effective management of annotator disagreements, and structured frameworks for assessment. Research indicates that employing skilled annotators can significantly enhance the quality of data annotations. However, since expert annotators are not always readily available, alternative solutions, including the use of prompted large language models (LLMs), are being explored as potential substitutes for traditional annotators [5].

To bolster the reliability of annotations, it is crucial to implement an annotation framework that emphasizes inter- and intra-annotator agreement. The EffiARA annotation framework is an example of this approach, which aims to understand annotator reliability and improve the performance of large language models used for classification tasks. This framework incorporates sample weighting based on annotator reliability, achieving higher classification performance [4]. Addressing annotator disagreement in a structured manner rather than overlooking it as noise can also improve reliability. This strategy involves revealing patterns in the data while managing biases inherent in the data labeling process [2].

Additionally, the training and experience levels of annotators play a critical role in achieving high-quality outputs. Professionals with extensive experience and relevant skills are likely to produce more consistent and accurate annotations compared to laypersons [3]. Nevertheless, even experts can misinterpret implicit data, which highlights that the quality of annotations is not solely dependent on the annotators' expertise but also on the annotation process and the characteristics of the tasks [1].

While striving for high reliability, there are notable drawbacks. First, relying on skilled annotators may lead to bottlenecks in the annotation process, especially in scenarios where such individuals are scarce. This may result in delays or limitations in dataset creation, particularly for niche areas within NLP, such as low-resource languages and dialects, where data availability is already constrained [2]. Moreover, the use of experts might elevate costs associated with data labeling, making large-scale annotation projects less viable [3].

Another potential drawback stems from the over-reliance on annotator consensus or majority voting to resolve disagreements. This method can inadvertently mask underlying nuances within the data, potentially leading to biased annotations that do not accurately capture the diversity of language use [2]. Lastly, the introduction of automated solutions, such as LLMs, while promising, may not entirely eliminate issues related to reliability as these models can still inherit biases present in training data and may not adapt to the complexities of nuanced language requirements without proper supervision [5].

In summary, achieving high reliability in annotation practices necessitates a careful balance of skilled annotators, structured frameworks to handle disagreements, and innovative solutions to expand annotation capabilities. However, it is essential to remain cognizant of the potential challenges that can arise from over-reliance on specific methods or resources.

1. [1]:  https://ar5iv.org/html/2409.10272, [2409.10272] Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data
2. [2]:  https://ar5iv.org/html/2410.13313, [2410.13313] Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language
3. [3]:  https://ar5iv.org/html/2409.10272, [2409.10272] Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data
4. [4]:  https://ar5iv.org/html/2410.14515, [2410.14515] Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media
5. [5]:  https://ar5iv.org/html/2410.13313, [2410.13313] Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language
---
1. [1]:  Passage ID 1: employ? Additionally, the differences between these two groups affect how crowdsourced data should be managed (see the section above on annotation strategy). A critical question arises regarding how this practice impacts the quality of training datasets created by non-experts. This conundrum was also observed in NLP annotations [53].For example, the study by [9] demonstrated that contrary to the common belief that ”experts are better”, experts did not consistently produced higher-quality annotations compared with non-experts. While lay users might make mistakes due to a lack of knowledge, experts possessing sufficient knowledge may sometimes misinterpret implicit data, a tendency not observed in lay users. Conversely, research in RS indicates that generating high-quality labels for RS data is a complex task that demands previous knowledge [26]. Additionally, [30] found that expert annotators are often necessary for accurately identifying pixels at object boundaries and in ambiguous
2. [2]:  Passage ID 2: complex socio-cultural dynamics that are especially vulnerable Kuwatly et al. (2020). Therefore, rather than treating annotator disagreement as mere "noise" or using majority vote labels to cover up disagreement, inevitable disagreements should be adequately addressed in annotation Davani et al. (2023, 2021). The main research question is how to reveal the underlying patterns while minimizing the impact of biased annotations against non-standard language use during the data labeling process to protect language diversity. Moreover, data may be limited or nonexistent, particularly for endangered dialects, minority language use Liu et al. (2022), and low-resource scenarios. The second question explores whether annotated features can improve models’ robustness against small datasets and varied language use, making them more accommodating of English variety. Finally, we observed that skilled and well-trained human annotators are not always readily available. Instead of relying on untrained
3. [3]:  Passage ID 3: usage of crowdsourcing procedures and the ongoing development of large training sets by multiple nonexpert annotators [10], which increase motivations to advance knowledge on how to increase labeling efficiency.Human annotators vary in their skills, capabilities, and the approaches they use to achieve optimal annotation. Some annotators may have years of relevant experience while others may be laymen. Moreover, the approach in which the annotation process is conducted may also affect the outcome quality, for instance, whether the final annotation is based on a group of annotators and how each annotator influences the final result. Annotators can also be influenced by the object, or the task, characteristics. In RS applications, annotation by humans can be particularly challenging for various reasons, such as objects within the same category appearing differently in different images or regions within the same image. For example, a solar panel, which may be assumed a simple
4. [4]:  Passage ID 4: a mitigation strategy, such as X’s community notes, which is currently a manual process. This study takes a knowledge-based approach to misinformation detection, modelling the problem similarly to one of natural language inference. The EffiARA annotation framework is introduced, aiming to utilise inter- and intra-annotator agreement to understand the reliability of each annotator and influence the training of large language models for classification based on annotator reliability. In assessing the EffiARA annotation framework, the Russo-Ukrainian Conflict Knowledge-Based Misinformation Classification Dataset (RUC-MCD) was developed and made publicly available. This study finds that sample weighting using annotator reliability performs the best, utilising both inter- and intra-annotator agreement and soft-label training. The highest classification performance achieved using Llama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.Efficient Annotator Reliability
5. [5]:  Passage ID 5: language use, making them more accommodating of English variety. Finally, we observed that skilled and well-trained human annotators are not always readily available. Instead of relying on untrained annotators who lack expertise in language or social studies, we investigate whether prompted large language models (LLMs) can serve as a viable alternative.As shown in Figure 1, our research addresses three key components: (1) proposing criteria for a prescriptive annotation framework that will be introduced in methodology, (2) conducting a small-scale statistical analysis to compare the framework with the descriptive paradigm and evaluate the performance of prescriptively-prompted LLMs, and (3) testing the framework under limited conditions, using smaller datasets with complex language features without human annotators.To assess annotation quality, we compared inter-rater reliability across three sets: 400 pieces from the Davidson et al., 2017 dataset following general definitions,