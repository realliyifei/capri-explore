# Grammatical Error Correction: A Survey of the State of the Art

## Question

What challenges and transformations are involved in preprocessing human annotations for Grammatical Error Correction (GEC) systems?

## URLs

1. https://ar5iv.org/html/2211.05166. [2211.05166] Grammatical Error Correction: A Survey of the State of the Art
2. https://ar5iv.org/html/2410.16473. [2410.16473] Multi-head Sequence Tagging Model for Grammatical Error Correction
3. https://ar5iv.org/html/2403.05493. [2403.05493] To Err Is Human, but Llamas Can Learn It Too
4. https://ar5iv.org/html/2305.14734. [2305.14734] Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation
5. https://ar5iv.org/html/2407.00924. [2407.00924] EXCGEC: A Benchmark of Edit-wise Explainable Chinese Grammatical Error Correction
6. https://ar5iv.org/html/2005.06600. [2005.06600] A Comprehensive Survey of Grammar Error Correction
7. https://ar5iv.org/html/2312.08400. [2312.08400] Beyond English: Evaluating LLMs for Arabic Grammatical Error Correction

## Answer (I don't know?: ✗)

Preprocessing human annotations for Grammatical Error Correction (GEC) systems involves several challenges and transformations, primarily due to the differences between how humans correct text and how GEC systems are designed to process it.

One of the main challenges is the mismatch between the format of human corrections and the requirements of GEC systems. Human annotators are typically trained to correct natural text at a character level, while GEC systems often operate on word tokenized sentences. This discrepancy necessitates several preprocessing steps to convert human annotations into a format that GEC systems can utilize effectively [1] [2]. 

The first transformation involves converting character-level edits to token-level edits. While this process is straightforward in many instances, it can become complicated in specific cases where a human-annotated character span does not correspond to a complete token. An example of this complexity is when changing a part of a word, such as changing "dancing" to "danced," which may be represented at the character level as [ing → ed], thereby affecting the overall tokenization [1]. This indicates that annotators must not only recognize raw texts but also engage in a higher degree of abstraction that requires defining edits with precision.

Additionally, human annotations must adhere to defined guidelines that explicate what constitutes an edit. For GEC systems to perform effectively, these guidelines must be meticulously articulated, ensuring that all annotators are on the same page regarding the nature and scope of edits being made [1] [2]. This is vital, as discrepancies in interpretation can lead to varied annotations for similar types of errors, complicating the subsequent training of GEC models.

Further complicating the preprocessing phase is the human element itself. Different annotators may approach corrections using different styles or definitions of edits, which can lead to inconsistencies in the data. Such variability highlights the need for comprehensive training and clear communication of standards among annotators [1] [2].

Moreover, human annotations must go through quality control processes to verify that they meet the established standards, adding another layer of complexity to the data preparation workflow before it can be utilized in machine learning models [5]. This quality assurance step is critical because the performance of GEC models heavily relies on the quality of the annotated data, which, per the given context, is a challenge in itself due to the labor-intensive nature of collecting high-quality annotated datasets [5].

In summary, the preprocessing of human annotations for GEC systems involves converting human-defined character-level edits to a token-based framework, adhering to explicit annotator guidelines, managing the variability introduced by different annotators, and ensuring the overall quality of the annotations. These elements are crucial to bridge the gap between human corrections and machine learning requirements in GEC [1] [2] [5].

1. [1]:  https://ar5iv.org/html/2211.05166, [2211.05166] Grammatical Error Correction: A Survey of the State of the Art
2. [2]:  https://ar5iv.org/html/2211.05166, [2211.05166] Grammatical Error Correction: A Survey of the State of the Art
3. [3]:  https://ar5iv.org/html/2211.05166, [2211.05166] Grammatical Error Correction: A Survey of the State of the Art
4. [4]:  https://ar5iv.org/html/2211.05166, [2211.05166] Grammatical Error Correction: A Survey of the State of the Art
5. [5]:  https://ar5iv.org/html/2211.05166, [2211.05166] Grammatical Error Correction: A Survey of the State of the Art
---
1. [1]:  Passage ID 1: an annotator towards a particular correction given the error types that are available (Sakaguchi et al., 2016). Ultimately, if annotators are tasked with explicitly defining the edits they make to correct a sentence, annotator guidelines must clearly define the notion of an edit.Preprocessing ChallengesWhile human annotators are trained to correct natural text, GEC systems are typically trained to correct word tokenised sentences (mainly for evaluation purposes). This mismatch means human annotations typically undergo several preprocessing steps in order to produce the desired output format (Bryant and Felice, 2016). The first of these transformations involves converting character-level edits to token-level edits. While this is often straightforward, it can sometimes be the case that a human-annotated character span does not map to a complete token; e.g. [ing →→\rightarrow ed] to denote the edit [dancing →→\rightarrow danced]. Although such cases can often (but not always) be
2. [2]:  Passage ID 2: an annotator towards a particular correction given the error types that are available (Sakaguchi et al., 2016). Ultimately, if annotators are tasked with explicitly defining the edits they make to correct a sentence, annotator guidelines must clearly define the notion of an edit.Preprocessing ChallengesWhile human annotators are trained to correct natural text, GEC systems are typically trained to correct word tokenised sentences (mainly for evaluation purposes). This mismatch means human annotations typically undergo several preprocessing steps in order to produce the desired output format (Bryant and Felice, 2016). The first of these transformations involves converting character-level edits to token-level edits. While this is often straightforward, it can sometimes be the case that a human-annotated character span does not map to a complete token; e.g. [ing →→\rightarrow ed] to denote the edit [dancing →→\rightarrow danced]. Although such cases can often (but not always) be
3. [3]:  Passage ID 3: CaoNational University of Singaporecaoh@u.nus.edu  Hwee Tou NgNational University of Singaporenght@comp.nus.edu.sg  Ted BriscoeMohamed bin Zayed University of Artificial Intelligenceted.briscoe@mbzuai.ac.aeAbstractGrammatical Error Correction (GEC) is the task of automatically detecting and correcting errors in text. The task not only includes the correction of grammatical errors, such as missing prepositions and mismatched subject-verb agreement, but also orthographic and semantic errors, such as misspellings and word choice errors respectively. The field has seen significant progress in the last decade, motivated in part by a series of five shared tasks, which drove the development of rule-based methods, statistical classifiers, statistical machine translation, and finally neural machine translation systems which represent the current dominant state of the art. In this survey paper, we condense the field into a single article and first outline some of the
4. [4]:  Passage ID 4: CaoNational University of Singaporecaoh@u.nus.edu  Hwee Tou NgNational University of Singaporenght@comp.nus.edu.sg  Ted BriscoeMohamed bin Zayed University of Artificial Intelligenceted.briscoe@mbzuai.ac.aeAbstractGrammatical Error Correction (GEC) is the task of automatically detecting and correcting errors in text. The task not only includes the correction of grammatical errors, such as missing prepositions and mismatched subject-verb agreement, but also orthographic and semantic errors, such as misspellings and word choice errors respectively. The field has seen significant progress in the last decade, motivated in part by a series of five shared tasks, which drove the development of rule-based methods, statistical classifiers, statistical machine translation, and finally neural machine translation systems which represent the current dominant state of the art. In this survey paper, we condense the field into a single article and first outline some of the
5. [5]:  Passage ID 5: reliability and human judgements, common experimental settingsSection 7System ComparisonRecent state-of-the-art systemsSection 8Future ChallengesDomain generalisation, personalised systems, feedback comment generation, model interpretability, semantic errors, contextual GEC, system combination, training data selection, unsupervised approaches, multilingual GEC, spoken GEC, improved evaluationSection 9Conclusion-Table 2: Survey structure2 DataLike most tasks in NLP, the cornerstone of modern GEC systems is data. State-of-the-art neural models depend on millions or billions of words and the quality of this data is paramount to model success. Collecting high quality annotated data is a slow and laborious process however, and there are fewer resources available in GEC than other fields such as machine translation. This section hence first outlines some key considerations of data collection in GEC and highlights the importance of robust annotation