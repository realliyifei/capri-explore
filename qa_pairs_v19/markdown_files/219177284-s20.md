# Conversational Machine Comprehension: a Literature Review

## Question

How is co-referencing utilized in the dialog turns within the CoQA dataset?

## URLs

1. https://ar5iv.org/html/2207.03240. [2207.03240] CoQAR: Question Rewriting on CoQA
2. https://ar5iv.org/html/2312.04362. [2312.04362] PCoQA: Persian Conversational Question Answering Dataset
3. https://ar5iv.org/html/2102.03749. [2102.03749] Role of Attentive History Selection in Conversational Information Seeking
4. https://ar5iv.org/html/1809.10735. [1809.10735] A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC
5. https://ar5iv.org/html/2409.11500. [2409.11500] Multi-Document Grounded Multi-Turn Synthetic Dialog Generation
6. https://ar5iv.org/html/2311.07589. [2311.07589] Dialogizer: Context-aware Conversational-QA Dataset Generation from Textual Sources
7. https://ar5iv.org/html/2205.11685. [2205.11685] A Dataset for Sentence Retrieval for Open-Ended Dialogues
8. https://ar5iv.org/html/2307.10172. [2307.10172] DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI
9. https://ar5iv.org/html/2405.01121. [2405.01121] Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts
10. https://ar5iv.org/html/2308.10390. [2308.10390] LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework

## Answer (I don't know?: ✗)

The CoQA (Conversational Question Answering) dataset leverages co-reference to enhance its interactive dialog format, allowing for better contextual understanding during the question-answering process. In CoQA, users engage in a conversation with the system where they can reference previous interactions, which facilitates a more natural and intuitive exploration of information. Each question asked within this dialog can refer back to previous responses, enabling continuity and coherence in conversation [1] [4].

The design of CoQA is specifically tailored to take advantage of this ability to make references to prior turns. It allows the participant (the "student") to ask follow-up questions based on previously received answers, creating a dynamic where the context from prior turns can be utilized for better answering current queries. This is particularly important as the nature of human conversation often involves referencing past statements and questions, making co-reference an essential feature for providing accurate and contextually relevant answers [1] [5].

Moreover, CoQA incorporates a feature that allows for the possibility of providing abstractive answers, meaning that while it can reference the content of previous turns, it also expects the system to synthesize responses rather than merely selecting from the same text passages. This capability reflects the complexity of dialogue where past interactions significantly inform current queries [1] [4]. Thus, the system's grasp of co-reference enhances user experience by allowing answers that are not only factually correct but also contextually relevant based on the flow of the conversation. 

Additionally, the design of CoQA necessitates a deeper understanding of how questions and answers are interrelated over multiple turns, posing an advanced challenge compared to traditional, single-turn question-answering systems. The multi-turn interaction mimics natural conversational behaviors, establishing a framework in which co-reference can significantly improve the quality of interactions and the overall efficacy of conversational question-answering systems [5] [3]. 

In summary, co-referencing within the CoQA dataset plays a pivotal role in managing the dialog history, enabling the system to understand and utilize previous conversation turns for more relevant and coherent answers. This approach exemplifies a critical advancement in NLP by addressing the intricacies of human conversation within automated systems.

1. [1]:  https://ar5iv.org/html/2104.11394, No Title
2. [2]:  https://ar5iv.org/html/2104.10810, No Title
3. [3]:  https://ar5iv.org/html/2201.05176, No Title
4. [4]:  https://ar5iv.org/html/1809.10735, [1809.10735] A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC
5. [5]:  https://ar5iv.org/html/2207.03240, [2207.03240] CoQAR: Question Rewriting on CoQA
---
1. [1]:  Passage ID 1: QA systems has always been a challenging task in natural language processing and used as a benchmark to evaluate machine’s ability of natural language understanding. However, such systems often struggle when the question answering is carried out in multiple turns by the users to seek more information based on what they have already learned, thus, giving rise to another complicated form called Conversational Question Answering (CQA). CQA systems are often criticized for not understanding or utilizing the previous context of the conversation when answering the questions. To address the research gap, in this paper, we explore how to integrate the conversational history into the neural machine comprehension system. On one hand, we introduce a framework based on publicly available pre-trained language model called BERT for incorporating history turns into the system. On the other hand, we propose a history selection mechanism that selects the turns that are relevant and contributes the most
2. [2]:  Passage ID 2: Natural Language Processing, EMNLP 2016,Austin, Texas, USA, November 1-4, 2016. 2383–2392.https://doi.org/10.18653/v1/d16-1264Reddyet al. (2019)Siva Reddy, Danqi Chen,and Christopher D. Manning.2019.CoQA: A Conversational Question AnsweringChallenge.TACL 7(2019), 249–266.https://doi.org/10.1162/tacl_a_00266Seoet al. (2016)Minjoon Seo, AniruddhaKembhavi, Ali Farhadi, and HannanehHajishirzi. 2016.Bidirectional Attention Flow for MachineComprehension.abs/1611.01603 (2016).Yang et al. (2019)Zhilin Yang, Zihang Dai,Yiming Yang, Jaime G. Carbonell,Ruslan Salakhutdinov, and Quoc V. Le.2019.XLNet: Generalized Autoregressive Pretraining forLanguage Understanding.CoRR abs/1906.08237(2019).arXiv:1906.08237Yeh and Chen (2019)Yi Ting Yeh andYun-Nung Chen. 2019.FlowDelta: Modeling Flow Information Gain inReasoning for Conversational Machine Comprehension.CoRR abs/1908.05117(2019).arXiv:1908.05117Zhang et al.
3. [3]:  Passage ID 3: the ability of question-answering systems to leverage the additional context in the form of a history of previously asked questions and answers. The dataset consists of 18,644 sequences (56,000 question-answer pairs). Each sequence starts with a lead-in that defines the topic of the questions, followed up three questions and answers.Qulac [Aliannejadi et al.,, 2019].This is the first dataset dedicated for clarifying questions in an IR setting. The dataset is built on top of the TREC Web Track 2009-2012 dataset and consists of 10K QA pairs for 198 topics with 762 facets. Each topic is coupled with a facet. Thus, the same question would receive a different answer based on user’s information need (facet).Natural Questions (NQ) [Kwiatkowski et al.,, 2019].Although NQ is a single-turn QA dataset, it is the among the most popular large-scale datasets for developing any end-to-end open-domain QA system which consists of the document search module and the machine comprehension
4. [4]:  Passage ID 4: in the related work.In each of these datasets, crowd workers are asked to (1) produce questions about a paragraph of text (context) and (2) produce a reply by either indicating there is no answer, or providing an extractive answer from the context by highlighting one contiguous span.QuAC and CoQA contain two other features: questions are asked in the form of a dialog, where co-reference to previous interactions is possible and directly answering yes/no is possible.CoQA also allows workers to edit the spans to provide abstractive answers.222Also, SQuAD 2.0 and QuAC  cover only Wikipedia text, CoQA  covers six other domains and QuAC is the only one of these datasets that doesn’t allow the questioner to see the context before formulating a question.We compare these three datasets along several of their new features: (1) unanswerable questions, (2) multi-turn interactions, and (3) abstractive answers.Unanswerable question coverage is complementary among datasets;SQuAD 2.0 focuses
5. [5]:  Passage ID 5: Brabant, Gwénolé Lecorvé, Lina M. Rojas-BarahonaOrange Innovation2 Avenue Pierre Marzin. Lannion. France.{quentin.brabant, gwenole.lecorve, linamaria.rojasbarahona}@orange.comAbstract content1.   IntroductionConversational Question Answering (CQA) [Reddy et al. (2019, Choi et al. (2018, Saha et al. (2018] is a task in which a system interacts with a so-called student. The interaction takes the form of a conversation, where the student asks questions, and the system is expected to provide the right answers.In this paper we focus on the case where the system searches for answers in a text passage,although settings relying on structured data (e.g. knowledge bases) also exist [Saha et al. (2018].Compared to non-conversational question answering (or QA for short), the system faces an additional difficulty: each question is asked in a conversational context that consists in previous conversation turns; implicit references to the conversational context may happen in the