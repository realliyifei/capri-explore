# Conversational Machine Comprehension: a Literature Review

## Question

How is co-referencing utilized in the dialog turns within the CoQA dataset?

## URLs

1. https://ar5iv.org/html/2207.03240
2. https://ar5iv.org/html/2312.04362
3. https://ar5iv.org/html/2102.03749
4. https://ar5iv.org/html/1809.10735
5. https://ar5iv.org/html/2409.11500
6. https://ar5iv.org/html/2311.07589
7. https://ar5iv.org/html/2205.11685
8. https://ar5iv.org/html/2307.10172
9. https://ar5iv.org/html/2405.01121
10. https://ar5iv.org/html/2308.10390

## Answer

## I don't know?: True

I don't know. The context provided does not specify how co-referencing is utilized in the dialog turns within the CoQA dataset. The information about CoQA primarily discusses the dataset's characteristics, its development, integration with pre-trained models like BERT, and the general challenges of conversational question answering (CQA) without delving into the specifics of co-referencing mechanics or their application within the dialogue turns. Therefore, there is insufficient information to provide an answer to your question about co-referencing in CoQA.

[1]: https://ar5iv.org/html/1808.07042, No Title
[2]: https://ar5iv.org/html/2104.11394, No Title
[3]: https://ar5iv.org/html/2104.11394, No Title
[4]: https://ar5iv.org/html/2312.16511, No Title
[5]: https://ar5iv.org/html/2006.00671, No Title

[1]: Passage ID 1: on CoQASince we first released the dataset in August 2018, the progress of developing better models on CoQA has been rapid.Instead of simply prepending the current question with its previous questions and answers, Huang et al. (2019) proposed a more sophisticated solution to effectively stack single-turn models along the conversational flow.Others (e.g., Zhu et al., 2018) attempted to incorporate the most recent pretrained language representation model BERT  Devlin et al. (2018)121212Pretrained BERT models were released in November 2018, which have demonstrated large improvements across a wide variety of NLP tasks. into CoQA and demonstrated superior results.As of the time we finalized the paper (Jan 8, 2019), the state-of-art F1 score on the test set was 82.8.8 ConclusionsIn this paper, we introduced CoQA, a large scale dataset for building conversational question answering systems.Unlike existing reading comprehension datasets, CoQA contains conversational questions,
[2]: Passage ID 2: QA systems has always been a challenging task in natural language processing and used as a benchmark to evaluate machine’s ability of natural language understanding. However, such systems often struggle when the question answering is carried out in multiple turns by the users to seek more information based on what they have already learned, thus, giving rise to another complicated form called Conversational Question Answering (CQA). CQA systems are often criticized for not understanding or utilizing the previous context of the conversation when answering the questions. To address the research gap, in this paper, we explore how to integrate the conversational history into the neural machine comprehension system. On one hand, we introduce a framework based on publicly available pre-trained language model called BERT for incorporating history turns into the system. On the other hand, we propose a history selection mechanism that selects the turns that are relevant and contributes the most
[3]: Passage ID 3: QA systems has always been a challenging task in natural language processing and used as a benchmark to evaluate machine’s ability of natural language understanding. However, such systems often struggle when the question answering is carried out in multiple turns by the users to seek more information based on what they have already learned, thus, giving rise to another complicated form called Conversational Question Answering (CQA). CQA systems are often criticized for not understanding or utilizing the previous context of the conversation when answering the questions. To address the research gap, in this paper, we explore how to integrate the conversational history into the neural machine comprehension system. On one hand, we introduce a framework based on publicly available pre-trained language model called BERT for incorporating history turns into the system. On the other hand, we propose a history selection mechanism that selects the turns that are relevant and contributes the most
[4]: Passage ID 4: Author. Email: jiwei_li@zju.edu.cn. at the time of submission (Aug 24th, 2022).22footnotetext: Equal contribution.11footnotetext: https://quac.ai/1 IntroductionThe task of conversational question answering, which requires machines to answer questions through reading and understanding a given context and history Question-Answer pairs, has been a rapidly growing area in natural language understanding [3, 23, 25, 26]. With the development of pre-trained language models [4, 10, 17], the upper limit of CQA is constantly broken. However, they are still limited by the scale of real-world datasets. More annotated datasets are needed to promote conversational question answering development.To alleviate the limitation of data scale, mainstream research has explored two methods.For example, there are a large number of single-turn datasets [15, 16, 25] in reading comprehension. While numerous single-turn datasets are available, there are some aspects that have not been fully exploited.
[5]: Passage ID 5: immediately relevant to something previously discussed), returning topic (the current question is asking about a topic again after it had previously been shifted away from), clarification of topic, or definition of an entity [Yatskar, 2019]. The model should, therefore, be able to take context from history which may or may not be immediate.4 Multi-Turn Conversational DatasetsThe surge in CMC research is credited to the emergence of large-scale multi-turn conversational datasets: CoQA [Reddy et al., 2019] and QuAC [Choi et al., 2018].4.1 CoQAConversational QA (CoQA) dataset consists of 126k questions sourced from 8k conversations.•Dataset preparation: Conversations are prepared over passages collected across 7 different domains, each with its source dataset, such as news articles derived from CNN [Hermann et al., 2015]. Amongst the 7 domains, two are used for out-of-domain evaluation (only for evaluation, not training), while the other five aid in-domain evaluation