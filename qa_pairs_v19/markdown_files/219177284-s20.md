# Conversational Machine Comprehension: a Literature Review

## Question

How is co-referencing utilized in the dialog turns within the CoQA dataset?

## URLs

1. https://ar5iv.org/html/2207.03240. [2207.03240] CoQAR: Question Rewriting on CoQA
2. https://ar5iv.org/html/2312.04362. [2312.04362] PCoQA: Persian Conversational Question Answering Dataset
3. https://ar5iv.org/html/2102.03749. [2102.03749] Role of Attentive History Selection in Conversational Information Seeking
4. https://ar5iv.org/html/1809.10735. [1809.10735] A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC
5. https://ar5iv.org/html/2409.11500. [2409.11500] Multi-Document Grounded Multi-Turn Synthetic Dialog Generation
6. https://ar5iv.org/html/2311.07589. [2311.07589] Dialogizer: Context-aware Conversational-QA Dataset Generation from Textual Sources
7. https://ar5iv.org/html/2205.11685. [2205.11685] A Dataset for Sentence Retrieval for Open-Ended Dialogues
8. https://ar5iv.org/html/2307.10172. [2307.10172] DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI
9. https://ar5iv.org/html/2405.01121. [2405.01121] Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts
10. https://ar5iv.org/html/2308.10390. [2308.10390] LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework

## Answer (I don't know?: ✗)

Co-reference is effectively utilized within the CoQA (Conversational Question Answering) dataset to enhance the dialog quality and interaction among participants in a conversational setup. Co-reference allows for previous dialog elements to be referenced without explicit mention in later exchanges, promoting smooth and contextually relevant conversations.

In the CoQA dataset, both questioners and responders have access to the document, which amplifies the likelihood of them making references to earlier parts of the conversation when they formulate their questions and answers. This dynamic nature is emphasized by the ability of the questioners to rely on co-references from prior dialog turns, which facilitates more coherent and context-rich interactions within the conversation [1] [4]. 

Moreover, CoQA not only allows the integration of co-references but also encounters challenges with non-factual questions, leading to longer answers that might require referencing multiple parts of the earlier dialogue. This is indicative of a system designed to manage complex conversation flows, reflecting human-like interactions where understanding the context is crucial [1]. The incorporation of linguistic phenomena like anaphora—which refers back to previous mentions—is critical as it underscores the relational dynamics in dialog systems, making it easier for the responder to maintain continuity and relevance in their responses [1].

The design of CoQA, as noted in its development inspirations, incorporates features meant to support these complexities; for example, while previous question-answer datasets like SQuAD 2.0 and QuAC focused more narrowly on specific responses, CoQA's structure is aimed at fostering multi-turn interactions where such co-referencing is necessary and beneficial [3][5]. This enables a richer dialog experience, pushing the boundaries of what QA systems can achieve in terms of understanding context and delivering responses that are not only directly responsive to the question but also reflective of the entire conversational context.

In conclusion, co-referencing in CoQA serves to promote a dialogic richness that is necessary for effective conversational understanding, allowing systems to mimic human dialogue more effectively by recognizing and utilizing previously mentioned elements within the ongoing interaction. This aspect is pivotal for advancing the goals of conversational AI, where understanding the evolution of the dialog is essential for successful information-seeking [2] [4].

1. [1]:  https://ar5iv.org/html/2312.04362, [2312.04362] PCoQA: Persian Conversational Question Answering Dataset
2. [2]:  https://ar5iv.org/html/2312.04362, [2312.04362] PCoQA: Persian Conversational Question Answering Dataset
3. [3]:  https://ar5iv.org/html/1809.10735, [1809.10735] A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC
4. [4]:  https://ar5iv.org/html/2207.03240, [2207.03240] CoQAR: Question Rewriting on CoQA
5. [5]:  https://ar5iv.org/html/2207.03240, [2207.03240] CoQAR: Question Rewriting on CoQA
---
1. [1]:  Passage ID 1: documents, retrieved from the Wikipedia. We take initiatives from both prominent CoQA and QuAC datasets to build our dataset. To this end, like CoQA, both questioner and responder have access to the document in order to control the rate of unanswerable questions. Since questioner’s accessibility to documents increases the odds of string matching and paraphrasing questions Choi et al. (2018), two further measures are taken to diminish the phenomena. First, the questioner is informed to ask questions that do not contain lexical matching, and second, in the post-processing stage, questions that contain a high level of lexical overlap with the sentence containing the answer, are paraphrased to ensure the quality of the dataset.Our dataset incorporates various linguistic phenomena related to conversations, including co-references to previous dialog turns, anaphora, and ellipsis. It introduces new challenges due to a higher presence of non-factual questions, resulting in longer answers.
2. [2]:  Passage ID 2: Question Answering (QA) systems, traditional approaches have largely focused on single-question scenarios, overlooking the dynamic nature of human information-seeking dialogs. However, to create more human-like and interactive QA systems, understanding context-dependent and evolving conversations is essential. To this end conversational question answering datasets have been introduced Reddy et al. (2019); Choi et al. (2018); Campos et al. (2020). Here, we present PCoQA, an innovative and extensive Persian Conversational Question Answering dataset tailored explicitly for Question Answering in Context, drawing inspiration from two influential predecessors, CoQA Reddy et al. (2019) and QuAC Choi et al. (2018). Our dataset contains 870 dialogs, 9,026 question-answer pairs, and corresponding documents, retrieved from the Wikipedia. We take initiatives from both prominent CoQA and QuAC datasets to build our dataset. To this end, like CoQA, both questioner and responder have access to the
3. [3]:  Passage ID 3: in the related work.In each of these datasets, crowd workers are asked to (1) produce questions about a paragraph of text (context) and (2) produce a reply by either indicating there is no answer, or providing an extractive answer from the context by highlighting one contiguous span.QuAC and CoQA contain two other features: questions are asked in the form of a dialog, where co-reference to previous interactions is possible and directly answering yes/no is possible.CoQA also allows workers to edit the spans to provide abstractive answers.222Also, SQuAD 2.0 and QuAC  cover only Wikipedia text, CoQA  covers six other domains and QuAC is the only one of these datasets that doesn’t allow the questioner to see the context before formulating a question.We compare these three datasets along several of their new features: (1) unanswerable questions, (2) multi-turn interactions, and (3) abstractive answers.Unanswerable question coverage is complementary among datasets;SQuAD 2.0 focuses
4. [4]:  Passage ID 4: Brabant, Gwénolé Lecorvé, Lina M. Rojas-BarahonaOrange Innovation2 Avenue Pierre Marzin. Lannion. France.{quentin.brabant, gwenole.lecorve, linamaria.rojasbarahona}@orange.comAbstract content1.   IntroductionConversational Question Answering (CQA) [Reddy et al. (2019, Choi et al. (2018, Saha et al. (2018] is a task in which a system interacts with a so-called student. The interaction takes the form of a conversation, where the student asks questions, and the system is expected to provide the right answers.In this paper we focus on the case where the system searches for answers in a text passage,although settings relying on structured data (e.g. knowledge bases) also exist [Saha et al. (2018].Compared to non-conversational question answering (or QA for short), the system faces an additional difficulty: each question is asked in a conversational context that consists in previous conversation turns; implicit references to the conversational context may happen in the
5. [5]:  Passage ID 5: for Computational Linguistics (Volume 2: ShortPapers), pages 784–789, Melbourne, Australia, July. Association forComputational Linguistics.Reddy et al. (2019Reddy, S., Chen, D., and Manning, C.(2019).CoQA: A Conversational Question Answering Challenge.Transactions of the Association for Computational Linguistics,7:249–266, May.Saha et al. (2018Saha, A., Pahuja, V., Khapra, M., Sankaranarayanan, K., and Chandar, S.(2018).Complex sequential question answering: Towards learning to converseover linked question answer pairs with a knowledge graph.In Thirty-Second AAAI Conference on Artificial Intelligence.Usbeck et al. (2018Usbeck, R., Ngomo, A.-C. N., Conrads, F., Röder, M., and Napolitano, G.(2018).8th challenge on question answering over linked data (QALD-8).In Joint proceedings of the 4th Workshop on Semantic DeepLearning (SemDeep-4) and NLIWoD4: Natural Language Interfaces for the Web ofData (NLIWOD-4) and 9th Question Answering