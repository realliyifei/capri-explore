# Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models

## Question

How can pre-trained language models be adapted for evaluation tasks that lack corresponding pre-training objectives?

## URLs

1. https://ar5iv.org/html/2408.08661. [2408.08661] MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector
2. https://ar5iv.org/html/2109.01048. [2109.01048] TravelBERT: Pre-training Language Model Incorporating Domain-specific Heterogeneous Knowledge into A Unified Representation
3. https://ar5iv.org/html/2411.04448. [2411.04448] Gradient Localization Improves Lifelong Pretraining of Language Models
4. https://ar5iv.org/html/2406.14833. [2406.14833] Efficient Continual Pre-training by Mitigating the Stability Gap
5. https://ar5iv.org/html/2412.01377. [2412.01377] Adapting Large Language Models to Log Analysis with Interpretable Domain Knowledge
6. https://ar5iv.org/html/2407.14076. [2407.14076] Domain-Specific Pretraining of Language Models: A Comparative Study in the Medical Field
7. https://ar5iv.org/html/2410.10739. [2410.10739] Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs
8. https://ar5iv.org/html/2312.11502. [2312.11502] Labrador: Exploring the limits of masked language modeling for laboratory data
9. https://ar5iv.org/html/2408.01402. [2408.01402] Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer
10. https://ar5iv.org/html/2402.17400. [2402.17400] Investigating Continual Pretraining in Large Language Models: Insights and Implications

## Answer (I don't know?: ✗)

Adapting pre-trained language models (PLMs) for evaluation tasks that lack corresponding pre-training objectives can be approached through several methodologies, leveraging techniques like continual pre-training, domain-specific fine-tuning, and structured tasks. 

First, continual pre-training (CPT) can enhance PLMs by integrating interpretable domain knowledge, thereby making them more effective for specific tasks. For instance, a model like SuperLog, which was developed through CPT, demonstrated significant performance improvements across evaluation tasks by using a comprehensive dataset specifically designed for log analysis—NLPLog, containing over 250,000 question-answer pairs [4]. This approach suggests that continually adapting models with relevant data can bridge gaps where direct pre-training objectives do not exist.

Second, domain-specific fine-tuning of PLMs is critical. This involves further training a pre-trained model on large, unlabeled datasets from the target domain, which helps the model become adept at understanding context-specific terminology and task requirements. Studies have shown that continued pre-training on domain-specific text provides substantial improvements in model performance for various tasks, as seen with models such as BioBERT for the biomedical domain [3]. Such fine-tuning tailors the representation capabilities of the PLM for the nuances of the evaluation task at hand, which might not have been adequately represented during the initial pre-training phase.

Third, the effectiveness of specialized models in comparison to general-purpose models has been noted, especially in domains with a wealth of specific data, like medical tasks. For example, in evaluating medical-related tasks, specialized models like PubMedBERT, which is trained on 14 million PubMed abstracts, have been shown to outperform general-purpose models in relevance and accuracy [5]. This highlights the necessity of aligning the training data with the evaluation context to optimize results.

Moreover, techniques like prompt engineering can position the PLM more effectively for specific tasks. Adapting prompts can guide the model's attention toward relevant aspects of the task, thus overcoming the absence of direct pre-training objectives [2]. By using various structural forms of prompts, the model's engagement with the task information can be enhanced, leading to better evaluation results.

Lastly, exploring alternative task formulations and applying multi-task learning strategies can further improve the adaptation process. This potentially allows the model to develop a more generalized capability that carries knowledge across different tasks, thereby filling gaps in the original pre-training objectives [2].

In summary, adapting pre-trained language models for evaluation tasks lacking corresponding pre-training objectives can be effectively achieved through continual pre-training, domain-specific fine-tuning, strategic prompt design, and leveraging multi-task learning strategies. The combination of these techniques ensures that the language model is harnessed to its full potential in targeted evaluative contexts.

1. [1]:  https://ar5iv.org/html/2407.14076, [2407.14076] Domain-Specific Pretraining of Language Models: A Comparative Study in the Medical Field
2. [2]:  https://ar5iv.org/html/2408.01402, [2408.01402] Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer
3. [3]:  https://ar5iv.org/html/2109.01048, [2109.01048] TravelBERT: Pre-training Language Model Incorporating Domain-specific Heterogeneous Knowledge into A Unified Representation
4. [4]:  https://ar5iv.org/html/2412.01377, [2412.01377] Adapting Large Language Models to Log Analysis with Interpretable Domain Knowledge
5. [5]:  https://ar5iv.org/html/2407.14076, [2407.14076] Domain-Specific Pretraining of Language Models: A Comparative Study in the Medical Field
---
1. [1]:  Passage ID 1: can we create specialized models, and can they really be as good as large general-purpose models in their domain? To answer these questions, we will take a look at different methods of training LLMs, domain-specific datasets and compare the benchmark results of specialized models to general-purpose models in benchmarks related to their domain. This paper will focus on the medical domain for all examples and comparisons.II PretrainingPretraining is the most fundamental step in creating intelligent and capable LLMs. During this training step, the model learns the structure of natural language and tries to memorize as much of the training data as possible. For causal language models, for example, this happens by inputting token sequences into the model and letting it predict the next token in the sequence. The closer the prediction is to the true next token in the sequence, the smaller the loss. The loss is used to adjust the weights of the model, in order to minimize the loss.
2. [2]:  Passage ID 2: large amounts of RL trajectories is challenging. Furthermore, our results highlight the importance of using pre-trained language models as a starting point for decision-making tasks and demonstrate the effectiveness of our prompt regularization methods in enhancing the model’s ability to distinguish task information contained in prompts.While LPDT has shown promising results, there are several limitations to our approach. Due to computing resource constraints, our language models are currently limited to GPT-2 and DistilGPT-2. To fully realize the potential of pre-trained language models for decision-making tasks, we hope to extend our approach to more open-source language models and utilize more efficient fine-tuning techniques in the future. Additionally, exploring alternative architectures or incorporating multi-task learning could further enhance the performance of LPDT. Future work will focus on addressing these limitations and expanding the scope of LPDT to a broader range of
3. [3]:  Passage ID 3: of the KG quality will hurt the pre-training process.4 Related Work4.1 Domain-specific Pre-trainingFine-tuning large PLMs Devlin et al. (2019); Brown et al. (2020); Radford et al. (2019, 2018) have achieved state-of-the-art results in downstream NLP tasks. Continued pre-training PLMs on a large corpus of unlabeled domain-specific text is helpful to the domain of a target task. Gururangan et al. (2020) investigate the impact of domain-adaptive pre-training and tasks-adaptive pre-training on language models. Further research on the domain adaptability ofPLMs has become a promising topic. For the scientific domain, there are SCIBERT Beltagy et al. (2019) and PatentBERT Lee and Hsiang (2019). For the biomedical domain, there are BioBERT Lee et al. (2020), extBERT Tai et al. (2020), PubMedBERT Gu et al. (2020), ClinicalBERT Huang et al. (2019), MT-ClinicalBERT Mulyar and McInnes (2020). For the financial domain, there are FinBERT Araci (2019); Liu et al. (2020b). Rongali et al.
4. [4]:  Passage ID 4: languages, which restricts their effectiveness in real-world applications. Our approach addresses these limitations by integrating interpretable domain knowledge into open-source LLMs through continual pre-training (CPT), enhancing performance on log tasks while retaining natural language processing capabilities. We created a comprehensive dataset, NLPLog, with over 250,000 question-answer pairs to facilitate this integration. Our model, SuperLog, trained with this dataset, achieves the best performance across four log analysis tasks, surpassing the second-best model by an average of 12.01%. Our contributions include a novel CPT paradigm that significantly improves model performance, the development of SuperLog with state-of-the-art results, and the release of a large-scale dataset to support further research in this domain. Subjects:Computation and Language (cs.CL); Software Engineering (cs.SE)Cite as:arXiv:2412.01377 [cs.CL] (or arXiv:2412.01377v1 [cs.CL] for this
5. [5]:  Passage ID 5: in comparison to the potential benefit of training on data in this format. Of course, we can first train our LLM on the normal cleaned data and then use this refined question-answer data later for finetuning.IV Performance of specialized LLMsAs mentioned previously, specialized LLMs are usually smaller than general-purpose LLMs. They are trained on less out-of-domain data, speeding up training time and reducing compute cost. In the following, we will take a look at medical LLMs trained with domain-specific pretraining and mixed-domain pretraining and compare their performance to general-purpose LLMs of equal or larger size in medical-related tasks.IV-A Domain-Specific Pretrained ModelsA well-known medicine-focused model is PubMedBERT. It is based on the BERT architecture [13] and trained on 14 million PubMed-abstracts with a total length of 3.1 billion words. The model was trained from scratch, only using the BERT architecture but not the pretrained weights of the