# Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models

## Question

How can pre-trained language models be adapted for evaluation tasks that lack corresponding pre-training objectives?

## URLs

1. https://ar5iv.org/html/2408.08661
2. https://ar5iv.org/html/2109.01048
3. https://ar5iv.org/html/2411.04448
4. https://ar5iv.org/html/2406.14833
5. https://ar5iv.org/html/2412.01377
6. https://ar5iv.org/html/2407.14076
7. https://ar5iv.org/html/2410.10739
8. https://ar5iv.org/html/2312.11502
9. https://ar5iv.org/html/2408.01402
10. https://ar5iv.org/html/2402.17400

## Answer

To adapt pre-trained language models (PLMs) for evaluation tasks that lack corresponding pre-training objectives, several strategies can be employed, drawing on insights from the provided context.

First, it is essential to recognize that many existing PLMs are primarily trained on high-resource languages, leaving minority languages and specific evaluation tasks underdeveloped [1]. As a starting point, researchers can create synthetic datasets tailored to the unique requirements of these evaluation tasks. For instance, in the context of healthcare or specific NLP applications, synthetic datasets can meet the gap left by the absence of labeled data [1]. These datasets can be generated using advanced tools like GPT-3.5-Turbo to ensure a diverse range of queries that encapsulate various user perspectives, such as those from laypersons or individuals with minimal technical expertise [2].

Moreover, when adapting PLMs for particular tasks, it is crucial to consider multi-task training. This involves using available downloadable task-specific corpora or building new ones, facilitating the model's ability to perform well across different tasks simultaneously [3]. By leveraging multi-task training, the model can effectively transfer knowledge from related tasks, thus improving its performance even in evaluation tasks that initially lack direct pre-training objectives.

Furthermore, enhancing the interpretability of PLM predictions can significantly influence the quality of evaluations. Techniques to explain model predictions can foster greater trust and understanding of the model's decisions, which is critical when the tasks being evaluated lack reliable pre-training signals. This approach is important in domains such as finance, but the principles can extend to other fields, promoting user confidence in the model’s outputs [4] [5].

Additionally, researchers must engage in continuous evaluation and refinement of the adaptation methodologies used for PLMs. This could involve conducting rigorous human-centric studies to measure efficacy, gathering feedback from end-users, and iteratively improving the training datasets and objectives used for fine-tuning the models [1]. By emphasizing human evaluation over solely relying on automatic metrics, researchers can ensure that the models are genuinely aligned with the users' needs and outcomes.

In summary, adapting PLMs for evaluation tasks that lack corresponding pre-training objectives involves creating synthetic datasets tailored to the tasks, employing multi-task training, enhancing interpretability of the models, and ensuring continuous human-centric evaluation. These strategies collectively address the challenges of gaps in training data and help optimize language models for diverse evaluation contexts.

[1]: https://ar5iv.org/html/2305.12544, No Title
[2]: https://ar5iv.org/html/2410.00427, No Title
[3]: https://ar5iv.org/html/1807.10854, No Title
[4]: https://ar5iv.org/html/2403.17561, No Title
[5]: https://ar5iv.org/html/2403.17561, No Title

[1]: Passage ID 1: as work to date has primarily focused on English or other high-resource languages Mondal et al. (2022) but devoted less efforts towards minority languages. Additionally, the lack of human evaluation of NLP-based health systems has made it challenging to measure their effectiveness in the real world. Current automatic evaluation metrics do not necessarily speak to patient outcomes. Hence, human-centric studies must be conducted in evaluating the efficacy of NLP-powered tools in healthcare.Research Directions.1.Healthcare benchmark construction. Although the documentation of recent LLMs reports very high performance for various medical question answering benchmarks, or medical licensing texts, there are many other tasks in healthcare that lack the data required to achieve similarly good performance. Access to medical datasets is often limited because of privacy issues, and therefore other approaches may be required to compile such benchmarks. Synthetic datasets are one such
[2]: Passage ID 2: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
[3]: Passage ID 3: have received. To train a model to perform a certain task well, the last step a practitioner must go through is to use available downloadable task-specific corpora, or build one’s own task-specific corpus. This last training step is usually supervised. It is also recommended that if several tasks are to be performed, multi-task training be used wherever possible.V ConclusionsEarly applications of natural language processing included a well-acclaimed but simpleminded algebra word problem solver program called STUDENT [272], as well as interesting but severely constrained conversational systems such as Eliza, which acted as a “psycho-therapist” [273]), and another that conversed about manipulating blocks in a microworld [274]. Nowadays, highly advanced applications of NLP are ubiquitous. These include Google’s and Microsoft’s machine translators, which translate more or less competently from a language to scores of other languages, as well as a number of devices which process
[4]: Passage ID 4: Researchers should explore techniques to explain the predictions of models, to improve trust and understanding of model decisions, which is essential for adoption in finance.4.3 Natural Language ProcessingNatural language processing (NLP) refers to the field of artificial intelligence that concerns with enabling computers to process, analyze and interpret human languages to extract useful information. Some of the common tasks in NLP are machine translation, text classification and text generation. Deep learning has been widely applied to solve real-world NLP problems. This section presents the recent advancements in deep learning models that have been designed for NLP over the past few years.4.3.1 Text ClassificationText classification known as text categorization, is a task that involves assigning predefined categories or labels to a piece of text based on its content. The task is commonly used in various applications such as document classification, sentiment analysis
[5]: Passage ID 5: Researchers should explore techniques to explain the predictions of models, to improve trust and understanding of model decisions, which is essential for adoption in finance.4.3 Natural Language ProcessingNatural language processing (NLP) refers to the field of artificial intelligence that concerns with enabling computers to process, analyze and interpret human languages to extract useful information. Some of the common tasks in NLP are machine translation, text classification and text generation. Deep learning has been widely applied to solve real-world NLP problems. This section presents the recent advancements in deep learning models that have been designed for NLP over the past few years.4.3.1 Text ClassificationText classification known as text categorization, is a task that involves assigning predefined categories or labels to a piece of text based on its content. The task is commonly used in various applications such as document classification, sentiment analysis