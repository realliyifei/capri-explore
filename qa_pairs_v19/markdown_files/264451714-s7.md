# Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models

## Question

How can pre-trained language models be adapted for evaluation tasks that lack corresponding pre-training objectives?

## URLs

1. https://ar5iv.org/html/2408.08661. [2408.08661] MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector
2. https://ar5iv.org/html/2109.01048. [2109.01048] TravelBERT: Pre-training Language Model Incorporating Domain-specific Heterogeneous Knowledge into A Unified Representation
3. https://ar5iv.org/html/2411.04448. [2411.04448] Gradient Localization Improves Lifelong Pretraining of Language Models
4. https://ar5iv.org/html/2406.14833. [2406.14833] Efficient Continual Pre-training by Mitigating the Stability Gap
5. https://ar5iv.org/html/2412.01377. [2412.01377] Adapting Large Language Models to Log Analysis with Interpretable Domain Knowledge
6. https://ar5iv.org/html/2407.14076. [2407.14076] Domain-Specific Pretraining of Language Models: A Comparative Study in the Medical Field
7. https://ar5iv.org/html/2410.10739. [2410.10739] Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs
8. https://ar5iv.org/html/2312.11502. [2312.11502] Labrador: Exploring the limits of masked language modeling for laboratory data
9. https://ar5iv.org/html/2408.01402. [2408.01402] Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer
10. https://ar5iv.org/html/2402.17400. [2402.17400] Investigating Continual Pretraining in Large Language Models: Insights and Implications

## Answer (I don't know?: ✗)

To adapt pre-trained language models (PLMs) for evaluation tasks that lack corresponding pre-training objectives, several strategies can be employed. These strategies generally involve fine-tuning the models on task-specific datasets and leveraging their existing pre-trained capabilities.

1. **Fine-Tuning on Task-Specific Datasets**: One fundamental approach is to take a PLM and undergo a supervised fine-tuning phase using datasets that are specifically designed for the evaluation tasks at hand. This approach allows the model to learn from annotated data that aligns closely with the objectives of the evaluation tasks, thereby equipping it with the knowledge required to perform well in these specific contexts [2][5]. For instance, if a model needs to evaluate summarization capabilities, it can be fine-tuned on datasets that consist of example source texts and their corresponding summaries.

2. **Multi-Task Training**: To further enhance performance across related tasks, practitioners are encouraged to use multi-task training. By designing a training regimen that spans multiple evaluation tasks, a single model can learn generalized representations that are beneficial across different objectives. This can be particularly effective when tasks share common linguistic features or when data from several tasks can be aggregated into a cohesive training set [2][5].

3. **Use of Synthetic Data**: In scenarios where real annotated datasets are sparse or inaccessible, synthetic data generation can fill the gap. For example, as mentioned, a synthetic multi-class dataset was created by prompting a model like GPT-3.5-Turbo to generate questions tailored to specific personas [4]. Such synthetic datasets can both increase the volume of training data and introduce variability that can help a model learn effective representations for tasks that lack explicit pre-training objectives.

4. **Leveraging Existing Knowledge from Pre-Training**: PLMs are initially trained on vast amounts of text data, which helps them learn language patterns and context comprehension [1][3]. By utilizing the knowledge encoded in these models and adapting it with minimal added supervision, models can perform well on evaluation tasks, even if these tasks diverge from the original pre-training objectives. It is crucial to identify relevant aspects of the pre-training data that align with the evaluation tasks to optimize adaptation.

5. **Evaluation Benchmarks Alignment**: Lastly, ensuring that the evaluation benchmarks used match the capabilities of the models and the structure learned during pre-training is vital. Established benchmarks often provide useful insights into the model's performance and can guide the process of fine-tuning and evaluation design [1][2].

In summary, adapting pre-trained language models for evaluation tasks that lack corresponding pre-training objectives involves a strategic combination of fine-tuning on task-specific datasets, utilizing synthetic data, employing multi-task training, and leveraging existing knowledge encoded during pre-training. This multifaceted approach maximizes the model's capabilities and improves its performance in specialized evaluation contexts.

1. [1]:  https://ar5iv.org/html/2409.16202, No Title
2. [2]:  https://ar5iv.org/html/2407.14076, [2407.14076] Domain-Specific Pretraining of Language Models: A Comparative Study in the Medical Field
3. [3]:  https://ar5iv.org/html/2310.17271, No Title
4. [4]:  https://ar5iv.org/html/2410.00427, No Title
5. [5]:  https://ar5iv.org/html/1807.10854, No Title
---
1. [1]:  Passage ID 1: human-like text, answer complex questions, and perform a wide array of natural language processing (NLP) tasks. Models such as GPT (Achiam et al., 2023), LLaMA (Touvron et al., 2023), Baichuan (Yang et al., 2023), Qwen (Bai et al., 2023), GLM (Du et al., 2022), and DeepSeek-V2 (DeepSeek-AI, 2024) have set new standards, demonstrating exceptional performance across various benchmarks. These models have been widely adopted in both academic and industrial settings, highlighting their versatility and potential. However, the evaluation of LLMs is a multifaceted challenge that extends beyond mere accuracy in answering questions. It involves assessing their ability to understand context, generate coherent and relevant responses, and apply specialized knowledge effectively.Evaluation BenchmarksThe evaluation of LLMs has traditionally relied on established NLP benchmark datasets, such as SQuAD (Rajpurkar et al., 2016), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019), which focus
2. [2]:  Passage ID 2: can we create specialized models, and can they really be as good as large general-purpose models in their domain? To answer these questions, we will take a look at different methods of training LLMs, domain-specific datasets and compare the benchmark results of specialized models to general-purpose models in benchmarks related to their domain. This paper will focus on the medical domain for all examples and comparisons.II PretrainingPretraining is the most fundamental step in creating intelligent and capable LLMs. During this training step, the model learns the structure of natural language and tries to memorize as much of the training data as possible. For causal language models, for example, this happens by inputting token sequences into the model and letting it predict the next token in the sequence. The closer the prediction is to the true next token in the sequence, the smaller the loss. The loss is used to adjust the weights of the model, in order to minimize the loss.
3. [3]:  Passage ID 3: use of pre-trained language models (PLMs), such as BERT (Devlin et al., 2019),T5 (Raffel et al., 2020), GPT-3 (Brown et al., 2020) and many more, has led to significant advances in natural language processing (NLP) tasks, with many achieving state-of-the-art results. However, the exact mechanisms by which these models learn222We may interchangeably use terms such as ‘learn’, ‘comprehend’ or ‘understand’ to refer to the ability of PLMs to understand language. These terms are used in the context of ‘natural language understanding’ and ‘machine reading comprehension’ as downstream tasks, highlighting the capabilities of such models to perform well. and represent language are still not fully understood.Figure 1: Humans are able to understand natural language text consisting of words with altered letters (e.g., part bold, or entiely msig letters). Can language models do the same?The pre-training data evidently plays an important role in the downstream performance of PLMs Kaplan
4. [4]:  Passage ID 4: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
5. [5]:  Passage ID 5: have received. To train a model to perform a certain task well, the last step a practitioner must go through is to use available downloadable task-specific corpora, or build one’s own task-specific corpus. This last training step is usually supervised. It is also recommended that if several tasks are to be performed, multi-task training be used wherever possible.V ConclusionsEarly applications of natural language processing included a well-acclaimed but simpleminded algebra word problem solver program called STUDENT [272], as well as interesting but severely constrained conversational systems such as Eliza, which acted as a “psycho-therapist” [273]), and another that conversed about manipulating blocks in a microworld [274]. Nowadays, highly advanced applications of NLP are ubiquitous. These include Google’s and Microsoft’s machine translators, which translate more or less competently from a language to scores of other languages, as well as a number of devices which process