# Relational World Knowledge Representation in Contextual Language Models: A Review

## Question

How does linearizing knowledge base triples enhance the training of language models?

## URLs

1. https://ar5iv.org/html/2402.11541. [2402.11541] Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought
2. https://ar5iv.org/html/2407.09506. [2407.09506] Integrating Large Language Models with Graph-based Reasoning for Conversational Question Answering
3. https://ar5iv.org/html/2410.20724. [2410.20724] Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation
4. https://ar5iv.org/html/2306.08302. [2306.08302] Unifying Large Language Models and Knowledge Graphs: A Roadmap
5. https://ar5iv.org/html/2406.08223. [2406.08223] Research Trends for the Interplay between Large Language Models and Knowledge Graphs
6. https://ar5iv.org/html/2409.10294. [2409.10294] MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation
7. https://ar5iv.org/html/2308.14436. [2308.14436] Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA
8. https://ar5iv.org/html/2409.03155. [2409.03155] Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models

## Answer (I don't know?: ✓)

I don't know. The context provided does not include specific information about how linearizing knowledge base triples enhances the training of language models. Although some passages discuss the utilization of knowledge graphs (KGs) in neural machine translation (NMT) and question generation, they do not explicitly address the process of linearization or its impact on the training of language models. Therefore, the context is insufficient to accurately answer your question.

1. [1]:  https://ar5iv.org/html/2410.00427, No Title
2. [2]:  https://ar5iv.org/html/2009.07715, No Title
3. [3]:  https://ar5iv.org/html/2410.00427, No Title
4. [4]:  https://ar5iv.org/html/2401.11972, No Title
5. [5]:  https://ar5iv.org/html/2009.07715, No Title
---
1. [1]:  Passage ID 1: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
2. [2]:  Passage ID 2: showed that the neural model substantially improves over two strong baselines, both in terms of the accuracy of referring expressions and the fluency of lexicalized texts.3.1.4 KG-NMT: Utilizing Knowledge Graphs for Neural Machine Translation AugmentationWe asked the following question in Section 1.1.3:RQ5:Can an NMT model enhanced with a bilingual KG improve translation quality?We answered the above mentioned question by presenting KG-NMT. KG-NMT is the first augmentation methodology, which relies on the use of KG to improve the performance of NMT systems for translating domain-specific expressions and named entities in texts. We implemented two strategies for incorporating KGEs into NMT models that work on word- and sub-word units-based models. Additionally, we carried out an extensive evaluation with a manual analysis, which showed consistent translation improvements provided by incorporating DBpedia KG in NMT. The overall methodology can be applied to any NMT
3. [3]:  Passage ID 3: ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the laws of thermodynamics?” Following a quality inspection of the synthetically produced questions, we assembled a training dataset of 1601 examples, consisting of 120 questions for each of the 12 topics and 161 general questions. We also derived a test dataset containing 364 examples with a balanced class distribution similar to the training dataset.In our experiments, we evaluated three classification approaches: vector similarity search, prompting a LLM (GPT-3.5-Turbo), and few-shot fine-tuning of a transformer model with the SetFit framework Tunstall et al. (2022). Concerning the vector search approach with the SPECTER2 model, we measured the cosine similarity to compare vectors of embedded user queries with paper embeddings in our vector database to retrieve the 100 most similar
4. [4]:  Passage ID 4: et al. (2018) utilized both textual and structured context of the knowledge base Freebase (Bollacker et al., 2008). The authors extracted the triplets, text descriptions containing the subject and object of the triplet, and the phrase containing the lexicalization of the predicate of the triplet from the knowledge base. Both the textual and structured information extracted were fed as input to an attention-based encoder-decoder model for question generation. Instead of directly injecting the knowledge graph triplets as input to the question generation model,Kumar et al. (2019) observed a significant improvement in the performance, when the embedding representation of the triplet was utilized as the input. The authors used pre-trained TransE embeddings (Bordes et al., 2013) of Freebase from OpenKE tool (Han et al., 2018) for the experiment. Following these studies, further advancements were observed in this direction by integrating graph neural networks for embedding knowledge graphs
5. [5]:  Passage ID 5: Li et al., 2018). Despite the significant advancement of previous work in NMT, translating entities and terminological expressions remains a challenge (Koehn and Knowles, 2017) and none of the above mentioned approaches have exploited the application of KGs in NMT systems. Hence it leads to the following research question:{mycolorrq}RQ5:Can an NMT model enhanced with a bilingual KG improve translation quality?1.1.4 Challenge 4: Low-resource Knowledge GraphsConsiderable amounts of partly human effort have been invested in making KGs available across languages. However, even popular KGs like DBpedia and Wikidata are most abundant in their English version (Lakshen et al., 2018). Additionally, region-specific facts are often limited to the KG specific to the region from which they emanate or to the KG in the language spoken in said region (Aprosio et al., 2013). This lack of multilingual knowledge availability limits the porting of NLP tasks such as EL, NLG, and NMT to