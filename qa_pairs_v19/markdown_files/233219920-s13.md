# Relational World Knowledge Representation in Contextual Language Models: A Review

## Question

How does linearizing knowledge base triples enhance the training of language models?

## URLs

1. https://ar5iv.org/html/2402.11541. [2402.11541] Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought
2. https://ar5iv.org/html/2407.09506. [2407.09506] Integrating Large Language Models with Graph-based Reasoning for Conversational Question Answering
3. https://ar5iv.org/html/2410.20724. [2410.20724] Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation
4. https://ar5iv.org/html/2306.08302. [2306.08302] Unifying Large Language Models and Knowledge Graphs: A Roadmap
5. https://ar5iv.org/html/2406.08223. [2406.08223] Research Trends for the Interplay between Large Language Models and Knowledge Graphs
6. https://ar5iv.org/html/2409.10294. [2409.10294] MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation
7. https://ar5iv.org/html/2308.14436. [2308.14436] Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA
8. https://ar5iv.org/html/2409.03155. [2409.03155] Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models

## Answer (I don't know?: ✗)

The process of linearizing knowledge base (KG) triples enhances the training of language models (LLMs) primarily by facilitating the comprehension and reasoning capabilities of these models when processing structured information. Specific insights into this enhancement can be drawn from the provided context.

Firstly, researchers have shifted towards using linear representations of KG triples as a method to simplify the integration of structured knowledge into LLMs. This involves converting complex KG into a more manageable form through linearization, which allows the LLMs to access the information in a format they can process more effectively [3][5]. By organizing structured knowledge into linearly represented triples or reasoning paths, researchers create inputs that maintain essential information while also being digestible by LLMs, thereby enhancing the model’s capacity for knowledge comprehension [3].

Moreover, the existing work highlights that these linearized forms of knowledge can compensate for the loss of structured information that occurs when KGs are converted into natural language (NL) representations. When researchers organize knowledge correctly, it minimizes ambiguity and maximizes clarity, which is critical for LLMs to draw accurate inferences from the data they are trained on [3]. As mentioned in the context, the process involves not just converting triples into simple statements but also ensuring that these representations convey logical relationships among entities clearly.

Additionally, a significant finding in the research underlines that LLMs can handle noisy and messy KG information remarkably well when it is presented in a linearized format. This challenges some traditional assumptions about the necessity of high-quality NL prompts for effective model training, suggesting that a focus on structured knowledge, even if linearized and imperfect, allows LLMs to perform better than when relying solely on well-crafted NL transformations [5]. This counterintuitive outcome encourages further exploration into how streamlined structured data can serve as a foundation for training LLMs, enhancing their ability to understand and generate responses based on KG knowledge.

The emphasis on integrating diverse knowledge through linearization is illustrated by the use of different prompting methods that evaluate the efficacy of KG knowledge injection into LLMs. Techniques that effectively incorporate linear representations of KGs provide a platform for LLMs to reason through questions and derive meaning from complex datasets, thus optimizing their training and performance in tasks related to knowledge understanding and question answering [5].

In summary, linearizing knowledge base triples aids in the enhancement of LLM training by simplifying access to structured information and allowing models to leverage their existing capabilities to interpret this data effectively. This practice not only reduces complexity but also fosters an environment wherein LLMs can thrive even with suboptimal input representations [3][5].

1. [1]:  https://ar5iv.org/html/2410.00427, No Title
2. [2]:  https://ar5iv.org/html/2402.11541, [2402.11541] Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought
3. [3]:  https://ar5iv.org/html/2402.11541, [2402.11541] Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought
4. [4]:  https://ar5iv.org/html/2112.05452, No Title
5. [5]:  https://ar5iv.org/html/2402.11541, [2402.11541] Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought
---
1. [1]:  Passage ID 1: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
2. [2]:  Passage ID 2: discussed in Section 2, the NL text generated by the underperforming triple-to-text generation model will affect LLM’s ability to understand KG and further perform CQA.To eliminate the impact of the Triple-to-text model on LLM, we leverage a Relation Extraction (RE) benchmark to generate QA pairs, in which the document and its relevant ground-truth triples are provided.Since the triples in each document can represent the logical structure of the entities, we consider them as a comprehensive knowledge graph mapping the document.Specifically, to generate QA pairs based on each document, we adopted the dataset construction method employed in LC-QuAD 2.0 Dubey et al. (2019). We first filled the triples into many different templates and constructed questions, and then used ChatGPT to paraphrase the questions, ensuring the diversity and complexity of questions. Following this approach, as illustrated in Figure 2(b), we generated 1-hop, 2-hop, and 3-hop questions for each document. This
3. [3]:  Passage ID 3: of LLMs depends on the KG inference module (maybe need training). However, this approach lacks the consideration for leveraging LLMs in the process of KG understanding and reasoning.Consequently, researchers have recently shown a keen interest in exploring how to supply high-quality relevant knowledge to pre-trained LLMs via constructing optimal prompts, thereby facilitating the model’s comprehension of KG Sorensen et al. (2022); White et al. (2023); Li et al. (2023); Wen et al. (2023); Hu et al. (2024). This more lightweighted approach involves converting the KG into linearly represented triples, reasoning paths, or natural language (NL) textual representations (sentences or a paragraph) and concatenating them to the input prompt to query the LLMs. To compensate for the loss of structured information, researchers strive to generate NL texts by effectively organizing structured knowledge Sun et al. (2020); Brate et al. (2022). However, the generation of KG-to-text itself poses a
4. [4]:  Passage ID 4: training negatively affects mostly the recall which is the most important metric for the intended goal.Hence, assuming NL answers of very high quality are provided by a QA system, then our approach should be capable of identifying (and therefore, filtering) incorrect answers.VI-B Experiment 2: NLG of limited qualityIn this setting, we are evaluating artificially generated NL answers.There are computed automatically from SPARQL queries and the corresponding results.To our best knowledge, there is no KGQA system available that is providing an API to produce full-fledged NL answers.Consequently, it is required to generate answer verbalization from the available information (i.e., SPARQL query candidates).Generating artificial answers in a three-step process includes (1) providing a question in textual form to the KGQA system, (2) sending the computed SPARQL query answer candidates to Wikidata, and (3) generating NL representation from the obtained list of the query
5. [5]:  Passage ID 5: knowledge in KGs on-the-fly remains inadequate.Researchers often co-train KG embeddings and LLM parameters to equip LLMs with the ability of comprehending KG knowledge.However, this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, black-box LLMs.In this paper, we employ complex question answering (CQA) as a task to assess the LLM’s ability of comprehending KG knowledge.We conducted a comprehensive comparison of KG knowledge injection methods (from triples to natural language text), aiming to explore the optimal prompting method for supplying KG knowledge to LLMs, thereby enhancing their comprehension of KG.Contrary to our initial expectations, our analysis revealed that LLMs effectively handle messy, noisy, and linearized KG knowledge, outperforming methods that employ well-designed natural language (NL) textual prompts.This counter-intuitive finding provides substantial insights for future research on