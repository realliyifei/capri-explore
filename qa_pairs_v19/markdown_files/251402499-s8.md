# Abstractive Meeting Summarization: A Survey

## Question

What are the challenges and limitations of using the ROUGE metric for evaluating abstractive meeting summaries?

## URLs

1. https://ar5iv.org/html/2403.05186. [2403.05186] ROUGE-K: Do Your Summaries Have Keywords?
2. https://ar5iv.org/html/2409.02413. [2409.02413] Abstractive Text Summarization: State of the Art, Challenges, and Improvements
3. https://ar5iv.org/html/2406.07494. [2406.07494] CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization
4. https://ar5iv.org/html/2406.00303. [2406.00303] Multi-Dimensional Optimization for Text Summarization via Reinforcement Learning
5. https://ar5iv.org/html/2404.11124. [2404.11124] What’s under the hood: Investigating Automatic Metrics on Meeting Summarization
6. https://ar5iv.org/html/2411.18444. [2411.18444] Is my Meeting Summary Good? Estimating Quality with a Multi-LLM Evaluator
7. https://ar5iv.org/html/2407.00747. [2407.00747] A Comparative Study of Quality Evaluation Methods for Text Summarization

## Answer (I don't know?: ✗)

The challenges and limitations of using the ROUGE metric for evaluating abstractive meeting summaries are significant and well-documented in recent research. First and foremost, ROUGE, which stands for Recall-Oriented Understudy for Gisting Evaluation, is primarily designed for extractive summarization and often fails to capture the complexities involved in abstractive summarization. One notable limitation is its struggle with structural disorganization errors, which ROUGE tends to penalize appropriately. However, it rewards hallucination errors, which can lead to misleading evaluations of summary quality [2].

ROUGE is recognized for being proficient at penalizing omissions—highlighting the absence of crucial information in summaries. Yet, this aspect is inadequate when assessing the high abstraction level inherent in meeting summaries. Meeting summarization requires complex reasoning and synthesis of information, where reliance solely on ROUGE can mask significant evaluation flaws due to its crude measurement approach [1][2]. 

Furthermore, while ROUGE serves as a default metric for many summarization tasks, its limitations become even more pronounced in the context of meeting summarization. The challenges specific to this field include not only high abstraction levels but also low extraction rates, further complicating ROUGE's effectiveness [1]. The ROUGE score, being primarily concerned with n-gram overlap, often fails to account for coherence and fluency, both of which are critical for evaluating summary quality in a more nuanced manner [3].

Moreover, the mismatch between ROUGE evaluations and human assessments has been previously highlighted, indicating a gap between automatic scoring and qualitative judgments that human evaluators might provide. Research shows that newer models, such as LLM-based metrics, are emerging that could potentially address some of these shortcomings by better aligning with human judgment about fluency, coverage, and coherence [3].

In summary, while ROUGE remains the most widely used evaluation metric for summarization, its challenges—such as the penalization of structural disorganization errors and the inadequate handling of abstraction in meeting summaries—significantly undermine its utility as a standalone assessment tool. The development and exploration of advanced metrics that can better capture the complexities of human language and summarization are greatly needed as the field evolves [5].

1. [1]:  https://ar5iv.org/html/2404.11124, [2404.11124] What’s under the hood: Investigating Automatic Metrics on Meeting Summarization
2. [2]:  https://ar5iv.org/html/2404.11124, [2404.11124] What’s under the hood: Investigating Automatic Metrics on Meeting Summarization
3. [3]:  https://ar5iv.org/html/2406.07494, [2406.07494] CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization
4. [4]:  https://ar5iv.org/html/2404.11124, [2404.11124] What’s under the hood: Investigating Automatic Metrics on Meeting Summarization
5. [5]:  https://ar5iv.org/html/2406.07494, [2406.07494] CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization
---
1. [1]:  Passage ID 1: and discourse structure, existing evaluation metrics often inadequately reflect these Rennard et al. (2023); Kumar and Kabiri (2022).The widespread default use of the ROUGE metric Lin (2004) has shown limitations Liu and Liu (2008); Cohan and Goharian (2016); Fabbri et al. (2021).While the research community acknowledges the potential of newer metrics, such as BERTScore Zhang et al. (2020b), their efficacy in meeting summarization remains uncharted.This is a crucial shortcoming, as the high abstraction level, low extraction rate, and the requirement for complex reasoning power of meeting summarization present new challenges to automatic evaluation metrics, which need to be explored Gao and Wan (2022).Our study aims to investigate how commonly used automatic metrics correlate with human annotations, exploring the inherent qualities these metrics encapsulate in their scores.To unify the fragmented understanding of challenges posed by meeting summarization and the typical errors that
2. [2]:  Passage ID 2: annotations, exploring the inherent qualities these metrics encapsulate in their scores.To unify the fragmented understanding of challenges posed by meeting summarization and the typical errors that manifest when these challenges are unmet.Using the QMSum dataset Zhong et al. (2021), we employ experts to annotate challenges in meeting transcripts and errors in summaries.Various models, including domain-standard encoder-decoder architectures and notable autoregressive models, generated these summaries.Our methodological approach enables us to draw correlations between the challenges and errors, the annotated errors, and a set of eight automatic metrics.The insights derived reveal shortcomings in the current automatic metrics employed for meeting summarization evaluation:Structural disorganization errors are often penalized, reflecting human assessments, whereas hallucination errors tend to be rewarded.While adept at penalizing omissions, the ROUGE Lin (2004) metric struggles to
3. [3]:  Passage ID 3: metrics (e.g., ROUGE for dialogue summarization as identified in Table 1) is limited (?, ?), and while it can serve as a proxy (?) it provides insufficient insights into the true efficacy of new techniques.The also popular model-based metrics (e.g., BARTScore) seem unable to align well with human judgments (?) for dialogue summarization.Recent developments in NLP use LLM-based metrics such as GEMBA (?) and ICE (?) for evaluation (?), building on LLMs’ advanced text comprehension.This set of metrics thereby mimics human judgment and assesses common aspects (e.g., fluency, coverage, coherence) with continuous (?), Likert scale (?, ?), or probability scores (?).LLM-based metrics offer a promising direction for dialogue summarization evaluation due to their customizability, though this area remains largely unexplored.7 Final Considerations7.1 Limitations of EvidenceGiven our selection of papers on dialogue summarization, we found limitations in current works that may bias
4. [4]:  Passage ID 4: Liu and Yang Liu. 2008.Correlation between ROUGE and Human Evaluation of Extractive Meeting Summaries.In Proceedings of ACL-08: HLT, Short Papers, pages 201–204, Columbus, Ohio. Association for Computational Linguistics.Liu et al. (2021)Zhengyuan Liu, Ke Shi, and Nancy F. Chen. 2021.Coreference-Aware Dialogue Summarization.ArXiv:2106.08556 [cs].Ma et al. (2022)Congbo Ma, Wei Emma Zhang, Mingyu Guo, Hu Wang, and Quan Z. Sheng. 2022.Multi-document summarization via deep learning techniques: A survey.ACM Comput. Surv., 55(5).Maddela et al. (2023)Mounica Maddela, Yao Dou, David Heineman, and Wei Xu. 2023.LENS: A Learnable Evaluation Metric for Text Simplification.In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16383–16408, Toronto, Canada. Association for Computational Linguistics.Maynez et al. (2020)Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald.
5. [5]:  Passage ID 5: considerable progress, mainly due to training methods, others, such as comprehension, factuality, and salience, remain difficult and hold significant research opportunities.We further investigate how these approaches are typically assessed, covering the datasets for the subdomains of dialogue (e.g., meeting, customer service, and medical), the established automatic metrics (e.g., ROUGE), and common human evaluation approaches for assigning scores and evaluating annotator agreement.We observe that only a few datasets with community interest span across all subdomains.Despite its limitations, the ROUGE metric is the most commonly used, while human evaluation, considered the gold standard, is frequently reported without sufficient detail on inner-annotator agreement and annotation guidelines.Additionally, we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge