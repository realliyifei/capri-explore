# Abstractive Meeting Summarization: A Survey

## Question

What are the challenges and limitations of using the ROUGE metric for evaluating abstractive meeting summaries?

## URLs

1. https://ar5iv.org/html/2403.05186. [2403.05186] ROUGE-K: Do Your Summaries Have Keywords?
2. https://ar5iv.org/html/2409.02413. [2409.02413] Abstractive Text Summarization: State of the Art, Challenges, and Improvements
3. https://ar5iv.org/html/2406.07494. [2406.07494] CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization
4. https://ar5iv.org/html/2406.00303. [2406.00303] Multi-Dimensional Optimization for Text Summarization via Reinforcement Learning
5. https://ar5iv.org/html/2404.11124. [2404.11124] What’s under the hood: Investigating Automatic Metrics on Meeting Summarization
6. https://ar5iv.org/html/2411.18444. [2411.18444] Is my Meeting Summary Good? Estimating Quality with a Multi-LLM Evaluator
7. https://ar5iv.org/html/2407.00747. [2407.00747] A Comparative Study of Quality Evaluation Methods for Text Summarization

## Answer (I don't know?: ✗)

The ROUGE metric, widely used for evaluating summarization quality, faces notable challenges and limitations, especially concerning abstractive meeting summaries. One primary challenge is its inability to accurately reflect the complexities specific to meeting summarization. Meetings often involve high levels of abstraction, low extraction rates, and require complex reasoning, factors that ROUGE does not adequately account for [1]. 

Additionally, while ROUGE is proficient at penalizing omissions in generated summaries, it struggles with structural disorganization errors. Specifically, when structural flaws occur in a summary, ROUGE appropriately reflects human assessments by penalizing these errors, but paradoxically, it tends to reward hallucination errors, where the summary might fabricate information that is not present in the original meeting content [2]. This discrepancy has been highlighted in recent studies, indicating a misalignment between what ROUGE scores and human evaluations perceive as quality in meeting summaries.

Furthermore, ROUGE’s focus on surface-level n-gram overlaps can result in oversimplification. It fails to capture deeper semantic meanings and nuances inherent in the generated summaries, particularly when the summaries, while being semantically accurate, differ significantly in phrasing or structure from the reference texts [5]. This limitation is especially critical in the context of meeting summaries, where the essence of the discussion may be lost if the evaluation metric does not recognize the underlying semantic content. Profoundly, the research community is recognizing this shortcoming and has started exploring alternative metrics, such as BERTScore, which leverage the capabilities of pre-trained language models to provide more nuanced evaluations of semantic similarities [1] [5].

Overall, while ROUGE remains a default metric due to its simplicity and ease of application, its inadequacies in reflecting the unique challenges posed by meeting summarization warrant further exploration into more robust evaluation methods that can accurately assess both surface-level and deeper semantic qualities of generated summaries.

1. [1]:  https://ar5iv.org/html/2404.11124, [2404.11124] What’s under the hood: Investigating Automatic Metrics on Meeting Summarization
2. [2]:  https://ar5iv.org/html/2404.11124, [2404.11124] What’s under the hood: Investigating Automatic Metrics on Meeting Summarization
3. [3]:  https://ar5iv.org/html/2404.11124, [2404.11124] What’s under the hood: Investigating Automatic Metrics on Meeting Summarization
4. [4]:  https://ar5iv.org/html/2409.02413, [2409.02413] Abstractive Text Summarization: State of the Art, Challenges, and Improvements
5. [5]:  https://ar5iv.org/html/2409.02413, [2409.02413] Abstractive Text Summarization: State of the Art, Challenges, and Improvements
---
1. [1]:  Passage ID 1: and discourse structure, existing evaluation metrics often inadequately reflect these Rennard et al. (2023); Kumar and Kabiri (2022).The widespread default use of the ROUGE metric Lin (2004) has shown limitations Liu and Liu (2008); Cohan and Goharian (2016); Fabbri et al. (2021).While the research community acknowledges the potential of newer metrics, such as BERTScore Zhang et al. (2020b), their efficacy in meeting summarization remains uncharted.This is a crucial shortcoming, as the high abstraction level, low extraction rate, and the requirement for complex reasoning power of meeting summarization present new challenges to automatic evaluation metrics, which need to be explored Gao and Wan (2022).Our study aims to investigate how commonly used automatic metrics correlate with human annotations, exploring the inherent qualities these metrics encapsulate in their scores.To unify the fragmented understanding of challenges posed by meeting summarization and the typical errors that
2. [2]:  Passage ID 2: annotations, exploring the inherent qualities these metrics encapsulate in their scores.To unify the fragmented understanding of challenges posed by meeting summarization and the typical errors that manifest when these challenges are unmet.Using the QMSum dataset Zhong et al. (2021), we employ experts to annotate challenges in meeting transcripts and errors in summaries.Various models, including domain-standard encoder-decoder architectures and notable autoregressive models, generated these summaries.Our methodological approach enables us to draw correlations between the challenges and errors, the annotated errors, and a set of eight automatic metrics.The insights derived reveal shortcomings in the current automatic metrics employed for meeting summarization evaluation:Structural disorganization errors are often penalized, reflecting human assessments, whereas hallucination errors tend to be rewarded.While adept at penalizing omissions, the ROUGE Lin (2004) metric struggles to
3. [3]:  Passage ID 3: Liu and Yang Liu. 2008.Correlation between ROUGE and Human Evaluation of Extractive Meeting Summaries.In Proceedings of ACL-08: HLT, Short Papers, pages 201–204, Columbus, Ohio. Association for Computational Linguistics.Liu et al. (2021)Zhengyuan Liu, Ke Shi, and Nancy F. Chen. 2021.Coreference-Aware Dialogue Summarization.ArXiv:2106.08556 [cs].Ma et al. (2022)Congbo Ma, Wei Emma Zhang, Mingyu Guo, Hu Wang, and Quan Z. Sheng. 2022.Multi-document summarization via deep learning techniques: A survey.ACM Comput. Surv., 55(5).Maddela et al. (2023)Mounica Maddela, Yao Dou, David Heineman, and Wei Xu. 2023.LENS: A Learnable Evaluation Metric for Text Simplification.In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16383–16408, Toronto, Canada. Association for Computational Linguistics.Maynez et al. (2020)Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald.
4. [4]:  Passage ID 4: available extractive and abstractive summarization algorithms in terms of ROUGE scores, human evaluation, and abstractiveness scores.Celikyilmaz et al. [95] generated abstractive summaries for lengthy documents by utilizing deep communicating agents within an encoder-decoder architecture. Multiple working agents, each responsible for a subsection of the input text, collaborate to complete the encoding operation. For the purpose of generating a targeted and comprehensive summary, these encoders are coupled with a single decoder that has been end-to-end trained using RL. In comparison to a single encoder or multiple non-communicating encoders, the results showed that multiple communicating encoders generate summaries of higher quality. Maximum likelihood (MLE), semantic cohesion, and RL loss were optimized during the training. Intermediate rewards, based on differential ROUGE measures, were incorporated to encourage unique sentence creation. Experiments conducted on the CNN/DailyMail
5. [5]:  Passage ID 5: language models capable of understanding and representing a broader vocabulary are crucial for improving the accuracy and richness of generated summaries.5.2.3 Evaluation MetricsEvaluating the quality of generated summaries remains a challenge [175, 176]. Traditional metrics, such as ROUGE, have been the cornerstone for evaluation due to their simplicity and ease of use. However, while ROUGE excels at doing surface-level comparisons between the generated summary and the reference, it often falls short of capturing deeper semantic similarities and nuances. This limitation becomes particularly evident when summaries, though semantically accurate, use phrasings or structures different from those in the reference text. Recognizing these shortcomings, the research community has been exploring alternative metrics. BERTScore [22], for instance, leverages the power of pre-trained LLMs to evaluate summaries based on contextual embeddings, offering a more nuanced measure of semantic