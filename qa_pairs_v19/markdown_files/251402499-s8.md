# Abstractive Meeting Summarization: A Survey

## Question

What are the challenges and limitations of using the ROUGE metric for evaluating abstractive meeting summaries?

## URLs

1. https://ar5iv.org/html/2403.05186
2. https://ar5iv.org/html/2409.02413
3. https://ar5iv.org/html/2406.07494
4. https://ar5iv.org/html/2406.00303
5. https://ar5iv.org/html/2404.11124
6. https://ar5iv.org/html/2411.18444
7. https://ar5iv.org/html/2407.00747

## Answer

The ROUGE metric, widely employed for the evaluation of summarization tasks, presents several challenges and limitations, particularly in the context of abstractive meeting summaries. First and foremost, it is evident that ROUGE struggles with a fundamental aspect of meeting summarization: its ability to adequately assess the structural organization of summaries. While ROUGE is proficient at penalizing omissions, it has been shown to inadequately reflect human judgments when it comes to what are known as hallucination errors—instances where generated summaries include fabricated information. Instead of being penalized, these errors may sometimes receive a reward in terms of ROUGE scoring, thus skewing the evaluation process [1] [2].

In addition, a significant shortcoming of ROUGE lies in its abstraction level and low extraction rate capabilities. Meeting summarization typically necessitates complex reasoning and a higher abstraction level compared to other summarization tasks. The complexity inherent in meetings presents new challenges that ROUGE does not effectively account for, particularly in aligning with human evaluations. Specifically, the metric has shown limitations across the board, struggling to provide sufficient insights into the efficacy of newer summarization techniques, which diminishes its utility in evaluating more sophisticated algorithms employed for meeting summary generation [1] [3].

Further complicating matters, ROUGE's reliance on surface-level statistical comparison, focusing primarily on n-gram overlap, does not encompass essential qualitative aspects of summaries such as fluency, coherence, and coverage. This approach renders it less effective in capturing the inherent qualities that contribute to successful summarization [3]. The limitations of ROUGE have led researchers to explore alternative evaluation metrics including LLM-based metrics like GEMBA and ICE, which are better positioned to mirror human judgment as they assess critical aspects of text understanding that ROUGE overlooks [3].

The issue of structural disorganization further comes to the forefront, as ROUGE tends to misalign with human assessments; structural errors may be penalized properly, yet the overall evaluation still fails to adequately represent the context and meaning of the summaries produced. This inconsistency suggests that relying solely on ROUGE could lead to misinterpretations of a model's true performance [2].

Moreover, the evolving landscape of NLP, where newer models and techniques are continuously being developed, underscores the necessity for more nuanced evaluation metrics that can be tailored to the specifics of dialogue and meeting summarization. ROUGE's limitations suggest a crucial gap in the current evaluation landscape, highlighting the need for the field to advance towards metrics that can accurately encapsulate the multifaceted nature of summarization tasks, particularly in complex settings such as meetings [1][4].

In conclusion, while ROUGE has served as a standard metric for summarization evaluation, it exhibits significant limitations in the context of abstractive meeting summaries, especially in terms of alignment with human judgment, handling of hallucination errors, and capturing qualitative measures required for effective evaluation. As the field progresses, there is an urgent need for refining or replacing ROUGE with more sophisticated metrics that reflect the evolving complexities of summarization tasks in NLP.

[1]: https://ar5iv.org/html/2404.11124, [2404.11124] What’s under the hood: Investigating Automatic Metrics on Meeting Summarization
[2]: https://ar5iv.org/html/2404.11124, [2404.11124] What’s under the hood: Investigating Automatic Metrics on Meeting Summarization
[3]: https://ar5iv.org/html/2406.07494, [2406.07494] CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization
[4]: https://ar5iv.org/html/2406.07494, [2406.07494] CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization
[5]: https://ar5iv.org/html/2404.11124, [2404.11124] What’s under the hood: Investigating Automatic Metrics on Meeting Summarization

[1]: Passage ID 1: and discourse structure, existing evaluation metrics often inadequately reflect these Rennard et al. (2023); Kumar and Kabiri (2022).The widespread default use of the ROUGE metric Lin (2004) has shown limitations Liu and Liu (2008); Cohan and Goharian (2016); Fabbri et al. (2021).While the research community acknowledges the potential of newer metrics, such as BERTScore Zhang et al. (2020b), their efficacy in meeting summarization remains uncharted.This is a crucial shortcoming, as the high abstraction level, low extraction rate, and the requirement for complex reasoning power of meeting summarization present new challenges to automatic evaluation metrics, which need to be explored Gao and Wan (2022).Our study aims to investigate how commonly used automatic metrics correlate with human annotations, exploring the inherent qualities these metrics encapsulate in their scores.To unify the fragmented understanding of challenges posed by meeting summarization and the typical errors that
[2]: Passage ID 2: annotations, exploring the inherent qualities these metrics encapsulate in their scores.To unify the fragmented understanding of challenges posed by meeting summarization and the typical errors that manifest when these challenges are unmet.Using the QMSum dataset Zhong et al. (2021), we employ experts to annotate challenges in meeting transcripts and errors in summaries.Various models, including domain-standard encoder-decoder architectures and notable autoregressive models, generated these summaries.Our methodological approach enables us to draw correlations between the challenges and errors, the annotated errors, and a set of eight automatic metrics.The insights derived reveal shortcomings in the current automatic metrics employed for meeting summarization evaluation:Structural disorganization errors are often penalized, reflecting human assessments, whereas hallucination errors tend to be rewarded.While adept at penalizing omissions, the ROUGE Lin (2004) metric struggles to
[3]: Passage ID 3: metrics (e.g., ROUGE for dialogue summarization as identified in Table 1) is limited (?, ?), and while it can serve as a proxy (?) it provides insufficient insights into the true efficacy of new techniques.The also popular model-based metrics (e.g., BARTScore) seem unable to align well with human judgments (?) for dialogue summarization.Recent developments in NLP use LLM-based metrics such as GEMBA (?) and ICE (?) for evaluation (?), building on LLMs’ advanced text comprehension.This set of metrics thereby mimics human judgment and assesses common aspects (e.g., fluency, coverage, coherence) with continuous (?), Likert scale (?, ?), or probability scores (?).LLM-based metrics offer a promising direction for dialogue summarization evaluation due to their customizability, though this area remains largely unexplored.7 Final Considerations7.1 Limitations of EvidenceGiven our selection of papers on dialogue summarization, we found limitations in current works that may bias
[4]: Passage ID 4: we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge taxonomy remains relevant.1 IntroductionFigure 1: Overview of the big six challenges in dialogue summarization, a short description of the challenges, and the estimation of their progress derived from related sub-challenges. Green means mostly mitigated, orange means good progress, and red stands for marked challenges still exist.Abstractive dialogue summarization, a task within Natural Language Processing (NLP) and text summarization, entails condensing key information from conversations into succinct and coherent summaries (?).This sub-field of text summarization is gaining prominenceand is relevant for various real-world scenarios, including customer service (e.g., social media (?) and e-commerce (?)), healthcare (?), daily life (?), meetings (?), and open-domain conversations (e.g.,
[5]: Passage ID 5: Liu and Yang Liu. 2008.Correlation between ROUGE and Human Evaluation of Extractive Meeting Summaries.In Proceedings of ACL-08: HLT, Short Papers, pages 201–204, Columbus, Ohio. Association for Computational Linguistics.Liu et al. (2021)Zhengyuan Liu, Ke Shi, and Nancy F. Chen. 2021.Coreference-Aware Dialogue Summarization.ArXiv:2106.08556 [cs].Ma et al. (2022)Congbo Ma, Wei Emma Zhang, Mingyu Guo, Hu Wang, and Quan Z. Sheng. 2022.Multi-document summarization via deep learning techniques: A survey.ACM Comput. Surv., 55(5).Maddela et al. (2023)Mounica Maddela, Yao Dou, David Heineman, and Wei Xu. 2023.LENS: A Learnable Evaluation Metric for Text Simplification.In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16383–16408, Toronto, Canada. Association for Computational Linguistics.Maynez et al. (2020)Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald.