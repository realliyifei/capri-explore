# "Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking

## Question

How are recent approaches tackling the performance drop in Dialogue State Tracking (DST) models caused by input variations?

## URLs

1. https://ar5iv.org/html/2211.05503. [2211.05503] MoNET: Tackle State Momentum via Noise-Enhanced Training for Dialogue State Tracking
2. https://ar5iv.org/html/2311.06345. [2311.06345] Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking
3. https://ar5iv.org/html/2310.14970. [2310.14970] Towards LLM-driven Dialogue State Tracking
4. https://ar5iv.org/html/2407.21633. [2407.21633] Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation
5. https://ar5iv.org/html/2101.09374. [2101.09374] Slot Self-Attentive Dialogue State Tracking
6. https://ar5iv.org/html/2308.15053. [2308.15053] Adapting Text-based Dialogue State Tracker for Spoken Dialogues
7. https://ar5iv.org/html/2404.08559. [2404.08559] MoPE: Mixture of Prefix Experts for Zero-Shot Dialogue State Tracking
8. https://ar5iv.org/html/2405.13003. [2405.13003] A Survey on Recent Advances in Conversational Data Generation
9. https://ar5iv.org/html/1907.03040. [1907.03040] BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer

## Answer (I don't know?: ✗)

Recent approaches to Dialogue State Tracking (DST) have sought to address performance drops caused by input variations through several innovative strategies. One critical aspect of enhancing DST systems is the incorporation of advanced neural architectures that manage different types of input and dialogue variations effectively.

One notable strategy involves restructuring DST as a reading comprehension task, where the goal is to determine the current state of the dialogue by answering targeted questions related to the dialogue history and context. For example, Gao et al. (2019) reformulated DST in this way, which can help in extracting the necessary information despite variations in user input [5]. This approach allows for greater flexibility and adaptability when confronted with varied utterance styles or words.

Another method is utilizing the capabilities of large language models such as Codex-Davinci-002 and ChatGPT, which have been shown to tackle DST challenges effectively. However, these models tend to be computationally expensive due to their substantial parameter sizes, complicating both the training and inference processes [5]. This highlights a tension between leveraging powerful models and the practical considerations of computational resources.

In addition, Tian et al. (2021a) implemented a two-pass dialogue state generation approach, which aims to correct potential errors carried over from earlier dialogue turns, thus addressing the noise introduced by variations and deviations in user utterances [4]. This method is crucial when long dialogues are involved, as performance tends to degrade with increased dialogue length due to cumulative errors. The two-pass strategy assists in maintaining the integrity of the dialogue state by allowing for adjustments based on historical context.

Moreover, turn-based objective functions have been proposed by Manotumruksa et al. (2021). This approach penalizes the model for incorrect predictions made in earlier dialogue turns, presenting a corrective mechanism for errors that arise due to varied input types [4]. The work suggests that there is a need to efficiently process and incorporate information from longer dialogues, further emphasizing the importance of managing input variations effectively.

Lastly, the implementation of regularization techniques has been noted to improve robustness against input deviations [4]. Overall, the integration of these advanced methodologies aims to enhance the adaptability and accuracy of DST models in the face of diverse and challenging user inputs, contributing to more effective conversational agents.

In conclusion, recent efforts in DST have focused on framing the problem through innovative tasks such as reading comprehension, employing large language models, and applying corrective strategies to manage input variations, ultimately enhancing the systems' performance and robustness in real-world applications.

1. [1]:  https://ar5iv.org/html/2311.15623, No Title
2. [2]:  https://ar5iv.org/html/2308.15053, [2308.15053] Adapting Text-based Dialogue State Tracker for Spoken Dialogues
3. [3]:  https://ar5iv.org/html/2207.14627, No Title
4. [4]:  https://ar5iv.org/html/2207.14627, No Title
5. [5]:  https://ar5iv.org/html/2404.08559, [2404.08559] MoPE: Mixture of Prefix Experts for Zero-Shot Dialogue State Tracking
---
1. [1]:  Passage ID 1: studies, for simplicity, often avoided using external contextual knowledge, instead, relying on transformations of latent representations in PLMs as sources of contextual knowledge. Although these latent representations are context-aware, they remain susceptible to interpretability issues. Some solutions, such as using a specialized neural component to extract contextualized word representations Peters et al. (2018) Liu et al. (2019), require additional fine-tuning.In contrast, our approach employs a computationally efficient algorithm to capture semantic patterns in the entire training corpus and generates features that guide attention to semantically important tokens in the input sequence. This process and the resulting features can be easily analyzed and understood.3 Model3.1 Task: Dialogue State TrackingDialogue State Tracking (DST) involves receiving the dialogue history, the current dialogue turn, and dialogue state history as inputs and yields the updated dialogue
2. [2]:  Passage ID 2: We show that post-processing can mitigate errors in words such as proper nouns. (3) We successfully construct a dialogue system that performs well with spoken utterance input.2 Related Work2.1 Dialogue State TrackingDialogue state tracking (DST) is one of the components of a task-oriented dialogue system that maps partial dialogues to the dialogue state. It usually extracts the user’s goal and intent in the form of a slot-value pair through the user and system dialogue conversation.As an example in Table 1, the DST task is to extract dialogue states such as the value of guesthouse in the slot of hotel-type and the value of bangkok city in the slot of restaurant-name from user’s utterance. There are several methods have recently attracted attention in DST tasks.Dialogue Systems with Description Input Some works have been proposed which include task descriptions as input, where the descriptions related to the dialog system slot or slot value examples are added as input data
3. [3]:  Passage ID 3: to a more robust downstream policy van Niekerk et al. (2021).These studies are rare in their kind and call for more similar work.5 ConclusionDialogue state tracking is a crucial component of a conversational agent to identify the user’s needs at each turn of the conversation.A growing body of work is addressing this task and we have outlined the latest developments.After giving an overview of the task and the different datasets available, we have categorized modern neural approaches according to the inference of the dialogue state.Despite encouraging results on benchmarks such as MultiWOZ, these systems lack flexibility and robustness, which are critical skills for a dialogue system.In recent years, many works have sought to address these limitations and we have summarized the advances.However, there are still significant challenges to be addressed in the future.There are many interesting avenues and we have proposed three key features of DST models to guide future
4. [4]:  Passage ID 4: through regularization techniques Heck et al. (2022).Related to this, an understudied aspect is dealing with utterances that deviate from the norm in the case of a written dialogue system.A DST model must be able to take into account all the history and adjust its predictionsof the dialogue state using all available information.Many works have found that performance degrades rapidly as the dialogue length increases. Another critical aspect is therefore efficiently processing long dialogues Zhang et al. (2021).The dialogue state condenses important information, but correcting an error made in an earlier turn may be difficult. To overcome this error propagation issue, Tian et al. (2021a) use a two-pass dialogue state generation to correct potential errors, whileManotumruksa et al. (2021) propose a turn-based objective function to penalize the model for incorrect prediction in early turns.Despite this need, Jakobovits et al. (2022) have shown that popular DST datasets are not
5. [5]:  Passage ID 5: approach.With the widespread adoption of large language models, Hu et al. (2022) and Heck et al. (2023) have turned to powerful language models like Codex-Davinci-002 and ChatGPT to tackle the DST challenge.However, these models have enormous parameters, making both training and inference processes difficult and costly.Simultaneously, the approaches to solving the DST problem have become increasingly diverse.Gao et al. (2019) reformulated DST as a reading comprehension task by answering the question: “What is the state of the current dialogue?"Shin et al. (2022) framed DST as a dialogue summarization problem.They trained a text-to-text template-based dialogue summary language model and recovered the dialogue state from the summarization using predefined rules.Hu et al. (2022) utilized a code-based large language model, formulating DST as a text-to-SQL problem, where the dialogue state is generated as an SQL query.Parameter Efficient Transfer Learning for DSTPETL for DST