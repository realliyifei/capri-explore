# Which *BERT? A Survey Organizing Contextualized Encoders

## Question

What are the techniques for reducing the size and inference time of BERT-like models, and what are their limitations?

## URLs

1. https://ar5iv.org/html/2411.02530. [2411.02530] A Comprehensive Study on Quantization Techniques for Large Language Models
2. https://ar5iv.org/html/2307.07982. [2307.07982] A Survey of Techniques for Optimizing Transformer Inference
3. https://ar5iv.org/html/2410.17957. [2410.17957] MCUBERT: Memory-Efficient BERT Inference on Commodity Microcontrollers
4. https://ar5iv.org/html/2412.04503. [2412.04503] A Primer on Large Language Models and their Limitations
5. https://ar5iv.org/html/2408.12526. [2408.12526] Exploiting Student Parallelism for Low-latency GPU Inference of BERT-like Models in Online Services
6. https://ar5iv.org/html/2308.00108. [2308.00108] DPBERT: Efficient Inference for BERT based on Dynamic Planning
7. https://ar5iv.org/html/2408.03130. [2408.03130] Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations
8. https://ar5iv.org/html/2104.08335. [2104.08335] Demystifying BERT: Implications for Accelerator Design
9. https://ar5iv.org/html/2402.09748. [2402.09748] Model Compression and Efficient Inference for Large Language Models: A Survey
10. https://ar5iv.org/html/2201.03327. [2201.03327] Latency Adjustable Transformer Encoder for Language Understanding

## Answer (I don't know?: ✗)

To reduce the size and inference time of BERT-like models, several techniques have been developed, each with its own limitations.

1. **Quantization**: This technique involves reducing the number of bits required to represent the model's parameters, which can significantly decrease memory usage and computational load. However, quantization can lead to a drop in model accuracy, particularly if the model is not robust to lower precision representations [3] [5].

2. **Pruning**: Pruning removes less important weights or neurons from the model, effectively reducing its size. Techniques such as SparseGPT have demonstrated that large models can retain their accuracy while making significant reductions in parameter count—up to 100 billion parameters without accuracy loss is noted [4]. However, finding the optimal pruning strategy can be complex and may require extensive retraining.

3. **Knowledge Distillation**: In this method, a smaller model (the student) is trained to mimic the behavior of a larger, pre-trained model (the teacher). While this can yield a compact model that performs comparably to its larger precursor, the distillation process can be time-consuming and may not fully capture the larger model's capabilities, especially in complex tasks [3][5].

4. **Early Exiting**: This technique allows the model to make predictions at various stages in the network, potentially reducing computation for easier tasks. Early exiting can accelerate inference but may also lead to inconsistencies in performance as it relies on intermediate classifiers [2]. The challenge here is appropriately tuning the exit points to balance speed and accuracy.

5. **Architectural Optimizations**: Modifying the transformer architecture itself to make it more efficient can lead to improvements in both size and inference speed. Approaches could include designing lighter self-attention mechanisms. However, these modifications must be carefully implemented; deviations from the original architecture may impact the model's ability to generalize well on various tasks [5].

6. **Transfer Learning Challenges**: It's critical that models maintain interpretability when using these techniques. Limitations in the interpretability of transfer learning architectures can pose significant challenges; understanding how modifications affect model behavior is crucial yet often difficult [1].

Each of these techniques offers mechanisms to address the resource demands of BERT-like models, yet they come with inherent trade-offs related to performance, accuracy, and complexity. As researchers continue to explore these methods, the objective remains to strike a balance between efficiency and the model’s ability to perform across a variety of tasks effectively.

1. [1]:  https://ar5iv.org/html/2405.01586, No Title
2. [2]:  https://ar5iv.org/html/2308.00108, [2308.00108] DPBERT: Efficient Inference for BERT based on Dynamic Planning
3. [3]:  https://ar5iv.org/html/2408.03130, [2408.03130] Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations
4. [4]:  https://ar5iv.org/html/2307.07982, [2307.07982] A Survey of Techniques for Optimizing Transformer Inference
5. [5]:  https://ar5iv.org/html/2307.07982, [2307.07982] A Survey of Techniques for Optimizing Transformer Inference
---
1. [1]:  Passage ID 1: It is difficult to deploy a model of such size into many environments with limited resources, such as a mobile or embedded systems.2.At times BERT tends to overfit when subject to fine tuning.3.Significant training and inference times.Training times does not impact main process.4.The inference times required by BERT vary depending on the model and needed hardware available but in many cases, this significantly limits us on quantity, cost and speed.5.Currently BERT language parameters are extremely large. Not all attention heads are always required.6.There are known model limitations to BERT specifically when it comes to semantics and long term contexts.7.There is a high chance of Catastrophic forgetting.8.The main challenge is interpretability of the transfer learning and transformation architecture - model driven interpretation is still a challenge.9.The combination of interpretability and transferability to downstream tasks is the major
2. [2]:  Passage ID 2: in accuracy, the number of parameters of those PLMs, such as BERT [2], RoBERTa [17] and XLNet [36], reaches millions or even billions, which renders them costly to do inference. This drawbacks make it even more challenging when we perform the inference on mobile devices due to sluggish computation speed. Therefore, it is desirable to minimize the inference time of the PLMs while maintaining an acceptable accuracy.In order to deal with the above-mentioned issues, approaches have been proposed to accelerate the inference, such as early exiting [34], knowledge distillation [20], and quantization [24] pruning [7], among which early exiting is a reference acceleration method designed for models with repetitive architecture. Recently, early exiting was applied to the variants of BERT, which consist of a sequence of transformer layers [27] and a task-specific classifier. According to the degree of difficulty of tasks, early exiting could be performed on one of the intermediate classifiers
3. [3]:  Passage ID 3: complexity present unique challenges and opportunities, prompting researchers and practitioners to explore novel model training, optimization, and deployment methods. This literature review focuses on various techniques for reducing resource requirements and compressing large language models, including quantization, pruning, knowledge distillation, and architectural optimizations. The primary objective is to explore each method in-depth and highlight its unique challenges and practical applications. The discussed methods are categorized into a taxonomy that presents an overview of the optimization landscape and helps navigate it to understand the research trajectory better.Index Terms: Neural Networks, Transformers, Inference Optimization, Quantization, Pruning, Knowledge Distillation, Attention, Attention Optimization, Decoding, Decoding OptimizationI IntroductionIn recent years, Large Language Models (LLMs) have emerged as the cornerstone of Natural Language Processing
4. [4]:  Passage ID 4: of optimizing large-scale transformer models:III-A1 Model size reductionLarge language models are highly demanding in terms of memory and computing resources, making them difficult to deploy in real-time applications.For example, BERT-base and BERT-large models have 110M and 340M parameters, respectively. Similarly, computer vision models have huge model size, e.g., the original ViT-base model consists of 86M trainable parameters [62]. Techniques like SparseGPT [63] can help in removing 100 billion parameters without any accuracy loss. Larger models also provide higher scope for compression. In other words, for a fixed target sparsity, larger models experience a much smaller accuracy drop than their smaller counterparts. For instance, the most extensive models from the OPT and BLOOM families can be pruned to 50% sparsity with minimal increase in perplexity [63].Therefore, model compression techniques can allow storing large models in limited storage capacity.III-A2
5. [5]:  Passage ID 5: in NLP and computer vision domains due to their capability to capture long-range interdependencies. As the complexity of the transformers continues to grow, there is an increasing need for model compression and hardware optimization methods to accelerate these models. To address the enormous challenges faced by the transformers, researchers have developed several model enhancement techniques, including pruning, quantization, neural architecture search, distillation, and lightweight self-attention design. In this paper, we provide taxonomy and a comprehensive overview of the recently proposed inference optimization techniques for transformer-based networks. We discuss the optimization methods, both from transformer architecture and hardware perspectives. We plot the predictive performance against the number of parameters/FLOPs of several optimization techniques to provide meaningful insights. We conclude this paper by emphasizing the future directions in this rapidly evolving field of