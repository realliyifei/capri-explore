# Which *BERT? A Survey Organizing Contextualized Encoders

## Question

What are the techniques for reducing the size and inference time of BERT-like models, and what are their limitations?

## URLs

1. https://ar5iv.org/html/2411.02530. [2411.02530] A Comprehensive Study on Quantization Techniques for Large Language Models
2. https://ar5iv.org/html/2307.07982. [2307.07982] A Survey of Techniques for Optimizing Transformer Inference
3. https://ar5iv.org/html/2410.17957. [2410.17957] MCUBERT: Memory-Efficient BERT Inference on Commodity Microcontrollers
4. https://ar5iv.org/html/2412.04503. [2412.04503] A Primer on Large Language Models and their Limitations
5. https://ar5iv.org/html/2408.12526. [2408.12526] Exploiting Student Parallelism for Low-latency GPU Inference of BERT-like Models in Online Services
6. https://ar5iv.org/html/2308.00108. [2308.00108] DPBERT: Efficient Inference for BERT based on Dynamic Planning
7. https://ar5iv.org/html/2408.03130. [2408.03130] Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations
8. https://ar5iv.org/html/2104.08335. [2104.08335] Demystifying BERT: Implications for Accelerator Design
9. https://ar5iv.org/html/2402.09748. [2402.09748] Model Compression and Efficient Inference for Large Language Models: A Survey
10. https://ar5iv.org/html/2201.03327. [2201.03327] Latency Adjustable Transformer Encoder for Language Understanding

## Answer (I don't know?: âœ—)

To reduce the size and inference time of BERT-like models, several techniques have been developed, each with its own advantages and limitations.

1. **Quantization**: This technique involves reducing the precision of the model parameters, which can significantly lower the model size and enhance inference speed. Quantization typically converts the weights from floating-point to lower-bit formats (e.g., int8) [2]. While this can yield considerable memory savings and accelerate computation, it may lead to a drop in model accuracy, particularly if not implemented carefully [3].

2. **Pruning**: Pruning entails removing less significant weights or neurons from the model, resulting in a smaller parameter footprint. Techniques like SparseGPT have demonstrated the ability to eliminate a substantial number of parameters (e.g., 100 billion) without compromising accuracy [3]. However, aggressive pruning might impact the model's ability to generalize, especially if excessive pruning is carried out without fine-tuning the model post-pruning [2][4].

3. **Knowledge Distillation**: This method involves training a smaller model (the student) to replicate the behavior of a larger, pre-trained model (the teacher). This allows for a less resource-intensive model that can maintain a degree of performance similar to its larger counterpart [1]. However, the student model may still struggle to capture all the nuanced learning of its teacher, leading to potential performance gaps [2].

4. **Early Exiting**: This strategy allows the model to terminate the inference process as soon as it confidently reaches a decision, rather than processing all layers of the network [1]. While it can speed up inference significantly, it is mostly beneficial for tasks with varying difficulty levels and may not fit all types of input uniformly [2].

5. **Neural Architecture Search**: This involves the automated design of model architectures tailored to optimize performance for specific tasks [4]. Although this can lead to highly efficient designs, it is computationally expensive and typically requires significant resources for exploring and evaluating a wide range of possible architectures [2].

6. **Lightweight Attention Mechanisms**: Modifications of the attention mechanisms in transformer models, such as using a reduced number of attention heads or incorporating low-rank approximations, can also help reduce both the size of the model and its inference time [4]. However, these modifications might come with trade-offs in model expressiveness and may limit the model's effectiveness on certain tasks [4][5].

While these techniques provide various avenues to optimize BERT-like models for size and inference speed, they each have limitations that can affect the performance, accuracy, or applicability of the models in real-world scenarios. Balancing size reduction and inference efficiency with model integrity and performance remains a constant challenge in the field of NLP.

1. [1]:  https://ar5iv.org/html/2308.00108, [2308.00108] DPBERT: Efficient Inference for BERT based on Dynamic Planning
2. [2]:  https://ar5iv.org/html/2408.03130, [2408.03130] Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations
3. [3]:  https://ar5iv.org/html/2307.07982, [2307.07982] A Survey of Techniques for Optimizing Transformer Inference
4. [4]:  https://ar5iv.org/html/2307.07982, [2307.07982] A Survey of Techniques for Optimizing Transformer Inference
5. [5]:  https://ar5iv.org/html/2307.07982, [2307.07982] A Survey of Techniques for Optimizing Transformer Inference
---
1. [1]:  Passage ID 1: in accuracy, the number of parameters of those PLMs, such as BERT [2], RoBERTa [17] and XLNet [36], reaches millions or even billions, which renders them costly to do inference. This drawbacks make it even more challenging when we perform the inference on mobile devices due to sluggish computation speed. Therefore, it is desirable to minimize the inference time of the PLMs while maintaining an acceptable accuracy.In order to deal with the above-mentioned issues, approaches have been proposed to accelerate the inference, such as early exiting [34], knowledge distillation [20], and quantization [24] pruning [7], among which early exiting is a reference acceleration method designed for models with repetitive architecture. Recently, early exiting was applied to the variants of BERT, which consist of a sequence of transformer layers [27] and a task-specific classifier. According to the degree of difficulty of tasks, early exiting could be performed on one of the intermediate classifiers
2. [2]:  Passage ID 2: complexity present unique challenges and opportunities, prompting researchers and practitioners to explore novel model training, optimization, and deployment methods. This literature review focuses on various techniques for reducing resource requirements and compressing large language models, including quantization, pruning, knowledge distillation, and architectural optimizations. The primary objective is to explore each method in-depth and highlight its unique challenges and practical applications. The discussed methods are categorized into a taxonomy that presents an overview of the optimization landscape and helps navigate it to understand the research trajectory better.Index Terms: Neural Networks, Transformers, Inference Optimization, Quantization, Pruning, Knowledge Distillation, Attention, Attention Optimization, Decoding, Decoding OptimizationI IntroductionIn recent years, Large Language Models (LLMs) have emerged as the cornerstone of Natural Language Processing
3. [3]:  Passage ID 3: of optimizing large-scale transformer models:III-A1 Model size reductionLarge language models are highly demanding in terms of memory and computing resources, making them difficult to deploy in real-time applications.For example, BERT-base and BERT-large models have 110M and 340M parameters, respectively. Similarly, computer vision models have huge model size, e.g., the original ViT-base model consists of 86M trainable parameters [62]. Techniques like SparseGPT [63] can help in removing 100 billion parameters without any accuracy loss. Larger models also provide higher scope for compression. In other words, for a fixed target sparsity, larger models experience a much smaller accuracy drop than their smaller counterparts. For instance, the most extensive models from the OPT and BLOOM families can be pruned to 50% sparsity with minimal increase in perplexity [63].Therefore, model compression techniques can allow storing large models in limited storage capacity.III-A2
4. [4]:  Passage ID 4: in NLP and computer vision domains due to their capability to capture long-range interdependencies. As the complexity of the transformers continues to grow, there is an increasing need for model compression and hardware optimization methods to accelerate these models. To address the enormous challenges faced by the transformers, researchers have developed several model enhancement techniques, including pruning, quantization, neural architecture search, distillation, and lightweight self-attention design. In this paper, we provide taxonomy and a comprehensive overview of the recently proposed inference optimization techniques for transformer-based networks. We discuss the optimization methods, both from transformer architecture and hardware perspectives. We plot the predictive performance against the number of parameters/FLOPs of several optimization techniques to provide meaningful insights. We conclude this paper by emphasizing the future directions in this rapidly evolving field of
5. [5]:  Passage ID 5: of transformer architectures and their family of architectures, such as BERT, GPT, and ViT. We discuss the challenges, advances and future opportunities in this ever-growing space of transformer research, whose goal is to reduce inference time, minimize memory requirements, and enhance hardware performance. To provide a comprehensive synopsis of key advances, we limit our discussion to inference-related optimizations and, thus, exclude training-related techniques. We also forecast possible future directions in this fast-evolving field of research.The following list summarizes different dimensions of transformer optimization/compression/acceleration methods and provides high-level definitions and the paper organization:1. In Section II, we provide a background on the fundamentals of the transformer model, including embedding, general attention and multi-headed attention (MHA). We also discuss the networks used in NLP and computer vision domains, such as BERT, GPT and vision