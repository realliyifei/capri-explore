# Which *BERT? A Survey Organizing Contextualized Encoders

## Question

What are the techniques for reducing the size and inference time of BERT-like models, and what are their limitations?

## URLs

1. https://ar5iv.org/html/2411.02530. [2411.02530] A Comprehensive Study on Quantization Techniques for Large Language Models
2. https://ar5iv.org/html/2307.07982. [2307.07982] A Survey of Techniques for Optimizing Transformer Inference
3. https://ar5iv.org/html/2410.17957. [2410.17957] MCUBERT: Memory-Efficient BERT Inference on Commodity Microcontrollers
4. https://ar5iv.org/html/2412.04503. [2412.04503] A Primer on Large Language Models and their Limitations
5. https://ar5iv.org/html/2408.12526. [2408.12526] Exploiting Student Parallelism for Low-latency GPU Inference of BERT-like Models in Online Services
6. https://ar5iv.org/html/2308.00108. [2308.00108] DPBERT: Efficient Inference for BERT based on Dynamic Planning
7. https://ar5iv.org/html/2408.03130. [2408.03130] Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations
8. https://ar5iv.org/html/2104.08335. [2104.08335] Demystifying BERT: Implications for Accelerator Design
9. https://ar5iv.org/html/2402.09748. [2402.09748] Model Compression and Efficient Inference for Large Language Models: A Survey
10. https://ar5iv.org/html/2201.03327. [2201.03327] Latency Adjustable Transformer Encoder for Language Understanding

## Answer (I don't know?: âœ—)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable