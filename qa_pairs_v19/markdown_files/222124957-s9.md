# Which *BERT? A Survey Organizing Contextualized Encoders

## Question

What are the techniques for reducing the size and inference time of BERT-like models, and what are their limitations?

## URLs

1. https://ar5iv.org/html/2411.02530
2. https://ar5iv.org/html/2307.07982
3. https://ar5iv.org/html/2410.17957
4. https://ar5iv.org/html/2412.04503
5. https://ar5iv.org/html/2408.12526
6. https://ar5iv.org/html/2308.00108
7. https://ar5iv.org/html/2408.03130
8. https://ar5iv.org/html/2104.08335
9. https://ar5iv.org/html/2402.09748
10. https://ar5iv.org/html/2201.03327

## Answer

## I don't know?: False

To reduce the size and inference time of BERT-like models, several techniques have been proposed that target model efficiency while striving to maintain accuracy. These techniques include quantization, pruning, knowledge distillation, and the application of architectural optimizations.

1. **Quantization**: This technique involves reducing the precision of the weights and activations in a model from higher bit-width representations (like 32-bit floats) to lower bit-width representations (like 8-bit integers). This can significantly decrease the model size and speed up inference without a substantial loss in accuracy [1][2]. However, quantization can introduce quantization errors, which might affect model performance, especially for sensitive tasks.

2. **Pruning**: Pruning entails removing unnecessary weights from a model. For instance, large models can often be pruned significantly while experiencing minimal accuracy loss [3]. Techniques such as SparseGPT can help reduce the number of parameters in models like BERT without loss of accuracy [3]. However, the challenge with pruning is ensuring that important network connections are preserved, which requires careful tuning and validation.

3. **Knowledge Distillation**: This method involves training a smaller "student" model to mimic the outputs of a larger "teacher" model. It has been shown that smaller models can approximate the performance of larger models by learning from their predictions, thus reducing inference time and size [1][2]. The limitation here is that the effectiveness of distillation can vary depending on the architecture and task, and the smaller model may not always capture the intricacies of more complex tasks.

4. **Early Exiting**: This technique allows a model to make predictions at intermediate layers rather than only at the final output layer. For example, in models like BERT, a task-specific classifier can be placed at various intermediate points, enabling the model to exit earlier on easier tasks, thereby saving computational resources [1]. However, early exiting may lead to a compromise in overall accuracy if not properly implemented.

5. **Architectural Optimizations**: These include using lightweight architectures or employing neural architecture search techniques to find efficient model designs [2][5]. While these optimizations can lead to reduced inference times and fewer parameters, they often require extensive computational resources during the design phase and may not generalize well across different tasks.

Despite these advancements, there are significant limitations to consider. For instance, while model compression techniques like pruning and quantization can reduce the size and increase the speed of models, they can also lead to challenges such as decreased robustness and varying levels of accuracy across different model architectures and tasks [5]. Additionally, training smaller models (through distillation) can result in less capacity to learn complex patterns compared to their larger counterparts [2].

In conclusion, while several promising techniques exist to make BERT-like models more efficient, each comes with its own set of challenges and potential drawbacks that must be carefully managed to ensure effective deployment in real-world applications.

[1]: https://ar5iv.org/html/2308.00108, [2308.00108] DPBERT: Efficient Inference for BERT based on Dynamic Planning
[2]: https://ar5iv.org/html/2408.03130, [2408.03130] Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations
[3]: https://ar5iv.org/html/2307.07982, [2307.07982] A Survey of Techniques for Optimizing Transformer Inference
[4]: https://ar5iv.org/html/2305.12544, No Title
[5]: https://ar5iv.org/html/2307.07982, [2307.07982] A Survey of Techniques for Optimizing Transformer Inference

[1]: Passage ID 1: in accuracy, the number of parameters of those PLMs, such as BERT [2], RoBERTa [17] and XLNet [36], reaches millions or even billions, which renders them costly to do inference. This drawbacks make it even more challenging when we perform the inference on mobile devices due to sluggish computation speed. Therefore, it is desirable to minimize the inference time of the PLMs while maintaining an acceptable accuracy.In order to deal with the above-mentioned issues, approaches have been proposed to accelerate the inference, such as early exiting [34], knowledge distillation [20], and quantization [24] pruning [7], among which early exiting is a reference acceleration method designed for models with repetitive architecture. Recently, early exiting was applied to the variants of BERT, which consist of a sequence of transformer layers [27] and a task-specific classifier. According to the degree of difficulty of tasks, early exiting could be performed on one of the intermediate classifiers
[2]: Passage ID 2: complexity present unique challenges and opportunities, prompting researchers and practitioners to explore novel model training, optimization, and deployment methods. This literature review focuses on various techniques for reducing resource requirements and compressing large language models, including quantization, pruning, knowledge distillation, and architectural optimizations. The primary objective is to explore each method in-depth and highlight its unique challenges and practical applications. The discussed methods are categorized into a taxonomy that presents an overview of the optimization landscape and helps navigate it to understand the research trajectory better.Index Terms: Neural Networks, Transformers, Inference Optimization, Quantization, Pruning, Knowledge Distillation, Attention, Attention Optimization, Decoding, Decoding OptimizationI IntroductionIn recent years, Large Language Models (LLMs) have emerged as the cornerstone of Natural Language Processing
[3]: Passage ID 3: of optimizing large-scale transformer models:III-A1 Model size reductionLarge language models are highly demanding in terms of memory and computing resources, making them difficult to deploy in real-time applications.For example, BERT-base and BERT-large models have 110M and 340M parameters, respectively. Similarly, computer vision models have huge model size, e.g., the original ViT-base model consists of 86M trainable parameters [62]. Techniques like SparseGPT [63] can help in removing 100 billion parameters without any accuracy loss. Larger models also provide higher scope for compression. In other words, for a fixed target sparsity, larger models experience a much smaller accuracy drop than their smaller counterparts. For instance, the most extensive models from the OPT and BLOOM families can be pruned to 50% sparsity with minimal increase in perplexity [63].Therefore, model compression techniques can allow storing large models in limited storage capacity.III-A2
[4]: Passage ID 4: (Kurdi et al., 2020)) and tools for education researchers (e.g., systems to build representations of classroom interactions (Alic et al., 2022)). Researchers have been testing the application of models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) in these areas since their release, and are now beginning to incorporate larger models.Gaps.Many of the deployed NLP applications in the education space have been developed prior to wide spread use of LLMs, and we are likely to see large-scale deployment of task-specific models based on LLMs soon. While much of the prior work includes standalone applications, developing models that can easily be incorporated into existing education pipelines, e.g., by integrating what students have learned thus far, is an area open for further exploration. Importantly, a long-standing goal in education is to personalize materials and assessments to the needs of individual students, and NLP has the potential to contribute towards that
[5]: Passage ID 5: in NLP and computer vision domains due to their capability to capture long-range interdependencies. As the complexity of the transformers continues to grow, there is an increasing need for model compression and hardware optimization methods to accelerate these models. To address the enormous challenges faced by the transformers, researchers have developed several model enhancement techniques, including pruning, quantization, neural architecture search, distillation, and lightweight self-attention design. In this paper, we provide taxonomy and a comprehensive overview of the recently proposed inference optimization techniques for transformer-based networks. We discuss the optimization methods, both from transformer architecture and hardware perspectives. We plot the predictive performance against the number of parameters/FLOPs of several optimization techniques to provide meaningful insights. We conclude this paper by emphasizing the future directions in this rapidly evolving field of