# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How do chance-corrected coefficients assess the level of agreement between two coders beyond what would be expected by chance?

## URLs

1. https://ar5iv.org/html/2303.12502
2. https://ar5iv.org/html/2402.08069
3. https://ar5iv.org/html/2408.13855
4. https://ar5iv.org/html/2006.12904
5. https://ar5iv.org/html/2407.11371
6. https://ar5iv.org/html/2410.05291
7. https://ar5iv.org/html/2106.07393
8. https://ar5iv.org/html/2410.03775
9. https://ar5iv.org/html/2107.11449

## Answer

## I don't know?: False

Chance-corrected coefficients, such as the Kappa coefficient, assess the level of agreement between two coders by quantifying the agreement that exceeds what would be expected by random chance. This is particularly relevant in tasks like annotation in natural language processing (NLP), where different coders may label the same data based on varying interpretations. 

The Kappa coefficient is a widely accepted metric used to measure inter-rater agreement while accounting for chance. It is defined mathematically as \(Kappa = \frac{(A_o - A_e)}{(1 - A_e)}\), where \(A_o\) represents the observed agreement between the coders, and \(A_e\) is the agreement expected by chance [1]. This formula enables a clear distinction between the actual agreement observed and the agreement that could have occurred purely by coincidence, thus providing a more nuanced view of coder alignment.

In NLP, especially in tasks involving span detection or labeling, a critical limitation has been the lack of chance-corrected metrics tailored for these specific applications. As noted in the literature, many tasks have historically relied on non-chance-corrected metrics, which can distort comparisons due to varying levels of chance agreement [4]. By proposing metrics that incorporate chance correction for span detection tasks, researchers aim to mitigate biases that could arise from using simpler measures that don‚Äôt account for this factor [4].

Moreover, the reliability of agreement measures like Kappa can be subject to manipulation through repeated training and coding, leading to artificially inflated scores that do not reflect true ongoing inter-rater agreement [2][3]. This points out a significant challenge in interpreting these coefficients without adequate context and control measures, such as rigorous evaluations or "mismatch analyses," whereby independent coders review the accuracy of code assignments [3].

Overall, chance-corrected coefficients like Kappa enhance the understanding of coder agreement by providing a clearer, more statistically grounded picture of how much agreement is genuine versus coincidence. Consequently, they offer a crucial tool for evaluating and refining annotation processes in NLP, leading to more robust and credible outcomes in research and application [4][5]. 

In summary, chance-corrected coefficients assess coder agreement beyond chance by adjusting for random expectations and providing insights that help ensure reliability and validity in NLP tasks. This approach helps researchers avoid pitfalls related to superficial measures of agreement and encourages deeper evaluations of the processes involved in text annotation and coding practices.

[1]: https://ar5iv.org/html/2407.11371, [2407.11371] Estimating Agreement by Chance for Sequence Annotation
[2]: https://ar5iv.org/html/2407.21037, No Title
[3]: https://ar5iv.org/html/2407.21037, No Title
[4]: https://ar5iv.org/html/2407.11371, [2407.11371] Estimating Agreement by Chance for Sequence Annotation
[5]: https://ar5iv.org/html/2407.11371, [2407.11371] Estimating Agreement by Chance for Sequence Annotation

[1]: Passage ID 1: literature. We then explain our methodology, and evaluate it first through a simulation study, and then through application to real-world corpora. Finally, we conclude with discussions of limitations, ethical considerations, and future research.2 Theoretical Foundation and MotivationEstimation of chance agreement is a key element in the evaluation of classification tasks. However, though the field of NLP features a wide variety of span detection and labeling tasks, there is a lack of widely adopted chance-corrected metrics for them.In classification tasks, the Kappa coefficient is one of the most popular chance-corrected inter-annotator agreement measures (Komagata, 2002; Artstein and Poesio, 2008; Eugenio and Glass, 2004; Hripcsak and Rothschild, 2005; Powers, 2015).The Kappa coefficient is defined as (Ao‚àíAe)/(1‚àíAe)subscriptùê¥ùëúsubscriptùê¥ùëí1subscriptùê¥ùëí(A_{o}-A_{e})/(1-A_{e}), where Aosubscriptùê¥ùëúA_{o} is the observed agreement without chance agreement correction, and
[2]: Passage ID 2: each sentence was formulated to be a perfect match with that code, so we could reasonably say that a higher level of model match with that assigned code meant it was ‚Äúaccurate‚Äù. With the transcript training approach, however, there was much more room for human coders to be wrong, in part due to human error and fatigue and drift, but also because they were coding real, more ambiguous, sentences. Human coders might themselves have differences of opinion about which code applied. So, rather than call it ‚Äúaccurate‚Äù when the model code matched the human code, we simply refer to it as a ‚Äúmatch‚Äù.What level of match is an acceptable is an unresolved issue. We provide more detailed statistics on interrater agreement for our first model in Friedman et al (2024), but there are no definitive break points for those statistics, and it is relatively easy to achieve momentarily high human-human kappas with repeated training and coding without necessarily achieving ongoing inter-rater alignment.
[3]: Passage ID 3: points for those statistics, and it is relatively easy to achieve momentarily high human-human kappas with repeated training and coding without necessarily achieving ongoing inter-rater alignment. Also, as just mentioned, close evaluation of past coding by humans reveals many cases where reasonable coders could disagree. We discuss below an approach we call ‚Äúmismatch analysis‚Äù where we ask independent coders to evaluate whether the original human coders or the LLM coder assigned the right code. More broadly, we go through an extensive process of testing the data in ways that can help build confidence in our process and the results.Coding Consistency MeasureAs we tested different coding strategies, we realized that the model might not always code a given sentence the same way every time, especially when using in-context learning, which has the model learn afresh each time it is applied to a transcript. To test how consistently the model coded a transcript‚Äôs sentences, we decided
[4]: Passage ID 4: most widely accepted metrics for evaluation of annotation reliability is Cohen‚Äôs Kappa, which offers an assessment of inter-rater reliability that is adjusted in order to avoid offering credit for the portion of observed agreement that can be attributed to chance. Some NLP tasks, such as Named Entity Recognition or other span detection/labeling tasks, lack an appropriate chance corrected metric. This paper addresses this gap by proposing such a measure for these tasks, demonstrating its application in both simulation and CoNLL03 corpus experiments.Numerous studies caution against using non-chance-corrected agreement metrics. They can lead to unfair task or system comparisons due to biases introduced due to varying levels of chance agreement across tasks and systems (Ide and Pustejovsky, 2017; Komagata, 2002; Gates and Ahn, 2017; Rand, 1971; Lavelli et¬†al., 2008; Artstein and Poesio, 2008). Furthermore, without correction for chance agreement, measurements tend to cluster within a
[5]: Passage ID 5: with expected agreement (chanceF1) because the more the agreement, the less the disagreement. However, the actual results are the opposite, primarily because Alpha coefficients include agreement for irrelevant segments, which does not align with the needs of most information extraction tasks.The main purpose of chance correction is to use different baselines for different tasks. In addition, chance correction may also change the ranking of model performance for the same task, although this is not common. As shown in the table 8, the gold standard annotation labels six entities with size of 3, 3, 3, 3, 3, 16. The annotator1 labels five 3-token entities correctly but misses the 16-token entity. The annotator2 labels the 16-token entities correctly but misses five 3-token entities. Note that the observed F1 score of annotator1 is lower than that of annotator2. But after the chance correction, the results are opposite (see table 9). Neither of the two Alpha coefficients demonstrated