# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How do chance-corrected coefficients assess the level of agreement between two coders beyond what would be expected by chance?

## URLs

1. https://ar5iv.org/html/2303.12502. [2303.12502] Measuring Agreement Among Several Raters Classifying Subjects Into One-Or-More (Hierarchical) Nominal Categories. A Generalisation of Fleiss‚Äô kappa
2. https://ar5iv.org/html/2402.08069. [2402.08069] Interrater agreement statistics under the two-rater dichotomous-response case with correlated decisions
3. https://ar5iv.org/html/2408.13855. [2408.13855] An Empirical Study of False Negatives and Positives of Static Code Analyzers From the Perspective of Historical Issues
4. https://ar5iv.org/html/2006.12904. [2006.12904] Min-Mid-Max Scaling, Limits of Agreement, and Agreement Score
5. https://ar5iv.org/html/2407.11371. [2407.11371] Estimating Agreement by Chance for Sequence Annotation
6. https://ar5iv.org/html/2410.05291. [2410.05291] Liberal-Conservative Hierarchies of Intercoder Reliability Estimators
7. https://ar5iv.org/html/2106.07393. [2106.07393] Cross-replication Reliability - An Empirical Approach to Interpreting Inter-rater Reliability ACL 2021 Main Conference
8. https://ar5iv.org/html/2410.03775. [2410.03775] Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge
9. https://ar5iv.org/html/2107.11449. [2107.11449] Applying Inter-rater Reliability and Agreement in Grounded Theory Studies in Software Engineering

## Answer (I don't know?: ‚úó)

Chance-corrected coefficients, such as Cohen's Kappa, assess the level of agreement between two coders by quantifying the degree of agreement while accounting for the agreement that could occur purely by chance. The Kappa coefficient is defined mathematically as \((A_o - A_e) / (1 - A_e)\), where \(A_o\) represents the observed agreement and \(A_e\) denotes the expected agreement due to chance. This calculation allows for a more accurate assessment of inter-annotator reliability by excluding the portion of agreement attributable to random assignments [1].

In natural language processing (NLP), many tasks, such as Named Entity Recognition and span detection, often lack appropriate chance-corrected metrics. This gap in methodology can result in misleading evaluations of annotation quality and performance of systems. It has been noted that using non-chance-corrected measures could lead to biased comparisons across different tasks or systems, as varying rates of chance agreement can skew results [2]. Thus, chance-corrected coefficients are crucial in ensuring fair evaluations, as they adjust scores based on the likelihood of agreement occurring randomly, thus providing a clearer picture of actual annotator reliability [2].

Furthermore, the application of chance-correction may influence the ranking of model performance. For instance, while an observed F1 score might favor one annotator, correction for chance could reverse this assessment, illustrating a different level of reliability that accounts for agreement on specific segments of the data, which may not be relevant to the task at hand [3]. This variability indicates why metrics like the Kappa coefficient are favored; they provide a basis for evaluating performance that holds up against varying baselines across different tasks.

Moreover, empirical studies illustrate the practical use of Cohen's Kappa in evaluating inter-coder reliability. For example, one study reported a Kappa coefficient of 0.84 when measuring agreement between coders, indicating a high level of reliability in their annotations, which reinforces the effectiveness of chance-corrected measures [4]. This methodological rigor is necessary, particularly in complex fields like NLP, where nuanced distinctions in data labeling can significantly impact outcomes.

Through chance correction, Kappa and similar coefficients can facilitate a more equitable framework for comparison among diverse NLP tasks, ultimately leading to stronger conclusions about the quality and consistency of coding practices, and fostering advancements in the reliability of annotations in the field [5].

1. [1]:  https://ar5iv.org/html/2407.11371, [2407.11371] Estimating Agreement by Chance for Sequence Annotation
2. [2]:  https://ar5iv.org/html/2407.11371, [2407.11371] Estimating Agreement by Chance for Sequence Annotation
3. [3]:  https://ar5iv.org/html/2407.11371, [2407.11371] Estimating Agreement by Chance for Sequence Annotation
4. [4]:  https://ar5iv.org/html/2107.11449, [2107.11449] Applying Inter-rater Reliability and Agreement in Grounded Theory Studies in Software Engineering
5. [5]:  https://ar5iv.org/html/2407.11371, [2407.11371] Estimating Agreement by Chance for Sequence Annotation
---
1. [1]:  Passage ID 1: literature. We then explain our methodology, and evaluate it first through a simulation study, and then through application to real-world corpora. Finally, we conclude with discussions of limitations, ethical considerations, and future research.2 Theoretical Foundation and MotivationEstimation of chance agreement is a key element in the evaluation of classification tasks. However, though the field of NLP features a wide variety of span detection and labeling tasks, there is a lack of widely adopted chance-corrected metrics for them.In classification tasks, the Kappa coefficient is one of the most popular chance-corrected inter-annotator agreement measures (Komagata, 2002; Artstein and Poesio, 2008; Eugenio and Glass, 2004; Hripcsak and Rothschild, 2005; Powers, 2015).The Kappa coefficient is defined as (Ao‚àíAe)/(1‚àíAe)subscriptùê¥ùëúsubscriptùê¥ùëí1subscriptùê¥ùëí(A_{o}-A_{e})/(1-A_{e}), where Aosubscriptùê¥ùëúA_{o} is the observed agreement without chance agreement correction, and
2. [2]:  Passage ID 2: most widely accepted metrics for evaluation of annotation reliability is Cohen‚Äôs Kappa, which offers an assessment of inter-rater reliability that is adjusted in order to avoid offering credit for the portion of observed agreement that can be attributed to chance. Some NLP tasks, such as Named Entity Recognition or other span detection/labeling tasks, lack an appropriate chance corrected metric. This paper addresses this gap by proposing such a measure for these tasks, demonstrating its application in both simulation and CoNLL03 corpus experiments.Numerous studies caution against using non-chance-corrected agreement metrics. They can lead to unfair task or system comparisons due to biases introduced due to varying levels of chance agreement across tasks and systems (Ide and Pustejovsky, 2017; Komagata, 2002; Gates and Ahn, 2017; Rand, 1971; Lavelli et¬†al., 2008; Artstein and Poesio, 2008). Furthermore, without correction for chance agreement, measurements tend to cluster within a
3. [3]:  Passage ID 3: with expected agreement (chanceF1) because the more the agreement, the less the disagreement. However, the actual results are the opposite, primarily because Alpha coefficients include agreement for irrelevant segments, which does not align with the needs of most information extraction tasks.The main purpose of chance correction is to use different baselines for different tasks. In addition, chance correction may also change the ranking of model performance for the same task, although this is not common. As shown in the table 8, the gold standard annotation labels six entities with size of 3, 3, 3, 3, 3, 16. The annotator1 labels five 3-token entities correctly but misses the 16-token entity. The annotator2 labels the 16-token entities correctly but misses five 3-token entities. Note that the observed F1 score of annotator1 is lower than that of annotator2. But after the chance correction, the results are opposite (see table 9). Neither of the two Alpha coefficients demonstrated
4. [4]:  Passage ID 4: pass, and 25 of the 230 artifacts and 6 of the 60 videos, at two stages in the development of the code books. We calculated the inter-coder reliability ratio as the number of agreements divided by the total number of codes [‚Ä¶]‚ÄùID128 To test the inter-coder reliability, the primary researcher coded all 154 records, and subsequently the second coder coded every fifth record in the dataset. Cohen‚Äôs Kappa coefficient was found to be 0.84, indicating high agreement between the two codersID55, ID59, ID65, and ID89 are the only ones of the few to lightly describe the expertise of raters involved in the coding process, but none of them mention a specific training on the coding process.ID55 ‚ÄúTo avoid the researcher‚Äôs bias, we have performed an inter-rater reliability test between mapping team and indented experts [‚Ä¶]‚ÄùID59 ‚ÄúThe initial team of paper taggers was made up of seven post-docs and graduate students with some association to the University of Trento and some
5. [5]:  Passage ID 5: "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Estimating Agreement by Chance for Sequence AnnotationDiya Li Freenome Holdings, Inc916lidiya@gmail.com&Carolyn Ros√© Carnegie Mellon University cprose@cs.cmu.edu\ANDAo Yuan Georgetown University ay312@georgetown.edu&Chunxiao Zhou National Institutes of Health chunxiao.zhou@nih.govAbstractIn the field of natural language processing, correction of performance assessment for chance agreement plays a crucial role in evaluating the reliability of annotations. However, there is a notable dearth of research focusing on chance correction for assessing the reliability of sequence annotation tasks, despite their widespread prevalence in the field. To address this gap, this paper introduces a novel model for generating random annotations, which serves as the foundation for estimating chance agreement in sequence annotation tasks. Utilizing the proposed randomization model and