# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How do chance-corrected coefficients assess the level of agreement between two coders beyond what would be expected by chance?

## URLs

1. https://ar5iv.org/html/2303.12502. [2303.12502] Measuring Agreement Among Several Raters Classifying Subjects Into One-Or-More (Hierarchical) Nominal Categories. A Generalisation of Fleiss’ kappa
2. https://ar5iv.org/html/2402.08069. [2402.08069] Interrater agreement statistics under the two-rater dichotomous-response case with correlated decisions
3. https://ar5iv.org/html/2408.13855. [2408.13855] An Empirical Study of False Negatives and Positives of Static Code Analyzers From the Perspective of Historical Issues
4. https://ar5iv.org/html/2006.12904. [2006.12904] Min-Mid-Max Scaling, Limits of Agreement, and Agreement Score
5. https://ar5iv.org/html/2407.11371. [2407.11371] Estimating Agreement by Chance for Sequence Annotation
6. https://ar5iv.org/html/2410.05291. [2410.05291] Liberal-Conservative Hierarchies of Intercoder Reliability Estimators
7. https://ar5iv.org/html/2106.07393. [2106.07393] Cross-replication Reliability - An Empirical Approach to Interpreting Inter-rater Reliability ACL 2021 Main Conference
8. https://ar5iv.org/html/2410.03775. [2410.03775] Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge
9. https://ar5iv.org/html/2107.11449. [2107.11449] Applying Inter-rater Reliability and Agreement in Grounded Theory Studies in Software Engineering

## Answer (I don't know?: ✗)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable