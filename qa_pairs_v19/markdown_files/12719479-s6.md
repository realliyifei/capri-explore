# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How do chance-corrected coefficients assess the level of agreement between two coders beyond what would be expected by chance?

## URLs

1. https://ar5iv.org/html/2303.12502. [2303.12502] Measuring Agreement Among Several Raters Classifying Subjects Into One-Or-More (Hierarchical) Nominal Categories. A Generalisation of Fleiss‚Äô kappa
2. https://ar5iv.org/html/2402.08069. [2402.08069] Interrater agreement statistics under the two-rater dichotomous-response case with correlated decisions
3. https://ar5iv.org/html/2408.13855. [2408.13855] An Empirical Study of False Negatives and Positives of Static Code Analyzers From the Perspective of Historical Issues
4. https://ar5iv.org/html/2006.12904. [2006.12904] Min-Mid-Max Scaling, Limits of Agreement, and Agreement Score
5. https://ar5iv.org/html/2407.11371. [2407.11371] Estimating Agreement by Chance for Sequence Annotation
6. https://ar5iv.org/html/2410.05291. [2410.05291] Liberal-Conservative Hierarchies of Intercoder Reliability Estimators
7. https://ar5iv.org/html/2106.07393. [2106.07393] Cross-replication Reliability - An Empirical Approach to Interpreting Inter-rater Reliability ACL 2021 Main Conference
8. https://ar5iv.org/html/2410.03775. [2410.03775] Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge
9. https://ar5iv.org/html/2107.11449. [2107.11449] Applying Inter-rater Reliability and Agreement in Grounded Theory Studies in Software Engineering

## Answer (I don't know?: ‚úó)

Chance-corrected coefficients, such as Cohen's Kappa and the Kappa coefficient mentioned in the provided context, serve to quantify the level of agreement between two coders by adjusting the observed agreement for the agreement that could occur by chance alone. This adjustment is crucial in tasks such as annotation in Natural Language Processing (NLP) where random agreement could mislead evaluations.

The Kappa coefficient is calculated using the formula (Ao - Ae) / (1 - Ae), where Ao is the observed agreement and Ae is the expected agreement by chance [1]. By measuring how much the observed agreement exceeds what would be expected by chance, Kappa provides a more accurate reflection of true consensus between coders. 

For instance, a scenario may occur where coders agree on a substantial number of classifications, but a significant portion of that agreement might be due to chance‚Äîsuch as when both coders happen to guess the same label for unrelated cases. Cohen's Kappa accounts for this by factoring in the probability of chance agreement, allowing for a more nuanced understanding of inter-rater reliability [5].

The context also highlights that certain NLP tasks, such as Named Entity Recognition and other labeling tasks, often lack established chance-corrected metrics [2]. This gap is important because using non-chance-corrected metrics could lead to inflated agreement scores, presenting a false sense of reliability and validity in coding tasks. Studies caution against these non-corrected metrics as they can distort comparisons across different tasks due to inherent biases related to varying levels of chance agreement across different systems and tasks [5].

The importance of statistical methods like Fleiss‚Äô Kappa and confidence interval analysis also reflects a methodological advancements in assessing consensus rates and enhancing the reliability and precision of outcomes in coding tasks [4]. These methods ensure that the observed levels of agreement are not solely a product of random chance but rather indicative of the actual agreement between coders, thereby contributing to ongoing trust in the results produced by collaborative coding models.

In summary, chance-corrected coefficients enhance the assessment of agreement levels by explicitly isolating the fraction of agreement that occurs beyond what is expected due to randomness, thus providing a more reliable measure of inter-rater reliability in NLP tasks.

1. [1]:  https://ar5iv.org/html/2407.11371, [2407.11371] Estimating Agreement by Chance for Sequence Annotation
2. [2]:  https://ar5iv.org/html/2407.21037, No Title
3. [3]:  https://ar5iv.org/html/2407.21037, No Title
4. [4]:  https://ar5iv.org/html/2411.16797, No Title
5. [5]:  https://ar5iv.org/html/2407.11371, [2407.11371] Estimating Agreement by Chance for Sequence Annotation
---
1. [1]:  Passage ID 1: literature. We then explain our methodology, and evaluate it first through a simulation study, and then through application to real-world corpora. Finally, we conclude with discussions of limitations, ethical considerations, and future research.2 Theoretical Foundation and MotivationEstimation of chance agreement is a key element in the evaluation of classification tasks. However, though the field of NLP features a wide variety of span detection and labeling tasks, there is a lack of widely adopted chance-corrected metrics for them.In classification tasks, the Kappa coefficient is one of the most popular chance-corrected inter-annotator agreement measures (Komagata, 2002; Artstein and Poesio, 2008; Eugenio and Glass, 2004; Hripcsak and Rothschild, 2005; Powers, 2015).The Kappa coefficient is defined as (Ao‚àíAe)/(1‚àíAe)subscriptùê¥ùëúsubscriptùê¥ùëí1subscriptùê¥ùëí(A_{o}-A_{e})/(1-A_{e}), where Aosubscriptùê¥ùëúA_{o} is the observed agreement without chance agreement correction, and
2. [2]:  Passage ID 2: each sentence was formulated to be a perfect match with that code, so we could reasonably say that a higher level of model match with that assigned code meant it was ‚Äúaccurate‚Äù. With the transcript training approach, however, there was much more room for human coders to be wrong, in part due to human error and fatigue and drift, but also because they were coding real, more ambiguous, sentences. Human coders might themselves have differences of opinion about which code applied. So, rather than call it ‚Äúaccurate‚Äù when the model code matched the human code, we simply refer to it as a ‚Äúmatch‚Äù.What level of match is an acceptable is an unresolved issue. We provide more detailed statistics on interrater agreement for our first model in Friedman et al (2024), but there are no definitive break points for those statistics, and it is relatively easy to achieve momentarily high human-human kappas with repeated training and coding without necessarily achieving ongoing inter-rater alignment.
3. [3]:  Passage ID 3: points for those statistics, and it is relatively easy to achieve momentarily high human-human kappas with repeated training and coding without necessarily achieving ongoing inter-rater alignment. Also, as just mentioned, close evaluation of past coding by humans reveals many cases where reasonable coders could disagree. We discuss below an approach we call ‚Äúmismatch analysis‚Äù where we ask independent coders to evaluate whether the original human coders or the LLM coder assigned the right code. More broadly, we go through an extensive process of testing the data in ways that can help build confidence in our process and the results.Coding Consistency MeasureAs we tested different coding strategies, we realized that the model might not always code a given sentence the same way every time, especially when using in-context learning, which has the model learn afresh each time it is applied to a transcript. To test how consistently the model coded a transcript‚Äôs sentences, we decided
4. [4]:  Passage ID 4: enhances the reliability and precision of responses. By employing statistical methods such as chi-square tests, Fleiss‚Äô Kappa, and confidence interval analysis, we evaluate consensus rates and inter-rater agreement to quantify the reliability of collaborative outputs. Key results reveal that Claude and GPT-4 exhibit the highest reliability and consistency, as evidenced by their narrower confidence intervals and higher alignment with question-generating models. Conversely, Gemini and LLaMA show more significant variability in their consensus rates, as reflected in wider confidence intervals and lower reliability percentages. These findings demonstrate that collaborative interactions among large language models (LLMs) significantly improve response reliability, offering novel insights into autonomous, cooperative reasoning and validation in AI systems.Keywords: Large Language Models, Collaborative Intelligence, Answer Validation, Game Theory, Statistical Analysis1
5. [5]:  Passage ID 5: most widely accepted metrics for evaluation of annotation reliability is Cohen‚Äôs Kappa, which offers an assessment of inter-rater reliability that is adjusted in order to avoid offering credit for the portion of observed agreement that can be attributed to chance. Some NLP tasks, such as Named Entity Recognition or other span detection/labeling tasks, lack an appropriate chance corrected metric. This paper addresses this gap by proposing such a measure for these tasks, demonstrating its application in both simulation and CoNLL03 corpus experiments.Numerous studies caution against using non-chance-corrected agreement metrics. They can lead to unfair task or system comparisons due to biases introduced due to varying levels of chance agreement across tasks and systems (Ide and Pustejovsky, 2017; Komagata, 2002; Gates and Ahn, 2017; Rand, 1971; Lavelli et¬†al., 2008; Artstein and Poesio, 2008). Furthermore, without correction for chance agreement, measurements tend to cluster within a