# Towards Reasoning in Large Language Models: A Survey

## Question

What are the methods and challenges associated with enhancing reasoning capabilities in small language models through fully supervised finetuning?

## URLs

1. https://ar5iv.org/html/2404.10346
2. https://ar5iv.org/html/2408.12337
3. https://ar5iv.org/html/2310.18338
4. https://ar5iv.org/html/2411.18915
5. https://ar5iv.org/html/2410.18890
6. https://ar5iv.org/html/2408.06195
7. https://ar5iv.org/html/2411.15382
8. https://ar5iv.org/html/2404.17140
9. https://ar5iv.org/html/2410.18693
10. https://ar5iv.org/html/2410.09699

## Answer

Enhancing reasoning capabilities in small language models (LLMs) through fully supervised fine-tuning involves several methods and faces distinct challenges, especially within complex domains like finance.

### Methods

1. **Teacher-Student Setup**: One of the prevalent methods is the teacher-student paradigm. In this approach, a larger pre-trained LLM (the teacher) generates training data, which is then used to fine-tune a smaller model (the student) [1][4]. This methodology aims to instill in the smaller model capabilities that mirror the performance of the larger model by providing it with targeted reasoning exemplars.

2. **Fine-Tuning with Numerical Reasoning**: The fine-tuning process specifically refines the student's ability to apply complex financial reasoning and perform numerical analyses across financial texts. Models are trained to generate programs that reflect the necessary calculations and expressions of financial concepts, honed through a fine-tuning dataset that focuses on relevant tasks [1][5].

3. **Utilization of Smaller Models**: The research underscores the effectiveness of fine-tuning smaller models like phi-3 variants, Mistral, and Orca-2 for financial question answering. For instance, a suite of models, including those with parameters ranging from 7B to 14B, is specifically evaluated to determine how fine-tuning influences their reasoning capabilities [3][4].

4. **Inducing Reasoning in Smaller Models**: The research adapts methods from previous approaches to induce reasoning in smaller models specifically for financial applications. By leveraging the strengths of large models, the fine-tuning process is expected to enhance the student models' capabilities significantly [2][3].

### Challenges

1. **Complexity of Financial Questions**: The finance domain presents unique challenges that require deep understanding and application of financial concepts along with numerical reasoning. This complexity makes it distinct from traditional question-answering tasks, creating hurdles for smaller models that may not inherently possess such intricate capabilities [2].

2. **Data Requirements for Fine-Tuning**: Fine-tuning small models requires comprehensive datasets that are well-crafted to reflect the financial reasoning needed. Determining the right data requirements and ensuring the quality and relevance of the data for effective learning is a significant challenge in the process [3].

3. **Performance Gaps**: While fine-tuning has proven effective, it can still be difficult for smaller models to completely match the reasoning abilities of more extensive models. The research indicates that achieving performance levels comparable to larger models can be challenging due to inherent differences in scale and capacity [1][4].

4. **Pre-trained Knowledge Utilization**: Understanding which aspects of pre-trained knowledge contribute to enhancements in question answering, and what improvements emerge specifically from fine-tuning, remains a critical area of exploration. Determining these relationships can be complex, necessitating extensive empirical testing [2][3].

In summary, the methodologies for enhancing reasoning capabilities in small models hinge on the teacher-student framework, targeted fine-tuning strategies, and leveraging specifically designed datasets. However, challenges such as domain complexity, data requirements, performance consistency, and knowledge utilization persist and warrant ongoing investigation for effective model development in the financial domain.

[1]: https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
[2]: https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
[3]: https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
[4]: https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
[5]: https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents

[1]: Passage ID 1: multi-hop numerical reasoning over financial texts. We assess the performance of several smaller models that have been fine-tuned to generate programs that encode the required financial reasoning and calculations. Our findings demonstrate that these fine-tuned smaller models approach the performance of the teacher model.To provide a granular analysis of model performance, we propose an approach to investigate the specific student model capabilities that are enhanced by fine-tuning. Our empirical analysis indicates that fine-tuning refines the student models ability to express and apply the required financial concepts along with adapting the entity extraction for the specific data format. In addition, we hypothesize and demonstrate that comparable financial reasoning capability can be induced using relatively smaller datasets.1 IntroductionIn recent years, the development of large language models (LLMs) has achieved significant advances in natural language processing (NLP).
[2]: Passage ID 2: the finance domain poses a unique set of challenges, requiring the understanding of financial concepts along with the ability to perform numerical reasoning. This complexity introduces a significant challenge that is distinct from classical question answering problems Yang et al. (2018); Rajpurkar et al. (2018)In this paper, we present an empirical study that provides experimental evidence supporting the effectiveness of fine-tuning small language models for financial question answering. Our research is guided by several critical questions:RQ1: To what degree does fine-tuning small language models improve their performance on financial question answering tasks?RQ2: What are the intrinsic characteristics of the base language model that contribute to its performance prior to fine-tuning?RQ3: Which aspects of question answering benefit directly from the pre-trained knowledge, and what specific improvements are enabled by fine-tuning?RQ4: What are the fine-tuning data
[3]: Passage ID 3: fine-tuning?RQ3: Which aspects of question answering benefit directly from the pre-trained knowledge, and what specific improvements are enabled by fine-tuning?RQ4: What are the fine-tuning data requirements to achieve these improvements?To address these questions, we adapt previous approaches on inducing reasoning in smaller models for the financial question answering task. In our experimental setup, we employ GPT-4 as the teacher model, building upon its documented success in the realm of financial question answering Chen et al. (2023); Phogat et al. (2023). For the student models, we explore a suite of state-of-the-art, yet relatively smaller, language models including phi-3 variants (3.5B and 14B parameters) Abdin et al. (2024), Mistral 7B, and Orca-2 configurations (7B and 13B parameters) Mitra et al. (2023). Our methodology involves training the student model using Python programs generated by the teacher model. The teacher generated code systematically delineates the
[4]: Passage ID 4: capabilities in small language models. One of the common approaches is following a teacher-student setup where a pre-trained LLM acts as a teacher, generating training data which is used to teach a small language model, the student. Mukherjee et al. (2023); Mitra et al. (2023) aim to train models to exhibit generic reasoning abilities. They utilize GPT-3.5 Turbo and GPT-4 as teacher models to generate training data with carefully crafted prompts. On the other hand, Fu et al. (2023); Magister et al. (2023); Ho et al. (2023) train task specific small language models with CoT based explanation from pre-trained LLMs. Specifically for problems involving mathematical reasoning, Wang et al. (2023a); Gou et al. (2023); Toshniwal et al. (2024); Wang et al. (2023c) propose to generate programs from the pre-trained LLMs and train the small language models. In contrast, we focus on fine-tuning small language models for financial question answering problems Chen et al. (2021b, 2022); Zhu et al.
[5]: Passage ID 5: "dark"); } else { localStorage.setItem("ar5iv_theme", "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Fine-tuning Smaller Language Models for Question Answering over Financial DocumentsKarmvir Singh Phogat,Sai Akhil Puranam,Sridhar Dasaratha,Chetan Harsha,Shashishekar RamakrishnaEY Global Delivery Services India LLP{Karmvir.Phogat,Sai.Puranam,Sridhar.Dasaratha}@gds.ey.com,{Chetan.Harsha,Shashishekar.R}@gds.ey.comAbstractRecent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain, focusing on the challenge of answering questions that require multi-hop numerical reasoning over financial texts. We assess the performance of several smaller models that have been fine-tuned to generate programs that encode the required financial reasoning and