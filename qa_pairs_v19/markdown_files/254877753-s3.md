# Towards Reasoning in Large Language Models: A Survey

## Question

What are the methods and challenges associated with enhancing reasoning capabilities in small language models through fully supervised finetuning?

## URLs

1. https://ar5iv.org/html/2404.10346. [2404.10346] Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards
2. https://ar5iv.org/html/2408.12337. [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
3. https://ar5iv.org/html/2310.18338. [2310.18338] Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning
4. https://ar5iv.org/html/2411.18915. [2411.18915] MATATA: A weakly-supervised MAthematical Tool-Assisted reasoning for Tabular Applications
5. https://ar5iv.org/html/2410.18890. [2410.18890] Improving Small-Scale Large Language Models Function Calling for Reasoning Tasks
6. https://ar5iv.org/html/2408.06195. [2408.06195] Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers
7. https://ar5iv.org/html/2411.15382. [2411.15382] On the Impact of Fine-Tuning on Chain-of-Thought Reasoning
8. https://ar5iv.org/html/2404.17140. [2404.17140] Small Language Models Need Strong Verifiers to Self-Correct Reasoning
9. https://ar5iv.org/html/2410.18693. [2410.18693] Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch
10. https://ar5iv.org/html/2410.09699. [2410.09699] Honest AI: Fine-Tuning ”Small” Language Models to Say ”I Don’t Know”, and Reducing Hallucination in RAG

## Answer (I don't know?: ✗)

Enhancing reasoning capabilities in small language models through fully supervised fine-tuning involves a variety of methods and challenges, particularly within specialized domains such as finance.

**Methods:**
1. **Teacher-Student Framework:** One common method utilized is the teacher-student model, where a larger pre-trained model (teacher) generates training data that teaches a smaller model (student). For instance, in the financial domain, fine-tuning involves using models like GPT-4 as the teacher to generate Python programs that encode financial reasoning necessary for question answering tasks [3][4]. This approach allows the student models to exhibit enhanced reasoning abilities as they learn from high-quality, structured data produced by the teacher.

2. **Specific Fine-Tuning Approaches:** Fine-tuning specifically targets the unique challenges within the financial domain. The smaller models are trained on multi-hop numerical reasoning tasks to better understand financial concepts and engage in numerical calculations relevant to finance [1]. By employing tailored prompts and carefully generated datasets, the fine-tuning process aligns the student models’ capabilities with the specific requirements of financial question answering [4][5].

3. **Empirical Analysis of Performance:** Recent studies assess the performance of smaller models after fine-tuning. The analysis provides insights into which areas of reasoning and financial concept application are successfully enhanced as a result of this process, demonstrating that fine-tuning can enable smaller models to approach the performance of larger, teacher models [1][2]. 

**Challenges:**
1. **Complexity of Financial Reasoning:** The finance domain is inherently complex, requiring not only understanding of financial concepts but also the ability to perform intricate numerical reasoning [2]. This complexity is distinct from classical question-answering models, creating a significant hurdle for smaller models which may not be able to grasp the full depth of financial reasoning without extensive fine-tuning.

2. **Data Requirements:** Another major challenge is related to the data requirements for effective fine-tuning. Determining the optimal fine-tuning datasets and their characteristics is crucial to achieving meaningful improvements in the smaller models [3]. Insufficient or improperly structured data can result in underperformance and failing to adapt to the reasoning demands posed by financial inquiries.

3. **Performance Variability:** While fine-tuning can dramatically improve reasoning capabilities, the extent of improvement can vary significantly based on the intrinsic characteristics of the base language model being fine-tuned [2]. Understanding how pre-trained knowledge translates into effective reasoning in specific task contexts remains a complex aspect to navigate.

In conclusion, while enhancing reasoning capabilities in small language models through fine-tuning presents a promising avenue for boosting performance in tasks like financial question answering, it also comes with considerable challenges. The intricacies of financial reasoning and the specific requirements for effective fine-tuning underscore the need for ongoing research and experimentation in this field.

1. [1]:  https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
2. [2]:  https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
3. [3]:  https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
4. [4]:  https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
5. [5]:  https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
---
1. [1]:  Passage ID 1: multi-hop numerical reasoning over financial texts. We assess the performance of several smaller models that have been fine-tuned to generate programs that encode the required financial reasoning and calculations. Our findings demonstrate that these fine-tuned smaller models approach the performance of the teacher model.To provide a granular analysis of model performance, we propose an approach to investigate the specific student model capabilities that are enhanced by fine-tuning. Our empirical analysis indicates that fine-tuning refines the student models ability to express and apply the required financial concepts along with adapting the entity extraction for the specific data format. In addition, we hypothesize and demonstrate that comparable financial reasoning capability can be induced using relatively smaller datasets.1 IntroductionIn recent years, the development of large language models (LLMs) has achieved significant advances in natural language processing (NLP).
2. [2]:  Passage ID 2: the finance domain poses a unique set of challenges, requiring the understanding of financial concepts along with the ability to perform numerical reasoning. This complexity introduces a significant challenge that is distinct from classical question answering problems Yang et al. (2018); Rajpurkar et al. (2018)In this paper, we present an empirical study that provides experimental evidence supporting the effectiveness of fine-tuning small language models for financial question answering. Our research is guided by several critical questions:RQ1: To what degree does fine-tuning small language models improve their performance on financial question answering tasks?RQ2: What are the intrinsic characteristics of the base language model that contribute to its performance prior to fine-tuning?RQ3: Which aspects of question answering benefit directly from the pre-trained knowledge, and what specific improvements are enabled by fine-tuning?RQ4: What are the fine-tuning data
3. [3]:  Passage ID 3: fine-tuning?RQ3: Which aspects of question answering benefit directly from the pre-trained knowledge, and what specific improvements are enabled by fine-tuning?RQ4: What are the fine-tuning data requirements to achieve these improvements?To address these questions, we adapt previous approaches on inducing reasoning in smaller models for the financial question answering task. In our experimental setup, we employ GPT-4 as the teacher model, building upon its documented success in the realm of financial question answering Chen et al. (2023); Phogat et al. (2023). For the student models, we explore a suite of state-of-the-art, yet relatively smaller, language models including phi-3 variants (3.5B and 14B parameters) Abdin et al. (2024), Mistral 7B, and Orca-2 configurations (7B and 13B parameters) Mitra et al. (2023). Our methodology involves training the student model using Python programs generated by the teacher model. The teacher generated code systematically delineates the
4. [4]:  Passage ID 4: capabilities in small language models. One of the common approaches is following a teacher-student setup where a pre-trained LLM acts as a teacher, generating training data which is used to teach a small language model, the student. Mukherjee et al. (2023); Mitra et al. (2023) aim to train models to exhibit generic reasoning abilities. They utilize GPT-3.5 Turbo and GPT-4 as teacher models to generate training data with carefully crafted prompts. On the other hand, Fu et al. (2023); Magister et al. (2023); Ho et al. (2023) train task specific small language models with CoT based explanation from pre-trained LLMs. Specifically for problems involving mathematical reasoning, Wang et al. (2023a); Gou et al. (2023); Toshniwal et al. (2024); Wang et al. (2023c) propose to generate programs from the pre-trained LLMs and train the small language models. In contrast, we focus on fine-tuning small language models for financial question answering problems Chen et al. (2021b, 2022); Zhu et al.
5. [5]:  Passage ID 5: "dark"); } else { localStorage.setItem("ar5iv_theme", "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Fine-tuning Smaller Language Models for Question Answering over Financial DocumentsKarmvir Singh Phogat,Sai Akhil Puranam,Sridhar Dasaratha,Chetan Harsha,Shashishekar RamakrishnaEY Global Delivery Services India LLP{Karmvir.Phogat,Sai.Puranam,Sridhar.Dasaratha}@gds.ey.com,{Chetan.Harsha,Shashishekar.R}@gds.ey.comAbstractRecent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain, focusing on the challenge of answering questions that require multi-hop numerical reasoning over financial texts. We assess the performance of several smaller models that have been fine-tuned to generate programs that encode the required financial reasoning and