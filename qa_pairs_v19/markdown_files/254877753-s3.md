# Towards Reasoning in Large Language Models: A Survey

## Question

What are the methods and challenges associated with enhancing reasoning capabilities in small language models through fully supervised finetuning?

## URLs

1. https://ar5iv.org/html/2404.10346. [2404.10346] Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards
2. https://ar5iv.org/html/2408.12337. [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
3. https://ar5iv.org/html/2310.18338. [2310.18338] Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning
4. https://ar5iv.org/html/2411.18915. [2411.18915] MATATA: A weakly-supervised MAthematical Tool-Assisted reasoning for Tabular Applications
5. https://ar5iv.org/html/2410.18890. [2410.18890] Improving Small-Scale Large Language Models Function Calling for Reasoning Tasks
6. https://ar5iv.org/html/2408.06195. [2408.06195] Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers
7. https://ar5iv.org/html/2411.15382. [2411.15382] On the Impact of Fine-Tuning on Chain-of-Thought Reasoning
8. https://ar5iv.org/html/2404.17140. [2404.17140] Small Language Models Need Strong Verifiers to Self-Correct Reasoning
9. https://ar5iv.org/html/2410.18693. [2410.18693] Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch
10. https://ar5iv.org/html/2410.09699. [2410.09699] Honest AI: Fine-Tuning ”Small” Language Models to Say ”I Don’t Know”, and Reducing Hallucination in RAG

## Answer (I don't know?: ✗)

Enhancing reasoning capabilities in small language models through fully supervised fine-tuning involves a variety of methods and encounters several challenges, particularly when applied to domains like finance.

### Methods

1. **Teacher-Student Setup**: A common method used in fine-tuning is the teacher-student model framework, where a larger, pre-trained language model (the teacher) generates training data that the smaller model (the student) uses to learn. In the context of financial question answering, models like GPT-4 serve as teachers, producing Python programs that encapsulate the reasoning and calculations needed for financial tasks [1][4]. This generated data aims to teach the student models how to tackle complex reasoning tasks by breaking them down into manageable computational steps.

2. **Focused Fine-Tuning**: The research indicates that fine-tuning smaller models can significantly enhance their capabilities in specific areas, such as financial reasoning. This is accomplished by adapting their understanding of financial concepts and improving their entity extraction methods tailored to specific data formats [1][2]. By training on relevant datasets, the models can better learn to express complex financial reasoning [5].

3. **Utilizing Smaller Datasets**: Interestingly, the effectiveness of fine-tuning can be achieved even when using relatively smaller datasets. Empirical evidence suggests that models can be trained to reach performance levels comparable to those of much larger models, indicating that well-curated data can effectively compensate for size limitations [1][5].

4. **Performance Assessment**: The combination of automated and manual evaluation methods is also critical. By utilizing the teacher model (e.g., GPT-4) to assess the outputs of the student models both before and after fine-tuning, researchers can pinpoint the specific areas of improvement in reasoning capabilities [5].

### Challenges

1. **Complexity in Financial Concepts**: The finance domain presents unique challenges owing to its inherent complexity. It requires not only a grasp of intricate financial concepts but also the ability to perform numerical reasoning, which distinguishes it from more traditional question-answering tasks [2]. This complexity makes the fine-tuning process more demanding, as the models must navigate through sophisticated material.

2. **Data Requirements and Quality**: The quality and relevance of the fine-tuning data are paramount. The finetuning process necessitates high-quality data that accurately represents the tasks the models will encounter post-training. In contexts where such data is scarce or poorly structured, achieving robust reasoning capabilities can become a significant challenge [3].

3. **Intrinsic Model Characteristics**: Understanding the baseline capabilities of the pre-trained models prior to fine-tuning is crucial. Researchers are exploring which aspects of these models contribute to their performance, and identifying the features that benefit most from the fine-tuning can be complex [2][3]. 

4. **Transfer of Knowledge**: Finally, navigating the nuances of transferring knowledge from the teacher model to the student can pose additional challenges. Ensuring that the smaller models adequately absorb and utilize the reasoning strategies encoded in the teacher-generated programs requires careful design and execution of the fine-tuning process [4][5].

In conclusion, while the methods for enhancing reasoning in small language models through fine-tuning show promise, especially when leveraging teacher models and focused datasets, the challenges pertaining to complexity, data quality, intrinsic model features, and knowledge transfer must be carefully managed to achieve optimal outcomes in financial reasoning tasks.

1. [1]:  https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
2. [2]:  https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
3. [3]:  https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
4. [4]:  https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
5. [5]:  https://ar5iv.org/html/2408.12337, [2408.12337] Fine-tuning Smaller Language Models for Question Answering over Financial Documents
---
1. [1]:  Passage ID 1: multi-hop numerical reasoning over financial texts. We assess the performance of several smaller models that have been fine-tuned to generate programs that encode the required financial reasoning and calculations. Our findings demonstrate that these fine-tuned smaller models approach the performance of the teacher model.To provide a granular analysis of model performance, we propose an approach to investigate the specific student model capabilities that are enhanced by fine-tuning. Our empirical analysis indicates that fine-tuning refines the student models ability to express and apply the required financial concepts along with adapting the entity extraction for the specific data format. In addition, we hypothesize and demonstrate that comparable financial reasoning capability can be induced using relatively smaller datasets.1 IntroductionIn recent years, the development of large language models (LLMs) has achieved significant advances in natural language processing (NLP).
2. [2]:  Passage ID 2: the finance domain poses a unique set of challenges, requiring the understanding of financial concepts along with the ability to perform numerical reasoning. This complexity introduces a significant challenge that is distinct from classical question answering problems Yang et al. (2018); Rajpurkar et al. (2018)In this paper, we present an empirical study that provides experimental evidence supporting the effectiveness of fine-tuning small language models for financial question answering. Our research is guided by several critical questions:RQ1: To what degree does fine-tuning small language models improve their performance on financial question answering tasks?RQ2: What are the intrinsic characteristics of the base language model that contribute to its performance prior to fine-tuning?RQ3: Which aspects of question answering benefit directly from the pre-trained knowledge, and what specific improvements are enabled by fine-tuning?RQ4: What are the fine-tuning data
3. [3]:  Passage ID 3: fine-tuning?RQ3: Which aspects of question answering benefit directly from the pre-trained knowledge, and what specific improvements are enabled by fine-tuning?RQ4: What are the fine-tuning data requirements to achieve these improvements?To address these questions, we adapt previous approaches on inducing reasoning in smaller models for the financial question answering task. In our experimental setup, we employ GPT-4 as the teacher model, building upon its documented success in the realm of financial question answering Chen et al. (2023); Phogat et al. (2023). For the student models, we explore a suite of state-of-the-art, yet relatively smaller, language models including phi-3 variants (3.5B and 14B parameters) Abdin et al. (2024), Mistral 7B, and Orca-2 configurations (7B and 13B parameters) Mitra et al. (2023). Our methodology involves training the student model using Python programs generated by the teacher model. The teacher generated code systematically delineates the
4. [4]:  Passage ID 4: capabilities in small language models. One of the common approaches is following a teacher-student setup where a pre-trained LLM acts as a teacher, generating training data which is used to teach a small language model, the student. Mukherjee et al. (2023); Mitra et al. (2023) aim to train models to exhibit generic reasoning abilities. They utilize GPT-3.5 Turbo and GPT-4 as teacher models to generate training data with carefully crafted prompts. On the other hand, Fu et al. (2023); Magister et al. (2023); Ho et al. (2023) train task specific small language models with CoT based explanation from pre-trained LLMs. Specifically for problems involving mathematical reasoning, Wang et al. (2023a); Gou et al. (2023); Toshniwal et al. (2024); Wang et al. (2023c) propose to generate programs from the pre-trained LLMs and train the small language models. In contrast, we focus on fine-tuning small language models for financial question answering problems Chen et al. (2021b, 2022); Zhu et al.
5. [5]:  Passage ID 5: models by introducing an approach designed for question answering over financial reports.2.Our experimental study on three financial datasets provides insights on the performance of the small models3.We propose an evaluation method that utilizes GPT-4 to assess the Python code outputs from the student models, pre- and post-fine-tuning. Combining GPT-4’s automated assessment with manual evaluation yields new insights into the distinct competencies that are enhanced in the student model during fine-tuning.4.Motivated by these insights we explore the use of smaller datasets for fine-tuning and provide empirical evidence demonstrating their effectiveness.Our experimental findings reveal that smaller language models fine-tuned for financial reasoning, can achieve performance that rivals that of the larger teacher model. Moreover, our empirical analysis suggests that the fine-tuning helps refine concept understanding and enables consistent reasoning with those