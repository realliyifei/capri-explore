# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How do the conventions for interpreting kappa-like coefficients differ between medical research and computational linguistics?

## URLs

1. https://ar5iv.org/html/2410.04981. [2410.04981] On the Rigour of Scientific Writing: Criteria, Analysis, and Insights
2. https://ar5iv.org/html/2203.09628. [2203.09628] Better to be in agreement than in bad company: a critical analysis of many kappa-like tests assessing one-million 2x2 contingency tables
3. https://ar5iv.org/html/1504.00854. [1504.00854] Evaluation Evaluation a Monte Carlo study
4. https://ar5iv.org/html/2403.03396. [2403.03396] Japanese-English Sentence Translation Exercises Dataset for Automatic Grading
5. https://ar5iv.org/html/2407.02751. [2407.02751] Emotion and Intent Joint Understanding in Multimodal Conversation: A Benchmarking Dataset
6. https://ar5iv.org/html/2204.11424. [2204.11424] It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers
7. https://ar5iv.org/html/2407.02472. [2407.02472] ValueScope: Unveiling Implicit Norms and Values via Return Potential Model of Social Interactions
8. https://ar5iv.org/html/2406.17675. [2406.17675] Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models
9. https://ar5iv.org/html/2210.00105. [2210.00105] A Decade of Knowledge Graphs in Natural Language Processing: A Survey
10. https://ar5iv.org/html/2401.05176. [2401.05176] Can ChatGPT Rival Neural Machine Translation? A Comparative Study

## Answer (I don't know?: ✓)

I don't know. The context provided does not contain specific information regarding the conventions for interpreting kappa-like coefficients in medical research as compared to computational linguistics. It primarily discusses the use of Cohen's Kappa coefficient within the context of classifying papers related to knowledge graphs (KGs) in natural language processing (NLP), noting the coefficients' values and their interpretation in this specific survey [1] [2]. 

While references to the interpretation of Cohen’s Kappa coefficient as "substantial" for values from 0.61 to 0.80 and "almost perfect" from 0.81 to 1.00 are given, there is no comparative analysis provided that contrasts these conventions with those specifically used in medical research [2]. Thus, crucial details or standards relevant to both fields are missing from the provided passages, rendering it impossible to answer your question about the differences in interpretation between the two disciplines.

1. [1]:  https://ar5iv.org/html/2210.00105, [2210.00105] A Decade of Knowledge Graphs in Natural Language Processing: A Survey
2. [2]:  https://ar5iv.org/html/2210.00105, [2210.00105] A Decade of Knowledge Graphs in Natural Language Processing: A Survey
3. [3]:  https://ar5iv.org/html/2210.00105, [2210.00105] A Decade of Knowledge Graphs in Natural Language Processing: A Survey
4. [4]:  https://ar5iv.org/html/2403.03396, [2403.03396] Japanese-English Sentence Translation Exercises Dataset for Automatic Grading
5. [5]:  https://ar5iv.org/html/2204.11424, [2204.11424] It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers
---
1. [1]:  Passage ID 1: affect the accuracy of the classification results. To mitigate this risk, the authors regularly discussed the used classification schemes and assigned labels to establish a common understanding of each class. In addition, we calculated Cohen’s Kappa coefficient to quantify the reliability of the inter-annotator agreement.7 ConclusionRecent years have witnessed a rising prominence of KGs in NLP research. Despite the rapidly growing body of literature, until now, no study has been published that summarizes the progress so far. To provide an overview of this maturing research area, we performed a multifaceted survey of tasks, research types, and contributions.Our findings show that a large number of tasks concerning KGs in NLP have been studied across various domains, including emerging topics like knowledge graph embedding or augmented language models. However, we observed a lack of secondary research and evaluations in practice, both of which are crucial to reflect the major
2. [2]:  Passage ID 2: the two authors independently classified a random sample of 50 papers. We calculated Cohen’s Kappa coefficient of these annotations for each facet (Cohen, 1960). The annotations of the task, research, and contribution facets had coefficients of 0.73, 0.87, and 0.76, respectively. Cohen suggested interpreting Kappa values from 0.61 to 0.80 as substantial and from 0.81 to 1.00 as almost perfect agreement.4 ResultsIn this chapter, we report the results of the data extraction process. It is arranged into subsections according to the formulated RQs.4.1 Characteristics of the Research Landscape (RQ1)In regard to the literature on KGs in NLP, we started our analysis by looking at the number of studies as an indicator of research interest. The distribution of publications over the ten-year observation period is illustrated in Figure 2. While the first publications appear in 2013, the annual publications grew slowly between 2013 and 2016. From 2017 onwards, the number of
3. [3]:  Passage ID 3: need for more works that present an overview of the research field.The frequency of tasks in our survey greatly varies, as reflected in Table 2. Studies concerning KG construction account for the majority of all papers. Applied NLP tasks such as QA and semantic search also have a strong research community. The most emergent topics in recent years have been augmented language models, QA, and KG embedding. Some of the outlined tasks are still confined to the research community, while others have found practical application in many real-life contexts. From Figure 4 it is evident that the KG construction tasks and semantic search over KGs are the most widely applied ones. Of the NLP tasks, QA and conversational interfaces have been adopted to many real-life domains, usually in the form of digital assistants. Tasks like KG embedding and augmented language models are still only being researched and lack a widespread practical adoption in real-world scenarios. We anticipate that as the
4. [4]:  Passage ID 4: 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations, pages 235–240, Hong Kong, China. Association for Computational Linguistics.Landis and Koch (1977)J. Richard Landis and Gary G. Koch. 1977.The measurement of observer agreement for categorical data.Biometrics, 33(1):159–174.Larsen-Freeman (2012)Diane Larsen-Freeman. 2012.On the roles of repetition in language teaching and learning.Applied Linguistics Review, 3(2):195–210.Mizumoto et al. (2019)Tomoya Mizumoto, Hiroki Ouchi, Yoriko Isobe, Paul Reisert, Ryo Nagata, Satoshi Sekine, and Kentaro Inui. 2019.Analytic score prediction and justification identification in automated short answer scoring.In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 316–325, Florence, Italy. Association for Computational Linguistics.Nagata (2019)Ryo Nagata. 2019.Toward a task of feedback comment
5. [5]:  Passage ID 5: Maryland.Manning (2015)Manning, Christopher D. 2015.Last words: Computational linguistics and deep learning.Computational Linguistics, 41(4):701–707.Manning et al. (2014b)Manning, Christopher D., Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J.Bethard, and David McClosky. 2014b.The Stanford CoreNLP natural language processing toolkit.In Association for Computational Linguistics (ACL) SystemDemonstrations, pages 55–60.McHugh (2012)McHugh, Mary L. 2012.Interrater reliability: the kappa statistic.Biochemia medica, 22(3):276–282.Miller et al. (2000)Miller, Scott, Heidi Fox, Lance Ramshaw, and Ralph Weischedel. 2000.A novel use of statistical parsing to extract information from text.In 1st Meeting of the North American Chapter of theAssociation for Computational Linguistics, page 226–233.Mintz et al. (2009)Mintz, Mike, Steven Bills, Rion Snow, and Dan Jurafsky. 2009.Distant supervision for relation extraction without