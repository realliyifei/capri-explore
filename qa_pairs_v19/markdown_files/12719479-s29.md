# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How do the conventions for interpreting kappa-like coefficients differ between medical research and computational linguistics?

## URLs

1. https://ar5iv.org/html/2410.04981. [2410.04981] On the Rigour of Scientific Writing: Criteria, Analysis, and Insights
2. https://ar5iv.org/html/2203.09628. [2203.09628] Better to be in agreement than in bad company: a critical analysis of many kappa-like tests assessing one-million 2x2 contingency tables
3. https://ar5iv.org/html/1504.00854. [1504.00854] Evaluation Evaluation a Monte Carlo study
4. https://ar5iv.org/html/2403.03396. [2403.03396] Japanese-English Sentence Translation Exercises Dataset for Automatic Grading
5. https://ar5iv.org/html/2407.02751. [2407.02751] Emotion and Intent Joint Understanding in Multimodal Conversation: A Benchmarking Dataset
6. https://ar5iv.org/html/2204.11424. [2204.11424] It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers
7. https://ar5iv.org/html/2407.02472. [2407.02472] ValueScope: Unveiling Implicit Norms and Values via Return Potential Model of Social Interactions
8. https://ar5iv.org/html/2406.17675. [2406.17675] Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models
9. https://ar5iv.org/html/2210.00105. [2210.00105] A Decade of Knowledge Graphs in Natural Language Processing: A Survey
10. https://ar5iv.org/html/2401.05176. [2401.05176] Can ChatGPT Rival Neural Machine Translation? A Comparative Study

## Answer (I don't know?: ‚úì)

I don't know. The context provided does not contain specific information about the conventions for interpreting kappa-like coefficients in medical research or computational linguistics. The passages primarily discuss the applications of Cohen's Kappa in various research contexts, such as inter-annotator agreement in NLP tasks [1][4], and make references to its use in evaluating collaborative responses from AI models [2]. However, there is no comparative analysis or convention specific to medical research versus computational linguistics for interpreting these statistical measures. Thus, the context is insufficient to answer the question fully and precisely.

1. [1]:  https://ar5iv.org/html/2210.00105, [2210.00105] A Decade of Knowledge Graphs in Natural Language Processing: A Survey
2. [2]:  https://ar5iv.org/html/2411.16797, No Title
3. [3]:  https://ar5iv.org/html/2004.01092, No Title
4. [4]:  https://ar5iv.org/html/2210.00105, [2210.00105] A Decade of Knowledge Graphs in Natural Language Processing: A Survey
5. [5]:  https://ar5iv.org/html/2408.09235, No Title
---
1. [1]:  Passage ID 1: affect the accuracy of the classification results. To mitigate this risk, the authors regularly discussed the used classification schemes and assigned labels to establish a common understanding of each class. In addition, we calculated Cohen‚Äôs Kappa coefficient to quantify the reliability of the inter-annotator agreement.7 ConclusionRecent years have witnessed a rising prominence of KGs in NLP research. Despite the rapidly growing body of literature, until now, no study has been published that summarizes the progress so far. To provide an overview of this maturing research area, we performed a multifaceted survey of tasks, research types, and contributions.Our findings show that a large number of tasks concerning KGs in NLP have been studied across various domains, including emerging topics like knowledge graph embedding or augmented language models. However, we observed a lack of secondary research and evaluations in practice, both of which are crucial to reflect the major
2. [2]:  Passage ID 2: enhances the reliability and precision of responses. By employing statistical methods such as chi-square tests, Fleiss‚Äô Kappa, and confidence interval analysis, we evaluate consensus rates and inter-rater agreement to quantify the reliability of collaborative outputs. Key results reveal that Claude and GPT-4 exhibit the highest reliability and consistency, as evidenced by their narrower confidence intervals and higher alignment with question-generating models. Conversely, Gemini and LLaMA show more significant variability in their consensus rates, as reflected in wider confidence intervals and lower reliability percentages. These findings demonstrate that collaborative interactions among large language models (LLMs) significantly improve response reliability, offering novel insights into autonomous, cooperative reasoning and validation in AI systems.Keywords: Large Language Models, Collaborative Intelligence, Answer Validation, Game Theory, Statistical Analysis1
3. [3]:  Passage ID 3: described in Section 5.After producing the second draft, two linguists worked independently on a first batch of documents of the NUBes¬†corpus. Their results were compared and multiple questions and disagreements that arose were discussed. The team also consulted a medical expert who aided them with some difficult scenarios, which are also examined in Section 5. All this greatly contributed towards producing the final version of the guidelines.Then, the two linguists annotated the same batch adhering to the final guidelines. The inter-annotator agreement was then calculated on the second draft annotations and the final guideline annotations. As Table 2 shows, the agreement (Cohen‚Äôs kappa, Œ∫ùúÖ\kappa, and linearly weighted Œ∫ùúÖ\kappa, l‚Äãw‚ÄãŒ∫ùëôùë§ùúÖlw\kappa) improved after the discussion, particularly for cues. The low agreement in polarity items is explained by the fact that they occur very few times (15) and the number of possible tags is also small (2: either it is a polarity item or it
4. [4]:  Passage ID 4: the two authors independently classified a random sample of 50 papers. We calculated Cohen‚Äôs Kappa coefficient of these annotations for each facet (Cohen, 1960). The annotations of the task, research, and contribution facets had coefficients of 0.73, 0.87, and 0.76, respectively. Cohen suggested interpreting Kappa values from 0.61 to 0.80 as substantial and from 0.81 to 1.00 as almost perfect agreement.4 ResultsIn this chapter, we report the results of the data extraction process. It is arranged into subsections according to the formulated RQs.4.1 Characteristics of the Research Landscape (RQ1)In regard to the literature on KGs in NLP, we started our analysis by looking at the number of studies as an indicator of research interest. The distribution of publications over the ten-year observation period is illustrated in Figure 2. While the first publications appear in 2013, the annual publications grew slowly between 2013 and 2016. From 2017 onwards, the number of
5. [5]:  Passage ID 5: to human evaluations?In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15607‚Äì15631, Toronto, Canada. Association for Computational Linguistics.Errica et¬†al. (2024)Federico Errica, Giuseppe Siracusano, Davide Sanvito, and Roberto Bifulco. 2024.What did i do wrong? quantifying llms‚Äô sensitivity and consistency to prompt engineering.Fleiss and Cohen (1973)Joseph¬†L Fleiss and Jacob Cohen. 1973.The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability.Educational and psychological measurement, 33(3):613‚Äì619.Gou et¬†al. (2024)Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024.Critic: Large language models can self-correct with tool-interactive critiquing.Hada et¬†al. (2024)Rishav Hada, Varun Gumma, Adrian de¬†Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana