# A Survey on Detection of LLMs-Generated Content

## Question

What are the limitations of using AUROC as a metric for evaluating detection algorithms, and what alternative metrics are suggested?

## URLs

1. https://ar5iv.org/html/2408.10193. [2408.10193] Area under the ROC Curve has the Most Consistent Evaluation for Binary Classification
2. https://ar5iv.org/html/2306.14658. [2306.14658] Beyond AUROC & co. for evaluating out-of-distribution detection performance
3. https://ar5iv.org/html/2409.20078. [2409.20078] Quantifying discriminability of evaluation metrics in link prediction for real networks
4. https://ar5iv.org/html/2008.02577. [2008.02577] A critical analysis of metrics used for measuring progress in artificial intelligence
5. https://ar5iv.org/html/2402.08893. [2402.08893] Inconsistency among evaluation metrics in link prediction
6. https://ar5iv.org/html/2205.04615. [2205.04615] Discriminating abilities of threshold-free evaluation metrics in link prediction
7. https://ar5iv.org/html/2010.16253. [2010.16253] Limitations of ROC on Imbalanced Data: Evaluation of LVAD Mortality Risk Scores
8. https://ar5iv.org/html/2401.13391. [2401.13391] Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely on between-group metrics
9. https://ar5iv.org/html/2305.04754. [2305.04754] Is AUC the best measure for practical comparison of anomaly detectors?

## Answer (I don't know?: ✗)

The AUROC (Area Under the Receiver Operating Characteristic curve) has several limitations when used as a metric for evaluating detection algorithms, particularly in the context of Out-Of-Distribution (OOD) detection methods. One key limitation is that AUROC captures performance across the entire range of possible thresholds but does not account for the practical implications of specific threshold choices. This can result in an oversimplified view of a model's effectiveness, especially when models are deployed in situations where poor separation between in-distribution (ID) and OOD samples could lead to critical failures [1].

Moreover, while AUROC is threshold-independent, this feature may be misleading in real-world applications where the choice of threshold significantly impacts outcomes. Current evaluation approaches often reduce OOD detection to a binary classification task, neglecting the necessity to thoughtfully consider these thresholds, which can result in misleading evaluations of model performance [1]. 

An alternative metric proposed is the Area Under the Threshold Curve (AUTC). This metric seeks to address the limitations of AUROC by explicitly penalizing poor separation between ID and OOD samples. The AUTC focuses on the model's ability to maintain high performance across a range of threshold settings, rather than segregating results into simplified binary classifications [1]. 

In combination with AUTC, it can be beneficial to use other established metrics, especially in scenarios that require multi-class classification. For example, accuracy on a test set is commonly used in such situations, providing a more rounded understanding of model effectiveness beyond binary comparisons, as noted in the context of the TALENT toolbox application in various classification tasks [2]. This shows a shift towards utilizing a robust set of evaluation methods rather than relying solely on AUROC.

Overall, recognizing the limitations of AUROC and incorporating metrics like AUTC alongside traditional accuracy measures allows for a more comprehensive evaluation of detection algorithms. Thus, a multi-metric approach can better capture the nuances of model performance in both theoretical analyses and real-world scenarios.

1. [1]:  https://ar5iv.org/html/2306.14658, [2306.14658] Beyond AUROC & co. for evaluating out-of-distribution detection performance
2. [2]:  https://ar5iv.org/html/2411.11148, No Title
3. [3]:  https://ar5iv.org/html/2402.13213, No Title
4. [4]:  https://ar5iv.org/html/2205.04615, [2205.04615] Discriminating abilities of threshold-free evaluation metrics in link prediction
5. [5]:  https://ar5iv.org/html/2401.02984, No Title
---
1. [1]:  Passage ID 1: AI, it is important to examine whether the basis for comparing OOD detection methods is consistent with practical needs. In this work, we take a closer look at the go-to metrics for evaluating OOD detection, and question the approach of exclusively reducing OOD detection to a binary classification task with little consideration for the detection threshold. We illustrate the limitations of current metrics (AUROC & its friends) and propose a new metric - Area Under the Threshold Curve (AUTC), which explicitly penalizes poor separation between ID and OOD samples. Scripts and data are available at https://github.com/glhr/beyond-auroc1 IntroductionWhen deployed out in the wild, computer vision systems may be faced with image content which they simply are not equipped to handle. For instance, a model trained to recognize certain types of skin lesions, once deployed in clinical practice, may encounter images with a different kind of skin condition, or images with no lesions at all [20,
2. [2]:  Passage ID 2: 4. We leverage the implementations of various methods in TALENT toolbox [32] for training and comparisons.4.1.4 MetricsSince most of the tasks in our analysis focus on binary classification, we primarily use the Area Under the Receiver Operating Characteristic curve (AUROC) to evaluate performance. AUROC provides a robust measure of the model’s ability to differentiate between the two classes within the dataset. For the two multi-class datasets, Volkert (VO) and MNIST (MN), we instead use accuracy on the test set as the performance metric. This approach allows us to appropriately assess the model’s effectiveness across different types of classification tasks.4.1.5 BaselinesFollowing the previous paradigm [43, 51], we conduct a comprehensive comparison of our model against a range of established techniques, including traditional algorithms like logistic regression and random forests, as well as advanced boosting frameworks such as XGBoost, LightGBM, and CatBoost.
3. [3]:  Passage ID 3: be able to discriminate between correct and incorrect answers better than random chance.Our primary success metric was the Area Under the Receiver Operating Characteristic curve (AUROC) (Bradley, 1997). The AUROC of a binary classifier ranges from 0% to 100%, where 0% corresponds to getting every prediction wrong, 50% is random chance, and 100% is perfect classification. AUROC is also equivalent to the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. Conveniently, AUROC is threshold independent: it captures the model’s performance across the entire range of possible thresholds.Our code can be found at https://github.com/bplaut/softmax-probs-predict-llm-correctness.1.2 Our resultsTable 1: Main AUROC results. AUROC and Q&A values are percentages, averaged over ten data points (five datasets and two phrasings). The p<10−4𝑝superscript104p<10^{-4} columns indicate how many data points resulted in a p-value less than
4. [4]:  Passage ID 4: becomes increasingly popular, especially in biological studies. Based on a toy model with tunable noise and predictability, we propose a method to measure the discriminating abilities of any given metric. We apply this method to the above three threshold-free metrics, showing that AUC and AUPR are remarkably more discriminating than BP, and AUC is slightly more discriminating than AUPR. The result suggests that it is better to simultaneously use AUC and AUPR in evaluating link prediction algorithms, at the same time, it warns us that the evaluation based only on BP may be unauthentic. This article provides a starting point towards a comprehensive picture about effectiveness of evaluation metrics for link prediction and other classification problems.  Comments:27 pages, 5 figures, 1 tableSubjects:Data Analysis, Statistics and Probability (physics.data-an)Cite as:arXiv:2205.04615 [physics.data-an] (or arXiv:2205.04615v3 [physics.data-an] for this version)
5. [5]:  Passage ID 5: such as Area Under the Receiver Operating Characteristic (AUROC),[78, 69] and Specificity [78] for a holistic understanding of LLM diagnostic validity.In evaluating language proficiency, BLEU,[44, 119, 50, 75, 54] ROUGE,[59, 46, 50, 52, 75] Distinct-N,[44, 46, 52] and METEOR [52] are widely used to assess alignment with human-like language, diversity of expression, and overall quality of the generated text. Advanced metrics such as GPT3-Score,[59] BARTScore,[59] and BERT-Score [49, 50, 54] are tailored to evaluate semantic coherence and relevance of text in specific contexts, addressing the demand for nuanced understanding. Perplexity [46, 52] quantifies model predictability and text naturalness, while Extrema and Vector Extrema [52] analyze the diversity and distribution in the embedding space to reflect the model’s linguistic creativity and depth. The adoption of these traditional language evaluation metrics appears to be driven by a lack of automated, efficient, and easily