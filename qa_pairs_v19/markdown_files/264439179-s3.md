# A Survey on Detection of LLMs-Generated Content

## Question

What are the limitations of using AUROC as a metric for evaluating detection algorithms, and what alternative metrics are suggested?

## URLs

1. https://ar5iv.org/html/2408.10193. [2408.10193] Area under the ROC Curve has the Most Consistent Evaluation for Binary Classification
2. https://ar5iv.org/html/2306.14658. [2306.14658] Beyond AUROC & co. for evaluating out-of-distribution detection performance
3. https://ar5iv.org/html/2409.20078. [2409.20078] Quantifying discriminability of evaluation metrics in link prediction for real networks
4. https://ar5iv.org/html/2008.02577. [2008.02577] A critical analysis of metrics used for measuring progress in artificial intelligence
5. https://ar5iv.org/html/2402.08893. [2402.08893] Inconsistency among evaluation metrics in link prediction
6. https://ar5iv.org/html/2205.04615. [2205.04615] Discriminating abilities of threshold-free evaluation metrics in link prediction
7. https://ar5iv.org/html/2010.16253. [2010.16253] Limitations of ROC on Imbalanced Data: Evaluation of LVAD Mortality Risk Scores
8. https://ar5iv.org/html/2401.13391. [2401.13391] Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely on between-group metrics
9. https://ar5iv.org/html/2305.04754. [2305.04754] Is AUC the best measure for practical comparison of anomaly detectors?

## Answer (I don't know?: ✗)

The use of the area under the receiver operating characteristic curve (AUROC) as a metric for evaluating detection algorithms, particularly in out-of-distribution (OOD) detection and anomaly detection, has several limitations. One significant limitation is its tendency to oversimplify the evaluation by reducing it to a binary classification task without adequate consideration for the detection threshold. This lack of focus on the detection threshold can hinder a more nuanced understanding of model performance, particularly in real-world applications where the cost of misclassifications may vary significantly [1]. Additionally, AUROC is often viewed as the gold-standard metric, but it has been criticized for using inconsistent misclassification cost matrices across different classifiers, which can lead to misleading comparisons [2] [5].

Furthermore, while AUROC provides a single value for model comparison, it may not completely reflect the discriminative abilities of the model, particularly in cases of imbalanced classes or when different thresholds yield varying results [2] [3]. In essence, AUROC may not adequately capture the nuances of model performance across different operating conditions and is less effective when the distribution of class instances is not balanced.

To address these shortcomings, alternative metrics are suggested. One proposed alternative is the Area Under the Threshold Curve (AUTC), which explicitly penalizes poor separation between in-distribution (ID) and out-of-distribution (OOD) samples, thus providing a more comprehensive evaluation of detection capabilities that considers the detection threshold explicitly [1]. Another alternative is the area under the precision-recall curve (AUPR), which has gained popularity, especially in biological studies, as it tends to be more informative in scenarios where classes are imbalanced [2]. Combining both AUC and AUPR can provide a more robust evaluation framework for OOD detection methods and other classification problems, as these metrics can capture different aspects of model performance that AUROC alone might miss [2].

Additionally, improved versions of AUROC have also been proposed to address its limitations. For example, the H-measure maintains the core concept of AUROC while ensuring consistent misclassification cost matrices across different classifiers, thus providing a more reliable comparison [5]. Another variant, AUC-mROC, acknowledges that the importance of earlier positions in a ranking is greater than that of later ones, rescaling the coordinates of the ROC curve to emphasize this aspect [5].

In summary, while AUROC has traditionally been used to evaluate detection algorithms, its limitations necessitate the exploration of alternative metrics such as AUTC, AUPR, H-measure, and AUC-mROC to achieve a more comprehensive and accurate assessment of model performance.

1. [1]:  https://ar5iv.org/html/2306.14658, [2306.14658] Beyond AUROC & co. for evaluating out-of-distribution detection performance
2. [2]:  https://ar5iv.org/html/2205.04615, [2205.04615] Discriminating abilities of threshold-free evaluation metrics in link prediction
3. [3]:  https://ar5iv.org/html/2305.04754, [2305.04754] Is AUC the best measure for practical comparison of anomaly detectors?
4. [4]:  https://ar5iv.org/html/2402.08893, [2402.08893] Inconsistency among evaluation metrics in link prediction
5. [5]:  https://ar5iv.org/html/2409.20078, [2409.20078] Quantifying discriminability of evaluation metrics in link prediction for real networks
---
1. [1]:  Passage ID 1: AI, it is important to examine whether the basis for comparing OOD detection methods is consistent with practical needs. In this work, we take a closer look at the go-to metrics for evaluating OOD detection, and question the approach of exclusively reducing OOD detection to a binary classification task with little consideration for the detection threshold. We illustrate the limitations of current metrics (AUROC & its friends) and propose a new metric - Area Under the Threshold Curve (AUTC), which explicitly penalizes poor separation between ID and OOD samples. Scripts and data are available at https://github.com/glhr/beyond-auroc1 IntroductionWhen deployed out in the wild, computer vision systems may be faced with image content which they simply are not equipped to handle. For instance, a model trained to recognize certain types of skin lesions, once deployed in clinical practice, may encounter images with a different kind of skin condition, or images with no lesions at all [20,
2. [2]:  Passage ID 2: popular metrics in early studies, while their effectiveness is recently under debate. At the same time, the area under the precision-recall curve (AUPR) becomes increasingly popular, especially in biological studies. Based on a toy model with tunable noise and predictability, we propose a method to measure the discriminating abilities of any given metric. We apply this method to the above three threshold-free metrics, showing that AUC and AUPR are remarkably more discriminating than BP, and AUC is slightly more discriminating than AUPR. The result suggests that it is better to simultaneously use AUC and AUPR in evaluating link prediction algorithms, at the same time, it warns us that the evaluation based only on BP may be unauthentic. This article provides a starting point towards a comprehensive picture about effectiveness of evaluation metrics for link prediction and other classification problems.  Comments:27 pages, 5 figures, 1 tableSubjects:Data Analysis, Statistics and
3. [3]:  Passage ID 3: estimates (Latecki et al., 2007), histogram–based models (Pevný, 2016) or neural networks (Schlegl et al., 2017). Several comparative studies exist – e.g. (Goldstein and Uchida, 2016; Pimentel et al., 2014; Lazarevic et al., 2003; Chandola et al., 2009) or (Markou and Singh, 2003) – in which the authors present an overview of existing anomaly detection methods, sometimes with a direct comparison on benchmark datasets. However, the authors never ask the question if the way the methods are compared to each other is appropriate for the anomaly detection setting. The area under the receiver operating characteristic curve (ROC AUC or just AUC in this text) seems to be the gold-standard of the field, as it allows us to compare models by a single number. In (Vanderlooy and Hüllermeier, 2008), the authors compare ”soft” variants of AUC in which the contribution of a sample to the total value of AUC is weighted by the degree in which it is labeled correctly or incorrectly. They demonstrate that
4. [4]:  Passage ID 4: AUC-Precision and NDCG should be chosen as a metric. (iii) If we don’t have any clues to determine the threshold, it is better not to use threshold-dependent metrics, while if for a specific problem, some thresholds are meaningful, we can choose one (and no more than one) threshold-dependent metric with those thresholds. (iv) To use AUC-mROC in the scenario where only a limited number of predictions are relevant and the values of those predictions decay fast with their positions.In general, during the early stages of a discipline’s development, exploratory work tends to be more attractive than reflective work, hence the majority of scientists typically allocate their primary efforts to exploring new frontiers. However, once the discipline reaches a certain level of maturity, reflective work becomes essential; otherwise, the defects in the foundation underlying a taller and taller building will lead to greater losses. Link prediction is a young and niche branch of network science.
5. [5]:  Passage ID 5: which is common in search and recommendation) [47]; this limitation has also been noted in the evaluation of binary classification algorithms [48]. To address potential shortcomings of AUC, several improved versions have been proposed. For example, Hand pointed out that: AUC uses different misclassification cost matrices when comparing different classifiers. Based on this, Hand proposed a metric called H-measure, which adopts the core concept of AUC while ensures consistent misclassification cost matrices across different classifiers [49]. Muscoloni et al. argued that the importance of earlier positions is greater than that of later ones, leading them to propose a new metric called AUC-mROC, based on rescaling the horizontal and vertical coordinates of the ROC curve [44].Given the multitude of evaluation metrics available, how do researchers actually choose them? As shown in Fig. 1, we survey high-influential papers as well as some selected studies in the last five years, and find