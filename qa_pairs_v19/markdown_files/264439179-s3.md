# A Survey on Detection of LLMs-Generated Content

## Question

What are the limitations of using AUROC as a metric for evaluating detection algorithms, and what alternative metrics are suggested?

## URLs

1. https://ar5iv.org/html/2408.10193. [2408.10193] Area under the ROC Curve has the Most Consistent Evaluation for Binary Classification
2. https://ar5iv.org/html/2306.14658. [2306.14658] Beyond AUROC & co. for evaluating out-of-distribution detection performance
3. https://ar5iv.org/html/2409.20078. [2409.20078] Quantifying discriminability of evaluation metrics in link prediction for real networks
4. https://ar5iv.org/html/2008.02577. [2008.02577] A critical analysis of metrics used for measuring progress in artificial intelligence
5. https://ar5iv.org/html/2402.08893. [2402.08893] Inconsistency among evaluation metrics in link prediction
6. https://ar5iv.org/html/2205.04615. [2205.04615] Discriminating abilities of threshold-free evaluation metrics in link prediction
7. https://ar5iv.org/html/2010.16253. [2010.16253] Limitations of ROC on Imbalanced Data: Evaluation of LVAD Mortality Risk Scores
8. https://ar5iv.org/html/2401.13391. [2401.13391] Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely on between-group metrics
9. https://ar5iv.org/html/2305.04754. [2305.04754] Is AUC the best measure for practical comparison of anomaly detectors?

## Answer (I don't know?: âœ—)

The use of the Area Under the Receiver Operating Characteristic (AUROC) curve as a metric for evaluating detection algorithms has several limitations highlighted in the provided context. Primarily, AUROC is critiqued for its inability to consider the impact of detection thresholds effectively. The traditional approach often reduces Out-of-Distribution (OOD) detection to a binary classification task, neglecting the critical role that thresholds play in practical deployment scenarios. This can lead to a lack of nuance in evaluating model performance, particularly in situations where the distinction between in-distribution (ID) and OOD samples is not sufficiently clear [1].

Furthermore, while AUROC supposedly provides a comprehensive measure of a model's ability to distinguish between classes, it is noted that this metric can sometimes mask underlying performance issues. For instance, AUROC ranges from 0% (complete failure) to 100% (perfect classification), where 50% represents random chance [4]. In scenarios where the dataset is imbalanced or where certain classes are of greater interest than others, relying solely on AUROC can be misleading. The same concern applies to the comparison methodologies that predominantly focus on AUROC, without adequately addressing whether the evaluation methods are suitable for anomaly or OOD detection contexts [5].

An alternative proposed to overcome the limitations associated with AUROC is the Area Under the Threshold Curve (AUTC). This new metric explicitly penalizes poor separations between ID and OOD samples, thus providing a more nuanced evaluation that accounts for varying thresholds in detection tasks [1]. The goal of AUTC is to reflect more accurately the real-world challenges faced in deploying detection algorithms, especially given that many applications require sensitivity to these distinctions.

Additionally, other metrics such as the Area Under the Precision-Recall curve (AUPR) are gaining traction in specific domains like biological studies. The AUPR is considered effective because it provides insights into the trade-off between precision and recall, which can be more informative than AUROC in certain contexts [3]. 

In conclusion, the limitations of AUROC stem from its lack of sensitivity to threshold variations and its potential to misrepresent model performance in non-binary situations. Alternatives like AUTC and AUPR are suggested to enhance the evaluation of detection algorithms, thereby better capturing their effectiveness in practical applications.

1. [1]:  https://ar5iv.org/html/2306.14658, [2306.14658] Beyond AUROC & co. for evaluating out-of-distribution detection performance
2. [2]:  https://ar5iv.org/html/2411.11148, No Title
3. [3]:  https://ar5iv.org/html/2205.04615, [2205.04615] Discriminating abilities of threshold-free evaluation metrics in link prediction
4. [4]:  https://ar5iv.org/html/2402.13213, No Title
5. [5]:  https://ar5iv.org/html/2305.04754, [2305.04754] Is AUC the best measure for practical comparison of anomaly detectors?
---
1. [1]:  Passage ID 1: AI, it is important to examine whether the basis for comparing OOD detection methods is consistent with practical needs. In this work, we take a closer look at the go-to metrics for evaluating OOD detection, and question the approach of exclusively reducing OOD detection to a binary classification task with little consideration for the detection threshold. We illustrate the limitations of current metrics (AUROC & its friends) and propose a new metric - Area Under the Threshold Curve (AUTC), which explicitly penalizes poor separation between ID and OOD samples. Scripts and data are available at https://github.com/glhr/beyond-auroc1 IntroductionWhen deployed out in the wild, computer vision systems may be faced with image content which they simply are not equipped to handle. For instance, a model trained to recognize certain types of skin lesions, once deployed in clinical practice, may encounter images with a different kind of skin condition, or images with no lesions at allÂ [20,
2. [2]:  Passage ID 2: 4. We leverage the implementations of various methods in TALENT toolbox [32] for training and comparisons.4.1.4 MetricsSince most of the tasks in our analysis focus on binary classification, we primarily use the Area Under the Receiver Operating Characteristic curve (AUROC) to evaluate performance. AUROC provides a robust measure of the modelâ€™s ability to differentiate between the two classes within the dataset. For the two multi-class datasets, Volkert (VO) and MNIST (MN), we instead use accuracy on the test set as the performance metric. This approach allows us to appropriately assess the modelâ€™s effectiveness across different types of classification tasks.4.1.5 BaselinesFollowing the previous paradigm [43, 51], we conduct a comprehensive comparison of our model against a range of established techniques, including traditional algorithms like logistic regression and random forests, as well as advanced boosting frameworks such as XGBoost, LightGBM, and CatBoost.
3. [3]:  Passage ID 3: popular metrics in early studies, while their effectiveness is recently under debate. At the same time, the area under the precision-recall curve (AUPR) becomes increasingly popular, especially in biological studies. Based on a toy model with tunable noise and predictability, we propose a method to measure the discriminating abilities of any given metric. We apply this method to the above three threshold-free metrics, showing that AUC and AUPR are remarkably more discriminating than BP, and AUC is slightly more discriminating than AUPR. The result suggests that it is better to simultaneously use AUC and AUPR in evaluating link prediction algorithms, at the same time, it warns us that the evaluation based only on BP may be unauthentic. This article provides a starting point towards a comprehensive picture about effectiveness of evaluation metrics for link prediction and other classification problems.  Comments:27 pages, 5 figures, 1 tableSubjects:Data Analysis, Statistics and
4. [4]:  Passage ID 4: be able to discriminate between correct and incorrect answers better than random chance.Our primary success metric was the Area Under the Receiver Operating Characteristic curve (AUROC)Â (Bradley, 1997). The AUROC of a binary classifier ranges from 0% to 100%, where 0% corresponds to getting every prediction wrong, 50% is random chance, and 100% is perfect classification. AUROC is also equivalent to the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. Conveniently, AUROC is threshold independent: it captures the modelâ€™s performance across the entire range of possible thresholds.Our code can be found at https://github.com/bplaut/softmax-probs-predict-llm-correctness.1.2 Our resultsTable 1: Main AUROC results. AUROC and Q&A values are percentages, averaged over ten data points (five datasets and two phrasings). The p<10âˆ’4ð‘superscript104p<10^{-4} columns indicate how many data points resulted in a p-value less than
5. [5]:  Passage ID 5: estimates (Latecki etÂ al., 2007), histogramâ€“based models (PevnÃ½, 2016) or neural networks (Schlegl etÂ al., 2017). Several comparative studies exist â€“ e.g. (Goldstein and Uchida, 2016; Pimentel etÂ al., 2014; Lazarevic etÂ al., 2003; Chandola etÂ al., 2009) or (Markou and Singh, 2003) â€“ in which the authors present an overview of existing anomaly detection methods, sometimes with a direct comparison on benchmark datasets. However, the authors never ask the question if the way the methods are compared to each other is appropriate for the anomaly detection setting. The area under the receiver operating characteristic curve (ROC AUC or just AUC in this text) seems to be the gold-standard of the field, as it allows us to compare models by a single number. In (Vanderlooy and HÃ¼llermeier, 2008), the authors compare â€softâ€ variants of AUC in which the contribution of a sample to the total value of AUC is weighted by the degree in which it is labeled correctly or incorrectly. They demonstrate that