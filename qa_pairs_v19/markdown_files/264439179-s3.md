# A Survey on Detection of LLMs-Generated Content

## Question

What are the limitations of using AUROC as a metric for evaluating detection algorithms, and what alternative metrics are suggested?

## URLs

1. https://ar5iv.org/html/2408.10193. [2408.10193] Area under the ROC Curve has the Most Consistent Evaluation for Binary Classification
2. https://ar5iv.org/html/2306.14658. [2306.14658] Beyond AUROC & co. for evaluating out-of-distribution detection performance
3. https://ar5iv.org/html/2409.20078. [2409.20078] Quantifying discriminability of evaluation metrics in link prediction for real networks
4. https://ar5iv.org/html/2008.02577. [2008.02577] A critical analysis of metrics used for measuring progress in artificial intelligence
5. https://ar5iv.org/html/2402.08893. [2402.08893] Inconsistency among evaluation metrics in link prediction
6. https://ar5iv.org/html/2205.04615. [2205.04615] Discriminating abilities of threshold-free evaluation metrics in link prediction
7. https://ar5iv.org/html/2010.16253. [2010.16253] Limitations of ROC on Imbalanced Data: Evaluation of LVAD Mortality Risk Scores
8. https://ar5iv.org/html/2401.13391. [2401.13391] Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely on between-group metrics
9. https://ar5iv.org/html/2305.04754. [2305.04754] Is AUC the best measure for practical comparison of anomaly detectors?

## Answer (I don't know?: âœ—)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable