# Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems

## Question

What is the training objective of Auto-regressive Modelling (AM), and what are its limitations and potential solutions?

## URLs

1. https://ar5iv.org/html/2410.18962. [2410.18962] Where Am I and What Will I See: An Auto-Regressive Model for Spatial Localization and View Prediction
2. https://ar5iv.org/html/2409.00046. [2409.00046] Rethinking Molecular Design: Integrating Latent Variable and Auto-Regressive Models for Goal Directed Generation
3. https://ar5iv.org/html/2410.05711. [2410.05711] Diffusion Auto-regressive Transformer for Effective Self-supervised Time Series Forecasting
4. https://ar5iv.org/html/2408.14461. [2408.14461] A domain decomposition-based autoregressive deep learning model for unsteady and nonlinear partial differential equations
5. https://ar5iv.org/html/2309.13638. [2309.13638] Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve
6. https://ar5iv.org/html/2407.17274. [2407.17274] Revolutionizing Text-to-Image Retrieval as Autoregressive Token-to-Voken Generation
7. https://ar5iv.org/html/2207.14255. [2207.14255] Efficient Training of Language Models to Fill in the Middle
8. https://ar5iv.org/html/2411.16585. [2411.16585] MarketGPT: Developing a Pre-trained transformer (GPT) for Modeling Financial Time Series
9. https://ar5iv.org/html/1912.12220. [1912.12220] On Network Traffic Forecasting using Autoregressive Models

## Answer (I don't know?: ✗)

The training objective of Auto-regressive Modelling (AM) is to predict each token in a text sequence based on all previously seen tokens. This can be mathematically expressed as a joint negative log-likelihood function, formulated as ℒ_A​M = -∑_{t=1}^{T} log p(x_t | X_{<t-1}) [1] [2]. In practical applications, AM is typically employed in modern language models that utilize architectures like GPT-2 and DialoGPT [1] [2].

However, AM has certain limitations, particularly in the context of recommendation systems where user behavior may not strictly follow the sequential patterns assumed by the AM model. According to Yuan et al. (2020a), the inherent sequential dependency in user preferences can be weak, meaning that strict adherence to previous tokens may not yield optimal recommendations [3] [4]. This suggests that while the AM objective is useful, it can miss important patterns by relying too heavily on prior information alone.

To address these limitations, various strategies have been introduced. One approach outlined by Yuan et al. (2020b) involves modifying the AM framework to incorporate Masked Language Modeling (MLM) principles. This method entails randomly masking a percentage of past user interactions and training the model to predict these masked tokens, which allows for capturing more generalized patterns of user behavior that are not strictly sequential [3] [4]. 

Additionally, research has shown that auto-regressive learning tasks can be adapted to various data types beyond traditional sequences. For instance, Geng et al. (2022b) and Zhao (2022) adapted AM to model paths in knowledge graphs and proposed new tasks such as Rearrange Sequence Prediction. These tasks aim to learn from the entire history of user interactions while accounting for potential rearrangements in the data, which can further improve recommendation accuracy [3] [4].

In summary, while Auto-regressive Modelling serves as a foundational technique for predicting sequences in NLP, its limitations particularly highlight the need for adaptations like the integration of MLM and other novel strategies that aim to capture non-linear and non-sequential user behaviors effectively.

1. [1]:  https://ar5iv.org/html/2302.03735, No Title
2. [2]:  https://ar5iv.org/html/2302.03735, No Title
3. [3]:  https://ar5iv.org/html/2302.03735, No Title
4. [4]:  https://ar5iv.org/html/2302.03735, No Title
5. [5]:  https://ar5iv.org/html/2307.06435, No Title
---
1. [1]:  Passage ID 1: datasets have led many language learning objectives to adopt self-supervised labels, converting them to classic probabilistic density estimation problems. Among language modelling objectives, autoregressive, reconstruction, and auxiliary are three categories commonly used (Liu et al., 2023b). Here, we only introduce several language modelling objectives used for RSs.Partial/ Auto-regressive Modelling (P/AM) Given a text sequence 𝐗1:T=[x1,x2,⋯​xT]subscript𝐗:1𝑇subscript𝑥1subscript𝑥2⋯subscript𝑥𝑇\mathbf{X}_{1:T}=[x_{1},x_{2},\cdots x_{T}], the training objective of AM can be summarized as a joint negative log-likelihood of each variable given all previous variables:ℒA​M=−∑t=1Tl​o​g​p​(xt|𝐗<t−1)subscriptℒ𝐴𝑀subscriptsuperscript𝑇𝑡1𝑙𝑜𝑔𝑝conditionalsubscript𝑥𝑡subscript𝐗absent𝑡1\mathcal{L}_{AM}=-\,\sum^{T}_{t=1}log\,p(x_{t}|\mathbf{X}_{<t-1})(1)Modern LMRS typically utilize popular pre-trained left-to-right LMs such as GPT-2 (Hada and Shevade, 2021) and DialoGPT (Wang et al.,
2. [2]:  Passage ID 2: datasets have led many language learning objectives to adopt self-supervised labels, converting them to classic probabilistic density estimation problems. Among language modelling objectives, autoregressive, reconstruction, and auxiliary are three categories commonly used (Liu et al., 2023b). Here, we only introduce several language modelling objectives used for RSs.Partial/ Auto-regressive Modelling (P/AM) Given a text sequence 𝐗1:T=[x1,x2,⋯​xT]subscript𝐗:1𝑇subscript𝑥1subscript𝑥2⋯subscript𝑥𝑇\mathbf{X}_{1:T}=[x_{1},x_{2},\cdots x_{T}], the training objective of AM can be summarized as a joint negative log-likelihood of each variable given all previous variables:ℒA​M=−∑t=1Tl​o​g​p​(xt|𝐗<t−1)subscriptℒ𝐴𝑀subscriptsuperscript𝑇𝑡1𝑙𝑜𝑔𝑝conditionalsubscript𝑥𝑡subscript𝐗absent𝑡1\mathcal{L}_{AM}=-\,\sum^{T}_{t=1}log\,p(x_{t}|\mathbf{X}_{<t-1})(1)Modern LMRS typically utilize popular pre-trained left-to-right LMs such as GPT-2 (Hada and Shevade, 2021) and DialoGPT (Wang et al.,
3. [3]:  Passage ID 3: (2022) treated the user’s clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction. However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly. Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training. Auto-regressive learning tasks can also be adapted to other types of data. Geng et al. (2022b) modeled a series of paths sampled from a knowledge graph in an auto-regressive manner for recommendation by generating the end node from the pre-trained model. Zhao (2022) proposed pre-training the Rearrange Sequence Prediction task to learn the sequence-level information of the user’s entire interaction history by predicting whether the user interaction history had been rearranged, which is similar to Permuted Language Modelling
4. [4]:  Passage ID 4: (2022) treated the user’s clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction. However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly. Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training. Auto-regressive learning tasks can also be adapted to other types of data. Geng et al. (2022b) modeled a series of paths sampled from a knowledge graph in an auto-regressive manner for recommendation by generating the end node from the pre-trained model. Zhao (2022) proposed pre-training the Rearrange Sequence Prediction task to learn the sequence-level information of the user’s entire interaction history by predicting whether the user interaction history had been rearranged, which is similar to Permuted Language Modelling
5. [5]:  Passage ID 5: Prompting LLMs to generate responses is possible at different training stages like pre-training, instruction-tuning, or alignment tuning.Full Language Modeling:An autoregressive language modeling objective where the model is asked to predict future tokens given the previous tokens, an example is shown in Figure 5. Prefix Language Modeling:A non-causal training objective, where a prefix is chosen randomly and only remaining target tokens are used to calculate the loss. An example is shown in Figure 5. Masked Language Modeling:In this training objective, tokens or spans (a sequence of tokens) are masked randomly and the model is asked to predict masked tokens given the past and future context. An example is shown in Figure 5. Unified Language Modeling:Unified language modeling [91] is a combination of causal, non-causal, and masked language training objectives. Here in masked language modeling, the attention is not bidirectional but unidirectional, attending either