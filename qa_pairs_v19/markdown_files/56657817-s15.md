# Analysis Methods in Neural Language Processing: A Survey

## Question

What are some methods for generating adversarial examples in NLP without needing access to model parameters?

## URLs

1. https://ar5iv.org/html/2203.06414
2. https://ar5iv.org/html/2003.10388
3. https://ar5iv.org/html/2401.12461
4. https://ar5iv.org/html/2404.05159
5. https://ar5iv.org/html/2310.01452
6. https://ar5iv.org/html/2404.02660
7. https://ar5iv.org/html/2405.20778
8. https://ar5iv.org/html/1910.07517
9. https://ar5iv.org/html/2308.00319
10. https://ar5iv.org/html/2103.00676

## Answer

## I don't know?: False

Adversarial examples in natural language processing (NLP) can be generated using several methods that do not require access to model parameters, commonly referred to as black-box attacks. These attacks work by observing the input-output behavior of the model rather than directly manipulating its internal mechanisms. Here are some notable techniques for generating such adversarial examples:

1. **Substituting Words**: One common approach is to replace words in the input text with synonyms or other contextually similar words that do not significantly alter the original meaning. This method carefully preserves the semantic integrity of the original sentence while eliciting different outputs from the model, exploiting its vulnerabilities without needing to know its inner workings.

2. **Character-level Distortions**: This technique involves introducing character errors such as typos or visually similar characters (e.g., replacing 'a' with '@') into the text. These small perturbations are often imperceptible to human readers but can confuse models, leading to misclassifications or inaccurate outputs [1].

3. **Word Insertion and Deletion**: Another strategy is to insert or delete words strategically. By adding irrelevant words or removing crucial ones, the adversarial examples can trick the model into making incorrect predictions. This method relies on the model's sensitivity to minor changes in input structure [1][3].

4. **Utilizing Model Responses**: Even without internal access, adversarial examples can be fine-tuned based on the model's responses to altered inputs. By probing the model with various inputs and iterating on the results, attackers can identify effective perturbations that maximize the intended discrepancy without knowing the model parameters [2][3].

5. **Gradient Estimation**: Although black-box attacks do not have direct access to model parameters, some methods use techniques like querying the model multiple times to estimate gradients indirectly. By examining changes in output related to small alterations in input, these methods craft adversarial examples that can exploit local weaknesses in the model [2][4].

Overall, these methods highlight the creativity and sophistication involved in generating adversarial examples without requiring direct access to model parameters. As the field of NLP continues to evolve, the study of adversarial attacks remains critical for enhancing the robustness and security of NLP models.

[1]: https://ar5iv.org/html/2310.01452, [2310.01452] Fooling the Textual Fooler via Randomizing Latent Representations
[2]: https://ar5iv.org/html/1812.08951, No Title
[3]: https://ar5iv.org/html/2103.00676, [2103.00676] Token-Modification Adversarial Attacks for Natural Language Processing: A Survey
[4]: https://ar5iv.org/html/2103.00676, [2103.00676] Token-Modification Adversarial Attacks for Natural Language Processing: A Survey
[5]: https://ar5iv.org/html/2103.00676, [2103.00676] Token-Modification Adversarial Attacks for Natural Language Processing: A Survey

[1]: Passage ID 1: state-of-the-art results in several NLP tasks (Devlin et al., 2019) using models such as Recurrent Neural Networks, Transformers, and Pretrained Language Models (PrLMs). However, several works (Iyyer et al., 2018) also reveal that deep NLP models can be fooled by the creation of adversarial examples (Jin et al., 2020). Adversarial examples are synthetically perturbed inputs that are optimized to increase the errors between the predictions of the model and the true labels while being imperceptible to human evaluators (Jin et al., 2020; Iyyer et al., 2018). These developments have sparked concerns about the security and robustness of deep neural networks deployed in NLP applications.To generate adversarial examples, adversarial attack methods manipulate different aspects of the input sentence, from introducing character errors such as typos or visually similar characters (Gao et al., 2018a; Eger et al., 2019) and replacing words without significantly changing the original semantic in
[2]: Passage ID 2: problem, as this requires computing gradients with respect to a discrete input.In the following, we review methods for handling these difficulties according to several criteria:the adversary’s knowledge,the specificity of the attack,the linguistic unit being modified,and the task on which the attacked model was trained.141414These criteria are partly taken from Yuan et al. (2017), where a more elaborate taxonomy is laid out. At present, though, the work on adversarial examples in NLP is more limited thanin computer vision, so our criteria will suffice.Table SM3(in the supplementary materials) categorizes work on adversarial examples in NLP according to these criteria.5.1 Adversary’s knowledgeAdversarial examples can be generated using access to model parameters, also known as white-box attacks, or without such access, with black-box attacks (Papernot et al., 2016a, 2017; Narodytska and Kasiviswanathan, 2017; Liu et al., 2017).White-box attacks are difficult to adapt
[3]: Passage ID 3: learning models to manipulations by an adversary through an adversarial attack [1]. While the majority of adversarial machine learning research has focused on image processing, there has been rapidly increasing interest in the natural language processing (NLP) domain, motivated by the unique challenges and opportunities presented by textual data. In this work, we survey the recent advancements in adversarial attacks within the NLP domain.An adversarial example is a carefully crafted instance created by introducing a perturbation to an original example. This perturbation is designed in such a way that the attacked victim model processes the original example as expected but fails on the adversarial example in a manner predetermined by the attacker. To illustrate this concept, [2] have aimed to perturb a document reading “Save me it’s over 100F” to introduce a specific token “22C” into the output of an English-to-German translation model (see Figure 1). Their attack algorithm achieved
[4]: Passage ID 4: learning models to manipulations by an adversary through an adversarial attack [1]. While the majority of adversarial machine learning research has focused on image processing, there has been rapidly increasing interest in the natural language processing (NLP) domain, motivated by the unique challenges and opportunities presented by textual data. In this work, we survey the recent advancements in adversarial attacks within the NLP domain.An adversarial example is a carefully crafted instance created by introducing a perturbation to an original example. This perturbation is designed in such a way that the attacked victim model processes the original example as expected but fails on the adversarial example in a manner predetermined by the attacker. To illustrate this concept, [2] have aimed to perturb a document reading “Save me it’s over 100F” to introduce a specific token “22C” into the output of an English-to-German translation model (see Figure 1). Their attack algorithm achieved
[5]: Passage ID 5: learning models to manipulations by an adversary through an adversarial attack [1]. While the majority of adversarial machine learning research has focused on image processing, there has been rapidly increasing interest in the natural language processing (NLP) domain, motivated by the unique challenges and opportunities presented by textual data. In this work, we survey the recent advancements in adversarial attacks within the NLP domain.An adversarial example is a carefully crafted instance created by introducing a perturbation to an original example. This perturbation is designed in such a way that the attacked victim model processes the original example as expected but fails on the adversarial example in a manner predetermined by the attacker. To illustrate this concept, [2] have aimed to perturb a document reading “Save me it’s over 100F” to introduce a specific token “22C” into the output of an English-to-German translation model (see Figure 1). Their attack algorithm achieved