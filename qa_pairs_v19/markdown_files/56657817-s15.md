# Analysis Methods in Neural Language Processing: A Survey

## Question

What are some methods for generating adversarial examples in NLP without needing access to model parameters?

## URLs

1. https://ar5iv.org/html/2203.06414. [2203.06414] A Survey of Adversarial Defences and Robustness in NLP
2. https://ar5iv.org/html/2003.10388. [2003.10388] Generating Natural Language Adversarial Examples on a Large Scale with Generative Models
3. https://ar5iv.org/html/2401.12461. [2401.12461] Fast Adversarial Training against Textual Adversarial Attacks
4. https://ar5iv.org/html/2404.05159. [2404.05159] Semantic Stealth: Adversarial Text Attacks on NLP Using Several Methods
5. https://ar5iv.org/html/2310.01452. [2310.01452] Fooling the Textual Fooler via Randomizing Latent Representations
6. https://ar5iv.org/html/2404.02660. [2404.02660] Adversarial Attacks and Dimensionality in Text Classifiers
7. https://ar5iv.org/html/2405.20778. [2405.20778] Improved Generation of Adversarial Examples Against Safety-aligned LLMs
8. https://ar5iv.org/html/1910.07517. [1910.07517] Adversarial Examples for Models of Code
9. https://ar5iv.org/html/2308.00319. [2308.00319] LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack
10. https://ar5iv.org/html/2103.00676. [2103.00676] Token-Modification Adversarial Attacks for Natural Language Processing: A Survey

## Answer (I don't know?: ✗)

Several methods for generating adversarial examples in natural language processing (NLP) do not require access to the model parameters and are particularly effective in black-box settings. One prominent approach is through adversarial word-level perturbations. In this method, the adversary can query the victim model multiple times to determine which words in the input text are most impactful in the model’s predictions, allowing them to effectively replace these words with synonyms or other alternatives that maintain similar meanings but can lead the model to make incorrect predictions [1][5].

In addition to word-level attacks, character-level adversarial attacks are another strategy employed to craft adversarial examples. These attacks pertain to perturbations made on the character level, including insertions, deletions, and swapping of characters within the input sequence. Although character-level attacks can be efficient, they can also be easily detected through simple mechanisms like spell-checkers, which can limit their effectiveness in certain scenarios [4].

Moreover, a lightweight and attack-agnostic defense mechanism, named AdvFooler, has been proposed that aims to complicate the process for adversaries generating black-box attacks. This defense works by randomizing the latent representations of the input at inference time, thereby making it harder for adversaries to predict how changes will affect the model's output without needing knowledge of the model architecture or parameters [5]. This illustrates a proactive approach to countering potential adversarial attacks in NLP.

Lastly, some adversaries utilize techniques such as adding natural and synthetic noise to the inputs as part of character-level attacks. Natural noise is derived from actual spelling mistakes, which can also serve to obfuscate the text in a manner that appears legitimate to human readers, thereby maintaining the imperceptibility of the perturbations [4]. 

In summary, adversarial examples in NLP can be generated through various methods that do not require access to model parameters, including word-level perturbations, character-level alterations, and the use of defenses like AdvFooler that disrupt the attack process without additional training costs. The landscape of adversarial attacks in NLP is diverse, reflecting the complexity and uniqueness of working with textual data, as well as the necessity of developing robust defenses against emerging threats in this field [2][3].

1. [1]:  https://ar5iv.org/html/2310.01452, [2310.01452] Fooling the Textual Fooler via Randomizing Latent Representations
2. [2]:  https://ar5iv.org/html/2103.00676, [2103.00676] Token-Modification Adversarial Attacks for Natural Language Processing: A Survey
3. [3]:  https://ar5iv.org/html/2203.06414, [2203.06414] A Survey of Adversarial Defences and Robustness in NLP
4. [4]:  https://ar5iv.org/html/2203.06414, [2203.06414] A Survey of Adversarial Defences and Robustness in NLP
5. [5]:  https://ar5iv.org/html/2310.01452, [2310.01452] Fooling the Textual Fooler via Randomizing Latent Representations
---
1. [1]:  Passage ID 1: state-of-the-art results in several NLP tasks (Devlin et al., 2019) using models such as Recurrent Neural Networks, Transformers, and Pretrained Language Models (PrLMs). However, several works (Iyyer et al., 2018) also reveal that deep NLP models can be fooled by the creation of adversarial examples (Jin et al., 2020). Adversarial examples are synthetically perturbed inputs that are optimized to increase the errors between the predictions of the model and the true labels while being imperceptible to human evaluators (Jin et al., 2020; Iyyer et al., 2018). These developments have sparked concerns about the security and robustness of deep neural networks deployed in NLP applications.To generate adversarial examples, adversarial attack methods manipulate different aspects of the input sentence, from introducing character errors such as typos or visually similar characters (Gao et al., 2018a; Eger et al., 2019) and replacing words without significantly changing the original semantic in
2. [2]:  Passage ID 2: learning models to manipulations by an adversary through an adversarial attack [1]. While the majority of adversarial machine learning research has focused on image processing, there has been rapidly increasing interest in the natural language processing (NLP) domain, motivated by the unique challenges and opportunities presented by textual data. In this work, we survey the recent advancements in adversarial attacks within the NLP domain.An adversarial example is a carefully crafted instance created by introducing a perturbation to an original example. This perturbation is designed in such a way that the attacked victim model processes the original example as expected but fails on the adversarial example in a manner predetermined by the attacker. To illustrate this concept, [2] have aimed to perturb a document reading “Save me it’s over 100F” to introduce a specific token “22C” into the output of an English-to-German translation model (see Figure 1). Their attack algorithm achieved
3. [3]:  Passage ID 3: generated for text data is determined by two factors, namely, the naturalness of the adversarial examples and the efficiency to generate these examples (Liet al., 2023). Some researchers have succeeded in detecting perturbations in text using simpler techniques like spell check and adversarial training (Pruthiet al., 2019), while others who used word-level attacks failed to efficiently generate adversarial examples due to the high-dimensional search space (Zanget al., 2019). Thus, efficiently generating adversarial attacks in NLP poses unique challenges. Despite the difficulty, stronger and imperceptible adversarial attacks have been proposed, which pose a significant threat to the security of deep neural networks (Boucher et al., 2022; Ballet et al., 2019). Consequently, several defense mechanisms have been proposed in recent years to counter adversarial attacks in NLP, and the considerable amount of work in adversarial defenses has provided good competition to the novel
4. [4]:  Passage ID 4: designed for NLP tasks. It is important to note that adversarial examples in computer vision cannot be directly applied to text as they are fundamentally different. Therefore, several attack methods that modify the text data while maintaining imperceptibility to humans have been proposed in literature. Typically, these methods alter the text data at the word, character, or sentence levels. The following section presents some of these attack methods in NLP.Character level adversarial attacksCharacter-level attacks perturb the input sequences at a character level. These operations include insertion, deletion, and swapping of characters in a given input sequence. Despite the fact, these attacks are quite effective, they can easily be detected with a spell-checker mechanism. One of the techniques used in character-level attacks is adding natural and synthetic noise to the inputs (Belinkov and Bisk, 2018). For natural noise authors collected natural spelling mistakes and used them to
5. [5]:  Passage ID 5: attacks, adversarial word-level perturbations are well-studied and effective attack strategies. Since these attacks work in black-box settings, they do not require access to the model architecture or model parameters and thus can be detrimental to existing NLP applications. To perform an attack, the adversary queries the victim model many times to determine the most important words in an input text and to replace these words with their corresponding synonyms. In this work, we propose a lightweight and attack-agnostic defense whose main goal is to perplex the process of generating an adversarial example in these query-based black-box attacks; that is to fool the textual fooler. This defense, named AdvFooler, works by randomizing the latent representation of the input at inference time. Different from existing defenses, AdvFooler does not necessitate additional computational overhead during training nor relies on assumptions about the potential adversarial perturbation set while having a