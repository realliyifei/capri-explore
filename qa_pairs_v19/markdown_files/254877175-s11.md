# A Survey of Deep Learning for Mathematical Reasoning

## Question

What strategies have been explored to enhance the selection of in-context examples for mathematical reasoning tasks?

## URLs

1. https://ar5iv.org/html/2411.18126. [2411.18126] Curriculum Demonstration Selection for In-Context Learning
2. https://ar5iv.org/html/2412.03966. [2412.03966] Demonstration Selection for In-Context Learning via Reinforcement Learning
3. https://ar5iv.org/html/2405.16265. [2405.16265] MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time
4. https://ar5iv.org/html/2308.00304. [2308.00304] Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models
5. https://ar5iv.org/html/2310.09881. [2310.09881] In-Context Learning with Iterative Demonstration Selection
6. https://ar5iv.org/html/2411.16454. [2411.16454] Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem Solving with Computational Graph-Based Retrieval
7. https://ar5iv.org/html/2407.05682. [2407.05682] Retrieved In-Context Principles from Previous Mistakes
8. https://ar5iv.org/html/2411.15645. [2411.15645] MC-NEST – Enhancing Mathematical Reasoning in Large Language Models with a Monte Carlo Nash Equilibrium Self-Refine Tree
9. https://ar5iv.org/html/2408.10839. [2408.10839] Benchmarking Large Language Models for Math Reasoning Tasks

## Answer (I don't know?: ✗)

Several strategies have been explored to enhance the selection of in-context examples for mathematical reasoning tasks, particularly through few-shot prompting techniques. Here are the key approaches discussed in the context:

1. **Semantic Similarity-Based Retrieval**: This method involves selecting examples based on their semantic similarity to the target problem. It aims to retrieve problems and solutions that share reasoning paths or structures similar to the new reasoning problem being addressed. The success of few-shot prompting heavily relies on how effectively these examples can guide the large language models (LLMs) by providing them with relevant contextual information [4].

2. **Utilization of Computational Graphs**: The reasoning paths for mathematical word problems can be represented as computational graphs. By identifying and using such graphs from previously solved problems, models can better mimic the problem-solving processes seen in analogous scenarios. This approach allows models to leverage learned reasoning paths to enhance their performance on new tasks [5].

3. **Critical Example Selection**: The choice of examples is vital to the effectiveness of few-shot prompting. Existing methods generally categorize the selection of examples into semantic similarity-based retrieval or through contrasting examples with different structures, which highlight diverse reasoning approaches. This variance can enrich the learning context provided to the models, thus improving their reasoning capabilities [4] [5].

4. **Response to Human Problem-Solving Strategies**: Humans often draw on known problems with similar reasoning paths when solving new problems. By tailoring the retrieval process to this human-like analogy-based approach, it facilitates a more aligned reasoning strategy in LLMs. Such analogical reasoning is crucial for enhancing the model's ability to tackle novel mathematical problems in a structured manner [5].

In summary, enhancing the selection of in-context examples for mathematical reasoning tasks involves a combination of semantic retrieval strategies, the use of computational graphs, careful selection of diverse processing examples, and emulating human problem-solving techniques. Each of these strategies aims to build a more robust foundation for models to engage in complex reasoning tasks effectively.

1. [1]:  https://ar5iv.org/html/2408.10839, [2408.10839] Benchmarking Large Language Models for Math Reasoning Tasks
2. [2]:  https://ar5iv.org/html/2408.10839, [2408.10839] Benchmarking Large Language Models for Math Reasoning Tasks
3. [3]:  https://ar5iv.org/html/2408.10839, [2408.10839] Benchmarking Large Language Models for Math Reasoning Tasks
4. [4]:  https://ar5iv.org/html/2411.16454, [2411.16454] Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem Solving with Computational Graph-Based Retrieval
5. [5]:  https://ar5iv.org/html/2411.16454, [2411.16454] Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem Solving with Computational Graph-Based Retrieval
---
1. [1]:  Passage ID 1: in different contexts.While previous research has often evaluated these methods on rather limited datasets (Qiao et al. 2023) or using similar methods (Luo et al. 2023), there remains a need for more comprehensive analysis. Addressing this research gap, our study presents an extensive benchmarking of seven advanced methods across five widely-used datasets. Additionally, whereas prior work has mainly focused on accuracy as the main performance metric, we also consider time and cost factors, for deploying these algorithms effectively.More specifically, to tackle mathematical reasoning with LLMs, we identify three core tasks:First, achieving high accuracy requires the model to correctly and consistently explain the reasoning path leading to the correct result. Second, robustness is demonstrated through the model’s ability to produce the correct result across multiple repetitive calls. Finally, efficient resource usage - encompassing time and API costs - is crucial for practical
2. [2]:  Passage ID 2: Overview of the mathematical reasoning methods evaluated in the benchmark, categorized into three groups: Prompt Engineering, Process Optimization, and External Engine. The primary procedure for each method is outlined. Symbols denote the presence of few-shot examples (\faListUl), the use of an external engine (\faGears), and the number of refinement iterations required (\faRepeat).Benchmarking DetailsThe goal of this benchmarking is to systematically evaluate different strategies for fitting and aligning large language models (LLMs) for mathematical reasoning. We perform a comparative analysis of these methods across five datasets, using both open-source and closed-source ground models, and evaluate performance across multiple dimensions. The following sections provide a detailed description of our benchmarking methodology.Datasets for Mathematical ReasoningTo assess the reasoning ability of the methods, we use several mathematical word problems. These tasks stem from five
3. [3]:  Passage ID 3: the problem of mathematical reasoning with LLMs from theoretical viewpoints.Lu et al. (2023) investigate the different deep learning methods used for mathematical reasoning,Ahn et al. (2024) give a thorough overview specifically covering LLMs for this task, andHuang and Chang (2023) analyse the different kinds of reasoning problems and the corresponding opportunities for LLMs.All surveys emphasize the missing generalizability and robustness, the challenges with complex questions, and the problem of hallucinations and trustworthiness.Setting the focus on the algebraic skills of LLMs, Yuan et al. (2023) find that GPT-3.5 and GPT-4 outperform the other foundation models. As key points, they mark the influence of tokenization, the importance of the model size, and the sensitivity of prompts, identifying different optimal prompts across the LLMs. However, they also emphasize that the algebraic ability of a model cannot be directly equated with the mathematical reasoning
4. [4]:  Passage ID 4: on the true computational graphs. We anticipate that more advanced methods will be developed in future work to construct high-quality training data without human labor.Figure 6: Some cases of the original and rewritten questions. The entity names, value of numbers and semantics are different after rewritting, while the computational graphs remain the same.5 Related WorkFew-shot Prompting for MWP Solving.Large Language Models have shown promising results in tackling math word problems Toshniwal et al. ; Yang et al. (2024); Yu et al. (2024a); Mirzadeh et al. (2024); Wei et al. (2022b). To enhance model performance on math word problems, few-shot prompting has become a widely adopted approach Wei et al. (2022b); Jiang et al. (2023); Melz (2023); Henkel et al. (2024). The choice of examples used in few-shot prompting is critical to its success. Existing methods for example selection generally fall into two categories: semantic similarity-based retrieval Huang et al. (2023);
5. [5]:  Passage ID 5: negatives serve as contrasting examples with different structures. 2.1 Overview of the Proposed FrameworkWhen solving a new reasoning problem, humans often draw upon known problems with similar reasoning paths and address them by analogy. In the context of math word problems, the reasoning path corresponds to its computational graph, as illustrated in Figure 1. Large language models (LLMs) are observed to fail to conduct genuine logical reasoning Mirzadeh et al. (2024) and exhibit strong token biases Li et al. (2024) when addressing reasoning tasks. Therefore, providing LLMs with the correct reasoning path from analogous problems can guide them to mimic the problem-solving process. This paper aims to develop a math word problem-solving system comprising a retriever and a generator. The retriever identifies problems and solutions with computational graphs similar to the query problem from a corpus, while the generator leverages these retrieved exemplars through in-context learning