# A Survey of Deep Learning for Mathematical Reasoning

## Question

What strategies have been explored to enhance the selection of in-context examples for mathematical reasoning tasks?

## URLs

1. https://ar5iv.org/html/2411.18126. [2411.18126] Curriculum Demonstration Selection for In-Context Learning
2. https://ar5iv.org/html/2412.03966. [2412.03966] Demonstration Selection for In-Context Learning via Reinforcement Learning
3. https://ar5iv.org/html/2405.16265. [2405.16265] MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time
4. https://ar5iv.org/html/2308.00304. [2308.00304] Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models
5. https://ar5iv.org/html/2310.09881. [2310.09881] In-Context Learning with Iterative Demonstration Selection
6. https://ar5iv.org/html/2411.16454. [2411.16454] Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem Solving with Computational Graph-Based Retrieval
7. https://ar5iv.org/html/2407.05682. [2407.05682] Retrieved In-Context Principles from Previous Mistakes
8. https://ar5iv.org/html/2411.15645. [2411.15645] MC-NEST – Enhancing Mathematical Reasoning in Large Language Models with a Monte Carlo Nash Equilibrium Self-Refine Tree
9. https://ar5iv.org/html/2408.10839. [2408.10839] Benchmarking Large Language Models for Math Reasoning Tasks

## Answer (I don't know?: ✗)

The context provided does not explicitly mention specific strategies for enhancing the selection of in-context examples for mathematical reasoning tasks. However, several related concepts around improving the reasoning ability of large language models (LLMs) in dealing with mathematical problems can be inferred.

1. **Utilization of Prompts**: One significant strategy is the use of prompt engineering techniques, including the Chain-of-Thought (CoT) and Program-of-Thought prompting algorithms. These methods have been shown to improve LLM performance on reasoning tasks, as they encourage the model to reason through a problem step-by-step rather than trying to arrive at an answer directly [4]. 

2. **Tool-Augmented Approaches**: Another approach employs tool-augmented LLMs, which combine programmatic assistance and natural language reasoning to enhance multi-step problem-solving capabilities. This technique allows models to leverage structured data manipulation alongside their reasoning abilities, potentially providing better in-context examples [4].

3. **In-Context Learning and Few-Shot Settings**: The optimization of reasoning tasks has also benefited from in-context learning, where LLMs can perform tasks in a few-shot manner without needing parameters to be modified. By utilizing the information inherent in their pretraining data, models can draw upon examples that may already set a context for the reasoning tasks they are tackling [2][3].

4. **Focus on Heuristic Search and Intermediate States**: There has been exploration into heuristic search methods designed to improve reasoning performance at an abstract level. By utilizing "thoughts" or intermediate natural language chunks, models can better manage complex mathematical reasoning tasks instead of focusing solely on token-level searches. This approach helps in refining the process of selecting relevant in-context examples by treating these "thoughts" as significant computational steps [5].

Though these strategies do not directly enumerate the enhancement of in-context examples' selection, they implicitly suggest methods to improve how models can engage with mathematical reasoning through better context management and reasoning techniques. However, without explicit details from the passages on this particular aspect, one can only speculate on the precise enhancements made in the selection process. Therefore, a comprehensive answer to the specific question on strategies for enhancing selection might not be fully attainable based solely on the provided context.

1. [1]:  https://ar5iv.org/html/2408.12337, No Title
2. [2]:  https://ar5iv.org/html/2202.07206, No Title
3. [3]:  https://ar5iv.org/html/2406.06592, No Title
4. [4]:  https://ar5iv.org/html/2411.18915, No Title
5. [5]:  https://ar5iv.org/html/2310.00194, No Title
---
1. [1]:  Passage ID 1: multi-hop numerical reasoning over financial texts. We assess the performance of several smaller models that have been fine-tuned to generate programs that encode the required financial reasoning and calculations. Our findings demonstrate that these fine-tuned smaller models approach the performance of the teacher model.To provide a granular analysis of model performance, we propose an approach to investigate the specific student model capabilities that are enhanced by fine-tuning. Our empirical analysis indicates that fine-tuning refines the student models ability to express and apply the required financial concepts along with adapting the entity extraction for the specific data format. In addition, we hypothesize and demonstrate that comparable financial reasoning capability can be induced using relatively smaller datasets.1 IntroductionIn recent years, the development of large language models (LLMs) has achieved significant advances in natural language processing (NLP).
2. [2]:  Passage ID 2: observations suggest that any evaluation of reasoning that does not take the pretraining data into account is difficult to interpret, and that we need to revisit evaluation of language models with respect to their pretraining data.2 Background and MethodologyReasoning ability has long been considered as a proxy for intelligence (Johnson-Laird, 2010). Thus, developing models with this skill has been also an essential goal of AI and natural language processing (NLP) (Bommasani et al., 2021).Recently, large language models have exhibited an ability to perform reasoning-related tasks in few-shot settings without requiring any modifications to their parameters through a method called in-context learning.Our goal is to evaluate this reasoning skill in-depth for numerical induction tasks.This section provides background information on in-context learning and introduces our method for measuring the performance gap of the models on numerical reasoning tasks based on differences in
3. [3]:  Passage ID 3: is structured as follows. We discuss related work on using LLMs to solve mathematical reasoning problems in Section 2. We describe our main method in Section 3. Our experimental setup including the task, model configuration, baselines and metrics are discussed in Section 4. We showcase our superior result also in Section 4.2 Related WorkImproving Mathematical reasoning ability of LLMs.Mathematical reasoning poses significant challenges for LLMs, and it is one of the key tasks for evaluating the reasoning ability of LLMs. With a huge amount of math problems in pretraining datasets, the pretrained LLMs (OpenAI, 2023; Gemini Team et al., 2024; Touvron et al., 2023) are able to solve simple problems, yet struggle with more complicated reasoning. To overcome that, the chain-of-thought (Wei et al., 2022b; Fu et al., 2023) type prompting algorithms were proposed. These techniques were effective in improving the performance of LLMs on reasoning tasks without modifying the model
4. [4]:  Passage ID 4: developments have popularized Large Language Models (LLMs) [1, 24, 3] as potent tools for tackling complex mathematical reasoning problems [18].Different strategies encompass an extensive range of techniques, including specialized models like [21, 31], Chain-of-Thought or Program-of-Thought [25, 4], or, tool-based compositional frameworks like [8, 14, 16, 27].Progress in the study of mathematical reasoning with structured and unstructured data has been measured thanks to various datasets and benchmarks, such as [5, 30, 17].Tool-augmented LLMs are particularly prevalent in those benchmarks as they combine program assistance, data manipulation, natural language reasoning, and many other tools.These frameworks leverage the increasing planning capabilities of LLMs [26, 2] to orchestrate tools, enabling multi-step problem solving.Figure 1: Example from FinQA [5] with questions over tabular and textual data.Recent studies focus on prompt engineering methods to get the best from
5. [5]:  Passage ID 5: multi-step reasoning, such as arithmetic (Dziri et al., 2023). Our approach is in large part motivated by the poor planning and reasoning performance exhibited by LLMs in these settings.Some recent approaches have employed various forms of heuristic search to improve performance in LLMs (Lu et al., 2021; Zhang et al., 2023), but these approaches have generally involved search at the level of individual tokens. This is in contrast to our approach, in which search is performed at the more abstract level of task states (described in natural language). This is similar to other recently proposed black-box approaches in which ‘thoughts’ – meaningful chunks of natural language – are utilized as intermediate computations to solve more complex problems. These approaches include scratchpads (Nye et al., 2021), chain-of-thought (Wei et al., 2022b), tree-of-thoughts (Yao et al., 2023), reflexion (Shinn et al., 2023), Society of Mind (Du et al., 2023), and Describe-Explain-Plan-Select (Wang