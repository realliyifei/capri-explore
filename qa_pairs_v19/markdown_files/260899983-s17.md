# Through the Lens of Core Competency: Survey on Evaluation of Large Language Models

## Question

How do datasets like RealToxicityPrompts and BAD help in understanding and evaluating harmful content in large language models (LLMs)?

## URLs

1. https://ar5iv.org/html/2405.10659. [2405.10659] Realistic Evaluation of Toxicity in Large Language Models
2. https://ar5iv.org/html/2410.12974. [2410.12974] BenchmarkCards: Large Language Model and Risk Reporting
3. https://ar5iv.org/html/2401.05561. [2401.05561] TrustLLM: Trustworthiness in Large Language Models
4. https://ar5iv.org/html/2410.15821. [2410.15821] The effect of fine-tuning on language model toxicity
5. https://ar5iv.org/html/2401.05778. [2401.05778] Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems
6. https://ar5iv.org/html/2403.14988. [2403.14988] Risk and Response in Large Language Models: Evaluating Key Threat Categories
7. https://ar5iv.org/html/2411.00027. [2411.00027] Personalization of Large Language Models: A Survey
8. https://ar5iv.org/html/2407.18418. [2407.18418] The Art of Refusal: A Survey of Abstention in Large Language Models
9. https://ar5iv.org/html/2009.11462. [2009.11462] Untitled Document
10. https://ar5iv.org/html/2308.05374. [2308.05374] Untitled Document

## Answer (I don't know?: ✓)

I don't know. The provided context does not include any information related to datasets like RealToxicityPrompts and BAD, nor does it discuss their role in understanding or evaluating harmful content in large language models (LLMs). The passages mainly focus on the use of LLMs in the biomedical domain and the challenges faced in evaluating their effectiveness in tasks related to biomedical natural language processing (BioNLP) [1] [2] [3]. Since there is no mention of harmful content or any related datasets in the provided context, it is insufficient to address the question.

1. [1]:  https://ar5iv.org/html/2305.16326, No Title
2. [2]:  https://ar5iv.org/html/2305.16326, No Title
3. [3]:  https://ar5iv.org/html/2305.16326, No Title
4. [4]:  https://ar5iv.org/html/2401.14559, No Title
5. [5]:  https://ar5iv.org/html/2305.12544, No Title
---
1. [1]:  Passage ID 1: by Qingyu Chen and 20 other authorsView PDFAbstract:The biomedical literature is rapidly expanding, posing a significant challenge for manual curation and knowledge discovery. Biomedical Natural Language Processing (BioNLP) has emerged as a powerful solution, enabling the automated extraction of information and knowledge from this extensive literature. Recent attention has been directed towards Large Language Models (LLMs) due to their impressive performance. However, there remains a critical gap in understanding the effectiveness of LLMs in BioNLP tasks and their broader implications for method development and downstream users. Currently, there is a lack of baseline performance data, benchmarks, and practical recommendations for using LLMs in the biomedical domain. To address this gap, we present a systematic evaluation of four representative LLMs: GPT-3.5 and GPT-4 (closed-source), LLaMA 2 (open-sourced), and PMC LLaMA (domain-specific) across 12 BioNLP datasets covering six
2. [2]:  Passage ID 2: by Qingyu Chen and 20 other authorsView PDFAbstract:The biomedical literature is rapidly expanding, posing a significant challenge for manual curation and knowledge discovery. Biomedical Natural Language Processing (BioNLP) has emerged as a powerful solution, enabling the automated extraction of information and knowledge from this extensive literature. Recent attention has been directed towards Large Language Models (LLMs) due to their impressive performance. However, there remains a critical gap in understanding the effectiveness of LLMs in BioNLP tasks and their broader implications for method development and downstream users. Currently, there is a lack of baseline performance data, benchmarks, and practical recommendations for using LLMs in the biomedical domain. To address this gap, we present a systematic evaluation of four representative LLMs: GPT-3.5 and GPT-4 (closed-source), LLaMA 2 (open-sourced), and PMC LLaMA (domain-specific) across 12 BioNLP datasets covering six
3. [3]:  Passage ID 3: by Qingyu Chen and 20 other authorsView PDFAbstract:The biomedical literature is rapidly expanding, posing a significant challenge for manual curation and knowledge discovery. Biomedical Natural Language Processing (BioNLP) has emerged as a powerful solution, enabling the automated extraction of information and knowledge from this extensive literature. Recent attention has been directed towards Large Language Models (LLMs) due to their impressive performance. However, there remains a critical gap in understanding the effectiveness of LLMs in BioNLP tasks and their broader implications for method development and downstream users. Currently, there is a lack of baseline performance data, benchmarks, and practical recommendations for using LLMs in the biomedical domain. To address this gap, we present a systematic evaluation of four representative LLMs: GPT-3.5 and GPT-4 (closed-source), LLaMA 2 (open-sourced), and PMC LLaMA (domain-specific) across 12 BioNLP datasets covering six
4. [4]:  Passage ID 4: common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations (Axelrod et al.,, 2011; Haddow and Koehn,, 2012).Recent advances in language modelling techniques in general and large-scale language models (LLMs) in particular have shown significant potential in improving a wide range of NLP tasks. Inspired by this idea, this research aims to answer two major Research Questions (RQ):RQ1In scenarios involving human interaction and continuous feedback, can we employ language models to improve the quality of adaptive MT at inference time? In the subsequent sections, I will be referring to this question as “Adaptive and Interactive MT”.RQ2In the absence of sufficient in-domain data, can we use pre-trained LLMs to improve the process of NMT domain adaptation? In the following sections, I will be referring to this question as “Domain-specific Text Generation for MT”.Figure 1.1:
5. [5]:  Passage ID 5: as work to date has primarily focused on English or other high-resource languages Mondal et al. (2022) but devoted less efforts towards minority languages. Additionally, the lack of human evaluation of NLP-based health systems has made it challenging to measure their effectiveness in the real world. Current automatic evaluation metrics do not necessarily speak to patient outcomes. Hence, human-centric studies must be conducted in evaluating the efficacy of NLP-powered tools in healthcare.Research Directions.1.Healthcare benchmark construction. Although the documentation of recent LLMs reports very high performance for various medical question answering benchmarks, or medical licensing texts, there are many other tasks in healthcare that lack the data required to achieve similarly good performance. Access to medical datasets is often limited because of privacy issues, and therefore other approaches may be required to compile such benchmarks. Synthetic datasets are one such