# Through the Lens of Core Competency: Survey on Evaluation of Large Language Models

## Question

How do datasets like RealToxicityPrompts and BAD help in understanding and evaluating harmful content in large language models (LLMs)?

## URLs

1. https://ar5iv.org/html/2405.10659. [2405.10659] Realistic Evaluation of Toxicity in Large Language Models
2. https://ar5iv.org/html/2410.12974. [2410.12974] BenchmarkCards: Large Language Model and Risk Reporting
3. https://ar5iv.org/html/2401.05561. [2401.05561] TrustLLM: Trustworthiness in Large Language Models
4. https://ar5iv.org/html/2410.15821. [2410.15821] The effect of fine-tuning on language model toxicity
5. https://ar5iv.org/html/2401.05778. [2401.05778] Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems
6. https://ar5iv.org/html/2403.14988. [2403.14988] Risk and Response in Large Language Models: Evaluating Key Threat Categories
7. https://ar5iv.org/html/2411.00027. [2411.00027] Personalization of Large Language Models: A Survey
8. https://ar5iv.org/html/2407.18418. [2407.18418] The Art of Refusal: A Survey of Abstention in Large Language Models
9. https://ar5iv.org/html/2009.11462. [2009.11462] Untitled Document
10. https://ar5iv.org/html/2308.05374. [2308.05374] Untitled Document

## Answer (I don't know?: ✗)

Datasets like RealToxicityPrompts and the recently introduced Thoroughly Engineered Toxicity (TET) dataset serve crucial roles in understanding and evaluating harmful content in large language models (LLMs). 

RealToxicityPrompts, developed by Gehman et al. (2020), consists of 100,000 sentences extracted from the OpenWebTextCorpus and is specifically designed for toxicity testing of LLMs. This dataset enables researchers to evaluate the ability of LLMs to generate toxic content based on specific prompts. The methodology involves splitting sentences to create prompts and then assessing the toxicity of the generated content, providing a framework for understanding how LLMs may respond to potentially harmful inquiries [1][3].

However, issues arise with the unnatural prompting methods used in existing toxicity datasets, which do not reflect real-world interactions. This is where the TET dataset contributes, comprising 2,546 prompts filtered from over 1 million genuine interactions with 25 different LLMs. The prompts were collected from diverse users on platforms like Vicuna demo and Chatbot Arena [2]. By utilizing realistically encountered prompts, researchers can obtain a more accurate picture of an LLM's performance in natural settings, enhancing the understanding of how such models might generate, or fail to generate, harmful content under typical user interactions [2].

Additionally, the purpose of these datasets goes beyond just testing toxicity; they are instrumental in evaluating LLM safety in broader terms. Evaluations often utilize various methodologies, including both human and automated assessments. For automated evaluation, for instance, the Perspective API is employed to assess toxic content as it processes millions of assessments daily [4]. This combination of datasets and assessment strategies allows researchers to not only detect instances of toxicity but also to develop strategies for toxicity mitigation.

In the context of harmful content classification, datasets also categorize the types of risks that LLMs could pose, covering aspects such as malicious uses, information hazards, discrimination, and misinformation [5]. This systematic classification informs strategies for LLM development, allowing for improved safety mechanisms tailored to specific risks associated with LLM outputs.

Overall, datasets like RealToxicityPrompts and TET are essential tools in the evaluation landscape of LLMs. They provide both a testing framework for toxicity and insight into the realistic interactions users have with these language models, ultimately guiding the design of safer and more responsible AI systems. Through their diverse methodologies and classifications, these datasets facilitate a comprehensive understanding of the risks involved with LLM-generated content.

1. [1]:  https://ar5iv.org/html/2405.10659, [2405.10659] Realistic Evaluation of Toxicity in Large Language Models
2. [2]:  https://ar5iv.org/html/2405.10659, [2405.10659] Realistic Evaluation of Toxicity in Large Language Models
3. [3]:  https://ar5iv.org/html/2405.10659, [2405.10659] Realistic Evaluation of Toxicity in Large Language Models
4. [4]:  https://ar5iv.org/html/2401.05561, [2401.05561] TrustLLM: Trustworthiness in Large Language Models
5. [5]:  https://ar5iv.org/html/2403.14988, [2403.14988] Risk and Response in Large Language Models: Evaluating Key Threat Categories
---
1. [1]:  Passage ID 1: which does not represent how people interact with chat models in real-life scenarios. For instance, RealToxicityPrompts Gehman et al. (2020) is a notable dataset designed for toxicity testing of Large Language Models, comprising 100000100000100000 sentences sourced from the OpenWebTextCorpus Gokaslan and Cohen (2019). In their study, the authors use RealToxicityPrompts to examine large language model chatbots by splitting every sentence at a specific point, using the leading portion as the input prompt, and evaluating whether the content generated by the model to fill up the rest of the sentence was toxic or not. Another noteworthy dataset is ToxiGen Hartvigsen et al. (2022), which consists of 274186274186274186 sentences generated by GPT-3 Brown et al. (2020). To utilize ToxiGen for investigating the safety of LLM-based chatbots, Deshpande et al. (2023) would pose a question or request, provide seven sentences in the dataset, and then prompt the model to answer in a style similar to
2. [2]:  Passage ID 2: the safety of LLM-based chatbots, Deshpande et al. (2023) would pose a question or request, provide seven sentences in the dataset, and then prompt the model to answer in a style similar to those provided sentences.To address the unrealistic nature of the current toxic dataset benchmark for large language models, we introduce the Thoroughly Engineered Toxicity (TET) dataset, comprising 2546 prompts filtered from over 1 million real-world interactions with 25 different Large Language Models compiled in the chat-lmsys-1M dataset Zheng et al. (2023). Collected from 210K unique IP addresses in the wild on the Vicuna demo and Chatbot Arena website111https://chat.lmsys.org, this dataset presents a repository of realistic prompts that people commonly use to engage with LLMs in real-world contexts.Besides the challenge of being distant from real-world usage, another well-known issue in evaluating LLMs involves their susceptibility to jailbreak prompts, where prompt engineering can
3. [3]:  Passage ID 3: 1,Linh Ngo Van3††thanks:   Corresponding author.,Thien Huu Nguyen2,41Oraichain Labs  2VinAI Research3Hanoi University of Science and Technology  4University of Oregontinh.ls@orai.io,  v.thienlt3@vinai.io,linhnv@soict.hust.edu.vn,  thien@cs.oregon.edu1 IntroductionLarge language models (LLMs), or any other system achieving such widespread popularity, necessitate a meticulous evaluation of safety to ensure their positive impact on the world. Numerous safety assessments Chang et al. (2023); Mukherjee et al. (2023); Wang et al. (2023b); Zhuo et al. (2023) have been conducted, each employing diverse strategies, safety definitions, and prompts.However, these evaluations and the datasets they employ have a significant drawback: they often rely on unnatural prompting methods, which does not represent how people interact with chat models in real-life scenarios. For instance, RealToxicityPrompts Gehman et al. (2020) is a notable dataset designed for toxicity testing of
4. [4]:  Passage ID 4: For LLMs, mitigating or preventing such content from being generated represents a paramount challenge. Research in this domain typically focuses on detection of toxicity [492, 493, 494], generation of toxic content (and related datasets) [249, 250], alongside strategies for toxicity mitigation [247, 69, 495].Presently, toxicity measurement is conducted through various methodologies, encompassing both human evaluation [496] and automated evaluation [488, 76, 497]. Of these, the Perspective API [488] stands as the predominant automated tool, processing over 500 million assessments daily [498]. In the TrustLLM benchmark, we employ the Perspective API¶¶¶https://www.perspectiveapi.com/ due to its extensive utilization in the field.In previous studies, various datasets [249, 250, 499] with many prompt templates have been employed to prompt LLMs to generate toxic content. However, it is challenging for these data to evoke toxicity [500] from the LLMs trained with reinforcement learning
5. [5]:  Passage ID 5: content. The dataset also features a three-level hierarchical taxonomy, that at the top level, they include:•Malicious Uses (243 prompts): Risks from users exploiting the LLM for harmful purposes.•Information Hazards (248 prompts): Risks from LLM exploiting private or safety-critical information.•Discrimination, Exclusion, Toxicity, Hateful, Offensive (176 prompts): Risks from the LLM reflecting harmful tendencies (e.g. unjust, toxic) present in the training data.•Human_Chatbot interaction Harms ( 117 prompts): Risks from LLMs engaging users in conversation, including handling mental health issues or developing emotional dependence.•Misinformation Harms (155 prompts): Risks from the LLM propagating false or misleading information.Furthermore, Wang et al. (2023) presented risky prompts to six different Large Language Models (LLMs) and classified their responses into six distinct ’action categories’:(0) cannot assist; (1) refute the opinion; (2) dual