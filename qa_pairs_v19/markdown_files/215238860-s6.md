# More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction

## Question

What strategies are currently employed to mitigate the noise issue in distant supervision for relation extraction?

## URLs

1. https://ar5iv.org/html/2106.11566. [2106.11566] SENT: Sentence-level Distant Relation Extraction via Negative Training
2. https://ar5iv.org/html/1903.03289. [1903.03289] Towards Time-Aware Distant Supervision for Relation Extraction
3. https://ar5iv.org/html/2305.11029. [2305.11029] Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction
4. https://ar5iv.org/html/2005.12565. [2005.12565] A Data-driven Approach for Noise Reduction in Distantly Supervised Biomedical Relation Extraction
5. https://ar5iv.org/html/2302.02078. [2302.02078] FGSI: Distant Supervision for Relation Extraction method based on Fine-Grained Semantic Information
6. https://ar5iv.org/html/2004.13786. [2004.13786] A Practical Framework for Relation Extraction with Noisy Labels Based on Doubly Transitional Loss
7. https://ar5iv.org/html/2105.08213. [2105.08213] Distantly Supervised Relation Extraction via Recursive Hierarchy-Interactive Attention and Entity-Order Perception
8. https://ar5iv.org/html/2009.14722. [2009.14722] RDSGAN: Rank-based Distant Supervision Relation Extraction with Generative Adversarial Framework
9. https://ar5iv.org/html/2108.09689. [2108.09689] Improving Distantly Supervised Relation Extraction with Self-Ensemble Noise Filtering
10. https://ar5iv.org/html/1911.06489. [1911.06489] Improving Distant Supervised Relation Extraction by Dynamic Neural Network

## Answer (I don't know?: ✗)

The issue of noise in distantly supervised relation extraction is a significant challenge, and several strategies have been developed to address it. 

One prominent approach is the use of multi-instance learning. This strategy involves examining all sentences that contain a specific entity pair to determine the relationship between them, which helps to mitigate the impact of noisy data. The models leverage different sentence selection mechanisms to prioritize sentences that are more relevant to the entities involved [1]. Researchers such as Riedel et al. (2010) and Hoffmann et al. (2011) have successfully applied this method to minimize the distortion caused by erroneous samples by aggregating information from multiple instances [1].

Another approach entails implementing advanced filtering mechanisms to reduce noise during the model training phase. Nayak et al. propose a self-ensemble filtering mechanism, which systematically filters out noisy samples from training datasets, thereby enhancing the robustness of the models and boosting their F1 scores [2]. This self-ensemble strategy underlines the need for improving the collection and processing of distantly supervised datasets, which often contain a significant amount of irrelevant information that can negatively influence model performance [2][4]. 

Furthermore, attention mechanisms have been employed to refine how models process input data. Various state-of-the-art methods apply neural networks focused on denoising operations, utilizing attention mechanisms to calculate specific attention weights over instances. However, a limitation of soft attention mechanisms is that they can still assign non-zero weights to noisy instances, failing to exclude them effectively. To counter this, some researchers advocate for "hard decisions," where decisively incorrect labels are removed from the training data, although this can incur the risk of losing potentially valuable instances [3]. 

The integration of knowledge about relation triples' direction in the model architecture further exemplifies an innovative approach. By enriching relation classification models, researchers can enhance the focus on relevant relational learning while simultaneously reducing noise, thus achieving state-of-the-art performance without the need for joint learning with knowledge graph completion [5]. 

In summary, the current strategies to mitigate noise in distantly supervised relation extraction include multi-instance learning techniques, robust filtering mechanisms, and the application of advanced attention models, all of which strive to improve data quality and model performance despite the inherent challenges posed by noisy datasets [1][2][3][5].

1. [1]:  https://ar5iv.org/html/2108.09689, [2108.09689] Improving Distantly Supervised Relation Extraction with Self-Ensemble Noise Filtering
2. [2]:  https://ar5iv.org/html/2108.09689, [2108.09689] Improving Distantly Supervised Relation Extraction with Self-Ensemble Noise Filtering
3. [3]:  https://ar5iv.org/html/2009.14722, [2009.14722] RDSGAN: Rank-based Distant Supervision Relation Extraction with Generative Adversarial Framework
4. [4]:  https://ar5iv.org/html/2108.09689, [2108.09689] Improving Distantly Supervised Relation Extraction with Self-Ensemble Noise Filtering
5. [5]:  https://ar5iv.org/html/2005.12565, [2005.12565] A Data-driven Approach for Noise Reduction in Distantly Supervised Biomedical Relation Extraction
---
1. [1]:  Passage ID 1: Nayak et al. (2021): (i) Pipeline approaches Zeng et al. (2014, 2015); Jat et al. (2017); Nayak and Ng (2019) (ii) Joint extraction approaches Takanobu et al. (2019); Nayak and Ng (2020). Most of these models work with distantly supervised noisy datasets. Thus noise mitigation is an important dimension in this area of research. Multi-instance relation extraction is one of the popular methods for noise mitigation. Riedel et al. (2010), Hoffmann et al. (2011), Surdeanu et al. (2012), Lin et al. (2016), Yaghoobzadeh et al. (2017), Vashishth et al. (2018), Wu et al. (2019), and Ye and Ling (2019) used this multi-instance learning concept in their proposed relation extraction models. For each entity pair, they used all the sentences that contain these two entities to find the relation between them. Their goal was to reduce the effect of noisy samples using this multi-instance setting. They used different types of sentence selection mechanisms to give importance to the sentences that contain
2. [2]:  Passage ID 2: of relevant relation-specific information in the sentence. As such, distantly supervised training data contains much noise which adversely affects the performance of the models. In this paper, we propose a self-ensemble filtering mechanism to filter out the noisy samples during the training process. We evaluate our proposed framework on the New York Times dataset which is obtained via distant supervision. Our experiments with multiple state-of-the-art neural relation extraction models show that our proposed filtering mechanism improves the robustness of the models and increases their F1 scores.1 IntroductionThe task of relation extraction is about finding relation or no relation between two entities. This is an important task to fill the gaps of existing knowledge bases (KB). Open information extraction (OpenIE) Banko et al. (2007) is one way of extracting relations from text. They consider the verb in a sentence as the relation and then find the noun phrases located to the left
3. [3]:  Passage ID 3: to extract the relations between entity pairs. One popular way to handle this task is distant supervision Mintz et al. (2009) which automatically generates numerous labeled data via aligning text with the existing knowledge bases. However, generated training data contains numerous noisy samples due to the strong assumption. To tackle this issue, most recent state-of-the-art methods perform neural networks Du et al. (2018); Li et al. (2019); Beltagy et al. (2019) on denoising operation with distant supervision. Various attention mechanisms Lin et al. (2016); Han et al. (2018); Gao et al. (2019) are proposed for calculating precise attention weights over instances, but soft attention mechanism usually assigns non-zero weights to noisy instances, which does not eliminate noisy data. Qin et al. (2018a, b); Ma et al. (2019) argue that wrongly-labeled instances must be treated with hard decision by removing false positive instances from the positive set, though hard decision may cause loss
4. [4]:  Passage ID 4: "dark"); } else { localStorage.setItem("ar5iv_theme", "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Improving Distantly Supervised Relation Extractionwith Self-Ensemble Noise FilteringTapas Nayak IIT Kharagpur India &Navonil Majumder SUTD Singapore {tnk02.05,n.majumder.2009,soujanya.poria}@gmail.com&Soujanya Poria SUTD SingaporeAbstractDistantly supervised models are very popular for relation extraction since we can obtain a large amount of training data using the distant supervision method without human annotation. In distant supervision, a sentence is considered as a source of a tuple if the sentence contains both entities of the tuple. However, this condition is too permissive and does not guarantee the presence of relevant relation-specific information in the sentence. As such, distantly supervised training data contains much noise which adversely affects the performance of the models. In this paper, we
5. [5]:  Passage ID 5: relation extraction becomes increasingly expensive. Distant supervision offers a viable approach to combat this by quickly producing large amounts of labeled, but considerably noisy, data. We aim to reduce such noise by extending an entity-enriched relation classification BERT model to the problem of multiple instance learning, and defining a simple data encoding scheme that significantly reduces noise, reaching state-of-the-art performance for distantly-supervised biomedical relation extraction. Our approach further encodes knowledge about the direction of relation triples, allowing for increased focus on relation learning by reducing noise and alleviating the need for joint learning with knowledge graph completion.1 IntroductionRelation extraction (RE) remains an important natural language processing task for understanding the interaction between entities that appear in texts. In supervised settings (GuoDong et al., 2005; Zeng et al., 2014; Wang et al., 2016), obtaining