# A Survey of Deep Learning for Mathematical Reasoning

## Question

How do recent studies assess the effectiveness of subword tokenization techniques in handling numeracy within language models?

## URLs

1. https://ar5iv.org/html/2406.11687. [2406.11687] Tokenization Falling Short: hTe Cusre of Tkoeniaztion
2. https://ar5iv.org/html/2403.11130. [2403.11130] Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models
3. https://ar5iv.org/html/2310.06204. [2310.06204] Estimating Numbers without Regression
4. https://ar5iv.org/html/2310.11628. [2310.11628] Learn Your Tokens: Word-Pooled Tokenization for Language Modeling
5. https://ar5iv.org/html/2402.14903. [2402.14903] 1 Illustrating the dependence of frontier model arithmetic performance on tokenization. We show how using commas can enforce right-to-left (R2L) tokenization for the same addition problem. R2L tokenization leads to improved model performance on both GPT-3.5 and GPT-4 (March 2023 models), which we show is due to tokenization alignment between addends and answer through various controls and error analyses.
6. https://ar5iv.org/html/2410.14166. [2410.14166] LLM The Genius Paradox: A Linguistic and Math Expert’s Struggle with Simple Word-based Counting Problems
7. https://ar5iv.org/html/2402.18376. [2402.18376] Tokenization Is More Than Compression

## Answer (I don't know?: ✗)

Recent studies have rigorously evaluated the effectiveness of subword tokenization techniques in addressing numeracy issues within language models (LMs). The analysis indicates several key insights into how tokenization impacts numerical understanding.

One significant observation made in the research is that subword tokenization, which has become the standard approach, often results in inaccurate numerical representation. This inadequacy can hinder the LMs' ability to process and understand numerical inputs effectively [4]. For instance, the utilization of subword tokenization typically splits numbers in a way that does not respect their inherent numerical relationships, leading to a lack of precision in numerical tasks.

To address this shortcoming, some studies propose alternative tokenization techniques, such as tokenizing numbers on a log-scaled number line. This method has been shown to enhance approximate numeracy in LMs, providing a more intuitive representation that aligns with natural numerical progression [3]. The research highlights that this approach leads to results that are near or surpass state-of-the-art performance with only minimal adjustments to the model, without necessitating any architectural changes [4]. This contrasts with other studies that have suggested architectural modifications were needed to achieve better numeracy, showcasing that effective tokenization alone can suffice [5].

Moreover, the transition to using log-scaled tokenization is not merely an isolated innovation but is conceptually tied to other methodologies intended to improve both approximate and exact numeracy within LMs [3]. Previous strategies, such as exponent embeddings for number encoding, have also been cited as beneficial, indicating a growing consensus on the optimal approaches for numerical representation in NLP frameworks [3]. The broader implications of these findings suggest that adjustments to tokenization strategies could be integrated into standard fine-tuning processes, providing benefits even when users do not have direct access to modify the LM architecture [5].

In conclusion, the recent findings emphasize the necessity of reassessing subword tokenization techniques employed for numeracy tasks within LMs. The research advocates for innovative methods that respect numerical relationships, such as log-scaled tokenization, which not only improve performance metrics but also simplify the model's requirements for enhancement [4][5]. By making tokenization changes that align better with numerical properties, LMs can achieve superior outcomes in tasks that necessitate numerical reasoning, thereby highlighting a critical area for ongoing exploration and potential advancement in NLP methodologies.

1. [1]:  https://ar5iv.org/html/2410.14166, [2410.14166] LLM The Genius Paradox: A Linguistic and Math Expert’s Struggle with Simple Word-based Counting Problems
2. [2]:  https://ar5iv.org/html/2310.11628, [2310.11628] Learn Your Tokens: Word-Pooled Tokenization for Language Modeling
3. [3]:  https://ar5iv.org/html/2310.06204, [2310.06204] Estimating Numbers without Regression
4. [4]:  https://ar5iv.org/html/2310.06204, [2310.06204] Estimating Numbers without Regression
5. [5]:  https://ar5iv.org/html/2310.06204, [2310.06204] Estimating Numbers without Regression
---
1. [1]:  Passage ID 1: et al., 2023; Chen et al., 2024), etc. We also show inability of specialized math or coding LLMs to transfer advanced capabilities to much simpler tasks, calling for more attention and research in model capability acquisition during training and comprehensive capability evaluation during benchmarking. Lastly, we find effectiveness of reasoning strategies to help elicit knowledge and problem-solving capabilities from LLMs, highlighting importance of cultivating consciousness of reasoning during model pretraining.2 BackgroundWe introduce related work in Section A.1.2.1 TokenizationWord-based tokenization algorithms used in earlier non-transformer models such as Word2Vec (Mikolov, 2013), FastText (Bojanowski et al., 2017) and GloVe (Pennington et al., 2014), split texts into words (probably with some extra rules) and find numerical representation for each of them. Words that are unseen in the training corpus or ignored due to limited vocabulary size are typically represented
2. [2]:  Passage ID 2: the literacy of language models.In Proceedings of the 2021 Conference on Empirical Methods inNatural Language Processing, pages 6960–6967, Online and Punta Cana,Dominican Republic. Association for Computational Linguistics.Thawani et al. (2021b)Avijit Thawani, Jay Pujara, Filip Ilievski, and Pedro Szekely.2021b.Representingnumbers in NLP: a survey and a vision.In Proceedings of the 2021 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human LanguageTechnologies, pages 644–656, Online. Association for ComputationalLinguistics.Wallace et al. (2019)Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019.Do NLP models knownumbers? probing numeracy in embeddings.In Proceedings of the 2019 Conference on Empirical Methods inNatural Language Processing and the 9th International Joint Conference onNatural Language Processing (EMNLP-IJCNLP), pages 5307–5315, Hong Kong,China. Association for
3. [3]:  Passage ID 3: changes to the underlying language model.5 Related workWe restrict our analysis to the task of approximately decoding numbers in MNP setting, which requires different methods and metrics from the tasks that instead evaluate their exact arithmetic skills (Thawani et al., 2021b).The method we highlight in this paper i.e. change of vocabulary to tokenize numbers on a log-scaled number line, has been previously used in different settings.Others have shown the benefits of using such exponent embeddings as number encoders for language models, whether it be for the task of masked number prediction (Berg-Kirkpatrick andSpokoyny, 2020) or masked word prediction (Thawani et al., 2021a). Our work extends these results with further evidence of the representational power gained by simply tokenizing numbers on the number line.Our simple intervention to improve approximate numeracy in LMs is also related to other work Chen et al. (2022) which aims to improve exact numeracy of LMs
4. [4]:  Passage ID 4: tokenizing numbers on the number line.Our simple intervention to improve approximate numeracy in LMs is also related to other work Chen et al. (2022) which aims to improve exact numeracy of LMs without any architecture change.6 ConclusionSubword tokenization, the standard approach to representing numbers leads to inaccurate numerical understanding.In this work, we analyze number representation approaches that make notational (e.g. scientific vs. decimal), vocabulary (i.e. tokenizing on the number line), and architectural changes (i.e. regressing to the number).We find that tokenization on the number line achieves near or better than state-of-the-artresults while requiring minimal intervention to the language model.This is a negative insight against recent results in the community which suggest that language models must be architecturally modified to gain numeracy.It will allow language models to conveniently improve their numeracy, including cases where users may not
5. [5]:  Passage ID 5: which suggest that language models must be architecturally modified to gain numeracy.It will allow language models to conveniently improve their numeracy, including cases where users may not have access to the model’s architecture and are only provided a typical finetuning regime with small changes to the tokenizer’s vocabulary.Finally, we find similar trends in the challenging setting of numerical fact estimation for solving Fermi Problems – indicating that vocabulary-change is sufficient to represent approximate numbers effectively with minimal effort.7 AcknowledgementsThis work was funded by the Defense Advanced Research Projects Agency with award N660011924033.We would like to thank Peter Clark (AI2) for insightful discussions on the project, and theanonymous reviewers at EACL 2023 and Negative Insights workshop for helping us refine earlier versions of this paper.8 Ethics and LimitationsOur findings and recommendations may not apply beyond the English language