# A Survey of Deep Learning for Mathematical Reasoning

## Question

How do recent studies assess the effectiveness of subword tokenization techniques in handling numeracy within language models?

## URLs

1. https://ar5iv.org/html/2406.11687. [2406.11687] Tokenization Falling Short: hTe Cusre of Tkoeniaztion
2. https://ar5iv.org/html/2403.11130. [2403.11130] Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models
3. https://ar5iv.org/html/2310.06204. [2310.06204] Estimating Numbers without Regression
4. https://ar5iv.org/html/2310.11628. [2310.11628] Learn Your Tokens: Word-Pooled Tokenization for Language Modeling
5. https://ar5iv.org/html/2402.14903. [2402.14903] 1 Illustrating the dependence of frontier model arithmetic performance on tokenization. We show how using commas can enforce right-to-left (R2L) tokenization for the same addition problem. R2L tokenization leads to improved model performance on both GPT-3.5 and GPT-4 (March 2023 models), which we show is due to tokenization alignment between addends and answer through various controls and error analyses.
6. https://ar5iv.org/html/2410.14166. [2410.14166] LLM The Genius Paradox: A Linguistic and Math Expert’s Struggle with Simple Word-based Counting Problems
7. https://ar5iv.org/html/2402.18376. [2402.18376] Tokenization Is More Than Compression

## Answer (I don't know?: ✗)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable