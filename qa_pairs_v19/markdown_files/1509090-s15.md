# Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

What challenges are associated with using annotated data sets in supervised methods for discovering multiword expressions?

## URLs

1. https://ar5iv.org/html/2405.08497. [2405.08497] Is Less More? Quality, Quantity and Context in Idiom Processing with Natural Language Models
2. https://ar5iv.org/html/2208.07832. [2208.07832] BERT(s) to Detect Multiword Expressions
3. https://ar5iv.org/html/2403.02009. [2403.02009] Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic?
4. https://ar5iv.org/html/2204.10050. [2204.10050] SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding
5. https://ar5iv.org/html/2411.02610. [2411.02610] Investigating Idiomaticity in Word Representations
6. https://ar5iv.org/html/2007.11381. [2007.11381] To Be or Not To Be a Verbal Multiword Expression: A Quest for Discriminating Features
7. https://ar5iv.org/html/2406.15175. [2406.15175] Enhancing Idiomatic Representation in Multiple Languages via an Adaptive Contrastive Triplet Loss
8. https://ar5iv.org/html/2410.14074. [2410.14074] Be My Donor. Transfer the NLP Datasets Between the Languages Using LLM
9. https://ar5iv.org/html/2405.02861. [2405.02861] Revisiting a Pain in the Neck: Semantic Phrase Processing Benchmark for Language Models

## Answer (I don't know?: ✓)

I don't know. The context provided does not contain specific information regarding the challenges associated with using annotated datasets in supervised methods for discovering multiword expressions in natural language processing (NLP). Although there are discussions about the significance of annotated data, the manual annotation process being time-intensive and expensive [1] [2], and the general challenges of collecting large amounts of annotations [5], these points do not directly address the specific challenges related to multiword expressions. Furthermore, the context lacks details on multiword expressions themselves and how they relate to the broader issues of annotation or whether any unique challenges arise in that area.

1. [1]:  https://ar5iv.org/html/2407.03895, No Title
2. [2]:  https://ar5iv.org/html/2407.03895, No Title
3. [3]:  https://ar5iv.org/html/2410.00427, No Title
4. [4]:  https://ar5iv.org/html/2411.06284, No Title
5. [5]:  https://ar5iv.org/html/2210.10109, No Title
---
1. [1]:  Passage ID 1: years showed significant advancements [98, 7, 19] in natural language processing (NLP): Large language models (LLMs) emerged [8], facilitating new methodologies by describing tasks in natural language without a strong formalism. Because resource-intensive LLMs are not always superior [112], smaller, supervised learning-based models are still highly relevant for specialized domains or use cases that require rapid inference or are constrained by hardware limitations (such as mobile devices or offline scenarios) [34].One of these domains is entity recognition [73]. Entity recognition  (ER) describes the task of assigning a label to a sequence of words (e.g. to extract a person, a date or any other predefined label). To apply supervised learning to ER, data must be annotated. The manual annotation process, in which humans annotate data points with these predefined labels, is time-intensive and expensive [106]. Its output is an annotated dataset, which is also called corpus (pl. corpora)
2. [2]:  Passage ID 2: years showed significant advancements [98, 7, 19] in natural language processing (NLP): Large language models (LLMs) emerged [8], facilitating new methodologies by describing tasks in natural language without a strong formalism. Because resource-intensive LLMs are not always superior [112], smaller, supervised learning-based models are still highly relevant for specialized domains or use cases that require rapid inference or are constrained by hardware limitations (such as mobile devices or offline scenarios) [34].One of these domains is entity recognition [73]. Entity recognition  (ER) describes the task of assigning a label to a sequence of words (e.g. to extract a person, a date or any other predefined label). To apply supervised learning to ER, data must be annotated. The manual annotation process, in which humans annotate data points with these predefined labels, is time-intensive and expensive [106]. Its output is an annotated dataset, which is also called corpus (pl. corpora)
3. [3]:  Passage ID 3: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
4. [4]:  Passage ID 4: answering.The integration of multimodal data has opened up new possibilities for AI applications. MLLMs can generate detailed descriptions of images, providing valuable assistance in fields like accessibility and content creation. These models can answer questions about images, demonstrating their ability to understand and reason about visual content. MLLMs also enable the creation of rich multimedia content, combining text, images, and audio to produce engaging and informative outputs.The field of NLP has undergone significant transformations over the past few decades, driven by advancements in computational power, the availability of large-scale datasets, and breakthroughs in machine learning algorithms. This journey can be broadly categorized into several key phases:1.Rule-based systems (1950s-1980s): Early NLP approaches relied heavily on hand-crafted rules and linguistic knowledge bases. While these systems could perform well in narrow domains, they struggled with the
5. [5]:  Passage ID 5: related topics and future directions.1 IntroductionThe majority of modern natural language processing (NLP) systems are based on data-driven machine learning models. The success of these models depends on the quality and quantity of the available target training data. While these models can obtain impressive performance if given enough supervision, it is usually expensive to collect large amounts of annotations, especially considering that the labeling process can be laborious and challenging for NLP tasks (§3.2). Active learning (AL), an approach that aims to achieve high accuracy with fewer training labels by allowing a model to choose the data to be annotated and used for learning, is a widely-studied approach to tackle this labeling bottleneck (Settles, 2009).Active learning has been studied for more than twenty years (Lewis and Gale, 1994; Lewis and Catlett, 1994; Cohn et al., 1994, 1996) and there have been several literature surveys on this topic (Settles, 2009; Olsson,