# Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

What challenges are associated with using annotated data sets in supervised methods for discovering multiword expressions?

## URLs

1. https://ar5iv.org/html/2405.08497. [2405.08497] Is Less More? Quality, Quantity and Context in Idiom Processing with Natural Language Models
2. https://ar5iv.org/html/2208.07832. [2208.07832] BERT(s) to Detect Multiword Expressions
3. https://ar5iv.org/html/2403.02009. [2403.02009] Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic?
4. https://ar5iv.org/html/2204.10050. [2204.10050] SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding
5. https://ar5iv.org/html/2411.02610. [2411.02610] Investigating Idiomaticity in Word Representations
6. https://ar5iv.org/html/2007.11381. [2007.11381] To Be or Not To Be a Verbal Multiword Expression: A Quest for Discriminating Features
7. https://ar5iv.org/html/2406.15175. [2406.15175] Enhancing Idiomatic Representation in Multiple Languages via an Adaptive Contrastive Triplet Loss
8. https://ar5iv.org/html/2410.14074. [2410.14074] Be My Donor. Transfer the NLP Datasets Between the Languages Using LLM
9. https://ar5iv.org/html/2405.02861. [2405.02861] Revisiting a Pain in the Neck: Semantic Phrase Processing Benchmark for Language Models

## Answer (I don't know?: ✗)

The identification and processing of multiword expressions (MWEs) pose significant challenges in the field of natural language processing (NLP). One notable challenge arises from the inherent complexity of MWEs, which exhibit unique linguistic behaviors differing from those of simple word combinations. This complexity includes variations in syntactic and semantic properties that can complicate the model's understanding of context and meaning [1]. 

Another challenge is the reliance on annotated datasets for training machine learning models. The annotation process itself can be resource-intensive and subject to bias or inconsistency, especially in the context of idiomatic expressions, where the meaning of phrases is often non-compositional. If the datasets are not sufficiently diverse or well-annotated, the performance of supervised models may be significantly hindered. Moreover, the scope and quality of training data directly influence the model's ability to generalize across different contexts, which is critical for effective MWE identification [4].

The recent trend in using deep learning models, particularly incorporating architectures like Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) networks, has shown promise. However, these models may not fully leverage linguistic features that could enhance performance [2]. The challenge remains in effectively integrating linguistic features with these deep learning techniques, as doing so requires careful consideration and experimentation, such as using ablation studies to assess the impact of various features [2]. 

Additionally, the absence of training data or insufficient quantities can limit the ability of models to learn effectively. Even top-performing methods that predominantly utilize deep neural networks struggle in scenarios where annotated datasets are sparse or unavailable, suggesting that data availability is a critical factor influencing MWE detection success [2]. 

Lastly, the non-standardized nature of MWEs also complicates model training and evaluation. Different MWEs can behave differently depending on their usage in various contexts, which demands that models not only recognize MWEs but also understand their semantic roles in sentences. Thus, detecting MWEs effectively requires a nuanced approach that can balance the immediate syntactic accuracy with deeper semantic comprehension, amplifying the risk of error in interpretation [4][3].

In summary, challenges associated with using annotated datasets in supervised methods for discovering MWEs include the complexity and variability of MWEs, the demands of quality annotation processes, limitations arising from the absence of extensive training datasets, and the intricate integration of linguistic and neural features to enhance model performance. These factors underscore the ongoing need for further research and development to advance MWE identification in NLP [1][3][4].

1. [1]:  https://ar5iv.org/html/2204.10050, [2204.10050] SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding
2. [2]:  https://ar5iv.org/html/2204.10050, [2204.10050] SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding
3. [3]:  https://ar5iv.org/html/2405.02861, [2405.02861] Revisiting a Pain in the Neck: Semantic Phrase Processing Benchmark for Language Models
4. [4]:  https://ar5iv.org/html/2208.07832, [2208.07832] BERT(s) to Detect Multiword Expressions
5. [5]:  https://ar5iv.org/html/2411.02610, [2411.02610] Investigating Idiomaticity in Word Representations
---
1. [1]:  Passage ID 1: at identifying whether a sentence contains an idiomatic expression, and (b) a task based on semantic text similarity which requires the model to adequately represent potentially idiomatic expressions in context. Each Subtask includes different settings regarding the amount of training data. Besides the task description, this paper introduces the datasets in English, Portuguese, and Galician and their annotation procedure, the evaluation metrics, and a summary of the participant systems and their results. The task had close to 100 registered participants organised into twenty five teams making over 650 and 150 submissions in the practice and evaluation phases respectively.1 IntroductionMultiword Expressions (MWEs) are a challenge for natural language processing (NLP), as their linguistic behaviour (e.g., syntactic, semantic) differs from that of generic word combinations Baldwin and Kim (2010); Ramisch and Villavicencio (2018). Moreover, MWEs are pervasive in all domains Biber
2. [2]:  Passage ID 2: training data is not available.While the top performing methods across this task have been driven by deep neural models independent of linguistic features, we highlight that this does not imply that the addition of linguistically motivated features does not lead to improvements on the task. Instead, it points to the possibility of integrating these methods into the more powerful neural models in future work where an ablation study might shed more light on the impact of each feature.AcknowledgementsThis work was partially supported by the UK EPSRC grant EP/T02450X/1, by the CDT in Speech and Language Technologies and their Applications (UKRI grant number EP/S023062/1), by a Ramón y Cajal grant (RYC2019-028473-I), and by the grant ED431F 2021/01 (Galician Government).ReferencesBaldwin and Kim (2010)Timothy Baldwin and Su Nam Kim. 2010.Multiword expressions.In Nitin Indurkhya and Fred J. Damerau, editors, Handbook ofNatural Language Processing, pages 267–292. CRC
3. [3]:  Passage ID 3: Within this context, the exploration of some research questions such as “Specialized model vs. LLM, which do we need in MwE processing?”, “How can NLP systems better handle discontinuous semantic phrases?” or “Can large language models serve as the general phrase processing system in some way?” warrants rethinking and further investigation in the future.SystemIEILCINCIR-L ↑↑\uparrowPPL ↓↓\downarrowR-L ↑↑\uparrowPPL ↓↓\downarrowR-L ↑↑\uparrowPPL ↓↓\downarrowHuman24.234.826.541.039.065.9Gemini-1.0-Pro18.842.433.462.945.259.1↪+↪absent{{\hookrightarrow}}\ + 3-shot28.228.751.470.876.063.1↪+↪absent{{\hookrightarrow}}\ + 5-shot27.828.750.166.090.042.4GPT-3.5-Turbo14.241.332.571.536.350.3↪+↪absent{{\hookrightarrow}}\ + 3-shot27.428.050.574.978.037.1↪+↪absent{{\hookrightarrow}}\ +
4. [4]:  Passage ID 4: (MT) [17, 16], which depends on a clear distinction between word tokens and phrases, has to be re-thought to accommodate MWEs [8, 29]. The usual approach in these applications is to identify MWEs first, and then treat them accordingly. Therefore, detecting MWEs is a key research area in NLP.In recent years, the identification of MWEs has been modelled as a supervised machine learning task where the machine learning models are trained on an annotated dataset. As we explain in Section 2, several datasets have been released to train these machine learning models. Furthermore shared tasks such as SemEval-2016 Task 10 [28] and PARSEME [27] have contributed to develop datasets. In recent years, neural network-based models, and in particular architectures incorporating RecurrentNeural Networks (RNNs) such as Long Short Term Memory (LSTM) and Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in MWE identification tasks [27]. Usually, these models utilise
5. [5]:  Passage ID 5: 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Association for Computational Linguistics, Doha, Qatar.Peters et al. (2018)Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018.Deep contextualized word representations.In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227–2237, Association for Computational Linguistics, New Orleans, Louisiana.Phelps et al. (2022)Phelps, Dylan, Xuan-Rui Fan, Edward Gow-Smith, Harish Tayyar Madabushi, Carolina Scarton, and Aline Villavicencio. 2022.Sample efficient approaches for idiomaticity detection.In Proceedings of the 18th Workshop on Multiword Expressions @LREC2022, pages 105–111, European Language Resources Association, Marseille, France.Phelps et al. (2024)Phelps, Dylan, Thomas