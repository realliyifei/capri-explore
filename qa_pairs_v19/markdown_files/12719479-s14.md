# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How do you calculate the expected agreement for multiple coders using pairwise comparisons?

## URLs

1. https://ar5iv.org/html/2303.12502. [2303.12502] Measuring Agreement Among Several Raters Classifying Subjects Into One-Or-More (Hierarchical) Nominal Categories. A Generalisation of Fleissâ€™ kappa
2. https://ar5iv.org/html/1806.05821. [1806.05821] Measuring intergroup agreement and disagreement
3. https://ar5iv.org/html/2310.12049. [2310.12049] Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models
4. https://ar5iv.org/html/2010.00370. [2010.00370] Strategy for Boosting Pair Comparison and Improving Quality Assessment Accuracy
5. https://ar5iv.org/html/2305.20042. [2305.20042] Crowdsourcing subjective annotations using pairwise comparisons reduces bias and error compared to the majority-vote method
6. https://ar5iv.org/html/2004.05691. [2004.05691] Active Sampling for Pairwise Comparisons via Approximate Message Passing and Information Gain Maximization
7. https://ar5iv.org/html/2402.17826. [2402.17826] Prediction-Powered Ranking of Large Language Models
8. https://ar5iv.org/html/2406.07791. [2406.07791] Judging the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs
9. https://ar5iv.org/html/2408.09235. [2408.09235] Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form Text

## Answer (I don't know?: âœ—)

To calculate the expected agreement for multiple coders using pairwise comparisons, we follow a systematic approach that typically involves the construction of a matrix representation of the ratings and an assessment of the agreement among raters based on their classifications.

1. **Matrix Construction**: First, we form a binary matrix \( P_{CM} \) based on pairwise comparisons. For each pair of subjects \( i \) and \( j \), if the converted score for subject \( i \) is higher than that for subject \( j \), we assign a value of 1 in the matrix at position \( (i,j) \); otherwise, we assign 0. This binary matrix does not include diagonal elements since they are comparisons of the subjects against themselves. This helps in identifying the preference of one subject over another clearly [2].

2. **Expected Agreement Calculation**: The expected agreement among multiple raters can be derived from calculating the intraclass correlation coefficient (ICC). This involves two key measures:
   - **Observed Agreement** (\( P_o \)): This is the proportion of all ratings that are in agreement among the raters. For a given subject \( i \), if all raters provided consistent classifications, then the observed agreement reaches its maximum, represented by \( \rho_i = 1 \), which indicates perfect agreement among raters for that subject. The overall observed agreement can be expressed as an average of all \( \rho_i \) values for the subjects [5].
   - **Expected Agreement** (\( P_e \)): This is calculated by comparing the ratings across all subjects and raters and involves assessing the agreement if raters were to classify subjects randomly. This provides a baseline standard against which our observed agreement can be compared to assess actual agreement levels [5].

3. **Final Agreement Score**: By plugging the observed (\( P_o \)) and expected (\( P_e \)) values into a defined formula, which often takes into account the proportion of agreements relative to the expected chance agreements, we can compute a final score that reflects the degree of agreement between the raters beyond what would be expected by random chance. This is essential for validating the reliability of the coding process, especially in comparative studies where multiple coders may render different judgments [5].

4. **Broader Context in NLP**: In the context of Natural Language Processing (NLP), this methodology applies not just to subjective evaluations, but to numerous automated assessments where system-generated outputs are compared against human judgments to ensure fairness and consistency across various tasks [3] [4]. 

By analyzing the pairwise comparisons through computing the agreement matrix and applying the corresponding statistical measures, we can arrive at a statistically meaningful assessment of coder agreement in a comparative study.

1. [1]:  https://ar5iv.org/html/2305.20042, [2305.20042] Crowdsourcing subjective annotations using pairwise comparisons reduces bias and error compared to the majority-vote method
2. [2]:  https://ar5iv.org/html/2010.00370, [2010.00370] Strategy for Boosting Pair Comparison and Improving Quality Assessment Accuracy
3. [3]:  https://ar5iv.org/html/2406.07791, [2406.07791] Judging the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs
4. [4]:  https://ar5iv.org/html/2408.09235, [2408.09235] Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form Text
5. [5]:  https://ar5iv.org/html/2303.12502, [2303.12502] Measuring Agreement Among Several Raters Classifying Subjects Into One-Or-More (Hierarchical) Nominal Categories. A Generalisation of Fleissâ€™ kappa
---
1. [1]:  Passage ID 1: was presented in a task, the replies were presented in random order and limited to a maximum of five first-level replies and their second-level replies. Each of the 820 possible comparisons was presented to two raters on Amazon Mechanical Turk. The raters had three possible choices for each comparison. They could select the conversation on the left, on the right, or judge them to have an equal level of disagreement. Each comparison was replicated across two different raters for a total of 1640 crowdsourcing tasks. Special care was given to make sure no single rater can perform more than 50 comparison tasks to simulate responses from a diverse population.For the scaling experiment, we take the final ratings calculated using all 1640 task answers as a benchmark. To measure the change in accuracy as a function of the number of comparisons, we selected random subsets of the comparisons and fed them into the Elo system to arrive at ratings and predicted labels. Comparing these predicted
2. [2]:  Passage ID 2: testAs emphasized earlier in the paper, the conversion from pair comparison preferences to quality rating/ranking scores is important. In order to compare the proposed conversion model with other commonly used models, the agreement test suggested inÂ (Li etÂ al. 2011) was conducted to evaluate the agreement between the converted data and the original ground truth. More specifically, the ground truth Pâ€‹Câ€‹Mğ‘ƒğ¶ğ‘€PCM is given as input to the under-test pair comparison data conversion model to obtain the recovered rating scores. A matrix Pâ€‹Câ€‹Mcğ‘ƒğ¶subscriptğ‘€ğ‘PCM_{c} is then formed by comparing all possible pairs using the converted scores. For each element within the matrix, if the converted score of stimulus iğ‘–i is larger than the one of jğ‘—j then 1 is assigned to Pâ€‹Câ€‹Mcâ€‹(i,j)ğ‘ƒğ¶subscriptğ‘€ğ‘ğ‘–ğ‘—PCM_{c}(i,j), otherwise 0 is assigned instead (i.e., it is a binary matrix without considering the diagonal elements). Before its comparison with the ground truth matrix Pâ€‹Câ€‹Mğ‘ƒğ¶ğ‘€PCM that aggregates both the
3. [3]:  Passage ID 3: 6.8, underscores the consistency between our measures of Pâ€‹Cğ‘ƒğ¶PC and Pâ€‹Fğ‘ƒğ¹PF. Notably, models like gpt-3.5-turbo-0125 manage to maintain higher levels of positional fairness (i.e., lower absolute Pâ€‹Fğ‘ƒğ¹PF scores) even when consistency is lower, indicating their robust performance across different measures. This demonstrates their balanced capabilities in maintaining fairness amidst varying levels of consistency.Cross-Judge AgreementWe also explore the agreement between LLM judges to gain deeper insights into their behavioral patterns and validate the fairness and prompt-independence of our position bias measurement. For each pair of judges, we calculate the number of instances in both benchmarks where their choices align, normalizing this count by the total number of instances. To address the potential bias introduced by ties, where judges rate the candidate solutions as equally valid, we compute an additional agreement score excluding such instances. These scores are represented
4. [4]:  Passage ID 4: about how to judge a response, mark it as â€œunder reviewâ€.Figure 10: Guidelines for human annotators to evaluate candidate LLMs outputs.Appendix C Additional ResultsIn this section, we provide detailed results in order to understand the capabilities of LLMs-as-judges.Figure 11: Performance of individual LLM as a judge evaluating their outputs and other candidate models across TruthfulQA, TriviaQA, and HotpotQA, compared to the ground truth established by human annotators.PromptHuman Evaluation (main experiment)LLMs-as-Judges (ablation)MajorityPAFleiss KappaMajorityPAFleiss KappaOpen Prompt60.0%82.0%0.7468.0%70.0%0.55Detailed Prompt60.0%82.0%0.7461.0%65.0%0.51Close Prompt60.0%82.0%0.7462.0%72.0%0.62Table 6: Comparison of Majority votes, Percent Agreement (PA), and Fleissâ€™ Kappa between Human Evaluation and LLMs-as-Judges across three prompt variations.You are a helpful assistant acting as an impartial judge. You will be given
5. [5]:  Passage ID 5: subject (iâˆˆ{1,â€¦,I}ğ‘–1â€¦ğ¼i\in\{1,\ldots,I\}) for the jğ‘—j-th rater (jâˆˆ{1,â€¦,J}ğ‘—1â€¦ğ½j\in\{1,\ldots,J\}), with xiâ€‹jâ€‹c=1subscriptğ‘¥ğ‘–ğ‘—ğ‘1x_{ijc}=1 when subject iğ‘–i was classified by rater jğ‘—j into category cğ‘c (câˆˆ{1,â€¦â€‹C}ğ‘1â€¦ğ¶c\in\{1,\ldots C\}), and xiâ€‹jâ€‹c=0subscriptğ‘¥ğ‘–ğ‘—ğ‘0x_{ijc}=0 otherwise. A measure of agreement is obtained by computing an intraclass correlation coefficient ÏisubscriptğœŒğ‘–\rho_{i} between all ğ±ğ¢ğ£subscriptğ±ğ¢ğ£\mathbf{x_{ij}} for a given subject iğ‘–i for all raters using a one-way ANOVA. If all the raters classified subject iğ‘–i in the same categories, perfect agreement is obtained, Ïi=1subscriptğœŒğ‘–1\rho_{i}=1. Pâ€‹oğ‘ƒğ‘œPo can be computed by taking the average of Ï1,Ï2,â€¦,ÏIsubscriptğœŒ1subscriptğœŒ2â€¦subscriptğœŒğ¼\rho_{1},\rho_{2},\ldots,\rho_{I}. Pâ€‹eğ‘ƒğ‘’Pe is determined by computing the intraclass correlation coefficient between all classification vectors ğ±ğ¢ğ£subscriptğ±ğ¢ğ£\mathbf{x_{ij}} for all raters and all subjects. Plugging Pâ€‹oğ‘ƒğ‘œPo and Pâ€‹eğ‘ƒğ‘’Pe in (1) gives the value of the â€˜chance-corrected