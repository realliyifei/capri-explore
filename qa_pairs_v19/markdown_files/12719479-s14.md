# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How do you calculate the expected agreement for multiple coders using pairwise comparisons?

## URLs

1. https://ar5iv.org/html/2303.12502
2. https://ar5iv.org/html/1806.05821
3. https://ar5iv.org/html/2310.12049
4. https://ar5iv.org/html/2010.00370
5. https://ar5iv.org/html/2305.20042
6. https://ar5iv.org/html/2004.05691
7. https://ar5iv.org/html/2402.17826
8. https://ar5iv.org/html/2406.07791
9. https://ar5iv.org/html/2408.09235

## Answer

## I don't know?: True

I don't know. The context provided does not contain specific information on calculating expected agreements for multiple coders using pairwise comparisons. While it discusses inter-coder agreement and the reliability of expert vs. non-expert annotations [1] [2] [3], it lacks any explicit details or methodologies related to the calculation of expected agreement through pairwise comparisons. Therefore, there's insufficient information to provide a comprehensive answer to your question.

[1]: https://ar5iv.org/html/2407.21037, No Title
[2]: https://ar5iv.org/html/1907.10450, No Title
[3]: https://ar5iv.org/html/1907.10450, No Title
[4]: https://ar5iv.org/html/1907.10450, No Title
[5]: https://ar5iv.org/html/1907.10450, No Title

[1]: Passage ID 1: points for those statistics, and it is relatively easy to achieve momentarily high human-human kappas with repeated training and coding without necessarily achieving ongoing inter-rater alignment. Also, as just mentioned, close evaluation of past coding by humans reveals many cases where reasonable coders could disagree. We discuss below an approach we call ‚Äúmismatch analysis‚Äù where we ask independent coders to evaluate whether the original human coders or the LLM coder assigned the right code. More broadly, we go through an extensive process of testing the data in ways that can help build confidence in our process and the results.Coding Consistency MeasureAs we tested different coding strategies, we realized that the model might not always code a given sentence the same way every time, especially when using in-context learning, which has the model learn afresh each time it is applied to a transcript. To test how consistently the model coded a transcript‚Äôs sentences, we decided
[2]: Passage ID 2: to the performance of video indexing methods based on supervised learning.Moreover, we suppose that inter-coder agreement might form an upper bound for such methods. In this regard, some first experiments are presented to predict average precision using support vector regression based on inter-coder agreement and training data size.Furthermore, the influence of image annotation quality induced by experts vs. non-experts on the person recognition performance is investigated based on annotations acquired during the study.The remainder of the paper is organized as follows. Section 2 discusses related work regarding inter-annotator studies. Section 3 deals with the comprehensive user study including a description of the used dataset, the study participants, the experimental design and the results of a comparison between expert vs. non-expert inter-coder agreements. Furthermore, a performance prediction for the task of concept classification is presented based on inter-coder agreement
[3]: Passage ID 3: experts and students.Earlier in this section we implied that a high agreement is an indicator for an accurate set of annotations. For the categories of concepts, experts and non-experts were found to be equally accurate annotators.In the next section, we analyze to which extent agreement contributes to video indexing performance and investigate the opportunity of performance prediction.We determined that experts are more accurate labelers when it comes to historically relevant persons of the GDR. However, judgments of different groups may have different biases as also reasoned for the concept categories. Therefore, in Section¬†5, we exemplary determine for the set of different personality annotations collected in our study whether high agreement on training images implies higher system performance for person recognition.4 Predicting Concept Classification Performance In this section, we discuss correlations between original average precision results and inter-coder
[4]: Passage ID 4: higher agreement for experts, but argue that majority voting filters out noise in non-expert annotations closing the gap to expert annotations of higher quality. A more recent study¬†[2] deals with the question whether machines perform better than humans in visual recognition tasks. For assessing human performance the inter-coder reliability by Krippendorff‚Äôs Œ±ùõº\alpha on 20 common categories of the PASCAL VOC benchmark is measured. For the best submission at PASCAL VOC‚Äôs leaderboard an above average human-level performance for visual concept annotation is reported being on a par or better than 19 of 23 participants.3 Annotation Study: Expert vs. Non-Expert Agreement on GDR-specific Concepts and PersonsIn this section, we aim at comparing the reliability of expert vs. non-expert annotations for for historical TV data in terms of inter-coder-agreement. For this purpose, we collected annotations of expert as well as student participants (Section¬†3.2) on a selected set of concept and
[5]: Passage ID 5: results of a comparison between expert vs. non-expert inter-coder agreements. Furthermore, a performance prediction for the task of concept classification is presented based on inter-coder agreement and training data size as input. In Section 5, the impact of expert vs. non-expert image annotations on person identification performance is investigated. Section 6 concludes the paper and outlines areas for future work.2 Related WorkIn this section, we briefly survey related work for inter-annotator studies conducted for natural language as well as image annotation tasks. Snow et al.¬†[8] have evaluated non-expert annotations by means of Mechanical Turk workers for natural language tasks. Among other experiments, they found that annotations of four non-experts are needed to rival the annotation quality of one expert annotator in selected tasks. Furthermore, they have trained machine learning classifiers on expert as well as non-expert annotations and reported better system