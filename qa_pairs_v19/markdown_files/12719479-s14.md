# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How do you calculate the expected agreement for multiple coders using pairwise comparisons?

## URLs

1. https://ar5iv.org/html/2303.12502. [2303.12502] Measuring Agreement Among Several Raters Classifying Subjects Into One-Or-More (Hierarchical) Nominal Categories. A Generalisation of Fleiss’ kappa
2. https://ar5iv.org/html/1806.05821. [1806.05821] Measuring intergroup agreement and disagreement
3. https://ar5iv.org/html/2310.12049. [2310.12049] Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models
4. https://ar5iv.org/html/2010.00370. [2010.00370] Strategy for Boosting Pair Comparison and Improving Quality Assessment Accuracy
5. https://ar5iv.org/html/2305.20042. [2305.20042] Crowdsourcing subjective annotations using pairwise comparisons reduces bias and error compared to the majority-vote method
6. https://ar5iv.org/html/2004.05691. [2004.05691] Active Sampling for Pairwise Comparisons via Approximate Message Passing and Information Gain Maximization
7. https://ar5iv.org/html/2402.17826. [2402.17826] Prediction-Powered Ranking of Large Language Models
8. https://ar5iv.org/html/2406.07791. [2406.07791] Judging the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs
9. https://ar5iv.org/html/2408.09235. [2408.09235] Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form Text

## Answer (I don't know?: ✗)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable