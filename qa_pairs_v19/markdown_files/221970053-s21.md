# A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English

## Question

How is the WinoGender dataset used to assess gender bias in pre-trained language models?

## URLs

1. https://ar5iv.org/html/2409.05653. [2409.05653] WinoPron: Revisiting English Winogender Schemas for Consistency, Coverage, and Grammatical Case
2. https://ar5iv.org/html/1804.09301. [1804.09301] Gender Bias in Coreference Resolution
3. https://ar5iv.org/html/2212.10678. [2212.10678] Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing
4. https://ar5iv.org/html/2110.05367. [2110.05367] Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting
5. https://ar5iv.org/html/2406.13925. [2406.13925] GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models
6. https://ar5iv.org/html/2306.12424. [2306.12424] VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution
7. https://ar5iv.org/html/2403.00277. [2403.00277] Gender Bias in Large Language Models across Multiple Languages
8. https://ar5iv.org/html/2403.14409. [2403.14409] Locating and Mitigating Gender Bias in Large Language Models

## Answer (I don't know?: ✗)

The WinoGender dataset is utilized to assess gender bias in pre-trained language models by focusing on their performance regarding the resolution of gender pronouns in various occupational contexts, thereby highlighting potential biases linked to gender associations in occupations. This dataset was developed to further investigate gender biases by employing structured tests that involve pronoun resolution, which is a critical aspect of Natural Language Processing (NLP) models, particularly those involving vision-language interactions [1].

Specifically, the dataset uses a templated structure designed to test gender bias in occupational pronoun resolution, applying frameworks such as WinoBias and WinoGender to probe how models derive gender-related conclusions from visual input [1]. WinoGender serves as a tool to stress-test the capabilities of vision-language models in relation to gender biases present in their reasoning and coreference resolution abilities—areas where previous models have shown inaccuracies by frequently associating women with specific roles (e.g., nurse) and men with others (e.g., doctor) [2][3]. 

Moreover, the dataset includes examples that present gendered pronouns in contexts where occupational roles are defined, thereby enabling an examination of how these models interpret and generate these roles differently based on gender associations inherent in their training data [1][3]. The findings suggest that such biases are rooted in unbalanced model behaviors reflecting societal stereotypes, where male-associated occupations receive positive descriptors more frequently than female-associated ones, reinforcing existing gender disparities [2][3][4].

The overall evaluation of gender bias through WinoGender not only assesses model performance on pronoun resolution but also delves into the intricate relationship between language representation and visual understanding, providing insights into how models behave differently when encountering male versus female representations [1][4]. This comprehensive analytic approach ultimately contributes to the broader understanding of gender biases in machine learning systems, underlining the necessity for systematic biases screening in LLMs across various domains and languages [2][4].

In summary, the WinoGender dataset plays a pivotal role in revealing gender bias in pre-trained language models through structured testing of pronoun resolution in occupational contexts, thereby enhancing awareness and encouraging solutions for mitigating bias in AI systems [1][2][3].

1. [1]:  https://ar5iv.org/html/2306.12424, [2306.12424] VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution
2. [2]:  https://ar5iv.org/html/2403.00277, [2403.00277] Gender Bias in Large Language Models across Multiple Languages
3. [3]:  https://ar5iv.org/html/2110.05367, [2110.05367] Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting
4. [4]:  https://ar5iv.org/html/2403.14409, [2403.14409] Locating and Mitigating Gender Bias in Large Language Models
5. [5]:  https://ar5iv.org/html/2403.00277, [2403.00277] Gender Bias in Large Language Models across Multiple Languages
---
1. [1]:  Passage ID 1: reasoning capabilities of VLMs as in the Winoground benchmark [18] but introduce the dimension of social biases. Secondly, we adopt the templated structure to test gender bias in occupational pronoun resolution from NLP research, but apply it to the vision-language domain—specifically the WinoGender [19] and WinoBias [20] frameworks, in turn inspired by Winograd Schema [21]. To our knowledge, VisoGender is the first dataset to combine both of these contributions by stress-testing gender bias in visual-linguistic reasoning and coreference resolution capabilities.Figure 1: Resolution of gender pronouns and retrieval with a neutral query. We resolve gender by (i) using zero-shot classification with Cross Models Encoders, such as CLIP, and (ii) next-token prediction with captioning models, such as BLIP. We have an additional simpler task to resolve the gender of a single person, e.g., with a template “The doctor and her / his stethoscope”.VisoGender contains images of a person
2. [2]:  Passage ID 2: Khan et al., 2023; Hutchinson et al., 2020; Sun and Peng, 2021; Yao and Huang, 2017). For gender bias, men are more likely to be described by professional and excellent words than women. One of our evaluations of gender bias is different from the ones listed above. Inspired by Bechdel test (Bechdel., 1986; Agarwal et al., 2015), we use the topics of dialogue to demonstrate that LLMs treat different genders differently.Gender Bias in Language ModelsExisting works investigating gender bias for Pretrained LMs are mainly focused on single language (Zhou and Sanfilippo, 2023) such as English  Mehrabi et al. (2021); Belém et al. (2024) and German  (Wambsganss et al., 2023). Some studies focus on bilingual aspects (Takeshita et al., 2020). Gender Bias benchmarks such as WinoBias  (Zhao et al., 2018a) and Winogender  (Rudinger et al., 2018) are often used to investigate gender bias in LMs. Both Natural Language Understanding (Gupta et al., 2022; Bolukbasi et al., 2016; Dixon et al.,
3. [3]:  Passage ID 3: 2019), have shown competitive performance in a wide variety of NLP downstream applications.However, such models are often prone to exhibit gender bias (de Vassimon Manela et al., 2021; Zhao et al., 2019; Webster et al., 2020), due to their large scale unsupervised training data from the web (Liu et al., 2019; Brown et al., 2020). Gender bias refers tounbalanced model behaviors with respect to a specific gender (Cheng et al., 2020).Among various gender-biased behaviours of pre-trained models, bias on professions is the most prominent and well-studied (de Vassimon Manela et al., 2021; Vig et al., 2020; Qian et al., 2019; Zhao et al., 2019).For example, in coreference resolution tasks, a pre-trained model would predict female pronoun and names for professions like “nurse” and “housekeeper”, while predict male pronouns for “computer programmer” or “doctor” (Kurita et al., 2019). The pre-trained models also wouldn’t prefer gender-neutral pronouns actively, which is unfair to other
4. [4]:  Passage ID 4: and seven knowledge competency test datasets. The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence. Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model’s capabilities in all other aspects.Keywords: Large language modelCausal intervention Debias1 IntroductionIn recent years, sophisticated artificial intelligence models, notably exemplified by ChatGPT [31, 28, 33], are specially designed to excel in comprehending complicated natural language and generating human-like text. However, as these models become increasingly integrated across various sectors [14, 6], the inherent biases within these systems has become a subject of growing concern.Bias refers to the existence of consistent inaccuracies, misattributions, or erroneous
5. [5]:  Passage ID 5: et al., 2018a) and Winogender  (Rudinger et al., 2018) are often used to investigate gender bias in LMs. Both Natural Language Understanding (Gupta et al., 2022; Bolukbasi et al., 2016; Dixon et al., 2018) and Natural Language Generation (Sheng et al., 2019; Huang et al., 2021; Lucy and Bamman, 2021) tasks show gender bias.For LLMs, the most related work for English professional documents refers to  (Wan et al., 2023), which evaluates the gender bias in LLM-generated references. This work found that females are more likely to receive communal words in the reference whereas males are more likely to be described as a leader.  Kotek et al. (2023) demonstrate LLMs express gender bias about occupation. LLMs have a higher likelihood of selecting an occupation that traditionally matches a person’s gender. In contrast, our work investigates gender bias in multiple languages, such explorations are significant since LLMs are treated as multilingual agents and evaluation from a single language