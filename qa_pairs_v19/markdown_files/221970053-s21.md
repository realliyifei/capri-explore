# A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English

## Question

How is the WinoGender dataset used to assess gender bias in pre-trained language models?

## URLs

1. https://ar5iv.org/html/2409.05653
2. https://ar5iv.org/html/1804.09301
3. https://ar5iv.org/html/2212.10678
4. https://ar5iv.org/html/2110.05367
5. https://ar5iv.org/html/2406.13925
6. https://ar5iv.org/html/2306.12424
7. https://ar5iv.org/html/2403.00277
8. https://ar5iv.org/html/2403.14409

## Answer

## I don't know?: False

The WinoGender dataset is utilized to investigate and quantify gender bias in pre-trained language models (PLMs) by focusing on the pronoun resolution capabilities related to gender-specific contexts in language. Specifically, it builds on the established frameworks of WinoBias and extends its application into gender biases in occupations, leveraging a templated structure for sentence formulation that incorporates gendered language.

WinoGender evaluates how models resolve gender pronouns and images within various occupational contexts by employing methods such as zero-shot classification and next-token prediction with models like CLIP (Contrastive Language–Image Pre-training) and BLIP (Bootstrapping Language-Image Pre-training) respectively [2]. The dataset is constructed around a variety of occupations and gendered pronouns to rigorously test models on their ability to accurately determine gender roles represented in visual and textual inputs.

By contrasting the outcomes of occupational roles with gendered pronouns, WinoGender effectively highlights biases present in PLMs. For instance, models can be subjected to tasks where sentences with male and female pronouns interchangeably depict individuals in highly gender-stereotyped occupations (e.g., doctors, nurses) creating scenarios that necessitate accurate gender representation [4]. This can reveal a predisposition of models to associate certain genders with specific occupations, thereby manifesting biases that have been noted in previous NL research [5].

Furthermore, the dataset also enables a direct comparison of model performance across different conditions — for instance, it assesses how well models perform in gender-stereotyped versus non-stereotyped contexts [1]. The WinoGender framework's design not only helps identify biases inherent within the models but also offers insights into the underlying mechanisms by which these biases operate, thus facilitating better understanding and potential mitigation strategies in NLP systems [2] [5].

In summary, WinoGender serves as an essential tool for exposing and analyzing gender biases in language models by employing a structured approach that integrates visual and linguistic reasoning centered around gender in occupational settings. It permits a nuanced evaluation of how these models operate and respond to gendered contexts, ultimately contributing to the broader dialogue around bias in machine learning models.

1. [1]:  https://ar5iv.org/html/1804.06876, No Title
2. [2]:  https://ar5iv.org/html/2306.12424, [2306.12424] VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution
3. [3]:  https://ar5iv.org/html/2005.14050, No Title
4. [4]:  https://ar5iv.org/html/1804.06876, No Title
5. [5]:  https://ar5iv.org/html/2403.00277, [2403.00277] Gender Bias in Large Language Models across Multiple Languages
---
1. [1]:  Passage ID 1: and Klein (2013), and end-to-end neural (the current state-of-the-art), E2E,  Lee et al. (2017).The following sections show that performance on WinoBias reveals gender bias in all systems, that our methods remove such bias, and that systems are less biased on OntoNotes data.WinoBias Reveals Gender BiasTable 2 summarizes development set evaluations using all three systems.Systems were evaluated on both types of sentences in WinoBias (T1 and T2), separately in pro-stereotyped and anti-stereotyped conditions ( T1-p vs. T1-a, T2-p vs T2-a).We evaluate the effect of named-entity anonymization (Anon.), debiasing supporting resources888Word embeddings for E2E and gender lists for Feature (Resour.) and using data-augmentation through gender swapping (Aug.).E2E and Feature were retrained in each condition using default hyper-parameters while Rule was not debiased because it is untrainable.We evaluate using the coreference scorer v8.01 Pradhan et al. (2014) and compute the average
2. [2]:  Passage ID 2: reasoning capabilities of VLMs as in the Winoground benchmark [18] but introduce the dimension of social biases. Secondly, we adopt the templated structure to test gender bias in occupational pronoun resolution from NLP research, but apply it to the vision-language domain—specifically the WinoGender [19] and WinoBias [20] frameworks, in turn inspired by Winograd Schema [21]. To our knowledge, VisoGender is the first dataset to combine both of these contributions by stress-testing gender bias in visual-linguistic reasoning and coreference resolution capabilities.Figure 1: Resolution of gender pronouns and retrieval with a neutral query. We resolve gender by (i) using zero-shot classification with Cross Models Encoders, such as CLIP, and (ii) next-token prediction with captioning models, such as BLIP. We have an additional simpler task to resolve the gender of a single person, e.g., with a template “The doctor and her / his stethoscope”.VisoGender contains images of a person
3. [3]:  Passage ID 3: Analysis to Detect Unintended ModelBiases.In Proceedings of Empirical Methods in Natural LanguageProcessing (EMNLP), pages 5744–5749, Hong Kong, China.Prabhumoye et al. (2019)Shrimai Prabhumoye, Elijah Mayfield, and Alan W. Black. 2019.Principled Frameworks for Evaluating Ethics in NLP Systems.In Proceedings of the Workshop on Innovative Use of NLP forBuilding Educational Applications, Florence, Italy.Prates et al. (2019)Marcelo Prates, Pedro Avelar, and Luis C. Lamb. 2019.Assessing gender bias in machine translation: A case study withgoogle translate.Neural Computing and Applications.Précenth (2019)Rasmus Précenth. 2019.Word embeddings and gender stereotypes in Swedish and English.Master’s thesis, Uppsala University.Preston (2009)Dennis R. Preston. 2009.Are you really smart (or stupid, or cute, or ugly, or cool)? Or doyou just talk that way?Language attitudes, standardization and language change. Oslo:Novus forlag, pages
4. [4]:  Passage ID 4: approach completely eliminates bias when evaluating on WinoBias , without significantly affecting overall coreference accuracy.2 WinoBiasTo better identify gender bias in coreference resolution systems, we build a new dataset centered on people entities referred by their occupations from a vocabulary of 40 occupations gathered from the US Department of Labor, shown in Table 1.333Labor Force Statistics from the Current Population Survey, 2017. https://www.bls.gov/cps/cpsaat11.htmWe use the associated occupation statistics to determine what constitutes gender stereotypical roles (e.g. 90% of nurses are women in this survey).Entities referred by different occupations are paired and used to construct test case scenarios.Sentences are duplicated using male and female pronouns, and contain equal numbers of correct co-reference decisions for all occupations.In total, the dataset contains 3,160 sentences, split equally for development and test, created by researchers familiar with
5. [5]:  Passage ID 5: Khan et al., 2023; Hutchinson et al., 2020; Sun and Peng, 2021; Yao and Huang, 2017). For gender bias, men are more likely to be described by professional and excellent words than women. One of our evaluations of gender bias is different from the ones listed above. Inspired by Bechdel test (Bechdel., 1986; Agarwal et al., 2015), we use the topics of dialogue to demonstrate that LLMs treat different genders differently.Gender Bias in Language ModelsExisting works investigating gender bias for Pretrained LMs are mainly focused on single language (Zhou and Sanfilippo, 2023) such as English  Mehrabi et al. (2021); Belém et al. (2024) and German  (Wambsganss et al., 2023). Some studies focus on bilingual aspects (Takeshita et al., 2020). Gender Bias benchmarks such as WinoBias  (Zhao et al., 2018a) and Winogender  (Rudinger et al., 2018) are often used to investigate gender bias in LMs. Both Natural Language Understanding (Gupta et al., 2022; Bolukbasi et al., 2016; Dixon et al.,