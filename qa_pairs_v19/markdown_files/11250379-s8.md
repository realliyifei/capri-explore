# A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin

## Question

How does incorporating linguistic context dependency enhance recognition accuracy in continuous Mandarin speech recognition?

## URLs

1. https://ar5iv.org/html/2310.09680. [2310.09680] Improved Contextual Recognition in Automatic Speech Recognition Systems by Semantic Lattice Rescoring
2. https://ar5iv.org/html/2403.01255. [2403.01255] Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey
3. https://ar5iv.org/html/2409.10969. [2409.10969] Enhancing Multilingual Speech Generation and Recognition Abilities in LLMs with Constructed Code-switched Data
4. https://ar5iv.org/html/2406.10313. [2406.10313] CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge
5. https://ar5iv.org/html/2408.14991. [2408.14991] Speech Recognition Transformers: Topological-lingualism Perspective
6. https://ar5iv.org/html/1711.01946. [1711.01946] MANDARIN TONE MODELING USING RECURRENT NEURAL NETWORKS
7. https://ar5iv.org/html/2408.10524. [2408.10524] XCB: an effective contextual biasing approach to bias cross-lingual phrases in speech recognition
8. https://ar5iv.org/html/2108.01122. [2108.01122] AUTOMATIC RECOGNITION OF SUPRASEGMENTALS IN SPEECH

## Answer (I don't know?: ✓)

I don't know. The provided context does not specifically address the relationship between linguistic context dependency and recognition accuracy in continuous Mandarin speech recognition. The passages focus broadly on text comprehension, large language models, dialogue systems, and context learning abilities but do not mention Mandarin speech recognition or the ways in which linguistic context improves accuracy in that specific area. Because there is no information in the context regarding Mandarin or the specifics of recognition accuracy enhancement linked to linguistic dependencies, I cannot provide a precise answer to your question.

1. [1]:  https://ar5iv.org/html/2402.13551, No Title
2. [2]:  https://ar5iv.org/html/2411.13577, No Title
3. [3]:  https://ar5iv.org/html/2411.13577, No Title
4. [4]:  https://ar5iv.org/html/2411.13577, No Title
5. [5]:  https://ar5iv.org/html/2411.13577, No Title
---
1. [1]:  Passage ID 1: WeChat AIliyanlxu@tencent.com moyumyu@global.tencent.com1 IntroductionText comprehension has been advanced significantly ascribed to Large Language Models (LLMs), especially with long context window enabled via techniques such as positional scaling Xiong et al. (2023); Peng et al. (2024) and efficient attention Wang et al. (2023); Chen et al. (2024). Nevertheless, though extending context window could resolve certain long context tasks end-to-end, e.g. question answering, more fine-grained tasks that require explicit global dependency beyond local evidence still remain a challenge.In book-level narrative understanding particularly, such as retrieving relevant plot passages of queries Xu et al. (2023b), or identifying recap passages of a given plot Li et al. (2024), each local passage in a novel rather serves specific purposes to other parts than being isolated, which may be easily neglected in the end-to-end process without explicit modeling these global dependency
2. [2]:  Passage ID 2: conversational ability, and further exploration is still needed in this area of evaluation. Explicitly requiring a spoken dialogue model to perform speech translation is not a typical use case in conversational scenarios. In most cases, when a user asks a question in a different language or with a distinct accent, the model is expected to automatically respond in the same language that the user is using. In this context, it seems more reasonable to evaluate the accuracy of the model’s generated speech in terms of language identification, combined with subjective human assessments, as a more intuitive and appropriate evaluation method.Context Learning. The context learning capability is crucial for maintaining the coherence of an entire conversation. Similar to a memory function, the challenge lies in how to preserve this capability when relying solely on speech. Typically, the evaluation of a spoken dialogue model’s context learning ability depends on specific long-duration dialogue
3. [3]:  Passage ID 3: conversational ability, and further exploration is still needed in this area of evaluation. Explicitly requiring a spoken dialogue model to perform speech translation is not a typical use case in conversational scenarios. In most cases, when a user asks a question in a different language or with a distinct accent, the model is expected to automatically respond in the same language that the user is using. In this context, it seems more reasonable to evaluate the accuracy of the model’s generated speech in terms of language identification, combined with subjective human assessments, as a more intuitive and appropriate evaluation method.Context Learning. The context learning capability is crucial for maintaining the coherence of an entire conversation. Similar to a memory function, the challenge lies in how to preserve this capability when relying solely on speech. Typically, the evaluation of a spoken dialogue model’s context learning ability depends on specific long-duration dialogue
4. [4]:  Passage ID 4: conversational ability, and further exploration is still needed in this area of evaluation. Explicitly requiring a spoken dialogue model to perform speech translation is not a typical use case in conversational scenarios. In most cases, when a user asks a question in a different language or with a distinct accent, the model is expected to automatically respond in the same language that the user is using. In this context, it seems more reasonable to evaluate the accuracy of the model’s generated speech in terms of language identification, combined with subjective human assessments, as a more intuitive and appropriate evaluation method.Context Learning. The context learning capability is crucial for maintaining the coherence of an entire conversation. Similar to a memory function, the challenge lies in how to preserve this capability when relying solely on speech. Typically, the evaluation of a spoken dialogue model’s context learning ability depends on specific long-duration dialogue
5. [5]:  Passage ID 5: contextual understanding and reasoning abilities in the field of natural language processing, such as GPT-4 [1], Llama 3.1 [52], and Qwen-2 [229]. Due to their training on large-scale corpora, these models achieve exceptional accuracy when handling complex contexts. To further expand the capabilities of large language models, some research [25, 33, 61, 224] has explored enabling them to understand other modalities, thereby building multimodal interaction abilities. The spoken dialogue model, also known as the speech-text dialogue model, allows users to interact with LLMs naturally and straightforwardly through speech. However, the transition from text intelligence to speech intelligence involves two inherent hurdles: one core issue is the insufficient amount of speech data compared to the massive datasets used for pre-training text-based large language models. For instance, Llama 3.1 [52] uses 800 billion training tokens, and Qwen-2 [229] is trained on over 7 trillion tokens, whereas