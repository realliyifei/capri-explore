# A Survey of Active Learning for Natural Language Processing

## Question

How can similarity scores and feature-based metrics be applied in active learning to select instances for NLP tasks?

## URLs

1. https://ar5iv.org/html/2210.10109. [2210.10109] A Survey of Active Learning for Natural Language Processing
2. https://ar5iv.org/html/2406.12114. [2406.12114] Enhancing Text Classification through LLM-Driven Active Learning and Human Annotation
3. https://ar5iv.org/html/2301.13720. [2301.13720] Zero-shot cross-lingual transfer language selection using linguistic similarity
4. https://ar5iv.org/html/2404.15720. [2404.15720] Annotator-Centric Active Learning for Subjective NLP Tasks
5. https://ar5iv.org/html/2411.01013. [2411.01013] A Similarity-Based Oversampling Method for Multi-label Imbalanced Text Data
6. https://ar5iv.org/html/2407.03895. [2407.03895] Scoping Review of Active Learning Strategies and their Evaluation Environments for Entity Recognition Tasks
7. https://ar5iv.org/html/2406.09206. [2406.09206] Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models
8. https://ar5iv.org/html/2410.08972. [2410.08972] ALVIN: Active Learning Via INterpolation

## Answer (I don't know?: ‚úì)

I don't know. The context provided is insufficient to answer the question about how similarity scores and feature-based metrics can be applied in active learning for NLP tasks. The passages primarily discuss synthetic data generation, classification of research topics, and the details of model training and evaluation in NLP, but they do not specifically address the concept of active learning or provide information on the application of similarity scores and feature-based metrics in that context. Consequently, the information needed to construct a precise response regarding the application of these metrics in active learning is lacking.

1. [1]:  https://ar5iv.org/html/2410.00427, No Title
2. [2]:  https://ar5iv.org/html/2410.00427, No Title
3. [3]:  https://ar5iv.org/html/2410.00427, No Title
4. [4]:  https://ar5iv.org/html/2306.08193, No Title
5. [5]:  https://ar5iv.org/html/2306.08193, No Title
---
1. [1]:  Passage ID 1: ask them questions?‚Äù was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as ‚ÄúWho discovered the laws of thermodynamics?‚Äù Following a quality inspection of the synthetically produced questions, we assembled a training dataset of 1601 examples, consisting of 120 questions for each of the 12 topics and 161 general questions. We also derived a test dataset containing 364 examples with a balanced class distribution similar to the training dataset.In our experiments, we evaluated three classification approaches: vector similarity search, prompting a LLM (GPT-3.5-Turbo), and few-shot fine-tuning of a transformer model with the SetFit framework Tunstall et¬†al. (2022). Concerning the vector search approach with the SPECTER2 model, we measured the cosine similarity to compare vectors of embedded user queries with paper embeddings in our vector database to retrieve the 100 most similar
2. [2]:  Passage ID 2: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman‚Äôs terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman‚Äôs language. For example, the question ‚ÄúHow are computers able to respond when we ask them questions?‚Äù was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as ‚ÄúWho discovered the
3. [3]:  Passage ID 3: KG. As a last construction step, we further enriched the KG with metadata from the Semantic Scholar API, including one-sentence too long; didn‚Äôt read (TLDR) summaries, citation counts, and publication references.4 Results and Discussion4.1 Model Training and EvaluationResearch Topic Classification.In the following sections, we report the results of training and evaluating the NLP models that underpin the three-phase search process of our developed dialogue system: (1) research topic classification, (2) article text clustering, and (3) comparative article summarization.The first phase involves classifying an uttered search goal or problem description into a fitting NLP research topic. This is especially helpful for users in exploratory search settings because they may not be familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman‚Äôs terms to NLP
4. [4]:  Passage ID 4: Section 2.2 will be familiar to NLPers. Section 3 contains the main contribution of the paper, in which I operationalise the three criteria (Information, Use and Misrepresentation) via definition of two sorts of interventions on model activations. In Section 4, I draw connections between my operationalisation of representation and the empirical NLP literature; this section will be of particular interest to NLPers, but it also serves as a relatively self-contained introduction to several interpretability techniques which have received recent interest from philosophers. I conclude (Section 5) by summarising the contributions of the paper, arguing that they should interest both philosophers and NLPers.1.1 Motivation: Two ExamplesTo demonstrate the need for an account of representation in NLP, I outline the following toy examples.1.1.1 Example 1Suppose BERT ([20]) is fine-tuned on the Corpus of Linguistic Acceptability (CoLA; [79]). CoLA is a binary classification task, where
5. [5]:  Passage ID 5: in NLP, I outline the following toy examples.1.1.1 Example 1Suppose BERT ([20]) is fine-tuned on the Corpus of Linguistic Acceptability (CoLA; [79]). CoLA is a binary classification task, where the model judges whether an input sentence is grammatical or not.BERT has been pre-trained on a large document-level corpus on two self-supervised tasks: a masked language modelling task and a next sentence prediction task. For the fine-tuning stage, a linear classifier is appended to the end of the model. The classifier takes in the activation above the first token from BERT‚Äôs final layer and predicts a probability distribution over labels C={grammatical,ungrammatical}ùê∂grammaticalungrammaticalC=\{\texttt{grammatical},\texttt{ungrammatical}\}. The combination of model and classifier are tuned (a supervised task) on the labelled training portion of CoLA.Suppose that at test time BERT incorrectly judges the following CoLA example as ungrammatical:John danced waltzes across the