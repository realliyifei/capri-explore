# A Survey on Recent Advances in Named Entity Recognition from Deep Learning models

## Question

What are the differences between relaxed F1 and strict F1 metrics in evaluating NER predictions?

## URLs

1. https://ar5iv.org/html/2402.14948. [2402.14948] Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach
2. https://ar5iv.org/html/2406.18049. [2406.18049] Improving Entity Recognition Using Ensembles of Deep Learning and Fine-tuned Large Language Models: A Case Study on Adverse Event Extraction from Multiple Sources
3. https://ar5iv.org/html/2309.16150. [2309.16150] AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events
4. https://ar5iv.org/html/2303.16416. [2303.16416] Improving Large Language Models for Clinical Named Entity Recognition via Prompt Engineering
5. https://ar5iv.org/html/2404.09135. [2404.09135] Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions11footnote 1This research was funded by supported by National Key R&D Program of China (No. 2021YFF0901400)
6. https://ar5iv.org/html/1910.11470. [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
7. https://ar5iv.org/html/2402.03832. [2402.03832] Rethinking Skill Extraction in the Job Market Domain using Large Language Models
8. https://ar5iv.org/html/2101.00160. [2101.00160] How Do Your Biomedical Named Entity Recognition Models Generalize to Novel Entities?
9. https://ar5iv.org/html/2303.06944. [2303.06944] A Human Subject Study of Named Entity Recognition (NER) in Conversational Music Recommendation Queries

## Answer (I don't know?: ✗)

The differences between relaxed F1 and strict F1 metrics in evaluating Named Entity Recognition (NER) predictions are primarily based on the conditions under which a prediction is considered correct.

1. **Definition of Correctness**:
   - **Strict F1**: This metric considers a prediction to be correct only when the predicted label for the complete entity matches exactly the same words as the gold label of that entity, including exact character offsets [1] [4]. This means that both the content and the entity boundaries must align perfectly with the human-annotated gold standard.
   - **Relaxed F1**: In contrast, relaxed F1 permits a prediction to be considered correct as long as part of the named entity is identified correctly, without requiring an exact match [1]. This metric is useful in scenarios where different systems may have varying segmentation techniques, allowing for some flexibility in boundary detection while still rewarding accurate identification of entity components.

2. **Handling of Segmentation Errors**:
   - **Strict F1** evaluates performance with a zero-tolerance policy towards segmentation errors. If there are any discrepancies in the boundaries of identified entities, the prediction is deemed incorrect, thus reflecting a more stringent evaluation [4].
   - **Relaxed F1** is designed to accommodate such discrepancies, meaning it can yield more favorable evaluations for systems that might struggle with boundary identification but still correctly recognize parts of entities. This makes relaxed F1 particularly useful when working with datasets where segmentation may vary significantly, such as in multilingual or domain-specific contexts [1] [4].

3. **Implications for Micro F-score Calculation**:
   - Both metrics utilize precision and recall in their calculations, but the conditions under which true positives are counted differ, impacting the overall F-score significantly. The relaxed approach potentially increases the number of true positives, thus raising the F1 score compared to strict evaluations, which might penalize a system harshly for minor boundary errors [4].

In conclusion, strict F1 serves well in environments that prioritize precise matches and strict boundaries, while relaxed F1 provides a more lenient evaluation suitable for scenarios where segmentation irregularities are prevalent. Each metric offers distinct advantages depending on the specific requirements and characteristics of the NER tasks at hand.

1. [1]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
2. [2]:  https://ar5iv.org/html/2404.05587, No Title
3. [3]:  https://ar5iv.org/html/2404.05587, No Title
4. [4]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
5. [5]:  https://ar5iv.org/html/2303.06944, [2303.06944] A Human Subject Study of Named Entity Recognition (NER) in Conversational Music Recommendation Queries
---
1. [1]:  Passage ID 1: Tjong Kim Sang (2002] considers a prediction to be correct only when the predicted label for the complete entity is matched to exactly the same words as the gold label of that entity. CoNLL also used (micro) F-score, taking the harmonic mean of the exact match precision and recall.The relaxed F1 and strict F1 metrics have been used in many NER shared tasks [Segura Bedmar et al. (2013, Krallinger et al. (2015, Bossy et al. (2013, Delėger et al. (2016].Relaxed F1 considers a prediction to be correct as long as part of the named entity is identified correctly.Strict F1 requires the character offsets of a prediction and the human annotation to match exactly.In these data, unlike CoNLL, word offsets are not given, so relaxed F1 is intended to allow comparison despite different systems having different word boundaries due to different segmentation techniques [Liu et al. (2015].6 NER systems6.1 Knowledge-based systemsKnowledge-based NER systems do not require annotated
2. [2]:  Passage ID 2: NER), our methodology showed a significant improvement of +10% in F1 performance compared to our competitors, demonstrating the effectiveness of our approach in a low data regime (Table 2). For Subtask 3 (Relation Extraction), our LLM Single-Question Answering model further improved the F1 score by 5.1%, building on the already competent performance of the heuristic baseline and highlighting the advantage of our method. Furthermore, it has been demonstrated that using 7-10 samples is the most effective strategy, as it optimises the balance between input complexity and model performance.This analysis highlights the potential of utilising advanced LLM techniques and carefully selected retrieval methods to significantly improve the accuracy and efficiency of NER tasks in specialised domains.Table 2: SOMD Performance Rankings (Weighted Average
3. [3]:  Passage ID 3: which uses SciBERT, achieved a solid foundation with a 59.9% F1 score. However, LLMs that use random samples without fine-tuning showed a decrease in performance, with the highest F1 score reaching only 57.4%.A closer examination of retrieval-based models indicates that LLMs perform better. The highest F1 score of 67.9% was achieved by sentence similarity retrievalmodels, while entity retrieval showed the best performance at 67.7% F1.The transition from GPT 3.5 to GPT 4 models resulted in a significant improvement of approximately 3–5%, although it required around three times more computation time. Notably, our best models were able to perform within a mere 3% below the theoretical maximum by utilizing SciBERT for sentence selection, leveraging oracle positive sentences.In Subtask 2 (Attributive NER), our methodology showed a significant improvement of +10% in F1 performance compared to our competitors, demonstrating the effectiveness of our approach in a low data regime (Table 2).
4. [4]:  Passage ID 4: of various NER systems developed for such NER datasets with a focus on chemical NER.5 NER evaluation metrics?) scored NER performance based on type, whether the predicted label was correct regardless of entity boundaries, and text, whether the predicted entity boundaries were correct regardless of the label.For each score category,precision was defined as the number of entities a system predicted correctly divided by the number that the system predicted,recall was defined as the number of entities a system predicted correctly divided by the number that were identified by the human annotators,and (micro) F-score was defined as the harmonic mean of precision and recall from both type and text.The exact match metrics introduced by CoNLL [Tjong Kim Sang andDe Meulder (2003, Tjong Kim Sang (2002] considers a prediction to be correct only when the predicted label for the complete entity is matched to exactly the same words as the gold label of that entity. CoNLL also used
5. [5]:  Passage ID 5: optimizing on F1.Also, BERT has a lower precision than the recall, but we see the opposite for humans.Considering Equations 1 and 2, the model appears to hypothesize spurious entities more often, while humans tend to miss entities more often.Table 5 also shows that the F1 scores under Exact and Entity schemes are larger than under Strict as some of the errors produced are because of segmentation or typing.However, we can notice a different behaviour for the two entity types for both BERT and humans.In the case of WoA, the Entity F1 scores are slightly larger than those obtained under the Exact schema, showing that boundary errors happen more frequently.On the contrary, for Artist entities, the segmentation is more often correct, but the typing is wrong.5.3 Error AnalysisFigure 1: Normalized correct, incorrect, missed and spurious outcomes per entity type under strict.Figure 1, showing a detailed error analysis, confirms that indeed BERT has more often spurious