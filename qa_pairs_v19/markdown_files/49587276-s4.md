# A Survey on Recent Advances in Named Entity Recognition from Deep Learning models

## Question

What are the differences between relaxed F1 and strict F1 metrics in evaluating NER predictions?

## URLs

1. https://ar5iv.org/html/2402.14948
2. https://ar5iv.org/html/2406.18049
3. https://ar5iv.org/html/2309.16150
4. https://ar5iv.org/html/2303.16416
5. https://ar5iv.org/html/2404.09135
6. https://ar5iv.org/html/1910.11470
7. https://ar5iv.org/html/2402.03832
8. https://ar5iv.org/html/2101.00160
9. https://ar5iv.org/html/2303.06944

## Answer

## I don't know?: False

The differences between relaxed F1 and strict F1 metrics in evaluating Named Entity Recognition (NER) predictions primarily revolve around how these metrics define correctness in entity prediction.

1. **Definition of Correctness**: 
   - The relaxed F1 metric considers a prediction to be correct as long as it identifies part of a named entity correctly, meaning that even if the predicted span overlaps partially or is only a part of the entire entity, it is taken as a correct prediction [1][2]. This flexibility allows for a broader interpretation of what constitutes a correct identification.
   - In contrast, the strict F1 metric sets a much higher bar for correctness as it requires the predicted span to match exactly with the human-annotated spans, meaning that both the character offsets and the words must be identical [2]. This exact-match requirement is more stringent, implying that the prediction must be perfectly aligned with the reference label to be considered correct.

2. **Application Context**:
   - The relaxed metrics are particularly useful in practical applications where varying segmentation techniques may lead to different word boundaries, thus allowing for a fair comparison between systems that may not segment text in the same way [2]. This is especially relevant in NER tasks where the granularity of entity identification can vary.
   - The strict metrics are more relevant in situations where accuracy in prediction is critical and exact matches are necessary, such as in high-stakes environments or when precise entity retrieval is required [2]. They align more closely with traditional evaluation methods that emphasize exactness.

3. **Performance Implications**:
   - Models evaluated with the relaxed F1 score may exhibit higher performance metrics because they can capitalize on overlaps, possibly leading to higher recall even if they sometimes misclassify the full entity [1]. For example, a model's ability to classify an ADE (Adverse Drug Event) might be assessed more positively if it captures parts of the entity.
   - Conversely, models assessed using strict F1 scores may display lower overall performance if they struggle to achieve perfect matches, as the strict criteria will penalize them more heavily for slight misalignments or errors [1]. The RoBERTa model mentioned in the provided context performed better in terms of strict F1, indicating its capacity for more accurate span identification than other models.

In summary, relaxed F1 metrics offer a more lenient evaluation approach that recognizes partial correctness, while strict F1 metrics impose a rigorous standard that only awards correct predictions when spans match perfectly. The choice of metric can significantly impact the evaluation of an NER system, influencing insights into model performance and areas for improvement.

1. [1]:  https://ar5iv.org/html/2104.07367, [2104.07367] BERT based Transformers lead the way in Extraction of Health Information from Social Media
2. [2]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
3. [3]:  https://ar5iv.org/html/2404.05587, No Title
4. [4]:  https://ar5iv.org/html/2404.05587, No Title
5. [5]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
---
1. [1]:  Passage ID 1: with two metrics, the Relaxed F1 score, and the Strict F1 score. The Relaxed metrics evaluate the scores for spans that have a partial or full overlap with the labels. The Strict metrics only evaluate the cases where the spans produced by the model perfectly match the span in the label.Table 3 showcases the performance of both NER pipelines on the validation set. It can be observed that the RoBERTa model provides a higher F1 score than the statistical model and is able to make much more accurate classifications of the ADE class. The statistical model however provides a higher recall which indicates it has fewer false negatives and is thus misclassifying the ADE samples as NoADE less often. The RoBERTa model is however far superior to the statistical model when considering the strict F1 scores. This implies that it is able to produce a perfect span more often and has learnt a better representation of the data.The final test set result achieved by the model placed on the
2. [2]:  Passage ID 2: Tjong Kim Sang (2002] considers a prediction to be correct only when the predicted label for the complete entity is matched to exactly the same words as the gold label of that entity. CoNLL also used (micro) F-score, taking the harmonic mean of the exact match precision and recall.The relaxed F1 and strict F1 metrics have been used in many NER shared tasks [Segura Bedmar et al. (2013, Krallinger et al. (2015, Bossy et al. (2013, Delėger et al. (2016].Relaxed F1 considers a prediction to be correct as long as part of the named entity is identified correctly.Strict F1 requires the character offsets of a prediction and the human annotation to match exactly.In these data, unlike CoNLL, word offsets are not given, so relaxed F1 is intended to allow comparison despite different systems having different word boundaries due to different segmentation techniques [Liu et al. (2015].6 NER systems6.1 Knowledge-based systemsKnowledge-based NER systems do not require annotated
3. [3]:  Passage ID 3: NER), our methodology showed a significant improvement of +10% in F1 performance compared to our competitors, demonstrating the effectiveness of our approach in a low data regime (Table 2). For Subtask 3 (Relation Extraction), our LLM Single-Question Answering model further improved the F1 score by 5.1%, building on the already competent performance of the heuristic baseline and highlighting the advantage of our method. Furthermore, it has been demonstrated that using 7-10 samples is the most effective strategy, as it optimises the balance between input complexity and model performance.This analysis highlights the potential of utilising advanced LLM techniques and carefully selected retrieval methods to significantly improve the accuracy and efficiency of NER tasks in specialised domains.Table 2: SOMD Performance Rankings (Weighted Average
4. [4]:  Passage ID 4: which uses SciBERT, achieved a solid foundation with a 59.9% F1 score. However, LLMs that use random samples without fine-tuning showed a decrease in performance, with the highest F1 score reaching only 57.4%.A closer examination of retrieval-based models indicates that LLMs perform better. The highest F1 score of 67.9% was achieved by sentence similarity retrievalmodels, while entity retrieval showed the best performance at 67.7% F1.The transition from GPT 3.5 to GPT 4 models resulted in a significant improvement of approximately 3–5%, although it required around three times more computation time. Notably, our best models were able to perform within a mere 3% below the theoretical maximum by utilizing SciBERT for sentence selection, leveraging oracle positive sentences.In Subtask 2 (Attributive NER), our methodology showed a significant improvement of +10% in F1 performance compared to our competitors, demonstrating the effectiveness of our approach in a low data regime (Table 2).
5. [5]:  Passage ID 5: of various NER systems developed for such NER datasets with a focus on chemical NER.5 NER evaluation metrics?) scored NER performance based on type, whether the predicted label was correct regardless of entity boundaries, and text, whether the predicted entity boundaries were correct regardless of the label.For each score category,precision was defined as the number of entities a system predicted correctly divided by the number that the system predicted,recall was defined as the number of entities a system predicted correctly divided by the number that were identified by the human annotators,and (micro) F-score was defined as the harmonic mean of precision and recall from both type and text.The exact match metrics introduced by CoNLL [Tjong Kim Sang andDe Meulder (2003, Tjong Kim Sang (2002] considers a prediction to be correct only when the predicted label for the complete entity is matched to exactly the same words as the gold label of that entity. CoNLL also used