# A Survey on Recent Advances in Named Entity Recognition from Deep Learning models

## Question

What are the differences between relaxed F1 and strict F1 metrics in evaluating NER predictions?

## URLs

1. https://ar5iv.org/html/2402.14948. [2402.14948] Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach
2. https://ar5iv.org/html/2406.18049. [2406.18049] Improving Entity Recognition Using Ensembles of Deep Learning and Fine-tuned Large Language Models: A Case Study on Adverse Event Extraction from Multiple Sources
3. https://ar5iv.org/html/2309.16150. [2309.16150] AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events
4. https://ar5iv.org/html/2303.16416. [2303.16416] Improving Large Language Models for Clinical Named Entity Recognition via Prompt Engineering
5. https://ar5iv.org/html/2404.09135. [2404.09135] Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions11footnote 1This research was funded by supported by National Key R&D Program of China (No. 2021YFF0901400)
6. https://ar5iv.org/html/1910.11470. [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
7. https://ar5iv.org/html/2402.03832. [2402.03832] Rethinking Skill Extraction in the Job Market Domain using Large Language Models
8. https://ar5iv.org/html/2101.00160. [2101.00160] How Do Your Biomedical Named Entity Recognition Models Generalize to Novel Entities?
9. https://ar5iv.org/html/2303.06944. [2303.06944] A Human Subject Study of Named Entity Recognition (NER) in Conversational Music Recommendation Queries

## Answer (I don't know?: ‚úó)

The relaxed F1 and strict F1 metrics are two different approaches used to evaluate Named Entity Recognition (NER) predictions, each with distinct criteria for considering a prediction correct.

1. **Definition of Correct Predictions**:
   - **Strict F1**: This metric requires that the predicted label for the complete named entity matches exactly with the gold label. This means that both the label and the character offsets of the predicted and the gold-standard entities must be identical for the prediction to be deemed correct [1] [2].
   - **Relaxed F1**: In contrast, relaxed F1 allows for part of the named entity to be identified correctly. Thus, it is less stringent than strict F1, permitting some flexibility in entity boundaries. A prediction can be marked as correct even if it does not match the gold label entirely, as long as a portion of it is captured correctly [1].

2. **Application and Context**:
   - The relaxed F1 metric is particularly useful in scenarios where exact word boundaries may vary due to different segmentation techniques employed by various NER systems. This flexibility helps facilitate comparisons across systems with different boundary interpretations [1]. 
   - Strict F1 is often used in contexts where precise labeling and boundary recognition are critical. This is important for guaranteeing that identified entities align perfectly with the expected outcomes [1] [2].

3. **Impact on Evaluation**:
   - Since strict F1 can lead to many entities being classified as incorrect due to minor mismatches, it often reflects a more conservative view on system performance. It highlights systems that accurately capture not just the entities but also their exact presentation [3].
   - Relaxed F1, on the other hand, generally results in higher performance scores because it captures more true positives by allowing partial correctness, which can be beneficial in practical applications where some level of error is acceptable [4] [5].

In summary, the primary difference between relaxed F1 and strict F1 lies in their definitions of correctness in predictions: relaxed F1 allows for partial matches, accommodating variations in entity boundaries, while strict F1 demands exact matches for both label and offsets, leading to more stringent evaluations.

1. [1]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
2. [2]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
3. [3]:  https://ar5iv.org/html/2303.06944, [2303.06944] A Human Subject Study of Named Entity Recognition (NER) in Conversational Music Recommendation Queries
4. [4]:  https://ar5iv.org/html/2402.03832, [2402.03832] Rethinking Skill Extraction in the Job Market Domain using Large Language Models
5. [5]:  https://ar5iv.org/html/2402.03832, [2402.03832] Rethinking Skill Extraction in the Job Market Domain using Large Language Models
---
1. [1]:  Passage ID 1: Tjong Kim¬†Sang (2002] considers a prediction to be correct only when the predicted label for the complete entity is matched to exactly the same words as the gold label of that entity. CoNLL also used (micro) F-score, taking the harmonic mean of the exact match precision and recall.The relaxed F1 and strict F1 metrics have been used in many NER shared tasks [Segura¬†Bedmar et al. (2013, Krallinger et al. (2015, Bossy et al. (2013, Delƒóger et al. (2016].Relaxed F1 considers a prediction to be correct as long as part of the named entity is identified correctly.Strict F1 requires the character offsets of a prediction and the human annotation to match exactly.In these data, unlike CoNLL, word offsets are not given, so relaxed F1 is intended to allow comparison despite different systems having different word boundaries due to different segmentation techniques [Liu et al. (2015].6 NER systems6.1 Knowledge-based systemsKnowledge-based NER systems do not require annotated
2. [2]:  Passage ID 2: of various NER systems developed for such NER datasets with a focus on chemical NER.5 NER evaluation metrics?) scored NER performance based on type, whether the predicted label was correct regardless of entity boundaries, and text, whether the predicted entity boundaries were correct regardless of the label.For each score category,precision was defined as the number of entities a system predicted correctly divided by the number that the system predicted,recall was defined as the number of entities a system predicted correctly divided by the number that were identified by the human annotators,and (micro) F-score was defined as the harmonic mean of precision and recall from both type and text.The exact match metrics introduced by CoNLL [Tjong Kim¬†Sang andDe¬†Meulder (2003, Tjong Kim¬†Sang (2002] considers a prediction to be correct only when the predicted label for the complete entity is matched to exactly the same words as the gold label of that entity. CoNLL also used
3. [3]:  Passage ID 3: optimizing on F1.Also, BERT has a lower precision than the recall, but we see the opposite for humans.Considering Equations 1 and 2, the model appears to hypothesize spurious entities more often, while humans tend to miss entities more often.Table 5 also shows that the F1 scores under Exact and Entity schemes are larger than under Strict as some of the errors produced are because of segmentation or typing.However, we can notice a different behaviour for the two entity types for both BERT and humans.In the case of WoA, the Entity F1 scores are slightly larger than those obtained under the Exact schema, showing that boundary errors happen more frequently.On the contrary, for Artist entities, the segmentation is more often correct, but the typing is wrong.5.3 Error AnalysisFigure 1: Normalized correct, incorrect, missed and spurious outcomes per entity type under strict.Figure 1, showing a detailed error analysis, confirms that indeed BERT has more often spurious
4. [4]:  Passage ID 4: higher, showing that LLMs are able to localize the skills within a sentence, but fail to capture the exact sequence. Concisely, we have the following findings:Few-shot demonstrations are critical to model performance,¬† with an average improvement of 20.0% for Extract-style and 28% for Ner-style in F1 when providing 5-shot demonstrations. Indeed, given the strict structure required by NER, it is essential to show the model examples of the specific output format, especially for Ner-style format.Extract-style outperforms Ner-style prompting on average, especially with the Relax evaluation scheme, in contrast to the findings of Wang et¬†al. (2023b).kùëòkNN-retrieval outperforms random selection of demonstrations across all benchmarks except Skillspan. On average, switching from random to kùëòkNN demonstrations slightly improves the precision but greatly improves the recall (4% increases for both Extract-style and Ner-style).Dataset-specific prompts improve the performance on average,
5. [5]:  Passage ID 5: prompts to guide the model toward the desired type of entities to extract. The full table with precision and recall can be found in appendix (Table 7).There is a large drop in performance across most datasets compared to fine-tuning models, for both the Extract and Ner-style approaches. Aside from Fijo, in which GPT-3.5 with in-context learning achieves a comparable performance, in all other datasets the decrease margin is significant, with up to 50% decreases in F1 metrics.The datasets with the largest performance drop are Gnehm and Sayfullina. We hypothesize that pre-trained models achieve better performances when the length of span entities is rather short (Table 2).The Relax metric shows much higher performance (up to 20% higher average F1 score). In particular the recall is considerably higher, showing that LLMs are able to localize the skills within a sentence, but fail to capture the exact sequence. Concisely, we have the following findings:Few-shot demonstrations are