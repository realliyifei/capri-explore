# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

What are the issues and limitations of using the coefficient S for measuring inter-coder agreement?

## URLs

1. https://ar5iv.org/html/1806.05821. [1806.05821] Measuring intergroup agreement and disagreement
2. https://ar5iv.org/html/2107.11449. [2107.11449] Applying Inter-rater Reliability and Agreement in Grounded Theory Studies in Software Engineering
3. https://ar5iv.org/html/2008.00977. [2008.00977] Reliability in Software Engineering Qualitative Research through Inter-Coder Agreement A guide using Krippendorff’s 𝛼 & Atlas.ti
4. https://ar5iv.org/html/2410.05291. [2410.05291] Liberal-Conservative Hierarchies of Intercoder Reliability Estimators
5. https://ar5iv.org/html/2402.08069. [2402.08069] Interrater agreement statistics under the two-rater dichotomous-response case with correlated decisions
6. https://ar5iv.org/html/2410.03775. [2410.03775] Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge
7. https://ar5iv.org/html/2307.08153. [2307.08153] Analyzing Dataset Annotation Quality Management in the Wild
8. https://ar5iv.org/html/2103.12170. [2103.12170] krippendorffsalpha: An R Package for Measuring Agreement Using Krippendorff’s Alpha Coefficient

## Answer (I don't know?: ✓)

I don't know. The context provided does not contain specific information about the coefficient S or discuss its issues and limitations regarding measuring inter-coder agreement. The passages focus on general aspects of inter-coder agreement, validity and reliability of coding in qualitative research, and the methodologies used in empirical software engineering studies, but they do not elaborate on the coefficients themselves or detail particular limitations associated with any specific coefficients, including S.

1. [1]:  https://ar5iv.org/html/2008.00977, [2008.00977] Reliability in Software Engineering Qualitative Research through Inter-Coder Agreement A guide using Krippendorff’s 𝛼 & Atlas.ti
2. [2]:  https://ar5iv.org/html/2008.00977, [2008.00977] Reliability in Software Engineering Qualitative Research through Inter-Coder Agreement A guide using Krippendorff’s 𝛼 & Atlas.ti
3. [3]:  https://ar5iv.org/html/2107.11449, [2107.11449] Applying Inter-rater Reliability and Agreement in Grounded Theory Studies in Software Engineering
4. [4]:  https://ar5iv.org/html/2008.00977, [2008.00977] Reliability in Software Engineering Qualitative Research through Inter-Coder Agreement A guide using Krippendorff’s 𝛼 & Atlas.ti
5. [5]:  https://ar5iv.org/html/2402.08069, [2402.08069] Interrater agreement statistics under the two-rater dichotomous-response case with correlated decisions
---
1. [1]:  Passage ID 1: López-FernándezETSI de Sistemas Informáticos, Universidad Politécnica de Madrid. C. Alan Turing s/n, 28031. Madrid, Spain.daniel.lopez@upm.esAbstract.In recent years, the research on empirical software engineering that uses qualitative data analysis (e.g., cases studies, interview surveys, and grounded theory studies) is increasing. However, most of this research does not deep into the reliability and validity of findings, specifically in the reliability of coding in which these methodologies rely on, despite there exist a variety of statistical techniques known as Inter-Coder Agreement (ICA) for analyzing consensus in team coding. This paper aims to establish a novel theoretical framework that enables a methodological approach for conducting this validity analysis. This framework is based on a set of coefficients for measuring the degree of agreement that different coders achieve when judging a common matter. We analyze different reliability coefficients and provide detailed
2. [2]:  Passage ID 2: used, such as inter-coder, inter-rater or inter-judge agreement, according to the customary of the topic.However, although more and more researchers apply inter-rater agreement to assess the validity of their results in systematic literature reviews and mapping studies as we analyzed in [32], it is a fact that few studies in software engineering analyze and test reliability and trustworthiness of coding, and thus, the validity of their findings. A systematic search in the main scientific repositories (namely, ACM Digital Library, Science Direct and Springer111The search string used is ‘‘inter-coder agreement AND software engineering AND qualitative research’’.) returns no more than 25 results. Similar results were obtained in a systematic literature review reported by Nili et al. [30] in information management research. Nevertheless, the amount of publications that test the reliability of their coding, and consequently, of their findings is notably higher in other areas, specially
3. [3]:  Passage ID 3: pass, and 25 of the 230 artifacts and 6 of the 60 videos, at two stages in the development of the code books. We calculated the inter-coder reliability ratio as the number of agreements divided by the total number of codes […]”ID128 To test the inter-coder reliability, the primary researcher coded all 154 records, and subsequently the second coder coded every fifth record in the dataset. Cohen’s Kappa coefficient was found to be 0.84, indicating high agreement between the two codersID55, ID59, ID65, and ID89 are the only ones of the few to lightly describe the expertise of raters involved in the coding process, but none of them mention a specific training on the coding process.ID55 “To avoid the researcher’s bias, we have performed an inter-rater reliability test between mapping team and indented experts […]”ID59 “The initial team of paper taggers was made up of seven post-docs and graduate students with some association to the University of Trento and some
4. [4]:  Passage ID 4: measure the extend of the agreement/disagreement between several judges when they subjectively interpret a common reality. In this way, these coefficients allow researchers to establish a value of reliability of the coding that will be analyzed later to infer relations and to lead to conclusions. Coding is reliable if coders can be shown to agree on the categories assigned to units to an extent determined by the purposes of the study [5].The ICA techniques are used in many different research context both from social sciences and engineering. For instance, these methods have been applied for evaluating the selection criteria of primary studies or data extraction in systematic literature reviews and the ratings when coding qualitative data. This has led to a variety of related terms interchangeably used, such as inter-coder, inter-rater or inter-judge agreement, according to the customary of the topic.However, although more and more researchers apply inter-rater agreement to assess
5. [5]:  Passage ID 5: and Clinical Psychology 82.6American Psychological Association, 2014, pp. 1219[22]Guangchao Charles Feng“Underlying determinants driving agreement among coders”In Quality & Quantity 47.5Springer, 2013, pp. 2983–2997[23]Guangchao Charles Feng“Factors affecting intercoder reliability: A Monte Carlo experiment”In Quality & Quantity 47.5Springer, 2013, pp. 2959–2982[24]Mousumi Banerjee, Michelle Capozzoli, Laura McSweeney and Debajyoti Sinha“Beyond kappa: A review of interrater agreement measures”In Canadian Journal of Statistics 27.1Wiley Online Library, 1999, pp. 3–23[25]Joseph L Fleiss“Measuring agreement between two judges on the presence or absence of a trait”In BiometricsJSTOR, 1975, pp. 651–659[26]J Richard Landis and Gary G Koch“A review of statistical methods in the analysis of data arising from observer reliability studies (Part I)”In Statistica Neerlandica 29.3Wiley Online Library, 1975, pp. 101–123[27]Klaus