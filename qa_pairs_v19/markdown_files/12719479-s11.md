# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

What are the issues and limitations of using the coefficient S for measuring inter-coder agreement?

## URLs

1. https://ar5iv.org/html/1806.05821. [1806.05821] Measuring intergroup agreement and disagreement
2. https://ar5iv.org/html/2107.11449. [2107.11449] Applying Inter-rater Reliability and Agreement in Grounded Theory Studies in Software Engineering
3. https://ar5iv.org/html/2008.00977. [2008.00977] Reliability in Software Engineering Qualitative Research through Inter-Coder Agreement A guide using Krippendorff‚Äôs ùõº & Atlas.ti
4. https://ar5iv.org/html/2410.05291. [2410.05291] Liberal-Conservative Hierarchies of Intercoder Reliability Estimators
5. https://ar5iv.org/html/2402.08069. [2402.08069] Interrater agreement statistics under the two-rater dichotomous-response case with correlated decisions
6. https://ar5iv.org/html/2410.03775. [2410.03775] Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge
7. https://ar5iv.org/html/2307.08153. [2307.08153] Analyzing Dataset Annotation Quality Management in the Wild
8. https://ar5iv.org/html/2103.12170. [2103.12170] krippendorffsalpha: An R Package for Measuring Agreement Using Krippendorff‚Äôs Alpha Coefficient

## Answer (I don't know?: ‚úì)

I don't know. The context provided does not contain any specific information regarding the coefficient S or its issues and limitations in measuring inter-coder agreement. The passages mainly discuss general topics related to inter-coder agreement, the use of statistical techniques for reliability in qualitative data analysis, and the lack of rigorous application of these methods in software engineering. However, there is no detailed analysis or mention of any specific coefficients, including S, or their advantages and disadvantages.

1. [1]:  https://ar5iv.org/html/1907.10450, No Title
2. [2]:  https://ar5iv.org/html/2008.00977, [2008.00977] Reliability in Software Engineering Qualitative Research through Inter-Coder Agreement A guide using Krippendorff‚Äôs ùõº & Atlas.ti
3. [3]:  https://ar5iv.org/html/2008.00977, [2008.00977] Reliability in Software Engineering Qualitative Research through Inter-Coder Agreement A guide using Krippendorff‚Äôs ùõº & Atlas.ti
4. [4]:  https://ar5iv.org/html/2008.00977, [2008.00977] Reliability in Software Engineering Qualitative Research through Inter-Coder Agreement A guide using Krippendorff‚Äôs ùõº & Atlas.ti
5. [5]:  https://ar5iv.org/html/2008.00977, [2008.00977] Reliability in Software Engineering Qualitative Research through Inter-Coder Agreement A guide using Krippendorff‚Äôs ùõº & Atlas.ti
---
1. [1]:  Passage ID 1: to the performance of video indexing methods based on supervised learning.Moreover, we suppose that inter-coder agreement might form an upper bound for such methods. In this regard, some first experiments are presented to predict average precision using support vector regression based on inter-coder agreement and training data size.Furthermore, the influence of image annotation quality induced by experts vs. non-experts on the person recognition performance is investigated based on annotations acquired during the study.The remainder of the paper is organized as follows. Section 2 discusses related work regarding inter-annotator studies. Section 3 deals with the comprehensive user study including a description of the used dataset, the study participants, the experimental design and the results of a comparison between expert vs. non-expert inter-coder agreements. Furthermore, a performance prediction for the task of concept classification is presented based on inter-coder agreement
2. [2]:  Passage ID 2: L√≥pez-Fern√°ndezETSI de Sistemas Inform√°ticos, Universidad Polit√©cnica de Madrid. C.¬†Alan Turing s/n, 28031. Madrid, Spain.daniel.lopez@upm.esAbstract.In recent years, the research on empirical software engineering that uses qualitative data analysis (e.g., cases studies, interview surveys, and grounded theory studies) is increasing. However, most of this research does not deep into the reliability and validity of findings, specifically in the reliability of coding in which these methodologies rely on, despite there exist a variety of statistical techniques known as Inter-Coder Agreement (ICA) for analyzing consensus in team coding. This paper aims to establish a novel theoretical framework that enables a methodological approach for conducting this validity analysis. This framework is based on a set of coefficients for measuring the degree of agreement that different coders achieve when judging a common matter. We analyze different reliability coefficients and provide detailed
3. [3]:  Passage ID 3: L√≥pez-Fern√°ndezETSI de Sistemas Inform√°ticos, Universidad Polit√©cnica de Madrid. C.¬†Alan Turing s/n, 28031. Madrid, Spain.daniel.lopez@upm.esAbstract.In recent years, the research on empirical software engineering that uses qualitative data analysis (e.g., cases studies, interview surveys, and grounded theory studies) is increasing. However, most of this research does not deep into the reliability and validity of findings, specifically in the reliability of coding in which these methodologies rely on, despite there exist a variety of statistical techniques known as Inter-Coder Agreement (ICA) for analyzing consensus in team coding. This paper aims to establish a novel theoretical framework that enables a methodological approach for conducting this validity analysis. This framework is based on a set of coefficients for measuring the degree of agreement that different coders achieve when judging a common matter. We analyze different reliability coefficients and provide detailed
4. [4]:  Passage ID 4: used, such as inter-coder, inter-rater or inter-judge agreement, according to the customary of the topic.However, although more and more researchers apply inter-rater agreement to assess the validity of their results in systematic literature reviews and mapping studies as we analyzed in [32], it is a fact that few studies in software engineering analyze and test reliability and trustworthiness of coding, and thus, the validity of their findings. A systematic search in the main scientific repositories (namely, ACM Digital Library, Science Direct and Springer111The search string used is ‚Äò‚Äòinter-coder agreement AND software engineering AND qualitative research‚Äô‚Äô.) returns no more than 25 results. Similar results were obtained in a systematic literature review reported by Nili et al. [30] in information management research. Nevertheless, the amount of publications that test the reliability of their coding, and consequently, of their findings is notably higher in other areas, specially
5. [5]:  Passage ID 5: used, such as inter-coder, inter-rater or inter-judge agreement, according to the customary of the topic.However, although more and more researchers apply inter-rater agreement to assess the validity of their results in systematic literature reviews and mapping studies as we analyzed in [32], it is a fact that few studies in software engineering analyze and test reliability and trustworthiness of coding, and thus, the validity of their findings. A systematic search in the main scientific repositories (namely, ACM Digital Library, Science Direct and Springer111The search string used is ‚Äò‚Äòinter-coder agreement AND software engineering AND qualitative research‚Äô‚Äô.) returns no more than 25 results. Similar results were obtained in a systematic literature review reported by Nili et al. [30] in information management research. Nevertheless, the amount of publications that test the reliability of their coding, and consequently, of their findings is notably higher in other areas, specially