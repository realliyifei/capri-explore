# Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

What are the current limitations and challenges faced by identification models in recognizing discontiguous and overlapping multiword expressions (MWEs)?

## URLs

1. https://ar5iv.org/html/1809.03056. [1809.03056] SHOMA at Parseme Shared Task on Automatic Identification of VMWEs: Neural Multiword Expression Tagging with High Generalisation

## Answer (I don't know?: ✗)

Current identification models face several significant limitations and challenges when recognizing discontiguous and overlapping multiword expressions (MWEs). These challenges stem from the intrinsic properties of MWEs, particularly their complexity, variability, and ambiguity.

1. **Discontinuity and Syntax Variability:** One of the primary issues identified in parsing MWEs is their discontinuous nature. These expressions can often span gaps or consist of words that do not appear together in a typical sequential manner, which complicates their identification. For instance, the systems evaluated showed underperformance specifically for discontiguous expressions, which resulted in lower accuracy rates for identifying such MWEs, as analyzed in the context of verbal constructions [3]. Additionally, MWEs exhibit syntactic variability, meaning they can take on different forms and structures, further complicating detection [1].

2. **Ambiguity:** The ambiguity associated with MWEs poses another challenge. MWEs frequently have meanings that are not easily deduced from their individual components, leading to parsing failures when models fail to recognize these non-compositional elements. Research indicates that simply compiling lists of potential MWEs does not suffice due to their complex characteristics, including their inherent ambiguity [1]. This multi-faceted property makes it difficult for identification models to decide when a sequence of words should be considered as a single expression.

3. **Performance on Unseen Data:** While current methodologies have shown some success, particularly with pre-trained embeddings, the systems still struggle with generalization, especially concerning previously unseen MWEs. Although the models may perform well on known MWEs, their ability to adapt and correctly identify new types of MWEs remains a significant hurdle [2]. The performance on unseen entries can fluctuate significantly, which highlights a limitation in the robustness of these systems [3].

4. **Inefficient Generalization:** The issue of generalization also correlates with the architectural choices in these models. While employing architectures that incorporate convolutional and recurrent neural networks (such as ConvNets and LSTMs) has yielded improvements, they still do not necessarily guarantee better detection rates for all types of MWEs. The addition of auxiliary layers such as Conditional Random Fields (CRFs) does not always result in enhanced performance, indicating that fine-tuning these architectures to handle the subtleties of MWEs is still a work in progress [3].

5. **Domain-Specific Knowledge:** Many models also rely on domain-specific knowledge or contextual features which may not be broadly applicable across different datasets or language pairs. The lack of task-specific domain knowledge, beyond generic part-of-speech (POS) tags and pre-trained word embeddings, restricts models from achieving comprehensive generalizability [4]. This introduces the need for a more nuanced understanding of context in which MWEs operate.

In summary, the challenges of recognizing discontiguous and overlapping MWEs in NLP arise from their inherent syntactic and semantic complexities, performance variability with unseen data, inefficiencies in current model architectures, and struggles with generalization without extensive domain knowledge. Addressing these limitations is crucial for advancing the capabilities of identification models in this field.

1. [1]:  https://ar5iv.org/html/1809.03056, [1809.03056] SHOMA at Parseme Shared Task on Automatic Identification of VMWEs: Neural Multiword Expression Tagging with High Generalisation
2. [2]:  https://ar5iv.org/html/1809.03056, [1809.03056] SHOMA at Parseme Shared Task on Automatic Identification of VMWEs: Neural Multiword Expression Tagging with High Generalisation
3. [3]:  https://ar5iv.org/html/1809.03056, [1809.03056] SHOMA at Parseme Shared Task on Automatic Identification of VMWEs: Neural Multiword Expression Tagging with High Generalisation
4. [4]:  https://ar5iv.org/html/1809.03056, [1809.03056] SHOMA at Parseme Shared Task on Automatic Identification of VMWEs: Neural Multiword Expression Tagging with High Generalisation
5. [5]:  https://ar5iv.org/html/1809.03056, [1809.03056] SHOMA at Parseme Shared Task on Automatic Identification of VMWEs: Neural Multiword Expression Tagging with High Generalisation
---
1. [1]:  Passage ID 1: and grammar [Green et al., 2011, Constant et al., 2017].?) have reported a notable amount of parsing failures caused by missing MWEs. Simply listing all the potential non-compositional MWEs is not enough due to their complex characteristics including discontinuity, heterogeneity, syntactic variability and most importantly their ambiguity [Constant et al., 2017].MWEs pose greater challenges when processing running texts which has recently attracted more attention [Qu et al., 2015, Schneider et al., 2016].The Parseme shared task on automatic identification of verbal multiword expressions (VMWEs) edition 1.1 [Ramisch et al., 2018] aims at studying automatic labelling of VMWEs in their contexts. We propose a system that can model the VMWE-annotated training data and predict MWEhood labels for blind test data.Previous edition of VMWE shared task [Savary et al., 2017] features successful systems such as the transition-based [Al Saied et al., 2017] and the CRF-based systems [Maldonado
2. [2]:  Passage ID 2: system participated in the open track of the Parseme shared task on automatic identification of verbal MWEs due to the use of pre-trained wikipedia word embeddings. It outperformed all participating systems in both open and closed tracks with the overall macro-average MWE-based F1 score of 58.09 averaged among all languages.A particular strength of the system is its superior performance on unseen data entries.1 IntroductionMultiword Expressions (MWEs) are linguisticunits consisting of more than one word that arestructurally and semantically idiosyncratic. Asthey cross word boundaries [Sag et al., 2002] and demonstrate multifaceted properties [Tsvetkov andWintner, 2014], MWEs don’t fit well in thetraditional grammar descriptions where there is a distinct line between lexicon and grammar [Green et al., 2011, Constant et al., 2017].?) have reported a notable amount of parsing failures caused by missing MWEs. Simply listing all the potential non-compositional MWEs is not
3. [3]:  Passage ID 3: system is on cases with bigger gaps and this can explain the low results for English. In general, the system seems to perform inefficiently for discontinuous expressions. For instance, the following LVCs have not been identified using the system.•EN: gave him a vicious stare•FR: la conduite des travaux est menée7 ConclusionsIn this paper we proposed a neural model for MWE identification consisting of ConvNet and LSTM layers and an optional CRF layer. We compared the results of the models with strong baselines and the state-of-the-art. Overall, We conclude that a simple addition of a CRF layer toa well-performing ConvNet+LSTM network does not necessarily improve the results. The ConvNet+LSTM model benefiting from pre-trained embeddings achieved the best performance across multilingual datasets of the Parseme shared task on identification of verbal MWEs. The system performs best in recognising un-seen MWEs which is noteworthy.References[Al Saied et al., 2017]
4. [4]:  Passage ID 4: al., 2017]. This is an under-explored but promising area in MWE identification.Inspired by the recent synthesis of structuredprediction models and recurrent neural networksfor the task of Named Entity Recognition(NER) [Lample et al., 2016, Ma and Hovy, 2016], in this work we introduce a similar architecture adapted to the task of MWE identification.Specifically, our system benefits from convolutional neural layers as N-gram detectors, bidirectional long-short term memories to take care of long distance relationships between words, and an optional conditional random field (CRF) layer to attend to the dependencies among output tags.Our model beats state-of-the-art models across all target languages in terms of generalisability to unseenMWE types. We achieve this generalisabilitywithout using any task-specific domain knowledge beyond generic POS tags and pretrained embeddings.111The code is available at https://github.com/shivaat/VMWE-Identification2 MethodologyWe employ
5. [5]:  Passage ID 5: ?) and ?) benefit from the proposed IiOoBb labelling approach for modelling the data.In recent years, neural network based models, and in particular architectures incorporating Recurrent Neural Networks (RNNs) – such as Long Short Term Memory (LSTM) – and Convolutional Neural Networks (ConvNets) have achieved state-of-the-art performance in sequence tagging tasks [Collobert et al., 2011, Lample et al., 2016, Shao et al., 2017]. ?) made the first attempt at using deep learning for MWE identification.They reported their best results with an architecture consisting of two and three ConvNets.The advantages of neural end-to-end sequence taggers for MWE identification can be attributed to the use of word embeddings which efficiently encodesemantic and syntactic information [Constant et al., 2017]. This is an under-explored but promising area in MWE identification.Inspired by the recent synthesis of structuredprediction models and recurrent neural networksfor the task of Named