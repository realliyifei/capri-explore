# A Survey of Deep Learning for Mathematical Reasoning

## Question

How effective are state-of-the-art deep learning models on the SVAMP dataset, and what does this reveal about their consistency in performance?

## URLs

1. https://ar5iv.org/html/2403.07384. [2403.07384] SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models
2. https://ar5iv.org/html/2411.14698. [2411.14698] Improving Mathematical Reasoning Capabilities of Small Language Models via Feedback-Driven Distillation
3. https://ar5iv.org/html/2404.14963. [2404.14963] Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Reasoners
4. https://ar5iv.org/html/2305.14386. [2305.14386] Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation
5. https://ar5iv.org/html/2409.17972. [2409.17972] BEATS: Optimizing LLM Mathematical Capabilities with BackVerify and Adaptive Disambiguate based Efficient Tree Search
6. https://ar5iv.org/html/2408.10839. [2408.10839] Benchmarking Large Language Models for Math Reasoning Tasks
7. https://ar5iv.org/html/2404.03938. [2404.03938] Data Augmentation with In-Context Learning and Comparative Evaluation in Math Word Problem Solving
8. https://ar5iv.org/html/2412.04857. [2412.04857] Neuro-Symbolic Data Generation for Math Reasoning
9. https://ar5iv.org/html/2304.09797. [2304.09797] Progressive-Hint Prompting Improves Reasoning in Large Language Models

## Answer (I don't know?: ✗)

State-of-the-art deep learning models, particularly large language models (LLMs), have demonstrated significant effectiveness on the SVAMP dataset. Specifically, the knowledge distillation approach discussed in the provided context achieves a notable baseline accuracy of 85.4% on the in-distribution (ID) tests of the SVAMP dataset, which marks a considerable improvement over the previous best of 65.0% obtained by fine-tuning methods [1]. Additionally, while the performance on out-of-distribution (OOD) tests achieves a solving accuracy of 76.4%, this is stated to be lower than some Chain-of-Thought (CoT) based LLMs but significantly higher than the results obtained from fine-tuned baselines [1].

Moreover, the efficiency of these models is underscored by their ability to achieve competitive performance while utilizing significantly fewer parameters than traditional LLMs. This suggests a level of consistency across different parameters and model sizes, despite the complexity and variability inherent in MWP solving tasks [2]. The use of knowledge distillation allows for smaller, more efficient student models that still achieve state-of-the-art accuracy, even surpassing LLMs with the CoT prompting method [5]. Therefore, it can be inferred that while LLMs have made great strides in solving MWPs effectively, the approach of distilling knowledge into smaller models exhibits a level of robustness and consistency reflected in their performance metrics across both ID and OOD datasets.

However, the context also hints at some limitations, particularly regarding small student models' ability to fully understand CoT explanations, which could impact their reasoning capabilities [2]. This inconsistency suggests that while state-of-the-art models perform well under many conditions, there may be scenarios—particularly those requiring deep reasoning or understanding of complex prompts—where their effectiveness could be variable.

In conclusion, state-of-the-art deep learning models, particularly those using knowledge distillation methods, show high effectiveness on the SVAMP dataset, with notable achievements in performance on both ID and OOD tests. This performance indicates a level of consistency within the models, although certain limitations in understanding complex reasoning nuances might affect their overall reliability in all scenarios [3][4].

1. [1]:  https://ar5iv.org/html/2305.14386, [2305.14386] Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation
2. [2]:  https://ar5iv.org/html/2305.14386, [2305.14386] Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation
3. [3]:  https://ar5iv.org/html/2304.09797, [2304.09797] Progressive-Hint Prompting Improves Reasoning in Large Language Models
4. [4]:  https://ar5iv.org/html/2305.14386, [2305.14386] Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation
5. [5]:  https://ar5iv.org/html/2305.14386, [2305.14386] Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation
---
1. [1]:  Passage ID 1: baseline, achieving 85.4% accuracy on the SVAMP (ID) dataset, which is a significant improvement over the prior best accuracy of 65.0% achieved by fine-tuning. On the SVAMP (OOD) dataset, our approach achieves a solving accuracy of 76.4%, which is lower than CoT-based LLMs, but much higher than the fine-tuned baselines. We also show the original performance of backbone solvers without any additional exercises. To obtain best performance in Table 1, our solvers use about 20x more MWPs than the original training set to train. Overall, our results indicate that our knowledge distillation approach achieves superior performance with much fewer parameters than the prior state-of-the-art LLMs, and outperforms the best fine-tuning and knowledge distillation baselines on all
2. [2]:  Passage ID 2: and outperforms fine-tuned state-of-the-art (SOTA) solvers by a large margin.Figure 1: Accuracies vs model sizes for representative baselines and our approach on SVAMP dataset. Our method achieves competitive performance with LLMs with significantly fewer parameters.In recent years, large language models (LLMs) have made remarkable strides in solving MWPs. However, the substantial number of parameters in LLMs results in computational inefficiency, and necessitates API availability for reproducibility. To mitigate these issues, a natural solution is distilling the knowledge from LLMs into smaller, more efficient student models. The majority of prior research has emphasized using the "explanation" component of the CoT approach as the distilled knowledge Ho et al. (2022); Li et al. (2022); Shridhar et al. (2022b); Magister et al. (2022). Nonetheless, these methodologies exhibit certain limitations. Firstly, small student models may struggle to understand CoT explanations,
3. [3]:  Passage ID 3: on SVAMP (89.1% →→\rightarrow 91.9%), GSM8K (92% →→\rightarrow 95.5%), AQuA (76.4% →→\rightarrow 79.9%) and MATH (50.3% →→\rightarrow 53.9%).1 IntroductionWhile Large Language Models (LLMs) have demonstrated remarkable performance across various NLP tasks [1, 2, 3], their ability to reason is often perceived as a limitation that cannot be overcome merely by increasing the scale of the model [4, 5]. Prompt engineering in large-scale models has shown comparable or superior performance to full training set fine-tuning in enhancing reasoning ability, while also being significantly more sample-efficient [6, 7]. One area of research that aims to address this limitation is the use of Chain-of-Thought (CoT) approaches to promote intermediate reasoning steps [8, 9, 10]. Other works in this area, such as Least-to-Most [9] and Complex CoT [10], have also explored this direction. Another area of research is self-consistency-related approaches. In comparison to CoT-related work that focuses
4. [4]:  Passage ID 4: and more Mukherjee and Garain (2008). The format of a typical MWP involves a textual description of a problem scenario, which needs to be translated into a mathematical expression (typically an equation) that can be solved to obtain the answer. MWP solving was presented as a task for artificial intelligence several decades ago Fletcher (1985). Previous fine-tuned methods usually apply Seq2Seq models Wang et al. (2017, 2018); Xie and Sun (2019); Zhang et al. (2020); Liang et al. (2022a). In recent years, large language models (LLMs) such as GPT-3 Brown et al. (2020) and PaLM Chowdhery et al. (2022) exhibit strong reasoning ability with the help of chain-of-thought (CoT) prompting Wei et al. (2022b); Wang et al. (2022a); Chen et al. (2022), which achieves striking performance on MWP solving and outperforms fine-tuned state-of-the-art (SOTA) solvers by a large margin.Figure 1: Accuracies vs model sizes for representative baselines and our approach on SVAMP dataset. Our method
5. [5]:  Passage ID 5: MWP datasets, including both in-distribution (ID) and out-of-distribution (OOD) tests Koncel-Kedziorski et al. (2016); Miao et al. (2020); Patel et al. (2021). We show that our method is significantly effective in improving student models under the OOD setting.•The experimental results demonstrate that our method achieves state-of-the-art accuracy, significantly outperforming fine-tuned baselines. Notably, the student model trained with our method even surpasses LLMs with CoT prompting, despite having significantly fewer parameters.2 Related Work2.1 Math Word Problem SolvingAfter many years of research on rule-based algorithms Hosseini et al. (2014); Mitra and Baral (2016) and semantic parsing methods Shi et al. (2015); Huang et al. (2017), deep learning has become the predominant technique for solving MWPs, thanks to its superior performance and better generalization ability. Deep neural solver (DNS) Wang et al. (2017) was among the first to apply Seq2Seq models