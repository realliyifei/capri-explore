# A Survey of Ellipsis in Chinese

## Question

What makes it difficult to identify null complement anaphora in Mandarin compared to English?

## URLs

1. https://ar5iv.org/html/2011.02063. [2011.02063] Treebanking User-Generated Content

## Answer (I don't know?: ✓)

I don't know. The context provided does not include information specifically addressing the challenges of identifying null complement anaphora in Mandarin compared to English. The passages focus on different aspects of language processing, such as grammatical errors in Chinese, anaphora resolution in Arabic, and the performance of NLP techniques for various languages, but none mention null complement anaphora or provide a comparison between Mandarin and English in that context. Thus, the context is insufficient to answer the question thoroughly.

1. [1]:  https://ar5iv.org/html/2305.12818, No Title
2. [2]:  https://ar5iv.org/html/2305.11529, No Title
3. [3]:  https://ar5iv.org/html/2307.13923, No Title
4. [4]:  https://ar5iv.org/html/2307.13923, No Title
5. [5]:  https://ar5iv.org/html/2307.13923, No Title
---
1. [1]:  Passage ID 1: is close to that achieved forEnglish (see Table 2), indicating thatthe ngrams are well-aligned and ColexNet+→→ColexNet+\overrightarrow{\mbox{ColexNet+}} has good transferability. Chinese performs better than Arabic andRussian. The possible reasons are as follows: (1) BothArabic and Russian are morphologically rich whereas Chineseis not. Morphological variation makes finding aligned ngramsin the forward pass harder, with a negative impact onperformance; (2) To prevent bad tokenization for Chinese,we allow all ngrams (unlimited-length combination ofcontinuous characters) in a verse to be candidates in theforward pass. This setting gives ngrams more freedom andthus better results are expected. For the threelow-resource languages, we find that they diverge morphologically andtypologically from most high-resourcelanguages. Apinayé and Mündü seem to frequently useseveral consecutive whitespace-tokenized syllables toexpress a single concept, which makes finding the
2. [2]:  Passage ID 2: in an 81% MRR and 71% F1 score.This work contributes to growing the body of research on anaphora resolution in Arabic, highlighting the potential of deep learning techniques to enhance the state-of-the-art in this field. Our work emphasizes the importance of investigating the transferability and generalizability of NLP techniques for Arabic.There is a clear need to create comprehensive annotated datasets considering the diversity of Arabic pronouns. More efforts are needed to devise clear annotation guidelines, especially given the complex morphological features of Arabic, such as attached pronouns. Such robust guidelines would not only enhance our current models but also pave the way for future research in Arabic anaphora resolution.Whereas this paper focuses primarily on Arabic pronoun resolution, the insights and methodology presented can be applied to other references within the text, such as cataphora, zero-anaphora, and bridging anaphora. We believe that this research
3. [3]:  Passage ID 3: framework of our method.3 MethodsFig. 1 illustrates the framework of our method, which involves the construction of parallel data comprising six types of native Chinese grammatical errors to facilitate the fine-tuning of open-source Language Model (LLMs). While human-annotated data offer high-quality samples, the associated high cost remains a significant concern. To address this, we adopt a compromise approach. We first guide ChatGPT to generate ungrammatical sentences with clues by providing those clues collected from the Internet. Then, we annotate the ungrammatical sentences without clues collected from the Internet. Additionally, we propose an error-invariant augmentation technique to substitute named entities in the parallel data with similar ones, further enhancing the model’s capability to correct native Chinese grammatical errors. Finally, we convert the parallel data into instructions, which are then utilized for fine-tuning LLMs. Detailed explanations of these steps
4. [4]:  Passage ID 4: framework of our method.3 MethodsFig. 1 illustrates the framework of our method, which involves the construction of parallel data comprising six types of native Chinese grammatical errors to facilitate the fine-tuning of open-source Language Model (LLMs). While human-annotated data offer high-quality samples, the associated high cost remains a significant concern. To address this, we adopt a compromise approach. We first guide ChatGPT to generate ungrammatical sentences with clues by providing those clues collected from the Internet. Then, we annotate the ungrammatical sentences without clues collected from the Internet. Additionally, we propose an error-invariant augmentation technique to substitute named entities in the parallel data with similar ones, further enhancing the model’s capability to correct native Chinese grammatical errors. Finally, we convert the parallel data into instructions, which are then utilized for fine-tuning LLMs. Detailed explanations of these steps
5. [5]:  Passage ID 5: capability to correct native Chinese grammatical errors. Finally, we convert the parallel data into instructions, which are then utilized for fine-tuning LLMs. Detailed explanations of these steps are provided in the following subsections.3.1 Hybrid Dataset Construction3.1.1 ChatGPT-generated DataAs shown in the first three lines of Table 1, the grammatical errors with clues are easy to detect and correct by recognizing the specific clues. For example, ”more than” and ”about” are used together leading to redundant component, ”The cause” and ”caused by” are used together leading to structural confusion, and ”prompting” and ”pace” are used together leading to improper collocation. Conversely, we can construct the ungrammatical sentences by inserting these cues into grammatical sentences. Thanks to the strong capabilities of ChatGPT, we can instruct ChatGPT to generate the ungrammatical sentences that meet our requirements by providing these clues collected from public websites