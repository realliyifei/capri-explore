# A Survey on Zero Pronoun Translation

## Question

How are researchers working to improve zero pronoun translation performance without changing the machine translation architecture?

## URLs

1. https://ar5iv.org/html/2305.10196. [2305.10196] A Survey on Zero Pronoun Translation
2. https://ar5iv.org/html/2410.20941. [2410.20941] Instruction-Tuned LLMs Succeed in Document-Level MT Without Fine-Tuning—But BLEU Turns a Blind Eye
3. https://ar5iv.org/html/2107.00318. [2107.00318] Zero-pronoun Data Augmentation for Japanese-to-English Translation
4. https://ar5iv.org/html/2010.07638. [2010.07638] Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses
5. https://ar5iv.org/html/2305.01181. [2305.01181] New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT
6. https://ar5iv.org/html/2405.08477. [2405.08477] Enhancing Gender-Inclusive Machine Translation with Neomorphemes and Large Language Models
7. https://ar5iv.org/html/2306.05183. [2306.05183] Improving Long Context Document-Level Machine Translation
8. https://ar5iv.org/html/2304.12959. [2304.12959] Why are we still translating sentences?
9. https://ar5iv.org/html/2404.02392. [2404.02392] Low-resource neural machine translation with morphological modeling
10. https://ar5iv.org/html/2310.12303. [2310.12303] Document-Level Language Models for Machine Translation

## Answer (I don't know?: ✗)

Researchers are focusing on enhancing zero pronoun translation performance primarily through innovative training methodologies rather than altering the core machine translation (MT) architecture. One significant approach involves the use of augmentation methods that provide additional training signals, allowing the translation model to better learn the correlations between local context and zero pronouns. This method has been shown to significantly improve the accuracy of zero pronoun translation in conversational settings, as evidenced by machine translation experiments [2].

The improvement in model performance is further complemented by targeted fine-tuning processes that aim to enhance pronoun translations specifically. For instance, researchers have demonstrated that their sentence-level model achieves a 0.5 BLEU score increase across various test sets, and a contextual model shows more substantial improvements, achieving BLEU scores from 31.81 to 32 on the WMT14 De-En test set and from 32.10 to 33.13 on the IWSLT13 De-En test set [4]. These enhancements in performance highlight the effectiveness of refining existing models through better training protocols without needing to redesign their architecture.

Moreover, researchers are also committed to evaluating their approaches on robust benchmarks, such as the GuoFeng Benchmark, which is tailored for discourse-aware tasks [1]. By employing comprehensive evaluation techniques, they aim to gather insights into both the strengths and weaknesses of their models, thereby facilitating continuous improvement in translation quality [3].

Additionally, understanding the unique linguistic phenomena associated with zero pronouns across different languages is crucial. For example, the zero pronoun phenomenon varies significantly in terms of word form and occurrence frequency among languages [3]. Researchers recognize the necessity of addressing these linguistic challenges as part of their improvement strategies, underscoring that a one-size-fits-all solution is inadequate for a field as complex as zero pronoun translation. 

Finally, ongoing efforts to replicate improvements across multiple language pairs, including Fr-En and Cs-En, affirm the generalizability of the methods being tested, which is essential for developing robust solutions that can be applied across different linguistic contexts without necessitating changes to the underlying architecture [4]. This multi-faceted approach—combining advanced training methods, rigorous evaluation, and a deep understanding of linguistic diversity—forms the cornerstone of current research aimed at elevating zero pronoun translation performance.

1. [1]:  https://ar5iv.org/html/2305.10196, [2305.10196] A Survey on Zero Pronoun Translation
2. [2]:  https://ar5iv.org/html/2107.00318, [2107.00318] Zero-pronoun Data Augmentation for Japanese-to-English Translation
3. [3]:  https://ar5iv.org/html/2305.10196, [2305.10196] A Survey on Zero Pronoun Translation
4. [4]:  https://ar5iv.org/html/2010.07638, [2010.07638] Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses
5. [5]:  https://ar5iv.org/html/2305.10196, [2305.10196] A Survey on Zero Pronoun Translation
---
1. [1]:  Passage ID 1: increase in scientific publications related to ZP over the past few years.This paper is a literature review of existing research on zero pronoun translation, providing insights into the challenges and opportunities of this area and proposing potential directions for future research.As we look to the future, we intend to delve deeper into the challenges of ZPT. Our plan is to leverage large language models, which have shown great potential in dealing with complex tasks, to tackle this particular challenge Lu et al. (2023); Wang et al. (2023b); Lyu et al. (2023). Moreover, we plan to evaluate our approach on more discourse-aware tasks. Specifically, we aim to utilize the GuoFeng Benchmark Wang et al. (2022, 2023a), which presents a comprehensive testing ground for evaluating the performance of models on a variety of discourse-level translation tasks. By doing so, we hope to gain more insights into the strengths and weaknesses of our approach, and continually refine it to achieve
2. [2]:  Passage ID 2: augmentation method that provides additional training signals for the translation model to learn correlations between local context and zero pronouns.We show that the proposed method significantly improves the accuracy of zero pronoun translation with machine translation experiments in the conversational domain.1 IntroductionWhile neural machine translation (NMT) has demonstrated high performance in single-sentence translation, it is still challenging to handle linguistic phenomena involving discourse contexts.One such issue is the translation of zero pronouns (ZP) in Japanese-to-English translation.In Japanese, subjects and objects are often omitted when the listener can infer them from the context. However, when translating them into English, the omitted words must be explicitly translated in most cases. For example, in the following sentence, the subject omitted in Japanese is the first person, and I has to be output in
3. [3]:  Passage ID 3: of models on a variety of discourse-level translation tasks. By doing so, we hope to gain more insights into the strengths and weaknesses of our approach, and continually refine it to achieve better performance.AcknowledgementThe authors express their sincere gratitude to all reviewers whose keen interest and insightful feedback have significantly improved the quality of this paper. Their affirmation and encouragement have further solidified our commitment to the path of computational linguistics. This work is part of the GuoFeng AI (guofeng-ai@googlegroups.com) and TranSmart Huang et al. (2021) projects.LimitationsWe list the main limitations of this work as follows:1.Zero Pronoun in Different Languages: The zero pronoun phenomenon may vary across languages in terms of word form, occurrence frequency and category distribution etc. Due to page limitation, some examples are mainly discussed in Chinese and/or English. However, most results and findings can be applied to
4. [4]:  Passage ID 4: from, we improve the model performance of both a sentence-level and a contextual model without using any additional data. We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation. We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En.111Code available at <https://github.com/ntunlp/pronoun-finetuning>.1 IntroductionThe advent of neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) brought about significant improvements that left the previously successful statistical machine translation
5. [5]:  Passage ID 5: et al. (2019), question answering Tan et al. (2021), and machine translation Wang (2019).When translating texts from pro-drop to non-pro-drop languages (e.g. Chinese⇒⇒\RightarrowEnglish), this phenomenon leads to serious problems for translation models in terms of: 1) completeness, since translation of such invisible pronouns cannot be normally reproduced; 2) correctness, because understanding the semantics of a source sentence needs to identifying and resolving the pronominal reference.Figure 1 shows ZP examples in three typological patterns determined by language family (detailed in Appendix §A.1). Taking a full-drop language for instance, the first-person subject and third-person object pronouns are omitted in Hindi input while these pronouns are all compulsory in English translation. This is not a problem for human beings since we can easily recall these missing pronoun from the context. However, even a real-life MT system still fails to accurately translate ZPs.Figure 1: