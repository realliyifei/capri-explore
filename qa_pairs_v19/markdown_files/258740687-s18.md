# A Survey on Zero Pronoun Translation

## Question

How are researchers working to improve zero pronoun translation performance without changing the machine translation architecture?

## URLs

1. https://ar5iv.org/html/2305.10196. [2305.10196] A Survey on Zero Pronoun Translation
2. https://ar5iv.org/html/2410.20941. [2410.20941] Instruction-Tuned LLMs Succeed in Document-Level MT Without Fine-Tuning—But BLEU Turns a Blind Eye
3. https://ar5iv.org/html/2107.00318. [2107.00318] Zero-pronoun Data Augmentation for Japanese-to-English Translation
4. https://ar5iv.org/html/2010.07638. [2010.07638] Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses
5. https://ar5iv.org/html/2305.01181. [2305.01181] New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT
6. https://ar5iv.org/html/2405.08477. [2405.08477] Enhancing Gender-Inclusive Machine Translation with Neomorphemes and Large Language Models
7. https://ar5iv.org/html/2306.05183. [2306.05183] Improving Long Context Document-Level Machine Translation
8. https://ar5iv.org/html/2304.12959. [2304.12959] Why are we still translating sentences?
9. https://ar5iv.org/html/2404.02392. [2404.02392] Low-resource neural machine translation with morphological modeling
10. https://ar5iv.org/html/2310.12303. [2310.12303] Document-Level Language Models for Machine Translation

## Answer (I don't know?: ✓)

I don't know. The provided context does not contain any specific information about improvements in zero pronoun translation performance or methodologies employed by researchers to enhance this aspect of machine translation without altering the architecture. The passages focus more broadly on language modeling, machine translation techniques, and evaluation methods, but do not address the particular issue of zero pronoun translation. Therefore, it is insufficient to answer the question.

1. [1]:  https://ar5iv.org/html/2401.14559, No Title
2. [2]:  https://ar5iv.org/html/2410.22335, No Title
3. [3]:  https://ar5iv.org/html/2404.02512, No Title
4. [4]:  https://ar5iv.org/html/2401.14559, No Title
5. [5]:  https://ar5iv.org/html/2411.11295, No Title
---
1. [1]:  Passage ID 1: I examined how to leverage language modelling techniques in general and LLMs in particular to improve translation features that involve human interaction and continuous feedback, such as adaptive MT, terminology-constrained MT, domain-aware automatic post-editing, auto-suggestion and auto-completion. In the second research question, I addressed a common scenario in the translation industry, namely receiving highly specialised projects, where there is hardly any parallel in-domain data. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. One way to address this question is through domain-specific text generation with LLMs and then fine-tuning an MT model using the generated bilingual in-domain synthetic data. Furthermore, this question overlaps with the first question, since real-time adaptive MT with LLMs can serve as an alternative approach to address in-domain data
2. [2]:  Passage ID 2: information. Furthermore, we explored key factors contributing to the model’s performance improvement, including but not limited to the design of the network architecture, optimization of training strategies, and adjustment of hyperparameters. These analyses not only deepen our understanding of the model’s internal working mechanisms but also provide valuable insights and guidance for future research, especially in terms of further enhancing the performance and application scope of machine translation systems.2 PreliminaryMachine translation, as an important branch of the field of Natural Language Processing (NLP), aims to achieve automatic conversion from one language to another. Early machine translation methods were primarily based on rules and dictionaries. Since the mid-20th century, machine translation has undergone a transition from rule-based translation to statistical methods Brown et al. (1993); Lopez (2008), and to the current Neural Machine Translation (NMT)
3. [3]:  Passage ID 3: developments in the field of automatic translation evaluation have demonstrated that techniques utilizing multilingual embeddings have a tendency to outperform other traditional approaches and display the strongest correlation with human assessments (Zerva et al., 2022). Notable instances of these techniques include BERTScore (Zhang et al., 2019) and COMET (Rei et al., 2020, 2022; Kocmi et al., 2022). To further improve the effectiveness of automatic translation evaluation, it is reasonable to investigate methodologies that leverage large language models, considering their notable capability for comprehension.In this work, we aim to assess the capability of LLMs and utilize them for reference-less translation evaluation involving English and Indian languages. The research questions that we pose are as follows:•Do LLMs possess zero-shot or in-context translation evaluation capabilities?•How do fine-tuned LLMs compare with existing state-of-the-art translation evaluation
4. [4]:  Passage ID 4: common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations (Axelrod et al.,, 2011; Haddow and Koehn,, 2012).Recent advances in language modelling techniques in general and large-scale language models (LLMs) in particular have shown significant potential in improving a wide range of NLP tasks. Inspired by this idea, this research aims to answer two major Research Questions (RQ):RQ1In scenarios involving human interaction and continuous feedback, can we employ language models to improve the quality of adaptive MT at inference time? In the subsequent sections, I will be referring to this question as “Adaptive and Interactive MT”.RQ2In the absence of sufficient in-domain data, can we use pre-trained LLMs to improve the process of NMT domain adaptation? In the following sections, I will be referring to this question as “Domain-specific Text Generation for MT”.Figure 1.1:
5. [5]:  Passage ID 5: TranslationDue to a lack of training data for low-resource languages, most LLMs cannot properly translate low-resource languages. Moreover, it is hard to finetune these LLM without sufficient language resources. Thus, various research efforts focus on improving LLMs to perform translation tasks with minimal or no task-specific training, commonly referred to as zero-shot and few-shot learning.Zero-shot translation refers to the capability of LLMs to translate between language pairs without having seen explicit examples of these translations during training. For instance, Zhang et al. proposed strategies such as random online back-translation and language-specific modeling, improving zero-shot performance in multilingual NMT by approximately 10 BLEU score [75]. Gao et al. introduced Cross-lingual Consistency Regularization (CrossConST), which enhances zero-shot performance by bridging the representation gap across languages, proving effective in both high-resource and low-resource