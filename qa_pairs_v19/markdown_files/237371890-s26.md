# Survey of Low-Resource Machine Translation

## Question

In what ways have bilingual lexicons been used to improve neural machine translation models?

## URLs

1. https://ar5iv.org/html/2004.13270. [2004.13270] Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models
2. https://ar5iv.org/html/2312.04807. [2312.04807] Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting
3. https://ar5iv.org/html/2310.13995. [2310.13995] On Bilingual Lexicon Induction with Large Language Models
4. https://ar5iv.org/html/2410.15144. [2410.15144] A survey of neural-network-based methods utilising comparable data for finding translation equivalents
5. https://ar5iv.org/html/1909.07907. [1909.07907] Pointer-based Fusion of Bilingual Lexicons into Neural Machine Translation
6. https://ar5iv.org/html/2303.15265. [2303.15265] Untitled Document
7. https://ar5iv.org/html/2404.04221. [2404.04221] How Lexical is Bilingual Lexicon Induction?
8. https://ar5iv.org/html/2408.09075. [2408.09075] Untitled Document
9. https://ar5iv.org/html/2305.06575. [2305.06575] Chain-of-Dictionary Prompting Elicits Translation in Large Language Models
10. https://ar5iv.org/html/2405.11200. [2405.11200] LexGen : Domain-aware Multilingual Lexicon Generation

## Answer (I don't know?: ✗)

Bilingual lexicons have been utilized in various ways to enhance neural machine translation (NMT) models, particularly through the integration of structured symbolic knowledge into the translation process. Here are some key methods and findings from the context provided:

1. **Incorporation of Structured Knowledge**: One of the primary approaches to improving NMT models involves integrating bilingual dictionaries into the system. Research has shown that implementing structured symbolic knowledge, such as a bilingual lexicon, leads to significantly better translation results. This method not only enhances the quality of translations but also contributes to improved training speed, reduced model parameters, and greater extensibility for adding new knowledge sources. Specifically, approaches like LexPG(+S)(+F) leverage this structured knowledge effectively within the model [4].

2. **Lexically Constrained Translation**: Another technique highlighted is lexically constrained translation, which places explicit constraints on the target output strings based on entries from a terminology dictionary. This method allows for greater control over the translation output by ensuring that specific terms are used in the generated sentences [2]. Various studies have explored modifying the beam search decoding algorithm to impose such lexical constraints effectively, enhancing the precision of the translations [2].

3. **Dynamic Updating of Lexicons**: Unlike traditional methods that may necessitate extensive data collection and model retraining to accommodate new information, bilingual lexicons can be updated dynamically while the neural model remains fixed. This provides a more efficient approach to incorporating new lexicon entries, allowing for continuous improvement in translation without the traditional overhead of retraining the entire model [5].

4. **Leveraging Complex Mapping Constraints**: The development of methodologies that accommodate complex mapping constraints, including the use of phrase-table structures, is another area of focus. This suggests that by employing bilingual lexicons more robustly, NMT models can better handle multi-word expressions and nuanced translations, which are common challenges in language translation tasks [4].

5. **Bilingual Lexicon Induction (BLI)**: The field of bilingual lexicon induction has also seen advancements through the calculation of cross-lingual word embeddings (CLWEs). By mapping source and target languages into a shared space, BLI approaches have improved the quality of translations by ensuring appropriate word alignments [3]. These methods validate the capabilities of multilingual large language models (mLLMs) in supporting BLI and offer new methodologies for fostering effective bilingual lexicons [3].

In summary, bilingual lexicons enhance neural machine translation through structured knowledge integration, dynamic updating capabilities, and methodologies that impose constraints, all of which contribute to improved accuracy, efficiency, and adaptability in translation systems. These findings underscore the critical role that bilingual dictionaries play in advancing the field of neural machine translation.

1. [1]:  https://ar5iv.org/html/2410.15144, [2410.15144] A survey of neural-network-based methods utilising comparable data for finding translation equivalents
2. [2]:  https://ar5iv.org/html/2312.04807, [2312.04807] Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting
3. [3]:  https://ar5iv.org/html/2310.13995, [2310.13995] On Bilingual Lexicon Induction with Large Language Models
4. [4]:  https://ar5iv.org/html/1909.07907, [1909.07907] Pointer-based Fusion of Bilingual Lexicons into Neural Machine Translation
5. [5]:  https://ar5iv.org/html/1909.07907, [1909.07907] Pointer-based Fusion of Bilingual Lexicons into Neural Machine Translation
---
1. [1]:  Passage ID 1: NLP and lexicography, while the former often omits the latter. In this paper, we present the most common approaches from NLP that endeavour to automatically induce one of the essential dictionary components, translation equivalents and focus on the neural-network-based methods using comparable data. We analyse them from a lexicographic perspective since their viewpoints are crucial for improving the described methods. Moreover, we identify the methods that integrate these viewpoints and can be further exploited in various applications that require them. This survey encourages a connection between the NLP and lexicography fields as the NLP field can benefit from lexicographic insights, and it serves as a helping and inspiring material for further research in the context of neural-network-based methods utilising comparable data.Keywords Cross-lingual embedding models  ⋅⋅\cdotBilingual lexicon induction  ⋅⋅\cdotComparable data  ⋅⋅\cdotComputational linguistics  ⋅⋅\cdotSurvey1
2. [2]:  Passage ID 2: machine translation mainly includes high-quality bilingual sentences, a bilingual terminology dictionary and translation templates. Intuitively, it is reasonable to believe that it is beneficial for improving translation quality to integrate multiple types of knowledge into NMT models in a flexible and efficient way. However, most existing methods focus on only how to integrate a single type of knowledge into NMT models, either a terminology dictionary Dinu et al. (2019); Dougal and Lonsdale (2020), bilingual sentences Cao and Xiong (2018); Liu et al. (2019a) or translation templates Yang et al. (2020).As a primary technique to utilize a terminology dictionary, lexically constrained translation allows for explicit phrase-based constraints to be placed on target output strings Hu et al. (2019). Several research works Hokamp and Liu (2017); Post and Vilar (2018) impose lexical constraints by modifying the beam search decoding algorithm. Another line of approach trains the model to
3. [3]:  Passage ID 3: usually support more languages than mLLMs. Finally, we run a series of insightful ablations and discuss the usefulness of BLI-oriented fine-tuning. In short, our work validates the BLI capabilities of mLLMs and proposes new methodology for BLI. We hope that the combination of our comprehensive analyses and discussions, including on limitations, will pave the way for the development of stronger BLI systems in the future. Our code is publicly available at github.com/cambridgeltl/prompt4bli.2 Related WorkBilingual Lexicon Induction. Over the past decade, predominant BLI approaches have relied on the calculation of cross-lingual word embeddings (CLWEs) where, in the most popular BLI variant, two transformation functions are learned to respectively map source and target monolingual static word embedding spaces into a shared cross-lingual space (Xing et al., 2015; Lample et al., 2018; Joulin et al., 2018; Artetxe et al., 2018a; Alvarez-Melis and Jaakkola, 2018; Patra et al., 2019;
4. [4]:  Passage ID 4: Conclusions and Future WorkWe present several simple and highly practical approaches to incorporatingstructured symbolic knowledge – a bilingual dictionary – into a standardneural machine translation model.Our experimental results show that these methods not only produce much betterresults, but are faster to train, smaller in parameter size, and overall moreextensible to adding new knowledge.For the future, we think it is possible to substitute the LSTM-basedneural conditional language model with a transformer (Vaswani et al., 2017).Experimenting on pretraining the neural conditional language model and onlytrain the proposed LexPG(+S)(+F) components may also be of interest, asthis would further demonstrate the flexibility of our proposed approach.On a separate track, we would also like to investigate the possibility ofleveraging even more structured knowledge, such as a phrase-table.The ability to accommodate complex mapping constraints, including
5. [5]:  Passage ID 5: as the results show, our proposed models are indeed able to put theseinformation into good use and boost its performance.To accommodate changes in a pretrained neural translation model, extensive datacollection and retraining is usually required for active learning (or evenlife-long learning).Our use of a bilingual lexicon provides an alternative.The bilingual lexicon can be updated dynamically while keeping the neural modelfixed.5.5 Improving The Bilingual LexiconAt the beginning of §4 we claim that the dictionary entries weused in our experiments are merely “dictionary-like”.Indeed, word alignment can only produce word-to-word alignments and cannothandle phrases.In some cases, words that should have been aligned to multi-word phrases areonly aligned to one of the words in the phrase (e.g. “Bundeskanzler” alignedto “Federal” instead of “Federal Chancellor”).More over, the aligner itself is not exactly error-proof.It is therefore reasonable to assume that there