# Survey of Low-Resource Machine Translation

## Question

In what ways have bilingual lexicons been used to improve neural machine translation models?

## URLs

1. https://ar5iv.org/html/2004.13270. [2004.13270] Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models
2. https://ar5iv.org/html/2312.04807. [2312.04807] Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting
3. https://ar5iv.org/html/2310.13995. [2310.13995] On Bilingual Lexicon Induction with Large Language Models
4. https://ar5iv.org/html/2410.15144. [2410.15144] A survey of neural-network-based methods utilising comparable data for finding translation equivalents
5. https://ar5iv.org/html/1909.07907. [1909.07907] Pointer-based Fusion of Bilingual Lexicons into Neural Machine Translation
6. https://ar5iv.org/html/2303.15265. [2303.15265] Untitled Document
7. https://ar5iv.org/html/2404.04221. [2404.04221] How Lexical is Bilingual Lexicon Induction?
8. https://ar5iv.org/html/2408.09075. [2408.09075] Untitled Document
9. https://ar5iv.org/html/2305.06575. [2305.06575] Chain-of-Dictionary Prompting Elicits Translation in Large Language Models
10. https://ar5iv.org/html/2405.11200. [2405.11200] LexGen : Domain-aware Multilingual Lexicon Generation

## Answer (I don't know?: ✓)

I don't know. The provided context does not contain specific information about bilingual lexicons or their applications in improving neural machine translation (NMT) models. The passages focus primarily on the evolution of machine translation methodologies, the advantages of neural machine translation over previous approaches, and some ongoing research themes in the field, but they do not discuss the role of bilingual lexicons in the NMT context. Therefore, there is insufficient information to answer the question directly.

1. [1]:  https://ar5iv.org/html/2410.22335, No Title
2. [2]:  https://ar5iv.org/html/2107.04239, No Title
3. [3]:  https://ar5iv.org/html/2107.04239, No Title
4. [4]:  https://ar5iv.org/html/2410.00427, No Title
5. [5]:  https://ar5iv.org/html/2304.07869, No Title
---
1. [1]:  Passage ID 1: information. Furthermore, we explored key factors contributing to the model’s performance improvement, including but not limited to the design of the network architecture, optimization of training strategies, and adjustment of hyperparameters. These analyses not only deepen our understanding of the model’s internal working mechanisms but also provide valuable insights and guidance for future research, especially in terms of further enhancing the performance and application scope of machine translation systems.2 PreliminaryMachine translation, as an important branch of the field of Natural Language Processing (NLP), aims to achieve automatic conversion from one language to another. Early machine translation methods were primarily based on rules and dictionaries. Since the mid-20th century, machine translation has undergone a transition from rule-based translation to statistical methods Brown et al. (1993); Lopez (2008), and to the current Neural Machine Translation (NMT)
2. [2]:  Passage ID 2: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
3. [3]:  Passage ID 3: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
4. [4]:  Passage ID 4: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
5. [5]:  Passage ID 5: task due to the inherent complex nature and the fluidity that natural languages bring. Nonetheless, in recent years, it has achieved state-of-the-art performance in several language pairs. Although, a lot of traction can be seen in the areas of multilingual neural machine translation (MNMT) in the recent years, there are no comprehensive survey done to identify what approaches work well. The goal of this paper is to investigate the realm of low resource languages and build a Neural Machine Translation model to achieve state-of-the-art results. The paper looks to build upon the mBART.CC25 [1] language model and explore strategies to augment it with various NLP and Deep Learning techniques like back translation and transfer learning. This implementation tries to unpack the architecture of the NMT application and determine the different components which offers us opportunities to amend the said application within the purview of the low resource languages problem space.1