# Survey of Low-Resource Machine Translation

## Question

What is the impact of parameter sharing on the performance and complexity of multilingual machine translation models?

## URLs

1. https://ar5iv.org/html/2302.09650. [2302.09650] Scaling Laws for Multilingual Neural Machine Translation
2. https://ar5iv.org/html/2212.07530. [2212.07530] Causes and Cures for Interference in Multilingual Translation
3. https://ar5iv.org/html/2405.10936. [2405.10936] A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers
4. https://ar5iv.org/html/2104.07358. [2104.07358] Adaptive Sparse Transformer for Multilingual Translation
5. https://ar5iv.org/html/2401.08417. [2401.08417] Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation
6. https://ar5iv.org/html/2410.12883. [2410.12883] Scaling Laws for Multilingual Language Models
7. https://ar5iv.org/html/2411.11072. [2411.11072] Multilingual Large Language Models: A Systematic Survey
8. https://ar5iv.org/html/2407.13469. [2407.13469] Fixed and Adaptive Simultaneous Machine Translation Strategies Using Adapters
9. https://ar5iv.org/html/2407.05975. [2407.05975] LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages

## Answer (I don't know?: ✗)

Parameter sharing in multilingual machine translation (MT) models significantly impacts both performance and complexity, particularly in how these models are able to leverage knowledge across languages and manage computational resources.

Firstly, parameter sharing allows for collective learning from multiple languages, which is especially beneficial for low-resource languages. Instead of training separate models for each language pair, which can be resource-intensive and inefficient, multilingual NMT models can share parameters across languages. This means that all encoder, decoder, and attention components can either be completely independent or fully shared among languages [1] [2]. In cases where parameters are fully shared, the model can generalize better by learning commonalities between languages, which aids in translation tasks involving languages with limited data [1].

Furthermore, low-resource language pairs can particularly benefit from the joint training of related rich-resource language pairs. By leveraging rich-resource languages, the model can learn from a more extensive dataset and improve its capability to perform translations in low-resource scenarios [1] [2]. This is crucial in the context of multilingual MT where resources may not be equally available across all languages.

Another important aspect of parameter sharing is its role in enabling zero-shot translation. Multilingual NMT has the capacity to translate between language pairs that were not seen during training, allowing for greater flexibility and utility of the model without requiring additional training data for every possible language pair [1]. This capability not only enhances the practical applications of multilingual MT but also reduces the complexity associated with developing dedicated models for each specific language pair in the system.

In terms of complexity, while parameter sharing can simplify the architecture (potentially leading to a reduction in time and resources spent on training), it can also introduce challenges. For instance, if all languages share the same parameters, specific languages might not achieve optimal performance if their linguistic properties diverge significantly from those of the other languages in the model [1]. Hence, finding the right balance between shared and independent parameters is crucial for maximizing performance across various languages.

In summary, parameter sharing has a profound impact on the performance and complexity of multilingual machine translation models. It allows these models to efficiently utilize data from multiple languages, enhance performance in low-resource settings, and facilitate zero-shot translations, all while introducing potential complexities that need to be managed carefully [1] [2].

1. [1]:  https://ar5iv.org/html/2107.04239, No Title
2. [2]:  https://ar5iv.org/html/2107.04239, No Title
3. [3]:  https://ar5iv.org/html/2408.12079, No Title
4. [4]:  https://ar5iv.org/html/2408.12079, No Title
5. [5]:  https://ar5iv.org/html/2405.10936, [2405.10936] A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers
---
1. [1]:  Passage ID 1: training and maintenance compared with training multiple separate models, and can collectively learn the knowledge from multiple languages to help low-resource languages. Second, low-resource language pairs benefit from related rich-resource languages pairs through joint training. Moreover, multilingual NMT offers the possibility to translate on language pairs that are unseen during training, which is called zero-shot translation. In the following paragraphs, we summarize the works on multilingual training from three perspectives (i.e., parameter sharing, designs for low-resource languages and zero-shot translation).Parameter sharing.There are different ways to share model parameters in multilingual training. First, all the encoder, decoder and attention components are independent among different languages Luong et al. (2015); Dong et al. (2015); Zoph and Knight (2016). Second, fully shared encoder, decoder and attention components are considered across languages, where a
2. [2]:  Passage ID 2: training and maintenance compared with training multiple separate models, and can collectively learn the knowledge from multiple languages to help low-resource languages. Second, low-resource language pairs benefit from related rich-resource languages pairs through joint training. Moreover, multilingual NMT offers the possibility to translate on language pairs that are unseen during training, which is called zero-shot translation. In the following paragraphs, we summarize the works on multilingual training from three perspectives (i.e., parameter sharing, designs for low-resource languages and zero-shot translation).Parameter sharing.There are different ways to share model parameters in multilingual training. First, all the encoder, decoder and attention components are independent among different languages Luong et al. (2015); Dong et al. (2015); Zoph and Knight (2016). Second, fully shared encoder, decoder and attention components are considered across languages, where a
3. [3]:  Passage ID 3: Memory High-quality filtering1 IntroductionThere are many languages around the world that are on the brink of extinction.Machine translation plays a crucial role in preserving endangered languages.However, a common challenge in this endeavor is the scarcity of parallel corpora available online, which hinders the development of effective translation systems.In recent years, the rise of Neural Machine Translation (NMT) [18] make to a technological leap.At the beginning of the research of NMT, researchers used deep neural networks, especially Recurrent Neural Network (RNN) and Long Short-term Memory Networks (LSTM), to better capture language context and complex structures.Moreover, some researchers transfered Convolution Neural Network (CNN) in the classfication task of Natural Language Processing (NLP) [16].However, today, most of the researchers design their models based on transformer [19].The transformer model, based on attention mechanisms, allows more flexible focus
4. [4]:  Passage ID 4: Memory High-quality filtering1 IntroductionThere are many languages around the world that are on the brink of extinction.Machine translation plays a crucial role in preserving endangered languages.However, a common challenge in this endeavor is the scarcity of parallel corpora available online, which hinders the development of effective translation systems.In recent years, the rise of Neural Machine Translation (NMT) [18] make to a technological leap.At the beginning of the research of NMT, researchers used deep neural networks, especially Recurrent Neural Network (RNN) and Long Short-term Memory Networks (LSTM), to better capture language context and complex structures.Moreover, some researchers transfered Convolution Neural Network (CNN) in the classfication task of Natural Language Processing (NLP) [16].However, today, most of the researchers design their models based on transformer [19].The transformer model, based on attention mechanisms, allows more flexible focus
5. [5]:  Passage ID 5: aspects, along with possible solutions.Besides, we highlight future research directions that aim at further enhancing LLMs with multilingualism.The survey aims to help the research community address multilingual problems and provide a comprehensive understanding of the core concepts, key techniques, and latest developments in multilingual natural language processing based on LLMs.11footnotetext: Equal Contribution. Contact e-mail: kyhuang@bjtu.edu.cn; fengran.mo@umontreal.ca1 IntroductionWith the rapid development of artificial intelligence (AI), the advent of large language models (LLMs) such as GPT-3.5 [1], GPT-4 [2], and LLaMA [3] has emerged as groundbreaking technologies, revolutionizing the field of natural language processing (NLP).LLMs have pushed the boundaries of what was previously thought possible with a “Prompt” style [4]. Their capability to understand and generate human-like text has achieved state-of-the-art performance in various downstream tasks such as