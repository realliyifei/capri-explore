# Survey of Low-Resource Machine Translation

## Question

What is the impact of parameter sharing on the performance and complexity of multilingual machine translation models?

## URLs

1. https://ar5iv.org/html/2302.09650. [2302.09650] Scaling Laws for Multilingual Neural Machine Translation
2. https://ar5iv.org/html/2212.07530. [2212.07530] Causes and Cures for Interference in Multilingual Translation
3. https://ar5iv.org/html/2405.10936. [2405.10936] A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers
4. https://ar5iv.org/html/2104.07358. [2104.07358] Adaptive Sparse Transformer for Multilingual Translation
5. https://ar5iv.org/html/2401.08417. [2401.08417] Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation
6. https://ar5iv.org/html/2410.12883. [2410.12883] Scaling Laws for Multilingual Language Models
7. https://ar5iv.org/html/2411.11072. [2411.11072] Multilingual Large Language Models: A Systematic Survey
8. https://ar5iv.org/html/2407.13469. [2407.13469] Fixed and Adaptive Simultaneous Machine Translation Strategies Using Adapters
9. https://ar5iv.org/html/2407.05975. [2407.05975] LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages

## Answer (I don't know?: ✗)

The impact of parameter sharing on the performance and complexity of multilingual machine translation (MNMT) models is multifaceted, involving both benefits and challenges.

1. **Positive Transfer and Performance Improvement**: Shared parameters among different languages can lead to positive transfer, which generally enhances the performance of MNMT models. For instance, when a model is trained with a multilingual corpus, it can leverage shared knowledge from multiple languages, resulting in improved translation performance—especially for low-resource languages. This phenomenon is often referred to as "knowledge transfer," which allows models to generalize better compared to models trained on fewer languages or on a bilingual basis [2] [5]. Specifically, models that incorporate knowledge from several languages tend to perform better in translation tasks as they can benefit from diverse linguistic structures and semantics, enhancing their ability to handle under-resourced languages effectively [5].

2. **Interference Issues**: However, the shared parameters can also lead to interference, a phenomenon where the performance of the model is adversely affected due to conflicting information from different languages. This challenge is termed the "curse of multilinguality," where the overlap of parameters intended for generalization may also introduce noise, leading to reduced efficiency in translation tasks [5]. For example, during training, the interactions among languages can complicate the learning process, potentially deteriorating performance on specific language pairs due to inconsistent data quality or conflicts in language-specific features [4].

3. **Architectural Innovations to Mitigate Issues**: To address these interference challenges while maximizing the benefits of shared parameters, researchers have proposed adaptive and sparse architectures. These models are designed to learn both shared and language-specific parameters, thus facilitating positive transfer while reducing interference. The sparse architecture activates only a relevant sub-network depending on the input language, enabling improved inference efficiency without compromising the translation quality [4]. This innovation has shown promise by achieving significant BLEU score improvements across various benchmarks without increasing computational costs, highlighting that careful architectural choices can manage the complexity of multilingual models effectively [4].

4. **Complexity Management**: The complexity of training and maintaining multiple models is alleviated by using a single multilingual framework, which reduces the need for numerous bilingual models for each language pair. This not only streamlines deployment and maintenance but also allows for more extensive use of collective data, which is critical for training robust language models [5]. Nonetheless, this simplification comes with the need for rigorous design to ensure that the shared parameters genuinely provide benefits without succumbing to the downsides of interference.

In conclusion, while parameter sharing in MNMT models holds great potential for enhancing performance, especially through knowledge transfer, it also poses challenges due to the risk of interference. Innovations in model architecture are crucial for optimizing these systems, balancing complexity and performance effectively across diverse languages.

1. [1]:  https://ar5iv.org/html/2407.05975, [2407.05975] LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages
2. [2]:  https://ar5iv.org/html/2405.10936, [2405.10936] A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers
3. [3]:  https://ar5iv.org/html/2405.10936, [2405.10936] A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers
4. [4]:  https://ar5iv.org/html/2104.07358, [2104.07358] Adaptive Sparse Transformer for Multilingual Translation
5. [5]:  https://ar5iv.org/html/2104.07358, [2104.07358] Adaptive Sparse Transformer for Multilingual Translation
---
1. [1]:  Passage ID 1: capability transfer.Zhao et al. (2024b)Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, and Lidong Bing. 2024b.How do large language models handle multilingualism?Zhu et al. (2024a)Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, and Alexandra Birch. 2024a.Question translation training for better multilingual reasoning.Zhu et al. (2024b)Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2024b.Multilingual machine translation with large language models: Empirical results and analysis.In Findings of the Association for Computational Linguistics: NAACL 2024.Outline•Section A: Discussion about the generalizability and reliability of the work.•Section B: The comprehensive details of the training data, including monolingual and parallel data, and the evaluation benchmark (Table 9).•Section C: The detailed information of different models, including
2. [2]:  Passage ID 2: the scale of training data, which improves performance across various language tasks.Recent research also indicates that scaling up model parameters enhances performance of PLMs [45], leading to the emergence of large-scale PLMs such as GPT-3 (175B) [43], OPT (175B) [46], PaLM (540B) [47] and Switch-Transformers (1.6T) [48], boasting billions or even trillions of parameters.Besides, a new training strategy “translation language model (TLM)” is designed specifically for enhancing multilingual capabilities tasks compared to the masked language model (e.g., BERT [39]), involving translation between multiple languages [49].2.3 Multilingual Paradigm TransitionAs shown in Figure 2, multilingual pre-trainedlanguage models (PLMs) are dominated by small language models (i.e., parameters less than 7B).They are designed for specific tasks, such as multilingual neural machine translation (NMT) [31, 50, 51], multilingual question answering (QA) [52, 53, 54] and multilingual
3. [3]:  Passage ID 3: knowledge stimulates or deteriorates with each other and investigate whether the model always outperforms its counterpart trained solely on the monolingual language corpus when further trained on the whole multilingual corpus.For instance, the ability of the Spanish model trained in 6 languages surpasses that of a model only trained on a monolingual corpus in Spanish.Despite underlying conflicts between different language-specific medical knowledge and potential biases due to varying data, the performance boost suggests that cross-lingual joint training promotes the performance of medical LLMs, shedding light on the potential of cross-lingual pre-training.Thus, further exploration into the effectiveness of the real-world and pseudo data is yet to be undertaken.Second, the ongoing scarcity of medical data in various languages persistently hampers further advancement.Although translation can mitigate some of these issues, it may not be effective due to the complex medical
4. [4]:  Passage ID 4: parameters shared among languages are the cause of interference while they may also enable positive transfer. Based on these insights, we propose an adaptive and sparse architecture for multilingual modeling, and train the model to learn shared and language-specific parameters to improve the positive transfer and mitigate the interference. The sparse architecture only activates a sub-network which preserves inference efficiency, and the adaptive design selects different sub-networks based on the input languages. Our model outperforms strong baselines across multiple benchmarks. On the large-scale OPUS dataset with 100100100 languages, we achieve +2.12.1+2.1, +1.31.3+1.3 and +6.26.2+6.2 BLEU improvements in one-to-many, many-to-one and zero-shot tasks respectively compared to standard Transformer without increasing the inference cost.1 IntroductionMultilingual neural machine translation (MNMT) develops one model for translations in multiple language directions Tan et al. (2019). A
5. [5]:  Passage ID 5: without increasing the inference cost.1 IntroductionMultilingual neural machine translation (MNMT) develops one model for translations in multiple language directions Tan et al. (2019). A key advantage of multilingual models is the knowledge transfer, which improves the translation performance especially for low-resource languages Zoph et al. (2016). Multilingual models tend to generalize better compared with bilingual translation due to the exposure to diverse languages Zoph and Knight (2016); Arivazhagan et al. (2019). Moreover, it is burdensome to train hundreds of bilingual models for each language pair, and one multilingual model reduces the deployment and maintenance cost Dabre et al. (2020).A known challenge for multilingual modeling is the curse of multilinguality, where the language interference hurts model performance Conneau et al. (2019).Language adapters attract research attention due to their strong performance in cross-lingual modeling Wang et al. (2019).