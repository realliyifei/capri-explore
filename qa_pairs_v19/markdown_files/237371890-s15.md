# Survey of Low-Resource Machine Translation

## Question

What is the impact of parameter sharing on the performance and complexity of multilingual machine translation models?

## URLs

1. https://ar5iv.org/html/2302.09650. [2302.09650] Scaling Laws for Multilingual Neural Machine Translation
2. https://ar5iv.org/html/2212.07530. [2212.07530] Causes and Cures for Interference in Multilingual Translation
3. https://ar5iv.org/html/2405.10936. [2405.10936] A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers
4. https://ar5iv.org/html/2104.07358. [2104.07358] Adaptive Sparse Transformer for Multilingual Translation
5. https://ar5iv.org/html/2401.08417. [2401.08417] Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation
6. https://ar5iv.org/html/2410.12883. [2410.12883] Scaling Laws for Multilingual Language Models
7. https://ar5iv.org/html/2411.11072. [2411.11072] Multilingual Large Language Models: A Systematic Survey
8. https://ar5iv.org/html/2407.13469. [2407.13469] Fixed and Adaptive Simultaneous Machine Translation Strategies Using Adapters
9. https://ar5iv.org/html/2407.05975. [2407.05975] LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages

## Answer (I don't know?: ✗)

Parameter sharing significantly impacts the performance and complexity of multilingual machine translation (MT) models, providing both benefits and challenges. By sharing model parameters across different languages, multilingual NMT models can leverage the strengths and knowledge from multiple languages, particularly aiding low-resource languages by learning from data available in richer-resource languages. This approach can lead to improved translation performance for low-resource language pairs through joint training, as these models can benefit from correlations among languages [2][3][4].

There are various configurations for parameter sharing in multilingual NMT models. For instance, one strategy involves independent components for each language, where the encoders, decoders, and attention mechanisms do not share parameters. This setup allows the model to capture language-specific nuances but may lead to increased model complexity due to the need to train separate parameters for each language [2]. Alternatively, configurations where all components are fully shared can reduce complexity by enabling a unified model framework, which can enhance efficiency and performance through the pooling of multilingual learning [2][3].

The use of shared parameters fosters what's known as zero-shot translation, the capability of a model to translate between language pairs that were not part of the training data. By leveraging knowledge from multiple languages, models can facilitate translations even in previously unseen language pairs. This feature is particularly valuable in scenarios where specific language pairs lack sufficient training data, demonstrating the extensive potential of parameter sharing in enhancing multilingual capabilities [2][3].

In terms of performance, shared parameters can lead to a more efficient learning process. The model's ability to learn from diverse linguistic structures and patterns across several languages translates into improved outcomes, especially for less represented languages. However, this performance boost comes with the potential trade-off of increased model complexity in terms of balancing shared and specific behaviors appropriately, which can complicate model interpretability and management [2][5].

In summary, parameter sharing in multilingual machine translation models can enhance performance, specifically aiding low-resource language translations and enabling zero-shot capabilities. Simultaneously, it presents challenges regarding model complexity and the potential need for intricate designs to ensure effective learning and performance across different languages [2][3][5].

1. [1]:  https://ar5iv.org/html/2410.22335, No Title
2. [2]:  https://ar5iv.org/html/2107.04239, No Title
3. [3]:  https://ar5iv.org/html/2107.04239, No Title
4. [4]:  https://ar5iv.org/html/2107.04239, No Title
5. [5]:  https://ar5iv.org/html/2408.12079, No Title
---
1. [1]:  Passage ID 1: information. Furthermore, we explored key factors contributing to the model’s performance improvement, including but not limited to the design of the network architecture, optimization of training strategies, and adjustment of hyperparameters. These analyses not only deepen our understanding of the model’s internal working mechanisms but also provide valuable insights and guidance for future research, especially in terms of further enhancing the performance and application scope of machine translation systems.2 PreliminaryMachine translation, as an important branch of the field of Natural Language Processing (NLP), aims to achieve automatic conversion from one language to another. Early machine translation methods were primarily based on rules and dictionaries. Since the mid-20th century, machine translation has undergone a transition from rule-based translation to statistical methods Brown et al. (1993); Lopez (2008), and to the current Neural Machine Translation (NMT)
2. [2]:  Passage ID 2: training and maintenance compared with training multiple separate models, and can collectively learn the knowledge from multiple languages to help low-resource languages. Second, low-resource language pairs benefit from related rich-resource languages pairs through joint training. Moreover, multilingual NMT offers the possibility to translate on language pairs that are unseen during training, which is called zero-shot translation. In the following paragraphs, we summarize the works on multilingual training from three perspectives (i.e., parameter sharing, designs for low-resource languages and zero-shot translation).Parameter sharing.There are different ways to share model parameters in multilingual training. First, all the encoder, decoder and attention components are independent among different languages Luong et al. (2015); Dong et al. (2015); Zoph and Knight (2016). Second, fully shared encoder, decoder and attention components are considered across languages, where a
3. [3]:  Passage ID 3: training and maintenance compared with training multiple separate models, and can collectively learn the knowledge from multiple languages to help low-resource languages. Second, low-resource language pairs benefit from related rich-resource languages pairs through joint training. Moreover, multilingual NMT offers the possibility to translate on language pairs that are unseen during training, which is called zero-shot translation. In the following paragraphs, we summarize the works on multilingual training from three perspectives (i.e., parameter sharing, designs for low-resource languages and zero-shot translation).Parameter sharing.There are different ways to share model parameters in multilingual training. First, all the encoder, decoder and attention components are independent among different languages Luong et al. (2015); Dong et al. (2015); Zoph and Knight (2016). Second, fully shared encoder, decoder and attention components are considered across languages, where a
4. [4]:  Passage ID 4: training and maintenance compared with training multiple separate models, and can collectively learn the knowledge from multiple languages to help low-resource languages. Second, low-resource language pairs benefit from related rich-resource languages pairs through joint training. Moreover, multilingual NMT offers the possibility to translate on language pairs that are unseen during training, which is called zero-shot translation. In the following paragraphs, we summarize the works on multilingual training from three perspectives (i.e., parameter sharing, designs for low-resource languages and zero-shot translation).Parameter sharing.There are different ways to share model parameters in multilingual training. First, all the encoder, decoder and attention components are independent among different languages Luong et al. (2015); Dong et al. (2015); Zoph and Knight (2016). Second, fully shared encoder, decoder and attention components are considered across languages, where a
5. [5]:  Passage ID 5: Memory High-quality filtering1 IntroductionThere are many languages around the world that are on the brink of extinction.Machine translation plays a crucial role in preserving endangered languages.However, a common challenge in this endeavor is the scarcity of parallel corpora available online, which hinders the development of effective translation systems.In recent years, the rise of Neural Machine Translation (NMT) [18] make to a technological leap.At the beginning of the research of NMT, researchers used deep neural networks, especially Recurrent Neural Network (RNN) and Long Short-term Memory Networks (LSTM), to better capture language context and complex structures.Moreover, some researchers transfered Convolution Neural Network (CNN) in the classfication task of Natural Language Processing (NLP) [16].However, today, most of the researchers design their models based on transformer [19].The transformer model, based on attention mechanisms, allows more flexible focus