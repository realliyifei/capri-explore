# The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations Conference or Workshop Item The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations

## Question

What distinguishes rating scales from Likert scales in survey design?

## URLs

1. https://ar5iv.org/html/2001.03231. [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
2. https://ar5iv.org/html/2403.10136. [2403.10136] Response Style Characterization for Repeated Measures Using the Visual Analogue Scale
3. https://ar5iv.org/html/2405.18638. [2405.18638] ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models
4. https://ar5iv.org/html/1901.09659. [1901.09659] Simple Surveys: Response Retrieval Inspired by Recommendation Systems
5. https://ar5iv.org/html/2409.07934. [2409.07934] Modeling Human Responses by Ordinal Archetypal Analysis
6. https://ar5iv.org/html/2005.10716. [2005.10716] Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation
7. https://ar5iv.org/html/2109.09195. [2109.09195] Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries
8. https://ar5iv.org/html/2205.11930. [2205.11930] The Authenticity Gap in Human Evaluation
9. https://ar5iv.org/html/2412.00543. [2412.00543] Evaluating the Consistency of LLM Evaluators

## Answer (I don't know?: ✗)

In survey design, the distinction between rating scales and Likert scales primarily lies in the structure and intent of the scales used to gauge respondents' attitudes or perceptions.

A Likert scale is specifically designed to measure attitudes and is characterized by a set of statements—often referred to as items—where respondents select a level of agreement or disagreement with each statement. Originally created by Rensis Likert in 1932, a typical Likert scale includes a range of responses (such as strongly disagree to strongly agree), allowing researchers to quantify attitudes toward a particular subject matter [2]. It involves a unidimensional construct, typically focusing on one specific aspect of the attitude being measured.

In contrast, rating scales may encompass a broader range of measurement types. Rating scales can include various formats, such as numeric ratings, star ratings, or semantic differential scales (where respondents rate something between two opposing descriptors like "human-like" to "machine-like") [1]. The key difference here is that while Likert scales focus specifically on agreement with statements, rating scales may evaluate a variety of dimensions and do not necessarily involve attitudinal agreement. For example, a rating scale might ask respondents to rate their satisfaction on a scale from 1 to 10, which does not involve agreement with a particular statement and can cover various aspects of perception or experience.

Moreover, the psychometric properties and analysis expectations between the two can be different. Likert scales often follow specific guidelines for construction that address validity concerns and the number of response options, which may vary based on research goals [3]. In contrast, rating scales may not adhere to such stringent guidelines regarding response formats.

Overall, the key distinction lies in the intent of measurement (attitudinal agreement for Likert scales versus broader evaluation in rating scales) and in the specific design features and analysis methods that accompany each type of scale [4] [5].

1. [1]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
2. [2]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
3. [3]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
4. [4]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
5. [5]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
---
1. [1]:  Passage ID 1: format. To avoid such confusion, it is important to be precise when describing a Likert scale as a five-option response format has a very different meaning from a five-item Likert scale. Furthermore, a set of items that prompts the user to select a rating on a bipolar scale of antonyms, i.e., human-like to machine-like, is not a true Likert scale. This is a semantic differential scale and should be referred to as such (Verhagenet al., 2015).Recommendation - We recommend that HRI researchers be deliberate when describing Likert response formats and scales to avoid confusion and misinterpretation.2.2. DesignBecause HRI is a relatively new field, HRI researchers often explore novel problems for which they appropriately need to craft problem-specific scales. However, care must be taken to correctly design and assess the validity of these scales before utilizing them for research. The design of the scale is one of the least agreed upon topics pertaining to Likert questionnaires
2. [2]:  Passage ID 2: is the first targeted at the HRI community, and we believe it is important to ground our discussion in the current understanding of the best methods related to the construction and testing of Likert data as found in the psychometric literature.Many of the debates surrounding Likert scale design and analysis are unsettled. As such, we present both sides of these arguments and reason through the areas of agreement and disagreement to arrive at our own recommendations for how HRI researchers can best navigate these often murky waters.2.1. What is a Likert Scale?Likert scales were created in 1932 by Rensis Likert and were originally designed to scientifically measure attitude (Likert, 1932). A Likert scale is defined as ”a set of statements(items) offered for a real or hypothetical situationunder study” in which an individual must choose their level of agreement with a series of statements (Joshiet al., 2015). The original response scale for a Likert item ranged from one to
3. [3]:  Passage ID 3: taken to correctly design and assess the validity of these scales before utilizing them for research. The design of the scale is one of the least agreed upon topics pertaining to Likert questionnaires in the psychometric literature. Disagreement arises around the optimal number or response choices in an item, the ideal number of items that should comprise a scale, whether a scale should be balanced, and whether or not to include a neutral midpoint. Below, we address each topic.Number of Response Options -Rensis Likert himself suggested a five point response format in his seminal work, A Technique for the Measurement of Attitudes (Likert, 1932). However, Likert did not base this decision in theory and rather suggested that variations on this five-point format may be appropriate (Likert, 1932). Further investigation has yet to provide a consensus on the optimal number of response options comprising a Likert item (Matell and Jacoby, 1971). (Preston andColman, 2000) found that scales
4. [4]:  Passage ID 4: statistical testing and questionnaire design so that we can stand more confidently in the inferences drawn from these data.2. Literature Review & Best PracticesLikert scales play a key role in the study of human-robot interaction. Between 2016 and 2019, Likert-type questionnaires appeared in more than 50% of all HRI papers. As such, it is imperative that we make proper use of Likert scales and are careful in our design and analysis so as not to de-legitimize our findings. We begin with a literature review to investigate the current best practices for Likert scale design and statistical testing. We acknowledge that reviews concerning the design and analysis of Likert scales have been previously conducted (Subedi, 2016; Carifio and Perla, 2007; Jamieson, 2004). However, our analysis is the first targeted at the HRI community, and we believe it is important to ground our discussion in the current understanding of the best methods related to the construction and testing of Likert
5. [5]:  Passage ID 5: interaction (HRI) grows accordingly. As such, we should endeavor to employ the best statistical practices. Likert scales are commonly used metrics in HRI to measure perceptions and attitudes. Due to misinformation or honest mistakes, most HRI researchers do not adopt best practices when analyzing Likert data. We conduct a review of psychometric literature to determine the current standard for Likert scale design and analysis. Next, we conduct a survey of four years of the International Conference on Human-Robot Interaction (2016 through 2019) and report on incorrect statistical practices and design of Likert scales. During these years, only 3 of the 110 papers applied proper statistical testing to correctly-designed Likert scales. Our analysis suggests there are areas for meaningful improvement in the design and testing of Likert scales. Lastly, we provide recommendations to improve the accuracy of conclusions drawn from Likert data.Metrics for HRI; Likert Scales; Statistical