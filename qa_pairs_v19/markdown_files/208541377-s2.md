# The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations Conference or Workshop Item The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations

## Question

What distinguishes rating scales from Likert scales in survey design?

## URLs

1. https://ar5iv.org/html/2001.03231. [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
2. https://ar5iv.org/html/2403.10136. [2403.10136] Response Style Characterization for Repeated Measures Using the Visual Analogue Scale
3. https://ar5iv.org/html/2405.18638. [2405.18638] ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models
4. https://ar5iv.org/html/1901.09659. [1901.09659] Simple Surveys: Response Retrieval Inspired by Recommendation Systems
5. https://ar5iv.org/html/2409.07934. [2409.07934] Modeling Human Responses by Ordinal Archetypal Analysis
6. https://ar5iv.org/html/2005.10716. [2005.10716] Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation
7. https://ar5iv.org/html/2109.09195. [2109.09195] Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries
8. https://ar5iv.org/html/2205.11930. [2205.11930] The Authenticity Gap in Human Evaluation
9. https://ar5iv.org/html/2412.00543. [2412.00543] Evaluating the Consistency of LLM Evaluators

## Answer (I don't know?: ✗)

In survey design, the primary distinction between rating scales and Likert scales lies in their structure and purpose. A Likert scale is a specific type of rating scale that is designed to measure attitudes by asking respondents to indicate their level of agreement or disagreement with a series of statements related to a particular topic. Likert scales typically have a defined number of response options, most commonly a five-point format ranging from "strongly disagree" to "strongly agree" [2][3]. This design allows researchers to gauge the intensity of respondents' feelings towards the statements presented.

On the other hand, rating scales can take various forms and do not necessarily adhere to the format established by Likert scales. For example, a rating scale might measure attributes on a continuum using different criteria, such as a scale ranging from "poor" to "excellent." A significant aspect to note is that while Likert scales measure attitudes through a series of statements, rating scales can vary widely in content and purpose, encompassing diverse measurement objectives beyond attitudes, since they might reflect more general evaluations of objects or experiences [1].

Additionally, it is important to clarify that not all instruments that use a scale of agreement or satisfaction are true Likert scales. For instance, a bipolar scale, which prompts users to select a rating between two antonyms (such as "human-like" to "machine-like"), is classified as a semantic differential scale rather than a Likert scale [1]. This distinction is crucial for researchers, especially in the field of Human-Robot Interaction (HRI), where precise measurement tools are important for quantifying participants' perceptions and attitudes towards robots [4]. 

The flexibility of rating scales allows for a broader application across different fields and contexts, whereas the focused nature of Likert scales, particularly in attitude measurement, makes them more suitable for assessing agreement levels in relation to specific statements [3][5]. As the design of scales is an area with ongoing debate, HRI researchers are encouraged to be meticulous when choosing and implementing these scales to ensure they are effectively measured and interpreted [2][5]. 

Ultimately, the choices in survey design, whether using a Likert scale or a more general rating scale, should be guided by the research objective, ensuring that the selected method aligns with the intended measurement and provides valid and reliable results.

1. [1]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
2. [2]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
3. [3]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
4. [4]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
5. [5]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
---
1. [1]:  Passage ID 1: format. To avoid such confusion, it is important to be precise when describing a Likert scale as a five-option response format has a very different meaning from a five-item Likert scale. Furthermore, a set of items that prompts the user to select a rating on a bipolar scale of antonyms, i.e., human-like to machine-like, is not a true Likert scale. This is a semantic differential scale and should be referred to as such (Verhagenet al., 2015).Recommendation - We recommend that HRI researchers be deliberate when describing Likert response formats and scales to avoid confusion and misinterpretation.2.2. DesignBecause HRI is a relatively new field, HRI researchers often explore novel problems for which they appropriately need to craft problem-specific scales. However, care must be taken to correctly design and assess the validity of these scales before utilizing them for research. The design of the scale is one of the least agreed upon topics pertaining to Likert questionnaires
2. [2]:  Passage ID 2: is the first targeted at the HRI community, and we believe it is important to ground our discussion in the current understanding of the best methods related to the construction and testing of Likert data as found in the psychometric literature.Many of the debates surrounding Likert scale design and analysis are unsettled. As such, we present both sides of these arguments and reason through the areas of agreement and disagreement to arrive at our own recommendations for how HRI researchers can best navigate these often murky waters.2.1. What is a Likert Scale?Likert scales were created in 1932 by Rensis Likert and were originally designed to scientifically measure attitude (Likert, 1932). A Likert scale is defined as ”a set of statements(items) offered for a real or hypothetical situationunder study” in which an individual must choose their level of agreement with a series of statements (Joshiet al., 2015). The original response scale for a Likert item ranged from one to
3. [3]:  Passage ID 3: taken to correctly design and assess the validity of these scales before utilizing them for research. The design of the scale is one of the least agreed upon topics pertaining to Likert questionnaires in the psychometric literature. Disagreement arises around the optimal number or response choices in an item, the ideal number of items that should comprise a scale, whether a scale should be balanced, and whether or not to include a neutral midpoint. Below, we address each topic.Number of Response Options -Rensis Likert himself suggested a five point response format in his seminal work, A Technique for the Measurement of Attitudes (Likert, 1932). However, Likert did not base this decision in theory and rather suggested that variations on this five-point format may be appropriate (Likert, 1932). Further investigation has yet to provide a consensus on the optimal number of response options comprising a Likert item (Matell and Jacoby, 1971). (Preston andColman, 2000) found that scales
4. [4]:  Passage ID 4: statistical testing and questionnaire design so that we can stand more confidently in the inferences drawn from these data.2. Literature Review & Best PracticesLikert scales play a key role in the study of human-robot interaction. Between 2016 and 2019, Likert-type questionnaires appeared in more than 50% of all HRI papers. As such, it is imperative that we make proper use of Likert scales and are careful in our design and analysis so as not to de-legitimize our findings. We begin with a literature review to investigate the current best practices for Likert scale design and statistical testing. We acknowledge that reviews concerning the design and analysis of Likert scales have been previously conducted (Subedi, 2016; Carifio and Perla, 2007; Jamieson, 2004). However, our analysis is the first targeted at the HRI community, and we believe it is important to ground our discussion in the current understanding of the best methods related to the construction and testing of Likert
5. [5]:  Passage ID 5: interaction (HRI) grows accordingly. As such, we should endeavor to employ the best statistical practices. Likert scales are commonly used metrics in HRI to measure perceptions and attitudes. Due to misinformation or honest mistakes, most HRI researchers do not adopt best practices when analyzing Likert data. We conduct a review of psychometric literature to determine the current standard for Likert scale design and analysis. Next, we conduct a survey of four years of the International Conference on Human-Robot Interaction (2016 through 2019) and report on incorrect statistical practices and design of Likert scales. During these years, only 3 of the 110 papers applied proper statistical testing to correctly-designed Likert scales. Our analysis suggests there are areas for meaningful improvement in the design and testing of Likert scales. Lastly, we provide recommendations to improve the accuracy of conclusions drawn from Likert data.Metrics for HRI; Likert Scales; Statistical