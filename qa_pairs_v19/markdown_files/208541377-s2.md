# The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations Conference or Workshop Item The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations

## Question

What distinguishes rating scales from Likert scales in survey design?

## URLs

1. https://ar5iv.org/html/2001.03231. [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
2. https://ar5iv.org/html/2403.10136. [2403.10136] Response Style Characterization for Repeated Measures Using the Visual Analogue Scale
3. https://ar5iv.org/html/2405.18638. [2405.18638] ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models
4. https://ar5iv.org/html/1901.09659. [1901.09659] Simple Surveys: Response Retrieval Inspired by Recommendation Systems
5. https://ar5iv.org/html/2409.07934. [2409.07934] Modeling Human Responses by Ordinal Archetypal Analysis
6. https://ar5iv.org/html/2005.10716. [2005.10716] Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation
7. https://ar5iv.org/html/2109.09195. [2109.09195] Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries
8. https://ar5iv.org/html/2205.11930. [2205.11930] The Authenticity Gap in Human Evaluation
9. https://ar5iv.org/html/2412.00543. [2412.00543] Evaluating the Consistency of LLM Evaluators

## Answer (I don't know?: ✗)

In survey design, the key distinction between rating scales and Likert scales lies primarily in their structure and purpose. While both are used to quantify subjective perceptions and attitudes, they differ in the types of responses they solicit and how those responses are interpreted.

1. **Response Format**: A Likert scale is specifically defined by presenting a set of statements (items) related to a particular topic where respondents indicate their level of agreement or disagreement on a symmetrical scale, usually ranging from "strongly disagree" to "strongly agree" [2]. This is distinct from general rating scales, which may not necessarily focus on agreement with statements and could encompass a wider variety of response types, such as rankings or evaluations on specific attributes, leaving room for less structured responses [1].

2. **Content and Structure**: Likert scales typically involve bipolar scales which measure attitudes along a single dimension. For example, respondents might be asked to select their level of agreement with statements about their feelings toward a robot [2]. In contrast, rating scales might ask for evaluations on multiple dimensions simultaneously, such as satisfaction, likelihood to recommend, or effectiveness. These scales can be unipolar or bipolar but do not necessarily follow the specific agreement format intrinsic to Likert scales.

3. **Psychometric Considerations**: In psychometrics, the validity and reliability of the scale are crucial. In the context of Likert scales, much debate surrounds their design—such as the number of response options and whether to include a neutral midpoint [3]. This discourse is less pronounced in more general rating scales, which might adopt differing methodologies without the same theoretical foundations.

4. **Purpose in Research**: Likert scales are predominantly used to measure attitudes, providing clear insights into the intensity of feelings towards a topic. This makes them highly suitable for attitudinal research, especially within fields like Human-Robot Interaction (HRI) where measuring perceptions can be critical [4]. Rating scales, on the other hand, can be leveraged in a broader range of contexts, serving various spatial or categorical purposes beyond attitudinal measurement, such as satisfaction ratings or performance assessments.

5. **Common Misunderstandings**: Importantly, the context and recommended best practices for each type differ; HRI researchers specifically need to be cognizant of these differences to avoid confusion and misinterpretation of data [1] [5]. There is also a tendency for researchers to use the terms interchangeably, which can dilute the precision required for effective analysis and interpretation in studies [1].

In summary, while both Likert scales and rating scales play significant roles in survey design, they are fundamentally different in their format, structure, purpose, and the psychometric principles that underpin their use. The distinctions are essential for researchers to accurately extrapolate and communicate their findings.

1. [1]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
2. [2]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
3. [3]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
4. [4]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
5. [5]:  https://ar5iv.org/html/2001.03231, [2001.03231] Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies
---
1. [1]:  Passage ID 1: format. To avoid such confusion, it is important to be precise when describing a Likert scale as a five-option response format has a very different meaning from a five-item Likert scale. Furthermore, a set of items that prompts the user to select a rating on a bipolar scale of antonyms, i.e., human-like to machine-like, is not a true Likert scale. This is a semantic differential scale and should be referred to as such (Verhagenet al., 2015).Recommendation - We recommend that HRI researchers be deliberate when describing Likert response formats and scales to avoid confusion and misinterpretation.2.2. DesignBecause HRI is a relatively new field, HRI researchers often explore novel problems for which they appropriately need to craft problem-specific scales. However, care must be taken to correctly design and assess the validity of these scales before utilizing them for research. The design of the scale is one of the least agreed upon topics pertaining to Likert questionnaires
2. [2]:  Passage ID 2: is the first targeted at the HRI community, and we believe it is important to ground our discussion in the current understanding of the best methods related to the construction and testing of Likert data as found in the psychometric literature.Many of the debates surrounding Likert scale design and analysis are unsettled. As such, we present both sides of these arguments and reason through the areas of agreement and disagreement to arrive at our own recommendations for how HRI researchers can best navigate these often murky waters.2.1. What is a Likert Scale?Likert scales were created in 1932 by Rensis Likert and were originally designed to scientifically measure attitude (Likert, 1932). A Likert scale is defined as ”a set of statements(items) offered for a real or hypothetical situationunder study” in which an individual must choose their level of agreement with a series of statements (Joshiet al., 2015). The original response scale for a Likert item ranged from one to
3. [3]:  Passage ID 3: taken to correctly design and assess the validity of these scales before utilizing them for research. The design of the scale is one of the least agreed upon topics pertaining to Likert questionnaires in the psychometric literature. Disagreement arises around the optimal number or response choices in an item, the ideal number of items that should comprise a scale, whether a scale should be balanced, and whether or not to include a neutral midpoint. Below, we address each topic.Number of Response Options -Rensis Likert himself suggested a five point response format in his seminal work, A Technique for the Measurement of Attitudes (Likert, 1932). However, Likert did not base this decision in theory and rather suggested that variations on this five-point format may be appropriate (Likert, 1932). Further investigation has yet to provide a consensus on the optimal number of response options comprising a Likert item (Matell and Jacoby, 1971). (Preston andColman, 2000) found that scales
4. [4]:  Passage ID 4: statistical testing and questionnaire design so that we can stand more confidently in the inferences drawn from these data.2. Literature Review & Best PracticesLikert scales play a key role in the study of human-robot interaction. Between 2016 and 2019, Likert-type questionnaires appeared in more than 50% of all HRI papers. As such, it is imperative that we make proper use of Likert scales and are careful in our design and analysis so as not to de-legitimize our findings. We begin with a literature review to investigate the current best practices for Likert scale design and statistical testing. We acknowledge that reviews concerning the design and analysis of Likert scales have been previously conducted (Subedi, 2016; Carifio and Perla, 2007; Jamieson, 2004). However, our analysis is the first targeted at the HRI community, and we believe it is important to ground our discussion in the current understanding of the best methods related to the construction and testing of Likert
5. [5]:  Passage ID 5: interaction (HRI) grows accordingly. As such, we should endeavor to employ the best statistical practices. Likert scales are commonly used metrics in HRI to measure perceptions and attitudes. Due to misinformation or honest mistakes, most HRI researchers do not adopt best practices when analyzing Likert data. We conduct a review of psychometric literature to determine the current standard for Likert scale design and analysis. Next, we conduct a survey of four years of the International Conference on Human-Robot Interaction (2016 through 2019) and report on incorrect statistical practices and design of Likert scales. During these years, only 3 of the 110 papers applied proper statistical testing to correctly-designed Likert scales. Our analysis suggests there are areas for meaningful improvement in the design and testing of Likert scales. Lastly, we provide recommendations to improve the accuracy of conclusions drawn from Likert data.Metrics for HRI; Likert Scales; Statistical