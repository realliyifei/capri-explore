# Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing

## Question

How does parameter sharing work in multilingual joint learning, and what are its implications for designing models?

## URLs

1. https://ar5iv.org/html/2305.13286. [2305.13286] How Influential is Multilingual Data? Studying the Cross-lingual Sharing Mechanism of LLMs
2. https://ar5iv.org/html/2305.14230. [2305.14230] Exploring Representational Disparities Between Multilingual and Bilingual Translation Models
3. https://ar5iv.org/html/2209.01530. [2209.01530] Informative Language Representation Learning for Massively Multilingual Neural Machine Translation
4. https://ar5iv.org/html/2406.17257. [2406.17257] Leveraging Parameter-Efficient Transfer Learning for Multi-Lingual Text-to-Speech Adaptation
5. https://ar5iv.org/html/2408.01394. [2408.01394] Improving Multilingual Neural Machine Translation by Utilizing Semantic and Linguistic Features
6. https://ar5iv.org/html/2112.13619. [2112.13619] Parameter Differentiation based Multilingual Neural Machine Translation
7. https://ar5iv.org/html/2402.17934. [2402.17934] Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures
8. https://ar5iv.org/html/2209.05735. [2209.05735] Learning ASR pathways: A sparse multilingual ASR model
9. https://ar5iv.org/html/1911.01464. [1911.01464] Emerging Cross-lingual Structure in Pretrained Language Models

## Answer (I don't know?: ✗)

Parameter sharing in multilingual joint learning is a crucial mechanism that enables multilingual large language models (MLLMs) to leverage knowledge across different languages. The concept revolves around the shared use of model parameters to enhance learning efficiency and improve performance on low-resource languages through information transfer from high-resource languages. This approach is motivated by the assumption that MLLMs can recognize and exploit common universal patterns across multiple languages, thereby obtaining significant gains in zero-shot cross-lingual transfer performance [1].

When discussing parameter sharing, research indicates that MLLMs consist of both language-specific and language-neutral parameters. Studies employing methods to prune model parameters have shown that while some parameters are shared across languages, others are distinctly language-specific. This dual structure facilitates the model’s ability to benefit from certain parameter updates that enhance performance on one language while potentially hindering it on another, a phenomenon known as negative interference [2]. The design of MLLMs can thus be informed by understanding how to optimize the sharing of parameters to minimize these negative interactions.

Furthermore, parameters can be organized in a manner that reflects linguistic proximities among languages. The efficiency of parameter sharing can correlate with the similarities between the languages involved, suggesting that MLLMs can achieve better cross-lingual performance when parameters are appropriately shared based on linguistic relationships [4]. This insight is foundational for the model design as it suggests methods to strategically group languages during the training process to enhance knowledge transfer.

Moreover, the fine-tuning process plays a significant role in how parameter sharing unfolds. Initially, MLLMs utilize data from multiple languages early in the fine-tuning stages, with the reliance on this multilingual data increasing as training progresses. This gradual increase in cross-lingual data reliance allows MLLMs to develop a rich understanding of linguistic patterns across different languages, enhancing their ability to perform effectively on tasks involving unseen languages [1]. 

The implications for the design of MLLMs are significant: they must be constructed to support efficient cross-lingual sharing of parameters, potentially utilizing subnetworks that can be dynamically adjusted to emphasize language-neutral parameters while minimizing the interference effects of language-specific ones. This architecture enhances the model's overall performance while addressing the challenges associated with the diversity of languages.

In summary, parameter sharing in multilingual joint learning is essential for enabling MLLMs to effectively learn from and transfer knowledge across languages. These shared parameters can improve performance, especially in low-resource contexts, and offer insights that can guide the design of models aimed at optimizing multilingual performance [3] [5]. Such strategies are critical for advancing the state-of-the-art in NLP, particularly in multilingual applications.

1. [1]:  https://ar5iv.org/html/2305.13286, [2305.13286] How Influential is Multilingual Data? Studying the Cross-lingual Sharing Mechanism of LLMs
2. [2]:  https://ar5iv.org/html/2305.13286, [2305.13286] How Influential is Multilingual Data? Studying the Cross-lingual Sharing Mechanism of LLMs
3. [3]:  https://ar5iv.org/html/2305.13286, [2305.13286] How Influential is Multilingual Data? Studying the Cross-lingual Sharing Mechanism of LLMs
4. [4]:  https://ar5iv.org/html/2112.13619, [2112.13619] Parameter Differentiation based Multilingual Neural Machine Translation
5. [5]:  https://ar5iv.org/html/2305.13286, [2305.13286] How Influential is Multilingual Data? Studying the Cross-lingual Sharing Mechanism of LLMs
---
1. [1]:  Passage ID 1: studied cross-lingual sharing at the level of model parameters, we present the first approach to study cross-lingual sharing at the data level. We find that MLLMs rely on data from multiple languages from the early stages of fine-tuning and that this reliance gradually increases as fine-tuning progresses. We further study how different fine-tuning languages influence model performance on a given test language and find that they can both reinforce and complement the knowledge acquired from data of the test language itself.1 IntroductionMultilingual joint learning is often motivated by the idea thatwhen multilingual large language models (MLLMs) learn information for multiple languages simultaneously, they can detect and leverage common universal patterns across them. Thus, these models can exploit data from one language to learn generalisations useful for another, obtaining impressive performance on zero-shot cross-lingual transfer for many languages (Wu and Dredze, 2019).
2. [2]:  Passage ID 2: model representation, Wang et al. (2020) explicitly test for the existence of language-specific and language-neutral parameters instead. They do so by employing a pruning method (Louizos et al., 2018) to determine the importance of model parameters across languages, and find that some parameters are shared while others remain language-specific. Moreover, Wang et al. (2020) focused on the negative interference effects (Ruder, 2017) of cross-lingual sharing i.e., parameter updates that help the model on one language, butharm its ability to handle another. They show that cross-lingual performance can be improved when parameters are more efficiently shared across languages, leading to a body of works on finding language-specific and language-neutral subnetworks within MLLMs to better understand (Foroutan et al., 2022) and guide (Lin et al., 2021; Choenni et al., 2022) cross-lingual sharing at the parameter level. In contrast to these works, we do not study cross-lingual sharing at the
3. [3]:  Passage ID 3: sharing in multilingual modelsMany methods have been proposed to investigate the cross-lingual abilities of MLLMs (Doddapaneni et al., 2021). Pires et al. (2019) and Karthikeyan et al. (2020) were the first to demonstrate that MLLMs share information cross-lingually by showing that they can perform zero-shot cross-lingual transfer between languages with zero lexical overlap. This led to a body of works that attempt to understand how and where this sharing emerges.One line of study focuses on how MLLMs distribute their parameters across languages by analyzing the distributional properties of the resulting language representations. In particular, these studies aim to understand to what extent MLLMs exploit universal language patterns for producing input representations in individual languages. As such, Singh et al. (2019) find that mBERT representations can be partitioned by language subspaces, suggesting that little cross-lingual sharing emerges. Yet, other works show that mBERT
4. [4]:  Passage ID 4: analyses reveal that the parameter sharing configuration obtained by our method correlates well with the linguistic proximities.1 IntroductionNeural machine translation (NMT) has achieved great success and drawn much attention in recent years(Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017).While conventional NMT can well handle the translation of a single language pair,training an individual model for each language pair is resource-consuming,considering there are thousands of languages in the world.Therefore, multilingual NMT is developed to handle multiple language pairs in one model,greatly reducing the cost of offline training and online deployment (Ha, Niehues, and Waibel 2016; Johnson et al. 2017).Besides,the parameter sharing in multilingual neural machine translationencourages positive knowledge transfer among different languagesand benefits low-resource translation(Zhang et al. 2020; Siddhant et al. 2020).Figure 1: The
5. [5]:  Passage ID 5: do languages influence each other? On how LLMs exploit multilingual data for individual languagesRochelle ChoenniUniversity of Amsterdamr.m.v.k.choenni@uva.nl&Dan GarretteGoogle Researchdhgarrette@google.com&Ekaterina ShutovaUniversity of Amsterdame.shutova@uva.nlHow do languages influence each other? Studying how LLMs exploit multilingual dataRochelle ChoenniUniversity of Amsterdamr.m.v.k.choenni@uva.nl&Dan GarretteGoogle Researchdhgarrette@google.com&Ekaterina ShutovaUniversity of Amsterdame.shutova@uva.nlHow do languages influence each other? Studying cross-lingual data sharing during LLM fine-tuningRochelle ChoenniUniversity of Amsterdamr.m.v.k.choenni@uva.nl&Dan GarretteGoogle Researchdhgarrette@google.com&Ekaterina ShutovaUniversity of Amsterdame.shutova@uva.nlAbstractMultilingual large language models (MLLMs) are jointly trained on data from many different languages such that representation of individual languages