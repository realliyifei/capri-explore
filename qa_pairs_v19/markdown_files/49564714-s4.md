# Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing

## Question

How does parameter sharing work in multilingual joint learning, and what are its implications for designing models?

## URLs

1. https://ar5iv.org/html/2305.13286. [2305.13286] How Influential is Multilingual Data? Studying the Cross-lingual Sharing Mechanism of LLMs
2. https://ar5iv.org/html/2305.14230. [2305.14230] Exploring Representational Disparities Between Multilingual and Bilingual Translation Models
3. https://ar5iv.org/html/2209.01530. [2209.01530] Informative Language Representation Learning for Massively Multilingual Neural Machine Translation
4. https://ar5iv.org/html/2406.17257. [2406.17257] Leveraging Parameter-Efficient Transfer Learning for Multi-Lingual Text-to-Speech Adaptation
5. https://ar5iv.org/html/2408.01394. [2408.01394] Improving Multilingual Neural Machine Translation by Utilizing Semantic and Linguistic Features
6. https://ar5iv.org/html/2112.13619. [2112.13619] Parameter Differentiation based Multilingual Neural Machine Translation
7. https://ar5iv.org/html/2402.17934. [2402.17934] Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures
8. https://ar5iv.org/html/2209.05735. [2209.05735] Learning ASR pathways: A sparse multilingual ASR model
9. https://ar5iv.org/html/1911.01464. [1911.01464] Emerging Cross-lingual Structure in Pretrained Language Models

## Answer (I don't know?: ✗)

Parameter sharing in multilingual joint learning is a technique that allows different language models to share components, such as encoders, decoders, and attention mechanisms, to improve translation performance across multiple languages. The extent and nature of parameter sharing can significantly impact the models' performance, particularly for low-resource languages. There are two main approaches to parameter sharing identified in the literature: fully shared models and language-specific models.

1. **Types of Parameter Sharing**: 
   - Models can operate with independent encoder, decoder, and attention components for each language. This means that each language model effectively functions separately without any shared learning benefits, as noted in various studies [1][2]. 
   - Alternatively, some models share all encoder, decoder, and attention components across languages. This shared architecture can help low-resource languages access knowledge learned from richer-resource language pairs [1][2].

2. **Trade-offs in Parameter Sharing**: 
   There are trade-offs involved in how parameters are shared among languages. On one hand, ensuring that each language possesses adequate parameters is crucial for its effective representation. On the other hand, sufficient sharing can benefit low-resource languages by leveraging the training data from high-resource languages. The challenge lies in striking a balance: sharing too liberally might hinder a particular language’s performance if its unique characteristics are not represented adequately, while excessive independence may prevent low-resource languages from benefiting from shared learning [3].

3. **Zero-Shot Translation**: 
   One significant advantage of multilingual NMT (Neural Machine Translation) systems is the ability to perform zero-shot translation. This capability implies that the model can translate between language pairs that were not explicitly included during training, relying on the shared knowledge from related languages [1][2]. For instance, if a model is trained with English-Spanish and English-French, it might manage to translate between Spanish and French even without direct training on that pair, showcasing the robustness provided by shared parameters.

4. **Impact of Model Design on Performance**: 
   Research shows that multilingual models can gradually increase their reliance on data from multiple languages as they fine-tune. This learning strategy allows joint learning approaches to reinforce and complement the knowledge of specific languages, thus improving overall model performance [4]. Moreover, studies have indicated that language-specific parameters may exist alongside shared parameters, which can help mitigate negative interference effects. This phenomenon occurs when an update beneficial for one language adversely affects another language's handling [5]. Efficient parameter sharing that identifies robust language-specific and neutral subnetworks can improve overall performance, leading to better task handling across languages.

In summary, effective parameter sharing in multilingual joint learning facilitates the learning process across multiple languages, enhances the model's ability to translate unseen language pairs, and presents significant design implications. It emphasizes a careful balance in parameter management to optimize both interlanguage benefits and individual language performance [3][5].

1. [1]:  https://ar5iv.org/html/2107.04239, No Title
2. [2]:  https://ar5iv.org/html/2107.04239, No Title
3. [3]:  https://ar5iv.org/html/2109.00486, No Title
4. [4]:  https://ar5iv.org/html/2305.13286, [2305.13286] How Influential is Multilingual Data? Studying the Cross-lingual Sharing Mechanism of LLMs
5. [5]:  https://ar5iv.org/html/2305.13286, [2305.13286] How Influential is Multilingual Data? Studying the Cross-lingual Sharing Mechanism of LLMs
---
1. [1]:  Passage ID 1: training and maintenance compared with training multiple separate models, and can collectively learn the knowledge from multiple languages to help low-resource languages. Second, low-resource language pairs benefit from related rich-resource languages pairs through joint training. Moreover, multilingual NMT offers the possibility to translate on language pairs that are unseen during training, which is called zero-shot translation. In the following paragraphs, we summarize the works on multilingual training from three perspectives (i.e., parameter sharing, designs for low-resource languages and zero-shot translation).Parameter sharing.There are different ways to share model parameters in multilingual training. First, all the encoder, decoder and attention components are independent among different languages Luong et al. (2015); Dong et al. (2015); Zoph and Knight (2016). Second, fully shared encoder, decoder and attention components are considered across languages, where a
2. [2]:  Passage ID 2: training and maintenance compared with training multiple separate models, and can collectively learn the knowledge from multiple languages to help low-resource languages. Second, low-resource language pairs benefit from related rich-resource languages pairs through joint training. Moreover, multilingual NMT offers the possibility to translate on language pairs that are unseen during training, which is called zero-shot translation. In the following paragraphs, we summarize the works on multilingual training from three perspectives (i.e., parameter sharing, designs for low-resource languages and zero-shot translation).Parameter sharing.There are different ways to share model parameters in multilingual training. First, all the encoder, decoder and attention components are independent among different languages Luong et al. (2015); Dong et al. (2015); Zoph and Knight (2016). Second, fully shared encoder, decoder and attention components are considered across languages, where a
3. [3]:  Passage ID 3: model components.The degree to which parameters are shared across multiple language directions varies considerably in the literature, with early models showing little sharing across languages (Dong et al., 2015) and some later models exploring the sharing of most or all parameters (Johnson et al., 2017). The amount of parameter sharing can be seen as a trade-off between ensuring that each language is sufficiently represented (has enough parameters allocated) and that low-resource languages can benefit from the joint training of parameters with other (higher-resource) language pairs (which also importantly reduces the complexity of the model by reducing the number of parameters required).Dong et al. (2015) present one of the earliest studies in multilingual NMT, focused on translation from a single language into multiple languages simultaneously. The central idea of this approach is to have a shared encoder and many language-specific decoders, including language-specific weights
4. [4]:  Passage ID 4: studied cross-lingual sharing at the level of model parameters, we present the first approach to study cross-lingual sharing at the data level. We find that MLLMs rely on data from multiple languages from the early stages of fine-tuning and that this reliance gradually increases as fine-tuning progresses. We further study how different fine-tuning languages influence model performance on a given test language and find that they can both reinforce and complement the knowledge acquired from data of the test language itself.1 IntroductionMultilingual joint learning is often motivated by the idea thatwhen multilingual large language models (MLLMs) learn information for multiple languages simultaneously, they can detect and leverage common universal patterns across them. Thus, these models can exploit data from one language to learn generalisations useful for another, obtaining impressive performance on zero-shot cross-lingual transfer for many languages (Wu and Dredze, 2019).
5. [5]:  Passage ID 5: model representation, Wang et al. (2020) explicitly test for the existence of language-specific and language-neutral parameters instead. They do so by employing a pruning method (Louizos et al., 2018) to determine the importance of model parameters across languages, and find that some parameters are shared while others remain language-specific. Moreover, Wang et al. (2020) focused on the negative interference effects (Ruder, 2017) of cross-lingual sharing i.e., parameter updates that help the model on one language, butharm its ability to handle another. They show that cross-lingual performance can be improved when parameters are more efficiently shared across languages, leading to a body of works on finding language-specific and language-neutral subnetworks within MLLMs to better understand (Foroutan et al., 2022) and guide (Lin et al., 2021; Choenni et al., 2022) cross-lingual sharing at the parameter level. In contrast to these works, we do not study cross-lingual sharing at the