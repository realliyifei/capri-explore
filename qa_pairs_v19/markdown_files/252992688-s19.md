# A Survey of Active Learning for Natural Language Processing

## Question

How do pre-annotation and interactive systems help in reducing annotation costs in active learning for NLP tasks?

## URLs

1. https://ar5iv.org/html/2401.07367. [2401.07367] Active Learning for NLP with Large Language Models
2. https://ar5iv.org/html/2404.15720. [2404.15720] Annotator-Centric Active Learning for Subjective NLP Tasks
3. https://ar5iv.org/html/2211.06224. [2211.06224] alanno: An Active Learning Annotation System for Mortals
4. https://ar5iv.org/html/2407.03895. [2407.03895] Scoping Review of Active Learning Strategies and their Evaluation Environments for Entity Recognition Tasks
5. https://ar5iv.org/html/2210.10109. [2210.10109] A Survey of Active Learning for Natural Language Processing
6. https://ar5iv.org/html/2406.09206. [2406.09206] Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models
7. https://ar5iv.org/html/2402.14101. [2402.14101] Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation
8. https://ar5iv.org/html/2402.15613. [2402.15613] Towards Efficient Active Learning in NLP via Pretrained Representations
9. https://ar5iv.org/html/1808.03806. [1808.03806] The Impact of Automatic Pre-annotation in Clinical Note Data Element Extraction - the CLEAN Tool

## Answer (I don't know?: ✓)

I don't know. The context provided does not specifically address how pre-annotation and interactive systems contribute to reducing annotation costs in active learning for NLP tasks. While it discusses the role and challenges of data annotation in NLP, the benefits and mechanisms of pre-annotation and interactive systems are not elaborated upon, leaving the question insufficiently answered.

1. [1]:  https://ar5iv.org/html/2210.10109, [2210.10109] A Survey of Active Learning for Natural Language Processing
2. [2]:  https://ar5iv.org/html/2210.10109, [2210.10109] A Survey of Active Learning for Natural Language Processing
3. [3]:  https://ar5iv.org/html/2303.16416, No Title
4. [4]:  https://ar5iv.org/html/2401.07367, [2401.07367] Active Learning for NLP with Large Language Models
5. [5]:  https://ar5iv.org/html/2407.03895, [2407.03895] Scoping Review of Active Learning Strategies and their Evaluation Environments for Entity Recognition Tasks
---
1. [1]:  Passage ID 1: related topics and future directions.1 IntroductionThe majority of modern natural language processing (NLP) systems are based on data-driven machine learning models. The success of these models depends on the quality and quantity of the available target training data. While these models can obtain impressive performance if given enough supervision, it is usually expensive to collect large amounts of annotations, especially considering that the labeling process can be laborious and challenging for NLP tasks (§3.2). Active learning (AL), an approach that aims to achieve high accuracy with fewer training labels by allowing a model to choose the data to be annotated and used for learning, is a widely-studied approach to tackle this labeling bottleneck (Settles, 2009).Active learning has been studied for more than twenty years (Lewis and Gale, 1994; Lewis and Catlett, 1994; Cohn et al., 1994, 1996) and there have been several literature surveys on this topic (Settles, 2009; Olsson,
2. [2]:  Passage ID 2: related topics and future directions.1 IntroductionThe majority of modern natural language processing (NLP) systems are based on data-driven machine learning models. The success of these models depends on the quality and quantity of the available target training data. While these models can obtain impressive performance if given enough supervision, it is usually expensive to collect large amounts of annotations, especially considering that the labeling process can be laborious and challenging for NLP tasks (§3.2). Active learning (AL), an approach that aims to achieve high accuracy with fewer training labels by allowing a model to choose the data to be annotated and used for learning, is a widely-studied approach to tackle this labeling bottleneck (Settles, 2009).Active learning has been studied for more than twenty years (Lewis and Gale, 1994; Lewis and Catlett, 1994; Cohn et al., 1994, 1996) and there have been several literature surveys on this topic (Settles, 2009; Olsson,
3. [3]:  Passage ID 3: particularly in the areas of data annotation. However, it is important to note that this does not eliminate the need for expert input in creating annotation guidelines and in the initial phases of model training. While our study demonstrates that GPT models can achieve competitive performance with fewer annotated examples compared to traditional NLP systems, the role of subject matter experts remains crucial. Experts are needed to write precise annotation guidelines, perform initial annotations for error analysis and example generation, and validate the model’s performance. Although the GPT models require fewer annotated instances, the costs associated with expert involvement, API usage, and running an LLM service should not be overlooked. A comprehensive comparison of resource requirements and costs between traditional NLP systems, word embedding models, and LLM-based systems would be valuable for future studies. This will provide a clearer understanding of the practical implications
4. [4]:  Passage ID 4: results compared to that with human annotations. The method reveals great potentials of LLMs as annotators in terms of accuracy and cost efficiency in active learning settings.Index Terms: natural language processing, large language model, active learning, annotationI IntroductionSupervised deep learning requires a large amount of ground truth labels. Usually the samples are labeled by humans. However, annotation by human annotators is expensive, laborious, and sometimes challenging, which is especially true in Natural Language Processing (NLP) systems [1, 2, 3]. On Google Cloud Platform111https://cloud.google.com/ai-platform/data-labeling/pricing, the text classification labeling costs for 1000 units (50 words per unit) $129 in Tier 1 and $90 for Tier 2. In [3], the authors assumes $0.11 per 50 tokens, and I will follow the same estimates in this work.To reduce the labeling cost and enhance the sample efficiency, Active Learning (AL) technique can be utilized, assuming
5. [5]:  Passage ID 5: years showed significant advancements [98, 7, 19] in natural language processing (NLP): Large language models (LLMs) emerged [8], facilitating new methodologies by describing tasks in natural language without a strong formalism. Because resource-intensive LLMs are not always superior [112], smaller, supervised learning-based models are still highly relevant for specialized domains or use cases that require rapid inference or are constrained by hardware limitations (such as mobile devices or offline scenarios) [34].One of these domains is entity recognition [73]. Entity recognition  (ER) describes the task of assigning a label to a sequence of words (e.g. to extract a person, a date or any other predefined label). To apply supervised learning to ER, data must be annotated. The manual annotation process, in which humans annotate data points with these predefined labels, is time-intensive and expensive [106]. Its output is an annotated dataset, which is also called corpus (pl. corpora)