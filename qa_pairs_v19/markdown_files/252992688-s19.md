# A Survey of Active Learning for Natural Language Processing

## Question

How do pre-annotation and interactive systems help in reducing annotation costs in active learning for NLP tasks?

## URLs

1. https://ar5iv.org/html/2401.07367. [2401.07367] Active Learning for NLP with Large Language Models
2. https://ar5iv.org/html/2404.15720. [2404.15720] Annotator-Centric Active Learning for Subjective NLP Tasks
3. https://ar5iv.org/html/2211.06224. [2211.06224] alanno: An Active Learning Annotation System for Mortals
4. https://ar5iv.org/html/2407.03895. [2407.03895] Scoping Review of Active Learning Strategies and their Evaluation Environments for Entity Recognition Tasks
5. https://ar5iv.org/html/2210.10109. [2210.10109] A Survey of Active Learning for Natural Language Processing
6. https://ar5iv.org/html/2406.09206. [2406.09206] Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models
7. https://ar5iv.org/html/2402.14101. [2402.14101] Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation
8. https://ar5iv.org/html/2402.15613. [2402.15613] Towards Efficient Active Learning in NLP via Pretrained Representations
9. https://ar5iv.org/html/1808.03806. [1808.03806] The Impact of Automatic Pre-annotation in Clinical Note Data Element Extraction - the CLEAN Tool

## Answer (I don't know?: ✗)

Pre-annotation and interactive systems play a crucial role in reducing annotation costs in active learning (AL) for natural language processing (NLP) tasks by optimizing the data labeling process and minimizing the required human input.

Pre-annotation involves the use of automated systems, such as large language models (LLMs), to provide initial labels for the dataset. This approach helps in reducing the workload on human annotators, who would otherwise be responsible for labeling all training examples. Specifically, studies indicate that LLMs can yield competitive performance even with fewer annotated examples compared to traditional NLP systems, demonstrating their potential as effective annotators in active learning settings [3]. However, despite this capacity for automation, the necessity for human expert input remains critical. Experts are essential for writing precise annotation guidelines and performing initial annotations necessary for error analysis and example generation [3]. Thus, while pre-annotation can significantly lessen the burden on human annotators, it does not entirely eliminate the need for their involvement.

Interactive systems further enhance the efficiency of the annotation process by allowing the model to select which data points should be annotated, thereby focusing resources on the most informative samples. This targeted selection process, enabled by active learning strategies, allows for higher accuracy with fewer training labels, minimizing both time and costs associated with data annotation [1] [2]. The iterative nature of interactive systems enables continuous improvement of the model’s performance, as human annotators can prioritize labeling samples that the model is most uncertain about, therefore refining the learning process more effectively.

Moreover, the cost-effectiveness of pre-annotation and interactive systems can be quantified in practical terms. For instance, conventional human annotation can be expensive; the costs associated with text classification labeling can reach up to $129 for 1,000 units on platforms like Google Cloud [4]. By integrating pre-annotation and an active learning framework, organizations can significantly reduce these costs while enhancing sample efficiency, thus providing a more sustainable approach to data annotation in NLP tasks [4].

In summary, pre-annotation and interactive systems together create a synergistic effect in the annotation process by leveraging automated labeling and intelligent data selection, leading to reduced costs and improved efficiency in active learning for NLP tasks. The balance between automated methods and expert oversight is essential to harness the full potential of these approaches while maintaining data quality and task relevance.

1. [1]:  https://ar5iv.org/html/2210.10109, [2210.10109] A Survey of Active Learning for Natural Language Processing
2. [2]:  https://ar5iv.org/html/2210.10109, [2210.10109] A Survey of Active Learning for Natural Language Processing
3. [3]:  https://ar5iv.org/html/2303.16416, No Title
4. [4]:  https://ar5iv.org/html/2401.07367, [2401.07367] Active Learning for NLP with Large Language Models
5. [5]:  https://ar5iv.org/html/2407.03895, [2407.03895] Scoping Review of Active Learning Strategies and their Evaluation Environments for Entity Recognition Tasks
---
1. [1]:  Passage ID 1: related topics and future directions.1 IntroductionThe majority of modern natural language processing (NLP) systems are based on data-driven machine learning models. The success of these models depends on the quality and quantity of the available target training data. While these models can obtain impressive performance if given enough supervision, it is usually expensive to collect large amounts of annotations, especially considering that the labeling process can be laborious and challenging for NLP tasks (§3.2). Active learning (AL), an approach that aims to achieve high accuracy with fewer training labels by allowing a model to choose the data to be annotated and used for learning, is a widely-studied approach to tackle this labeling bottleneck (Settles, 2009).Active learning has been studied for more than twenty years (Lewis and Gale, 1994; Lewis and Catlett, 1994; Cohn et al., 1994, 1996) and there have been several literature surveys on this topic (Settles, 2009; Olsson,
2. [2]:  Passage ID 2: related topics and future directions.1 IntroductionThe majority of modern natural language processing (NLP) systems are based on data-driven machine learning models. The success of these models depends on the quality and quantity of the available target training data. While these models can obtain impressive performance if given enough supervision, it is usually expensive to collect large amounts of annotations, especially considering that the labeling process can be laborious and challenging for NLP tasks (§3.2). Active learning (AL), an approach that aims to achieve high accuracy with fewer training labels by allowing a model to choose the data to be annotated and used for learning, is a widely-studied approach to tackle this labeling bottleneck (Settles, 2009).Active learning has been studied for more than twenty years (Lewis and Gale, 1994; Lewis and Catlett, 1994; Cohn et al., 1994, 1996) and there have been several literature surveys on this topic (Settles, 2009; Olsson,
3. [3]:  Passage ID 3: particularly in the areas of data annotation. However, it is important to note that this does not eliminate the need for expert input in creating annotation guidelines and in the initial phases of model training. While our study demonstrates that GPT models can achieve competitive performance with fewer annotated examples compared to traditional NLP systems, the role of subject matter experts remains crucial. Experts are needed to write precise annotation guidelines, perform initial annotations for error analysis and example generation, and validate the model’s performance. Although the GPT models require fewer annotated instances, the costs associated with expert involvement, API usage, and running an LLM service should not be overlooked. A comprehensive comparison of resource requirements and costs between traditional NLP systems, word embedding models, and LLM-based systems would be valuable for future studies. This will provide a clearer understanding of the practical implications
4. [4]:  Passage ID 4: results compared to that with human annotations. The method reveals great potentials of LLMs as annotators in terms of accuracy and cost efficiency in active learning settings.Index Terms: natural language processing, large language model, active learning, annotationI IntroductionSupervised deep learning requires a large amount of ground truth labels. Usually the samples are labeled by humans. However, annotation by human annotators is expensive, laborious, and sometimes challenging, which is especially true in Natural Language Processing (NLP) systems [1, 2, 3]. On Google Cloud Platform111https://cloud.google.com/ai-platform/data-labeling/pricing, the text classification labeling costs for 1000 units (50 words per unit) $129 in Tier 1 and $90 for Tier 2. In [3], the authors assumes $0.11 per 50 tokens, and I will follow the same estimates in this work.To reduce the labeling cost and enhance the sample efficiency, Active Learning (AL) technique can be utilized, assuming
5. [5]:  Passage ID 5: years showed significant advancements [98, 7, 19] in natural language processing (NLP): Large language models (LLMs) emerged [8], facilitating new methodologies by describing tasks in natural language without a strong formalism. Because resource-intensive LLMs are not always superior [112], smaller, supervised learning-based models are still highly relevant for specialized domains or use cases that require rapid inference or are constrained by hardware limitations (such as mobile devices or offline scenarios) [34].One of these domains is entity recognition [73]. Entity recognition  (ER) describes the task of assigning a label to a sequence of words (e.g. to extract a person, a date or any other predefined label). To apply supervised learning to ER, data must be annotated. The manual annotation process, in which humans annotate data points with these predefined labels, is time-intensive and expensive [106]. Its output is an annotated dataset, which is also called corpus (pl. corpora)