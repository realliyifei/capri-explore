# Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

How does the syntactic type of an antecedent affect the decision to use "it" or "that" in anaphora?

## URLs

1. https://ar5iv.org/html/2103.06924. [2103.06924] Anaphoric Binding: an integrated overview
2. https://ar5iv.org/html/1805.11824. [1805.11824] Anaphora and Coreference Resolution: A Review
3. https://ar5iv.org/html/2004.07898. [2004.07898] Bridging Anaphora Resolution as Question Answering
4. https://ar5iv.org/html/1910.09329. [1910.09329] A Neural Entity Coreference Resolution review
5. https://ar5iv.org/html/2305.11529. [2305.11529] A Sequence-to-Sequence Approach for Arabic Pronoun Resolution

## Answer (I don't know?: ✓)

I don't know. The context provided does not contain specific information about the syntactic type of antecedents and its influence on the choice between "it" or "that" in anaphora. While there is discussion of anaphora resolution processes, candidate antecedents, and examples of grammatical relationships, none of the passages directly address how syntactic types affect the decision-making process for specific pronouns like "it" or "that". Therefore, the context is insufficient to answer the question accurately.

1. [1]:  https://ar5iv.org/html/1805.11824, [1805.11824] Anaphora and Coreference Resolution: A Review
2. [2]:  https://ar5iv.org/html/2305.11529, [2305.11529] A Sequence-to-Sequence Approach for Arabic Pronoun Resolution
3. [3]:  https://ar5iv.org/html/2103.06924, [2103.06924] Anaphoric Binding: an integrated overview
4. [4]:  https://ar5iv.org/html/2305.11529, [2305.11529] A Sequence-to-Sequence Approach for Arabic Pronoun Resolution
5. [5]:  https://ar5iv.org/html/2103.06924, [2103.06924] Anaphoric Binding: an integrated overview
---
1. [1]:  Passage ID 1: antecedent resolution. This algorithm started with parsing each sentence in the text, POS tagging and lemmatizing it. These linguistic features were stored in an internal data structure. This global data structure was appended with some other features like base nouns, number agreement, person name identification, gender, animacy, etc. This model also constructed a finite state machine with the aim of identifying the NPs. The parsed sentence was then sequentially checked for anaphoric references and pleonastic it occurrences. The remaining mentions were considered as possible candidates for antecedents and were heuristically evaluated using a scoring function. The toolkit was extensively evaluated on reportage, editorials, reviews, religion, fiction, etc.As the research in CR started to shift towards machine learning algorithms which used classification and ranking it slowly became clear that to beat the machine learning systems, rules had to be ordered according to their importance.
2. [2]:  Passage ID 2: antecedent, as this study focused on anaphora resolution (pairs), not coreference resolution (clusters). Finally, the dataset was restricted to pronominal anaphoric expressions, which accounted for approximately 88% of the data. This decision was based on the prevalence of pronominal anaphors in Arabic and the focus of many studies in the field [9, 34, 19, 7, 8]. Applying these data-cleaning procedures ensured a high-quality dataset suitable for training and evaluating the proposed model.4.1.3 Identifying Candidate AntecedentsIn this study, we focus on the pronoun and nominal anaphora resolution; thus, the candidate list is restricted to nouns and noun phrases. The AnATAr corpus used in this study is already segmented into word items, which may be standalone words or phrases, and we have followed this original segmentation. To determine whether a segment is a candidate antecedent, we have used CAMeL [35] along with CoreNLP Stanford [36] taggers as a double-check to identify the
3. [3]:  Passage ID 3: research question in neural natural language processing is to design models that possibly have an appropriate inductive bias such that their internal linguistic representations and capabilities resemble as much as possible the ones of human language learners after being exposed with as little volume of raw training data as the ones humans learners are exposed to.646464[68].A most outstanding feature of natural language is the possibility of there being so called long distance relations, that is relations between expressions among which a string of other expressions of arbitrary length may intervene. This builds on another feature that has been widely recognized as underlying natural language, namely the hierarchical nature of its complex expressions.656565[27]As amply documented in the overview above, grammatical anaphoric binding relations, among anaphors and antecedents, are grammar regulated connections that are long distance relations par excellence. Hence, anaphoric binding
4. [4]:  Passage ID 4: mention that differed from the annotated one. As shown in Figure 5d, in some instances, the antecedent was mentioned in multiple ways, and the annotation identified the first mention, while the model selected the nearest one, which was correct but not the same as the annotation. This observation highlights the need for clear guidelines in the annotation process for Anaphora Resolution tasks, particularly for confusion cases involving attached pronouns. Moreover, it emphasizes the need to establish a consistent annotation method, such as selecting the first mention or the last, to facilitate evaluating and comparing anaphora resolution models.Using the candidate mask led to a 20% improvement in correctly identifying the chosen antecedent in the selected instances, thereby reducing the number of false positives. It is worth noting, however, that the mask does not eliminate a false positive by zeroing the selected antecedent’s score since the latter is already on the candidate list.
5. [5]:  Passage ID 5: the overview above, grammatical anaphoric binding relations, among anaphors and antecedents, are grammar regulated connections that are long distance relations par excellence. Hence, anaphoric binding is essential, and of utmost importance, for the endeavour of designing neural models with appropriate inductive bias for natural language.Appendix In order to illustrate the combined effect of the binding constraints specified in Section 4,as well as the outcome obtained from a grammar that integrates Binding Theory,we work through the example below and the corresponding grammaticalrepresentation in Figure 1.Every student said [he likes himself].This is amulti-clausal sentence with two anaphoric nominals in the embeddedclause, a pronoun (he) and a short-distance reflexive (himself), and with a quantificationalNP (every student) in the upper clause. In this sentence,the reflexive has the pronoun as the only admissible antecedent,and the pronoun, in turn, can either