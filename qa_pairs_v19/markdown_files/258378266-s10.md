# Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey

## Question

What are the challenges of using QA pairs as positive samples in neural ranking models?

## URLs

1. https://ar5iv.org/html/2102.11903. No content available
2. https://ar5iv.org/html/2405.10517. [2405.10517] Towards Better Question Generation in QA-Based Event Extraction
3. https://ar5iv.org/html/2406.05733. [2406.05733] MrRank: Improving Question Answering Retrieval System through Multi-Result Ranking Model
4. https://ar5iv.org/html/1707.06372. [1707.06372] Learning to Rank Question Answer Pairs with Holographic Dual LSTM Architecture
5. https://ar5iv.org/html/2411.08248. [2411.08248] Deceiving Question-Answering Models: A Hybrid Word-Level Adversarial Approach
6. https://ar5iv.org/html/2106.11096. [2106.11096] Learning to Rank Question Answer Pairs with Bilateral Contrastive Data Augmentation The work described in this paper is substantially supported by a grant from the Asian Institute of Supply Chains and Logistics, the Chinese University of Hong Kong.
7. https://ar5iv.org/html/2410.08289. [2410.08289] Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference
8. https://ar5iv.org/html/2104.02947. [2104.02947] Distantly Supervised Transformers For E-Commerce Product QA
9. https://ar5iv.org/html/2308.02926. [2308.02926] Towards Consistency Filtering-Free Unsupervised Learning for Dense Retrieval
10. https://ar5iv.org/html/2209.12617. [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language

## Answer (I don't know?: ‚úó)

Using QA (Question Answer) pairs as positive samples in neural ranking models poses several challenges. One primary challenge is the acquisition and annotation of ground-truth positive answers, which is often time-consuming and labor-intensive. The process typically involves manually evaluating and labeling the relevance of candidate answers to specific questions, making the task resource-intensive and prone to human error [4].

Moreover, QA pairs can sometimes be viewed in a simplistic binary manner‚Äîas either positive or negative without considering the nuances of relevance. This can lead to suboptimal training outcomes, where the model may not learn to distinguish the levels of relevance among various answers effectively. To address this issue, researchers are designing more sophisticated training objectives that contrastively rank QA pairs, encouraging the model to learn relative relevance rather than absolute judgments. This is reflected in the introduction of methods such as the BiG (BERT-based grounded ranking) which aims to better handle the complexities of answer relevance through improved ranking mechanisms [3][4].

Another inherent challenge is the complexity of the models involved, particularly in the wake of advances in neural architectures. While models such as BERT have demonstrated significant improvements in answer selection, they require a substantial amount of high-quality data to train effectively. The dependency on larger datasets complicates the process of developing robust and effective QA systems, particularly when balanced against the need for good annotation and the costs associated with obtaining such datasets [4].

Additionally, as the landscape of QA systems evolves, the robustness of these models against adversarial attacks has emerged as a significant concern. These attacks can exploit vulnerabilities within the QA systems, producing misleading outputs that are difficult to identify and correct. Adversarial strategies can involve subtle manipulations of language, which in turn challenges the efficacy and reliability of QA pairs in training [2][5]. 

In summary, the challenges of using QA pairs as positive samples in neural ranking models include the labor-intensive nature of acquiring and annotating answers, the difficulties in teaching models to discern varying degrees of relevance, the need for extensive and high-quality datasets, and the models' susceptibility to adversarial attacks. Addressing these challenges is crucial for enhancing the performance and reliability of QA systems in real-world applications.

1. [1]:  https://ar5iv.org/html/2209.12617, [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
2. [2]:  https://ar5iv.org/html/2411.08248, [2411.08248] Deceiving Question-Answering Models: A Hybrid Word-Level Adversarial Approach
3. [3]:  https://ar5iv.org/html/2106.11096, [2106.11096] Learning to Rank Question Answer Pairs with Bilateral Contrastive Data Augmentation The work described in this paper is substantially supported by a grant from the Asian Institute of Supply Chains and Logistics, the Chinese University of Hong Kong.
4. [4]:  https://ar5iv.org/html/2106.11096, [2106.11096] Learning to Rank Question Answer Pairs with Bilateral Contrastive Data Augmentation The work described in this paper is substantially supported by a grant from the Asian Institute of Supply Chains and Logistics, the Chinese University of Hong Kong.
5. [5]:  https://ar5iv.org/html/2411.08248, [2411.08248] Deceiving Question-Answering Models: A Hybrid Word-Level Adversarial Approach
---
1. [1]:  Passage ID 1: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence
2. [2]:  Passage ID 2: advanced natural language processing (NLP) tasks such as textual classification, neural machine translation (NMT), abstractive summarization and question-answering (QA). However, the robustness of the models, particularly QA models, against adversarial attacks is a critical concern that remains insufficiently explored. This paper introduces QA-Attack (Question Answering Attack), a novel word-level adversarial strategy that fools QA models. Our attention-based attack exploits the customized attention mechanism and deletion ranking strategy to identify and target specific words within contextual passages. It creates deceptive inputs by carefully choosing and substituting synonyms, preserving grammatical integrity while misleading the model to produce incorrect responses. Our approach demonstrates versatility across various question types, particularly when dealing with extensive long textual inputs. Extensive experiments on multiple benchmark datasets demonstrate that QA-Attack
3. [3]:  Passage ID 3: rather than be simply regarded as a positive QA pair.Therefore, we further design a contrastive training objective for ranking QA pairs.We conduct experiments by applying the proposed BiG method with both the attention-based ranking model and the pre-trained BERT ranking model. Experimental results on three benchmark datasets including TREC-QA, WikiQA, and ANTIQUE show the effectiveness and the applicability of our method, which significantly improves the original ranking models across all datasets.2 Method2.1 PreliminariesLet an ad-hoc neural ranking model be denoted as RŒ∏‚Äã(ùê™,ùêö)subscriptùëÖùúÉùê™ùêöR_{\theta}(\mathbf{q},\mathbf{a}), which measures the relevance degree of a given question answer pair (ùê™,ùêö)ùê™ùêö(\mathbf{q},\mathbf{a}) parameterized by the model parameters Œ∏ùúÉ\theta.Given a set of training samples, there are three mainstream approaches to optimize the ranking performance, namely, pointwise training, pairwise training, and listwise training Lai et¬†al. (2018); Zhang
4. [4]:  Passage ID 4: IntroductionRanking question answer pairs, also known as answer selection, is a fundamental task in question answering (QA) systems.It aims to rank a set of candidate answers for selecting the relevant or correct answers to the given question.Many efforts have been made on developing various neural models to measure the relevance degree between the question and answer pair, including Siamese Structure¬†Tay et¬†al. (2018); Chen et¬†al. (2020), Attention-based Structure¬†Chen et¬†al. (2017); Shen et¬†al. (2018); Deng et¬†al. (2021), and Compare-Aggregate Structure¬†Yoon et¬†al. (2019).Recently, models with contextualized representations, e.g., ELMo¬†Peters et¬†al. (2018) and BERT¬†Devlin et¬†al. (2019), contribute to major improvement on answer selection¬†Yoon et¬†al. (2019); Garg et¬†al. (2020).Acquiring the ground-truth positive answer to the given question or manually annotating the relevance degree between a QA pair can be extremely time-consuming. Therefore, the existing answer selection
5. [5]:  Passage ID 5: demonstrates versatility across various question types, particularly when dealing with extensive long textual inputs. Extensive experiments on multiple benchmark datasets demonstrate that QA-Attack successfully deceives baseline QA models and surpasses existing adversarial techniques regarding success rate, semantics changes, BLEU score, fluency and grammar error rate.keywords: Neural Language Processing, Adversarial Attack, Deep Learning, Question Answering1 IntroductionQuestion-answering (QA) models, a key task within Sequence-to-Sequence (Seq2Seq) frameworks, aim to enhance computers‚Äô ability to process and respond to natural language queries. As these models have evolved, they have been widely adopted in real-world applications such as customer service chatbots[40], search engines [69], and information retrieval in fields like medicine [19] and law [35]. However, despite the significant progress in deep learning and natural language processing (NLP), these models remain