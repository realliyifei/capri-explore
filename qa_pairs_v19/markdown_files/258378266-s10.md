# Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey

## Question

What are the challenges of using QA pairs as positive samples in neural ranking models?

## URLs

1. https://ar5iv.org/html/2102.11903. No content available
2. https://ar5iv.org/html/2405.10517. [2405.10517] Towards Better Question Generation in QA-Based Event Extraction
3. https://ar5iv.org/html/2406.05733. [2406.05733] MrRank: Improving Question Answering Retrieval System through Multi-Result Ranking Model
4. https://ar5iv.org/html/1707.06372. [1707.06372] Learning to Rank Question Answer Pairs with Holographic Dual LSTM Architecture
5. https://ar5iv.org/html/2411.08248. [2411.08248] Deceiving Question-Answering Models: A Hybrid Word-Level Adversarial Approach
6. https://ar5iv.org/html/2106.11096. [2106.11096] Learning to Rank Question Answer Pairs with Bilateral Contrastive Data Augmentation The work described in this paper is substantially supported by a grant from the Asian Institute of Supply Chains and Logistics, the Chinese University of Hong Kong.
7. https://ar5iv.org/html/2410.08289. [2410.08289] Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference
8. https://ar5iv.org/html/2104.02947. [2104.02947] Distantly Supervised Transformers For E-Commerce Product QA
9. https://ar5iv.org/html/2308.02926. [2308.02926] Towards Consistency Filtering-Free Unsupervised Learning for Dense Retrieval
10. https://ar5iv.org/html/2209.12617. [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language

## Answer (I don't know?: ‚úó)

Using QA pairs as positive samples in neural ranking models presents several challenges that stem from the complexities involved in both constructing effective models and evaluating them:

1. **Manual Annotation and Relevance Determination**: One significant challenge is the labor-intensive process of acquiring ground-truth positive answers or manually annotating the relevance degree between a QA pair. This task can be extremely time-consuming and prone to subjectivity, leading to inconsistencies and potential biases in the training data [4]. The reliance on human judgment to establish these relevance labels complicates the training of neural models, which depend on high-quality and accurately labeled data for learning effective representations.

2. **Variability in QA Pairs**: Not all QA pairs are treated equally in terms of their quality and informativeness. While some pairs may be clearly relevant, others can be more ambiguous or context-dependent. This variance can complicate the ranking process, as a model may struggle to discern which pairs genuinely correspond to correct answers when faced with lower-quality or deceptive samples. Consequently, the effectiveness of the ranking model can suffer if the training set includes poorly constructed QA pairs [3] [4].

3. **Contrastive Learning and Ranking Objectives**: The challenge of effectively utilizing QA pairs as positive samples for ranking can also be addressed by implementing contrastive learning techniques. Such methods can refine the ranking process by explicitly distinguishing between positive and negative pairs. However, the design and implementation of an appropriate contrastive training objective are non-trivial and require careful consideration to ensure that the model adequately learns to rank relevant pairs higher than irrelevant ones [3].

4. **Measure of Relevance**: The complexity in defining and measuring the relevance degree of QA pairs, such as through pointwise, pairwise, or listwise approaches, adds further challenges. Each method has its pros and cons, and selecting the most appropriate one for a given task requires a deep understanding of both the model mechanics and the specific domain from which the QA pairs are derived [3] [4].

5. **Robustness to Adversarial Attacks**: The vulnerability of QA models to adversarial attacks exacerbates these challenges. Techniques such as QA-Attack highlight how models can be misled by carefully crafted input variations, raising questions about the stability and reliability of these ranking systems when exposed to potentially deceptive QA pairs [2] [5]. This necessitates the integration of adversarial robustness into model design and evaluation processes, further complicating the development cycle.

In summary, while QA pairs are essential for training neural ranking models, challenges such as the need for manual annotation, variability in QA quality, measurement of relevance, and susceptibility to adversarial attacks significantly complicate their effective use. These hurdles highlight the complexity of designing robust QA systems that can reliably rank and select answers from diverse datasets.

1. [1]:  https://ar5iv.org/html/2209.12617, [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
2. [2]:  https://ar5iv.org/html/2411.08248, [2411.08248] Deceiving Question-Answering Models: A Hybrid Word-Level Adversarial Approach
3. [3]:  https://ar5iv.org/html/2106.11096, [2106.11096] Learning to Rank Question Answer Pairs with Bilateral Contrastive Data Augmentation The work described in this paper is substantially supported by a grant from the Asian Institute of Supply Chains and Logistics, the Chinese University of Hong Kong.
4. [4]:  https://ar5iv.org/html/2106.11096, [2106.11096] Learning to Rank Question Answer Pairs with Bilateral Contrastive Data Augmentation The work described in this paper is substantially supported by a grant from the Asian Institute of Supply Chains and Logistics, the Chinese University of Hong Kong.
5. [5]:  https://ar5iv.org/html/2411.08248, [2411.08248] Deceiving Question-Answering Models: A Hybrid Word-Level Adversarial Approach
---
1. [1]:  Passage ID 1: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence
2. [2]:  Passage ID 2: advanced natural language processing (NLP) tasks such as textual classification, neural machine translation (NMT), abstractive summarization and question-answering (QA). However, the robustness of the models, particularly QA models, against adversarial attacks is a critical concern that remains insufficiently explored. This paper introduces QA-Attack (Question Answering Attack), a novel word-level adversarial strategy that fools QA models. Our attention-based attack exploits the customized attention mechanism and deletion ranking strategy to identify and target specific words within contextual passages. It creates deceptive inputs by carefully choosing and substituting synonyms, preserving grammatical integrity while misleading the model to produce incorrect responses. Our approach demonstrates versatility across various question types, particularly when dealing with extensive long textual inputs. Extensive experiments on multiple benchmark datasets demonstrate that QA-Attack
3. [3]:  Passage ID 3: rather than be simply regarded as a positive QA pair.Therefore, we further design a contrastive training objective for ranking QA pairs.We conduct experiments by applying the proposed BiG method with both the attention-based ranking model and the pre-trained BERT ranking model. Experimental results on three benchmark datasets including TREC-QA, WikiQA, and ANTIQUE show the effectiveness and the applicability of our method, which significantly improves the original ranking models across all datasets.2 Method2.1 PreliminariesLet an ad-hoc neural ranking model be denoted as RŒ∏‚Äã(ùê™,ùêö)subscriptùëÖùúÉùê™ùêöR_{\theta}(\mathbf{q},\mathbf{a}), which measures the relevance degree of a given question answer pair (ùê™,ùêö)ùê™ùêö(\mathbf{q},\mathbf{a}) parameterized by the model parameters Œ∏ùúÉ\theta.Given a set of training samples, there are three mainstream approaches to optimize the ranking performance, namely, pointwise training, pairwise training, and listwise training Lai et¬†al. (2018); Zhang
4. [4]:  Passage ID 4: IntroductionRanking question answer pairs, also known as answer selection, is a fundamental task in question answering (QA) systems.It aims to rank a set of candidate answers for selecting the relevant or correct answers to the given question.Many efforts have been made on developing various neural models to measure the relevance degree between the question and answer pair, including Siamese Structure¬†Tay et¬†al. (2018); Chen et¬†al. (2020), Attention-based Structure¬†Chen et¬†al. (2017); Shen et¬†al. (2018); Deng et¬†al. (2021), and Compare-Aggregate Structure¬†Yoon et¬†al. (2019).Recently, models with contextualized representations, e.g., ELMo¬†Peters et¬†al. (2018) and BERT¬†Devlin et¬†al. (2019), contribute to major improvement on answer selection¬†Yoon et¬†al. (2019); Garg et¬†al. (2020).Acquiring the ground-truth positive answer to the given question or manually annotating the relevance degree between a QA pair can be extremely time-consuming. Therefore, the existing answer selection
5. [5]:  Passage ID 5: demonstrates versatility across various question types, particularly when dealing with extensive long textual inputs. Extensive experiments on multiple benchmark datasets demonstrate that QA-Attack successfully deceives baseline QA models and surpasses existing adversarial techniques regarding success rate, semantics changes, BLEU score, fluency and grammar error rate.keywords: Neural Language Processing, Adversarial Attack, Deep Learning, Question Answering1 IntroductionQuestion-answering (QA) models, a key task within Sequence-to-Sequence (Seq2Seq) frameworks, aim to enhance computers‚Äô ability to process and respond to natural language queries. As these models have evolved, they have been widely adopted in real-world applications such as customer service chatbots[40], search engines [69], and information retrieval in fields like medicine [19] and law [35]. However, despite the significant progress in deep learning and natural language processing (NLP), these models remain