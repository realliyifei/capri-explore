# Which *BERT? A Survey Organizing Contextualized Encoders

## Question

In what ways have self-supervised discourse structure objectives been applied in the pretraining of encoders for natural language understanding?

## URLs

1. https://ar5iv.org/html/2411.16765. [2411.16765] SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction
2. https://ar5iv.org/html/2411.15127. [2411.15127] PRimuS: Pretraining IMU Encoders with Multimodal Self-Supervision
3. https://ar5iv.org/html/2405.00420. [2405.00420] Self-supervised Pre-training of Text Recognizers
4. https://ar5iv.org/html/2407.12210. [2407.12210] A Closer Look at Benchmarking Self-Supervised Pre-training with Image Classification
5. https://ar5iv.org/html/2408.07519. [2408.07519] Whitening Consistently Improves Self-Supervised Learning
6. https://ar5iv.org/html/2401.17632. [2401.17632] What do self-supervised speech and speaker models learn? New findings from a cross model layer-wise analysis
7. https://ar5iv.org/html/2412.04425. [2412.04425] CA-SSLR: Condition-Aware Self-Supervised Learning Representation for Generalized Speech Processing
8. https://ar5iv.org/html/2408.17059. [2408.17059] A Survey of the Self Supervised Learning Mechanisms for Vision Transformers
9. https://ar5iv.org/html/2301.05712. [2301.05712] A Survey on Self-supervised Learning: Algorithms, Applications, and Future Trends
10. https://ar5iv.org/html/2407.00756. [2407.00756] Less Forgetting for Better Generalization: Exploring Continual-learning Fine-tuning Methods for Speech Self-supervised Representations

## Answer (I don't know?: âœ—)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable