# Which *BERT? A Survey Organizing Contextualized Encoders

## Question

In what ways have self-supervised discourse structure objectives been applied in the pretraining of encoders for natural language understanding?

## URLs

1. https://ar5iv.org/html/2411.16765
2. https://ar5iv.org/html/2411.15127
3. https://ar5iv.org/html/2405.00420
4. https://ar5iv.org/html/2407.12210
5. https://ar5iv.org/html/2408.07519
6. https://ar5iv.org/html/2401.17632
7. https://ar5iv.org/html/2412.04425
8. https://ar5iv.org/html/2408.17059
9. https://ar5iv.org/html/2301.05712
10. https://ar5iv.org/html/2407.00756

## Answer

## I don't know?: True

I don't know. The context provided does not include any specific information regarding self-supervised discourse structure objectives or their application in the pretraining of encoders for natural language understanding. The passages primarily discuss the generation of synthetic datasets, classification methods, and performance of various models without delving into specific training objectives or methodologies related to self-supervised learning. Thus, I am unable to provide a precise answer to your question based on the given context.

1. [1]:  https://ar5iv.org/html/2410.00427, No Title
2. [2]:  https://ar5iv.org/html/1807.10854, No Title
3. [3]:  https://ar5iv.org/html/2410.00427, No Title
4. [4]:  https://ar5iv.org/html/1807.10854, No Title
5. [5]:  https://ar5iv.org/html/1807.10854, No Title
---
1. [1]:  Passage ID 1: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
2. [2]:  Passage ID 2: the top performers in classifying types of questions using the TREC database [146].Between their requirement for such understanding and their ease of examination due to the typical encoder–decoder structure they use,neural machine translation (NMT) systems (Section IV-G) are splendid testbeds for researching internal semantic representations. Poliak et al. [147] trained encoders on four different language pairs: English and Arabic, English and Spanish, English and Chinese, and English and German. The decoding classifiers were trained on four distinct datasets: Multi-NLI [148], which is an expanded version of SNLI [149], as well as three recast datasets from the JHU Decompositional Semantics Initiative [150] (FrameNet Plus or FN+ [151], Definite Pronoun Resolution or DPR [152], and Semantic Proto-Roles or SPR [153]). None of the results were particularly strong, although they were strongest in SPR. This led to the conclusion that NMT models do a poor job of capturing paraphrased
3. [3]:  Passage ID 3: ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the laws of thermodynamics?” Following a quality inspection of the synthetically produced questions, we assembled a training dataset of 1601 examples, consisting of 120 questions for each of the 12 topics and 161 general questions. We also derived a test dataset containing 364 examples with a balanced class distribution similar to the training dataset.In our experiments, we evaluated three classification approaches: vector similarity search, prompting a LLM (GPT-3.5-Turbo), and few-shot fine-tuning of a transformer model with the SetFit framework Tunstall et al. (2022). Concerning the vector search approach with the SPECTER2 model, we measured the cosine similarity to compare vectors of embedded user queries with paper embeddings in our vector database to retrieve the 100 most similar
4. [4]:  Passage ID 4: breaking performance records in myriad tasks in miscellaneous fields. In particular, deep neural architectures, have instilled models with higher performance in natural language tasks, in terms of “imperfect” metrics. Consolidating the analysis of all the models surveyed, a few general trends can be surmised. Both convolutional and recurrent specimen had contributed to the state of the art in the recent past, however, of very late, stacks of attention-powered Transformer units as encoders and often decoders, have consistently produced superior results across the rich and varying terrain of the NLP field. These models are generally heavily pre-trained on general language knowledge in an unsupervised or supervised manner, and somewhat lightly trained on specific tasks in a supervised fashion. Second, attention mechanisms alone, without recurrences or convolutions, seem to provide the best connections between encoders and decoders. Third, forcing networks to examine different features (by
5. [5]:  Passage ID 5: there are multiple options of encoders and decoders available, RNN variants are a common choice for each, particularly the latter. Such a network is shown in (a). Attention mechanisms, such as that present in (b), allow the decoder to determine which portions of the encoding are most relevant at each output step.II-A Natural Language ProcessingThe field of natural language processing, also known as computational linguistics, involves the engineering of computational models and processes to solve practical problems in understanding human languages. These solutions are used to build useful software. Work in NLP can be divided into two broad sub-areas: core areas and applications, although it is sometimes difficult to distinguish clearly to which areas issues belong. The core areas address fundamental problems such as language modeling, which underscores quantifying associations among naturally occurring words; morphological processing, dealing with segmentation of meaningful