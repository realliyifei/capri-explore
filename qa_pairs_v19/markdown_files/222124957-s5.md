# Which *BERT? A Survey Organizing Contextualized Encoders

## Question

In what ways have self-supervised discourse structure objectives been applied in the pretraining of encoders for natural language understanding?

## URLs

1. https://ar5iv.org/html/2411.16765. [2411.16765] SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction
2. https://ar5iv.org/html/2411.15127. [2411.15127] PRimuS: Pretraining IMU Encoders with Multimodal Self-Supervision
3. https://ar5iv.org/html/2405.00420. [2405.00420] Self-supervised Pre-training of Text Recognizers
4. https://ar5iv.org/html/2407.12210. [2407.12210] A Closer Look at Benchmarking Self-Supervised Pre-training with Image Classification
5. https://ar5iv.org/html/2408.07519. [2408.07519] Whitening Consistently Improves Self-Supervised Learning
6. https://ar5iv.org/html/2401.17632. [2401.17632] What do self-supervised speech and speaker models learn? New findings from a cross model layer-wise analysis
7. https://ar5iv.org/html/2412.04425. [2412.04425] CA-SSLR: Condition-Aware Self-Supervised Learning Representation for Generalized Speech Processing
8. https://ar5iv.org/html/2408.17059. [2408.17059] A Survey of the Self Supervised Learning Mechanisms for Vision Transformers
9. https://ar5iv.org/html/2301.05712. [2301.05712] A Survey on Self-supervised Learning: Algorithms, Applications, and Future Trends
10. https://ar5iv.org/html/2407.00756. [2407.00756] Less Forgetting for Better Generalization: Exploring Continual-learning Fine-tuning Methods for Speech Self-supervised Representations

## Answer (I don't know?: ✓)

I don't know. The provided context does not contain specific information about the application of self-supervised discourse structure objectives in the pretraining of encoders for natural language understanding. While the passages discuss various self-supervised learning (SSL) techniques, their effectiveness, and related tasks in NLP and other domains, they do not elaborate on discourse structures or objectives specifically tied to self-supervised pretraining methods for encoding language understanding tasks. Therefore, the context is insufficient to provide a complete answer to the question.

1. [1]:  https://ar5iv.org/html/2411.16765, [2411.16765] SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction
2. [2]:  https://ar5iv.org/html/2408.17059, [2408.17059] A Survey of the Self Supervised Learning Mechanisms for Vision Transformers
3. [3]:  https://ar5iv.org/html/2407.00756, [2407.00756] Less Forgetting for Better Generalization: Exploring Continual-learning Fine-tuning Methods for Speech Self-supervised Representations
4. [4]:  https://ar5iv.org/html/2408.07519, [2408.07519] Whitening Consistently Improves Self-Supervised Learning
5. [5]:  https://ar5iv.org/html/2301.05712, [2301.05712] A Survey on Self-supervised Learning: Algorithms, Applications, and Future Trends
---
1. [1]:  Passage ID 1: RoBERTa [34] and ALBERT [31]), based on masked language modeling, have served as dependable representations that can be used and adapted for a variety of language understanding tasks [4].Pre-trained encoder-decoder models, like T5 [43], unify multiple text-to-text tasks in a single model. Finally, decoder-only models like the GPT series [42] have demonstrated the power of auto-regressive pre-training, when very large-scale training data is available and the downstream tasks can be framed in terms of auto-regressive generation.In speech processing, self-supervised learning approaches have taken inspiration from text encoder models while addressing the unique challenges posed by continuous audio signals, which have no inherent segmentation into tokens nor a pre-defined token vocabulary. For example, Hidden-Unit BERT (HuBERT) [20] adapted the objective of BERT by adding an offline clustering step to provide pseudo-labels for masked prediction. Other approaches include wav2vec 2.0 [2],
2. [2]:  Passage ID 2: label data available. In this survey we thus develop a comprehensive taxonomy of systematically classifying the SSL techniques based upon their representations and pre-training tasks being applied. Additionally, we discuss the motivations behind SSL, review popular pre-training tasks, and highlight the challenges and advancements in this field. Furthermore, we present a comparative analysis of different SSL methods, evaluate their strengths and limitations, and identify potential avenues for future research.keywords: Self-supervised, Vision Transformer, Survey, Transformer1 IntroductionDeep learning based algorithms have exhibited impressive results across various disciplines specially in computer vision (CV) [1] and natural language processing (NLP) [2]. Deep learning based models utilize a pre-training task on large datasets to enhance their performance. This primary step is often utilized as an initialization point and then the model is optimized for any specific use case.
3. [3]:  Passage ID 3: during pretraining, especially in low-resource scenarios. This work explores middle-ground solutions, conjecturing that reducing the forgetting of the self-supervised task during the downstream fine-tuning leads to better generalization. To prove this, focusing on speech recognition, we benchmark different continual-learning approaches during fine-tuning and show that they improve both in-domain and out-of-domain generalization abilities. Relative performance gains reach 15.7%percent15.715.7\% and 22.5%percent22.522.5\% with XLSR used as the encoder on two English and Danish speech recognition tasks. Further probing experiments show that these gains are indeed linked to less forgetting.keywords: Self-supervised learning (SSL), continual learning, speech recognition.1 IntroductionSelf-supervised representations are often used in low-resource scenarios where downstream data can consist of only a handful of hours of annotated audio [1, 2]. During evaluation and benchmarking,
4. [4]:  Passage ID 4: learning is a field of machine learning that aims to learn representations from unlabeled data. It is becoming a dominant paradigm in natural language processing (NLP) and speech technologies and is also heavily researched in computer vision. However, the differences in the domains are significant, text data represents much higher-level concepts and is more structured, while visual data has a much lower semantic level and is often higher dimensional. Self-supervised learning relies on pretext tasks, which are more intuitive, and empirically better performing in NLP, while in vision the pretext tasks need to encourage learning from low-level features to high-level semantics.Over the past years, many methods have been proposed for visual self-supervised learning. Most of these build on instance discrimination learning, where the model is trained to distinguish between different views of the same image. This is done by creating different augmented versions of the same image (called
5. [5]:  Passage ID 5: Firstly, whether pre-training consistently leads to performance improvements. Secondly, whether pre-training achieves higher accuracy when faced with limited labeled data, but eventually levels off at a performance comparable to the baseline when sufficient labeled data is available. Thirdly, whether pre-training converges to baseline performance before reaching its plateau in accuracy. To address these hypotheses, the authors conducted experiments on the synthetic COCO dataset with rendering, allowing for the availability of a large number of labels. The results revealed that self-supervised pre-training adheres to the assumption outlined in the third hypothesis. This suggests that SSL does not surpass supervised learning in terms of learning capability, but does perform effectively when dealing with limited labeled data.OthersBesides the aforementioned works, several other approaches have employed CL. Among them, [52, 84] investigated the utilization of vision transformers