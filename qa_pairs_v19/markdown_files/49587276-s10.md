# A Survey on Recent Advances in Named Entity Recognition from Deep Learning models

## Question

How do neural network systems that infer features compare to those that rely on feature engineering in Named Entity Recognition (NER) tasks across various languages?

## URLs

1. https://ar5iv.org/html/2401.10825. [2401.10825] A survey on recent advances in Named Entity Recognition
2. https://ar5iv.org/html/1812.09449. [1812.09449] A Survey on Deep Learning for Named Entity Recognition
3. https://ar5iv.org/html/2409.10521. [2409.10521] LSTM Recurrent Neural Networks for Cybersecurity Named Entity Recognition
4. https://ar5iv.org/html/1910.11470. [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
5. https://ar5iv.org/html/2205.15485. [2205.15485] FinBERT-MRC: financial named entity recognition using BERT under the machine reading comprehension paradigm
6. https://ar5iv.org/html/2404.04959. [2404.04959] A Two Dimensional Feature Engineering Method for Relation Extraction
7. https://ar5iv.org/html/2311.13755. [2311.13755] Transformer-based Named Entity Recognition in Construction Supply Chain Risk Management in Australia
8. https://ar5iv.org/html/2403.17561. [2403.17561] A Survey on Deep Learning and State-of-the-art Applications
9. https://ar5iv.org/html/2412.01948. [2412.01948] The Evolution and Future Perspectives of Artificial Intelligence Generated Content

## Answer (I don't know?: ✗)

Neural network systems that infer features and those that rely on feature engineering represent two distinct approaches in Named Entity Recognition (NER) tasks, each with its own advantages and limitations.

Neural network-based NER systems have gained traction in recent years due to their ability to achieve state-of-the-art performance through minimal feature engineering. These systems utilize architectures that allow them to learn representations from raw data without the need for extensive prior feature selection or crafting [1] [2] [3]. This inherent capability makes them generally more domain-independent, as they do not require domain-specific resources such as lexicons or ontologies [1]. Consequently, neural network systems can adapt more readily to various NER tasks across multiple languages.

Conversely, traditional feature-engineered NER systems depend on manually crafted features, which include handcrafted rules, orthographic features, and lexicons. While these models have been effective in the past, their reliance on the quality and applicability of these features can limit their performance, especially in multilingual contexts or when confronted with domain-specific nuances [4]. Additionally, existing surveys indicate that many of the traditional models primarily focus on single languages or specific domains, which can hinder their generalizability across diverse NER tasks [4] [5].

Despite the advantages of neural network models, there remains a gap in comprehensive surveys examining their effectiveness compared to feature-engineered systems, particularly in multilingual and multi-domain settings [4] [5]. Current literature is fragmented and often emphasizes feature-engineered systems, leaving a notable absence of analysis concerning how neural approaches stack up against traditional models in broader contexts.

In summary, neural network systems demonstrate significant promise in overcoming the limitations of feature-engineered systems, particularly in their adaptability and performance across various domains and languages. However, there is still a need for more systematic studies that analyze and compare these two approaches comprehensively across different NER tasks, which would elucidate the specific circumstances under which each method excels or falters. Thus, while the existing context outlines the superiority of neural network systems in certain areas, more empirical research is needed to fully understand their comparative strengths and weaknesses in the realm of multilingual NER challenges.

1. [1]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
2. [2]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
3. [3]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
4. [4]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
5. [5]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
---
1. [1]:  Passage ID 1: etc. Thus it is important to highlight recent advances in named entity recognition, especially recent neural NER architectures which have achieved state of the art performance with minimal feature engineering.The first NER task was organized by ?) in the Sixth Message Understanding Conference. Since then, there have been numerous NER tasks [Tjong Kim Sang andDe Meulder (2003, Tjong Kim Sang (2002, Piskorski et al. (2017, Segura Bedmar et al. (2013, Bossy et al. (2013, Uzuner et al. (2011].Early NER systems were based on handcrafted rules, lexicons, orthographic features and ontologies.These systems were followed by NER systems based on feature-engineering and machine learning [Nadeau and Sekine (2007].Starting with ?), neural network NER systems with minimal feature engineering have become popular.Such models are appealing because they typically do not require domain specific resources like lexicons or ontologies, and are thus poised to be more domain independent.Various
2. [2]:  Passage ID 2: etc. Thus it is important to highlight recent advances in named entity recognition, especially recent neural NER architectures which have achieved state of the art performance with minimal feature engineering.The first NER task was organized by ?) in the Sixth Message Understanding Conference. Since then, there have been numerous NER tasks [Tjong Kim Sang andDe Meulder (2003, Tjong Kim Sang (2002, Piskorski et al. (2017, Segura Bedmar et al. (2013, Bossy et al. (2013, Uzuner et al. (2011].Early NER systems were based on handcrafted rules, lexicons, orthographic features and ontologies.These systems were followed by NER systems based on feature-engineering and machine learning [Nadeau and Sekine (2007].Starting with ?), neural network NER systems with minimal feature engineering have become popular.Such models are appealing because they typically do not require domain specific resources like lexicons or ontologies, and are thus poised to be more domain independent.Various
3. [3]:  Passage ID 3: etc. Thus it is important to highlight recent advances in named entity recognition, especially recent neural NER architectures which have achieved state of the art performance with minimal feature engineering.The first NER task was organized by ?) in the Sixth Message Understanding Conference. Since then, there have been numerous NER tasks [Tjong Kim Sang andDe Meulder (2003, Tjong Kim Sang (2002, Piskorski et al. (2017, Segura Bedmar et al. (2013, Bossy et al. (2013, Uzuner et al. (2011].Early NER systems were based on handcrafted rules, lexicons, orthographic features and ontologies.These systems were followed by NER systems based on feature-engineering and machine learning [Nadeau and Sekine (2007].Starting with ?), neural network NER systems with minimal feature engineering have become popular.Such models are appealing because they typically do not require domain specific resources like lexicons or ontologies, and are thus poised to be more domain independent.Various
4. [4]:  Passage ID 4: included a few introductory neural network NER systems.There have also been surveys focused on NER systems for specific domains and languages, including biomedical NER, [Leaman and Gonzalez (2008], Chinese clinical NER [Lei et al. (2013], Arabic NER [Shaalan (2014, Etaiwi et al. (2017], and NER for Indian languages [Patil et al. (2016].The existing surveys primarily cover feature-engineered machine learning models (including supervised, semi-supervised, and unsupervised systems), and mostly focus on a single language or a single domain.There is not yet, to our knowledge, a comprehensive survey of modern neural network NER systems, nor is there a survey that compares feature engineered and neural network systems in both multi-lingual (CoNLL 2002 and CoNLL 2003) and multi-domain (e.g., news and medical) settings.3 MethodologyTo identify articles for this survey, we searched Google, Google Scholar, and Semantic Scholar.Our query terms included named entity recognition, neural
5. [5]:  Passage ID 5: included a few introductory neural network NER systems.There have also been surveys focused on NER systems for specific domains and languages, including biomedical NER, [Leaman and Gonzalez (2008], Chinese clinical NER [Lei et al. (2013], Arabic NER [Shaalan (2014, Etaiwi et al. (2017], and NER for Indian languages [Patil et al. (2016].The existing surveys primarily cover feature-engineered machine learning models (including supervised, semi-supervised, and unsupervised systems), and mostly focus on a single language or a single domain.There is not yet, to our knowledge, a comprehensive survey of modern neural network NER systems, nor is there a survey that compares feature engineered and neural network systems in both multi-lingual (CoNLL 2002 and CoNLL 2003) and multi-domain (e.g., news and medical) settings.3 MethodologyTo identify articles for this survey, we searched Google, Google Scholar, and Semantic Scholar.Our query terms included named entity recognition, neural