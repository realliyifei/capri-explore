# A Survey on Recent Advances in Named Entity Recognition from Deep Learning models

## Question

How do neural network systems that infer features compare to those that rely on feature engineering in Named Entity Recognition (NER) tasks across various languages?

## URLs

1. https://ar5iv.org/html/2401.10825. [2401.10825] A survey on recent advances in Named Entity Recognition
2. https://ar5iv.org/html/1812.09449. [1812.09449] A Survey on Deep Learning for Named Entity Recognition
3. https://ar5iv.org/html/2409.10521. [2409.10521] LSTM Recurrent Neural Networks for Cybersecurity Named Entity Recognition
4. https://ar5iv.org/html/1910.11470. [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
5. https://ar5iv.org/html/2205.15485. [2205.15485] FinBERT-MRC: financial named entity recognition using BERT under the machine reading comprehension paradigm
6. https://ar5iv.org/html/2404.04959. [2404.04959] A Two Dimensional Feature Engineering Method for Relation Extraction
7. https://ar5iv.org/html/2311.13755. [2311.13755] Transformer-based Named Entity Recognition in Construction Supply Chain Risk Management in Australia
8. https://ar5iv.org/html/2403.17561. [2403.17561] A Survey on Deep Learning and State-of-the-art Applications
9. https://ar5iv.org/html/2412.01948. [2412.01948] The Evolution and Future Perspectives of Artificial Intelligence Generated Content

## Answer (I don't know?: ✗)

Neural network systems that infer features typically outperform those that rely on feature engineering in Named Entity Recognition (NER) tasks across various languages. The context illustrates that for specific languages, the best feature-engineered systems consistently lag behind the best neural network models. For instance, the best feature-engineered system for Spanish is 1.59% less effective than the best feature-inferring neural network system, and 1.65% less effective than the best neural network system utilizing lexical resources [1]. In German, the best feature-engineered system showed a deficit of 2.34% compared to the best feature-inferring system [1]. This trend is also evident, albeit with smaller margins, in Dutch and English, indicating that while the gaps may vary, feature-engineered models are generally outperformed by their neural network counterparts [1].

The overarching advantage of neural network systems is their minimal reliance on handcrafted features or specific domain resources, which broadens their applicability across various domains and languages [2]. Unlike the earlier NER systems that depended heavily on manually crafted rules, lexicons, and various features, modern neural architectures have emerged that maintain state-of-the-art performance with lesser feature engineering, which often makes them more adaptable [4]. This shift has been particularly important given the diverse nature of NER tasks and the necessity for systems that can work across different information domains.

Moreover, the context points out that hybrid models combining word and character features surpass both purely word-based and character-based models, further emphasizing the progression of NER technology towards more complex and capable systems [1]. The advancements made by neural network architectures reflect the lessons learned from past methodologies based on feature engineering [4]. 

Despite the recognized improvements that neural networks have brought to NER tasks—exemplified by their performance improvements over previous feature-based models across multiple languages (such as improvements of 1.59% for Spanish and 2.34% for German) [5]—there remains a lack of comprehensive surveys comparing these neural architectures directly across multilingual and multidomain settings [3]. 

In summary, neural network systems represent a paradigm shift in NER, consistently outperforming feature-engineered systems due to their minimal reliance on tailored features and capabilities to generalize across different language tasks. The comparative analysis across languages like Spanish, German, Dutch, and English reiterates the efficacy of neural networks in enhancing NER systems overall.

1. [1]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
2. [2]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
3. [3]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
4. [4]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
5. [5]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
---
1. [1]:  Passage ID 1: access to domain specific rules, knowledge, features, and lexicons.For example, the best feature-engineered system for Spanish, ?), is 1.59% below the best feature-inferring neural network system, [Lample et al. (2016], and 1.65% below the best neural network system that incorporates lexical resources [Bharadwaj et al. (2016].Similarly, the best feature-engineered system for German, ?), is 2.34% below the best feature-inferring neural network system, ?).The differences are smaller for Dutch and English, but in neither case is the best feature-engineered model better than the best neural network model. In DrugNER, the word+character NN model outperforms the feature engineered system by 8.90% on MedLine test data and 3.50% on the overall dataset.Our next finding is that word+character hybrid models are generally better than both word-based and character-based models.For example, the best hybrid NN model for English, ?), is 0.52% better than the best word-based model, ?), and 5.12%
2. [2]:  Passage ID 2: etc. Thus it is important to highlight recent advances in named entity recognition, especially recent neural NER architectures which have achieved state of the art performance with minimal feature engineering.The first NER task was organized by ?) in the Sixth Message Understanding Conference. Since then, there have been numerous NER tasks [Tjong Kim Sang andDe Meulder (2003, Tjong Kim Sang (2002, Piskorski et al. (2017, Segura Bedmar et al. (2013, Bossy et al. (2013, Uzuner et al. (2011].Early NER systems were based on handcrafted rules, lexicons, orthographic features and ontologies.These systems were followed by NER systems based on feature-engineering and machine learning [Nadeau and Sekine (2007].Starting with ?), neural network NER systems with minimal feature engineering have become popular.Such models are appealing because they typically do not require domain specific resources like lexicons or ontologies, and are thus poised to be more domain independent.Various
3. [3]:  Passage ID 3: included a few introductory neural network NER systems.There have also been surveys focused on NER systems for specific domains and languages, including biomedical NER, [Leaman and Gonzalez (2008], Chinese clinical NER [Lei et al. (2013], Arabic NER [Shaalan (2014, Etaiwi et al. (2017], and NER for Indian languages [Patil et al. (2016].The existing surveys primarily cover feature-engineered machine learning models (including supervised, semi-supervised, and unsupervised systems), and mostly focus on a single language or a single domain.There is not yet, to our knowledge, a comprehensive survey of modern neural network NER systems, nor is there a survey that compares feature engineered and neural network systems in both multi-lingual (CoNLL 2002 and CoNLL 2003) and multi-domain (e.g., news and medical) settings.3 MethodologyTo identify articles for this survey, we searched Google, Google Scholar, and Semantic Scholar.Our query terms included named entity recognition, neural
4. [4]:  Passage ID 4: previous approaches to NER based on feature engineering and other supervised or semi-supervised learning algorithms. Our results highlight the improvements achieved by neural networks, and show how incorporating some of the lessons learned from past work on feature-based NER systems can yield further improvements.1 Introduction00footnotetext:  This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/Named entity recognition is the task of identifying named entities like person, location, organization, drug, time, clinical procedure, biological protein, etc. in text. NER systems are often used as the first step in question answering, information retrieval, co-reference resolution, topic modeling, etc. Thus it is important to highlight recent advances in named entity recognition, especially recent neural NER architectures which have achieved state of the art performance with minimal feature
5. [5]:  Passage ID 5: words and sub-word units.We show in Table 1 and Table 2 and discuss in Section 7how neural NER systems have improved performance over past works including supervised, semi-supervised, and knowledge based NER systems.For example, NN models on news corpora improved the previous state-of-the-art by 1.59% in Spanish, 2.34% in German, 0.36% in English, and 0.14%, in Dutch, without any external resources or feature engineering.We provide resources, including links to shared tasks on NER, and links to the code for each category of NER system.To the best of our knowledge, this is the first survey focusing on neural architectures for NER, and comparing to previous feature-based systems.We first discuss previous summary research on NER in section 2. Then we explain our selection criterion and methodology for selecting which systems to review in section 3. We highlight standard, past and recent NER datasets (from shared tasks and other research) in section 4 and evaluation metrics in