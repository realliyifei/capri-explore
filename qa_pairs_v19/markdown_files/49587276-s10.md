# A Survey on Recent Advances in Named Entity Recognition from Deep Learning models

## Question

How do neural network systems that infer features compare to those that rely on feature engineering in Named Entity Recognition (NER) tasks across various languages?

## URLs

1. https://ar5iv.org/html/2401.10825
2. https://ar5iv.org/html/1812.09449
3. https://ar5iv.org/html/2409.10521
4. https://ar5iv.org/html/1910.11470
5. https://ar5iv.org/html/2205.15485
6. https://ar5iv.org/html/2404.04959
7. https://ar5iv.org/html/2311.13755
8. https://ar5iv.org/html/2403.17561
9. https://ar5iv.org/html/2412.01948

## Answer

Neural network systems that infer features in Named Entity Recognition (NER) tasks generally exhibit distinct advantages over traditional systems that rely heavily on feature engineering. The advancements in recent neural NER architectures have achieved state-of-the-art performance while requiring minimal feature engineering, making them more appealing as they tend to be more domain-independent and do not necessitate domain-specific resources like lexicons or ontologies [1] [2] [3]. 

Historically, NER systems began with handcrafted rules and relied on lexicons, orthographic features, and ontological knowledge. As the field advanced, feature-engineered machine learning models became prevalent, leading to a period where various techniques like supervised, semi-supervised, and unsupervised systems were utilized [1] [2]. However, these feature-engineered systems are often limited to specific domains or languages, making them less versatile as compared to their neural counterparts [4] [5]. 

Despite the evident advantages of neural network systems, the existing literature indicates a gap in comprehensive evaluations and comparisons between feature-engineered and neural network systems across diverse domains and multilingual settings [4] [5]. Surveys have primarily concentrated on feature-engineered models and have not fully addressed modern neural network NER systems or offered a comparative analysis of their performances across multiple languages and domains [4]. The NER tasks, such as those mapped in the CoNLL2002 and CoNLL2003 datasets, are essential for understanding the comparative performances, yet there remains a lack of extensive surveys that tackle this issue comprehensively [5].

In summary, while neural networks in NER bring a refreshing shift towards robust performance with less dependency on handcrafted features, further research is required to systematically compare their efficacy and versatility against feature-engineered models, particularly in handling multi-lingual and multi-domain challenges in NER tasks.

[1]: https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
[2]: https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
[3]: https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
[4]: https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
[5]: https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models

[1]: Passage ID 1: etc. Thus it is important to highlight recent advances in named entity recognition, especially recent neural NER architectures which have achieved state of the art performance with minimal feature engineering.The first NER task was organized by ?) in the Sixth Message Understanding Conference. Since then, there have been numerous NER tasks [Tjong Kim Sang andDe Meulder (2003, Tjong Kim Sang (2002, Piskorski et al. (2017, Segura Bedmar et al. (2013, Bossy et al. (2013, Uzuner et al. (2011].Early NER systems were based on handcrafted rules, lexicons, orthographic features and ontologies.These systems were followed by NER systems based on feature-engineering and machine learning [Nadeau and Sekine (2007].Starting with ?), neural network NER systems with minimal feature engineering have become popular.Such models are appealing because they typically do not require domain specific resources like lexicons or ontologies, and are thus poised to be more domain independent.Various
[2]: Passage ID 2: etc. Thus it is important to highlight recent advances in named entity recognition, especially recent neural NER architectures which have achieved state of the art performance with minimal feature engineering.The first NER task was organized by ?) in the Sixth Message Understanding Conference. Since then, there have been numerous NER tasks [Tjong Kim Sang andDe Meulder (2003, Tjong Kim Sang (2002, Piskorski et al. (2017, Segura Bedmar et al. (2013, Bossy et al. (2013, Uzuner et al. (2011].Early NER systems were based on handcrafted rules, lexicons, orthographic features and ontologies.These systems were followed by NER systems based on feature-engineering and machine learning [Nadeau and Sekine (2007].Starting with ?), neural network NER systems with minimal feature engineering have become popular.Such models are appealing because they typically do not require domain specific resources like lexicons or ontologies, and are thus poised to be more domain independent.Various
[3]: Passage ID 3: etc. Thus it is important to highlight recent advances in named entity recognition, especially recent neural NER architectures which have achieved state of the art performance with minimal feature engineering.The first NER task was organized by ?) in the Sixth Message Understanding Conference. Since then, there have been numerous NER tasks [Tjong Kim Sang andDe Meulder (2003, Tjong Kim Sang (2002, Piskorski et al. (2017, Segura Bedmar et al. (2013, Bossy et al. (2013, Uzuner et al. (2011].Early NER systems were based on handcrafted rules, lexicons, orthographic features and ontologies.These systems were followed by NER systems based on feature-engineering and machine learning [Nadeau and Sekine (2007].Starting with ?), neural network NER systems with minimal feature engineering have become popular.Such models are appealing because they typically do not require domain specific resources like lexicons or ontologies, and are thus poised to be more domain independent.Various
[4]: Passage ID 4: included a few introductory neural network NER systems.There have also been surveys focused on NER systems for specific domains and languages, including biomedical NER, [Leaman and Gonzalez (2008], Chinese clinical NER [Lei et al. (2013], Arabic NER [Shaalan (2014, Etaiwi et al. (2017], and NER for Indian languages [Patil et al. (2016].The existing surveys primarily cover feature-engineered machine learning models (including supervised, semi-supervised, and unsupervised systems), and mostly focus on a single language or a single domain.There is not yet, to our knowledge, a comprehensive survey of modern neural network NER systems, nor is there a survey that compares feature engineered and neural network systems in both multi-lingual (CoNLL 2002 and CoNLL 2003) and multi-domain (e.g., news and medical) settings.3 MethodologyTo identify articles for this survey, we searched Google, Google Scholar, and Semantic Scholar.Our query terms included named entity recognition, neural
[5]: Passage ID 5: included a few introductory neural network NER systems.There have also been surveys focused on NER systems for specific domains and languages, including biomedical NER, [Leaman and Gonzalez (2008], Chinese clinical NER [Lei et al. (2013], Arabic NER [Shaalan (2014, Etaiwi et al. (2017], and NER for Indian languages [Patil et al. (2016].The existing surveys primarily cover feature-engineered machine learning models (including supervised, semi-supervised, and unsupervised systems), and mostly focus on a single language or a single domain.There is not yet, to our knowledge, a comprehensive survey of modern neural network NER systems, nor is there a survey that compares feature engineered and neural network systems in both multi-lingual (CoNLL 2002 and CoNLL 2003) and multi-domain (e.g., news and medical) settings.3 MethodologyTo identify articles for this survey, we searched Google, Google Scholar, and Semantic Scholar.Our query terms included named entity recognition, neural