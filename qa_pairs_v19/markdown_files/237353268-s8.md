# Neuron-level Interpretation of Deep NLP Models: A Survey

## Question

How do causation-based methods like ablation determine and assess the significance of neurons in deep NLP models?

## URLs

1. https://ar5iv.org/html/2409.15827
2. https://ar5iv.org/html/2206.13288
3. https://ar5iv.org/html/2405.06855
4. https://ar5iv.org/html/2108.13138
5. https://ar5iv.org/html/2408.01416
6. https://ar5iv.org/html/2211.04367
7. https://ar5iv.org/html/2407.04690
8. https://ar5iv.org/html/2412.02104
9. https://ar5iv.org/html/2207.13243

## Answer

Causation-based methods, particularly ablation, are instrumental in evaluating the significance of neurons in deep Natural Language Processing (NLP) models. The central premise of ablation involves selectively disabling certain neurons to observe how their removal affects the model's performance on specific tasks. By clamping the output of chosen neurons to zero or a fixed value, researchers can measure the degradation of the model’s predictive capabilities, which illuminates the functional contributions of those neurons [3] [4].

Previous studies have highlighted that a small subset of neurons can significantly represent particular linguistic abilities or functions, underscoring the relevance of identifying these critical neurons [5]. For example, ablation procedures have systematically targeted the top-performing neurons identified in tasks, allowing researchers to evaluate their causal roles. When these neurons are ablated (i.e., their activations set to zero), the resulting decline in performance indicates their importance in processing related linguistic phenomena [5]. This methodology thus provides a clear link between individual neuron contributions and overall model effectiveness, facilitating a deeper understanding of neural encoding.

Moreover, the impact of ablation experiments can be quantified to assess not only the essentiality of certain neurons but also how redundantly the information they carry is preserved across the network. For instance, findings have shown that specific neurons are crucial for core linguistic competence, evident from performance drops across multiple languages when these neurons are disabled [4]. This suggests that while some neurons are specialized for particular tasks, others may share overlapping functions, contributing to a distributed representation of knowledge within the model.

Additionally, the manipulation of neuron activation levels—such as enhancing the activations of particular neurons—serves as a complementary approach to ablation. This technique allows researchers to investigate how amplifying certain neuron activities alters model predictions, thus highlighting the contribution of those neurons to task performance [4]. For example, by doubling the activations of specific neurons, researchers have illustrated that such enhancements can significantly improve the model’s functionality in retrieving factual knowledge [4].

However, it is important to note that causation-based methods have limitations; they primarily focus on identifying neurons that have learned encoded concepts rather than assessing their overall importance to the model's performance [3]. This calls for careful interpretation of results, ensuring that while neuron significance can be inferred from performance changes, the complexity of neural network interactions implies that further, more comprehensive analysis may be necessary to capture the entire landscape of neuron influence.

In summary, causation-based methods such as ablation utilize neuron suppression and activation manipulation to elucidate the roles of specific neurons within deep NLP models. These approaches provide insights into neural contributions to linguistic abilities, enabling researchers to map the neural underpinnings of model performance comprehensively.

[1]: https://ar5iv.org/html/2409.15827, [2409.15827] Unveiling Language Competence Neurons: A Psycholinguistic Approach to Model Interpretability
[2]: https://ar5iv.org/html/2206.13288, [2206.13288] Discovering Salient Neurons in deep NLP models
[3]: https://ar5iv.org/html/2108.13138, [2108.13138] Neuron-level Interpretation of Deep NLP Models: A Survey
[4]: https://ar5iv.org/html/2409.15827, [2409.15827] Unveiling Language Competence Neurons: A Psycholinguistic Approach to Model Interpretability
[5]: https://ar5iv.org/html/2409.15827, [2409.15827] Unveiling Language Competence Neurons: A Psycholinguistic Approach to Model Interpretability

[1]: Passage ID 1: human-like causal attributions based on verb types (e.g., stimulus-experiencer versus experiencer-stimulus verbs). Ablating the top neurons significantly impaired the model’s performance, while doubling their activation greatly enhanced it. These findings suggest that language abilities—like the model’s understanding of implicit causality—are likely encoded in specific sets of neurons, and manipulating these neurons directly impacts the model’s competence in the task.These results contribute to ongoing discussions about model interpretability, particularly at the neuron level. The finding that certain neurons are strongly linked to specific linguistic phenomena, especially when the model shows human-like competence, underscores the importance of neuron-level analyses for understanding how LLMs process and represent language. When the model demonstrates language abilities, these abilities appear to correspond to identifiable neurons, suggesting that these neurons play a critical role
[2]: Passage ID 2: Correlation Analysis to extract salient neurons in the model, with respect to any extrinsic property, with the goal of understanding how such knowledge is preserved within neurons. We carry out a fine-grained analysis to answer the following questions: (i) can we identify subsets of neurons in the network that learn a specific linguistic property? (ii) is a certain linguistic phenomenon in a given model localized (encoded in few individual neurons) or distributed across many neurons? (iii) how redundantly is the information preserved? (iv) how does fine-tuning pre-trained models towards downstream NLP tasks impact the learned linguistic knowledge? (v) how do models vary in learning different linguistic properties? Our data-driven, quantitative analysis illuminates interesting findings: (i) we found small subsets of neurons that can predict different linguistic tasks; (ii) neurons capturing basic lexical information, such as suffixation, are localized in the lowermost layers; (iii)
[3]: Passage ID 3: Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.LimitationIn addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.3.4 Causation-based methodsThe methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts.They do not inherently reflecttheir importance towards the model’s performance. Causation-based methods identify neurons with respect to model’s prediction.AblationThe central idea behind ablation is to notice the affect of a neuron on model’s performance by varying its value. This is done either by clamping its value to zero or a fixed valueand observethe change in network’s performance.Ablation has been effectively used to find i) salient neurons with respect to a
[4]: Passage ID 4: involves selectively disabling certain neurons to assess their contribution to a model’s performance, while activation manipulation, such as doubling or suppressing activations, can be used to amplify or diminish a neuron’s influence on predictions.The use of ablation techniques has yielded important insights into neural network behavior. For example, Zhang et al. (2024) demonstrated that ablating specific neurons in NLP models could disrupt language performance for many languages, suggesting that these neurons play a key role in core linguitics competence. Activation manipulation, though less explored, offers a complementary approach. By amplifying the activations of select neurons, researchers can assess how much those neurons contribute to a model’s behavior. For instance,Mu and Andreas (2020) used activation enhancement to demonstrate that certain neurons were crucial for tasks involving factual knowledge retrieval in LLMs. The combination of ablation and activation techniques
[5]: Passage ID 5: Previous studies have shown that a small subset of neurons can disproportionately represent specific abilities or functions Templeton (2024).Figure 1: The contribution proportion of the top 50 neurons across three different experiments. Each point represents a neuron. The top 5 are highlighted with black circular outlines.3.6 Neuron Ablation ProcedureTo evaluate the causal role of the selected neurons, we conducted a series of ablation experiments. The goal was to determine whether the removal of these neurons would result in a degradation of the model’s performance on the relevant psycholinguistic tasks.We followed this procedure for neuron ablation: First, we ablated the top 5 and top 50 neurons identified in each task by setting their activations to zero during inference. We then re-ran the model on both the probing and testing stimuli to assess whether the ablated neurons had a direct effect on the model’s predictions. As a baseline, we conducted random ablation, where