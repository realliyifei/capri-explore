# Neuron-level Interpretation of Deep NLP Models: A Survey

## Question

How do causation-based methods like ablation determine and assess the significance of neurons in deep NLP models?

## URLs

1. https://ar5iv.org/html/2409.15827. [2409.15827] Unveiling Language Competence Neurons: A Psycholinguistic Approach to Model Interpretability
2. https://ar5iv.org/html/2206.13288. [2206.13288] Discovering Salient Neurons in deep NLP models
3. https://ar5iv.org/html/2405.06855. [2405.06855] Linear Explanations for Individual Neurons
4. https://ar5iv.org/html/2108.13138. [2108.13138] Neuron-level Interpretation of Deep NLP Models: A Survey
5. https://ar5iv.org/html/2408.01416. [2408.01416] The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability
6. https://ar5iv.org/html/2211.04367. [2211.04367] Much Easier Said Than Done: Falsifying the Causal Relevance of Linear Decoding Methods
7. https://ar5iv.org/html/2407.04690. [2407.04690] Missed Causes and Ambiguous Effects: Counterfactuals Pose Challenges for Interpreting Neural Networks
8. https://ar5iv.org/html/2412.02104. [2412.02104] Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey
9. https://ar5iv.org/html/2207.13243. [2207.13243] Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks

## Answer (I don't know?: ✗)

Causation-based methods, particularly ablation techniques, play a critical role in determining and assessing the significance of neurons in deep NLP models by evaluating their contributions to the model's performance. Ablation involves selectively disabling specific neurons to observe the effects on the model’s predictions, which allows researchers to infer the causal relationship between those neurons and the model's linguistic abilities [3].

The central idea of ablation is to manipulate the activation of neurons—either by clamping their value to zero or to a fixed value—and monitoring how this change affects the overall performance of the model. This method is effective in identifying neurons that significantly contribute to specific linguistic functions [2][3]. For instance, previous studies demonstrated that ablating targeted neurons in natural language processing (NLP) models could disrupt language performance across various languages, highlighting the core competencies those neurons encapsulate [3]. Additionally, activation manipulation can complement ablation techniques by enhancing or suppressing neuron activations to further examine their influence on the model's outputs [3][5].

The methodology behind neuron ablation typically involves a structured procedure where researchers first identify salient neurons—the ones predicted to be important for specific tasks. For example, the top neurons can be ablated by setting their activations to zero and then reassessing the model's ability to perform on relevant psycholinguistic tasks. This approach provides direct evidence of the role those neurons play in the model's predictions [4]. A baseline comparison often includes random ablation of neurons to ensure that observed effects can be attributed specifically to the targeted neurons rather than random chance [4].

Moreover, the limitations of probing-based methods, which focus on identifying neurons with encoded concepts, are addressed by causation-based methods like ablation. While probing lacks direct insight into a neuron's impact on performance, ablation explicitly assesses the change in performance caused by manipulating neuron activity. This delineation allows researchers to not only identify influential neurons but also to understand their importance in the context of the model's output [2][3].

In summary, causation-based methods, particularly ablation, establish the significance of neurons by investigating their contributions to model performance through systematic manipulation and assessment. By linking specific neurons to distinct linguistic tasks, these methods provide deeper insights into the internal mechanisms of NLP models and enhance our understanding of their interpretability [1][2][5].

1. [1]:  https://ar5iv.org/html/2409.15827, [2409.15827] Unveiling Language Competence Neurons: A Psycholinguistic Approach to Model Interpretability
2. [2]:  https://ar5iv.org/html/2108.13138, [2108.13138] Neuron-level Interpretation of Deep NLP Models: A Survey
3. [3]:  https://ar5iv.org/html/2409.15827, [2409.15827] Unveiling Language Competence Neurons: A Psycholinguistic Approach to Model Interpretability
4. [4]:  https://ar5iv.org/html/2409.15827, [2409.15827] Unveiling Language Competence Neurons: A Psycholinguistic Approach to Model Interpretability
5. [5]:  https://ar5iv.org/html/2409.15827, [2409.15827] Unveiling Language Competence Neurons: A Psycholinguistic Approach to Model Interpretability
---
1. [1]:  Passage ID 1: human-like causal attributions based on verb types (e.g., stimulus-experiencer versus experiencer-stimulus verbs). Ablating the top neurons significantly impaired the model’s performance, while doubling their activation greatly enhanced it. These findings suggest that language abilities—like the model’s understanding of implicit causality—are likely encoded in specific sets of neurons, and manipulating these neurons directly impacts the model’s competence in the task.These results contribute to ongoing discussions about model interpretability, particularly at the neuron level. The finding that certain neurons are strongly linked to specific linguistic phenomena, especially when the model shows human-like competence, underscores the importance of neuron-level analyses for understanding how LLMs process and represent language. When the model demonstrates language abilities, these abilities appear to correspond to identifiable neurons, suggesting that these neurons play a critical role
2. [2]:  Passage ID 2: Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.LimitationIn addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.3.4 Causation-based methodsThe methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts.They do not inherently reflecttheir importance towards the model’s performance. Causation-based methods identify neurons with respect to model’s prediction.AblationThe central idea behind ablation is to notice the affect of a neuron on model’s performance by varying its value. This is done either by clamping its value to zero or a fixed valueand observethe change in network’s performance.Ablation has been effectively used to find i) salient neurons with respect to a
3. [3]:  Passage ID 3: involves selectively disabling certain neurons to assess their contribution to a model’s performance, while activation manipulation, such as doubling or suppressing activations, can be used to amplify or diminish a neuron’s influence on predictions.The use of ablation techniques has yielded important insights into neural network behavior. For example, Zhang et al. (2024) demonstrated that ablating specific neurons in NLP models could disrupt language performance for many languages, suggesting that these neurons play a key role in core linguitics competence. Activation manipulation, though less explored, offers a complementary approach. By amplifying the activations of select neurons, researchers can assess how much those neurons contribute to a model’s behavior. For instance,Mu and Andreas (2020) used activation enhancement to demonstrate that certain neurons were crucial for tasks involving factual knowledge retrieval in LLMs. The combination of ablation and activation techniques
4. [4]:  Passage ID 4: Previous studies have shown that a small subset of neurons can disproportionately represent specific abilities or functions Templeton (2024).Figure 1: The contribution proportion of the top 50 neurons across three different experiments. Each point represents a neuron. The top 5 are highlighted with black circular outlines.3.6 Neuron Ablation ProcedureTo evaluate the causal role of the selected neurons, we conducted a series of ablation experiments. The goal was to determine whether the removal of these neurons would result in a degradation of the model’s performance on the relevant psycholinguistic tasks.We followed this procedure for neuron ablation: First, we ablated the top 5 and top 50 neurons identified in each task by setting their activations to zero during inference. We then re-ran the model on both the probing and testing stimuli to assess whether the ablated neurons had a direct effect on the model’s predictions. As a baseline, we conducted random ablation, where
5. [5]:  Passage ID 5: assessed their ability to resolve syntactic ambiguity. Additionally, Qiu et al. (2023) investigated LLMs’ capacity for pragmatic reasoning. Cai et al. (2024) performed a systematic evaluation of human-like language use in models such as ChatGPT and Vicuna, showing that LLMs closely mirror human language patterns in many respects.Our study extends this line of work by using psycholinguistic tasks to examine whether specific neurons in GPT-2-XL encode representations that mirror human language competence, providing a new level of granularity in model interpretability.2.3 Neuron Ablation and Activation TechniquesNeuron ablation and activation manipulation have become increasingly popular methods for investigating the internal mechanisms of neural networks, including LLMs. Ablation involves selectively disabling certain neurons to assess their contribution to a model’s performance, while activation manipulation, such as doubling or suppressing activations, can be used to amplify