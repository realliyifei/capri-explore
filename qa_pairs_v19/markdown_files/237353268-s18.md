# Neuron-level Interpretation of Deep NLP Models: A Survey

## Question

How have researchers identified neurons that capture lexical concepts in deep NLP models, and what methods have they used?

## URLs

1. https://ar5iv.org/html/2206.13288
2. https://ar5iv.org/html/2108.13138
3. https://ar5iv.org/html/2406.10868
4. https://ar5iv.org/html/2405.20331
5. https://ar5iv.org/html/2006.14032
6. https://ar5iv.org/html/1807.10854
7. https://ar5iv.org/html/2305.19911
8. https://ar5iv.org/html/2307.10246

## Answer

## I don't know?: False

Researchers have identified neurons that capture lexical concepts in deep NLP models primarily through the analysis of neural language models, specifically utilizing techniques such as probing and leverage contextual understanding. Various studies, including those by Dalvi et al. (2019) and others, have contributed to this understanding by detailing how individual neurons correspond to linguistic information.

Dalvi et al. (2019) conducted an analysis of LSTM-based neural machine translation (NMT) models, revealing that certain neurons are specialized for capturing core linguistic concepts such as nouns, verb forms, numbers, and articles. Their work indicated a difference in neuron distribution based on the nature of the linguistic concept, highlighting that closed-class concepts—where new words are not regularly added—are managed by fewer neurons compared to open-class concepts—where new words can be introduced at any time, which are represented across a larger neuron pool [1][2].

Additionally, the findings suggested that lexical information, such as suffixes and word morphology, is predominantly housed in the lower layers of models, while more complex properties like syntactic information are learned in the higher layers [3][4]. This hierarchical approach to neuron function allows for a nuanced understanding of the structure of language that the models engage with.

In another approach, specific neurons have been instantiated through contextual search methods, which allow researchers to generate synthetic instances to investigate the specific neuron’s role. For example, a neuron identified as specializing in "horse racing" was established to be more generally engaged with "racing" concepts after creating novel contexts for testing [5]. This method demonstrates the versatility of neurons in representing broad concepts beyond their initially attributed labels.

Moreover, the use of linguistic concept probing—where researchers deliberately target neurons to assess their representation of morphology, semantics, and syntax—has emerged as an important method for analyzing models' capacities to generalize across different tasks [5]. Such methodology often sheds light on how linguistic structures are preserved across many neurons, indicating redundancy in the representation of linguistic information. 

In summary, through a combination of LSTM analysis, layer probing in transformer models, and contextual generation methods, researchers have identified how neurons in NLP models capture various lexical concepts. The findings emphasize the localization and distribution of linguistic phenomena across the network, providing insights that enhance our understanding of language representation in deep learning architectures.

1. [1]:  https://ar5iv.org/html/2108.13138, [2108.13138] Neuron-level Interpretation of Deep NLP Models: A Survey
2. [2]:  https://ar5iv.org/html/2108.13138, [2108.13138] Neuron-level Interpretation of Deep NLP Models: A Survey
3. [3]:  https://ar5iv.org/html/2206.13288, [2206.13288] Discovering Salient Neurons in deep NLP models
4. [4]:  https://ar5iv.org/html/2206.13288, [2206.13288] Discovering Salient Neurons in deep NLP models
5. [5]:  https://ar5iv.org/html/2108.13138, [2108.13138] Neuron-level Interpretation of Deep NLP Models: A Survey
---
1. [1]:  Passage ID 1: specialize in core linguistic conceptsDalvi et al. (2019) in their analysis of LSTM-based NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept.For example: closed class555closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ”chillax” a verb formed blending ”chill” and ”relax”. concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019)
2. [2]:  Passage ID 2: specialize in core linguistic conceptsDalvi et al. (2019) in their analysis of LSTM-based NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept.For example: closed class555closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ”chillax” a verb formed blending ”chill” and ”relax”. concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019)
3. [3]:  Passage ID 3: annotated data is available. We analyze the representations trained from neural language models of which we study the recent transformer based models. We use LCA to understand: i) how concepts are learned within neurons and ii) how these neurons distribute across the network. More specifically we probe for the following questions:•Question: Do the individual neurons in the transformers capture linguistic information and which parts of the network learn more about certain linguistic phenomena?•Finding: Neurons that capture tasks such as predicting shallow lexical information (such as suffixes) or word morphology are predominantly found in the lower layers and those learning more complex properties such as syntactic information are found in the higher layers. [Section 5.1]•Question: Is certain linguistic phenomenon in a given model localized or distributed across many neurons? And how redundantly is the information preserved?•Finding: Closed-class properties like
4. [4]:  Passage ID 4: annotated data is available. We analyze the representations trained from neural language models of which we study the recent transformer based models. We use LCA to understand: i) how concepts are learned within neurons and ii) how these neurons distribute across the network. More specifically we probe for the following questions:•Question: Do the individual neurons in the transformers capture linguistic information and which parts of the network learn more about certain linguistic phenomena?•Finding: Neurons that capture tasks such as predicting shallow lexical information (such as suffixes) or word morphology are predominantly found in the lower layers and those learning more complex properties such as syntactic information are found in the higher layers. [Section 5.1]•Question: Is certain linguistic phenomenon in a given model localized or distributed across many neurons? And how redundantly is the information preserved?•Finding: Closed-class properties like
5. [5]:  Passage ID 5: by generating synthetic instances. For example, they showed that a “horse racing” neuron identified via concept search method was in fact a general “racing” neuron by generating novel contexts against this neuron.5.1.2 Linguistic ConceptsA number of studiesprobed for neurons that capture core-linguistic concepts such asmorphology,semantic tags, etc. Probing for linguistic structure is important to understand models’ capacity to generalize Marasović (2018).444but is not the only reason to carry such an analysis.For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well Vauquois (1968); Jones et al. (2012). Below we discuss major findings along this line of work:Neurons specialize in core linguistic conceptsDalvi et al. (2019) in their analysis of LSTM-based NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles,