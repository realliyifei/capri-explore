# Neuron-level Interpretation of Deep NLP Models: A Survey

## Question

How have researchers identified neurons that capture lexical concepts in deep NLP models, and what methods have they used?

## URLs

1. https://ar5iv.org/html/2206.13288. [2206.13288] Discovering Salient Neurons in deep NLP models
2. https://ar5iv.org/html/2108.13138. [2108.13138] Neuron-level Interpretation of Deep NLP Models: A Survey
3. https://ar5iv.org/html/2406.10868. [2406.10868] Analyzing Key Neurons in Large Language Models
4. https://ar5iv.org/html/2405.20331. [2405.20331] CoSy: Evaluating Textual Explanations of Neurons
5. https://ar5iv.org/html/2006.14032. [2006.14032] Compositional Explanations of Neurons
6. https://ar5iv.org/html/1807.10854. [1807.10854] A Survey of the Usages of Deep Learning for Natural Language Processing
7. https://ar5iv.org/html/2305.19911. [2305.19911] Neuron to Graph: Interpreting Language Model Neurons at Scale
8. https://ar5iv.org/html/2307.10246. [2307.10246] Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)

## Answer (I don't know?: ✗)

Researchers have identified neurons that capture lexical concepts in deep NLP models using several methods, primarily focused on visualization and concept search techniques. This involves probing the neural networks to determine how they represent linguistic information.

One of the notable approaches is the analysis of LSTM-based Neural Machine Translation (NMT) models conducted by Dalvi et al. (2019), who found that specific neurons correlate with core linguistic concepts like nouns, verb forms, numbers, and articles. This work demonstrated that the number of neurons dedicated to representing a particular concept is influenced by the nature of that concept; for example, closed-class words, which do not evolve significantly, are represented by fewer neurons than open-class words, which continually adopt new terms as language evolves [1][3].

Further insights into neuron specialization were obtained through the use of recent transformer-based models, where researchers employed Layer-wise Concept Activation (LCA) analysis. This method allowed them to explore how individual neurons capture linguistic properties and the distribution of these neurons across different layers of the network. Findings indicated that lower layers predominantly handle tasks related to shallow lexical information, such as morphological aspects of words, while higher layers are more concerned with complex syntactic structures [2]. This hierarchical representation reflects how different levels of linguistic information are organized within the model, with simpler concepts localizing in lower layers and more intricate structures becoming more distributed in higher layers.

Visualization methods have also yielded significant results; for instance, research by Karpathy et al. (2015) revealed neurons that activate based on the positional context of a word in a sentence. Other studies, such as those by Li et al. (2016), identified specific neurons that activate for intensifiers and negation, which are crucial for sentiment analysis tasks [4].

Moreover, the exploration of how neurons represent lexical concepts contributes to understanding the architecture of the models themselves, revealing how concepts like morphology and syntax are hierarchically organized. Durrani et al. (2020) conducted an analysis of neuron distribution across layers in pre-trained language models, confirming that concepts like morphology and syntax are effectively spread out across the network, underscoring the redundant and distributed nature of representing linguistic phenomena [5].

In summary, researchers have utilized methods such as LSTM and transformer-based analysis, LCA, concept visualization, and hierarchical organization studies to identify and understand how neurons in NLP models capture and represent lexical concepts. These methods collectively allow for a comprehensive view of how linguistic information is processed and structured within deep learning architectures.

1. [1]:  https://ar5iv.org/html/2108.13138, [2108.13138] Neuron-level Interpretation of Deep NLP Models: A Survey
2. [2]:  https://ar5iv.org/html/2206.13288, [2206.13288] Discovering Salient Neurons in deep NLP models
3. [3]:  https://ar5iv.org/html/2108.13138, [2108.13138] Neuron-level Interpretation of Deep NLP Models: A Survey
4. [4]:  https://ar5iv.org/html/2108.13138, [2108.13138] Neuron-level Interpretation of Deep NLP Models: A Survey
5. [5]:  https://ar5iv.org/html/2108.13138, [2108.13138] Neuron-level Interpretation of Deep NLP Models: A Survey
---
1. [1]:  Passage ID 1: specialize in core linguistic conceptsDalvi et al. (2019) in their analysis of LSTM-based NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept.For example: closed class555closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ”chillax” a verb formed blending ”chill” and ”relax”. concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019)
2. [2]:  Passage ID 2: annotated data is available. We analyze the representations trained from neural language models of which we study the recent transformer based models. We use LCA to understand: i) how concepts are learned within neurons and ii) how these neurons distribute across the network. More specifically we probe for the following questions:•Question: Do the individual neurons in the transformers capture linguistic information and which parts of the network learn more about certain linguistic phenomena?•Finding: Neurons that capture tasks such as predicting shallow lexical information (such as suffixes) or word morphology are predominantly found in the lower layers and those learning more complex properties such as syntactic information are found in the higher layers. [Section 5.1]•Question: Is certain linguistic phenomenon in a given model localized or distributed across many neurons? And how redundantly is the information preserved?•Finding: Closed-class properties like
3. [3]:  Passage ID 3: by generating synthetic instances. For example, they showed that a “horse racing” neuron identified via concept search method was in fact a general “racing” neuron by generating novel contexts against this neuron.5.1.2 Linguistic ConceptsA number of studiesprobed for neurons that capture core-linguistic concepts such asmorphology,semantic tags, etc. Probing for linguistic structure is important to understand models’ capacity to generalize Marasović (2018).444but is not the only reason to carry such an analysis.For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well Vauquois (1968); Jones et al. (2012). Below we discuss major findings along this line of work:Neurons specialize in core linguistic conceptsDalvi et al. (2019) in their analysis of LSTM-based NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles,
4. [4]:  Passage ID 4: two broad categories: i) concept discovery and ii) architectural analysis.5.1 Concept DiscoveryIn the following, we survey what lexical concepts or core-linguistic phenomenon are learned by the neurons in the network.5.1.1 Lexical ConceptsSome of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.VisualizationsKarpathy et al. (2015) found neurons that learn position of a word in the input sentence:activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example “I like this movie a lot” or “the movie is incredibly good”. Similarly they discovered neurons that captured “negation”. Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was
5. [5]:  Passage ID 5: concepts for the end performance than linguistic concepts.5.2 Architectural AnalysisAlongside studying what concepts are captured within deep NLP models, researchers have also studied: i) how these concepts are organized in the network? ii) how distributed and redundant they are? and iii) how this compares acrossarchitectures? Such an analysis is helpful in better understanding of the network and can be potentially useful in architectural search and model distillation.5.2.1 Information DistributionHuman languages are hierarchical in structure where morphology and phonology sit at the bottom followed by lexemes, followed by syntactic structures. Concepts such as semantics and pragmatics are placed on the top of the hierarchy. Durrani et al. (2020) analyzed linguistic hierarchy by studying the spread of neurons across layers in various pre-trained language models. They extracted salient neurons with respect to different linguistic concepts (e.g. morphology and syntax) and