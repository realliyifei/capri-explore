# Survey on the Use of Typological Information in Natural Language Processing

## Question

How does incorporating typological information enhance the performance of multilingual dependency parsing?

## URLs

1. https://ar5iv.org/html/2410.07809. [2410.07809] Linguistically-Informed Multilingual Instruction Tuning: Is There an Optimal Set of Languages to Tune?
2. https://ar5iv.org/html/2404.16627. [2404.16627] Incorporating Lexical and Syntactic Knowledge for Unsupervised Cross-Lingual Transfer
3. https://ar5iv.org/html/2401.06034. [2401.06034] LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization
4. https://ar5iv.org/html/2407.05022. [2407.05022] A Principled Framework for Evaluating on Typologically Diverse Languages
5. https://ar5iv.org/html/2110.08875. [2110.08875] Predicting the Performance of Multilingual NLP Models
6. https://ar5iv.org/html/2106.02082. [2106.02082] Language Embeddings for Typology and Cross-lingual Transfer Learning
7. https://ar5iv.org/html/2310.13440. [2310.13440] The Past, Present, and Future of Typological Databases in NLP
8. https://ar5iv.org/html/1909.09279. [1909.09279] Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers
9. https://ar5iv.org/html/2411.14198. [2411.14198] Why do language models perform worse for morphologically complex languages?

## Answer (I don't know?: ✗)

Incorporating typological information into multilingual dependency parsing enhances performance primarily by facilitating cross-lingual transfer and improving the robustness of parsing models. This integration plays a critical role, particularly in the context of resource allocation in natural language processing (NLP) for languages with limited resources.

Firstly, linguistic typology helps in grouping languages into syntactically-homogeneous clusters. This coarseness in grouping allows models to leverage broader syntactic similarities, which aids in improving the transfer performance during parsing tasks [3]. Research has indicated that typological information should be consistent with actual corpus statistics to yield optimal results for cross-lingual transfer [3]. Effective classification not only aligns with existing syntactic structures but also enhances the likelihood of accurately applying learned knowledge from resource-rich languages to those that are under-resourced [4].

Moreover, typology can serve as an inductive bias that guides machine learning algorithms, allowing them to better handle the challenges posed by the substantial structural divergence among languages [4]. The NLP community has recognized this potential and invested resources in creating universal annotation schemas and expanding treebanks across diverse language families, making typological information even more critical [4]. 

Although typology provides a rough proxy for cross-lingual transferability, relying on its structured insights is essential. Tools that leverage this information, like vector representations of languages based on typology, geography, and phylogeny, have been instrumental in broadening the scope of multilingual NLP [2]. However, there is still a noted underutilization of typological features in current databases, suggesting room for improvement and calling for better integration of data-driven typological knowledge into machine learning paradigms [2].

An investigation into the methods of incorporating typological data reveals that while neural models demonstrate potential in utilizing this information, the results have been inconsistent. This inconsistency underlines the importance of understanding how typology informs models and how to systematically integrate it into the parsing process [1] [5]. 

In summary, the performance of multilingual dependency parsing benefits significantly from incorporating typological information through coarsely grouping languages, consistent application of typological features, and enhancing overall model robustness in cross-lingual settings. The existing research highlights the promising avenues for maximizing the impact of typology in NLP applications while also acknowledging that challenges remain in fully leveraging its potential [3] [4] [5].

1. [1]:  https://ar5iv.org/html/1909.09279, [1909.09279] Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers
2. [2]:  https://ar5iv.org/html/2401.06034, [2401.06034] LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization
3. [3]:  https://ar5iv.org/html/1909.09279, [1909.09279] Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers
4. [4]:  https://ar5iv.org/html/1909.09279, [1909.09279] Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers
5. [5]:  https://ar5iv.org/html/1909.09279, [1909.09279] Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers
---
1. [1]:  Passage ID 1: (2018) and Scholivet et al. (2019) did in several cases.There are many possible hypotheses that can attempt to explain the state-of-the-art. Might neural models already implicitly learn typological information on their own? Is the hand-specified typology information sufficiently accurate — or provided in the right granularity — to always be useful? How do cross-lingual parsers use, or ignore, typology when making predictions? Without understanding answers to these questions, it is difficult to develop a principled way for robustly incorporating linguistic knowledge as an inductive bias for cross-lingual transfer.In this paper, we explore these questions in the context of two predominantly-used typology-based neural architectures for delexicalized dependency parsing.222We focus on delexicalized parsing in order to isolate the effects of syntax by removing lexical influences. The first method implements a variant of selective sharing Naseem et al. (2012); the second adds
2. [2]:  Passage ID 2: (2017) has been instrumental in extending the reach of multilingual NLP, particularly for less-resourced languages. These tools provide vector representations of languages, leveraging typological, geographical, and phylogenetic data, thus offering a structured approach to understanding linguistic diversity. Complementing this, recent research has conducted a comprehensive survey on the utilization of typological information in NLP, highlighting its potential in guiding the development of multilingual NLP technologies Ponti et al. (2019). This survey emphasized the underutilization of typological features in existing databases and the need for integrating data-driven induction of typological knowledge into machine learning algorithms.Recent advancements in prefix tuning Li and Liang (2021) and subspace learning Zhang et al. (2020) have contributed significantly to improving generalization in PLMs. These methods focus on learning prefix subspaces to stabilize the direct learning of
3. [3]:  Passage ID 3: The benefit of typological information is derived from coarsely grouping languages into syntactically-homogeneous clusters rather than from learning to leverage variations along individual typological dimensions in a compositional manner; 2) Typology consistent with the actual corpus statistics yields better transfer performance; 3) Typological similarity is only a rough proxy of cross-lingual transferability with respect to parsing.111Code: github.com/ajfisch/TypologyParser11footnotetext: The first two authors contributed equally.1 IntroductionOver the last decade, dependency parsers for resource-rich languages have steadily continued to improve. In parallel, significant research efforts have been dedicated towards advancing cross-lingual parsing. This direction seeks to capitalize on existing annotations in resource-rich languages by transferring them to the rest of the world’s over 7,000 languages Bender (2011). The NLP community has devoted substantial resources towards this
4. [4]:  Passage ID 4: on existing annotations in resource-rich languages by transferring them to the rest of the world’s over 7,000 languages Bender (2011). The NLP community has devoted substantial resources towards this goal, such as the creation of universal annotation schemas, and the expansion of existing treebanks to diverse language families. Nevertheless, cross-lingual transfer gains remain modest when put in perspective: the performance of transfer models can often be exceeded using only a handful of annotated sentences in the target language (Section 5). The considerable divergence of language structures proves challenging for current models.One promising direction for handling these divergences is linguistic typology.Linguistic typology classifies languages according to their structural and functional features. By explicitly highlighting specific similarities and differences in languages’ syntactic structures, typology holds great potential for facilitating cross-lingual transfer O’Horan
5. [5]:  Passage ID 5: "dark"); } else { localStorage.setItem("ar5iv_theme", "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Working Hard or Hardly Working:Challenges of Integrating Typology into Neural Dependency ParsersAdam Fisch∗  Jiang Guo∗  Regina Barzilay Computer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology {fisch,jiang_guo,regina}@csail.mit.eduAbstractThis paper explores the task of leveraging typology in the context of cross-lingual dependency parsing. While this linguistic information has shown great promise in pre-neural parsing, results for neural architectures have been mixed. The aim of our investigation is to better understand this state-of-the-art. Our main findings are as follows: 1) The benefit of typological information is derived from coarsely grouping languages into syntactically-homogeneous clusters rather than from learning to leverage variations along individual typological