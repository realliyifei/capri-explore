# Survey of Low-Resource Machine Translation

## Question

How has backtranslation been adapted and improved for low-resource neural machine translation?

## URLs

1. https://ar5iv.org/html/2404.08259. [2404.08259] Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024
2. https://ar5iv.org/html/2011.07403. [2011.07403] A Hybrid Approach for Improved Low Resource Neural Machine Translation using Monolingual Data
3. https://ar5iv.org/html/2412.00966. [2412.00966] From Priest to Doctor: Domain Adaptaion for Low-Resource Neural Machine Translation
4. https://ar5iv.org/html/1912.10514. [1912.10514] Tag-less Back-Translation
5. https://ar5iv.org/html/2411.11295. [2411.11295] Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation
6. https://ar5iv.org/html/2408.12079. [2408.12079] High-Quality Data Augmentation for Low-Resource NMT: Combining a Translation Memory, a GAN Generator, and Filtering
7. https://ar5iv.org/html/2409.15879. [2409.15879] Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning
8. https://ar5iv.org/html/2409.02712. [2409.02712] A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations
9. https://ar5iv.org/html/2105.15071. [2105.15071] Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data
10. https://ar5iv.org/html/2107.04239. [2107.04239] A Survey on Low-Resource Neural Machine Translation

## Answer (I don't know?: ✗)

Backtranslation has been significantly adapted and improved for low-resource neural machine translation (NMT) through various innovative techniques aimed at addressing the challenges posed by language scarcity and data quality. One prominent approach is the integration of backtranslation within the framework of NMT-Adapt, which leverages monolingual data to enhance translation quality in both directions—translating from English to a low-resource language and vice versa. This method combines several strategies, including denoising autoencoding and adversarial objectives, emphasizing the importance of utilizing both types of data effectively [1] [5].

NMT-Adapt employs a multi-task domain adaptation strategy that concurrently trains models on multiple relevant tasks. For instance, the technique involves refining backtranslation processes by linking them to domain adaptation strategies, allowing the model to iteratively improve translation quality through feedback from both the high-resource source domain and the low-resource target domain [3] [5]. The use of adversarial training within this setup also helps the model adapt the outputs generated during backtranslation to better fit the domain of the low-resource language, thus tackling issues related to data noisiness and structural differences between languages [5].

Moreover, the application of backtranslation is complemented by transfer learning, where knowledge gained from high-resource languages is used to create synthetic training data for the low-resource languages [2]. This not only increases the volume of training data available but also introduces linguistic structures that are essential for enhancing translation accuracy. Consequently, the combination of backtranslation with these other methods leads to a robust technique that can effectively handle data scarcity and improve performance [2] [4].

In summary, backtranslation has been enhanced for low-resource NMT through methods that include multi-task domain adaptation, adversarial training, and the use of synthetic data creation via transfer learning. These strategies collectively contribute to a more resilient translation model capable of producing better quality translations in scenarios where direct parallel data is limited [1] [3] [5].

1. [1]:  https://ar5iv.org/html/2105.15071, [2105.15071] Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data
2. [2]:  https://ar5iv.org/html/2404.08259, [2404.08259] Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024
3. [3]:  https://ar5iv.org/html/2105.15071, [2105.15071] Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data
4. [4]:  https://ar5iv.org/html/2011.07403, [2011.07403] A Hybrid Approach for Improved Low Resource Neural Machine Translation using Monolingual Data
5. [5]:  https://ar5iv.org/html/2105.15071, [2105.15071] Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data
---
1. [1]:  Passage ID 1: for training high-quality machine translation systems for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language. Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines.1 IntroductionWhile machine translation (MT) has made incredible strides due to the advent of deep neural machine translation (NMT) Sutskever et al. (2014); Bahdanau
2. [2]:  Passage ID 2: the help of growing Large Language Models, more and more low-resource languages achieve better results through the presence of other languages. However, studies have shown that not all low-resource languages can benefit from multilingual systems, especially those with insufficient training and evaluation data. In this paper, we revisit state-of-the-art Neural Machine Translation techniques to develop automatic translation systems between German and Bavarian. We investigate conditions of low-resource languages such as data scarcity and parameter sensitivity and focus on refined solutions that combat low-resource difficulties and creative solutions such as harnessing language similarity. Our experiment entails applying Back-translation and Transfer Learning to automatically generate more training data and achieve higher translation performance. We demonstrate noisiness in the data and present our approach to carry out text preprocessing extensively. Evaluation was conducted using
3. [3]:  Passage ID 3: how NMT-Adapt leverages a novel multi-task domain adaptation approach to translating English into a low-resource language. In Section 3.2, we then describe how we perform source-domain adaptation to translate a low-resource language into English. Finally, in Section 3.3, we demonstrate how we can leverage these two domain adaptations, to perform iterative backtranslation – further improving translation quality in both directions.Figure 1: Illustration of the training tasks for translating from English into a low-resource language (LRL) and from an LRL to English. 3.1 English to Low-resourceTo translate from English into a low-resource language, NMT-Adapt is initialized with a pretrained mBART model whose pretraining is described in Liu et al. (2020). Then, as shown in Figure 1, we continue to train the model simultaneously with four tasks inspired by Lample et al. (2018a) and update the model with a weighted sum of the gradients from different tasks.The language identifying
4. [4]:  Passage ID 4: Science > Computation and LanguagearXiv:2011.07403 (cs)  [Submitted on 14 Nov 2020 (v1), last revised 22 Nov 2021 (this version, v3)]Title:A Hybrid Approach for Improved Low Resource Neural Machine Translation using Monolingual DataAuthors:Idris Abdulmumin, Bashir Shehu Galadanci, Abubakar Isa, Habeebah Adamu Kakudi, Ismaila Idris Sinan View a PDF of the paper titled A Hybrid Approach for Improved Low Resource Neural Machine Translation using Monolingual Data, by Idris Abdulmumin and 4 other authorsView PDFAbstract:Many language pairs are low resource, meaning the amount and/or quality of available parallel data is not sufficient to train a neural machine translation (NMT) model which can reach an acceptable standard of accuracy. Many works have explored using the readily available monolingual data in either or both of the languages to improve the standard of translation models in low, and even high, resource languages. One of the most successful of such works is the
5. [5]:  Passage ID 5: two directions for each low-resource language: (1) low-resource to English and (2) English to low-resource. We claim that translating into English can be formulated as a typical unsupervised domain adaptation task, with the high-resource language as the source domain and the related low-resource, the target domain. We then show that adversarial domain adaptation can be applied to this related language translation task. For the second scenario, translating into the low-resource language, the task is more challenging as it involves unsupervised adaptation of the generated output to a new domain. To approach this task, NMT-Adapt jointly optimizes four tasks to perform low-resource translation: (1) denoising autoencoder (2) adversarial training (3) high-resource translation and (4) low-resource backtranslation.We test our proposed method and demonstrate its effectiveness in improving low-resource translation from three distinct families: (1) Iberian languages, (2) Indic languages, and