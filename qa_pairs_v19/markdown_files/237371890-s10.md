# Survey of Low-Resource Machine Translation

## Question

How has backtranslation been adapted and improved for low-resource neural machine translation?

## URLs

1. https://ar5iv.org/html/2404.08259. [2404.08259] Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024
2. https://ar5iv.org/html/2011.07403. [2011.07403] A Hybrid Approach for Improved Low Resource Neural Machine Translation using Monolingual Data
3. https://ar5iv.org/html/2412.00966. [2412.00966] From Priest to Doctor: Domain Adaptaion for Low-Resource Neural Machine Translation
4. https://ar5iv.org/html/1912.10514. [1912.10514] Tag-less Back-Translation
5. https://ar5iv.org/html/2411.11295. [2411.11295] Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation
6. https://ar5iv.org/html/2408.12079. [2408.12079] High-Quality Data Augmentation for Low-Resource NMT: Combining a Translation Memory, a GAN Generator, and Filtering
7. https://ar5iv.org/html/2409.15879. [2409.15879] Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning
8. https://ar5iv.org/html/2409.02712. [2409.02712] A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations
9. https://ar5iv.org/html/2105.15071. [2105.15071] Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data
10. https://ar5iv.org/html/2107.04239. [2107.04239] A Survey on Low-Resource Neural Machine Translation

## Answer (I don't know?: ✗)

Back-translation has been adapted and improved for low-resource neural machine translation (NMT) through a variety of innovative techniques aimed at increasing translation quality while addressing the data scarcity that characterizes these languages.

One significant method is the integration of back-translation with denoising autoencoding and adversarial objectives, as exemplified in the NMT-Adapt approach. This technique leverages monolingual data from low-resource languages and combines it with any available parallel data from linguistically related high-resource languages. By exploiting linguistic similarities, this method facilitates better translation performance into low-resource languages, demonstrating significant improvements compared to other baseline translation models [2]. 

Moreover, recent studies indicate that large language models (LLMs) have also played a crucial role in enhancing translation outcomes for low-resource languages. However, it has been observed that not all low-resource languages benefit equally from multilingual systems, especially those lacking adequate training and evaluation data. Thus, targeted adaptations of back-translation, alongside transfer learning, are implemented to automatically generate supplemental training data. This generates richer datasets for low-resource languages, thereby improving translation quality [3]. 

Additionally, the back-translation process itself has evolved in response to the specific challenges posed by low-resource settings. Researchers have noted that back-translation can help create high-quality synthetic parallel datasets even when original parallel corpora are scarce [1]. In practice, this can involve using an initial model trained on related high-resource languages to perform translations, which are then reversed through back-translation, effectively doubling the utility of the available data by yielding additional training instances [2]. 

Furthermore, researchers have also refined preprocessing methods to clean noisy data that typically accompany low-resource training datasets. This preprocessing ensures that the input data fed into the models is of high quality, thereby further enhancing translation performance through back-translation methods [3]. 

In conclusion, the adaptation and improvement of back-translation for low-resource NMT primarily hinge on the combination of leveraging related languages, the integration of advanced techniques such as adversarial training and denoising autoencoding, and the systematic addressing of data issues through techniques like transfer learning and intensive preprocessing. These innovations collectively aim to mitigate the impact of data scarcity, leading to enhanced translation quality for low-resource languages.

1. [1]:  https://ar5iv.org/html/2407.08819, No Title
2. [2]:  https://ar5iv.org/html/2105.15071, [2105.15071] Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data
3. [3]:  https://ar5iv.org/html/2404.08259, [2404.08259] Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024
4. [4]:  https://ar5iv.org/html/2411.11295, [2411.11295] Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation
5. [5]:  https://ar5iv.org/html/2411.11295, [2411.11295] Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation
---
1. [1]:  Passage ID 1: available authentic data, we synthesise further translations by using three different models: a fine-tuned neural model, a rule-based system developed specifically for this language pair, and a large language model. Our experiments show that all approaches achieve comparable translation quality in this low-resource scenario, yet round-trip translations highlight differences in model performance.Rule-Based, Neural and LLM Back-Translation:Comparative Insights from a Variant of LadinSamuel Frontull  and Georg MoserDepartment of Computer ScienceUniversity of Innsbruck, Innsbruck, Austria{samuel.frontull, georg.moser}@uibk.ac.at1 IntroductionIn recent years, a variety of methods have been developed to apply neural machine translation (NMT) also in low-resource scenarios Shi et al. (2022); Haddow et al. (2022); Ranathunga et al. (2023).The back-translation technique has shown to be particularly effective in such settings Sennrich et al. (2016); Edunov et al.
2. [2]:  Passage ID 2: for training high-quality machine translation systems for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language. Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines.1 IntroductionWhile machine translation (MT) has made incredible strides due to the advent of deep neural machine translation (NMT) Sutskever et al. (2014); Bahdanau
3. [3]:  Passage ID 3: the help of growing Large Language Models, more and more low-resource languages achieve better results through the presence of other languages. However, studies have shown that not all low-resource languages can benefit from multilingual systems, especially those with insufficient training and evaluation data. In this paper, we revisit state-of-the-art Neural Machine Translation techniques to develop automatic translation systems between German and Bavarian. We investigate conditions of low-resource languages such as data scarcity and parameter sensitivity and focus on refined solutions that combat low-resource difficulties and creative solutions such as harnessing language similarity. Our experiment entails applying Back-translation and Transfer Learning to automatically generate more training data and achieve higher translation performance. We demonstrate noisiness in the data and present our approach to carry out text preprocessing extensively. Evaluation was conducted using
4. [4]:  Passage ID 4: understanding and reasoning abilities pave the way toward Artificial General Intelligence (AGI) and can facilitate societal development across a wide range of domains [83, 44, 84, 82, 35, 32].2.2 Machine Translation on Low-Resource LanguageMachine Translation for low-resource languages has been a long-standing challenge in the field of NLP. While machine translation for high-resource languages, such as English, Chinese, or Spanish, has seen considerable improvements, particularly with the advent of NMT techniques, low-resource languages have lagged due to the scarcity of large parallel corpora and linguistic resources.Early efforts in machine translation, particularly for low-resource languages, were based on rule-based and SMT approaches. Rule-based systems relied on linguistic rules crafted by experts and extensive lexicons, but they were often labor-intensive and brittle when applied to complex languages. SMT, introduced in the 1990s, offered a data-driven approach, where
5. [5]:  Passage ID 5: understanding and reasoning abilities pave the way toward Artificial General Intelligence (AGI) and can facilitate societal development across a wide range of domains [83, 44, 84, 82, 35, 32].2.2 Machine Translation on Low-Resource LanguageMachine Translation for low-resource languages has been a long-standing challenge in the field of NLP. While machine translation for high-resource languages, such as English, Chinese, or Spanish, has seen considerable improvements, particularly with the advent of NMT techniques, low-resource languages have lagged due to the scarcity of large parallel corpora and linguistic resources.Early efforts in machine translation, particularly for low-resource languages, were based on rule-based and SMT approaches. Rule-based systems relied on linguistic rules crafted by experts and extensive lexicons, but they were often labor-intensive and brittle when applied to complex languages. SMT, introduced in the 1990s, offered a data-driven approach, where