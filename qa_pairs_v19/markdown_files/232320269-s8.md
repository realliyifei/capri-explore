# A Survey on Predicting the Factuality and the Bias of News Media

## Question

How are linguistic features categorized and applied in the NELA toolkit for analyzing news media?

## URLs

1. https://ar5iv.org/html/2101.10973. [2101.10973] Tell Me Who Your Friends Are: Using Content Sharing Behavior for News Source Veracity Detection
2. https://ar5iv.org/html/2408.00156. [2408.00156] Measuring Falseness in News Articles based on Concealment and Overstatement
3. https://ar5iv.org/html/2407.02122. [2407.02122] Fake News Detection: It’s All in the Data!
4. https://ar5iv.org/html/2309.08674. [2309.08674] Fake News Detectors are Biased against Texts Generated by Large Language Models
5. https://ar5iv.org/html/2005.04518. [2005.04518] What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context
6. https://ar5iv.org/html/2403.09858. [2403.09858] FakeWatch \faEye: A Framework for Detecting Fake News to Ensure Credible Elections
7. https://ar5iv.org/html/2103.12506. [2103.12506] A Survey on Predicting the Factuality and the Bias of News Media
8. https://ar5iv.org/html/2306.05524. [2306.05524] Check Me If You Can: Detecting ChatGPT-Generated Academic Writing using CheckGPT
9. https://ar5iv.org/html/2006.01211. [2006.01211] Do All Good Actors Look The Same? Exploring News Veracity Detection Across The U.S. and The U.K.
10. https://ar5iv.org/html/2108.03731. [2108.03731] Leveraging Commonsense Knowledge on Classifying False News and Determining Checkworthiness of Claims

## Answer (I don't know?: ✗)

The NELA toolkit employs a comprehensive approach to analyzing news media by categorizing linguistic features into several distinct groups that help model various aspects of news content. The features are designed to detect nuances such as bias, complexity, sentiment, and overall style, which are critical for assessing the credibility and factuality of news articles.

In detail, the NELA toolkit categorizes its features into five main categories: 

1. **Style**: This includes part-of-speech tags and the use of specific words, including function words and pronouns. Features for clickbait title classification also fall under this category. The style metrics assist in understanding the linguistic choices made by authors and how these may influence reader perception or emotional response [2].

2. **Complexity**: Features here encompass the type-token ratio, readability scores, and counts of cognitive process words (such as those indicating insight or certainty). These metrics are crucial for gauging how accessible or challenging a news article may be for its intended audience, which can impact its persuasive effectiveness [2].

3. **Bias**: This category includes features that model bias using established lexicons and classifiers for subjectivity. By examining how language reflects political or ideological leanings, researchers can ascertain potential biases in the reporting of different news media, which is fundamental for understanding the media landscape [2][3].

4. **Affect**: In this context, affective features are derived from sentiment scores obtained through specialized lexicons. These scores evaluate the emotional tone of the news articles, thereby providing insight into the emotional framing of news content [2][3].

5. **Moral**: This category evaluates language concerning moral judgments and orientations, adding another layer of analysis to understand content framing based on moral perspectives [3].

To operationalize these features, the NELA toolkit extracts and analyzes them across various news articles, creating a synthesized feature representation that can characterize the overall content of a news medium. For example, by averaging the NELA features for multiple articles, the toolkit can provide a general representation of a news outlet’s linguistic profile, thus mitigating the impact of outlier articles, such as those that might deviate from a medium’s usual ideological alignment [1][4].

Moreover, research has shown that the features encapsulated in the NELA toolkit were significant in discerning differences among varieties of news content, such as human-written fake news versus news generated by language models (LMs). These comparisons highlighted that divergence in NELA features is more pronounced when contrasting LLM-generated fake news with human-written real news, indicating the toolkit's effectiveness in classifying and assessing news veracity [5].

In summary, the NELA toolkit categorizes linguistic features into style, complexity, bias, affect, and moral dimensions, using these to analyze and predict the political bias and factual accuracy of news media. These categories allow for a nuanced understanding of the nature of news reporting and help in developing sophisticated tools for news veracity detection.

1. [1]:  https://ar5iv.org/html/2005.04518, [2005.04518] What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context
2. [2]:  https://ar5iv.org/html/2103.12506, [2103.12506] A Survey on Predicting the Factuality and the Bias of News Media
3. [3]:  https://ar5iv.org/html/2006.01211, [2006.01211] Do All Good Actors Look The Same? Exploring News Veracity Detection Across The U.S. and The U.K.
4. [4]:  https://ar5iv.org/html/2005.04518, [2005.04518] What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context
5. [5]:  https://ar5iv.org/html/2309.08674, [2309.08674] Fake News Detectors are Biased against Texts Generated by Large Language Models
---
1. [1]:  Passage ID 1: the political bias and the factuality of reporting of news media.Figure 1 illustrates the system architecture.3.1 What Was WrittenWe describe the features that we used to model the content generated by the news media, analyzing both the articles they publish on their website as well as relevant activity on social media.3.1.1 Articles on the News Medium WebsiteGiven a target news medium, we first collect a number of articles it has published. Then, we extract various types of features from the text of these articles. Below we describe these features in more detail.Linguistic Features:These features focus on language use, and they model text structure, topic, sentiment, subjectivity, complexity, bias, and morality.They have proved useful for detecting fake articles, as well as for predicting the political bias and the factuality of reporting of news media Horne et al. (2018b); Baly et al. (2018a).We extracted such features using the News Landscape (NELA) toolkit Horne
2. [2]:  Passage ID 2: vs. hoaxes vs. satire vs. propaganda Pennebaker etal. (2001). They can be also modeled using linguistic markers Mihaylova etal. (2018) such as factives from Hooper (1975), assertives from Hooper (1975), implicatives from Karttunen (1971), hedges from Hyland (2005), Wiki-bias terms from Recasens etal. (2013), subjectivity cues from Riloff andWiebe (2003), andsentiment cues from Liu etal. (2005); see Table 1 for examples.There are 141 such features implemented in the NELA toolkit Horne etal. (2018), grouped in the following categories:•Style: part-of-speech tags, use of specific words (function words, pronouns, etc.), and features for clickbait title classification;•Complexity: type-token ratio, readability, number of cognitive process words (identifying discrepancy, insight, certainty, etc.);•Bias: features modeling bias using lexiconsand subjectivity as calculated using pre-trained classifiers;•Affect: sentiment scores from lexiconsand full
3. [3]:  Passage ID 3: we use two types of feature sets, one hand-crafted feature set commonly used in news veracity studies and one automatic feature extraction method. Specifically, we use:•NELA - NELA is a set of hand-crafted features that can be categorized into five categories: style, complexity, bias, affect, and moral. In total the feature set contains 194 features. The feature set was introduced in (?) and has been used in multiple news veracity studies since (?; ?).•Doc2Vec - Another way to capture differences in text is to use an automatic feature extraction technique. In this case we use a paragraph embedding model, Doc2Vec (?) with 100 dimension representation.We also test the scaled and normalized versions of these feature sets, called NELA-scaled and Doc2Vec-scaled.5 Model TestingWith these feature sets, we train and test classification models using Random Forest and three different testing schemes:•Article Split - In this setting we remove 20% of the articles from
4. [4]:  Passage ID 4: as for predicting the political bias and the factuality of reporting of news media Horne et al. (2018b); Baly et al. (2018a).We extracted such features using the News Landscape (NELA) toolkit Horne et al. (2018b), and we will refer to them as the NELA features in the rest of this paper.We averaged the NELA features for the individual articles in order to obtain a NELA representation for a news medium.Using arithmetic averaging is a good idea as it captures the general trend of articles in a medium, while limiting the impact of outliers.For instance, if a medium is known to align with left-wing ideology, this should not change if it published a few articles that align with right-wing ideology.We use this method to aggregate all features that we collected at a level of granularity that is finer than the medium-level.Embedding Features:We encoded each article using BERT Devlin et al. (2019) by feeding the first 510 WordPieces222There is a limit of maximum of 512 input tokens, and
5. [5]:  Passage ID 5: prior work on news veracity detection Horne and Adali (2017), we computed News Landscape (NELA) features. These features, derived from the NELA toolkit, encapsulate six dimensions of news content: style, complexity, bias, affect, morale, and event. We applied these features to both GossipCop++ and PolitiFact++. Employing Tukey’s pairwise test Tukey (1949), we discerned significant feature disparities among human-written fake news, LLM-generated fake news, and human-written real news.Our analysis, as presented in Table 7, reveals that most of the NELA features differ significantly between human-written and LLM-generated fake news. Moreover, the divergence between LLM-generated fake news and human-written real news is more pronounced than between human-written fake and real news. This underscores the relative ease of detecting LLM-generated fake news, shedding light on the bias observed in RQ1. The NELA features for PolitiFact++ are detailed in Appendix 7.To further understand the