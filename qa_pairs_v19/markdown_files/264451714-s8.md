# Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models

## Question

What strategies are used to address the challenges of handling multi-token entities in pre-trained language models?

## URLs

1. https://ar5iv.org/html/2407.00731. [2407.00731] Large Language Models Struggle in Token-Level Clinical Named Entity Recognition
2. https://ar5iv.org/html/2405.17067. [2405.17067] Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization
3. https://ar5iv.org/html/2402.04437. [2402.04437] Structured Entity Extraction Using Large Language Models
4. https://ar5iv.org/html/2403.12503. [2403.12503] Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices
5. https://ar5iv.org/html/2411.09539. [2411.09539] A Practical Guide to Fine-tuning Language Models with Limited Data
6. https://ar5iv.org/html/2408.13296. [2408.13296] The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities (Version 1.0)
7. https://ar5iv.org/html/2406.18085. [2406.18085] Multilingual Knowledge Graph Completion from Pretrained Language Models with Knowledge Constraints
8. https://ar5iv.org/html/2410.15319. [2410.15319] Causality for Large Language Models
9. https://ar5iv.org/html/2401.10825. [2401.10825] A survey on recent advances in Named Entity Recognition
10. https://ar5iv.org/html/2306.17733. [2306.17733] Token-Event-Role Structure-based Multi-Channel Document-Level Event Extraction

## Answer (I don't know?: ✓)

I don't know. The context provided does not contain sufficient information regarding specific strategies used to address the challenges of handling multi-token entities in pre-trained language models. The passages discuss various aspects of NLP, tokenization issues, and the importance of domain-specific data, but they do not detail any specific strategies for handling multi-token entities. Thus, I cannot provide an accurate answer to your question based on the given context.

1. [1]:  https://ar5iv.org/html/2407.00731, [2407.00731] Large Language Models Struggle in Token-Level Clinical Named Entity Recognition
2. [2]:  https://ar5iv.org/html/2405.17067, [2405.17067] Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization
3. [3]:  https://ar5iv.org/html/2411.05503, No Title
4. [4]:  https://ar5iv.org/html/2209.12617, No Title
5. [5]:  https://ar5iv.org/html/2411.09539, [2411.09539] A Practical Guide to Fine-tuning Language Models with Limited Data
---
1. [1]:  Passage ID 1: manually-crated rules and traditional machine learning techniques, such as MetaMap 3, KnowledgeMap 4, cTAKES 5, etc. With the trending of deep learning methods and the Transformer architecture 6, more researchers shift to building clinical NER systems and other NLP applications upon pre-trained language models such as BERT 7. A typical solution is to insert a multilayer perception (MLP) on top of the language model and train the entire model via fine-tuning. For example, Alsentzer et al. fine-tune BioClinicalBERT on four i2b2 NER tasks 8. Similarly, Sung et al. present BERN2, which uses Bio-LM 9 as the foundation model and achieves better performance via multitask learning 10.A common theme among the aforementioned deep-learning-based methods is their data-hungry nature, which unfortunately poses significant challenges in the medical field, where issues of data scarcity, heterogeneity, and confidentiality are consistently prevalent. The situation is even worse for rare diseases due
2. [2]:  Passage ID 2: Schuster_Nakajima_2012 , and Unigram kudo2018subword . However, no vocabulary can perfectly cover all possible ways of various expressions in the inputs. When applying the greedy principle, the algorithms are easy to generate unsatisfactory results which are not fully aligned with the correct intention of users’ input. Unfortunately, in cases of tokenization errors, all subsequent optimization operations for LLMs cannot completely solve this underlying problems caused by their tokenization algorithms.In the domain of natural language processing (NLP), the existing studies related to tokenization primarily focus on refining or optimizing various tokenization algorithms. Meanwhile, the discussions on LLMs’ vulnerability including attack or challenge techniques, have been more concerned with the security of LLMs. About the challenges posed by tokenization deficiencies, only Sander and Max land2024fishing have discussed this issue from the perspective of under-trained tokens in LLMs. It
3. [3]:  Passage ID 3: general NLP models in domain-specific tasks like named entity recognition, relation extraction, and others [56, 76]. This demonstrated the importance of specialized data and triggered a trend in fine-tuning models on annotated, domain-specific datasets.A more recent study [77] showed that training BERT [16] on the British National Corpus [12] (i.e., a carefully curated yet much smaller text collection than that used to train the original model) achieved even better performance than the original BERT model.Thus, even in the age of powerful neural models, the quality and specificity of data remain critical factors in achieving high-performance NLP systems. Moreover, without datasets for training and validation, the field of Kyrgyz NLP simply cannot advance.2.2 Processing Methods for Less-Resourced LanguagesAddressing the challenges faced by LRLs requires innovative approaches that compensate for the lack of resources. Several common methods have been employed to process LRLs
4. [4]:  Passage ID 4: Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence (AI) concerned with text understanding and text generation. The former subject is studied in the sub-branch natural language understanding (NLU) 1; 2; 3 and the latter in natural language generation (NLG) 4; 5. Over the years, both fields, i.e., NLU and NLG developed enormously with an extensive literature which requires nowadays a dedicated discussion of specialized subtasks when presenting approaches or methods thereof despite the fact that a systems understanding of NLP can only be achieved holistically.In this paper, we focus on subtasks of NLP centered around question answering (QA). The task of a QA system is to find an answer (output) in the form of a natural language for a given question (input) usually presented in form of a sentence. While our focus is on QA systems and their
5. [5]:  Passage ID 5: Multitask, LR warm-up15K+LoRASemi-supervised learningTable 2: Suggested approaches for NLP task groups with limited data. All information in this table was compiled from reviewed papers.Abbreviations not defined within the text: ER (Experience Replay), LLRD (Layer-wise Learning Rate Decay), LR (Learning Rate), NLI (Natural Language Inference), RAG (Retrieval-Augmented Generation).*Labeled data for Sequence Labeling is provided in sentences; for other tasks, in examples.Model selection.Choosing the right pre-trained model is crucial for achieving optimal performance in targeted tasks, domains, and languages.The most important factors include the model architecture, the number of parameters, and the pre-training data size, type, and quality (Alabi et al., 2020).Large generative models can be applied to all NLP tasks with appropriate task formulation (Liu et al., 2023c).However, for NLU tasks, bidirectional encoder models with only a few million parameters