# Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models

## Question

What strategies are used to address the challenges of handling multi-token entities in pre-trained language models?

## URLs

1. https://ar5iv.org/html/2407.00731
2. https://ar5iv.org/html/2405.17067
3. https://ar5iv.org/html/2402.04437
4. https://ar5iv.org/html/2403.12503
5. https://ar5iv.org/html/2411.09539
6. https://ar5iv.org/html/2408.13296
7. https://ar5iv.org/html/2406.18085
8. https://ar5iv.org/html/2410.15319
9. https://ar5iv.org/html/2401.10825
10. https://ar5iv.org/html/2306.17733

## Answer

## I don't know?: True

The context provided discusses various aspects of Natural Language Processing (NLP) but does not specifically mention strategies employed to handle multi-token entities in pre-trained language models. Therefore, I cannot accurately answer the question about strategies for addressing the challenges of handling multi-token entities based on the available information.

Although the text refers to the fact that multi-token sequences are relevant in NLP applications—specifically stating that "real-world entities and objects are often represented by multi-token sub-sequences (i.e. names) where a span of one or more tokens represents a single entity" [5]—it does not elucidate specific strategies or methods that pre-trained language models employ to manage these multi-token entities.

The context focuses more on challenges related to negation in NLP [2][4], the complexities of embedding model sizes [3], the generation of queries in simplifying layman's terms [1], and the overall functioning of various NLP models without delving into the management of multi-token entities’ intricacies. As such, the information provided does not include sufficient details regarding the precise strategies used to address this issue, leading to the conclusion that I don't know how pre-trained language models specifically handle multi-token entities.

1. [1]:  https://ar5iv.org/html/2410.00427, No Title
2. [2]:  https://ar5iv.org/html/2302.02291, No Title
3. [3]:  https://ar5iv.org/html/2411.05036, No Title
4. [4]:  https://ar5iv.org/html/2302.02291, No Title
5. [5]:  https://ar5iv.org/html/2403.15453, No Title
---
1. [1]:  Passage ID 1: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
2. [2]:  Passage ID 2: The trick here is also that tokens are read at the same time and not in order (either left-to-right or right-to-left). This allows BERT to take the context of each word into account. Similar in its construction, XLNet (Topal et al., 2021) improved its masking mechanism with peculiar assumptions during its pre-training stage, and improved over the work done by BERT. Despite these advances, studies (Ettinger, 2020) have shown that even these approaches have still struggled with negation. Which is why this study aims to examine the problem in a holistic manner.3. MethodologyThis paper addressed two research problems by utilizing Natural Language Processing (NLP) techniques, a lexicon-based approach, and sequence labeling. To guide the design of the aforementioned automated tool, we answered the following two questions:•RQ1: How do we detect negation in a given text?•RQ2: How can a automated system apply negation to appropriate words to improve sentiment
3. [3]:  Passage ID 3: research in this area. Addressing these challenges may lead to more cognitively plausible language models and deeper insights into the neural basis of language.V Advanced Topics and Research GapsV-A Embedding Model Compression: Techniques for Reducing Model Size and Memory FootprintV-A1 Need for Compression: Computational and Storage Challenges of Large Embedding MatricesThe impressive performance of large language models (LLMs) in NLP applications often comes with substantial computational and storage requirements, particularly due to the embedding layers that represent vocabulary items as dense vectors. As vocabularies expand to include hundreds of thousands of tokens and embedding dimensions increase to capture nuanced semantic relationships, the memory footprint can become prohibitive, especially in resource-constrained environments like mobile devices. This scalability challenge is further compounded by the increasing demand for multilingual and multimodal models,
4. [4]:  Passage ID 4: 2019). The applicability of NLP comes in a variety of forms. In our everyday discourse, such as social media conversations, negation signals are employed. In practice, some NLP techniques used in this space are Word2Vec and GloVe (Yu et al., 2017). BERT and XLNet deliver state-of-the-art outcomes as well (Devlin et al., 2018). However, each model has its own set of constraints. Most algorithms underperform when confronted with negation, which is why it is critical to identify novel approaches to overcome this issue in the NLP domain.2.5. Word Negation and Sequence LabelingThe accuracy of NLP models such as sentiment or perception analysis depends on word negation and sequence labeling. Some NLP models typically work by analyzing each word, or sequence of words, independently. These algorithms deconstruct text into its minimum units called tokens (Webster and Kit, 1992). Once these tokens have been cleaned up, the algorithms read each word or tokenized entity independently and
5. [5]:  Passage ID 5: at their limit, encompass complex narratives, emotions, and literature. Without extra effort, these sequences-of-bytes cannot be effectively used to extract insights or create meaning. Indeed, most natural language processing (NLP) systems, including large language models (LLMs), do not take as input data in its native sequence-of-bytes format. Instead, raw characters are first broken up into individual tokens, which are typically individual words, but can also be sub-word chunks, that are gleaned from the sequence-of-bytes. These tokens are what is actually used to train NLP systems, So when an NLP system translates text or answers a question, it is actually generating sequences-of-tokens, not individual characters or bytes.Contrary to the token-by-token view used by most NLP systems, real-world entities and objects are often represented by multi-token sub-sequences (i.e. names) where a span of one or more tokens represents a single entity. The NLP task that aims to identify