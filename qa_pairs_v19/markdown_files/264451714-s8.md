# Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models

## Question

What strategies are used to address the challenges of handling multi-token entities in pre-trained language models?

## URLs

1. https://ar5iv.org/html/2407.00731. [2407.00731] Large Language Models Struggle in Token-Level Clinical Named Entity Recognition
2. https://ar5iv.org/html/2405.17067. [2405.17067] Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization
3. https://ar5iv.org/html/2402.04437. [2402.04437] Structured Entity Extraction Using Large Language Models
4. https://ar5iv.org/html/2403.12503. [2403.12503] Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices
5. https://ar5iv.org/html/2411.09539. [2411.09539] A Practical Guide to Fine-tuning Language Models with Limited Data
6. https://ar5iv.org/html/2408.13296. [2408.13296] The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities (Version 1.0)
7. https://ar5iv.org/html/2406.18085. [2406.18085] Multilingual Knowledge Graph Completion from Pretrained Language Models with Knowledge Constraints
8. https://ar5iv.org/html/2410.15319. [2410.15319] Causality for Large Language Models
9. https://ar5iv.org/html/2401.10825. [2401.10825] A survey on recent advances in Named Entity Recognition
10. https://ar5iv.org/html/2306.17733. [2306.17733] Token-Event-Role Structure-based Multi-Channel Document-Level Event Extraction

## Answer (I don't know?: ✗)

The handling of multi-token entities in pre-trained language models (LLMs) poses several challenges, and various strategies have emerged to address these issues, particularly in the context of clinical natural language processing (NLP). Here are some key approaches:

1. **Token-Level Named Entity Recognition (NER)**: The distinction between token-level and document-level NER is vital. Token-level NER requires precise representation of each individual token, contrasting with document-level NER where pooled representations suffice. This complexity necessitates refined token-level approaches to effectively identify and classify multi-token entities [1].

2. **Fine-Tuning and Task-Specific Training**: One common strategy involves fine-tuning existing LLMs like BERT and BioClinicalBERT on specific tasks, including clinical NER. For instance, researchers often insert a multilayer perceptron (MLP) on top of the pre-trained models and train the entire setup through fine-tuning to adapt the LLM for the nuances of the target task, thereby enhancing its capability to recognize multi-token entities [2]. Additionally, some studies utilize synthetic data for instruction-fine-tuning, which can improve performance across various tasks [4].

3. **Synthetic Data Generation**: By leveraging generative models like ChatGPT, researchers create synthetic data that mimics real-world scenarios. This data can be essential for training models to recognize and categorize multi-token entities, especially in domains where labeled data is scarce due to confidentiality and heterogeneity concerns [2][4].

4. **Multitask Learning**: Implementing multitask learning can enhance the efficiency and effectiveness of models in recognizing multi-token entities. By training on multiple related tasks simultaneously, LLMs can learn generic features that improve their performance in identifying diverse entity types across different contexts [2].

5. **Prompting Techniques**: Using prompting strategies tailored for token-level NER can also assist in dealing with multi-token entities. By providing specific instructions to models like ChatGPT, researchers can coax them into better recognizing and outputting multi-token entities based on crafted prompts [4].

6. **Addressing Tokenization Errors**: Some challenges stem from inherent tokenization errors in LLMs, which can lead to difficulty in accurately representing multi-token entities. Researchers are investigating solutions to improve tokenization, though this remains a less-explored area within the domain of LLM vulnerabilities. Improving tokenization may ultimately reduce errors in entity identification [3].

7. **Incorporating Local Context**: Another strategy is to explore the local context of tokens to enhance the understanding of multi-token entities. Studies have shown the importance of local contextual information in improving token-level NER performance, although this is still under investigation and requires further exploration with open-source local models [4].

These strategies collectively form a comprehensive approach to improving the handling of multi-token entities in LLMs, especially within specialized fields like clinical NLP. As LLMs continue to evolve, ongoing research into their adaptation for specific tasks will likely yield more refined techniques and enhanced performance in entity recognition tasks.

1. [1]:  https://ar5iv.org/html/2407.00731, [2407.00731] Large Language Models Struggle in Token-Level Clinical Named Entity Recognition
2. [2]:  https://ar5iv.org/html/2407.00731, [2407.00731] Large Language Models Struggle in Token-Level Clinical Named Entity Recognition
3. [3]:  https://ar5iv.org/html/2405.17067, [2405.17067] Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization
4. [4]:  https://ar5iv.org/html/2407.00731, [2407.00731] Large Language Models Struggle in Token-Level Clinical Named Entity Recognition
5. [5]:  https://ar5iv.org/html/2411.09539, [2411.09539] A Practical Guide to Fine-tuning Language Models with Limited Data
---
1. [1]:  Passage ID 1: on adapting LLMs to specific NLP applications in clinical settings.There are several factors that contribute to the struggle of LLMs on this task. Firstly, token-level NER is more challenging than document-level NER. Token-level NER is a token classification task and places significant emphasis on the representation of each individual token. This is in contrast to document-level NER which is a sequence classification task where a pooled representation is often sufficient for predictions, making the role of individual token representation less critical. Secondly, the foundational pre-training objective of most LLMs (decoder-only transformers) is causal language modeling, i.e., next token prediction. Unlike encoder-only transformers like BERT, LLMs don’t have an encoder structure that is pre-trained to maximize the model’s text representation power, making them less effective in classification tasks. Thirdly, token-level NER often involves a variety of special, previously unseen
2. [2]:  Passage ID 2: manually-crated rules and traditional machine learning techniques, such as MetaMap 3, KnowledgeMap 4, cTAKES 5, etc. With the trending of deep learning methods and the Transformer architecture 6, more researchers shift to building clinical NER systems and other NLP applications upon pre-trained language models such as BERT 7. A typical solution is to insert a multilayer perception (MLP) on top of the language model and train the entire model via fine-tuning. For example, Alsentzer et al. fine-tune BioClinicalBERT on four i2b2 NER tasks 8. Similarly, Sung et al. present BERN2, which uses Bio-LM 9 as the foundation model and achieves better performance via multitask learning 10.A common theme among the aforementioned deep-learning-based methods is their data-hungry nature, which unfortunately poses significant challenges in the medical field, where issues of data scarcity, heterogeneity, and confidentiality are consistently prevalent. The situation is even worse for rare diseases due
3. [3]:  Passage ID 3: Schuster_Nakajima_2012 , and Unigram kudo2018subword . However, no vocabulary can perfectly cover all possible ways of various expressions in the inputs. When applying the greedy principle, the algorithms are easy to generate unsatisfactory results which are not fully aligned with the correct intention of users’ input. Unfortunately, in cases of tokenization errors, all subsequent optimization operations for LLMs cannot completely solve this underlying problems caused by their tokenization algorithms.In the domain of natural language processing (NLP), the existing studies related to tokenization primarily focus on refining or optimizing various tokenization algorithms. Meanwhile, the discussions on LLMs’ vulnerability including attack or challenge techniques, have been more concerned with the security of LLMs. About the challenges posed by tokenization deficiencies, only Sander and Max land2024fishing have discussed this issue from the perspective of under-trained tokens in LLMs. It
4. [4]:  Passage ID 4: 23 and further enhanced with training on PubMed Central, showing superior performance on various medical question-answering tasks.In parallel, considerable efforts have been devoted to adapting these LLMs to the task of NER through prompting or fine-tuning. For example, Zhou et al. instruction-fine-tune the LLaMA model 15 using ChatGPT-generated synthetic data for NER from broad-coverage unlabeled web text 16. Their UniversalNER model shows promising NER performance across multiple domains. However, they only generate document-level outputs, i.e., a list of extracted entities in the given text as shown in Figure 2(a), without considering their exact span information, which limits their impact and usage in practical scenarios. Hu et al. explore the token-level NER capabilities of ChatGPT-3.5 and ChatGPT-4 on two clinical NER tasks by manually crafting specific prompts 24. However, they limit their exploration to ChatGPT models and do not investigate local and open-source LLMs.
5. [5]:  Passage ID 5: Multitask, LR warm-up15K+LoRASemi-supervised learningTable 2: Suggested approaches for NLP task groups with limited data. All information in this table was compiled from reviewed papers.Abbreviations not defined within the text: ER (Experience Replay), LLRD (Layer-wise Learning Rate Decay), LR (Learning Rate), NLI (Natural Language Inference), RAG (Retrieval-Augmented Generation).*Labeled data for Sequence Labeling is provided in sentences; for other tasks, in examples.Model selection.Choosing the right pre-trained model is crucial for achieving optimal performance in targeted tasks, domains, and languages.The most important factors include the model architecture, the number of parameters, and the pre-training data size, type, and quality (Alabi et al., 2020).Large generative models can be applied to all NLP tasks with appropriate task formulation (Liu et al., 2023c).However, for NLU tasks, bidirectional encoder models with only a few million parameters