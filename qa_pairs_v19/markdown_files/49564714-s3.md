# Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing

## Question

What methods have been developed for language transfer, particularly in the context of dependency parsing?

## URLs

1. https://ar5iv.org/html/1902.09492. [1902.09492] Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing
2. https://ar5iv.org/html/2012.13163. [2012.13163] Cross-lingual Universal Dependency Parsing Only from One Monolingual Treebank
3. https://ar5iv.org/html/2403.20056. [2403.20056] Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets
4. https://ar5iv.org/html/2301.13720. [2301.13720] Zero-shot cross-lingual transfer language selection using linguistic similarity
5. https://ar5iv.org/html/2101.11216. [2101.11216] PPT: Parsimonious Parser Transfer for Unsupervised Cross-Lingual Adaptation
6. https://ar5iv.org/html/2401.12072. [2401.12072] Cross-lingual Transfer Learning for Javanese Dependency Parsing
7. https://ar5iv.org/html/2412.01109. [2412.01109] Revisiting Absence withSymptoms that *T* Show up Decades Later to Recover Empty Categories
8. https://ar5iv.org/html/1811.00570. [1811.00570] On Difficulties of Cross-Lingual Transfer with Order Differences: A Case Study on Dependency Parsing
9. https://ar5iv.org/html/1910.05479. [1910.05479] Zero-shot Dependency Parsing with Pre-trained Multilingual Sentence Representations
10. https://ar5iv.org/html/1909.09265. [1909.09265] Cross-lingual Dependency Parsing with Unlabeled Auxiliary Languages

## Answer (I don't know?: ✗)

In the context of dependency parsing and language transfer, several methods have been developed to enhance performance, especially for low-resource languages. A prominent approach is **cross-lingual transfer learning**. This technique leverages models trained on high-resource languages to assist in parsing tasks for languages with limited labeled data. Cross-lingual transfer has been applied effectively to dependency parsing tasks, where it is essential to predict linguistic labels like syntactic dependencies [1] [2].

One straightforward method within this framework is **direct transfer**. Direct transfer involves training a dependency parser on labeled data from a source (high-resource) language and then applying this model directly to parse data in the target (low-resource) language without requiring labeled data from the target [2]. This method is attractive due to its simplicity, although it can be limited by the quality and relevance of the source language data.

Another advancement in the area is the **hierarchical transfer learning (HTL)** method. HTL builds upon standard transfer learning by incorporating two stages, enabling knowledge transfer not only from a high-resource source language but also from an intermediate language [4] [5]. This multi-stage approach aims to improve the parsing performance further by enriching the model's knowledge base through additional intermediary resources, addressing the limitations posed by the direct transfer approach.

Additionally, there is a significant emphasis on utilizing **language modeling** alongside cross-lingual transfer for universal dependency parsing. This approach aims to eliminate the prerequisite of having annotated treebanks for the target language, thus expanding the potential for dependency parsing across a broader range of languages. Researchers have utilized powerful contextualized language models that do not require labeled data to enhance parsing capability [3] [4].

Moreover, the **Universal Dependencies (UD)** project has played a crucial role in facilitating these methods by providing a dataset of dependency treebanks across numerous languages. The availability of this dataset allows models to leverage existing data from more than 100 languages, including Javanese, as a part of the training process [5]. The research also highlights that, despite promising techniques, many previous studies have reported that parsing performance remains suboptimal, highlighting an ongoing challenge in developing effective cross-lingual systems [3][4].

In summary, the primary methods developed for language transfer in dependency parsing include direct transfer, hierarchical transfer learning, and language modeling within a cross-lingual framework, all aimed at addressing the scarcity of labeled data in low-resource languages and improving overall parsing performance.

1. [1]:  https://ar5iv.org/html/1909.09265, [1909.09265] Cross-lingual Dependency Parsing with Unlabeled Auxiliary Languages
2. [2]:  https://ar5iv.org/html/2101.11216, [2101.11216] PPT: Parsimonious Parser Transfer for Unsupervised Cross-Lingual Adaptation
3. [3]:  https://ar5iv.org/html/2012.13163, [2012.13163] Cross-lingual Universal Dependency Parsing Only from One Monolingual Treebank
4. [4]:  https://ar5iv.org/html/2401.12072, [2401.12072] Cross-lingual Transfer Learning for Javanese Dependency Parsing
5. [5]:  https://ar5iv.org/html/2401.12072, [2401.12072] Cross-lingual Transfer Learning for Javanese Dependency Parsing
---
1. [1]:  Passage ID 1: where a model learned from one language is transferred to another, has become an important technique to improve the quality and coverage of natural language processing (NLP) tools for languages in the world.This technique has been widely applied in many applications,including part-of-speech (POS) tagging Kim et al. (2017), dependency parsing Ma and Xia (2014), named entity recognition Xie et al. (2018), entity linking Sil et al. (2018), coreference resolution Kundu et al. (2018), and question answering Joty et al. (2017).Noteworthy improvements are achieved on low resource language applications due to cross-lingual transfer learning.In this paper, we study cross-lingual transfer for dependency parsing. A dependency parser consists of (1) an encoder that transforms an input text sequence into latent representations and (2) a decoding algorithm that generates the corresponding parse tree.In cross-lingual transfer, most recent approaches assume that the inputs from different
2. [2]:  Passage ID 2: of labelled datasets. The majorityof the world’s languages, however, are low-resource, with little to nolabelled data available (Joshi et al., 2020). Predicting linguistic labels, suchas syntactic dependencies, underlies many downstream NLP applications, and themost effective systems rely on labelled data. Their lack hinders the access toNLP technology in many languages. One solution is cross-lingual model transfer,which adapts models trained on high-resource languages to low-resourceones. This paper presents a flexible framework for cross-lingual transferof syntactic dependency parsers which can leverage any pre-trainedarc-factored dependency parser, and assumes no access to labelled target languagedata.One straightforward method of cross-lingual parsing is direct transfer. Itworks by training a parser on the source language labelled data andsubsequently using it to parse the target language directly. Direct transfer isattractive as it does not require labelled target
3. [3]:  Passage ID 3: languages, the project of universal dependencies (UD) treebanks was launched [7]. Though great efforts have been made as dozens of multilingual treebanks were annotated in terms of the project, it is still far from the huge demand of parsing thousands of human languages if we account for annotating treebanks to build parsers. Thanks to the remarkable progress in deep learning on language modeling, researchers can build powerful contextualized language models without any labeled text data. This motivates us to incorporate language modeling and cross-lingual transferring method for universal dependency parsing, which differs from the existing UD parsing or many cross-lingual parsing that we completely release the prerequisite of treebank for any target languages.Cross-lingual parsing has been explored in previous studies, which focus on sharing limited word-level information by using multilingual word embeddings [8, 9, 10], the parsing performance is quite unsatisfactory and still far
4. [4]:  Passage ID 4: and dependency parsing tasks. The most recent parser performance Alfina et al. (2023) using this dataset is not satisfactory, with only 77.08% on Unlabeled Attachment Score (UAS) and 71.21% on Labeled Attachment Score (LAS). The lack of training data is a typical low-resource problem considered one of the biggest NLP research problems Ruder (2023).Transfer learning (TL) involves leveraging a model’s knowledge from a high-resource source domain to improve performance on various NLP tasks, particularly in low-resource domains Weiss et al. (2016), by transferring learned information to target tasks. Inspired by Maulana et al. (2022) that utilizes cross-lingual transfer learning to develop an Indonesian dependency parser, we want to try to replicate its outcome in Javanese with a limited available dataset. Moreover, we also implement hierarchical transfer learning (HTL) with two stages of transfer learning that offer increased flexibility over TL by enabling knowledge transfer between
5. [5]:  Passage ID 5: natural language processing. We utilized the Universal Dependencies dataset consisting of dependency treebanks from more than 100 languages, including Javanese. We propose two learning strategies to train the model: transfer learning (TL) and hierarchical transfer learning (HTL). While TL only uses a source language to pre-train the model, the HTL method uses a source language and an intermediate language in the learning process. The results show that our best model uses the HTL method, which improves performance with an increase of 10 % for both UAS and LAS evaluations compared to the baseline model.1 IntroductionDespite over 80 million native speakers of Javanese Simons et al. (2023), this language is underrepresented in NLP due to a scarcity of annotated resources. Limited works in Javanese have focused on stemmer Soyusiawaty et al. (2020), POS tagger Askhabi et al. (2020), sentiment analysis Tho et al. (2021), and machine translation Lesatari et al. (2021). However, few have