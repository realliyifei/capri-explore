# Diachronic word embeddings and semantic shifts: a survey

## Question

How can we effectively compare word vectors from different time-specific models?

## URLs

1. https://ar5iv.org/html/2005.03812. [2005.03812] Comparative Analysis of Word Embeddings for Capturing Word Similarities
2. https://ar5iv.org/html/1807.04441. [1807.04441] Tracking the Evolution of Words with Time-reflective Text Representations
3. https://ar5iv.org/html/1901.09785. [1901.09785] Evaluating Word Embedding Models: Methods and Experimental Results
4. https://ar5iv.org/html/2402.12011. [2402.12011] A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change Submitted to NAACL 2024
5. https://ar5iv.org/html/2104.08433. [2104.08433] Are Word Embedding Methods Stable and Should We Care About It?
6. https://ar5iv.org/html/2402.11094. [2402.11094] Word Embeddings Revisited: Do LLMs Offer Something New?
7. https://ar5iv.org/html/2303.07196. [2303.07196] A Comprehensive Empirical Evaluation of Existing Word Embedding Approaches
8. https://ar5iv.org/html/1906.02376. [1906.02376] Training Temporal Word Embeddings with a Compass
9. https://ar5iv.org/html/2406.12165. [2406.12165] Statistical Uncertainty in Word Embeddings: GloVe-V

## Answer (I don't know?: ✗)

To effectively compare word vectors from different time-specific models, one must address the challenges posed by the temporal evolution of language and the limitations in the traditional approaches to vector space models. Here are the fundamental strategies derived from the context provided:

1. **Temporal Word Embeddings (TWEM)**: By utilizing temporal word embeddings, which incorporate the aspect of time into the feature space, we can track how word meanings evolve across different periods. These embeddings provide vectors for words at discrete timestamps, allowing for a comparative analysis of semantic shifts over time. This method facilitates identifying distinct words that share similar meanings during different periods by retrieving temporal embeddings that occupy similar regions in the respective vector spaces [1][5].

2. **Diachronic Text Corpora Partitioning**: The training process for TWEM requires diachronic text corpora, which entail partitioning the data into temporal slices. Each of these slices represents a specific time period and helps in creating vectors based on the context of language usage during that time. However, it is crucial to address the stochastic nature of neural network training, as a Word2vec-like model applied to different temporal slices produces vectors in disparate coordinate systems. This necessitates careful alignment of these coordinate systems to enable meaningful comparisons across the time-specific models [5].

3. **Alignment Strategies**: To compare vectors from different temporal models effectively, one could implement alignment strategies that compute the relationship between embeddings across different time slices. Previous approaches, such as pairwise or joint alignment strategies, have been used to find similarities between vectors trained on different corpora, which can be adapted to the temporal context [2][3].

4. **Vector Normalization**: Ensuring that word vectors are normalized can improve the comparability across different time periods. This could involve establishing a common framework where the coordinates of the vectors are adjusted to fit a standardized space, mitigating the effects of variances caused by different training instances or periods.

5. **Benchmarking Against Established Models**: It may be beneficial to benchmark the temporal models against traditional embedding models like Sentence-BERT or Universal Sentence Encoders. This would provide a reference point to assess whether the changes in word meanings captured by temporal embeddings are indeed significant and relevant in comparison to static embeddings [4].

By employing these strategies, researchers can construct a robust framework for comparing word vectors generated from different time-specific models, leading to clearer insights into semantic evolution and contextual language representations over time. Such advancements would enrich our understanding and ability to conduct nuanced text analytics in NLP [1][2][5].

1. [1]:  https://ar5iv.org/html/1807.04441, [1807.04441] Tracking the Evolution of Words with Time-reflective Text Representations
2. [2]:  https://ar5iv.org/html/1906.02376, [1906.02376] Training Temporal Word Embeddings with a Compass
3. [3]:  https://ar5iv.org/html/1807.04441, [1807.04441] Tracking the Evolution of Words with Time-reflective Text Representations
4. [4]:  https://ar5iv.org/html/2402.11094, [2402.11094] Word Embeddings Revisited: Do LLMs Offer Something New?
5. [5]:  https://ar5iv.org/html/1906.02376, [1906.02376] Training Temporal Word Embeddings with a Compass
---
1. [1]:  Passage ID 1: findings from scientific and professional communities, and social media. Vector space models were developed to analyze text data using data mining and machine learning algorithms. While ample vector space models exist for text data, the evolutionary aspect of ever changing text corpora is still missing in vector-based representations. The advent of word embeddings has enabled us to create a contextual vector space, but the embeddings fail to consider the temporal aspects of the feature space successfully. This paper presents an approach to include temporal aspects in feature spaces. The inclusion of the time aspect in the feature space provides vectors for every natural language element, such as words or entities, at every timestamp. Such temporal word vectors allow us to track how the meaning of a word changes over time, by studying the changes in its neighborhood. Moreover, a time-reflective text representation will pave the way to a new set of text analytic abilities involving time
2. [2]:  Passage ID 2: or better quality than the ones generated with comparable state-of-the-art approaches. In particular, when compared to scalable models based on pairwise alignment strategies (?; ?), our model achieves better performance when trained on a limited size corpus and comparable performance when trained on a large corpus.At the same time, when compared to models based on joint alignment strategies (?; ?), our model is more efficient or it obtains better performance even on a limited size corpus.A possible future direction of our work is to test our temporal word embeddings as features in natural language processing tasks, e.g., named entity recognition, applied to historical text corpora.Also, we would like to apply the proposed model to compare word meanings along dimensions different from time, by slicing the corpus with appropriate criteria. For example, inspired by previous work by ?, we plan to use word embeddings trained with articles published by different newspapers to compare
3. [3]:  Passage ID 3: to answer the following questions.1.How well does our algorithm track the evolution of specific medical terms? (Section V-A)2.How does our method compare qualitatively to other methods in terms of tracking semantic evolution? (Section V-B)3.How sensitive is our vector space model to changes in the hyperparameters? (Section V-C)4.How does our method compare quantitatively to other methods such as regular tf-idf and dynamic embeddings in terms of neighborhood monotony? (Section V-D)V-A Case studyIn this section, we present a study in tracking semantic evolution using our algorithm for two medical terms: leukemia and gastric cancer. We limit our analysis to the top-16 nearest neighbors for each timestamp, from which we present the most interesting words that show significant changes over time. The results are promising because, as we describe next, the neighborhoods help explain the context or contemporary state-of-the-art related to a particular
4. [4]:  Passage ID 4: LLMs have shown remarkable success across various well-known NLP tasks Bubeck et al. (2023); Dai et al. (2022); Du et al. (2022); Smith et al. (2022); Sarkar et al. (2023); Akter et al. (2023), it remains an open question if their enhanced performance can solely be attributed to their larger scale or if the embeddings they generate are fundamentally distinct from those created by traditional encoding models such as Sentence-BERT or Universal Sentence Encoders Mahajan et al. (2023); Sarkar et al. (2022). Since the introduction of the Transformer architecture, model development has exploded with innovations, improving year after year. However, the medium has remained the same: written language. In theory, the similarity and relatedness of words have not changed, but the way that models treat words and their similarity has. More recently, Fournier et al. (2020) evaluated smaller transformer models like GPT-2 and BERT on analogy tasks and found that these models performed poorly on
5. [5]:  Passage ID 5: intervals, one can track the semantic shift occurred in the word usage. Moreover, temporal word embeddings make it possible to find distinct words that share a similar meaning in different periods of time, e.g., by retrieving temporal embeddings that occupy similar regions in the vector spaces that correspond to distinct time periods.The training process of a TWEM relies on diachronic text corpora, which are obtained by partitioning text corpora into temporal “slices” (?; ?). Because of the stochastic nature of the neural networks training process, if we apply a Word2vec-like model on each slice, the output vectors of each slice will be placed in a vector space that has a different coordinate system.This will preclude comparison of vectors across different times (?). A close analogy would be to ask two cartographers to draw a map of Italy during different periods, without giving either of them a compass: the maps would be similar, although one will be rotated by an unknown angle