# Diachronic word embeddings and semantic shifts: a survey

## Question

How can we effectively compare word vectors from different time-specific models?

## URLs

1. https://ar5iv.org/html/2005.03812
2. https://ar5iv.org/html/1807.04441
3. https://ar5iv.org/html/1901.09785
4. https://ar5iv.org/html/2402.12011
5. https://ar5iv.org/html/2104.08433
6. https://ar5iv.org/html/2402.11094
7. https://ar5iv.org/html/2303.07196
8. https://ar5iv.org/html/1906.02376
9. https://ar5iv.org/html/2406.12165

## Answer

To effectively compare word vectors from different time-specific models, several methodologies and considerations must be employed. The understanding of word embeddings and their nuances is critical in this context, as highlighted in the surveyed literature on NLP.

First, it is essential to recognize that different word embedding techniques have evolved to address the inherent limitations of earlier methods, such as one-hot encoding, which fails to capture the syntactic and semantic properties of words [3]. Instead, modern techniques, including Word2Vec and GloVe, produce distributed representations that encapsulate more nuanced relationships between words [4]. When comparing word vectors, one should ensure that the embeddings being analyzed are derived from models that utilize similar underlying architectures and parameters for consistency. For instance, Transformer-based models such as BERT and RoBERTa have created embeddings that exhibit greater semantic clustering compared to classical methods, as evidenced by their performance on tasks like the Bigger Analogy Test Set (BATS) [4].

Next, utilizing vector arithmetic and measuring similarity can help in the comparison of embeddings. Techniques such as cosine similarity allow for quantitative assessments of how closely related different word vectors are across models [4]. It is pivotal to evaluate how these models address temporal word evolution by assessing how word meanings change over time. This involves not merely comparing static embeddings but also considering temporal dynamics, whereby terms might shift semantically, as illustrated with the evolution of the word "gay" over time [5].

Moreover, incorporating time-dependent word representations into the analysis is advantageous. This allows for observing the continual evolution of language and capturing how specific words develop different meanings, particularly in a rapidly changing world influenced by technological advancements and socio-cultural shifts [5]. Identifying key metrics and benchmarks for evaluations can provide a systematic approach to comparing models. 

In conclusion, to effectively compare word vectors from different time-specific models, it is crucial to analyze the embeddings' architecture, utilize similarity measures, factor in the semantic evolution over time, and rely on established benchmarks for evaluation. This multi-faceted approach ensures a comprehensive understanding of the strengths and limitations of each model within the broader scope of NLP.

[1]: https://ar5iv.org/html/2010.15036, [2010.15036] A Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation Language Models
[2]: https://ar5iv.org/html/2010.15036, [2010.15036] A Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation Language Models
[3]: https://ar5iv.org/html/2404.14631, No Title
[4]: https://ar5iv.org/html/2402.11094, [2402.11094] Word Embeddings Revisited: Do LLMs Offer Something New?
[5]: https://ar5iv.org/html/1906.02376, [1906.02376] Training Temporal Word Embeddings with a Compass

[1]: Passage ID 1: language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP related tasks. In the end, this survey briefly discusses the commonly used ML and DL based classifiers, evaluation metrics and the applications of these word embeddings in different NLP tasks.Text Mining, Natural Language Processing, Word representation, Language
[2]: Passage ID 2: language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP related tasks. In the end, this survey briefly discusses the commonly used ML and DL based classifiers, evaluation metrics and the applications of these word embeddings in different NLP tasks.Text Mining, Natural Language Processing, Word representation, Language
[3]: Passage ID 3: of LFW and EDWS in enhancing Word2Vec’s performance, surpassing previous state-of-the-art methods.Keywords: Natural language processing Word embedding Word2Vec Learnable weights Window size scheduling1 IntroductionNLP researchers have long aimed to obtain high-quality word vector representations. One traditional approach is one-hot encoding, where a vector has a “1" at the index corresponding to the word and “0"s elsewhere. However, this approach suffers from the curse of dimensionality when dealing with large vocabulary sizes that can reach millions [2]. Additionally, one-hot encoding fails to capture syntactic or semantic properties because all word distances in the vector space are equal.Distributed word representations have been developed to overcome the limitations of one-hot encoding [8]. In this approach, words are represented by lower-dimensional vectors, typically a few hundred dimensions, where each element can take on various values [18]. These distributed word
[4]: Passage ID 4: (USE). This paper systematically investigates this issue by comparing classical word embedding techniques against LLM-based word embeddings in terms of their latent vector semantics. Our results show that LLMs tend to cluster semantically related words more tightly than classical models. LLMs also yield higher average accuracy on the Bigger Analogy Test Set (BATS) over classical methods. Finally, some LLMs tend to produce word embeddings similar to SBERT, a relatively lighter classical model.1 IntroductionWith the introduction of Word2Vec Mikolov et al. (2013a) and GLoVe Pennington et al. (2014), word embedding techniques and their semantic analysis have gained significant popularity in the field of deep NLP research. Since then, the size and complexity of the embedding models have increased dramatically, especially with the introduction of transformer-based architectures like BERT Devlin et al. (2018), BART Lewis et al. (2019), RoBERTa Liu et al. (2019), followed by rigorously
[5]: Passage ID 5: ModelTWETemporal Word EmbeddingNAC-SNews Article Corpus SmallNAC-LNews Article Corpus LargeMLPCMachine Learning Papers CorpusT1Testset1T2Testset2WEMWord Embedding ModelTWATemporal Word AnalogyIntroductionLanguage is constantly evolving, reflecting the continuous changes in the world and the needs of the speakers. While new words are introduced to refer to new concepts and experiences (e.g., Internet, hashtag, microaggression), some words are subject to semantic shifts, i.e., their meanings change over time (?). For example, in the English language, the word gay originally meant joyful, happy; only during the 202020th century the word began to be used in association with sexual orientation (?).Finding methods to represent and analyze word evolution over time is a key task to understand the dynamics of human language, revealing statistical laws of semantic evolution (?). In addition, time-dependent word representations may be useful when natural language