# Diachronic word embeddings and semantic shifts: a survey

## Question

How can we effectively compare word vectors from different time-specific models?

## URLs

1. https://ar5iv.org/html/2005.03812. [2005.03812] Comparative Analysis of Word Embeddings for Capturing Word Similarities
2. https://ar5iv.org/html/1807.04441. [1807.04441] Tracking the Evolution of Words with Time-reflective Text Representations
3. https://ar5iv.org/html/1901.09785. [1901.09785] Evaluating Word Embedding Models: Methods and Experimental Results
4. https://ar5iv.org/html/2402.12011. [2402.12011] A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change Submitted to NAACL 2024
5. https://ar5iv.org/html/2104.08433. [2104.08433] Are Word Embedding Methods Stable and Should We Care About It?
6. https://ar5iv.org/html/2402.11094. [2402.11094] Word Embeddings Revisited: Do LLMs Offer Something New?
7. https://ar5iv.org/html/2303.07196. [2303.07196] A Comprehensive Empirical Evaluation of Existing Word Embedding Approaches
8. https://ar5iv.org/html/1906.02376. [1906.02376] Training Temporal Word Embeddings with a Compass
9. https://ar5iv.org/html/2406.12165. [2406.12165] Statistical Uncertainty in Word Embeddings: GloVe-V

## Answer (I don't know?: ✗)

To effectively compare word vectors from different time-specific models, it is essential to address the challenges associated with the temporal nature of the data. One of the primary issues is that when using methods like Word2Vec on different temporal slices of text corpora, the output vectors are placed into vector spaces with different coordinate systems due to the stochastic nature of the training process. This scenario complicates the comparison of vectors across different time periods [4]. 

To mitigate this issue, researchers can utilize temporal word embeddings (TWEM) that consider the evolution of words over time by creating a framework that accounts for both the content of the text and the timestamps when it was produced [2]. By integrating the time aspect into feature spaces, TWEM enables the generation of distinct vectors for each natural language element at various timestamps, facilitating the analysis of how word meanings shift over different periods [2] [4]. 

Moreover, one method to compare temporal word vectors effectively is by tracking the neighborhood of words through their nearest neighbors across slices. This methodology allows for the identification of significant changes in word usage and meaning over time. For example, analyzing the top-16 nearest neighbors of a term at each timestamp can reveal the contextual evolution and semantic shifts of specific words, such as medical terms like leukemia and gastric cancer [3]. This approach provides insights into the contemporary state-of-the-art related to the specific term and allows for qualitative comparisons between different time periods [3]. 

Furthermore, using a methodology that incorporates diachronic text corpora—where corpora are divided into temporal "slices"—ensures a structured way to analyze the temporal dynamics of word meanings [4]. This process also allows researchers to counteract the misalignment of vector spaces created by different training iterations, which can lead to incorrect comparisons if not standard precautions are taken.

Additionally, exploring the concept of neighborhood monotony can quantitatively assess the sensitivity of vector space models to hyperparameters and their relative performance against traditional methods like term frequency-inverse document frequency (tf-idf) [3]. This comparison may reveal insights into how well the model captures the evolution of word meanings and therefore enhance the understanding of language dynamics over time.

In summary, to effectively compare word vectors across different time-specific models, it is crucial to develop frameworks that incorporate temporal aspects into linguistic analysis and utilize strategies that assess semantic evolution through neighborhood tracking and diachronic evaluation methods [2] [3] [4].

1. [1]:  https://ar5iv.org/html/2410.00427, No Title
2. [2]:  https://ar5iv.org/html/1807.04441, [1807.04441] Tracking the Evolution of Words with Time-reflective Text Representations
3. [3]:  https://ar5iv.org/html/1807.04441, [1807.04441] Tracking the Evolution of Words with Time-reflective Text Representations
4. [4]:  https://ar5iv.org/html/1906.02376, [1906.02376] Training Temporal Word Embeddings with a Compass
5. [5]:  https://ar5iv.org/html/2410.00427, No Title
---
1. [1]:  Passage ID 1: ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the laws of thermodynamics?” Following a quality inspection of the synthetically produced questions, we assembled a training dataset of 1601 examples, consisting of 120 questions for each of the 12 topics and 161 general questions. We also derived a test dataset containing 364 examples with a balanced class distribution similar to the training dataset.In our experiments, we evaluated three classification approaches: vector similarity search, prompting a LLM (GPT-3.5-Turbo), and few-shot fine-tuning of a transformer model with the SetFit framework Tunstall et al. (2022). Concerning the vector search approach with the SPECTER2 model, we measured the cosine similarity to compare vectors of embedded user queries with paper embeddings in our vector database to retrieve the 100 most similar
2. [2]:  Passage ID 2: findings from scientific and professional communities, and social media. Vector space models were developed to analyze text data using data mining and machine learning algorithms. While ample vector space models exist for text data, the evolutionary aspect of ever changing text corpora is still missing in vector-based representations. The advent of word embeddings has enabled us to create a contextual vector space, but the embeddings fail to consider the temporal aspects of the feature space successfully. This paper presents an approach to include temporal aspects in feature spaces. The inclusion of the time aspect in the feature space provides vectors for every natural language element, such as words or entities, at every timestamp. Such temporal word vectors allow us to track how the meaning of a word changes over time, by studying the changes in its neighborhood. Moreover, a time-reflective text representation will pave the way to a new set of text analytic abilities involving time
3. [3]:  Passage ID 3: to answer the following questions.1.How well does our algorithm track the evolution of specific medical terms? (Section V-A)2.How does our method compare qualitatively to other methods in terms of tracking semantic evolution? (Section V-B)3.How sensitive is our vector space model to changes in the hyperparameters? (Section V-C)4.How does our method compare quantitatively to other methods such as regular tf-idf and dynamic embeddings in terms of neighborhood monotony? (Section V-D)V-A Case studyIn this section, we present a study in tracking semantic evolution using our algorithm for two medical terms: leukemia and gastric cancer. We limit our analysis to the top-16 nearest neighbors for each timestamp, from which we present the most interesting words that show significant changes over time. The results are promising because, as we describe next, the neighborhoods help explain the context or contemporary state-of-the-art related to a particular
4. [4]:  Passage ID 4: intervals, one can track the semantic shift occurred in the word usage. Moreover, temporal word embeddings make it possible to find distinct words that share a similar meaning in different periods of time, e.g., by retrieving temporal embeddings that occupy similar regions in the vector spaces that correspond to distinct time periods.The training process of a TWEM relies on diachronic text corpora, which are obtained by partitioning text corpora into temporal “slices” (?; ?). Because of the stochastic nature of the neural networks training process, if we apply a Word2vec-like model on each slice, the output vectors of each slice will be placed in a vector space that has a different coordinate system.This will preclude comparison of vectors across different times (?). A close analogy would be to ask two cartographers to draw a map of Italy during different periods, without giving either of them a compass: the maps would be similar, although one will be rotated by an unknown angle
5. [5]:  Passage ID 5: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the