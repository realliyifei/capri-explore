# A Survey of Deep Learning for Mathematical Reasoning

## Question

How does Chain-of-thought prompting (CoT) improve mathematical reasoning in large language models (LLMs)?

## URLs

1. https://ar5iv.org/html/2201.11903. [2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
2. https://ar5iv.org/html/2411.11984. [2411.11984] Understanding Chain-of-Thought in LLMs through Information Theory
3. https://ar5iv.org/html/2406.06592. [2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision
4. https://ar5iv.org/html/2410.10336. [2410.10336] CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning
5. https://ar5iv.org/html/2409.20441. [2409.20441] Instance-adaptive Zero-shot Chain-of-Thought Prompting
6. https://ar5iv.org/html/2305.15408. [2305.15408] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
7. https://ar5iv.org/html/2409.12183. [2409.12183] To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning
8. https://ar5iv.org/html/2409.19381. [2409.19381] MetaMath: Integrating Natural Language and Code for Enhanced Mathematical Reasoning in Large Language Models
9. https://ar5iv.org/html/2309.13339. [2309.13339] Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic

## Answer (I don't know?: ✗)

Chain-of-Thought (CoT) prompting significantly enhances mathematical reasoning in Large Language Models (LLMs) by encouraging the models to decompose complex reasoning tasks into manageable, sequential steps, mimicking human cognitive processes. This structured approach allows LLMs to navigate and solve mathematical problems more effectively than they would by attempting to produce a direct answer, which often leads to errors or misunderstandings.

The core mechanism behind CoT prompting is its ability to improve the expressivity and reasoning capabilities of LLMs. According to the studies, CoT prompting has been shown to boost the performance of LLMs on various reasoning tasks, particularly in mathematics, where complex reasoning is needed [1][2]. By guiding the model to explicitly lay out intermediate reasoning steps, CoT facilitates a clearer thought process that can lead to more accurate problem-solving outcomes [3].

In addition to improving accuracy, CoT helps LLMs generate human-readable explanations, which enhances interpretability and trust in the responses produced by these models [3]. This is particularly pertinent in mathematical reasoning, where understanding the rationale behind an answer can be just as important as the answer itself. For example, when LLMs utilize CoT, they provide insights into their decision-making processes, allowing users to follow the logical flow of the computations [3].

The research has indicated that while traditional methods saw improvements through model scaling, the introduction of CoT prompting has addressed specific challenges in mathematical problem-solving. For instance, the greedy decoding strategy traditionally utilized in LLMs has been limiting, and newer approaches such as self-consistency decoding leverage multiple reasoning paths to refine results further [2]. This strategy not only enhances the performance of LLMs but also provides a robust means of confirming the validity of the answers produced.

Moreover, CoT prompting has been extensively implemented in various LLM applications, such as ChatGPT and Llama, which default to CoT strategies when presented with reasoning problems [3]. This widespread adoption points to its effectiveness in practical scenarios, affirming its utility in enhancing reasoning abilities.

However, while the success of CoT is evident, researchers continue to explore its underlying mechanisms and limitations. The use of CoT prompting, alongside approaches that combine natural language with symbolic reasoning, indicates an ongoing effort to refine mathematical problem-solving capabilities of LLMs [4]. These combined techniques aim to ensure accuracy while maintaining the interpretability of the solutions provided.

In summary, Chain-of-Thought prompting enhances mathematical reasoning in LLMs by structuring the problem-solving process into intermediate steps, improving expressivity, and providing clarity in logical reasoning. This method not only leads to better accuracy in responses but also enriches the interpretability and reliability of LLM outputs in mathematical contexts. As research progresses, the exploration of CoT's mechanisms will continue to yield insights into its effectiveness and potential applications in broader reasoning tasks.

1. [1]:  https://ar5iv.org/html/2305.15408, [2305.15408] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
2. [2]:  https://ar5iv.org/html/2406.06592, [2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision
3. [3]:  https://ar5iv.org/html/2409.12183, [2409.12183] To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning
4. [4]:  https://ar5iv.org/html/2410.10336, [2410.10336] CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning
5. [5]:  https://ar5iv.org/html/2402.18312, No Title
---
1. [1]:  Passage ID 1: studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. By using circuit complexity theory, we first give impossibility results showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a
2. [2]:  Passage ID 2: cost-effective compared to existing methods.1 IntroductionDespite significant progress in various large language model (LLM) benchmarks achieved through simply scaling up the model (Wei et al., 2022a), the development of complex reasoning abilities, particularly in tasks like mathematical problem-solving and code generation, necessitates a deeper understanding and remains an active research frontier.Chain-of-Thought (CoT) Prompting (Wei et al., 2022b) was proposed to guide the LLM to break down a reasoning task into a sequence of intermediate steps, similar to a human’s reasoning process. CoT boosts the performance of LLMs on many reasoning tasks, but the greedy decoding strategy limits its performance. To address that, Wang et al. (2023) proposed the self-consistency decoding strategy, leveraging multiple reasoning paths to reach a voted answer. Besides these prompting focused work, fine-tuning LLM with question and CoT solution pairs (Perez et al., 2021; Ouyang et al., 2022)
3. [3]:  Passage ID 3: the mean improvement from CoT across experiments).1 IntroductionChain-of-thought (CoT) (Nye et al., 2022; Wei et al., 2022) has become a widely used prompting technique for eliciting reasoning from language models. CoT can provide human-readable explanations of how problems are solved (Joshi et al., 2023; Lanham et al., 2023), but most frequently it is invoked to improve an LLM’s ability to answer complex questions via intermediate computation (Madaan & Yazdanbakhsh, 2022; Wang et al., 2023a; Dziri et al., 2023). Current post-training schemes for LLMs heavily infuse CoT capabilities into models: systems like ChatGPT or Llama 3.1 default to CoT when given reasoning problems (OpenAI, 2023; Dubey et al., 2024).CoT has seen widespread usage, but it is most heavily explored in the domain of mathematical reasoning (Zhou et al., 2023a; Fu et al., 2023; Chae et al., 2024; Xu et al., 2024b; Qi et al., 2024).In fact, many “reasoning” methods for LLMs are evaluated only in the math
4. [4]:  Passage ID 4: LLMs cannot parse symbolic expressions as reliably asrule-based reasoners. Symbolic Chain-of-Thought Prompting. Symbolic CoT prompting Lyu et al. (2023) combines natural language (NL) and symbolic language (SL) in the reasoning chain. NL decomposes complex queries into subproblems, and SL programs (e.g., Python, LEAN) handle each subproblem. A deterministic solver executes the SL to derive the final answer, ensuring faithfulness while NL aids interpretability. Recent efforts aim to reduce reliance on SL programs by leveraging LLMs Xu et al. (2024), but these methods focus primarily on logical reasoning rather than complex mathematical tasks and still require verifiers to ensure accuracy.Mathematical Reasoning. Mathematical reasoning with LLMs has been explored widely Lewkowycz et al. (2022); Luo et al. (2024); Ahn et al. (2024); Imani et al. (2023); Chen et al. (2024); Meadows and Freitas (2022); Mirzadeh et al. (2024), with CoT methods yielding significant performance
5. [5]:  Passage ID 5: the later half, attention heads that move information along ontological relationships appear exclusively in the initial half, and so on. To the best of our knowledge, this is the first attempt towards mechanistic investigation of CoT reasoning in LLMs.1 IntroductionThe emergence of Large Language Models (LLMs) is by far one of the most remarkable examples of enigmatic ubiquity. Building these models, refining their abilities and unleashing newer potentials have garnered tremendous enthusiasm lately. Yet, we still barely understand the mechanisms within the model that actually implements those abilities – how they emerge from the training dynamics and why they are often bewilderingly brittle. One of these remarkable capabilities of LLMs that has unleashed their potential in complex reasoning is Chain-of-Thought (CoT) prompting  (Wei et al., 2022b; Kojima et al., 2022). Instead of providing a direct answer to the question, in CoT prompting, we expect the model to generate a verbose