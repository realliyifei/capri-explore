# A Survey of Deep Learning for Mathematical Reasoning

## Question

How does Chain-of-thought prompting (CoT) improve mathematical reasoning in large language models (LLMs)?

## URLs

1. https://ar5iv.org/html/2201.11903. [2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
2. https://ar5iv.org/html/2411.11984. [2411.11984] Understanding Chain-of-Thought in LLMs through Information Theory
3. https://ar5iv.org/html/2406.06592. [2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision
4. https://ar5iv.org/html/2410.10336. [2410.10336] CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning
5. https://ar5iv.org/html/2409.20441. [2409.20441] Instance-adaptive Zero-shot Chain-of-Thought Prompting
6. https://ar5iv.org/html/2305.15408. [2305.15408] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
7. https://ar5iv.org/html/2409.12183. [2409.12183] To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning
8. https://ar5iv.org/html/2409.19381. [2409.19381] MetaMath: Integrating Natural Language and Code for Enhanced Mathematical Reasoning in Large Language Models
9. https://ar5iv.org/html/2309.13339. [2309.13339] Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic

## Answer (I don't know?: ✗)

Chain-of-Thought (CoT) prompting significantly enhances mathematical reasoning in Large Language Models (LLMs) by guiding them to break down complex tasks into manageable intermediate steps, similar to how humans process information. This approach not only improves the accuracy of answers but also facilitates the generation of human-readable explanations.

One of the core mechanisms of CoT prompting is its ability to decompose a reasoning task into a sequence of sub-problems. This structured breakdown enables LLMs to handle complex questions more effectively, particularly in the realm of mathematical problem-solving and other reasoning tasks [2]. By leveraging CoT, LLMs can provide clear derivations for their answers, which enhances interpretability and allows users to follow the model's reasoning process [3].

Additionally, techniques such as self-consistency decoding have been developed to maximize the effectiveness of CoT. This approach involves exploring multiple reasoning paths and aggregating the results to reach a voted answer, thereby addressing the limitations of greedy decoding strategies that were previously employed [2]. Overall, these advancements illustrate that CoT can lead to substantial improvements in LLM performance on tasks that require intricate reasoning, with studies showing significant boosts in outcomes across various experiments [3].

Moreover, the approach has been applied extensively in mathematical reasoning, where it has become a staple technique for improving LLM capabilities [3]. However, it is important to note that while CoT prompting remarkably increases accuracy, it does not completely eliminate the challenges faced by LLMs in parsing symbolic expressions compared to rule-based systems. Efforts like symbolic CoT prompting aim to combine natural language processing with symbolic languages (such as Python) to tackle each subproblem methodically. These advancements ensure not only that the model’s solutions are accurate but also that they remain interpretable [4].

Most prominently, CoT prompting aligns with a broader strategy of enhancing LLM capabilities through various methodologies, including fine-tuning with question and CoT solution pairs [2]. By presenting few-shot examples and structured reasoning paths, CoT encourages LLMs to engage in more systematic reasoning, leading to better performance on tasks that require logical and mathematical reasoning [5].

In summary, Chain-of-Thought prompting improves mathematical reasoning in LLMs through structured breakdowns of complex tasks, the generation of interpretable paths, and ongoing methodological advancements that enhance performance. The combination of these factors fosters more reliable and human-like reasoning in LLMs, especially in challenging domains like mathematics.

1. [1]:  https://ar5iv.org/html/2305.15408, [2305.15408] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
2. [2]:  https://ar5iv.org/html/2406.06592, [2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision
3. [3]:  https://ar5iv.org/html/2409.12183, [2409.12183] To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning
4. [4]:  https://ar5iv.org/html/2410.10336, [2410.10336] CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning
5. [5]:  https://ar5iv.org/html/2309.13339, [2309.13339] Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic
---
1. [1]:  Passage ID 1: studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. By using circuit complexity theory, we first give impossibility results showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a
2. [2]:  Passage ID 2: cost-effective compared to existing methods.1 IntroductionDespite significant progress in various large language model (LLM) benchmarks achieved through simply scaling up the model (Wei et al., 2022a), the development of complex reasoning abilities, particularly in tasks like mathematical problem-solving and code generation, necessitates a deeper understanding and remains an active research frontier.Chain-of-Thought (CoT) Prompting (Wei et al., 2022b) was proposed to guide the LLM to break down a reasoning task into a sequence of intermediate steps, similar to a human’s reasoning process. CoT boosts the performance of LLMs on many reasoning tasks, but the greedy decoding strategy limits its performance. To address that, Wang et al. (2023) proposed the self-consistency decoding strategy, leveraging multiple reasoning paths to reach a voted answer. Besides these prompting focused work, fine-tuning LLM with question and CoT solution pairs (Perez et al., 2021; Ouyang et al., 2022)
3. [3]:  Passage ID 3: the mean improvement from CoT across experiments).1 IntroductionChain-of-thought (CoT) (Nye et al., 2022; Wei et al., 2022) has become a widely used prompting technique for eliciting reasoning from language models. CoT can provide human-readable explanations of how problems are solved (Joshi et al., 2023; Lanham et al., 2023), but most frequently it is invoked to improve an LLM’s ability to answer complex questions via intermediate computation (Madaan & Yazdanbakhsh, 2022; Wang et al., 2023a; Dziri et al., 2023). Current post-training schemes for LLMs heavily infuse CoT capabilities into models: systems like ChatGPT or Llama 3.1 default to CoT when given reasoning problems (OpenAI, 2023; Dubey et al., 2024).CoT has seen widespread usage, but it is most heavily explored in the domain of mathematical reasoning (Zhou et al., 2023a; Fu et al., 2023; Chae et al., 2024; Xu et al., 2024b; Qi et al., 2024).In fact, many “reasoning” methods for LLMs are evaluated only in the math
4. [4]:  Passage ID 4: LLMs cannot parse symbolic expressions as reliably asrule-based reasoners. Symbolic Chain-of-Thought Prompting. Symbolic CoT prompting Lyu et al. (2023) combines natural language (NL) and symbolic language (SL) in the reasoning chain. NL decomposes complex queries into subproblems, and SL programs (e.g., Python, LEAN) handle each subproblem. A deterministic solver executes the SL to derive the final answer, ensuring faithfulness while NL aids interpretability. Recent efforts aim to reduce reliance on SL programs by leveraging LLMs Xu et al. (2024), but these methods focus primarily on logical reasoning rather than complex mathematical tasks and still require verifiers to ensure accuracy.Mathematical Reasoning. Mathematical reasoning with LLMs has been explored widely Lewkowycz et al. (2022); Luo et al. (2024); Ahn et al. (2024); Imani et al. (2023); Chen et al. (2024); Meadows and Freitas (2022); Mirzadeh et al. (2024), with CoT methods yielding significant performance
5. [5]:  Passage ID 5: (2023); Zheng et al. (2023); Welleck et al. (2022).Chain-of-Thought Prompting.Prior works show that LLMs have the corresponding power for complex tasks but require a proper strategy to unleash, e.g. human-in-the-loop Ouyang et al. (2022) alignment tuning and Chain-of-Thought prompting (CoT) Wei et al. (2022).In order to generate a chain of thoughts that decomposes the original problem into several small parts which a language model can easily handle, CoT creates few-shot exemplars ofa detailed reasoning path to let the model follow.Least-to-most Zhou et al. (2023) explicitly prompts the LLM to divide complex questions into sub-problems and conquer them one by one.Moreover, zero-shot-CoT Kojima et al. (2022) showcases the impressive effectiveness of simply attaching the sentence “Let’s think step by step.” before any zero-shot reasoning trace starts.We build our approach under a zero-shot setting and integrate zero-shot-CoT as a baseline to improve with.While existing