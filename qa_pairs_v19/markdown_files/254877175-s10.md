# A Survey of Deep Learning for Mathematical Reasoning

## Question

How does Chain-of-thought prompting (CoT) improve mathematical reasoning in large language models (LLMs)?

## URLs

1. https://ar5iv.org/html/2201.11903. [2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
2. https://ar5iv.org/html/2411.11984. [2411.11984] Understanding Chain-of-Thought in LLMs through Information Theory
3. https://ar5iv.org/html/2406.06592. [2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision
4. https://ar5iv.org/html/2410.10336. [2410.10336] CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning
5. https://ar5iv.org/html/2409.20441. [2409.20441] Instance-adaptive Zero-shot Chain-of-Thought Prompting
6. https://ar5iv.org/html/2305.15408. [2305.15408] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
7. https://ar5iv.org/html/2409.12183. [2409.12183] To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning
8. https://ar5iv.org/html/2409.19381. [2409.19381] MetaMath: Integrating Natural Language and Code for Enhanced Mathematical Reasoning in Large Language Models
9. https://ar5iv.org/html/2309.13339. [2309.13339] Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic

## Answer (I don't know?: ✗)

Chain-of-Thought prompting (CoT) improves mathematical reasoning in large language models (LLMs) by guiding them to systematically break down complex problems into manageable steps, thereby emulating human reasoning processes. This approach allows LLMs to explicitly articulate the intermediate steps leading to a final answer, which enhances both the interpretability and accuracy of their outputs ([2], [3]). 

CoT prompting has been shown to yield significant performance improvements in various reasoning tasks, particularly in mathematical problem-solving. This is largely due to its ability to encourage models to capture and express their reasoning, which is essential when tackling intricate equations and logical deductions ([1], [4]). The way CoT works involves decomposing complex mathematical queries into simpler subproblems, which can then be addressed sequentially. For example, when the reasoning process is expressed in natural language, LLMs can follow the logical flow more clearly, making it less likely they will skip critical steps or make hasty conclusions ([4], [3]).

Moreover, the combination of CoT prompting with symbolic reasoning techniques—where symbolic languages are employed to handle particular subproblems—helps merge the interpretability of human reasoning with the computational power of LLMs. This hybrid approach reduces the model’s reliance on symbolic programming by leveraging the LLM’s capabilities more effectively in logical reasoning tasks, although challenges remain in ensuring accuracy through verification ([4], [3]).

Recent advancements, such as the self-consistency decoding strategy, further enhance the capabilities of CoT by allowing LLMs to consider multiple reasoning paths and aggregate results to arrive at a final answer. This method mitigates some performance limitations seen with greedy decoding strategies, making it particularly useful in complex reasoning contexts ([2], [3]).

Despite these advancements, the fundamental architectural constraints of LLMs present ongoing challenges. Research has demonstrated that bounded-depth Transformers struggle to generate correct answers for basic arithmetic tasks unless they are significantly increased in size. In contrast, autoregressive Transformers equipped with CoT capabilities can solve the same tasks effectively without the need for extensive model scaling ([1], [2]). 

In summary, CoT enhances mathematical reasoning in LLMs through a structured approach that breaks down tasks into smaller, logically sequenced steps, supports better accuracy and interpretability of outputs, and is complemented by innovative decoding strategies. However, the mechanism's underlying complexities still require further exploration to fully understand how these improvements are achieved ([1], [5]).

1. [1]:  https://ar5iv.org/html/2305.15408, [2305.15408] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
2. [2]:  https://ar5iv.org/html/2406.06592, [2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision
3. [3]:  https://ar5iv.org/html/2409.12183, [2409.12183] To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning
4. [4]:  https://ar5iv.org/html/2410.10336, [2410.10336] CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning
5. [5]:  https://ar5iv.org/html/2402.18312, No Title
---
1. [1]:  Passage ID 1: studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. By using circuit complexity theory, we first give impossibility results showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a
2. [2]:  Passage ID 2: cost-effective compared to existing methods.1 IntroductionDespite significant progress in various large language model (LLM) benchmarks achieved through simply scaling up the model (Wei et al., 2022a), the development of complex reasoning abilities, particularly in tasks like mathematical problem-solving and code generation, necessitates a deeper understanding and remains an active research frontier.Chain-of-Thought (CoT) Prompting (Wei et al., 2022b) was proposed to guide the LLM to break down a reasoning task into a sequence of intermediate steps, similar to a human’s reasoning process. CoT boosts the performance of LLMs on many reasoning tasks, but the greedy decoding strategy limits its performance. To address that, Wang et al. (2023) proposed the self-consistency decoding strategy, leveraging multiple reasoning paths to reach a voted answer. Besides these prompting focused work, fine-tuning LLM with question and CoT solution pairs (Perez et al., 2021; Ouyang et al., 2022)
3. [3]:  Passage ID 3: the mean improvement from CoT across experiments).1 IntroductionChain-of-thought (CoT) (Nye et al., 2022; Wei et al., 2022) has become a widely used prompting technique for eliciting reasoning from language models. CoT can provide human-readable explanations of how problems are solved (Joshi et al., 2023; Lanham et al., 2023), but most frequently it is invoked to improve an LLM’s ability to answer complex questions via intermediate computation (Madaan & Yazdanbakhsh, 2022; Wang et al., 2023a; Dziri et al., 2023). Current post-training schemes for LLMs heavily infuse CoT capabilities into models: systems like ChatGPT or Llama 3.1 default to CoT when given reasoning problems (OpenAI, 2023; Dubey et al., 2024).CoT has seen widespread usage, but it is most heavily explored in the domain of mathematical reasoning (Zhou et al., 2023a; Fu et al., 2023; Chae et al., 2024; Xu et al., 2024b; Qi et al., 2024).In fact, many “reasoning” methods for LLMs are evaluated only in the math
4. [4]:  Passage ID 4: LLMs cannot parse symbolic expressions as reliably asrule-based reasoners. Symbolic Chain-of-Thought Prompting. Symbolic CoT prompting Lyu et al. (2023) combines natural language (NL) and symbolic language (SL) in the reasoning chain. NL decomposes complex queries into subproblems, and SL programs (e.g., Python, LEAN) handle each subproblem. A deterministic solver executes the SL to derive the final answer, ensuring faithfulness while NL aids interpretability. Recent efforts aim to reduce reliance on SL programs by leveraging LLMs Xu et al. (2024), but these methods focus primarily on logical reasoning rather than complex mathematical tasks and still require verifiers to ensure accuracy.Mathematical Reasoning. Mathematical reasoning with LLMs has been explored widely Lewkowycz et al. (2022); Luo et al. (2024); Ahn et al. (2024); Imani et al. (2023); Chen et al. (2024); Meadows and Freitas (2022); Mirzadeh et al. (2024), with CoT methods yielding significant performance
5. [5]:  Passage ID 5: the later half, attention heads that move information along ontological relationships appear exclusively in the initial half, and so on. To the best of our knowledge, this is the first attempt towards mechanistic investigation of CoT reasoning in LLMs.1 IntroductionThe emergence of Large Language Models (LLMs) is by far one of the most remarkable examples of enigmatic ubiquity. Building these models, refining their abilities and unleashing newer potentials have garnered tremendous enthusiasm lately. Yet, we still barely understand the mechanisms within the model that actually implements those abilities – how they emerge from the training dynamics and why they are often bewilderingly brittle. One of these remarkable capabilities of LLMs that has unleashed their potential in complex reasoning is Chain-of-Thought (CoT) prompting  (Wei et al., 2022b; Kojima et al., 2022). Instead of providing a direct answer to the question, in CoT prompting, we expect the model to generate a verbose