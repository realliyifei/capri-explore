# A Survey of Evaluation Methods of Generated Medical Textual Reports

## Question

What are the two most popular types of automatic evaluation metrics in Natural Language Generation (NLG), and how do they differ?

## URLs

1. https://ar5iv.org/html/2408.09169. [2408.09169] Automatic Metrics in Natural Language Generation: A Survey of Current Evaluation Practices
2. https://ar5iv.org/html/2403.14275. [2403.14275] Is Reference Necessary in the Evaluation of NLG Systems? When and Where?
3. https://ar5iv.org/html/2402.01383. [2402.01383] LLM-based NLG Evaluation: Current Status and Challenges
4. https://ar5iv.org/html/2203.11131. [2203.11131] Towards Explainable Evaluation Metrics for Natural Language Generation
5. https://ar5iv.org/html/2410.16834. [2410.16834] Analyzing and Evaluating Correlation Measures in NLG Meta-Evaluation
6. https://ar5iv.org/html/2410.10724. [2410.10724] Large Language Models Are Active Critics in NLG Evaluation
7. https://ar5iv.org/html/2305.15067. [2305.15067] Not All Metrics Are Guilty: Improving NLG Evaluation with LLM Paraphrasing
8. https://ar5iv.org/html/2008.12009. [2008.12009] A Survey of Evaluation Metrics Used for NLG Systems
9. https://ar5iv.org/html/2310.02040. [2310.02040] Jury: A Comprehensive Evaluation Toolkit
10. https://ar5iv.org/html/2303.04048. [2303.04048] Is ChatGPT a Good NLG Evaluator? A Preliminary Study

## Answer (I don't know?: ‚úó)

The two most popular types of automatic evaluation metrics in Natural Language Generation (NLG) are n-gram-based metrics and metrics based on trained transformer models. 

1. **n-gram-based Metrics**: These metrics measure the lexical overlap between generated texts and reference texts. They include widely used metrics such as ROUGE (used predominantly for summarization), BLEU, Distinct-n, and METEOR. These metrics evaluate the quality of generated content by comparing the occurrence and frequency of n-grams (contiguous sequences of n items from a given sample of text) in the generated text against those in reference texts. For example, ROUGE, which is the dominant metric in summarization evaluation, focuses on recall and overlaps between the n-grams from the generated text and those from the reference summaries [1]. However, n-gram-based metrics have been criticized for their inadequacy in capturing the nuances and complexities of different NLG tasks, as they primarily focus on surface-level similarities rather than deeper semantic understanding [2].

2. **Transformer Model-based Metrics**: In response to the limitations of heuristic-based metrics, there has been a shift toward metrics that employ trained transformer models. This approach utilizes more sophisticated algorithms that can analyze and gauge the semantic meaning of text, rather than merely counting n-gram overlaps. Such metrics are capable of capturing deeper relationships and variations in language, hence providing a more nuanced evaluation of generated texts [2][5]. The rising number of NLG models has spurred the development of these newer metrics, reflecting a broader awareness of the need for more accurate evaluative measures in an expanding field [2][3].

The key difference between these two types of metrics lies in their evaluation focus: n-gram-based metrics emphasize lexical matching and surface-level overlap, which can overlook the contextual and semantic richness of the generated content. In contrast, transformer model-based metrics aim to provide a deeper understanding by utilizing machine learning models that comprehend semantic relations, intent, and context across various linguistic constructs [4][5]. 

In summary, while n-gram-based metrics like ROUGE and BLEU serve as essential tools for quick evaluations, their limitations necessitated the shift towards more advanced evaluation methods utilizing transformer models, which offer a more comprehensive perspective on the quality of NLG outputs.

1. [1]:  https://ar5iv.org/html/2303.04048, [2303.04048] Is ChatGPT a Good NLG Evaluator? A Preliminary Study
2. [2]:  https://ar5iv.org/html/2008.12009, [2008.12009] A Survey of Evaluation Metrics Used for NLG Systems
3. [3]:  https://ar5iv.org/html/2008.12009, [2008.12009] A Survey of Evaluation Metrics Used for NLG Systems
4. [4]:  https://ar5iv.org/html/2408.09169, [2408.09169] Automatic Metrics in Natural Language Generation: A Survey of Current Evaluation Practices
5. [5]:  https://ar5iv.org/html/2008.12009, [2008.12009] A Survey of Evaluation Metrics Used for NLG Systems
---
1. [1]:  Passage ID 1: automatic NLG metric can effectively indicate the quality of the textual generations and thus can save lots of human labor from conducting human evaluation. Therefore, it is vital to design automatic evaluation metrics for NLG tasks, e.g., text summarization, story generation, data-to-text generation, machine translation, and many others. Generally, the score that points out how well the systems perform on each task is computed by comparing the system texts with one or more reference texts for semantic matching. In the literature, the metrics can be roughly categorized into four types:nùëõn-gram-based Metrics.Essentially, the nùëõn-gram-based metrics aim to measure the lexical overlap between a generated text and a reference text. The standard nùëõn-gram overlap-based metrics generally include ROUGE¬†Lin (2004), BLEU¬†Papineni et¬†al. (2002), Distinct-n¬†Li et¬†al. (2016), and METEOR¬†Denkowski and Lavie (2011). For example, ROUGE is the dominant metric in the summarization evaluation area.
2. [2]:  Passage ID 2: the state of the art in several existing NLG tasks but has also facilitated researchers to explore various newer NLG tasks such as image captioning. Such rapid progress in NLG has necessitated the development of accurate automatic evaluation metrics that would allow us to track the progress in the field of NLG. However, unlike classification tasks, automatically evaluating NLG systems in itself is a huge challenge. Several works have shown that early heuristic-based metrics such as BLEU, ROUGE are inadequate for capturing the nuances in the different NLG tasks. The expanding number of NLG models and the shortcomings of the current metrics has led to a rapid surge in the number of evaluation metrics proposed since 2014. Moreover, various evaluation metrics have shifted from using pre-determined heuristic-based formulae to trained transformer models. This rapid change in a relatively short time has led to the need for a survey of the existing NLG metrics to help existing and new
3. [3]:  Passage ID 3: 2019). This rapid surge in a relatively short time has lead to the need for a survey of existing NLG metrics. Such a survey would help existing and new researchers to quickly come up to speed with the developments that have happened in the last few years.1.1. Goals of this surveyThe goals of this survey can be summarised as follows:‚Ä¢Highlighting challenges in evaluating NLG systems: The first goal of this work is to make the readers aware that evaluating NLG systems is indeed a challenging task. To do so, in section 2 we first introduce popular NLG tasks ranging from machine translation to image captioning. For each task, we provide examples containing an input coupled with correct and incorrect responses. Using these examples, we show that distinguishing between correct and incorrect responses is a nuanced task requiring knowledge about the language, the domain and the task at hand. Further, in section 3 we provide a list of factors to be considered while evaluating NLG
4. [4]:  Passage ID 4: and Physics, Prague, CzechiaAdarsa SivaprasadUniversity of Aberdeen, Aberdeen, Scotland, Untied KingdomAbstractAutomatic metrics are extensively used to evaluate natural language processing systems.However, there has been increasing focus on how they are used and reported by practitioners within the field.In this paper, we have conducted a survey on the use of automatic metrics, focusing particularly on natural language generation (NLG) tasks. We inspect which metrics are used as well as why they are chosen and how their use is reported. Our findings from this survey reveal significant shortcomings, including inappropriate metric usage, lack of implementation details and missing correlations with human judgements. We conclude with recommendations that we believe authors should follow to enable more rigour within the field.Automatic Metrics in Natural Language Generation: A Survey of Current Evaluation Practices1 IntroductionEvaluation practices in the field
5. [5]:  Passage ID 5: heuristic-based formulae to trained transformer models. This rapid change in a relatively short time has led to the need for a survey of the existing NLG metrics to help existing and new researchers to quickly come up to speed with the developments that have happened in NLG evaluation in the last few years. Through this survey, we first wish to highlight the challenges and difficulties in automatically evaluating NLG systems. Then, we provide a coherent taxonomy of the evaluation metrics to organize the existing metrics and to better understand the developments in the field. We also describe the different metrics in detail and highlight their key contributions. Later, we discuss the main shortcomings identified in the existing metrics and describe the methodology used to evaluate evaluation metrics. Finally, we discuss our suggestions and recommendations on the next steps forward to improve the automatic evaluation metrics.Automatic Evaluation metrics, Abstractive summarization,