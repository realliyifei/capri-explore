# A Survey of Evaluation Methods of Generated Medical Textual Reports

## Question

What are the two most popular types of automatic evaluation metrics in Natural Language Generation (NLG), and how do they differ?

## URLs

1. https://ar5iv.org/html/2408.09169. [2408.09169] Automatic Metrics in Natural Language Generation: A Survey of Current Evaluation Practices
2. https://ar5iv.org/html/2403.14275. [2403.14275] Is Reference Necessary in the Evaluation of NLG Systems? When and Where?
3. https://ar5iv.org/html/2402.01383. [2402.01383] LLM-based NLG Evaluation: Current Status and Challenges
4. https://ar5iv.org/html/2203.11131. [2203.11131] Towards Explainable Evaluation Metrics for Natural Language Generation
5. https://ar5iv.org/html/2410.16834. [2410.16834] Analyzing and Evaluating Correlation Measures in NLG Meta-Evaluation
6. https://ar5iv.org/html/2410.10724. [2410.10724] Large Language Models Are Active Critics in NLG Evaluation
7. https://ar5iv.org/html/2305.15067. [2305.15067] Not All Metrics Are Guilty: Improving NLG Evaluation with LLM Paraphrasing
8. https://ar5iv.org/html/2008.12009. [2008.12009] A Survey of Evaluation Metrics Used for NLG Systems
9. https://ar5iv.org/html/2310.02040. [2310.02040] Jury: A Comprehensive Evaluation Toolkit
10. https://ar5iv.org/html/2303.04048. [2303.04048] Is ChatGPT a Good NLG Evaluator? A Preliminary Study

## Answer (I don't know?: ‚úó)

The two most popular types of automatic evaluation metrics in Natural Language Generation (NLG) are n-gram-based metrics and the newer transformer-based metrics. 

1. **n-gram-based Metrics**: These metrics focus on measuring lexical overlap between generated texts and reference texts. The most well-known n-gram-based metrics include ROUGE, BLEU, Distinct-n, and METEOR. For instance, ROUGE is predominantly used in the field of text summarization to evaluate the quality of generated summaries by comparing them against one or more reference summaries. These metrics primarily assess how many n-grams (which are contiguous sequences of n items from a given sample of text) overlap between the generated and reference texts, thereby focusing on surface-level similarities in word choice and sequence [1].

2. **Transformer-based Metrics**: In contrast, the newer metrics that have emerged since 2014 utilize trained transformer models, which are designed to capture the semantic nuances of generated texts more effectively. Unlike the heuristic-based n-gram metrics that often miss deeper contextual information, transformer-based metrics aim to evaluate the quality of NLG output by understanding the meaning rather than just the surface form. These metrics are considered more effective in capturing the complexities and variations across different NLG tasks, such as generating coherent paragraphs or accurate image captions [2][5].

The primary difference between these two types of metrics lies in their approach to evaluation: n-gram-based metrics primarily rely on lexical overlap and are typically limited in their ability to understand the contextual or semantic depth of texts. In contrast, transformer-based metrics leverage advanced neural network architectures to analyze and compare generated outputs against references in terms of semantic meaning, thereby providing a more nuanced evaluation of textual quality [2][5]. 

This evolution towards more sophisticated metrics reflects the growing complexity of NLG tasks and highlights the challenges faced in the evaluation process, prompting a need for robust automatic metrics that can keep pace with advancements in the field [3][4].

1. [1]:  https://ar5iv.org/html/2303.04048, [2303.04048] Is ChatGPT a Good NLG Evaluator? A Preliminary Study
2. [2]:  https://ar5iv.org/html/2008.12009, [2008.12009] A Survey of Evaluation Metrics Used for NLG Systems
3. [3]:  https://ar5iv.org/html/2008.12009, [2008.12009] A Survey of Evaluation Metrics Used for NLG Systems
4. [4]:  https://ar5iv.org/html/2408.09169, [2408.09169] Automatic Metrics in Natural Language Generation: A Survey of Current Evaluation Practices
5. [5]:  https://ar5iv.org/html/2008.12009, [2008.12009] A Survey of Evaluation Metrics Used for NLG Systems
---
1. [1]:  Passage ID 1: automatic NLG metric can effectively indicate the quality of the textual generations and thus can save lots of human labor from conducting human evaluation. Therefore, it is vital to design automatic evaluation metrics for NLG tasks, e.g., text summarization, story generation, data-to-text generation, machine translation, and many others. Generally, the score that points out how well the systems perform on each task is computed by comparing the system texts with one or more reference texts for semantic matching. In the literature, the metrics can be roughly categorized into four types:nùëõn-gram-based Metrics.Essentially, the nùëõn-gram-based metrics aim to measure the lexical overlap between a generated text and a reference text. The standard nùëõn-gram overlap-based metrics generally include ROUGE¬†Lin (2004), BLEU¬†Papineni et¬†al. (2002), Distinct-n¬†Li et¬†al. (2016), and METEOR¬†Denkowski and Lavie (2011). For example, ROUGE is the dominant metric in the summarization evaluation area.
2. [2]:  Passage ID 2: the state of the art in several existing NLG tasks but has also facilitated researchers to explore various newer NLG tasks such as image captioning. Such rapid progress in NLG has necessitated the development of accurate automatic evaluation metrics that would allow us to track the progress in the field of NLG. However, unlike classification tasks, automatically evaluating NLG systems in itself is a huge challenge. Several works have shown that early heuristic-based metrics such as BLEU, ROUGE are inadequate for capturing the nuances in the different NLG tasks. The expanding number of NLG models and the shortcomings of the current metrics has led to a rapid surge in the number of evaluation metrics proposed since 2014. Moreover, various evaluation metrics have shifted from using pre-determined heuristic-based formulae to trained transformer models. This rapid change in a relatively short time has led to the need for a survey of the existing NLG metrics to help existing and new
3. [3]:  Passage ID 3: 2019). This rapid surge in a relatively short time has lead to the need for a survey of existing NLG metrics. Such a survey would help existing and new researchers to quickly come up to speed with the developments that have happened in the last few years.1.1. Goals of this surveyThe goals of this survey can be summarised as follows:‚Ä¢Highlighting challenges in evaluating NLG systems: The first goal of this work is to make the readers aware that evaluating NLG systems is indeed a challenging task. To do so, in section 2 we first introduce popular NLG tasks ranging from machine translation to image captioning. For each task, we provide examples containing an input coupled with correct and incorrect responses. Using these examples, we show that distinguishing between correct and incorrect responses is a nuanced task requiring knowledge about the language, the domain and the task at hand. Further, in section 3 we provide a list of factors to be considered while evaluating NLG
4. [4]:  Passage ID 4: and Physics, Prague, CzechiaAdarsa SivaprasadUniversity of Aberdeen, Aberdeen, Scotland, Untied KingdomAbstractAutomatic metrics are extensively used to evaluate natural language processing systems.However, there has been increasing focus on how they are used and reported by practitioners within the field.In this paper, we have conducted a survey on the use of automatic metrics, focusing particularly on natural language generation (NLG) tasks. We inspect which metrics are used as well as why they are chosen and how their use is reported. Our findings from this survey reveal significant shortcomings, including inappropriate metric usage, lack of implementation details and missing correlations with human judgements. We conclude with recommendations that we believe authors should follow to enable more rigour within the field.Automatic Metrics in Natural Language Generation: A Survey of Current Evaluation Practices1 IntroductionEvaluation practices in the field
5. [5]:  Passage ID 5: heuristic-based formulae to trained transformer models. This rapid change in a relatively short time has led to the need for a survey of the existing NLG metrics to help existing and new researchers to quickly come up to speed with the developments that have happened in NLG evaluation in the last few years. Through this survey, we first wish to highlight the challenges and difficulties in automatically evaluating NLG systems. Then, we provide a coherent taxonomy of the evaluation metrics to organize the existing metrics and to better understand the developments in the field. We also describe the different metrics in detail and highlight their key contributions. Later, we discuss the main shortcomings identified in the existing metrics and describe the methodology used to evaluate evaluation metrics. Finally, we discuss our suggestions and recommendations on the next steps forward to improve the automatic evaluation metrics.Automatic Evaluation metrics, Abstractive summarization,