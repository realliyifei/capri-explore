# Automatic Speech Recognition Datasets in Cantonese: A Survey and New Dataset

## Question

How is data pre-processed and tokenized in Cantonese automatic speech recognition datasets?

## URLs

1. https://ar5iv.org/html/2201.02419. [2201.02419] Automatic Speech Recognition Datasets in Cantonese: A Survey and New Dataset
2. https://ar5iv.org/html/2306.11252. [2306.11252] HK-LegiCoST: Leveraging Non-Verbatim Transcripts for Speech Translation
3. https://ar5iv.org/html/2305.13516. [2305.13516] Scaling Speech Technology to 1,000+ Languages
4. https://ar5iv.org/html/2407.04051. [2407.04051] FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs
5. https://ar5iv.org/html/2306.12925. [2306.12925] AudioPaLM: A Large Language Model That Can Speak and Listen
6. https://ar5iv.org/html/2309.07081. [2309.07081] Can Whisper perform speech-based in-context learning?
7. https://ar5iv.org/html/2405.14259. [2405.14259] Let‚Äôs Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Multi-modal Text Recognition
8. https://ar5iv.org/html/2406.07801. [2406.07801] PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models
9. https://ar5iv.org/html/2305.04672. [2305.04672] Augmented Datasheets for Speech Datasets and Ethical Decision-Making

## Answer (I don't know?: ‚úì)

I don't know. The context provided does not contain specific information about the data pre-processing and tokenization methods used in the Cantonese automatic speech recognition (ASR) datasets. The passages focus primarily on the creation of the Multi-Domain Cantonese Corpus (MDCC), its evaluation against existing datasets, and discussions on the structure and purpose of the datasets, but they do not delve into the technical details of data pre-processing or tokenization techniques employed in the ASR process. Without this specific information, it is not possible to provide a complete answer regarding how data is pre-processed and tokenized in these Cantonese ASR datasets.

1. [1]:  https://ar5iv.org/html/2201.02419, [2201.02419] Automatic Speech Recognition Datasets in Cantonese: A Survey and New Dataset
2. [2]:  https://ar5iv.org/html/2201.02419, [2201.02419] Automatic Speech Recognition Datasets in Cantonese: A Survey and New Dataset
3. [3]:  https://ar5iv.org/html/2201.02419, [2201.02419] Automatic Speech Recognition Datasets in Cantonese: A Survey and New Dataset
4. [4]:  https://ar5iv.org/html/2201.02419, [2201.02419] Automatic Speech Recognition Datasets in Cantonese: A Survey and New Dataset
5. [5]:  https://ar5iv.org/html/2405.14259, [2405.14259] Let‚Äôs Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Multi-modal Text Recognition
---
1. [1]:  Passage ID 1: } else { localStorage.setItem("ar5iv_theme", "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Automatic Speech Recognition Datasets in Cantonese: A Survey and New DatasetAbstractAutomatic speech recognition (ASR) on low resource languages improves the access of linguistic minorities to technological advantages provided by artificial intelligence (AI). In this paper, we address the problem of data scarcity for the Hong Kong Cantonese language by creating a new Cantonese dataset. Our dataset, Multi-Domain Cantonese Corpus (MDCC), consists of 73.6 hours of clean read speech paired with transcripts, collected from Cantonese audiobooks from Hong Kong. It comprises philosophy, politics, education, culture, lifestyle and family domains, covering a wide range of topics. We also review all existing Cantonese datasets and analyze them according to their speech type, data source, total size and availability. We further conduct experiments
2. [2]:  Passage ID 2: MDCC for the ASR research in the Cantonese language, which consists of 73.6 hours of clean read speech. We evaluate our dataset and compare it with the Common Voice zh-HK dataset using the Fairseq S2T Transformer model, and confirm that the results indicate the effectiveness of our proposed dataset. Our model trained on joint data outperforms Wav2Vec2-Large model on Cantonese dataset. 777 https://huggingface.co/ctl/wav2vec2-large-xlsr-cantonese The data was not compared in the experiment section since the huggingface uses different splits of Common Voice zh-HK corpus. For future work we plan to collect data from more audiobooks to enrich our dataset. In addition, we will create new Cantonese ASR corpora from different sources such as meetings and movies. Another future work direction is performing more experiments that combine the performance of the MDCC with multilingual datasets. We believe that our dataset and analysis can pave the way for future research works on the Cantonese ASR
3. [3]:  Passage ID 3: is a massive-multilingual collection of transcribed speech collected and validated via Mozilla‚Äôs Common Voice initiative. The speakers are required to read sentences from Wikipedia and the annotators verify each sentence. We use 96.0 hour split of verified Cantonese utterances in our experiments. The detailed data statistics are shown in Section 5. The dataset is available on the Common Voice website.Although each of the existing corpora has advantages, not all of them are suitable for developing Cantonese ASR systems. None of the corpora except Common Voice zh-HK are large enough for data-intensive ASR model fine-tuning. Furthermore, even for Common Voice zh-HK, empirical experiments based on recent deep learning models are limited. To fill this research gap, we propose MDCC to enrich ASR data resources in Cantonese. Furthermore, we implement a state-of-the-art ASR model and report its performance on the Common Voice zh-HK dataset and MDCC.3.¬†¬†¬†Corpus CreationThis section
4. [4]:  Passage ID 4: a wide range of topics. We also review all existing Cantonese datasets and analyze them according to their speech type, data source, total size and availability. We further conduct experiments with Fairseq S2T Transformer, a state-of-the-art ASR model, on the biggest existing dataset, Common Voice zh-HK, and our proposed MDCC, and the results show the effectiveness of our dataset. In addition, we create a powerful and robust Cantonese ASR model by applying multi-dataset learning on MDCC and Common Voice zh-HK.Keywords:‚ÄâSpeech Corpus, Hong Kong Cantonese, Automatic Speech Recognition System\newciteslanguageresourceLanguage ResourcesAutomatic Speech Recognition Datasets in Cantonese: A Survey and New DatasetTiezheng Yu‚ãÜ‚Ä†‚Ä†thanks: ‚ãÜ These authors contributed equally., Rita Frieske‚ãÜ, Peng Xu‚ãÜ ‚Ä†‚Ä†‚Ä†thanks: ‚Ä† The work was done when the author was studying in The Hong Kong University of Science and Technology., Samuel Cahyawijaya‚ãÜ,Cheuk Tung Shadow Yiu, Holy
5. [5]:  Passage ID 5: speech corpus. We deliberately choose Cantonese subset for evaluation as the language is homophonous and tonal. NTUML2021 corpus consists of lecture recordings from the ‚ÄúMachine Learning‚Äù course at National Taiwan University in 2021, with corresponding transcriptions and English translations labeled by over 20 bilingual native Chinese speakers. FormosaSpeech corpus includes Chinese recordings of Taiwanese accents amassing up to 6.4 hours of audio-text pairs. NAF consists of images from U.S national archives with labelled bounding boxes and annotations, ideal for evaluating OCR performance.For all ASR experiments, we report word error rate (WER) for evaluations on English datasets, and mix error rate (MER) for that on Chinese datasets. For OCR experiments, we report character error rate (CER) and exact match (EM). For exact match, we require that the recognized texts fully matches the ground truth. All GFD fused models can be run on a single A6000 GPU. We fix r=0.2ùëü0.2r=0.2 in