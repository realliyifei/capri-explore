# Survey of Low-Resource Machine Translation

## Question

How do low-resource machine translation systems leverage synthetic data from related languages?

## URLs

1. https://ar5iv.org/html/2406.17363. [2406.17363] Leveraging Synthetic Audio Data for End-to-End Low-Resource Speech Translation
2. https://ar5iv.org/html/2307.05779. [2307.05779] Neural Machine Translation Data Generation and Augmentation using ChatGPT
3. https://ar5iv.org/html/2411.11295. [2411.11295] Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation
4. https://ar5iv.org/html/2409.02712. [2409.02712] A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations
5. https://ar5iv.org/html/2306.11372. [2306.11372] Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts
6. https://ar5iv.org/html/2404.07503. [2404.07503] Best Practices and Lessons Learned on Synthetic Data for Language Models
7. https://ar5iv.org/html/2408.12079. [2408.12079] High-Quality Data Augmentation for Low-Resource NMT: Combining a Translation Memory, a GAN Generator, and Filtering
8. https://ar5iv.org/html/2410.14815. [2410.14815] Adapting Multilingual LLMs to Low-Resource Languages using Continued Pre-training and Synthetic Corpus
9. https://ar5iv.org/html/2105.13065. [2105.13065] Extremely low-resource machine translation for closely related languages
10. https://ar5iv.org/html/1911.03362. [1911.03362] Domain, Translationese and Noise in Synthetic Data for Neural Machine Translation

## Answer (I don't know?: ✗)

Low-resource machine translation (MTE) systems often face challenges due to a lack of sufficient parallel corpora for training effective models. To address this issue, these systems leverage synthetic data generated from high-resource languages, a methodology that aids in enhancing translation performance for less commonly spoken languages.

One effective approach to generating synthetic data is the use of **unsupervised prompting methods**. This involves assembling synthetic exemplars from a diverse set of high-resource languages to prompt large language models (LLMs) to translate low-resource languages into English. By employing this method, researchers have demonstrated that LLMs can perform translations between English and numerous low-resource languages effectively. Specifically, their unsupervised method performed comparably to supervised few-shot learning strategies across various models and even outran supervised prompting in certain cases by a notable margin in chrF++ scores, particularly for low-resource languages [1].

In addition to unsupervised prompting, the **back-translation (BT)** technique is another key method used in low-resource MTE systems. This technique creates a synthetic corpus from monolingual data, generating translations that can serve as training data for a target-to-source machine translation model. Essentially, a target-to-source model produces translations of monolingual data, which then augments the dataset for the source-to-target model. This process not only utilizes existing monolingual data but also helps improve translation by enhancing the decoder's effectiveness, ultimately leading to better output [4].

Moreover, challenges in low-resource settings can be mitigated by techniques that combine these synthetic methods with the utilization of context-aware retrieval systems. For example, integrating retrieval-augmented generation (RAG) models combines dictionary entries and keyword-to-document mappings. This ensures that even when leveraging synthetic data, the translations remain accurate and contextually relevant, thereby enhancing the training process for low-resource languages [5].

Furthermore, the architecture of MTE systems can include mechanisms to mitigate the lack of diversity in the synthetic data generated by existing models. Research suggests that prompt engineering might help improve this diversity, potentially leading to better translations [3]. Innovations in language models and linguistic understanding are critical, as they can contribute to the generation of more varied and effective synthetic training data.

In summary, low-resource machine translation systems leverage synthetic data from related high-resource languages primarily through unsupervised prompting and back-translation methods. These strategies allow for the augmentation of training datasets, enhancing translation quality and performance despite initial resource limitations. By combining these synthetic approaches with advanced retrieval systems, researchers can ensure that MTE systems not only produce better translations but also maintain a context-aware translation process, thus improving overall outcomes in low-resource language settings.

1. [1]:  https://ar5iv.org/html/2306.11372, [2306.11372] Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts
2. [2]:  https://ar5iv.org/html/2408.12079, [2408.12079] High-Quality Data Augmentation for Low-Resource NMT: Combining a Translation Memory, a GAN Generator, and Filtering
3. [3]:  https://ar5iv.org/html/2307.05779, [2307.05779] Neural Machine Translation Data Generation and Augmentation using ChatGPT
4. [4]:  https://ar5iv.org/html/2105.13065, [2105.13065] Extremely low-resource machine translation for closely related languages
5. [5]:  https://ar5iv.org/html/2411.11295, [2411.11295] Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation
---
1. [1]:  Passage ID 1: Moreover, competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance. To elicit LLMs’ ability onto low-resource languages without any supervised data, we propose to assemble synthetic exemplars from a diverse set of high-resource languages to prompt the LLMs to translate from any language into English. These prompts are then used to create intra-lingual exemplars to perform tasks in the target languages. Our unsupervised prompting method performs on par with supervised few-shot learning in LLMs of different sizes for translations between English and 13 Indic and 21 African low-resource languages. We also show that fine-tuning a 7B model on data generated from our method helps it perform competitively with a 175B model. In non-English translation tasks, our method even outperforms supervised prompting by up to 3 chrF++ in many low-resource languages.
2. [2]:  Passage ID 2: Language Processing (NLP) [16].However, today, most of the researchers design their models based on transformer [19].The transformer model, based on attention mechanisms, allows more flexible focus on different parts of the input sentence, further improving translation accuracy.It also set new benchmarks for most other tasks in the field of NLP.Low-resource translation [9] refers to the situation where machine translation encounters a shortage of parallel corpora for training models.This commonly occurs for languages with limited linguistic resources or smaller speaker populations, leading to difficulties in achieving effective translation performance.A Translation Memory (TM) is a repository that archives pairs of source sentences along with their corresponding translations.Recent studies have validated the beneficial impact of TM on enhancing NMT models.This enhancement has been demonstrated through various approaches, including concatenating both the source and target
3. [3]:  Passage ID 3: well in naturally low resource settings, where it is not possible for ChatGPT to generate synthetic parallel data.8 ConclusionsOur experiments demonstrate a possible issue with using large language models to generate synthetic data: the lack of diversity in generated data. It is becoming evident that many of the capabilities of these models are limited only by the specific prompts used to communicate with them. More research is necessary to determine whether prompt engineering can improve the diversity issue. For example, using more sentences during the ’Assistant’ few shot phase in section 3.1.1. As new and varied prompt-based models are released, the problem may also naturally be solved by better language models and more thorough linguistic understanding on the part of the models.Our experiments do show, however, that augmenting natural data with entirely synthetic data shows promise for training machine translators. Despite a domain mismatch, translation quality improved,
4. [4]:  Passage ID 4: system, but reported that high-resource pairs see a performance degradation and lower-resourced pairs see a performance increase.2.2 Back-translation for low-resource MTEvery sentence is essential for neural machine translation in a low-resource machine translation environment. One popular way to leverage monolingual data is by creating a synthetic corpus via a method called back-translation (BT). Traditional BT Sennrich et al. (2016) is easy to use and requires training a target-to-source MT system to generate translations of the monolingual data, which are used as training data for the source-to-target MT model. This means that traditional BT requires two NMT models, where one generates synthetic data for the other. The idea behind BT is that the monolingual human data on the target side improves the quality of the decoder to generate better output for the language and the synthetic source helps as a data augmentation tactic.Closely related to back-translation is a method
5. [5]:  Passage ID 5: examples within privacy policy question-answering datasets, enhancing the training process. However, these methods often rely on external data, which poses challenges when the data is sensitive and cannot be accessed due to privacy concerns, or when computational resources are limited.3.5 LLMs Empowered Machine Translation Task3.5.1 MethodologyWe implement a RAG LLM low resource language translator by combining retrieval-based techniques with LLMs to ensure accurate and context-aware translations. The overall architecture is shown in Figure 1. The system utilizes dictionary entries, which are indexed through two complementary approaches: keyword-to-document mappings and vector embeddings. Key-to-document mappings in systems refer to a process where keywords are linked directly to the documents or data entries that contain or are relevant to those keywords. The keyword retriever will retrieve corresponding documents according to the key-to-document mapping, if the keyword is