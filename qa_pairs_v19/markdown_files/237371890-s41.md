# Survey of Low-Resource Machine Translation

## Question

How do low-resource machine translation systems leverage synthetic data from related languages?

## URLs

1. https://ar5iv.org/html/2406.17363
2. https://ar5iv.org/html/2307.05779
3. https://ar5iv.org/html/2411.11295
4. https://ar5iv.org/html/2409.02712
5. https://ar5iv.org/html/2306.11372
6. https://ar5iv.org/html/2404.07503
7. https://ar5iv.org/html/2408.12079
8. https://ar5iv.org/html/2410.14815
9. https://ar5iv.org/html/2105.13065
10. https://ar5iv.org/html/1911.03362

## Answer

## I don't know?: False

Low-resource machine translation (MT) systems can effectively leverage synthetic data from related languages through several methodologies that take advantage of linguistic similarities and data augmentation techniques. One prominent strategy is the use of multilingual training which allows low-resource languages to be trained jointly with higher-resource languages. This joint training can harness shared linguistic features, such as vocabulary and grammatical structures, thus boosting the MT capabilities of low-resource languages [1][5].

One approach specifically mentioned is back-translation, where existing high-resource language data is translated into the low-resource target language and then used to create synthetic training datasets. For example, in the development of NMT systems for Ladino, a low-resource language, data was gathered using English-to-Spanish and Turkish-to-Spanish translations because Spanish shares linguistic characteristics with Ladino. Consequently, this enabled the creation of English-to-Synthetic-Ladino and Turkish-to-Synthetic-Ladino datasets [1].

Additionally, another effective method involves the assembly of synthetic exemplars from high-resource languages. Such exemplars can serve as prompts or training examples for low-resource languages, facilitating their translation capabilities without the need for supervised data. This unsupervised prompting method has been shown to perform comparably with supervised few-shot learning, allowing models to translate between English and various low-resource languages, including many Indic and African languages [3]. This highlights how leveraging data from related languages can help mitigate the challenges of data scarcity often faced in low-resource contexts.

Moreover, the use of multilingual lexicons and word alignments also facilitates the translation process, providing valuable linguistic resources to assist low-resource language pairs. However, the reliance on these additional tools can complicate extensibility across different languages, prompting a need for more streamlined methods [4]. This indicates a balance that low-resource systems must strike between using additional resources and maintaining adaptability.

Furthermore, languages sharing a similar typology or linguistic family can directly influence one another, allowing for concepts such as loanwords to be integrated into low-resource languages, which further enriches the synthetic data approach [5]. By constructing hybrid datasets that combine information from related high-resource languages, low-resource MT systems can improve their overall performance and capability to generalise to practical applications.

In summary, low-resource machine translation systems leverage synthetic data from related languages through multilingual training, back-translation methods, and the creation of synthetic exemplars from high-resource languages, thereby facilitating improved translation capabilities despite the intrinsic data scarcity. These strategies collectively enhance the robustness and efficacy of low-resource machine translation efforts.

1. [1]:  https://ar5iv.org/html/2401.14559, No Title
2. [2]:  https://ar5iv.org/html/2409.04512, [2409.04512] Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages
3. [3]:  https://ar5iv.org/html/2306.11372, [2306.11372] Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts
4. [4]:  https://ar5iv.org/html/2408.12780, No Title
5. [5]:  https://ar5iv.org/html/2107.04239, No Title
---
1. [1]:  Passage ID 1: of generic texts. Similarly, in our work (Öktem et al.,, 2022), we built NMT systems for a very low-resource language, Ladino, utilising diverse data augmentation approaches, employing rule-based MT and back-translation. We collected data in English-to-Spanish and Turkish-to-Spanish language pairs, since Spanish shares a wide range of linguistic characteristics with Ladino. Then, we used a rule-based system to translate Spanish to Ladino. This process resulted in English-to-Synthetic-Ladino and Turkish-to-Synthetic-Ladino datasets, which I used later for building NMT models. Furthermore, I have experience with building a multilingual MT model222Notes on multilingual MT: https://blog.machinetranslation.io/multilingual-nmt/ for 10 low-resource Indic languages.333MT demo: https://www.machinetranslation.io/ Multilingual models can exploit the similarity between languages and particularly benefitlow-resource languages (Imankulova et al.,, 2019; Liu et al.,, 2020; Gala et al.,,
2. [2]:  Passage ID 2: and translating human language across a wide range of tasks and languages. However since high-resource languages like English, Spanish, and Chinese have access to a wealth of annotated datasets and linguistic resources, most of this development has been focused on those languages. Low-resource languages, on the other hand, have a lot more difficulties since they lack large-scale and high-quality datasets Thabah and Purkayastha (2021). Training effective NLP models are challenging due to this data scarcity, which frequently leads to subpar performance and poor generalization. Low-resource languages have distinct grammatical structures, linguistic diversity, and cultural quirks that make it more difficult to create accurate models and limit their use in practical contexts Yang et al. (2023). Multilingual LLMs have limitations on processing the prompts in low-resource languages Sanjib Narzary (2022). This is because the amount of data used to train or fine-tune the model is very less. As
3. [3]:  Passage ID 3: Moreover, competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance. To elicit LLMs’ ability onto low-resource languages without any supervised data, we propose to assemble synthetic exemplars from a diverse set of high-resource languages to prompt the LLMs to translate from any language into English. These prompts are then used to create intra-lingual exemplars to perform tasks in the target languages. Our unsupervised prompting method performs on par with supervised few-shot learning in LLMs of different sizes for translations between English and 13 Indic and 21 African low-resource languages. We also show that fine-tuning a 7B model on data generated from our method helps it perform competitively with a 175B model. In non-English translation tasks, our method even outperforms supervised prompting by up to 3 chrF++ in many low-resource languages.
4. [4]:  Passage ID 4: Neural Machine Translation (NMT) models, trained mainly on parallel data, in the latest WMT shared task (Kocmi et al., 2024).However, LLM translation for low-resource languages (LRLs) still lags significantly behind NMT models (Robinson et al., 2023; Zhu et al., 2024b). While the strong performance of LLMs on high-resource languages can be attributed to the skewed language distribution during pre-training and the unintentional consumption of parallel data at scale (Briakou et al., 2023), no such relief exists for LRLs.This leads to the main question motivating this paper: What would it take to adapt LLMs for low-resource MT?Recent work on LRL translation with LLMs has explored using resources like multilingual lexicons (Lu et al., 2023), word alignments (Mao and Yu, 2024) or linguistic tools (Zhang et al., 2024b). While effective, reliance on such tools hinders ease of extensibility across languages. Instead, in this work, we take inspiration from research done for high-resource
5. [5]:  Passage ID 5: for unsupervised NMT.3 Exploiting Data From Auxiliary LanguagesFigure 4: Overview of works exploiting data from auxiliary languagesHuman languages share similarities with each other in several aspects: (1) languages in the same/similar language family or typology may share similar writing script, word vocabulary, word order and grammar, (2) languages can influence each other, and a foreign word from another language can be incorporated into a language as it is (referred as loanword). Accordingly, corpora of related languages can be exploited to assist the translation between a low-resource language pair Dabre et al. (2020). The methods to leverage multilingual data into low-resource NMT can be categorized into several types: (1) multilingual training, where the low-resource language pair is jointly trained with other language pairs in one model Johnson et al. (2017), (2) transfer learning Zoph et al. (2016), where a parent NMT model usually containing rich-resource language