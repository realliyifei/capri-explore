# Survey of Low-Resource Machine Translation

## Question

How do low-resource machine translation systems leverage synthetic data from related languages?

## URLs

1. https://ar5iv.org/html/2406.17363. [2406.17363] Leveraging Synthetic Audio Data for End-to-End Low-Resource Speech Translation
2. https://ar5iv.org/html/2307.05779. [2307.05779] Neural Machine Translation Data Generation and Augmentation using ChatGPT
3. https://ar5iv.org/html/2411.11295. [2411.11295] Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation
4. https://ar5iv.org/html/2409.02712. [2409.02712] A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations
5. https://ar5iv.org/html/2306.11372. [2306.11372] Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts
6. https://ar5iv.org/html/2404.07503. [2404.07503] Best Practices and Lessons Learned on Synthetic Data for Language Models
7. https://ar5iv.org/html/2408.12079. [2408.12079] High-Quality Data Augmentation for Low-Resource NMT: Combining a Translation Memory, a GAN Generator, and Filtering
8. https://ar5iv.org/html/2410.14815. [2410.14815] Adapting Multilingual LLMs to Low-Resource Languages using Continued Pre-training and Synthetic Corpus
9. https://ar5iv.org/html/2105.13065. [2105.13065] Extremely low-resource machine translation for closely related languages
10. https://ar5iv.org/html/1911.03362. [1911.03362] Domain, Translationese and Noise in Synthetic Data for Neural Machine Translation

## Answer (I don't know?: ✗)

Low-resource machine translation (MT) systems can leverage synthetic data from related languages through various approaches that utilize language similarities and existing data in high-resource languages. One prominent method is the use of rule-based translation systems combined with data augmentation techniques. For instance, in the case of Ladino, a very low-resource language, a system was developed using English-to-Spanish and Turkish-to-Spanish data, capitalizing on the linguistic similarities between Spanish and Ladino to create synthetic datasets [2]. By translating Spanish into Ladino through a rule-based system, the researchers were able to generate English-to-Synthetic-Ladino and Turkish-to-Synthetic-Ladino datasets, which later served as training data for Neural Machine Translation (NMT) models.

Furthermore, leveraging monolingual data from high-resource languages allows researchers to adapt NMT systems for low-resource languages. A technique named NMT-Adapt combines strategies like denoising autoencoding, back-translation, and adversarial objectives to exploit monolingual data in facilitating translations into low-resource languages [1]. This approach demonstrates significant improvements in translation quality for seven different languages from various language families by utilizing linguistic overlap, which is particularly beneficial when only monolingual data is available alongside parallel data from related, high-resource languages.

The effectiveness of synthetic data in low-resource settings is also supported by methods such as transfer learning and back-translation. These methods help to automatically generate more training data, which is crucial in overcoming challenges related to data scarcity [3][4]. Studies indicate that by harnessing language similarities through these techniques, low-resource languages can achieve better translation performance despite their limited data. Therefore, the synthesis of data from related languages significantly enhances the capabilities of low-resource translation systems, facilitating improvements where traditional approaches may falter.

In conclusion, low-resource machine translation systems effectively utilize synthetic data generated from related high-resource languages through rule-based systems, data augmentation techniques, and monolingual data adaptation. These strategies enable such systems to enrich their training datasets and enhance translation accuracy, ultimately leading to improved performance in low-resource environments.

1. [1]:  https://ar5iv.org/html/2105.15071, No Title
2. [2]:  https://ar5iv.org/html/2401.14559, No Title
3. [3]:  https://ar5iv.org/html/2404.08259, No Title
4. [4]:  https://ar5iv.org/html/2404.08259, No Title
5. [5]:  https://ar5iv.org/html/2401.14559, No Title
---
1. [1]:  Passage ID 1: for training high-quality machine translation systems for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language. Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines.1 IntroductionWhile machine translation (MT) has made incredible strides due to the advent of deep neural machine translation (NMT) Sutskever et al. (2014); Bahdanau
2. [2]:  Passage ID 2: of generic texts. Similarly, in our work (Öktem et al.,, 2022), we built NMT systems for a very low-resource language, Ladino, utilising diverse data augmentation approaches, employing rule-based MT and back-translation. We collected data in English-to-Spanish and Turkish-to-Spanish language pairs, since Spanish shares a wide range of linguistic characteristics with Ladino. Then, we used a rule-based system to translate Spanish to Ladino. This process resulted in English-to-Synthetic-Ladino and Turkish-to-Synthetic-Ladino datasets, which I used later for building NMT models. Furthermore, I have experience with building a multilingual MT model222Notes on multilingual MT: https://blog.machinetranslation.io/multilingual-nmt/ for 10 low-resource Indic languages.333MT demo: https://www.machinetranslation.io/ Multilingual models can exploit the similarity between languages and particularly benefitlow-resource languages (Imankulova et al.,, 2019; Liu et al.,, 2020; Gala et al.,,
3. [3]:  Passage ID 3: the help of growing Large Language Models, more and more low-resource languages achieve better results through the presence of other languages. However, studies have shown that not all low-resource languages can benefit from multilingual systems, especially those with insufficient training and evaluation data. In this paper, we revisit state-of-the-art Neural Machine Translation techniques to develop automatic translation systems between German and Bavarian. We investigate conditions of low-resource languages such as data scarcity and parameter sensitivity and focus on refined solutions that combat low-resource difficulties and creative solutions such as harnessing language similarity. Our experiment entails applying Back-translation and Transfer Learning to automatically generate more training data and achieve higher translation performance. We demonstrate noisiness in the data and present our approach to carry out text preprocessing extensively. Evaluation was conducted using
4. [4]:  Passage ID 4: the help of growing Large Language Models, more and more low-resource languages achieve better results through the presence of other languages. However, studies have shown that not all low-resource languages can benefit from multilingual systems, especially those with insufficient training and evaluation data. In this paper, we revisit state-of-the-art Neural Machine Translation techniques to develop automatic translation systems between German and Bavarian. We investigate conditions of low-resource languages such as data scarcity and parameter sensitivity and focus on refined solutions that combat low-resource difficulties and creative solutions such as harnessing language similarity. Our experiment entails applying Back-translation and Transfer Learning to automatically generate more training data and achieve higher translation performance. We demonstrate noisiness in the data and present our approach to carry out text preprocessing extensively. Evaluation was conducted using
5. [5]:  Passage ID 5: I examined how to leverage language modelling techniques in general and LLMs in particular to improve translation features that involve human interaction and continuous feedback, such as adaptive MT, terminology-constrained MT, domain-aware automatic post-editing, auto-suggestion and auto-completion. In the second research question, I addressed a common scenario in the translation industry, namely receiving highly specialised projects, where there is hardly any parallel in-domain data. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. One way to address this question is through domain-specific text generation with LLMs and then fine-tuning an MT model using the generated bilingual in-domain synthetic data. Furthermore, this question overlaps with the first question, since real-time adaptive MT with LLMs can serve as an alternative approach to address in-domain data