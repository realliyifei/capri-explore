# A Survey on Dynamic Neural Networks for Natural Language Processing

## Question

How are confidence-based early exit strategies implemented in BERT inference models?

## URLs

1. https://ar5iv.org/html/2107.00175. [2107.00175] ELBERT: FAST ALBERT WITH CONFIDENCE-WINDOW BASED EARLY EXIT
2. https://ar5iv.org/html/2402.05948. [2402.05948] DE3-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical Networks
3. https://ar5iv.org/html/2312.11882. [2312.11882] ConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for Accelerating Language Models Inference
4. https://ar5iv.org/html/2405.15039. [2405.15039] CeeBERT: Cross-Domain Inference in Early Exit BERT
5. https://ar5iv.org/html/2006.04152. [2006.04152] BERT Loses Patience: Fast and Robust Inference with Early Exit
6. https://ar5iv.org/html/2204.06328. [2204.06328] HuBERT-EE: Early Exiting HuBERT for Efficient Speech Recognition

## Answer (I don't know?: ✗)

Confidence-based early exit strategies in BERT inference models are implemented by utilizing the model's prediction confidence, which is often quantified through metrics such as entropy or label score derived from the prediction probability distribution. These strategies allow the model to make dynamic decisions about whether to exit early during the inference process based on the confidence level of the predictions made at various stages of the model.

In confidence-based early exiting, models like DeeBERT and RightTool employ prediction confidence to establish criteria for exiting the inference process before reaching the model's final layer. If the model's confidence meets a pre-defined threshold, the inference can be halted early, reducing computational resources and time spent processing straightforward samples while not compromising significantly on accuracy [1] [2].

Specifically, the implementation involves adding internal classifiers at multiple layers of the BERT architecture. Each classifier can predict labels at different depths, offering a way to evaluate prediction correctness progressively. When the internal classifier's confidence for a certain sample is assessed and found to be sufficiently high, the model can exit early, avoiding unnecessary computations through the deeper layers, which are more resource-intensive to process [2] [3].

To ensure effectiveness, it is crucial to establish reliable thresholds for these confidence measures. The absence of ground-truth labels during inference presents challenges because it complicates the estimation of prediction correctness, which is critical for making effective exit decisions [2] [3]. Hence, various software implementations of early exiting attempt to optimize the thresholds and the criteria for when to trust early predictions, often involving techniques like model calibration and architecture refinement [4] [5].

In summary, confidence-based early exiting in BERT models is an effective strategy to enhance inference efficiency by dynamically assessing prediction confidence at various model layers, allowing for a significant reduction in computational load while maintaining model performance.

1. [1]:  https://ar5iv.org/html/2402.05948, [2402.05948] DE3-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical Networks
2. [2]:  https://ar5iv.org/html/2402.05948, [2402.05948] DE3-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical Networks
3. [3]:  https://ar5iv.org/html/2402.05948, [2402.05948] DE3-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical Networks
4. [4]:  https://ar5iv.org/html/2402.05948, [2402.05948] DE3-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical Networks
5. [5]:  https://ar5iv.org/html/2405.15039, [2405.15039] CeeBERT: Cross-Domain Inference in Early Exit BERT
---
1. [1]:  Passage ID 1: Dynamic Early ExitingEarly exiting, a parallel line of research for accelerating the inference of PLMs, is to dynamically stop inference for various input samples. DeeBERT [20], FastBERT [32], and RightTool [28] are pioneers in applying early exiting to accelerate the inference of BERT-style models. These studies employ the confidence level, such as entropy or label score of the prediction probability distribution, to estimate the correctness of early predictions for exit decision-making. On this basis, Lin et al. [33] extended confidence-based early exiting from classification tasks to sequence labeling tasks. To further improve the acceleration performance of early exiting models, one approach is to explore new exiting strategies for the inference stage. PABEE [21] uses the cross-layer consistency to determine early exiting. PCEE-BERT [23] employs a hybrid exiting strategy that combines confidence-based and patience-based strategies. BERxiT [22] learns to generate an exiting
2. [2]:  Passage ID 2: correctness) to establish the exiting criteria. According to the types of exiting indicators, there are mainly three early exiting strategies for the dynamic exiting of BERT models. The first strategy is confidence-based early exiting (e.g., Deebert [20], RightTool [28] and CascadeBERT [25]). It utilizes the model confidence, i.e., entropy or label score of the prediction probability distribution, to estimate the correctness of early predictions for exit decision-making. The second strategy is patience-based early exiting (e.g., PABEE [21] and LeeBERT [29]), relying on the cross-layer consistency to determine when to exit. Early exiting in the third type learns to generate an early exiting indicator that scores the prediction correctness (e.g., BERxiT [22]). However, a reliable exiting decision is to exit when the early predictions are sufficiently correct. Unfortunately, the absence of ground-truth labels during inference poses extreme challenges to estimating prediction correctness,
3. [3]:  Passage ID 3: the threshold, without incurring additional training costs. In Fig. 1, we illustrate early exiting based on BERT, where an internal classifier is added to each intermediate layer of BERT. This enables the early exiting of samples when the early predictions from these internal classifiers are deemed sufficiently correct, eliminating the need to traverse the entire model. This strategy employs adaptive inference to deal with easy samples with shallow layers of BERT and process difficult samples with deeper layers. This approach effectively mitigates the overthinking problem and accelerates model inference while maintaining high accuracy.A typical implementation for early exiting is to devise an early exiting indicator that reflects the correctness of early predictions (i.e., prediction correctness) to establish the exiting criteria. According to the types of exiting indicators, there are mainly three early exiting strategies for the dynamic exiting of BERT models. The first strategy
4. [4]:  Passage ID 4: consistency to determine early exiting. PCEE-BERT [23] employs a hybrid exiting strategy that combines confidence-based and patience-based strategies. BERxiT [22] learns to generate an exiting indicator that scores the correctness of early predictions. HASHEE [24] utilizes hash functions to assign each token to a fixed exiting layer, providing a novel static paradigm for early exiting. In addition, an alternative approach to improve acceleration performance is to strengthen the capability of internal classifiers through various techniques, such as model calibration, architecture refinement, and optimized training schemes. CascadeBERT [25] employs early exiting within multiple cascaded complete models, thus strengthening the representation capability of internal classifiers. Moreover, it formulates a difficulty-aware objective to calibrate the prediction probability distribution. LeeBERT [29] introduces a cross-layer distillation loss with learned weights to promote the training of
5. [5]:  Passage ID 5: the target task as used in the previous baselines (see Section 5.2), thus making Early Exit PLMs robust to domain changes and enhancing their efficiency.•In Section 4, we introduce CeeBERT, an upper confidence-based algorithm that relies solely on confidence scores for learning the optimal threshold to make exit decisions.•In Section 5, we evaluate CeeBERT on five different datasets drawn from diverse tasks to demonstrate its proficiency in improving inference latency compared to the state-of-the-art algorithms.2 Related workEarly exit methods are applied for various tasks such as image classification, image captioning and NLP tasks to reduce the computational resources and inference latency.Early exits in Image tasks:For image classification tasks, BranchyNet Teerapittayanon et al. (2016) uses classification entropy at each attached exit to decide whether to infer the sample at the side branch based on the entropy of prediction.Shallow-deepKaya et al. (2019)