# A Survey on Dynamic Neural Networks for Natural Language Processing

## Question

How are confidence-based early exit strategies implemented in BERT inference models?

## URLs

1. https://ar5iv.org/html/2107.00175
2. https://ar5iv.org/html/2402.05948
3. https://ar5iv.org/html/2312.11882
4. https://ar5iv.org/html/2405.15039
5. https://ar5iv.org/html/2006.04152
6. https://ar5iv.org/html/2204.06328

## Answer

Confidence-based early exit strategies in BERT inference models are implemented by utilizing the model's confidence scores, which typically include measures such as entropy or label score derived from the prediction probability distribution. This approach allows the model to make exit decisions based on the estimated correctness of early predictions, thereby potentially reducing the computational resources and inference time required for processing inputs [1][2].

In practice, these strategies involve adding internal classifiers at different layers of the BERT model. When processing an input, these classifiers provide early predictions, and if the confidence associated with these predictions meets a predefined threshold, the model can "exit" early without needing to evaluate all layers [3]. This mechanism is illustrated in the adaptive inference approach, where easier samples are handled by the shallower layers of the model while more complex samples, which may require deeper processing, can continue through additional layers [3].

Several specific implementations exemplify this strategy. Models like DeeBERT, RightTool, and CascadeBERT utilize confidence-based early exiting methods to ascertain the correctness of their predictions. They focus primarily on entropy and label scores to determine if an exit is warranted for given inputs [2][4]. These methods aim to maintain high accuracy while reducing the time spent in inference, thereby directly addressing the overthinking problem typically encountered in BERT's deeper layers, where non-essential computations can incur unnecessary latency [3].

Moreover, implementing these confidence-based strategies can involve refining the model through techniques like model calibration and optimizing training schemes to enhance the internal classifiers' capabilities. This step is crucial as it contributes to the reliability of the early exit decisions made during inference [4]. The ability to generate reliable early exiting indicators—scores that reflect the correctness of early predictions—is a critical component in ensuring that the system exits the computation at an optimal juncture, minimizing both resource use and latency [3][4].

Furthermore, other hybrids, such as PCEE-BERT, combine confidence-based strategies with patience-based strategies. This blend allows for a more dynamic approach to early exiting, wherein decision-making incorporates both the model's confidence on the current input and the consistency of predictions across layers [4]. Such comprehensive strategies enhance the robustness and efficiency of early exit PLMs, enabling them to adapt effectively to varied tasks and domain changes [5].

Overall, confidence-based early exit strategies are a promising avenue for improving BERT inference efficiency, balancing the need for accuracy with the advantages of faster processing times.

[1]: https://ar5iv.org/html/2402.05948, [2402.05948] DE3-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical Networks
[2]: https://ar5iv.org/html/2402.05948, [2402.05948] DE3-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical Networks
[3]: https://ar5iv.org/html/2402.05948, [2402.05948] DE3-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical Networks
[4]: https://ar5iv.org/html/2402.05948, [2402.05948] DE3-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical Networks
[5]: https://ar5iv.org/html/2405.15039, [2405.15039] CeeBERT: Cross-Domain Inference in Early Exit BERT

[1]: Passage ID 1: Dynamic Early ExitingEarly exiting, a parallel line of research for accelerating the inference of PLMs, is to dynamically stop inference for various input samples. DeeBERT [20], FastBERT [32], and RightTool [28] are pioneers in applying early exiting to accelerate the inference of BERT-style models. These studies employ the confidence level, such as entropy or label score of the prediction probability distribution, to estimate the correctness of early predictions for exit decision-making. On this basis, Lin et al. [33] extended confidence-based early exiting from classification tasks to sequence labeling tasks. To further improve the acceleration performance of early exiting models, one approach is to explore new exiting strategies for the inference stage. PABEE [21] uses the cross-layer consistency to determine early exiting. PCEE-BERT [23] employs a hybrid exiting strategy that combines confidence-based and patience-based strategies. BERxiT [22] learns to generate an exiting
[2]: Passage ID 2: correctness) to establish the exiting criteria. According to the types of exiting indicators, there are mainly three early exiting strategies for the dynamic exiting of BERT models. The first strategy is confidence-based early exiting (e.g., Deebert [20], RightTool [28] and CascadeBERT [25]). It utilizes the model confidence, i.e., entropy or label score of the prediction probability distribution, to estimate the correctness of early predictions for exit decision-making. The second strategy is patience-based early exiting (e.g., PABEE [21] and LeeBERT [29]), relying on the cross-layer consistency to determine when to exit. Early exiting in the third type learns to generate an early exiting indicator that scores the prediction correctness (e.g., BERxiT [22]). However, a reliable exiting decision is to exit when the early predictions are sufficiently correct. Unfortunately, the absence of ground-truth labels during inference poses extreme challenges to estimating prediction correctness,
[3]: Passage ID 3: the threshold, without incurring additional training costs. In Fig. 1, we illustrate early exiting based on BERT, where an internal classifier is added to each intermediate layer of BERT. This enables the early exiting of samples when the early predictions from these internal classifiers are deemed sufficiently correct, eliminating the need to traverse the entire model. This strategy employs adaptive inference to deal with easy samples with shallow layers of BERT and process difficult samples with deeper layers. This approach effectively mitigates the overthinking problem and accelerates model inference while maintaining high accuracy.A typical implementation for early exiting is to devise an early exiting indicator that reflects the correctness of early predictions (i.e., prediction correctness) to establish the exiting criteria. According to the types of exiting indicators, there are mainly three early exiting strategies for the dynamic exiting of BERT models. The first strategy
[4]: Passage ID 4: consistency to determine early exiting. PCEE-BERT [23] employs a hybrid exiting strategy that combines confidence-based and patience-based strategies. BERxiT [22] learns to generate an exiting indicator that scores the correctness of early predictions. HASHEE [24] utilizes hash functions to assign each token to a fixed exiting layer, providing a novel static paradigm for early exiting. In addition, an alternative approach to improve acceleration performance is to strengthen the capability of internal classifiers through various techniques, such as model calibration, architecture refinement, and optimized training schemes. CascadeBERT [25] employs early exiting within multiple cascaded complete models, thus strengthening the representation capability of internal classifiers. Moreover, it formulates a difficulty-aware objective to calibrate the prediction probability distribution. LeeBERT [29] introduces a cross-layer distillation loss with learned weights to promote the training of
[5]: Passage ID 5: the target task as used in the previous baselines (see Section 5.2), thus making Early Exit PLMs robust to domain changes and enhancing their efficiency.•In Section 4, we introduce CeeBERT, an upper confidence-based algorithm that relies solely on confidence scores for learning the optimal threshold to make exit decisions.•In Section 5, we evaluate CeeBERT on five different datasets drawn from diverse tasks to demonstrate its proficiency in improving inference latency compared to the state-of-the-art algorithms.2 Related workEarly exit methods are applied for various tasks such as image classification, image captioning and NLP tasks to reduce the computational resources and inference latency.Early exits in Image tasks:For image classification tasks, BranchyNet Teerapittayanon et al. (2016) uses classification entropy at each attached exit to decide whether to infer the sample at the side branch based on the entropy of prediction.Shallow-deepKaya et al. (2019)