# A Survey on Natural Language Processing for Programming

## Question

What are the differences between transcription tasks and synthesis tasks in programming, particularly in terms of input and output characteristics?

## URLs

1. https://ar5iv.org/html/2212.05773
2. https://ar5iv.org/html/2108.07732
3. https://ar5iv.org/html/2410.00215
4. https://ar5iv.org/html/2403.08950
5. https://ar5iv.org/html/2403.18969
6. https://ar5iv.org/html/2409.02098
7. https://ar5iv.org/html/2411.06284
8. https://ar5iv.org/html/2411.13577
9. https://ar5iv.org/html/2408.14713

## Answer

## I don't know?: False

Transcription tasks and synthesis tasks in programming exhibit distinct characteristics in terms of their inputs and outputs.

**Transcription Tasks**: These tasks are typically related to converting spoken or written natural language into a structured format. The input in transcription tasks can be in the form of natural language (NL) or programming language (PL), and they are processed using techniques derived from natural language processing (NLP). Specifically, transcription tasks involve transforming input efficiently into a target format following established machine translation techniques [1]. However, the passage does not provide explicit examples of outputs for transcription tasks, implying that these outputs may be more foundational or preliminary in nature, serving as a basis for subsequent processes.

**Synthesis Tasks**: In contrast, synthesis tasks focus on generating code from a given context, which can also be derived from natural language or programming language inputs. These tasks can be further divided into two categories: program synthesis and code completion. The output of program synthesis is a relatively independent and complete unit, such as a function or a class, essentially representing a coherent piece of functionality in software [1]. On the other hand, code completion outputs are less restrictive, ranging from individual tokens to small code snippets, thereby serving as suggestions to the programmer rather than standalone solutions. Consequently, while both tasks serve the overarching goal of aiding in code generation, they differ significantly in their operational scope and output completeness.

To summarize, in transcription tasks, the focus is on accurately converting input to a structured form without necessarily producing a complete or functional code output. In synthesis tasks, inputs are transformed into complete programming structures or assistance with code generation, reflecting a more complex interaction that contributes directly to software development. These differences in their characteristics illustrate their distinct roles within the broader field of programming and language processing.

[1]: https://ar5iv.org/html/2212.05773, [2212.05773] A Survey on Natural Language Processing for Programming
[2]: https://ar5iv.org/html/2211.14591, No Title
[3]: https://ar5iv.org/html/2209.12617, No Title
[4]: https://ar5iv.org/html/2209.12617, No Title
[5]: https://ar5iv.org/html/2410.00427, No Title

[1]: Passage ID 1: does not explicitly occur in either input or output, we include tasks of such form for two reasons.First, PL has been demonstrated to contain abundant statistical properties similar to NL Mou et al. (2016).Second, most of the ways that PL is processed are derived from NLP, like machine translation techniques Tufano et al. (2019) in the transcription task (§ 2.5).2.4 Synthesis TasksThe synthesis task generates a program given a context (which can be NL, PL, or their mixture), thus can accelerate the development process.It can be further divided into program synthesis and code completion by the formal completeness of the output.The output of program synthesis is a relatively independent unit, such as a function and a class, while the output of code completion is less restricted, ranging from tokens to code snippets.Program synthesisis also called code generation.It is the systematic derivation of a program from a given specification Manna and Waldinger
[2]: Passage ID 2: ambiguous [4]. In fact, it is categorized as AI-complete, meaning that its resolution requires the “synthesis of human-level intelligence” with regard to natural language [9].Consequently, a machine must be able to understand each component of language, that is its phonology, morphology, syntax, semantics, and pragmatics.This is reflected in a set of specific challenges, called NLP tasks, that provide a measure as to a machine’s linguistic capabilities. These tasks can generally be solved by various methods. However, in recent years the research field has been dominated by machine learning approaches, in particular artificial neural networks that enable deep learning [6]. Importantly, such models cannot operate directly on discrete symbolic inputs. Hence, TR becomes a crucial second dimension of NLP.II-A Natural Language Processing TasksAs previously outlined, computational language understanding can be roughly broken down into the lexical, syntactic, semantic, and pragmatic
[3]: Passage ID 3: Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence (AI) concerned with text understanding and text generation. The former subject is studied in the sub-branch natural language understanding (NLU) 1; 2; 3 and the latter in natural language generation (NLG) 4; 5. Over the years, both fields, i.e., NLU and NLG developed enormously with an extensive literature which requires nowadays a dedicated discussion of specialized subtasks when presenting approaches or methods thereof despite the fact that a systems understanding of NLP can only be achieved holistically.In this paper, we focus on subtasks of NLP centered around question answering (QA). The task of a QA system is to find an answer (output) in the form of a natural language for a given question (input) usually presented in form of a sentence. While our focus is on QA systems and their
[4]: Passage ID 4: Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence (AI) concerned with text understanding and text generation. The former subject is studied in the sub-branch natural language understanding (NLU) 1; 2; 3 and the latter in natural language generation (NLG) 4; 5. Over the years, both fields, i.e., NLU and NLG developed enormously with an extensive literature which requires nowadays a dedicated discussion of specialized subtasks when presenting approaches or methods thereof despite the fact that a systems understanding of NLP can only be achieved holistically.In this paper, we focus on subtasks of NLP centered around question answering (QA). The task of a QA system is to find an answer (output) in the form of a natural language for a given question (input) usually presented in form of a sentence. While our focus is on QA systems and their
[5]: Passage ID 5: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the