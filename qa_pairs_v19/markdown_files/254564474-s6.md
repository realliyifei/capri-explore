# A Survey on Natural Language Processing for Programming

## Question

What are the differences between transcription tasks and synthesis tasks in programming, particularly in terms of input and output characteristics?

## URLs

1. https://ar5iv.org/html/2212.05773. [2212.05773] A Survey on Natural Language Processing for Programming
2. https://ar5iv.org/html/2108.07732. [2108.07732] Program Synthesis with Large Language Models
3. https://ar5iv.org/html/2410.00215. [2410.00215] Characterizing and Efficiently Accelerating Multimodal Generation Model Inference
4. https://ar5iv.org/html/2403.08950. [2403.08950] Exploring Prompt Engineering Practices in the Enterprise
5. https://ar5iv.org/html/2403.18969. [2403.18969] A Survey on Large Language Models from Concept to Implementation
6. https://ar5iv.org/html/2409.02098. [2409.02098] CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation
7. https://ar5iv.org/html/2411.06284. [2411.06284] A Comprehensive Survey and Guide to Multimodal Large Language Models in Vision-Language Tasks
8. https://ar5iv.org/html/2411.13577. [2411.13577] WavChat: A Survey of Spoken Dialogue Models
9. https://ar5iv.org/html/2408.14713. [2408.14713] StyleSpeech: Parameter-efficient Fine Tuning for Pre-trained Controllable Text-to-Speech

## Answer (I don't know?: ✗)

Transcription tasks and synthesis tasks in programming have distinctive characteristics in terms of their input-output dynamics.

**Transcription Tasks** focus on converting an existing program to meet specific requirements. This process includes tasks like program translation and code refinement. Program translation aims to convert code between high-level programming languages (PL), such as translating C# code to Java, which is particularly useful for updating projects that use deprecated PLs or migrating algorithms across different PLs [2]. Code refinement, on the other hand, addresses the conversion of buggy programs into correct versions, closely linked with the practice of vulnerability identification [2]. The input for transcription tasks is typically an already existing program, and the output is a modified version of this program, tailored to specific constraints or improved for correctness [2][3].

**Synthesis Tasks**, contrastingly, are focused on generating new code based on given specifications. These tasks can be subdivided into program synthesis (or code generation) and code completion. Program synthesis develops independent units of code, such as functions or classes, from scratch based on a specified context [1]. Code completion, however, offers completion suggestions for existing code context, filling in gaps like method calls or variable names [2]. The main feature of synthesis tasks is that the output can be relatively independent of existing programs, presenting new constructs that fulfill the initially specified requirements [1][3]. 

To summarize, while transcription tasks are centered around modifying existing code to fit specific parameters or correct errors, synthesis tasks are about creating code from specifications, with outputs that can be units derived from earlier context. This distinction underscores the different approaches to programming tasks, with transcription tasks being purely transformative, whereas synthesis tasks are generative.

1. [1]:  https://ar5iv.org/html/2212.05773, [2212.05773] A Survey on Natural Language Processing for Programming
2. [2]:  https://ar5iv.org/html/2212.05773, [2212.05773] A Survey on Natural Language Processing for Programming
3. [3]:  https://ar5iv.org/html/2212.05773, [2212.05773] A Survey on Natural Language Processing for Programming
4. [4]:  https://ar5iv.org/html/2212.05773, [2212.05773] A Survey on Natural Language Processing for Programming
5. [5]:  https://ar5iv.org/html/2411.06284, [2411.06284] A Comprehensive Survey and Guide to Multimodal Large Language Models in Vision-Language Tasks
---
1. [1]:  Passage ID 1: does not explicitly occur in either input or output, we include tasks of such form for two reasons.First, PL has been demonstrated to contain abundant statistical properties similar to NL Mou et al. (2016).Second, most of the ways that PL is processed are derived from NLP, like machine translation techniques Tufano et al. (2019) in the transcription task (§ 2.5).2.4 Synthesis TasksThe synthesis task generates a program given a context (which can be NL, PL, or their mixture), thus can accelerate the development process.It can be further divided into program synthesis and code completion by the formal completeness of the output.The output of program synthesis is a relatively independent unit, such as a function and a class, while the output of code completion is less restricted, ranging from tokens to code snippets.Program synthesisis also called code generation.It is the systematic derivation of a program from a given specification Manna and Waldinger
2. [2]:  Passage ID 2: in early research Tu et al. (2014); Hindle et al. (2016). It suggests the next program token given a context and has been widely applied to IDEs Li et al. (2018).The application scenario includes the completion of method calls, keywords, variables, and arguments.With the bloom of the pre-trained models, the scenario has been extended to punctuations, statements, and even code snippets Svyatkovskiy et al. (2020), further blurring the line between program synthesis and code completion.2.5 Transcription TasksThe transcription task converts a given program to meet a specific requirement.Concretely, program translation aims to convert between high-level PL Roziere et al. (2020); Zhu et al. (2022), e.g., C# and Java.It can accelerate the update of projects written by deprecated PL, and the migration of algorithms implemented by various PLs.Code refinement aims to convert a buggy program into correct one Wang et al. (2021).It is closely related to vulnerability identification
3. [3]:  Passage ID 3: programs given NL query Husain et al. (2019).It has a similar application scenario and input/output format to program synthesis. The difference is that its output is extracted from existing programs, rather than being synthesized from scratch.2.3 Classification TasksThe classification task detects whether given programs have specific characteristics,e.g., being cloned (clone detection), or being vulnerable (vulnerability identification).They are essential in protecting software from the effects of ad-hoc reuse Svajlenko et al. (2014) and cyber attacks Zhou et al. (2019).The granularity of the input ranges from a coarse-grained softwarerepository Hovsepyan et al. (2012) to a fine-grained function Russell et al. (2018); Zhou et al. (2019).Despite the fact that NL does not explicitly occur in either input or output, we include tasks of such form for two reasons.First, PL has been demonstrated to contain abundant statistical properties similar to NL Mou et al.
4. [4]:  Passage ID 4: the summarization task, which is abstractive and regarded as a generation task in NLP.We classify it as structure-based since PL lies in its input side, and the key to the task is understanding the content of PL by the structure.2.1 Summarization TasksThe summarization task summarizes a program into an NL description.It is crucial for the maintenance of software, especially those involving multiple developers.According to the format of the output, it can be further divided into comment generation Nie et al. (2022) and docstring generation Clement et al. (2020).The output of the latter contains some structural information, such as parameters and input/output examples.2.2 Retrieval TasksThe retrieval task mainly refers to the code search.It aims to retrieve relevant programs given NL query Husain et al. (2019).It has a similar application scenario and input/output format to program synthesis. The difference is that its output is extracted from existing programs,
5. [5]:  Passage ID 5: answering.The integration of multimodal data has opened up new possibilities for AI applications. MLLMs can generate detailed descriptions of images, providing valuable assistance in fields like accessibility and content creation. These models can answer questions about images, demonstrating their ability to understand and reason about visual content. MLLMs also enable the creation of rich multimedia content, combining text, images, and audio to produce engaging and informative outputs.The field of NLP has undergone significant transformations over the past few decades, driven by advancements in computational power, the availability of large-scale datasets, and breakthroughs in machine learning algorithms. This journey can be broadly categorized into several key phases:1.Rule-based systems (1950s-1980s): Early NLP approaches relied heavily on hand-crafted rules and linguistic knowledge bases. While these systems could perform well in narrow domains, they struggled with the