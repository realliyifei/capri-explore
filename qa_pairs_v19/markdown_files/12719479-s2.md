# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

Why is reliability important for validating a coding scheme in computational linguistics?

## URLs

1. https://ar5iv.org/html/2302.09587
2. https://ar5iv.org/html/2307.02863
3. https://ar5iv.org/html/2402.17649
4. https://ar5iv.org/html/2402.12146
5. https://ar5iv.org/html/2407.21037
6. https://ar5iv.org/html/2311.11844
7. https://ar5iv.org/html/2411.12142
8. https://ar5iv.org/html/2410.02184
9. https://ar5iv.org/html/2411.16797

## Answer

## I don't know?: False

Reliability is crucial for validating a coding scheme in computational linguistics, particularly in the context of natural language processing (NLP) and software engineering, because it directly impacts the trustworthiness and applicability of the research findings.

Firstly, reliability ensures that the experimental outcomes are replicable and consistent across different implementations and datasets. In the field of NLP, inconsistency in results has been a noted problem; many researchers report challenges in replicating others' work or even their own due to unreliable methodologies or performance evaluations (e.g., Prinz et al., 2011; Gundersen and Kjensmo, 2018) [3]. This inconsistency raises questions about the scientific validity and credibility of research outcomes. Establishing reliability through consistent results against existing benchmarks and standards would bolster the trust that researchers and practitioners have in the findings of computational linguistics studies.

Moreover, the risks of experimental biases—such as data noise and labeling errors—are significant in producing unreliable findings in coding schemes, as evidenced by She et al. (2023), which highlights the prevalence of these biases in language models for code research [1]. Such biases can lead to inflated performance metrics when tested on duplicated code rather than de-duplicated corpora, as shown by Allamanis et al. (2019) [1]. Therefore, ensuring reliability helps to mitigate these issues, ensuring that results are reflective of real-world applications.

The call for improved reliability is underscored by the understanding that many papers in NLP lack robust validation methods, which can result in misleading findings (McCullough et al., 2008) [2]. The integrity of research in computational linguistics is contingent upon sound practices that enhance the dependability of coding schemes. Enhancing the reliability of published findings becomes necessary not only for academic rigor but also for practical implementations in software development and application.

Finally, the use of advanced explainable AI methods to assess the reliability of coding schemes is important. Research has suggested that understanding the logic behind model outcomes is essential, particularly in the automated generation of code [4]. By employing explainable AI approaches, researchers can identify how specific tokens contribute to transformations, which can help pinpoint issues in the reliability of these models and improve trustworthiness.

In summary, reliability is vital in validating coding schemes in computational linguistics as it contributes to the replicability of results, ensures the accuracy of performance evaluations by mitigating biases, addresses concerns regarding the scientific validity of findings, and supports efforts to implement robust, explainable methodologies in research and applications. These facets are inherently interconnected, emphasizing the foundational role of reliability in advancing empirical research and practical applications in the NLP community.

1. [1]:  https://ar5iv.org/html/2302.09587, [2302.09587] On the Reliability and Explainability of Language Models for Program Generation
2. [2]:  https://ar5iv.org/html/2303.16166, [2303.16166] When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in NLP
3. [3]:  https://ar5iv.org/html/2303.16166, [2303.16166] When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in NLP
4. [4]:  https://ar5iv.org/html/2302.09587, [2302.09587] On the Reliability and Explainability of Language Models for Program Generation
5. [5]:  https://ar5iv.org/html/2005.04322, No Title
---
1. [1]:  Passage ID 1: we apply explainable AI approaches to assist us in understanding why code models are not robust enough for automated language generation, which is currently rarely explored in software engineering.6.4. Reliability in Language Models for Source CodeWith the increasing application of advanced large language models in software engineering, ensuring their trustworthiness becomes important (Lo, 2023; Liu et al., 2022b).She et al. (She et al., 2023) have reviewed common experimental biases in language models for code research, such as data noise, labeling errors, and inappropriate evaluation approaches.Allamanis et al. (Allamanis, 2019) have investigated the effects of code duplication and they found that performance metrics could be inflated by up to 100% when testing on duplicated code corpora, as opposed to de-duplicated corpora which more accurately reflect real-world usage by software engineers.Nie et al. (Nie et al., 2023) explored labeling errors in vulnerability detection
2. [2]:  Passage ID 2: the foundation for further research, ultimately resulting in unreliable and potentially misleading findings (McCullough et al., 2008).In light of the above considerations, this study is a call to action, underpinned by empirical evidence, to enhance the dependability of published NLP findings. In particular, our contributions are:1.We examine the extent to which research works consider the attributes studied in the field of software quality assurance (Buckley and Poston, 1984; Tripathy and Naik, 2011), or SQA (§2), and show that code correctness has been neglected by the NLP community thus far (§3);2.Through a case study on open-source implementations of the widespread Conformer architecture (Gulati et al., 2020), we prove that:-At least one impactful bug is present in all the analyzed implementations (§4.2);-Bugs do notprevent from achievinggood andreproducible results that outperformother architecturesin speech recognition and translation across
3. [3]:  Passage ID 3: improving research software quality within the NLP community.††\twemojilight bulbDenotes equal contributions.1 IntroductionIn the field of natural language processing (NLP), as well as in broader contexts, the validity and soundness of research findings are typically upheld by “establishing consistency of results versus existing implementations, standard benchmarks, or sanity checks via statistically significant experimental results” (Rozier and Rozier, 2014). Nevertheless, recent evidence indicates that several of these aspects are absent in many papers (Raff, 2019), questioning the scientific credibility of NLP research.On one hand, many scientists have reported difficulties in replicating the work of others, or even their own, (Prinz et al., 2011; Gundersen and Kjensmo, 2018; Wieling et al., 2018a; Chen et al., 2019; Gundersen, 2019), also in the specific context of NLP (Wieling et al., 2018b; Belz et al., 2021a; Marie et al., 2021; Narang et al., 2021; Gehrmann et al.,
4. [4]:  Passage ID 4: language model-based approaches have been proposed and evaluated on various benchmark datasets, demonstrating promising performance.However, there is still uncertainty about the reliability of these models, particularly their realistic ability to consistently transform code sequences.This raises the question: are these techniques sufficiently trustworthy for automated program generation?Consequently, Further research is needed to understand model logic and assess reliability and explainability.To bridge these research gaps, we conduct a thorough empirical study of eight popular language models on five representative datasets to determine the capabilities and limitations of automated program generation approaches.We further employ advanced explainable AI approaches to highlight the tokens that significantly contribute to the code transformation.We discover that state-of-the-art approaches suffer from inappropriate performance evaluation stemming from severe data duplication,
5. [5]:  Passage ID 5: interest from the NLP community. Many researchers have proposed novel methods to handle code-switched data, showing improvements on core NLP tasks such as language identification (LID), named entity recognition (NER), and part-of-speech (POS) tagging. However, many of these approaches are usually evaluated on a few language pairs and a specific domain, and it is not clear whether these models are exclusive to such scenarios or they can generalize to other tasks, domains, and language pairs.Moreover, research in code-switching currently has a slow process of comparison in which researchers have to replicate previous methods to report scores on different datasets.Furthermore, choosing the best-published model for benchmarking purposes is not an easy task either.These problems exist mainly because 1) there is no official benchmark for general code-switching evaluation that allows direct comparisons across multiple tasks, and 2) methods are usually not comprehensively evaluated across