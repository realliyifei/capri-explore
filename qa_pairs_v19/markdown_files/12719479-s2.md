# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

Why is reliability important for validating a coding scheme in computational linguistics?

## URLs

1. https://ar5iv.org/html/2302.09587. [2302.09587] On the Reliability and Explainability of Language Models for Program Generation
2. https://ar5iv.org/html/2307.02863. [2307.02863] ValiText -- a unified validation framework for computational text-based measures of social constructs
3. https://ar5iv.org/html/2402.17649. [2402.17649] Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs
4. https://ar5iv.org/html/2402.12146. [2402.12146] Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
5. https://ar5iv.org/html/2407.21037. [2407.21037] An Application of Large Language Models to Coding Negotiation Transcripts
6. https://ar5iv.org/html/2311.11844. [2311.11844] How to Use Large Language Models for Text Coding: The Case of Fatherhood Roles in Public Policy Documents
7. https://ar5iv.org/html/2411.12142. [2411.12142] A Computational Method for Measuring “Open Codes” in Qualitative Analysis
8. https://ar5iv.org/html/2410.02184. [2410.02184] CodeJudge: Evaluating Code Generation with Large Language Models
9. https://ar5iv.org/html/2411.16797. [2411.16797] Enhancing Answer Reliability Through Inter-Model Consensus of Large Language Models

## Answer (I don't know?: ✗)

Reliability is crucial for validating a coding scheme in computational linguistics because it ensures that the models and systems employed produce consistent and trustworthy outcomes. In the context of coding schemes, reliability pertains to the ability of the system to deliver performance that aligns with real-world usage and expectations [4]. The advancement and increased application of large language models in software engineering underscore the significance of this reliability, as any inconsistencies could lead to flawed linguistic analyses or automated code generation [5].

The complexity involved in constructing a Question Answering (QA) system complicates its evaluation, highlighting the importance of robust assessment techniques [1] [2]. Without consistent reliability, it is difficult to determine whether the evaluations being conducted are reflective of a model's true capabilities. The existence of experimental biases, such as data noise and labeling errors, can severely undermine the reliability of coding schemes in NLP [4]. Allamanis et al. point out that reliance on duplicated code corpora can inflate performance metrics significantly, sometimes doubling the perceived effectiveness of the system [4]. This discrepancy further illustrates the necessity of using de-duplicated datasets for a more genuine reflection of the model’s reliability and real-world applicability.

Moreover, if the output of NLP models and coding schemes lacks reliability, the trust in automated systems diminishes, ultimately affecting their usability in practical applications. This can lead to hesitance in adopting powerful coding tools that rely on these models for generating or transforming code. For example, the questions surrounding the trustworthiness of language models for automated program generation arise due to doubts about their reliability and the logical consistency of their outputs [5]. As such, ongoing research into explainability and model logic is required to clarify the foundations on which these coding schemes operate, ensuring that their applications are dependable [5].

In conclusion, reliability is paramount in validating coding schemes in computational linguistics because it directly impacts the utility and trustworthiness of NLP technologies, affects the validity of their evaluations, and ultimately influences the adoption of these advanced models in practical applications. The complex interplay between data quality, evaluation methods, and interpretability all underline the critical need for reliable approaches in NLP [1] [2][5].

1. [1]:  https://ar5iv.org/html/2209.12617, No Title
2. [2]:  https://ar5iv.org/html/2209.12617, No Title
3. [3]:  https://ar5iv.org/html/2209.12617, No Title
4. [4]:  https://ar5iv.org/html/2302.09587, [2302.09587] On the Reliability and Explainability of Language Models for Program Generation
5. [5]:  https://ar5iv.org/html/2302.09587, [2302.09587] On the Reliability and Explainability of Language Models for Program Generation
---
1. [1]:  Passage ID 1: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence
2. [2]:  Passage ID 2: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence
3. [3]:  Passage ID 3: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence
4. [4]:  Passage ID 4: we apply explainable AI approaches to assist us in understanding why code models are not robust enough for automated language generation, which is currently rarely explored in software engineering.6.4. Reliability in Language Models for Source CodeWith the increasing application of advanced large language models in software engineering, ensuring their trustworthiness becomes important (Lo, 2023; Liu et al., 2022b).She et al. (She et al., 2023) have reviewed common experimental biases in language models for code research, such as data noise, labeling errors, and inappropriate evaluation approaches.Allamanis et al. (Allamanis, 2019) have investigated the effects of code duplication and they found that performance metrics could be inflated by up to 100% when testing on duplicated code corpora, as opposed to de-duplicated corpora which more accurately reflect real-world usage by software engineers.Nie et al. (Nie et al., 2023) explored labeling errors in vulnerability detection
5. [5]:  Passage ID 5: language model-based approaches have been proposed and evaluated on various benchmark datasets, demonstrating promising performance.However, there is still uncertainty about the reliability of these models, particularly their realistic ability to consistently transform code sequences.This raises the question: are these techniques sufficiently trustworthy for automated program generation?Consequently, Further research is needed to understand model logic and assess reliability and explainability.To bridge these research gaps, we conduct a thorough empirical study of eight popular language models on five representative datasets to determine the capabilities and limitations of automated program generation approaches.We further employ advanced explainable AI approaches to highlight the tokens that significantly contribute to the code transformation.We discover that state-of-the-art approaches suffer from inappropriate performance evaluation stemming from severe data duplication,