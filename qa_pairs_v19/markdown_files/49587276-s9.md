# A Survey on Recent Advances in Named Entity Recognition from Deep Learning models

## Question

How are character labels converted into word labels in character-level architectures for NER models?

## URLs

1. https://ar5iv.org/html/2203.06925. [2203.06925] WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named Entity Recognition
2. https://ar5iv.org/html/2401.10825. [2401.10825] A survey on recent advances in Named Entity Recognition
3. https://ar5iv.org/html/1709.04109. [1709.04109] Empower Sequence Labeling with Task-Aware Neural Language Model
4. https://ar5iv.org/html/1705.04044. [1705.04044] End-to-end Recurrent Neural Network Models for Vietnamese Named Entity Recognition: Word-level vs. Character-level
5. https://ar5iv.org/html/1910.11470. [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
6. https://ar5iv.org/html/2011.06727. [2011.06727] A Survey on Recent Advances in Sequence Labeling from Deep Learning Models
7. https://ar5iv.org/html/2401.12941. [2401.12941] Multicultural Name Recognition For Previously Unseen Names
8. https://ar5iv.org/html/2212.09897. [2212.09897] Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training
9. https://ar5iv.org/html/2002.00738. [2002.00738] An Efficient Architecture for Predicting the Case of Characters using Sequence Models
10. https://ar5iv.org/html/2109.01636. [2109.01636] Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding

## Answer (I don't know?: ✗)

In character-level architectures for Named Entity Recognition (NER) models, character labels are converted into word labels through a post-processing step after the model has predicted labels for each character in the input sequence. The character labels, predicted by the recurrent neural networks (RNNs) that process the character sequences, undergo this transformation to obtain meaningful word-level annotations for the entire input sentence [1][2].

The character-level model operates by treating a sentence as a sequence of characters, which are then passed through an RNN. This architecture allows the model to predict individual labels corresponding to each character [1][2]. Following the prediction phase, the character-level labels must be converted into word-level labels, which typically involves grouping the character predictions into words based on their positions in the original input [3]. This process often entails taking into account the boundaries of words, which can be informally defined by white spaces or other delimiters present in the text.

Additionally, various models in the literature have explored different innovations in this area. For instance, one approach successfully implemented a combination of convolutional neural networks (CNNs) and LSTMs to help improve the recognition of entities by extracting features from both character and word levels [4][5]. The interaction between these layers can enhance the model's ability to generalize and recognize named entities effectively across different languages. 

In sum, the transformation from character-level labels to word-level labels in character-level NER architectures involves a post-processing mechanism that consolidates character predictions into meaningful word annotations, leveraging the inherent structure of the input text [1][2][3].

1. [1]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
2. [2]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
3. [3]:  https://ar5iv.org/html/2401.12941, [2401.12941] Multicultural Name Recognition For Previously Unseen Names
4. [4]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
5. [5]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
---
1. [1]:  Passage ID 1: Character level architecturesCharactersChar EmbeddingChar LSTM-FChar LSTM-BChar RepresentationLabelBB-ORGeB-ORGsB-ORGtB-ORG␣B-ORGBB-ORGuB-ORGyB-ORG’OsOFigure 2: Character level NN architecture for NERIn this model, a sentence is taken to be a sequence of characters. This sequence is passed through an RNN, predicting labels for each character (Figure 2).Character labels transformed into word labels via post processing.The potential of character NER neural models was first highlighted by ?) using highway networks over convolution neural networks (CNN) on character sequences of words and then using another layer of LSTM + softmax for the final predictions.This model was implemented by ?) for Vietnamese NER and achieved 80.23% F-score on ?)’s Vietnamese test data.Character models were also used in various other languages like Chinese [Dong et al. (2016] where it has achieved near state of the art performance.?) proposed CharNER 111111Code:
2. [2]:  Passage ID 2: Character level architecturesCharactersChar EmbeddingChar LSTM-FChar LSTM-BChar RepresentationLabelBB-ORGeB-ORGsB-ORGtB-ORG␣B-ORGBB-ORGuB-ORGyB-ORG’OsOFigure 2: Character level NN architecture for NERIn this model, a sentence is taken to be a sequence of characters. This sequence is passed through an RNN, predicting labels for each character (Figure 2).Character labels transformed into word labels via post processing.The potential of character NER neural models was first highlighted by ?) using highway networks over convolution neural networks (CNN) on character sequences of words and then using another layer of LSTM + softmax for the final predictions.This model was implemented by ?) for Vietnamese NER and achieved 80.23% F-score on ?)’s Vietnamese test data.Character models were also used in various other languages like Chinese [Dong et al. (2016] where it has achieved near state of the art performance.?) proposed CharNER 111111Code:
3. [3]:  Passage ID 3: while the best F scores for Spanish, Dutch, and German were obtained by an RNN with a character, word, and affix architecture by Yadav et al. (2018). These results were obtained using datasets from the CoNLL02 and CoNLL03 shared tasks (Tjong Kim Sang, 2002) (Tjong Kim Sang andDe Meulder, 2003). Another shared task, DrugNER, evaluated NER results on a corpus of medical and drug terminology. On this task, the word and character level architecture from Yadav et al. (2018) performed better than just word level architecture alone from Chalapathy et al. (2016).This paper will compare models trained on combined word and character input and those trained solely on word input. Based on previous studies and the intuition that character level input can allow the model to learn sub-word character patterns that can be used to identify commonalities between names and contexts seen in training and those seen the test data or real world applications, I hypothesize that the word and character level
4. [4]:  Passage ID 4: implemented a model with a CNN over the characters of word, concatenated with word embeddings of the central word and its neighbors, fed to a feed forward network, and followed by the Viterbi algorithm to predict labels for each word. The model achieved 82.21% F score on Spanish CoNLL 2002 data and 71.23% F score on Portuguese NER data [Santos and Cardoso (2007].CharactersChar EmbeddingChar LSTM-FChar LSTM-BWord FeaturesWord RepresentationWord LSTM-FWord LSTM-BWord CRFLabelBestBestB-ORGBuyBuyI-ORG’s’sOFigure 3: Word+character level NN architecture for NERThe second type of model concatenates word embeddings with LSTMs (sometimes bi-directional) over the characters of a word,passing this representation through another sentence-level Bi-LSTM, and predicting the final tags using a final softmax or CRF layer (Figure 3). ?)131313Code: https://github.com/glample/tagger introduced this architecture and achieved 85.75%, 81.74%, 90.94%, 78.76% Fscores on Spanish, Dutch, English and
5. [5]:  Passage ID 5: implemented a model with a CNN over the characters of word, concatenated with word embeddings of the central word and its neighbors, fed to a feed forward network, and followed by the Viterbi algorithm to predict labels for each word. The model achieved 82.21% F score on Spanish CoNLL 2002 data and 71.23% F score on Portuguese NER data [Santos and Cardoso (2007].CharactersChar EmbeddingChar LSTM-FChar LSTM-BWord FeaturesWord RepresentationWord LSTM-FWord LSTM-BWord CRFLabelBestBestB-ORGBuyBuyI-ORG’s’sOFigure 3: Word+character level NN architecture for NERThe second type of model concatenates word embeddings with LSTMs (sometimes bi-directional) over the characters of a word,passing this representation through another sentence-level Bi-LSTM, and predicting the final tags using a final softmax or CRF layer (Figure 3). ?)131313Code: https://github.com/glample/tagger introduced this architecture and achieved 85.75%, 81.74%, 90.94%, 78.76% Fscores on Spanish, Dutch, English and