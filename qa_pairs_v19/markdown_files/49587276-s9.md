# A Survey on Recent Advances in Named Entity Recognition from Deep Learning models

## Question

How are character labels converted into word labels in character-level architectures for NER models?

## URLs

1. https://ar5iv.org/html/2203.06925. [2203.06925] WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named Entity Recognition
2. https://ar5iv.org/html/2401.10825. [2401.10825] A survey on recent advances in Named Entity Recognition
3. https://ar5iv.org/html/1709.04109. [1709.04109] Empower Sequence Labeling with Task-Aware Neural Language Model
4. https://ar5iv.org/html/1705.04044. [1705.04044] End-to-end Recurrent Neural Network Models for Vietnamese Named Entity Recognition: Word-level vs. Character-level
5. https://ar5iv.org/html/1910.11470. [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
6. https://ar5iv.org/html/2011.06727. [2011.06727] A Survey on Recent Advances in Sequence Labeling from Deep Learning Models
7. https://ar5iv.org/html/2401.12941. [2401.12941] Multicultural Name Recognition For Previously Unseen Names
8. https://ar5iv.org/html/2212.09897. [2212.09897] Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training
9. https://ar5iv.org/html/2002.00738. [2002.00738] An Efficient Architecture for Predicting the Case of Characters using Sequence Models
10. https://ar5iv.org/html/2109.01636. [2109.01636] Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding

## Answer (I don't know?: ✗)

In character-level architectures for Named Entity Recognition (NER) models, character labels are converted into word labels through a post-processing step after the initial predictions are made by the model. The process begins with a sequence of characters that is passed through a recurrent neural network (RNN), which predicts labels for each individual character. This character-level labeling is crucial as it allows the model to capture sub-word information, which is particularly beneficial for languages with rich morphology or when encountering rare or unseen words [1].

Once the character labels are predicted, a specific method is employed to transform these character-level labels into word-level labels. This is achieved through a systematic process that aggregates the labels assigned to each character within a word, typically by taking the most frequent label or employing a majority vote among the labels assigned to the characters that make up the word. The alignment of character-level predictions to their corresponding word constructs is essential to ensure that the final output aligns with the canonical form of the entities being recognized [2].

The architecture may also utilize additional components such as a conditional random field (CRF) layer, which can further refine the mapping from character labels to word labels by considering the dependencies among labels and potentially enforcing consistent labeling sequences across words [4]. This is particularly relevant in scenarios with entity boundaries, as the CRF layer can help mitigate errors that may arise from individual character predictions.

Empirical results in multiple languages have demonstrated that character-level models can significantly outperform traditional word-only models, as they are capable of leveraging character compositionality to recognize patterns that are not evident when only considering whole words [3] [5]. Thus, the conversion from character labels to word labels is a critical step, effectively bridging the gap between raw character sequences and structured word-level entity information.

1. [1]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
2. [2]:  https://ar5iv.org/html/2401.12941, [2401.12941] Multicultural Name Recognition For Previously Unseen Names
3. [3]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
4. [4]:  https://ar5iv.org/html/1709.04109, [1709.04109] Empower Sequence Labeling with Task-Aware Neural Language Model
5. [5]:  https://ar5iv.org/html/2401.12941, [2401.12941] Multicultural Name Recognition For Previously Unseen Names
---
1. [1]:  Passage ID 1: Character level architecturesCharactersChar EmbeddingChar LSTM-FChar LSTM-BChar RepresentationLabelBB-ORGeB-ORGsB-ORGtB-ORG␣B-ORGBB-ORGuB-ORGyB-ORG’OsOFigure 2: Character level NN architecture for NERIn this model, a sentence is taken to be a sequence of characters. This sequence is passed through an RNN, predicting labels for each character (Figure 2).Character labels transformed into word labels via post processing.The potential of character NER neural models was first highlighted by ?) using highway networks over convolution neural networks (CNN) on character sequences of words and then using another layer of LSTM + softmax for the final predictions.This model was implemented by ?) for Vietnamese NER and achieved 80.23% F-score on ?)’s Vietnamese test data.Character models were also used in various other languages like Chinese [Dong et al. (2016] where it has achieved near state of the art performance.?) proposed CharNER 111111Code:
2. [2]:  Passage ID 2: while the best F scores for Spanish, Dutch, and German were obtained by an RNN with a character, word, and affix architecture by Yadav et al. (2018). These results were obtained using datasets from the CoNLL02 and CoNLL03 shared tasks (Tjong Kim Sang, 2002) (Tjong Kim Sang andDe Meulder, 2003). Another shared task, DrugNER, evaluated NER results on a corpus of medical and drug terminology. On this task, the word and character level architecture from Yadav et al. (2018) performed better than just word level architecture alone from Chalapathy et al. (2016).This paper will compare models trained on combined word and character input and those trained solely on word input. Based on previous studies and the intuition that character level input can allow the model to learn sub-word character patterns that can be used to identify commonalities between names and contexts seen in training and those seen the test data or real world applications, I hypothesize that the word and character level
3. [3]:  Passage ID 3: implemented a model with a CNN over the characters of word, concatenated with word embeddings of the central word and its neighbors, fed to a feed forward network, and followed by the Viterbi algorithm to predict labels for each word. The model achieved 82.21% F score on Spanish CoNLL 2002 data and 71.23% F score on Portuguese NER data [Santos and Cardoso (2007].CharactersChar EmbeddingChar LSTM-FChar LSTM-BWord FeaturesWord RepresentationWord LSTM-FWord LSTM-BWord CRFLabelBestBestB-ORGBuyBuyI-ORG’s’sOFigure 3: Word+character level NN architecture for NERThe second type of model concatenates word embeddings with LSTMs (sometimes bi-directional) over the characters of a word,passing this representation through another sentence-level Bi-LSTM, and predicting the final tags using a final softmax or CRF layer (Figure 3). ?)131313Code: https://github.com/glample/tagger introduced this architecture and achieved 85.75%, 81.74%, 90.94%, 78.76% Fscores on Spanish, Dutch, English and
4. [4]:  Passage ID 4: in this paper, which are sequence labeling and how to improve it with additional information.Sequence Labeling.As one of the fundamental tasks in NLP, linguistic sequence labeling, including POS tagging, chunking, and NER, has been studied for years.Handcrafted features were widely used in traditional methods like CRFs, HMMs, and maximum entropy classifiers (?; ?; ?; ?), but also make it hard to apply them to new tasks or domains.Recently, getting rid of handcrafted features, there are attempts to build end-to-end systems for sequence labeling tasks, such as BiLSTM-CNN (?), LSTM-CRF (?), and the current state-of-the-art method in NER and POS tagging tasks, LSTM-CNN-CRF (?).These models all incorporate character-level structure, and report meaningful improvement over pure word-level model.Also, CRF layer has also been demonstrated to be effective in capturing the dependency among labels.Our model is based on the success of LSTM-CRF model and is further modified to better
5. [5]:  Passage ID 5: for downstream tasks to not exhibit bias based on cultural background, a model should perform well on names from a variety of backgrounds. In this paper I experiment with the training data and input structure of an English Bi-LSTM name recognition model. I look at names from 103 countries to compare how well the model performs on names from different cultures, specifically in the context of a downstream task where extracted names will be matched to information on file. I find that a model with combined character and word input outperforms word-only models and may improve on accuracy compared to classical NER models that are not geared toward identifying unseen entity values.1 Introduction and BackgroundNamed entity recognition (NER) is a natural language processing (NLP) task in which a computer finds and labels elements in a piece of text that fall into a set of predetermined categories, such as person names, locations, medical terminology, and organization names. This task is