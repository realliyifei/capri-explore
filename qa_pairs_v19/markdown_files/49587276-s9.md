# A Survey on Recent Advances in Named Entity Recognition from Deep Learning models

## Question

How are character labels converted into word labels in character-level architectures for NER models?

## URLs

1. https://ar5iv.org/html/2203.06925. [2203.06925] WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named Entity Recognition
2. https://ar5iv.org/html/2401.10825. [2401.10825] A survey on recent advances in Named Entity Recognition
3. https://ar5iv.org/html/1709.04109. [1709.04109] Empower Sequence Labeling with Task-Aware Neural Language Model
4. https://ar5iv.org/html/1705.04044. [1705.04044] End-to-end Recurrent Neural Network Models for Vietnamese Named Entity Recognition: Word-level vs. Character-level
5. https://ar5iv.org/html/1910.11470. [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
6. https://ar5iv.org/html/2011.06727. [2011.06727] A Survey on Recent Advances in Sequence Labeling from Deep Learning Models
7. https://ar5iv.org/html/2401.12941. [2401.12941] Multicultural Name Recognition For Previously Unseen Names
8. https://ar5iv.org/html/2212.09897. [2212.09897] Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training
9. https://ar5iv.org/html/2002.00738. [2002.00738] An Efficient Architecture for Predicting the Case of Characters using Sequence Models
10. https://ar5iv.org/html/2109.01636. [2109.01636] Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding

## Answer (I don't know?: ✗)

In character-level architectures for Named Entity Recognition (NER), character labels are transformed into word labels through a post-processing step following the predictions made by the model. Initially, a sentence is treated as a sequence of characters, which is then processed by a recurrent neural network (RNN). This network predicts labels for each character in the input sequence [1][2]. 

Once the character-level predictions are made, a post-processing mechanism is employed to convert these character labels into corresponding word labels. This step is crucial because while the model can accurately predict labels at the character level, these must be aggregated or mapped to reflect the correct labels for entire words within the sentence [1][2]. 

This process of transformation is necessary for NER tasks since entities are typically identified at the word level rather than the character level. For instance, in scenarios where character information helps define word boundaries or identify specific entity types, post-processing will ensure that the predictions align with the grammatical and syntactic structure of the input data [1]. 

Overall, the combination of character-level predictions and the subsequent transformation into word labels enables these architectures to leverage the granular information provided at the character level while still producing meaningful outputs corresponding to words, which is essential for effective NER performance.

1. [1]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
2. [2]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
3. [3]:  https://ar5iv.org/html/2401.12941, [2401.12941] Multicultural Name Recognition For Previously Unseen Names
4. [4]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
5. [5]:  https://ar5iv.org/html/1910.11470, [1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
---
1. [1]:  Passage ID 1: Character level architecturesCharactersChar EmbeddingChar LSTM-FChar LSTM-BChar RepresentationLabelBB-ORGeB-ORGsB-ORGtB-ORG␣B-ORGBB-ORGuB-ORGyB-ORG’OsOFigure 2: Character level NN architecture for NERIn this model, a sentence is taken to be a sequence of characters. This sequence is passed through an RNN, predicting labels for each character (Figure 2).Character labels transformed into word labels via post processing.The potential of character NER neural models was first highlighted by ?) using highway networks over convolution neural networks (CNN) on character sequences of words and then using another layer of LSTM + softmax for the final predictions.This model was implemented by ?) for Vietnamese NER and achieved 80.23% F-score on ?)’s Vietnamese test data.Character models were also used in various other languages like Chinese [Dong et al. (2016] where it has achieved near state of the art performance.?) proposed CharNER 111111Code:
2. [2]:  Passage ID 2: Character level architecturesCharactersChar EmbeddingChar LSTM-FChar LSTM-BChar RepresentationLabelBB-ORGeB-ORGsB-ORGtB-ORG␣B-ORGBB-ORGuB-ORGyB-ORG’OsOFigure 2: Character level NN architecture for NERIn this model, a sentence is taken to be a sequence of characters. This sequence is passed through an RNN, predicting labels for each character (Figure 2).Character labels transformed into word labels via post processing.The potential of character NER neural models was first highlighted by ?) using highway networks over convolution neural networks (CNN) on character sequences of words and then using another layer of LSTM + softmax for the final predictions.This model was implemented by ?) for Vietnamese NER and achieved 80.23% F-score on ?)’s Vietnamese test data.Character models were also used in various other languages like Chinese [Dong et al. (2016] where it has achieved near state of the art performance.?) proposed CharNER 111111Code:
3. [3]:  Passage ID 3: while the best F scores for Spanish, Dutch, and German were obtained by an RNN with a character, word, and affix architecture by Yadav et al. (2018). These results were obtained using datasets from the CoNLL02 and CoNLL03 shared tasks (Tjong Kim Sang, 2002) (Tjong Kim Sang andDe Meulder, 2003). Another shared task, DrugNER, evaluated NER results on a corpus of medical and drug terminology. On this task, the word and character level architecture from Yadav et al. (2018) performed better than just word level architecture alone from Chalapathy et al. (2016).This paper will compare models trained on combined word and character input and those trained solely on word input. Based on previous studies and the intuition that character level input can allow the model to learn sub-word character patterns that can be used to identify commonalities between names and contexts seen in training and those seen the test data or real world applications, I hypothesize that the word and character level
4. [4]:  Passage ID 4: implemented a model with a CNN over the characters of word, concatenated with word embeddings of the central word and its neighbors, fed to a feed forward network, and followed by the Viterbi algorithm to predict labels for each word. The model achieved 82.21% F score on Spanish CoNLL 2002 data and 71.23% F score on Portuguese NER data [Santos and Cardoso (2007].CharactersChar EmbeddingChar LSTM-FChar LSTM-BWord FeaturesWord RepresentationWord LSTM-FWord LSTM-BWord CRFLabelBestBestB-ORGBuyBuyI-ORG’s’sOFigure 3: Word+character level NN architecture for NERThe second type of model concatenates word embeddings with LSTMs (sometimes bi-directional) over the characters of a word,passing this representation through another sentence-level Bi-LSTM, and predicting the final tags using a final softmax or CRF layer (Figure 3). ?)131313Code: https://github.com/glample/tagger introduced this architecture and achieved 85.75%, 81.74%, 90.94%, 78.76% Fscores on Spanish, Dutch, English and
5. [5]:  Passage ID 5: implemented a model with a CNN over the characters of word, concatenated with word embeddings of the central word and its neighbors, fed to a feed forward network, and followed by the Viterbi algorithm to predict labels for each word. The model achieved 82.21% F score on Spanish CoNLL 2002 data and 71.23% F score on Portuguese NER data [Santos and Cardoso (2007].CharactersChar EmbeddingChar LSTM-FChar LSTM-BWord FeaturesWord RepresentationWord LSTM-FWord LSTM-BWord CRFLabelBestBestB-ORGBuyBuyI-ORG’s’sOFigure 3: Word+character level NN architecture for NERThe second type of model concatenates word embeddings with LSTMs (sometimes bi-directional) over the characters of a word,passing this representation through another sentence-level Bi-LSTM, and predicting the final tags using a final softmax or CRF layer (Figure 3). ?)131313Code: https://github.com/glample/tagger introduced this architecture and achieved 85.75%, 81.74%, 90.94%, 78.76% Fscores on Spanish, Dutch, English and