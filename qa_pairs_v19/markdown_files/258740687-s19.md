# A Survey on Zero Pronoun Translation

## Question

Why are general-purpose metrics insufficient for evaluating pronoun-aware translation quality in zero pronoun translation?

## URLs

1. https://ar5iv.org/html/2410.18697. [2410.18697] How Good Are LLMs for Literary Translation, Really? Literary Translation Evaluation with Humans and LLMs
2. https://ar5iv.org/html/2304.12959. [2304.12959] Why are we still translating sentences?
3. https://ar5iv.org/html/2406.07081. [2406.07081] Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning
4. https://ar5iv.org/html/2402.01404. No content available
5. https://ar5iv.org/html/2410.20941. [2410.20941] Instruction-Tuned LLMs Succeed in Document-Level MT Without Fine-Tuning—But BLEU Turns a Blind Eye
6. https://ar5iv.org/html/2310.14855. [2310.14855] Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing
7. https://ar5iv.org/html/2412.01340. [2412.01340] A 2-step Framework for Automated Literary Translation Evaluation: Its Promises and Pitfalls
8. https://ar5iv.org/html/1808.10196. [1808.10196] Pronoun Translation in English–French Machine Translation: An Analysis of Error Types
9. https://ar5iv.org/html/2401.06468. [2401.06468] Adapting Large Language Models for Document-Level Machine Translation

## Answer (I don't know?: ✓)

I don't know. The context provided does not specifically address the topic of pronoun-aware translation quality or zero pronoun translation, thus I cannot provide a precise answer regarding the insufficiency of general-purpose metrics for evaluating these aspects. The passages focus on broader themes related to the evaluation of machine translation quality, the challenges of existing metrics, and the need for more nuanced evaluations, but they do not mention pronouns or specific cases like zero pronoun translation.

1. [1]:  https://ar5iv.org/html/2306.13041, No Title
2. [2]:  https://ar5iv.org/html/2305.12544, No Title
3. [3]:  https://ar5iv.org/html/2306.13041, No Title
4. [4]:  https://ar5iv.org/html/2401.05176, No Title
5. [5]:  https://ar5iv.org/html/2401.05176, No Title
---
1. [1]:  Passage ID 1: and, mediately, also contribute to better and more transparent machine translation systems.1 IntroductionThe field of evaluation metrics for Natural Language Generation (NLG), especially machine translation (MT) is in a crisis (Marie et al. 2021).111We published an earlier version of this paper (Leiter et al. 2022a) under a different title. Both versions consider the conceptualization of explainable metrics and are overall similar. However, while the old version contains several novel experiments, this new version puts a stronger emphasis on the survey of approaches for the explanation of MT metrics including the latest LLM based approaches. For example, this comprises techniques that return fine-grained error labels or natural language explanations.Despite the development of multiple high-quality evaluation metrics in recent years (Zhao et al. 2019; Zhang et al. 2020a; Rei et al. 2020; Sellam et al. 2020; Yuan et al. 2021), the Natural Language Processing (NLP) community
2. [2]:  Passage ID 2: as work to date has primarily focused on English or other high-resource languages Mondal et al. (2022) but devoted less efforts towards minority languages. Additionally, the lack of human evaluation of NLP-based health systems has made it challenging to measure their effectiveness in the real world. Current automatic evaluation metrics do not necessarily speak to patient outcomes. Hence, human-centric studies must be conducted in evaluating the efficacy of NLP-powered tools in healthcare.Research Directions.1.Healthcare benchmark construction. Although the documentation of recent LLMs reports very high performance for various medical question answering benchmarks, or medical licensing texts, there are many other tasks in healthcare that lack the data required to achieve similarly good performance. Access to medical datasets is often limited because of privacy issues, and therefore other approaches may be required to compile such benchmarks. Synthetic datasets are one such
3. [3]:  Passage ID 3: multiple high-quality evaluation metrics in recent years (Zhao et al. 2019; Zhang et al. 2020a; Rei et al. 2020; Sellam et al. 2020; Yuan et al. 2021), the Natural Language Processing (NLP) community appears hesitant to adopt them for assessing NLG systems (Marie et al. 2021; Gehrmann et al. 2023). Empirical investigations of Marie et al. (2021) indicate that the majority of MT papers relies on surface-level evaluation metrics such as BLEU and METEOR (Papineni et al. 2002; Banerjee and Lavie 2005), which were created two decades ago, a trend that may allegedly have worsened in recent times.These surface-level metrics cannot (even) measure semantic similarity of their inputs and are thus fundamentally flawed, particularly when it comes to assessing the quality of recent state-of-the-art MT systems (e.g. Peyrard 2019; Freitag et al. 2022),raising concerns about the credibility of the scientific field.We argue that the potential reasons for this neglect of recent high-quality metrics
4. [4]:  Passage ID 4: Our results highlight the challenges in relying solely on automated scores to assess translation quality, as they may not fully capture the intricacies of dimensions such as adherence to norms, cultural sensitivity, clarity, and practicality. Human evaluation as a more contextual and culture-specific way of assessment is necessary to obtain a comprehensive understanding of translation quality.What implications do these results have for future research in TQA? First, we should acknowledge the limitations of traditional metrics. Our findings highlight that metrics like BLEU and chrF, which primarily focus on n-gram overlap and exact phrasing, cannot effectively capture the superior performance of language models such as ChatGPT in a range of critical dimensions for high-quality human-like translations. This calls for the development and adoption of more nuanced evaluation metrics that consider cultural aspects and contextual appropriateness. Next, carefully crafted and customized
5. [5]:  Passage ID 5: translations. This calls for the development and adoption of more nuanced evaluation metrics that consider cultural aspects and contextual appropriateness. Next, carefully crafted and customized prompts are needed to fully unleash the great potential of ChatGPT as a capable machine translator. Our results show that providing a single example or some relevant contextual information can greatly reduce its errors and drive up its scores in all the analytic rubrics. This highlights the practical significance of tailoring prompts to guide the generation process and enhance the translation quality of ChatGPT.6 ConclusionThis study compares the translation quality of ChatGPT and three NMT engines by using both automated metrics and human evaluation. For the former, we compute four widely-adopted metrics - BLEU, chrF, COMET, and BERTScore - and find that they fail to distinguish high-quality and lower-quality translations. For the latter, we conduct annotation based on both the