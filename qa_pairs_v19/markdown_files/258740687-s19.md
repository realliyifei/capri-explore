# A Survey on Zero Pronoun Translation

## Question

Why are general-purpose metrics insufficient for evaluating pronoun-aware translation quality in zero pronoun translation?

## URLs

1. https://ar5iv.org/html/2410.18697. [2410.18697] How Good Are LLMs for Literary Translation, Really? Literary Translation Evaluation with Humans and LLMs
2. https://ar5iv.org/html/2304.12959. [2304.12959] Why are we still translating sentences?
3. https://ar5iv.org/html/2406.07081. [2406.07081] Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning
4. https://ar5iv.org/html/2402.01404. No content available
5. https://ar5iv.org/html/2410.20941. [2410.20941] Instruction-Tuned LLMs Succeed in Document-Level MT Without Fine-Tuning—But BLEU Turns a Blind Eye
6. https://ar5iv.org/html/2310.14855. [2310.14855] Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing
7. https://ar5iv.org/html/2412.01340. [2412.01340] A 2-step Framework for Automated Literary Translation Evaluation: Its Promises and Pitfalls
8. https://ar5iv.org/html/1808.10196. [1808.10196] Pronoun Translation in English–French Machine Translation: An Analysis of Error Types
9. https://ar5iv.org/html/2401.06468. [2401.06468] Adapting Large Language Models for Document-Level Machine Translation

## Answer (I don't know?: ✗)

General-purpose metrics for evaluating machine translation (MT) often fall short, particularly when it comes to assessing pronoun-aware translation quality, including in cases of zero pronoun translation, due to their focus and inherent limitations.

Firstly, these metrics tend to prioritize the adequacy of content words while neglecting the specific nuances associated with pronoun translation. According to the context, general-purpose MT evaluation methods, like those used in WMT, are generally more robust to minor disfluencies but may overlook the complexities involved in translating pronouns effectively [3]. This focus can obscure the fine-grained evaluation needed to identify errors in pronoun usage, particularly in cases where pronouns may be omitted altogether (zero pronoun translation).

Moreover, the challenges associated with pronouns extend beyond mere agreement of gender and number. The study highlights that both the functional role of pronouns and their referential properties are crucial for successful translation [2][3]. Most general-purpose metrics do not adequately capture these two dimensions. Without assessing how well a system recognizes and handles pronouns in different contexts—especially in zero pronoun scenarios—evaluators may overlook significant errors that impact translation quality.

Additionally, manual evaluation methods have shown considerable disagreement, further emphasizing the difficulty in reliably assessing pronoun translation [4]. This inconsistency suggests that even expert human evaluators struggle with pronoun-related translation aspects, which are not well represented by general-purpose metrics that might provide a broad overview but miss these specific difficulties.

The complexity of translating pronouns arises not only from the need for gender and number agreement but also from contextual factors that influence how pronouns are used within sentences. The research points out that lacking awareness of pronoun functions leads to confusion between different types of pronouns, such as personal pronouns and demonstratives [4]. This type of knowledge is not quantifiable through traditional metrics, which makes them insufficient for a detailed evaluation of pronoun usage in translation.

Lastly, the results of recent Transformer-based NMT systems indicate that while progress has been made, there is still considerable room for improvement, particularly concerning zero pronoun cases and cross-sentence dependencies [5]. This complexity of performance, combined with evaluation difficulties, suggests that a new approach or dedicated metrics focusing on pronoun fidelity and contextual understanding is necessary for a more accurate assessment.

In summary, general-purpose metrics are insufficient for evaluating pronoun-aware translation quality in zero pronoun scenarios because they fail to address the intricacies of pronoun function and referential properties, tend to overlook critical errors, and do not capture the complexities of human judgment on these issues. They lack the specificity required to evaluate a translation's effectiveness in handling pronouns, particularly in challenging contexts.

1. [1]:  https://ar5iv.org/html/2304.12959, [2304.12959] Why are we still translating sentences?
2. [2]:  https://ar5iv.org/html/1808.10196, [1808.10196] Pronoun Translation in English–French Machine Translation: An Analysis of Error Types
3. [3]:  https://ar5iv.org/html/1808.10196, [1808.10196] Pronoun Translation in English–French Machine Translation: An Analysis of Error Types
4. [4]:  https://ar5iv.org/html/1808.10196, [1808.10196] Pronoun Translation in English–French Machine Translation: An Analysis of Error Types
5. [5]:  https://ar5iv.org/html/1808.10196, [1808.10196] Pronoun Translation in English–French Machine Translation: An Analysis of Error Types
---
1. [1]:  Passage ID 1: Linguistics.Guillou and Hardmeier (2016)Liane Guillou and Christian Hardmeier. 2016.PROTEST: A test suitefor evaluating pronouns in machine translation.In Proceedings of the Tenth International Conference onLanguage Resources and Evaluation (LREC’16), pages 636–643,Portorož, Slovenia. European Language Resources Association (ELRA).Guo et al. (2019)Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and ZhengZhang. 2019.Star-transformer.In Proceedings of the 2019 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers), pages 1315–1325,Minneapolis, Minnesota. Association for Computational Linguistics.Hendy et al. (2023)Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, HitokazuMatsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023.How good are gpt models atmachine translation? a comprehensive
2. [2]:  Passage ID 2: the manual annotation and assessment of theperformance of nine English–French MT systems against the PROTEST test suite, acomparison with the manual evaluation from the DiscoMT 2015 shared task (for asubset of the systems), and a detailed corpus study highlighting some of thecommon categories of errors revealed in a meta-evaluation of the humanjudgements.The results of our study confirm that pronoun translation remains a seriousproblem for rule-based, statistical and neural MT. They strengthen previousresults indicating that rendering pronouns in translation requires modellingboth functional and referential properties [3], and revealsevere weaknesses in previous modelling attempts that only addressed theseproblems in part. We find that neural MT does not automatically resolve theproblem of pronoun translation. While early NMT approaches fail to outperformSMT on pronouns, a recent Transformer-based NMT system [5] achievesvery promising results for non-anaphoric pronouns
3. [3]:  Passage ID 3: gap-filling and the PROTEST test suite evaluation reveals problems ofboth kinds. It also shows the danger of using non-native speakers as evaluators,resulting in a high number of annotation errors despite best efforts.General-purpose MT evaluation methods such as those used at WMT [29]arguably focus more on the adequacy of content words and may be more robust tominor disfluencies. The effect of pronoun translation on general-purpose humanMT evaluation is an interesting follow-up problem for future work.8.2 MT PerformanceGender agreement was long assumed to be the most important problem that neededto be addressed to solve the issue of pronoun translation[1, 30]. This was eventually recognised to beinsufficient, and Guillou suggested that the function of pronouns was anotherimportant factor affecting their translation [3]. Ourevaluation results confirm that both of these factors play an important role.In our study, gender agreement is by far the most common error type
4. [4]:  Passage ID 4: in those caseswhere the required information is available in the scope of the context sentence.It is therefore too early to suggest that NMT has solved the problem of pronountranslation, but the results are encouraging.We find two major sources of pronoun translation errors in ourEnglish–French corpus. First, lacking awareness of pronoun functioncauses confusion between, primarily, personal pronouns and demonstratives in thetarget language. Second, lacking awareness of the referential propertiesof the pronouns results in incorrect gender and number agreement.We recommend that system developers address both of these factors, as simpleheuristic approaches can demonstrably lead to decreased performance.We also highlight that the evaluation of pronoun translation is in itself adifficult problem, as evidenced by the disagreement between the two manualevaluation methods applied to the DiscoMT data set. As an alternative to fullyautomatic evaluation we recommend the use of
5. [5]:  Passage ID 5: pronountypes and sheds light on the difficulties of the task. We find that therule-based approaches in our corpus perform poorly as a result ofoversimplification, whereas SMT and early NMT systems exhibit significantshortcomings due to a lack of awareness of the functional andreferential properties of pronouns. A recent Transformer-based NMT systemwith cross-sentence context shows very promising results on non-anaphoricpronouns and intra-sentential anaphora, but there is still considerable room forimprovement in examples with cross-sentence dependencies.1 Introduction00footnotetext: *Both authors contributed equally.Pronoun translation still poses serious challenges for machine translation (MT)systems despite years of research[1, 2, 3, 4]. This can beascribed to a combination of factors including an incomplete understanding ofthe problem, evaluation difficulties, and the fact that low system performance isoften obscured by the presence of many trivial problem