# A Survey of Deep Learning for Mathematical Reasoning

## Question

What strategies have been explored to enhance the selection of in-context examples for mathematical reasoning tasks?

## URLs

1. https://ar5iv.org/html/2411.18126. [2411.18126] Curriculum Demonstration Selection for In-Context Learning
2. https://ar5iv.org/html/2412.03966. [2412.03966] Demonstration Selection for In-Context Learning via Reinforcement Learning
3. https://ar5iv.org/html/2405.16265. [2405.16265] MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time
4. https://ar5iv.org/html/2308.00304. [2308.00304] Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models
5. https://ar5iv.org/html/2310.09881. [2310.09881] In-Context Learning with Iterative Demonstration Selection
6. https://ar5iv.org/html/2411.16454. [2411.16454] Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem Solving with Computational Graph-Based Retrieval
7. https://ar5iv.org/html/2408.10839. [2408.10839] Benchmarking Large Language Models for Math Reasoning Tasks
8. https://ar5iv.org/html/2411.15645. [2411.15645] MC-NEST – Enhancing Mathematical Reasoning in Large Language Models with a Monte Carlo Nash Equilibrium Self-Refine Tree
9. https://ar5iv.org/html/2407.05682. [2407.05682] Retrieved In-Context Principles from Previous Mistakes

## Answer (I don't know?: ✗)

To enhance the selection of in-context examples for mathematical reasoning tasks, several strategies have been explored, particularly within the framework of few-shot prompting. This approach is crucial because the type and quality of examples used can significantly impact the performance of large language models (LLMs) in solving mathematical word problems (MWPs).

1. **Semantic Similarity-Based Retrieval**: One primary strategy involves leveraging semantic similarity-based retrieval methods to select examples that are contextually relevant to the current problem. The goal here is to choose problems with reasoning paths similar to that of the query, which helps the model better understand the problem-solving process by analogy [4].

2. **Careful Example Selection**: The choice of examples in few-shot prompting is emphasized as a critical component for success. Existing methods for selecting these examples typically fall into two categories: semantic retrieval and contrasting examples, which differ in structure but still provide relevant context for reasoning [4]. By obtaining a well-curated set of exemplars, the LLM can improve its ability to generate accurate solutions.

3. **Use of Analogous Problems**: The proposed frameworks also suggest that, when humans confront a new reasoning problem, they often draw upon previously solved problems that share similar reasoning paths [5]. Applying this principle, the retriever component of the proposed systems identifies analogous problems from a corpus, which can then be used as examples during the LLM's reasoning process. This method enables the LLM to mimic effective problem-solving strategies observed in those analogous examples.

4. **Optimization and Refinement**: Beyond initial example selection, the processes of optimization and refinement of selected examples play a role in enhancing the quality of in-context examples. Techniques that refine examples based on previous successes or failures can lead to improved model performance [2]. For instance, the number of refinement iterations might be adjusted according to the specific characteristics of the task at hand.

5. **Addressing Hallucinations and Trustworthiness**: Recognizing the challenges that LLMs face, such as hallucinations (producing incorrect responses based on their training data) and issues related to trustworthiness, additional strategies are employed to tighten the selection of examples to mitigate these risks. Prior research highlights the need for robust, generalizable strategies that improve the reliability of results in complex reasoning tasks [3].

By employing these varied strategies—semantic retrieval, careful selection, the use of similar analogs, optimization processes, and robust methodologies—a more effective framework for providing in-context examples for mathematical reasoning tasks can be established. These approaches are aimed not only at improving accuracy but also at enhancing the overall robustness and efficiency of LLMs when tackling complex mathematical word problems [1] [2] [5].

1. [1]:  https://ar5iv.org/html/2408.10839, [2408.10839] Benchmarking Large Language Models for Math Reasoning Tasks
2. [2]:  https://ar5iv.org/html/2408.10839, [2408.10839] Benchmarking Large Language Models for Math Reasoning Tasks
3. [3]:  https://ar5iv.org/html/2408.10839, [2408.10839] Benchmarking Large Language Models for Math Reasoning Tasks
4. [4]:  https://ar5iv.org/html/2411.16454, [2411.16454] Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem Solving with Computational Graph-Based Retrieval
5. [5]:  https://ar5iv.org/html/2411.16454, [2411.16454] Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem Solving with Computational Graph-Based Retrieval
---
1. [1]:  Passage ID 1: in different contexts.While previous research has often evaluated these methods on rather limited datasets (Qiao et al. 2023) or using similar methods (Luo et al. 2023), there remains a need for more comprehensive analysis. Addressing this research gap, our study presents an extensive benchmarking of seven advanced methods across five widely-used datasets. Additionally, whereas prior work has mainly focused on accuracy as the main performance metric, we also consider time and cost factors, for deploying these algorithms effectively.More specifically, to tackle mathematical reasoning with LLMs, we identify three core tasks:First, achieving high accuracy requires the model to correctly and consistently explain the reasoning path leading to the correct result. Second, robustness is demonstrated through the model’s ability to produce the correct result across multiple repetitive calls. Finally, efficient resource usage - encompassing time and API costs - is crucial for practical
2. [2]:  Passage ID 2: Overview of the mathematical reasoning methods evaluated in the benchmark, categorized into three groups: Prompt Engineering, Process Optimization, and External Engine. The primary procedure for each method is outlined. Symbols denote the presence of few-shot examples (\faListUl), the use of an external engine (\faGears), and the number of refinement iterations required (\faRepeat).Benchmarking DetailsThe goal of this benchmarking is to systematically evaluate different strategies for fitting and aligning large language models (LLMs) for mathematical reasoning. We perform a comparative analysis of these methods across five datasets, using both open-source and closed-source ground models, and evaluate performance across multiple dimensions. The following sections provide a detailed description of our benchmarking methodology.Datasets for Mathematical ReasoningTo assess the reasoning ability of the methods, we use several mathematical word problems. These tasks stem from five
3. [3]:  Passage ID 3: the problem of mathematical reasoning with LLMs from theoretical viewpoints.Lu et al. (2023) investigate the different deep learning methods used for mathematical reasoning,Ahn et al. (2024) give a thorough overview specifically covering LLMs for this task, andHuang and Chang (2023) analyse the different kinds of reasoning problems and the corresponding opportunities for LLMs.All surveys emphasize the missing generalizability and robustness, the challenges with complex questions, and the problem of hallucinations and trustworthiness.Setting the focus on the algebraic skills of LLMs, Yuan et al. (2023) find that GPT-3.5 and GPT-4 outperform the other foundation models. As key points, they mark the influence of tokenization, the importance of the model size, and the sensitivity of prompts, identifying different optimal prompts across the LLMs. However, they also emphasize that the algebraic ability of a model cannot be directly equated with the mathematical reasoning
4. [4]:  Passage ID 4: on the true computational graphs. We anticipate that more advanced methods will be developed in future work to construct high-quality training data without human labor.Figure 6: Some cases of the original and rewritten questions. The entity names, value of numbers and semantics are different after rewritting, while the computational graphs remain the same.5 Related WorkFew-shot Prompting for MWP Solving.Large Language Models have shown promising results in tackling math word problems Toshniwal et al. ; Yang et al. (2024); Yu et al. (2024a); Mirzadeh et al. (2024); Wei et al. (2022b). To enhance model performance on math word problems, few-shot prompting has become a widely adopted approach Wei et al. (2022b); Jiang et al. (2023); Melz (2023); Henkel et al. (2024). The choice of examples used in few-shot prompting is critical to its success. Existing methods for example selection generally fall into two categories: semantic similarity-based retrieval Huang et al. (2023);
5. [5]:  Passage ID 5: negatives serve as contrasting examples with different structures. 2.1 Overview of the Proposed FrameworkWhen solving a new reasoning problem, humans often draw upon known problems with similar reasoning paths and address them by analogy. In the context of math word problems, the reasoning path corresponds to its computational graph, as illustrated in Figure 1. Large language models (LLMs) are observed to fail to conduct genuine logical reasoning Mirzadeh et al. (2024) and exhibit strong token biases Li et al. (2024) when addressing reasoning tasks. Therefore, providing LLMs with the correct reasoning path from analogous problems can guide them to mimic the problem-solving process. This paper aims to develop a math word problem-solving system comprising a retriever and a generator. The retriever identifies problems and solutions with computational graphs similar to the query problem from a corpus, while the generator leverages these retrieved exemplars through in-context learning