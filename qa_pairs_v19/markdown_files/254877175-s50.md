# A Survey of Deep Learning for Mathematical Reasoning

## Question

What strategies have been explored to enhance the selection of in-context examples for mathematical reasoning tasks?

## URLs

1. https://ar5iv.org/html/2411.18126. [2411.18126] Curriculum Demonstration Selection for In-Context Learning
2. https://ar5iv.org/html/2412.03966. [2412.03966] Demonstration Selection for In-Context Learning via Reinforcement Learning
3. https://ar5iv.org/html/2405.16265. [2405.16265] MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time
4. https://ar5iv.org/html/2308.00304. [2308.00304] Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models
5. https://ar5iv.org/html/2310.09881. [2310.09881] In-Context Learning with Iterative Demonstration Selection
6. https://ar5iv.org/html/2411.16454. [2411.16454] Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem Solving with Computational Graph-Based Retrieval
7. https://ar5iv.org/html/2408.10839. [2408.10839] Benchmarking Large Language Models for Math Reasoning Tasks
8. https://ar5iv.org/html/2411.15645. [2411.15645] MC-NEST – Enhancing Mathematical Reasoning in Large Language Models with a Monte Carlo Nash Equilibrium Self-Refine Tree
9. https://ar5iv.org/html/2407.05682. [2407.05682] Retrieved In-Context Principles from Previous Mistakes

## Answer (I don't know?: ✗)

The exploration of strategies to enhance in-context learning (ICL) for mathematical reasoning tasks primarily revolves around leveraging diverse reasoning techniques and prompt engineering. These strategies have shown promise in improving the performance of large language models (LLMs) on complex numerical tasks.

1. **Reasoning Techniques**: Various reasoning strategies have been examined and compared to establish their efficacy in numerical reasoning. For instance, the self-consistency approach was found to provide a consistent advantage over other reasoning strategies, significantly improving performance across several counting tasks. This suggests that incorporating detailed reasoning procedures before arriving at answers can enhance the effectiveness of LLMs in mathematical contexts [1].

2. **Chain-of-Thought Prompting**: Chain-of-Thought (CoT) prompting is another notable strategy that has gained traction in improving LLMs' mathematical reasoning abilities. This approach involves generating intermediate reasoning steps that lead to the final answer, thus enhancing the model's ability to solve complex problems without necessitating any changes to the model's parameters. Research indicates that CoT prompting has effectively addressed the challenges LLMs face when tackling complicated reasoning tasks, thereby facilitating better solutions to mathematical problems [3] [4].

3. **In-Context Learning with Task-Specific Examples**: The use of in-context examples, particularly task-specific training data, has emerged as an efficient approach to boost LLM performance. Providing similar examples as context aids in guiding the model to generate more appropriate responses, particularly in few-shot settings. This method allows LLMs to leverage previous examples to infer the structure and reasoning required for solving new problems [1] [5].

4. **Evaluation and Pretraining Considerations**: An important consideration in enhancing in-context examples is the evaluation of reasoning based on the pretraining data of the models. Observations suggest that any assessment of reasoning capabilities must account for the influence of pretraining data, highlighting the need for a more nuanced perspective in evaluating models [2]. This indicates that selecting examples that align well with the pretraining context can further optimize reasoning performance.

5. **Prompt Engineering**: Recent advancements have highlighted the importance of prompt engineering methods, which are designed to elicit the best responses from LLMs. This involves crafting prompts that not only adhere to the structure of the tasks but also resonate with the LLMs' training experiences. By fine-tuning the prompts used for illustrating examples, researchers can significantly influence the models' performance on reasoning tasks [5].

In summary, enhancing the selection of in-context examples for mathematical reasoning tasks includes implementing various reasoning strategies like self-consistency and chain-of-thought prompting, utilizing task-specific examples within the in-context learning framework, considering pretraining influences on evaluation, and focusing on effective prompt engineering. These strategies together form a robust methodological landscape for improving LLM performance in mathematical reasoning.

1. [1]:  https://ar5iv.org/html/2410.14166, No Title
2. [2]:  https://ar5iv.org/html/2202.07206, No Title
3. [3]:  https://ar5iv.org/html/2406.06592, No Title
4. [4]:  https://ar5iv.org/html/2406.06592, No Title
5. [5]:  https://ar5iv.org/html/2411.18915, No Title
---
1. [1]:  Passage ID 1: LLMs with task-specific train data 555Motivated by benefits of reasoning procedures to counting tasks demonstrated in Section 6.1, we provide detailed reasoning before correct answers in the ground-truth responses. and evaluate on both in-distribution test data and widely adopted benchmarks. By providing similar examples as context, in-context learning (ICL) (Brown, 2020; Wei et al., 2023) has become another popular train-free method to efficiently improve LLM performance.We describe implementation details in Section C.1.6.1 Reasoning.In Figure 5, we compare diverse reasoning strategies introduced before with baseline strategies, i.e., directly responding with numeric values and open-ended generation. We find that all studied reasoning approaches are helpful to greatly improve performance over those without reasoning across four counting tasks, among which self-consistency exhibits consistent advantage over other reasoning strategies for diverse LLMs. In addition, we show
2. [2]:  Passage ID 2: observations suggest that any evaluation of reasoning that does not take the pretraining data into account is difficult to interpret, and that we need to revisit evaluation of language models with respect to their pretraining data.2 Background and MethodologyReasoning ability has long been considered as a proxy for intelligence (Johnson-Laird, 2010). Thus, developing models with this skill has been also an essential goal of AI and natural language processing (NLP) (Bommasani et al., 2021).Recently, large language models have exhibited an ability to perform reasoning-related tasks in few-shot settings without requiring any modifications to their parameters through a method called in-context learning.Our goal is to evaluate this reasoning skill in-depth for numerical induction tasks.This section provides background information on in-context learning and introduces our method for measuring the performance gap of the models on numerical reasoning tasks based on differences in
3. [3]:  Passage ID 3: is structured as follows. We discuss related work on using LLMs to solve mathematical reasoning problems in Section 2. We describe our main method in Section 3. Our experimental setup including the task, model configuration, baselines and metrics are discussed in Section 4. We showcase our superior result also in Section 4.2 Related WorkImproving Mathematical reasoning ability of LLMs.Mathematical reasoning poses significant challenges for LLMs, and it is one of the key tasks for evaluating the reasoning ability of LLMs. With a huge amount of math problems in pretraining datasets, the pretrained LLMs (OpenAI, 2023; Gemini Team et al., 2024; Touvron et al., 2023) are able to solve simple problems, yet struggle with more complicated reasoning. To overcome that, the chain-of-thought (Wei et al., 2022b; Fu et al., 2023) type prompting algorithms were proposed. These techniques were effective in improving the performance of LLMs on reasoning tasks without modifying the model
4. [4]:  Passage ID 4: is structured as follows. We discuss related work on using LLMs to solve mathematical reasoning problems in Section 2. We describe our main method in Section 3. Our experimental setup including the task, model configuration, baselines and metrics are discussed in Section 4. We showcase our superior result also in Section 4.2 Related WorkImproving Mathematical reasoning ability of LLMs.Mathematical reasoning poses significant challenges for LLMs, and it is one of the key tasks for evaluating the reasoning ability of LLMs. With a huge amount of math problems in pretraining datasets, the pretrained LLMs (OpenAI, 2023; Gemini Team et al., 2024; Touvron et al., 2023) are able to solve simple problems, yet struggle with more complicated reasoning. To overcome that, the chain-of-thought (Wei et al., 2022b; Fu et al., 2023) type prompting algorithms were proposed. These techniques were effective in improving the performance of LLMs on reasoning tasks without modifying the model
5. [5]:  Passage ID 5: developments have popularized Large Language Models (LLMs) [1, 24, 3] as potent tools for tackling complex mathematical reasoning problems [18].Different strategies encompass an extensive range of techniques, including specialized models like [21, 31], Chain-of-Thought or Program-of-Thought [25, 4], or, tool-based compositional frameworks like [8, 14, 16, 27].Progress in the study of mathematical reasoning with structured and unstructured data has been measured thanks to various datasets and benchmarks, such as [5, 30, 17].Tool-augmented LLMs are particularly prevalent in those benchmarks as they combine program assistance, data manipulation, natural language reasoning, and many other tools.These frameworks leverage the increasing planning capabilities of LLMs [26, 2] to orchestrate tools, enabling multi-step problem solving.Figure 1: Example from FinQA [5] with questions over tabular and textual data.Recent studies focus on prompt engineering methods to get the best from