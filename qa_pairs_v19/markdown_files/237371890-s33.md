# Survey of Low-Resource Machine Translation

## Question

What are the two main types of automatic metrics used in machine translation (MT) research, and how do they differ?

## URLs

1. https://ar5iv.org/html/2407.03277. [2407.03277] Evaluating Automatic Metrics with Incremental Machine Translation Systems
2. https://ar5iv.org/html/2404.02512. [2404.02512] Towards Large Language Model driven Reference-less Translation Evaluation for English and Indian Languages
3. https://ar5iv.org/html/2402.18747. [2402.18747] Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains
4. https://ar5iv.org/html/2407.12832. [2407.12832] Sentence-level Aggregation of Lexical Metrics Correlate Stronger with Human Judgements than Corpus-level Aggregation
5. https://ar5iv.org/html/2405.18348. [2405.18348] Can Automatic Metrics Assess High-Quality Translations?
6. https://ar5iv.org/html/2111.04079. [2111.04079] Variance-Aware Machine Translation Test Sets
7. https://ar5iv.org/html/2306.13041. [2306.13041] Towards Explainable Evaluation Metrics for Machine Translation
8. https://ar5iv.org/html/2106.15195. [2106.15195] Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers

## Answer (I don't know?: ✗)

The two main types of automatic metrics used in machine translation (MT) research are neural metrics and non-neural metrics. These metrics are employed to evaluate the quality of machine-translated content through comparison with reference translations, commonly represented by ground truth data.

1. **Neural Metrics**: These metrics leverage neural network architectures and are designed to correlate closely with human judgments of translation quality. They take into account the contextual and semantic understanding that neural networks can capture from language data. Recent studies have shown that neural metrics exhibit significantly higher correlation with human evaluations compared to their non-neural counterparts. For example, research by Freitag et al. (2022, 2023) highlighted the superior performance of neural metrics in aligning with human assessments of translation quality, suggesting their robustness in evaluating MT outputs [2].

2. **Non-Neural Metrics**: These typically include traditional scoring methods such as BLEU, which compare the n-gram overlaps between the machine-translated output and reference translations. Non-neural metrics do not utilize deep learning techniques and primarily rely on surface-level text matching. While these metrics are widely used, their ability to reflect the true quality of translations tends to be limited when compared with the insights provided by neural metrics. In fact, they are often considered less reliable for capturing the nuances of language and meaning present in translations [1][5].

The primary difference between these two types of metrics lies in their approach and effectiveness. Neural metrics leverage the power of deep learning models to analyze and evaluate translations, allowing them to assess context and semantics, whereas non-neural metrics focus on surface-level similarities without understanding the underlying meaning of the text. This fundamental gap influences their correlation with human judgments, as neural metrics tend to better capture the qualitative aspects of translation while non-neural metrics might overlook critical differences in nuanced language usage [2][5].

In summary, while both neural and non-neural metrics are integral to MT research, neural metrics demonstrate a stronger alignment with human evaluations, likely due to their ability to understand language at a deeper level, thus providing a more transparent and robust framework for assessing MT quality [2].

1. [1]:  https://ar5iv.org/html/2306.13041, [2306.13041] Towards Explainable Evaluation Metrics for Machine Translation
2. [2]:  https://ar5iv.org/html/2407.03277, [2407.03277] Evaluating Automatic Metrics with Incremental Machine Translation Systems
3. [3]:  https://ar5iv.org/html/2306.13041, [2306.13041] Towards Explainable Evaluation Metrics for Machine Translation
4. [4]:  https://ar5iv.org/html/2402.18747, [2402.18747] Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains
5. [5]:  https://ar5iv.org/html/2106.15195, [2106.15195] Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers
---
1. [1]:  Passage ID 1: and, mediately, also contribute to better and more transparent machine translation systems.1 IntroductionThe field of evaluation metrics for Natural Language Generation (NLG), especially machine translation (MT) is in a crisis (Marie et al. 2021).111We published an earlier version of this paper (Leiter et al. 2022a) under a different title. Both versions consider the conceptualization of explainable metrics and are overall similar. However, while the old version contains several novel experiments, this new version puts a stronger emphasis on the survey of approaches for the explanation of MT metrics including the latest LLM based approaches. For example, this comprises techniques that return fine-grained error labels or natural language explanations.Despite the development of multiple high-quality evaluation metrics in recent years (Zhao et al. 2019; Zhang et al. 2020a; Rei et al. 2020; Sellam et al. 2020; Yuan et al. 2021), the Natural Language Processing (NLP) community
2. [2]:  Passage ID 2: evaluation is resource-intensive and time-consuming, and the number of translation systems included in a meta-evaluation tends to be relatively small. In this study, we explore the use of commercial machine translations, collected weekly over a period of 6 years for 12 translation directions, for the evaluation of MT metrics. Given the common use of human A/B testing Tang et al. (2010); Caswell and Liang (2020), our base assumption is that commercial systems show real improvements over time and that we can assess metrics as to whether they prefer more recent MT outputs. Using our dataset, we revisit a number of recent findings in MT metrics research, and find that our dataset supports these.Freitag et al. (2022, 2023) revealed that neural metrics exhibit significantly higher correlation with human judgments compared to non-neural ones. In our experiments, we analyze metric scores over time and evaluate metrics’ ability to accurately rank MT systems. Our findings demonstrate that
3. [3]:  Passage ID 3: will more generally inspire explainable NLG metric design.2 Background & TerminologyIn this section, we first introduce and relate definitions and dimensions of MT metrics and explainability, which we use in the later parts of the paper. Further, we collect goals and target audiences of explainable MT metrics.2.1 Machine Translation Evaluation MetricsMT metrics grade machine translated content, the hypothesis, based on ground truth data. We differentiate these metrics along several dimensions.A summary is shown in Table 1. We note that metrics can be categorized along many dimensions (e.g. Sai et al. 2022; Celikyilmaz et al. 2020). Here, we focus on a minimal specification that we will leverage later. Some parts of this section refer to explainability (see definitions in §2.2).DimensionDescriptionInput typeWhether source, reference translation or both are used as ground truth for comparisonGranularityAt which level a metric operates: Word-level,
4. [4]:  Passage ID 4: metrics in fact robust across any domain (including domains not seen in training)? Or can their apparent strong performancebe attributed in part tothe artificially good domain match between training and test data?Figure 1: Automatic machine translation metric performance on the WMT and biomedical domains, averaged across metric types (see Figure 2 for full results).To answer these questions, we first collect human multidimensional quality metrics (MQM) annotations in the biomedical (bio) domain. Vocabulary overlap and error analysis suggest that this new dataset is distinct from the domains used in WMT.This data covers 11 language pairs and 21 translation systems, with 25k total judgments.In addition to the MQM annotations, we also create new high-quality reference translations for all directions.We release this data publicly, along with code for replication of our experiments.111github.com/amazon-science/bio-mqm-datasetNext, we examine how different types of metrics
5. [5]:  Passage ID 5: better automatic MT evaluation along with a simple meta-evaluation scoring method to assess its credibility.1 IntroductionNew research publications in machine translation (MT) regularly introduce new methods and algorithms to improve the translation quality of MT systems. In the literature, translation quality is usually evaluated with automatic metrics such as BLEU (Papineni et al., 2002) and, more rarely, by humans. To assess whether an MT system performs better than another MT system, their scores given by an automatic metric are directly compared. While such comparisons between MT systems are exhibited in the large majority of MT papers, there are no well-defined guideline nor clear prerequisites under which a comparison between MT systems is considered valid. Consequently, we assume that evaluation in MT is conducted with different degrees of thoroughness across papers and that evaluation practices have evolved over the years. What could be considered, by the research