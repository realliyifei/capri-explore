# Narrative Why-Question Answering: A Review of Challenges and Datasets

## Question

How is causation categorized in text based on causal signals and keywords?

## URLs

1. https://ar5iv.org/html/2411.10371. [2411.10371] How to Use the IEEEtran LaTeX Templates
2. https://ar5iv.org/html/2407.10020. [2407.10020] Causality extraction from medical text using Large Language Models (LLMs)
3. https://ar5iv.org/html/2305.00050. [2305.00050] Causal Reasoning and Large Language Models: Opening a New Frontier for Causality
4. https://ar5iv.org/html/2407.13054. [2407.13054] Comprehensive Review and Empirical Evaluation of Causal Discovery Algorithms for Numerical Data
5. https://ar5iv.org/html/2406.07378. [2406.07378] Large Language Models for Constrained-Based Causal Discovery
6. https://ar5iv.org/html/2208.09163. [2208.09163] UniCausal: Unified Benchmark and Repository for Causal Text Mining
7. https://ar5iv.org/html/2209.06367. [2209.06367] A Review and Roadmap of Deep Learning Causal Discovery in Different Variable Paradigms
8. https://ar5iv.org/html/2303.10112. [2303.10112] Causal Discovery from Temporal Data: An Overview and New Perspectives
9. https://ar5iv.org/html/2302.03788. [2302.03788] Toward a Theory of Causation for Interpreting Neural Code Models

## Answer (I don't know?: ✓)

I don't know. The context provided does not contain information specifically about how causation is categorized in text based on causal signals and keywords. The passages focus on generating synthetic datasets for natural language processing (NLP), classification techniques, and related tasks, but do not address the categorization of causation or the methodologies for identifying causal signals and keywords. Thus, there is insufficient information to answer your question.

1. [1]:  https://ar5iv.org/html/2410.00427, No Title
2. [2]:  https://ar5iv.org/html/2410.00427, No Title
3. [3]:  https://ar5iv.org/html/2410.00427, No Title
4. [4]:  https://ar5iv.org/html/2010.15036, No Title
5. [5]:  https://ar5iv.org/html/2209.12617, No Title
---
1. [1]:  Passage ID 1: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
2. [2]:  Passage ID 2: KG. As a last construction step, we further enriched the KG with metadata from the Semantic Scholar API, including one-sentence too long; didn’t read (TLDR) summaries, citation counts, and publication references.4 Results and Discussion4.1 Model Training and EvaluationResearch Topic Classification.In the following sections, we report the results of training and evaluating the NLP models that underpin the three-phase search process of our developed dialogue system: (1) research topic classification, (2) article text clustering, and (3) comparative article summarization.The first phase involves classifying an uttered search goal or problem description into a fitting NLP research topic. This is especially helpful for users in exploratory search settings because they may not be familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP
3. [3]:  Passage ID 3: ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the laws of thermodynamics?” Following a quality inspection of the synthetically produced questions, we assembled a training dataset of 1601 examples, consisting of 120 questions for each of the 12 topics and 161 general questions. We also derived a test dataset containing 364 examples with a balanced class distribution similar to the training dataset.In our experiments, we evaluated three classification approaches: vector similarity search, prompting a LLM (GPT-3.5-Turbo), and few-shot fine-tuning of a transformer model with the SetFit framework Tunstall et al. (2022). Concerning the vector search approach with the SPECTER2 model, we measured the cosine similarity to compare vectors of embedded user queries with paper embeddings in our vector database to retrieve the 100 most similar
4. [4]:  Passage ID 4: analysis by conceptual primitives from text and linked with commonsense concepts and named entities.Recent studies have used these contextual and transformer-based LMs in their model in various NLP tasks. Furthermore, various studies have been presented which use the domain-specific LMs for different NLP tasks. These hybrid and domain-specific LMs have improved the performance and ability to capture complex word attributes, such as semantics, OOV, context, and syntax, into account in various NLP task.4. Classification techniquesChoosing an appropriate classifier is one of the main steps in the text classification task. Without having a comprehensive knowledge of every algorithm, we cannot find out the most effective model for the text classification task. Out of many ML algorithms used in text classification, we will present some famous and commonly used classification algorithms. These are used for sentiment classification tasks such as Naïve Bayes (NB), Support vector
5. [5]:  Passage ID 5: 120; 117; 121; 118; 32. However, when the classification of questions is ambiguous, the system must allow for multiple types of answers.Analyzing the question and detecting the answer type, or finding the embeddings of most similar vectors of question and context, is one of the core tasks of QA algorithms. The QA algorithms also facilitate the query formulation during the query processing stage via parsing the passing tokens with precise descriptions in order to identify what kind of answer is supposed to be returned (e.g., paragraph, description, etc). Different strategies have been proposed to deal with such requirements, such as: Regular Expressions (regex), Part-of-Speech (POS), matched n-gram features, finding the first noun phrase after the question headword ‘wh*’, and Named Entity Recognition (NER) to identify the given tokens such as human, place, location, entity. The question types classification with different answer types have been investigated in many studies such