# A Short Survey on Sense-Annotated Corpora

## Question

How does EuroSense perform multilingual disambiguation, and what is its precision across various languages?

## URLs

1. https://ar5iv.org/html/1805.04685. [1805.04685] Huge Automatically Extracted Training Sets for Multilingual Word Sense Disambiguation
2. https://ar5iv.org/html/1802.04744. [1802.04744] A Short Survey on Sense-Annotated Corpora
3. https://ar5iv.org/html/2106.06082. [2106.06082] One Sense per Translation
4. https://ar5iv.org/html/2106.06462. [2106.06462] Semi-Supervised and Unsupervised Sense Annotation via Translations
5. https://ar5iv.org/html/2104.14694. [2104.14694] Let’s Play Mono-Poly: BERT Can Reveal Words’ Polysemy Level and Partitionability into Senses
6. https://ar5iv.org/html/2407.09861. [2407.09861] \theςυβςεςτιον Σψνταξ ανδ Γραμμαρ ιν Γρεεϰ: Λανγυαγε Ρεςουρςες

## Answer (I don't know?: ✓)

I don't know. The context provided does not contain any specific information regarding EuroSense, its methods for multilingual disambiguation, or its precision metrics across various languages. The passages present information on different concepts related to natural language processing, models, and datasets, but do not specifically address EuroSense or its performance metrics. Therefore, I cannot provide a precise answer to your question based on the given context.

1. [1]:  https://ar5iv.org/html/2105.12530, No Title
2. [2]:  https://ar5iv.org/html/2104.10810, No Title
3. [3]:  https://ar5iv.org/html/2310.18345, No Title
4. [4]:  https://ar5iv.org/html/2409.16202, No Title
5. [5]:  https://ar5iv.org/html/2112.05452, No Title
---
1. [1]:  Passage ID 1: reported accuracy of approximately 90 per cent.roRoSentiLexWe translated the MPQA lexicon by using a bilingual Romanian-English dictionary (Mihalcea, 2014).4.3 N-gramsWe have evaluated several variations of n-grams from various levels oflinguistic analysis to encode linguistic information.Given the diversity of the datasets,we used different types of n-gramsto identify those that are more effective in discriminatingdeceptive and truthful content.For each n-gram type and for each dataset we extracted unigrams, bigrams, trigrams, unigrams+bigrams, bigrams+trigrams, andunigrams+bigrams+trigrams.Some examples are shown inTable 9.Table 8: Linguistic tools used on each language for the extraction offeatures.ToolEnglishDutchRussianSpanishRomanianPhonemesEspeak-ngEspeak-ngEspeak-ngEspeak-ngEspeak-ngLemmatizerStanford CoreNLP-OpenNLP--StemmerStanford CoreNLPsnowballsnowballStanford CoreNLPsnowballPOS TaggerStanford
2. [2]:  Passage ID 2: in a natural language. The question answering systems can be categorized as:3.1.1. Single-turn MCThere has been rapid a progress after the introduction of pre-trained language models and most of the question answering systems have achieved human-level accuracy on Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016).The SQuAD dataset is a standard benchmark for the machine comprehension problem consisting of Wikipedia articles and questions posed on those articles by a group of co-workers. The system must select the right answer span for the question from all the possible answers in the given passage.Word embeddings generated using ELMo are used with BiLM and BiDAF (Seoet al., 2016) set up to achieve higher accuracy scores on SQuAD leaderboard 111https://rajpurkar.github.io/SQuAD-explorer/. The results of ELMo are improved by BERT model when used in different architecture settings such as (Zhang et al., 2019) introduced semantic information into the BERT
3. [3]:  Passage ID 3: method obtained 74% precision on their proposed Conservapedia dataset.Aleksandrova et al. (2019) proposed a semi-automatic method to construct a multi-lingual bias detection corpus, consisting of Bulgarian, French, and English sentences from Wikipedia. Their method was applicable for building a corpus from a Wikipedia archive in any language, as it does not rely on language-specific features. Additionally, they provided the performance of three baseline models, namely BoW, fastText (Joulin et al., 2017), and logistic regression (Hosmer and Lemeshow, 2000), among which BoW achieved the best overall average F-measure of 59.57% across the three languages.For neural network approaches, Hube and Fetahu (2019) employed RNN to capture the inter-dependency of words and their context. To address the weakness of RNN in modeling long-range information, a hierarchical attention mechanism (Yang et al., 2016) was adopted, which applied word-level attention on each sentence to compute sentence
4. [4]:  Passage ID 4: human-like text, answer complex questions, and perform a wide array of natural language processing (NLP) tasks. Models such as GPT (Achiam et al., 2023), LLaMA (Touvron et al., 2023), Baichuan (Yang et al., 2023), Qwen (Bai et al., 2023), GLM (Du et al., 2022), and DeepSeek-V2 (DeepSeek-AI, 2024) have set new standards, demonstrating exceptional performance across various benchmarks. These models have been widely adopted in both academic and industrial settings, highlighting their versatility and potential. However, the evaluation of LLMs is a multifaceted challenge that extends beyond mere accuracy in answering questions. It involves assessing their ability to understand context, generate coherent and relevant responses, and apply specialized knowledge effectively.Evaluation BenchmarksThe evaluation of LLMs has traditionally relied on established NLP benchmark datasets, such as SQuAD (Rajpurkar et al., 2016), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019), which focus
5. [5]:  Passage ID 5: question answering track,” in Evaluation ofMultilingual and Multi-modal Information Retrieval, C. Peters, P. Clough,F. C. Gey, J. Karlgren, B. Magnini, D. W. Oard, M. de Rijke, andM. Stempfhuber, Eds.   Berlin,Heidelberg: Springer Berlin Heidelberg, 2007, pp. 223–256.[6]S. Babych, A. Henn, J. Pawellek, and S. Padó, “Dependency-based answervalidation for German,” in CLEF 2011 Labs and Workshop, NotebookPapers, 19-22 September 2011, Amsterdam, The Netherlands, ser. CEURWorkshop Proceedings, V. Petras, P. Forner, and P. D. Clough, Eds., vol.1177.   CEUR-WS.org, 2011.[7]L. Vanderwende, “Answering and questioning for machine reading,” inMachine Reading, Papers from the 2007 AAAI Spring Symposium, 2007, pp.91–96.[8]A. Grappy, B. Grau, M. Falco, A. Ligozat, I. Robba, and A. Vilnat,“Selecting answers to questions from web documents by a robust validationprocess,” in 2011 IEEE/WIC/ACM International Conferences on WebIntelligence and Intelligent Agent