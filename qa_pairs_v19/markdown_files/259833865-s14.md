# A Survey of Evaluation Methods of Generated Medical Textual Reports

## Question

What methods and metrics are used for classifying utterances in the evaluation of medical texts?

## URLs

1. https://ar5iv.org/html/2404.15777. [2404.15777] A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry Corresponding author : Yining Huang email: huangyining1987@gmail.com
2. https://ar5iv.org/html/2007.08749. [2007.08749] Towards an Automated SOAP Note: Classifying Utterances from Medical Conversations
3. https://ar5iv.org/html/2406.15888. [2406.15888] Real-time Speech Summarization for Medical Conversations
4. https://ar5iv.org/html/2405.10630. [2405.10630] Medical Dialogue: A Survey of Categories, Methods, Evaluation and Challenges
5. https://ar5iv.org/html/2310.12489. [2310.12489] MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations
6. https://ar5iv.org/html/2303.05737. [2303.05737] Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings
7. https://ar5iv.org/html/2212.01669. [2212.01669] A Survey on Medical Document Summarization
8. https://ar5iv.org/html/2405.16402. [2405.16402] Assessing Empathy in Large Language Models with Real-World Physician-Patient Interactions
9. https://ar5iv.org/html/2412.03784. [2412.03784] Speech Recognition-based Feature Extraction for Enhanced Automatic Severity Classification in Dysarthric Speech

## Answer (I don't know?: ✗)

The evaluation of medical texts, particularly for classifying utterances, utilizes several methods and metrics that are crucial for assessing the performance of natural language processing (NLP) models in healthcare settings. 

One prominent method mentioned is the use of NLP frameworks based on advanced models such as distilBERT, a lighter variant of BERT optimized for faster computations. In a specific application targeting patient-initiated electronic health record (EHR) messages related to COVID-19, this framework was employed to classify and triage communications efficiently. The primary tasks involved identifying messages that reported positive COVID-19 test results from a dataset comprising diverse patient communications, including health inquiries and test reports [2].

In terms of evaluation metrics used to assess the effectiveness of such NLP models, the study highlighted several key metrics: classification accuracy, sensitivity, and the macro F1 score. The macro F1 score, which considers both the precision and recall across multiple classes, was reported to be 94%. Additionally, sensitivity metrics showed varying effectiveness, including a sensitivity of 85% for messages not reporting a positive COVID-19 test and a perfect sensitivity of 100% for positive test report messages [2]. These metrics are important for evaluating the ability of the NLP model to correctly classify messages and ensure that patients receive appropriate responses based on the urgency and nature of their inquiries.

Further evaluation metrics employed in broader tasks include ROUGE metrics for summarization (to measure the overlap between generated and reference summaries), BERTScore, which leverages embeddings from pre-trained BERT models to evaluate the semantic similarity of texts, and various traditional metrics for named entity recognition (NER) and relation extraction (RE) tasks, such as Precision, Recall, and F1 scores. Specifically, in experiments involving fine-tuning, lexical and semantic metrics were highlighted as vital to accurately reflect model performance and alignment with medical terminology [4][5].

Overall, the comprehensive evaluation approach integrates advanced NLP models with a variety of metrics tailored to healthcare settings. This dual focus on model performance and metrics ensures that the tools developed not only achieve high computational efficiency but also maintain relevance and accuracy in real-world medical contexts [1][3][4][5].

1. [1]:  https://ar5iv.org/html/2305.12544, No Title
2. [2]:  https://ar5iv.org/html/2404.15777, [2404.15777] A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry Corresponding author : Yining Huang email: huangyining1987@gmail.com
3. [3]:  https://ar5iv.org/html/2405.01769, No Title
4. [4]:  https://ar5iv.org/html/2408.14418, No Title
5. [5]:  https://ar5iv.org/html/2404.15777, [2404.15777] A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry Corresponding author : Yining Huang email: huangyining1987@gmail.com
---
1. [1]:  Passage ID 1: as work to date has primarily focused on English or other high-resource languages Mondal et al. (2022) but devoted less efforts towards minority languages. Additionally, the lack of human evaluation of NLP-based health systems has made it challenging to measure their effectiveness in the real world. Current automatic evaluation metrics do not necessarily speak to patient outcomes. Hence, human-centric studies must be conducted in evaluating the efficacy of NLP-powered tools in healthcare.Research Directions.1.Healthcare benchmark construction. Although the documentation of recent LLMs reports very high performance for various medical question answering benchmarks, or medical licensing texts, there are many other tasks in healthcare that lack the data required to achieve similarly good performance. Access to medical datasets is often limited because of privacy issues, and therefore other approaches may be required to compile such benchmarks. Synthetic datasets are one such
2. [2]:  Passage ID 2: in all evaluations.In a retrospective cohort study[22] focused on processing patient-initiated electronic health record (EHR) messages, a natural language processing (NLP) framework was developed based on the distilBERT model, a lighter version of BERT optimized for faster computation without significant performance loss. This NLP model was utilized to classify and triage patient communications within the EHR system, specifically targeting messages related to COVID-19. The model’s primary tasks involved accurately identifying messages reporting positive COVID-19 test results from a dataset comprising various patient-initiated EHR messages, including test reports and health inquiries. Evaluation metrics centered on classification accuracy, sensitivity, and the macro F1 score. The model demonstrated high effectiveness with a macro F1 score of 94%, and sensitivities of 85% for COVID-19 related messages (not reporting a positive test), 96% for COVID-19 positive test reports, and 100%
3. [3]:  Passage ID 3: LLMs (e.g., GPT-4 (OpenAI, 2023) and ChatGPT (OpenAI, 2022)) and their performance for medical applications; (2) open-sourced LLMs in the medical domain, including their training strategies, data, and performance; (3) multimodal medical LLMs that bridge natural language with other modalities and being applied beyond text-only tasks. In §4.3, §4.4, §4.5, and §4.6, we will delve into some of the practical applications of LLMs for clinical applications.We will present and discuss performance comparison of various task-specific methods and LLMs. Finally, in §4.7, we summarize our insights and discuss potential future directions.4.1 Tasks and Benchmarks for Medical NLPSentence Classification A fundamental task in clinical NLP is to process sentences and documents, which could help extract meaningful information from clinical documents and assist clinicians in decision-making processes. Dernoncourt & Lee (2017) proposed a dataset for sequential sentence classification, where sentences
4. [4]:  Passage ID 4: doctor-patient dialogues and their text transcripts written by human annotators. For experiments involving fine-tuning we use the NoteChat-1000 dataset (Wang et al. 2024) for training, which includes 1000 synthetic doctor-patient dialogue transcripts generated by multiple LLMs in a cooperative roleplay setting, conditioned on clinical notes.Evaluation Metrics:We utilized three types of evaluation metrics to assess the performance of the summarization models. For lexical similarity, we used the ROUGE metrics (Lin 2004), specifically focusing on ROUGE-L, which measures the longest common subsequence overlap between generated and reference summaries. To capture the semantic similarity, we employed BERTScore (Zhang et al. 2020), which uses embeddings from pre-trained BERT models to compare the contextual meaning of the texts. Additionally, recognizing the importance of accurately identifying medical terminology in summaries, we assessed the overlap of domain-specific named entities
5. [5]:  Passage ID 5: and Text Summarization. Their performance was measured across 26 datasets encompassing various types of medical data such as clinical notes, patient electronic health records (EHRs), and medical research articles. The evaluation focused on metrics like Precision, Recall, F1 scores for NER and RE tasks; Recall@1 for Entity Linking; F1 and Accuracy for Text Classification and Question Answering; alongside ROUGE and BERTScore for Text Summarization. The findings revealed LLMs’ strong zero-shot capabilities, particularly in tasks with smaller training datasets, sometimes even outperforming state-of-the-art models that were fine-tuned specifically for those tasks. However, their performance varied across different tasks and datasets, indicating no single LLM consistently outperformed others in all evaluations.In a retrospective cohort study[22] focused on processing patient-initiated electronic health record (EHR) messages, a natural language processing (NLP) framework was developed