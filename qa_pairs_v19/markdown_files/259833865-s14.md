# A Survey of Evaluation Methods of Generated Medical Textual Reports

## Question

What methods and metrics are used for classifying utterances in the evaluation of medical texts?

## URLs

1. https://ar5iv.org/html/2404.15777. [2404.15777] A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry Corresponding author : Yining Huang email: huangyining1987@gmail.com
2. https://ar5iv.org/html/2007.08749. [2007.08749] Towards an Automated SOAP Note: Classifying Utterances from Medical Conversations
3. https://ar5iv.org/html/2406.15888. [2406.15888] Real-time Speech Summarization for Medical Conversations
4. https://ar5iv.org/html/2405.10630. [2405.10630] Medical Dialogue: A Survey of Categories, Methods, Evaluation and Challenges
5. https://ar5iv.org/html/2310.12489. [2310.12489] MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations
6. https://ar5iv.org/html/2303.05737. [2303.05737] Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings
7. https://ar5iv.org/html/2212.01669. [2212.01669] A Survey on Medical Document Summarization
8. https://ar5iv.org/html/2405.16402. [2405.16402] Assessing Empathy in Large Language Models with Real-World Physician-Patient Interactions
9. https://ar5iv.org/html/2412.03784. [2412.03784] Speech Recognition-based Feature Extraction for Enhanced Automatic Severity Classification in Dysarthric Speech

## Answer (I don't know?: ✗)

In the evaluation of medical texts, particularly for classifying utterances, several methods and metrics are employed, primarily focusing on assessing performance in natural language processing (NLP) tasks. The discussions in the provided passages highlight two main approaches to classification tasks:

1. **Methods Used for Classification**:
   - **Natural Language Processing Frameworks**: The use of advanced NLP models, such as the distilBERT model, has been highlighted as effective for classifying patient-initiated EHR messages related to COVID-19. This particular model was designed for faster computation and maintained high classification effectiveness [1].
   - **Hierarchical Modeling**: In the context of classifying utterances from medical conversations, a systematic analysis was performed that adapted existing deep learning architectures to the task. This modeling captures context in a hierarchical manner—considering both word and utterance levels—which resulted in substantial improvements in classification tasks for terms like SOAP (Subjective, Objective, Assessment, Plan) notes [3].

2. **Metrics Used for Evaluation**:
   - **Classification Metrics**: Various performance metrics are employed to evaluate the effectiveness of NLP models. These include Precision, Recall, F1 scores, and the macro F1 score. Specifically, the macro F1 score indicates a balance between precision and recall across different classes, and tends to provide insights into the classifiers' overall performance [1][2].
   - **Specificity in Performance Measurement**: For utterance classification in medical contexts, accuracy, sensitivity, and other domain-specific metrics are applied. For instance, in one study, the macro F1 score recorded was 94%, alongside sensitivities that measured how well the model identified COVID-19 related messages [1]. Similarly, entity-level F1 scores and accuracy metrics were tracked for tasks involving Named Entity Recognition (NER) and Question Answering (QA) in biomedical texts [4].
   - **Benchmark Studies**: Comprehensive benchmarking studies have been highlighted to assess the performance of models across various medical text types, revealing effective yet sometimes limited capabilities in understanding biomedical text. These evaluations use metrics appropriate for different NLP tasks, showcasing the diverse capabilities and gaps in current models [4].

In summary, the methods include employing advanced NLP models and hierarchical context modeling for utterance classification in medical texts. The evaluation metrics comprise traditional classification measures (Precision, Recall, F1 scores) as well as task-specific metrics like accuracy, which together provide a comprehensive understanding of model performance in real-world medical data scenarios. 

These findings underscore the ongoing need for continued research and improvement in classification methods to enhance medical text processing capabilities [5].

1. [1]:  https://ar5iv.org/html/2404.15777, [2404.15777] A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry Corresponding author : Yining Huang email: huangyining1987@gmail.com
2. [2]:  https://ar5iv.org/html/2404.15777, [2404.15777] A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry Corresponding author : Yining Huang email: huangyining1987@gmail.com
3. [3]:  https://ar5iv.org/html/2007.08749, [2007.08749] Towards an Automated SOAP Note: Classifying Utterances from Medical Conversations
4. [4]:  https://ar5iv.org/html/2404.15777, [2404.15777] A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry Corresponding author : Yining Huang email: huangyining1987@gmail.com
5. [5]:  https://ar5iv.org/html/2007.08749, [2007.08749] Towards an Automated SOAP Note: Classifying Utterances from Medical Conversations
---
1. [1]:  Passage ID 1: in all evaluations.In a retrospective cohort study[22] focused on processing patient-initiated electronic health record (EHR) messages, a natural language processing (NLP) framework was developed based on the distilBERT model, a lighter version of BERT optimized for faster computation without significant performance loss. This NLP model was utilized to classify and triage patient communications within the EHR system, specifically targeting messages related to COVID-19. The model’s primary tasks involved accurately identifying messages reporting positive COVID-19 test results from a dataset comprising various patient-initiated EHR messages, including test reports and health inquiries. Evaluation metrics centered on classification accuracy, sensitivity, and the macro F1 score. The model demonstrated high effectiveness with a macro F1 score of 94%, and sensitivities of 85% for COVID-19 related messages (not reporting a positive test), 96% for COVID-19 positive test reports, and 100%
2. [2]:  Passage ID 2: and Text Summarization. Their performance was measured across 26 datasets encompassing various types of medical data such as clinical notes, patient electronic health records (EHRs), and medical research articles. The evaluation focused on metrics like Precision, Recall, F1 scores for NER and RE tasks; Recall@1 for Entity Linking; F1 and Accuracy for Text Classification and Question Answering; alongside ROUGE and BERTScore for Text Summarization. The findings revealed LLMs’ strong zero-shot capabilities, particularly in tasks with smaller training datasets, sometimes even outperforming state-of-the-art models that were fine-tuned specifically for those tasks. However, their performance varied across different tasks and datasets, indicating no single LLM consistently outperformed others in all evaluations.In a retrospective cohort study[22] focused on processing patient-initiated electronic health record (EHR) messages, a natural language processing (NLP) framework was developed
3. [3]:  Passage ID 3: recognition (ASR) and natural language understanding (NLU) offer potential solutions to generate these summaries automatically, but rigorous quantitative baselines for benchmarking research in this domain are lacking. In this paper, we bridge this gap for two tasks: classifying utterances from medical conversations according to (i) the SOAP section and (ii) the speaker role. Both are fundamental building blocks along the path towards an end-to-end, automated SOAP note for medical conversations. We provide details on a dataset that contains human and ASR transcriptions of medical conversations and corresponding machine learning optimized SOAP notes. We then present a systematic analysis in which we adapt an existing deep learning architecture to the two aforementioned tasks. The results suggest that modelling context in a hierarchical manner, which captures both word and utterance level context, yields substantial improvements on both classification tasks. Additionally, we develop and
4. [4]:  Passage ID 4: medical text data like clinical notes and electronic health records (EHRs). Researches in this subsection demonstrates LLMs’ capacity to improve biomedical NLP tasks. These advancements highlight the models’ potential in transforming how medical data is processed, making it more accessible and actionable for healthcare providers. Moreover, comprehensive benchmark studies assess the performance of models like ChatGPT across different types of medical text, including clinical trial descriptions and biomedical corpus. These studies use a range of metrics such as entity-level F1 scores for NER and accuracy for QA tasks to evaluate the models. The results indicate effective, though sometimes limited, capabilities in biomedical text understanding, suggesting areas for further improvement and adaptation. The evaluations discussed provide practitioners and researchers with examples of how LLMs can be applied in data processing scenarios within the healthcare sector. They offer insights into
5. [5]:  Passage ID 5: due to the small dataset and moderate inter-rater reliability (Jeblee et al., 2019). Finally, one study has examined classification of medically utterances from a sizeable dataset of medical conversations, but there was no analysis of classification performance on individual SOAP sections (Krishna et al., 2020).Though previous work has focused on extracting clinically relevant information from clinical narratives (Xu et al., 2010) and biomedical text in general (Aronson, 2001; Savova et al., 2010), conversational language used in face-to-face dialogues can differ substantially from that found in writing. Only one study to date has incorporated ASR output in the analysis of NLU techniques on medical conversations (Selvaraj and Konam, 2019). Other papers have written about end-to-end medical scribes which are in development (Finley et al., 2018a; Klann and Szolovits, 2009), outlined future research which will include ASR adaptation (Jeblee et al., 2019), and improved clinical ASR