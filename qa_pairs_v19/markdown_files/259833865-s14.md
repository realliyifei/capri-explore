# A Survey of Evaluation Methods of Generated Medical Textual Reports

## Question

What methods and metrics are used for classifying utterances in the evaluation of medical texts?

## URLs

1. https://ar5iv.org/html/2404.15777. [2404.15777] A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry Corresponding author : Yining Huang email: huangyining1987@gmail.com
2. https://ar5iv.org/html/2007.08749. [2007.08749] Towards an Automated SOAP Note: Classifying Utterances from Medical Conversations
3. https://ar5iv.org/html/2406.15888. [2406.15888] Real-time Speech Summarization for Medical Conversations
4. https://ar5iv.org/html/2405.10630. [2405.10630] Medical Dialogue: A Survey of Categories, Methods, Evaluation and Challenges
5. https://ar5iv.org/html/2310.12489. [2310.12489] MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations
6. https://ar5iv.org/html/2303.05737. [2303.05737] Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings
7. https://ar5iv.org/html/2212.01669. [2212.01669] A Survey on Medical Document Summarization
8. https://ar5iv.org/html/2405.16402. [2405.16402] Assessing Empathy in Large Language Models with Real-World Physician-Patient Interactions
9. https://ar5iv.org/html/2412.03784. [2412.03784] Speech Recognition-based Feature Extraction for Enhanced Automatic Severity Classification in Dysarthric Speech

## Answer (I don't know?: ✗)

In the evaluation of medical texts for classifying utterances, a combination of methods and metrics is employed. One prominent approach involves the use of deep learning architectures tailored for natural language processing (NLP) tasks related to medical conversations. For instance, a systematic analysis was conducted to adapt existing deep learning models specifically for classifying utterances according to SOAP sections (Subjective, Objective, Assessment, Plan) and speaker roles [5]. This task is essential for generating automated SOAP notes that summarize medical conversations effectively.

The metrics utilized for assessing the performance of these classification tasks include Precision, Recall, F1 score, and Accuracy. These metrics provide a comprehensive view of the model's ability to classify utterances accurately and are critical in capturing the balance between different types of classification errors [4] [5]. Additionally, some benchmarks such as Recall@1 and other similar metrics may be employed for specific tasks like entity recognition and linking [4].

Moreover, the application of hierarchical modeling techniques helps to improve classification performance. By capturing context at both word and utterance levels, these models can better understand the nuances in medical dialogues, leading to more accurate classifications [5]. These methods address the complexity of language and the specific needs of the medical domain, which often involve extracting relevant information from conversations that can affect patient care and clinical decisions.

For classification-related tasks in healthcare, the integration of methodologies such as automatic speech recognition (ASR) and natural language understanding (NLU) is also seen as significant, although rigorous quantitative baselines for these approaches are still developing [5]. Overall, the focus on both sophisticated modeling techniques and robust evaluation metrics underpins efforts to enhance NLP systems in medical contexts, ensuring their efficacy and reliability in handling medical texts.

1. [1]:  https://ar5iv.org/html/2305.12544, No Title
2. [2]:  https://ar5iv.org/html/2404.15777, [2404.15777] A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry Corresponding author : Yining Huang email: huangyining1987@gmail.com
3. [3]:  https://ar5iv.org/html/2405.01769, No Title
4. [4]:  https://ar5iv.org/html/2404.15777, [2404.15777] A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry Corresponding author : Yining Huang email: huangyining1987@gmail.com
5. [5]:  https://ar5iv.org/html/2007.08749, [2007.08749] Towards an Automated SOAP Note: Classifying Utterances from Medical Conversations
---
1. [1]:  Passage ID 1: as work to date has primarily focused on English or other high-resource languages Mondal et al. (2022) but devoted less efforts towards minority languages. Additionally, the lack of human evaluation of NLP-based health systems has made it challenging to measure their effectiveness in the real world. Current automatic evaluation metrics do not necessarily speak to patient outcomes. Hence, human-centric studies must be conducted in evaluating the efficacy of NLP-powered tools in healthcare.Research Directions.1.Healthcare benchmark construction. Although the documentation of recent LLMs reports very high performance for various medical question answering benchmarks, or medical licensing texts, there are many other tasks in healthcare that lack the data required to achieve similarly good performance. Access to medical datasets is often limited because of privacy issues, and therefore other approaches may be required to compile such benchmarks. Synthetic datasets are one such
2. [2]:  Passage ID 2: in all evaluations.In a retrospective cohort study[22] focused on processing patient-initiated electronic health record (EHR) messages, a natural language processing (NLP) framework was developed based on the distilBERT model, a lighter version of BERT optimized for faster computation without significant performance loss. This NLP model was utilized to classify and triage patient communications within the EHR system, specifically targeting messages related to COVID-19. The model’s primary tasks involved accurately identifying messages reporting positive COVID-19 test results from a dataset comprising various patient-initiated EHR messages, including test reports and health inquiries. Evaluation metrics centered on classification accuracy, sensitivity, and the macro F1 score. The model demonstrated high effectiveness with a macro F1 score of 94%, and sensitivities of 85% for COVID-19 related messages (not reporting a positive test), 96% for COVID-19 positive test reports, and 100%
3. [3]:  Passage ID 3: LLMs (e.g., GPT-4 (OpenAI, 2023) and ChatGPT (OpenAI, 2022)) and their performance for medical applications; (2) open-sourced LLMs in the medical domain, including their training strategies, data, and performance; (3) multimodal medical LLMs that bridge natural language with other modalities and being applied beyond text-only tasks. In §4.3, §4.4, §4.5, and §4.6, we will delve into some of the practical applications of LLMs for clinical applications.We will present and discuss performance comparison of various task-specific methods and LLMs. Finally, in §4.7, we summarize our insights and discuss potential future directions.4.1 Tasks and Benchmarks for Medical NLPSentence Classification A fundamental task in clinical NLP is to process sentences and documents, which could help extract meaningful information from clinical documents and assist clinicians in decision-making processes. Dernoncourt & Lee (2017) proposed a dataset for sequential sentence classification, where sentences
4. [4]:  Passage ID 4: and Text Summarization. Their performance was measured across 26 datasets encompassing various types of medical data such as clinical notes, patient electronic health records (EHRs), and medical research articles. The evaluation focused on metrics like Precision, Recall, F1 scores for NER and RE tasks; Recall@1 for Entity Linking; F1 and Accuracy for Text Classification and Question Answering; alongside ROUGE and BERTScore for Text Summarization. The findings revealed LLMs’ strong zero-shot capabilities, particularly in tasks with smaller training datasets, sometimes even outperforming state-of-the-art models that were fine-tuned specifically for those tasks. However, their performance varied across different tasks and datasets, indicating no single LLM consistently outperformed others in all evaluations.In a retrospective cohort study[22] focused on processing patient-initiated electronic health record (EHR) messages, a natural language processing (NLP) framework was developed
5. [5]:  Passage ID 5: recognition (ASR) and natural language understanding (NLU) offer potential solutions to generate these summaries automatically, but rigorous quantitative baselines for benchmarking research in this domain are lacking. In this paper, we bridge this gap for two tasks: classifying utterances from medical conversations according to (i) the SOAP section and (ii) the speaker role. Both are fundamental building blocks along the path towards an end-to-end, automated SOAP note for medical conversations. We provide details on a dataset that contains human and ASR transcriptions of medical conversations and corresponding machine learning optimized SOAP notes. We then present a systematic analysis in which we adapt an existing deep learning architecture to the two aforementioned tasks. The results suggest that modelling context in a hierarchical manner, which captures both word and utterance level context, yields substantial improvements on both classification tasks. Additionally, we develop and