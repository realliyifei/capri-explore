# Relational World Knowledge Representation in Contextual Language Models: A Review

## Question

What distinguishes encyclopedic knowledge bases from commonsense knowledge bases, and could you provide examples of each?

## URLs

1. https://ar5iv.org/html/2411.03964
2. https://ar5iv.org/html/2212.13428
3. https://ar5iv.org/html/2302.04752
4. https://ar5iv.org/html/2211.00989
5. https://ar5iv.org/html/2001.04170
6. https://ar5iv.org/html/2303.16421
7. https://ar5iv.org/html/2208.09292
8. https://ar5iv.org/html/2306.08302

## Answer

## I don't know?: False

Encyclopedic knowledge bases and commonsense knowledge bases differ primarily in the type of information they store and their focus. Encyclopedic knowledge bases are designed to provide factual knowledge about real-world entities, abstract concepts, or events, often represented as triplets. For instance, a triplet might include relationships such as (Barack Obama, birthPlace, Honolulu) [4]. They focus on factual accuracy and coverage, making them suitable for various domains, including general knowledge or specialized fields like biomedical or e-commerce knowledge [5].

In contrast, commonsense knowledge bases emphasize the everyday knowledge that individuals possess about the world, including social and physical reasoning that may not be explicitly documented in typical factual databases. These bases, such as ConceptNet and ASER, focus on relationships and reasoning that assist in understanding human behavior and social interactions [4] [5]. For example, commonsense knowledge might inform us that (a cat typically has fur) or (a child might cry when they are hungry), which are not strictly factual but rather contextual understandings of behavior that often appear infrequently in traditional text corpora [2].

The distinction in purpose is significant: encyclopedic knowledge bases are often utilized for applications requiring precise factual verification and structured information retrieval, such as relation extraction or machine reading [4]. In contrast, commonsense knowledge bases are critical for applications requiring a nuanced understanding of context and human behavior, such as in natural language processing tasks that involve reasoning about everyday situations [2] [3].

To summarize, an encyclopedic knowledge base like DBPedia focuses on extracting and structuring factual information, while commonsense knowledge bases such as ConceptNet provide a framework for understanding and reasoning about common human experiences and social interactions. This distinction fundamentally shapes how each type of knowledge base can be applied in natural language processing and related areas.

[1]: https://ar5iv.org/html/2305.12544, No Title
[2]: https://ar5iv.org/html/2303.16421, [2303.16421] ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models
[3]: https://ar5iv.org/html/2303.16421, [2303.16421] ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models
[4]: https://ar5iv.org/html/2305.12544, No Title
[5]: https://ar5iv.org/html/2212.13428, [2212.13428] A Survey on Knowledge-Enhanced Pre-trained Language Models

[1]: Passage ID 1: construction of such knowledge bases is an interesting direction and requires many challenges to be addressed, such as knowledge coverage, factuality of the knowledge, knowledge linking, and so on. These challenges are amplified when the knowledge bases are constructed for specialized domains such as healthcare or chemistry. However, once these problems are addressed, researchers will be able to utilize LLMs to dynamically curate a knowledge base from up-to-date raw text and an ontology for complex applications such as tracking medication interactions from articles from PubMed.3.General and Cultural Commonsense. Cultural knowledge available in NLP models is often limited to a handful of Western cultures and does not account for the vast diversity of the cultural views of the world (Arora et al., 2023). With the increasingly wide spread of NLP applications, this limitation may result in direct adverse impact on the users of these applications, by not accounting for their values,
[2]: Passage ID 2: these types of commonsense knowledge require a deeper understanding of human behavior and social interactions, and they appear infrequently in text corpora. This suggests that current LLMs need to be improved on these domains of commonsense, which requires models to go beyond superficial semantic understanding and learn about human behaviors.4 Are GPTs Aware of the Commonsense Knowledge for Answering a Question?In Section 3, we found that GPTs perform well on commonsense QA datasets. This intrigues us to explore whether GPTs are experienced experts that are aware of what knowledge is needed and can leverage the knowledge for question answering, or if they are inexperienced problem solvers that rely on memorizing a large amount of information that covers the questions.To answer this question, we sample 20 questions from each commonsense QA dataset and ask ChatGPT “What knowledge is necessary for answering this question?”. For datasets that have ≥\geq10 wrong answered
[3]: Passage ID 3: Are GPTs aware of the underlying commonsense knowledge for answering a specific question?(4) Can GPTs effectively leverage commonsense for answering questions?Answering these questions is crucial for understanding the capabilities and limitations of LLMs and for developing better methods to evaluate and improve their performance on commonsense tasks.In this paper, to evaluate models’ ability in answering commonsense questions, we use 11 commonsense QA datasets that cover 8 diverse commonsense domains including physical, social, temporal, and numerical reasoning, etc. Firstly, We ask models to answer these questions and evaluate the accuracy of their responses. To evaluate whether large language models have an understanding of the necessary commonsense knowledge for answering these questions, we ask the model to describe the necessary knowledge and evaluate whether the descriptions are accurate. To assess whether large language models can recall and describe the necessary knowledge
[4]: Passage ID 4: it is debatable whether language models truly reason or just generate statistically-alike sequences, and to what extent AI systems can learn to reason from few-shot exemplars.4 Knowledge BasesBackground.A knowledge base is a collection of facts about real-world objects, abstract concepts, or events. The knowledge inside a knowledge base is usually represented as a triplet consisting of a head entity, a tail entity, and their relationships. For instance (Barack Obama, birthPlace, Honolulu) is an example of a triplet indicating a place-of-birth relationship. Some knowledge bases focus more on factual knowledge, such as DBPedia (Auer et al., 2007) and YAGO (Suchanek et al., 2007), while others focus more on commonsense, such as ConceptNet (Speer et al., 2017) and ASER (Zhang et al., 2020).Knowledge bases have found use in many downstream applications, including relation extraction (Weston et al., 2013), machine reading (Yang and Mitchell, 2017), and reflection generation
[5]: Passage ID 5: contrast to encyclopedic knowledge, domain knowledge is knowledge of a specific, specialized field discipline, such as biomedical, e-commerce, and sentiment, which are explored a lot, as shown in Table II.Biomedical knowledge is usually represented as triples containing symptoms or diseases as head or tail entities, e.g., (bacterial pneumonia, with associated morphology, inflammation).E-commerce knowledge is formed with product names, while their descriptions are represented by a set of phrases.For example, the product “iPhone XS” is described as “iOS; 4G signal; T-Mobile service; OLED screen; …”.Sentiment knowledge could be represented in many ways, including sentiment words, word polarity, etc.3.2 Formats of KnowledgeThere are four formats of knowledge that are often incorporated into PLMs, i.e., entity lexicon, knowledge graph, plain text, and labeled images.3.2.1 Entity LexiconTo incorporate knowledge through entities, we need to integrate knowledge embeddings