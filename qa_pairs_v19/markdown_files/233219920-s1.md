# Relational World Knowledge Representation in Contextual Language Models: A Review

## Question

What distinguishes encyclopedic knowledge bases from commonsense knowledge bases, and could you provide examples of each?

## URLs

1. https://ar5iv.org/html/2411.03964. [2411.03964] What Really is Commonsense Knowledge?
2. https://ar5iv.org/html/2212.13428. [2212.13428] A Survey on Knowledge-Enhanced Pre-trained Language Models
3. https://ar5iv.org/html/2302.04752. [2302.04752] Benchmarks for Automated Commonsense Reasoning: A Survey
4. https://ar5iv.org/html/2211.00989. [2211.00989] How Stable is Knowledge Base Knowledge?
5. https://ar5iv.org/html/2001.04170. [2001.04170] Joint Reasoning for Multi-Faceted Commonsense Knowledge
6. https://ar5iv.org/html/2303.16421. [2303.16421] ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models
7. https://ar5iv.org/html/2208.09292. [2208.09292] UnCommonSense: Informative Negative Knowledge about Everyday Concepts
8. https://ar5iv.org/html/2306.08302. [2306.08302] Unifying Large Language Models and Knowledge Graphs: A Roadmap

## Answer (I don't know?: ✗)

Encyclopedic knowledge bases and commonsense knowledge bases serve different purposes and contain distinctly different types of information, which can be characterized by their structure and the kind of knowledge they represent.

Encyclopedic knowledge is generally composed of factual information regarding specific entities or categories that are not necessarily common knowledge but are nevertheless documented and can be referenced. This type of knowledge often involves simple relationships between these entities. For example, a fact such as “Ljubljana is the capital of Slovenia” is considered encyclopedic knowledge [4]. Encyclopedic knowledge bases integrate data from extensive sources like human experts, encyclopedias, and databases, resulting in structured information that can be used for reference purposes [3]. Common encyclopedic knowledge bases include Wikidata, Freebase, and Dbpedia, which contain vast amounts of structured data derived from encyclopedic sources [3].

In contrast, commonsense knowledge represents the everyday understanding of the world that people possess, often concerning routine activities and relationships rather than specific entities. It is concerned with the types of knowledge that individuals use in day-to-day reasoning but may not explicitly document in encyclopedias. Commonsense knowledge is typically expressed in a relational format, often as triples, where entities are described in terms of their relationships. An example of a commonsense relation is articulated as: (having no food, CauseDesire, go to a store) [5]. This highlights how commonsense knowledge is not just about discrete facts but involves understanding causes and effects in everyday scenarios.

The distinction is also evident in how these knowledge bases are used in various applications. For instance, commonsense knowledge is pivotal in tasks such as commonsense question answering, validation, and story generation, as it helps interpret context and infer meanings beyond the explicit information provided in texts [5]. In contrast, encyclopedic knowledge bases are primarily used for referencing facts in a precise manner, lacking the inferential reasoning that commonsense knowledge provides.

To summarize, the key distinctions between encyclopedic and commonsense knowledge bases lie in their content and application: encyclopedic knowledge focuses on specific, documented facts, while commonsense knowledge encompasses the broader, underlying understanding of everyday phenomena. Examples include encyclopedic bases like Wikidata and commonsense bases like ConceptNet, each serving its specific role in natural language processing and related applications [3] [5].

1. [1]:  https://ar5iv.org/html/2303.16421, [2303.16421] ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models
2. [2]:  https://ar5iv.org/html/2303.16421, [2303.16421] ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models
3. [3]:  https://ar5iv.org/html/2306.08302, [2306.08302] Unifying Large Language Models and Knowledge Graphs: A Roadmap
4. [4]:  https://ar5iv.org/html/2302.04752, [2302.04752] Benchmarks for Automated Commonsense Reasoning: A Survey
5. [5]:  https://ar5iv.org/html/2212.13428, [2212.13428] A Survey on Knowledge-Enhanced Pre-trained Language Models
---
1. [1]:  Passage ID 1: these types of commonsense knowledge require a deeper understanding of human behavior and social interactions, and they appear infrequently in text corpora. This suggests that current LLMs need to be improved on these domains of commonsense, which requires models to go beyond superficial semantic understanding and learn about human behaviors.4 Are GPTs Aware of the Commonsense Knowledge for Answering a Question?In Section 3, we found that GPTs perform well on commonsense QA datasets. This intrigues us to explore whether GPTs are experienced experts that are aware of what knowledge is needed and can leverage the knowledge for question answering, or if they are inexperienced problem solvers that rely on memorizing a large amount of information that covers the questions.To answer this question, we sample 20 questions from each commonsense QA dataset and ask ChatGPT “What knowledge is necessary for answering this question?”. For datasets that have ≥\geq10 wrong answered
2. [2]:  Passage ID 2: Are GPTs aware of the underlying commonsense knowledge for answering a specific question?(4) Can GPTs effectively leverage commonsense for answering questions?Answering these questions is crucial for understanding the capabilities and limitations of LLMs and for developing better methods to evaluate and improve their performance on commonsense tasks.In this paper, to evaluate models’ ability in answering commonsense questions, we use 11 commonsense QA datasets that cover 8 diverse commonsense domains including physical, social, temporal, and numerical reasoning, etc. Firstly, We ask models to answer these questions and evaluate the accuracy of their responses. To evaluate whether large language models have an understanding of the necessary commonsense knowledge for answering these questions, we ask the model to describe the necessary knowledge and evaluate whether the descriptions are accurate. To assess whether large language models can recall and describe the necessary knowledge
3. [3]:  Passage ID 3: Encyclopedic knowledge graphs are often constructed by integrating information from diverse and extensive sources, including human experts, encyclopedias, and databases. Wikidata [20] is one of the most widely used encyclopedic knowledge graphs, which incorporates varieties of knowledge extracted from articles on Wikipedia. Other typical encyclopedic knowledge graphs, like Freebase [66], Dbpedia [67], and YAGO [31] are also derived from Wikipedia. In addition, NELL [32] is a continuously improving encyclopedic knowledge graph, which automatically extracts knowledge from the web, and uses that knowledge to improve its performance over time. There are several encyclopedic knowledge graphs available in languages other than English such as CN-DBpedia [68] and Vikidia [69]. The largest knowledge graph, named Knowledge Occean (KO)777https://ko.zhonghuapu.com/, currently contains 4,8784,3636 entities and17,3115,8349 relations in both English and Chinese.2.2.2 Commonsense Knowledge
4. [4]:  Passage ID 4: rarely taught explicitly anddeals with foundational concept, e.g.“Two separate solid objects do not spatially overlapat any given time”.Common knowledge [128] is often taught explicitly andoften deals with more contingent or secondary facts, but can be assumed tobe commonly known across a broad community.“The common U.S. coins are pennies, nickels, dimes,and quarters,” is common knowledge among residents of the US.Encyclopedic knowledge generallyinvolves simple relations between specific entities or categories, and cannotbe assumed generally known; e.g.“Ljubljana is the capital of Slovenia”.Expert knowledge often involves morerelations and is mostly known only to experts (broadly construed); e.g.“The partial derivative of the magnetic field with respect to timeis the curl of the electric field.”Obviously, the boundaries between these categories are extremely vague and itwould be wasted effort to try to make them precise. However,if automated common sense reasoning
5. [5]:  Passage ID 5: language inference.3.1.3 Commonsense KnowledgeCommonsense knowledge is the routine knowledge people have of their everyday world and activities [81]. Commonly used knowledge bases are listed in Table I.Commonsense knowledge is represented as triples in KBs, where head and tail entities are more often phrases than just words, which are different from encyclopedic knowledge.For example, a commonsense triple is like (having no food, CauseDesire, go to a store), while an encyclopedic triple is like (China, capital, Beijing).As shown in Table 1, ConceptNet [9] is the most widely used open-domain commonsense knowledge graph containing 34 relations, such as RelatedTo, IsA, Causes, etc.It’s helpful in commonsense question answering, commonsense validation, and commonsense story generation.ATOMIC [26] focuses on inferential knowledge organized by “if-then” structure, e.g., “if X pays Y a compliment, then Y will likely return the compliment”, covering causes vs. effects, agents