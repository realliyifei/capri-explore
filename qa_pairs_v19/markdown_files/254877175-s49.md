# A Survey of Deep Learning for Mathematical Reasoning

## Question

How does Chain-of-thought prompting (CoT) improve the mathematical reasoning capabilities of large language models?

## URLs

1. https://ar5iv.org/html/2201.11903. [2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
2. https://ar5iv.org/html/2411.11984. [2411.11984] Understanding Chain-of-Thought in LLMs through Information Theory
3. https://ar5iv.org/html/2404.14812. [2404.14812] Pattern-Aware Chain-of-Thought Prompting in Large Language Models
4. https://ar5iv.org/html/2405.13004. [2405.13004] MathDivide: Improved mathematical reasoning by large language models
5. https://ar5iv.org/html/2403.14312. [2403.14312] ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting
6. https://ar5iv.org/html/2305.15408. [2305.15408] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
7. https://ar5iv.org/html/2409.12183. [2409.12183] To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning
8. https://ar5iv.org/html/2406.06592. [2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision
9. https://ar5iv.org/html/2410.21333. [2410.21333] Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse

## Answer (I don't know?: ✗)

Chain-of-Thought prompting (CoT) enhances the mathematical reasoning capabilities of large language models (LLMs) through several mechanisms, primarily by encouraging multi-step reasoning, providing structured problem-solving, and leveraging demonstrated examples for improved task performance.

1. **Encouraging Multi-Step Reasoning**: CoT prompts guide LLMs to break down complex reasoning tasks into a series of intermediary steps. This process mirrors human cognitive strategies, which improves the model's ability to tackle difficult tasks such as arithmetic and symbolic reasoning. Research indicates that when models employ CoT, they produce significantly superior results compared to those providing direct answers without reasoning steps [1] [3]. The structured approach helps the model effectively handle problems that would otherwise be too complex to solve in a single step.

2. **Structured Problem-Solving**: By generating intermediate computations or reasoning steps, CoT allows LLMs to produce human-readable explanations of their thought processes, rendering their conclusions more transparent and comprehensible. This transparency is crucial, especially in mathematical contexts, where understanding the 'why' behind an answer is as important as the answer itself [4]. The cumulative knowledge built during these reasoning steps leads to a more thorough exploration of the problem space, aiding in reaching correct solutions.

3. **Quality of Demonstrations**: The effectiveness of CoT is heavily influenced by the quality of the examples or demonstrations used in prompts. When LLMs are confronted with prompts that encourage step-by-step reasoning, they tend to perform better. However, in cases where no examples are provided—known as Zero-Shot-CoT—models can experience difficulties and exhibit hallucination-related issues, leading to incorrect or nonsensical answers [3]. Manually designing high-quality demonstrations can alleviate this problem, suggesting that the input quality directly impacts the CoT's success in reasoned problem-solving.

4. **Improvement Through Iterative Refinement**: Empirical studies demonstrate that refining CoT prompts through iteration improves their effectiveness. More specific reasoning steps yield a highly complete chain of thought, resulting in more accurate answers. This iterative process not only enhances the clarity of the model's reasoning but also solidifies its ability to arrive at correct solutions through more logical and organized thought patterns [5].

5. **Advanced Decoding Strategies**: Traditional greedy decoding strategies, which can limit the performance of CoT, have been improved by techniques such as self-consistency decoding. This strategy allows the model to explore multiple reasoning paths and arrive at a consensus answer, further boosting performance in tasks requiring significant reasoning depth and complexity [2].

In summary, CoT prompts enhance the mathematical reasoning capabilities of LLMs by structuring their thought processes, improving problem handling through multi-step reasoning, ensuring high-quality demonstrations, and refining reasoning paths iteratively. These methods collectively lead to more effective reasoning and problem-solving outcomes in complex mathematical tasks.

1. [1]:  https://ar5iv.org/html/2305.15408, [2305.15408] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
2. [2]:  https://ar5iv.org/html/2406.06592, [2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision
3. [3]:  https://ar5iv.org/html/2404.14812, [2404.14812] Pattern-Aware Chain-of-Thought Prompting in Large Language Models
4. [4]:  https://ar5iv.org/html/2409.12183, [2409.12183] To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning
5. [5]:  https://ar5iv.org/html/2403.14312, [2403.14312] ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting
---
1. [1]:  Passage ID 1: studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. By using circuit complexity theory, we first give impossibility results showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a
2. [2]:  Passage ID 2: cost-effective compared to existing methods.1 IntroductionDespite significant progress in various large language model (LLM) benchmarks achieved through simply scaling up the model (Wei et al., 2022a), the development of complex reasoning abilities, particularly in tasks like mathematical problem-solving and code generation, necessitates a deeper understanding and remains an active research frontier.Chain-of-Thought (CoT) Prompting (Wei et al., 2022b) was proposed to guide the LLM to break down a reasoning task into a sequence of intermediate steps, similar to a human’s reasoning process. CoT boosts the performance of LLMs on many reasoning tasks, but the greedy decoding strategy limits its performance. To address that, Wang et al. (2023) proposed the self-consistency decoding strategy, leveraging multiple reasoning paths to reach a voted answer. Besides these prompting focused work, fine-tuning LLM with question and CoT solution pairs (Perez et al., 2021; Ouyang et al., 2022)
3. [3]:  Passage ID 3: of Automation, Chinese Academy of Sciences{yufeng.zhang, xuepeng.wang}@ia.ac.cn{lingxiang.wu, jinqiao.wang}@nlpr.ia.ac.cn1 IntroductionLarge language models (LLMs) have been proven highly effective in solving complex reasoning tasks. One technique contributing to their success is the chain-of-thought (CoT) prompting Wei et al. (2022b), which motivates the LLMs to perform multi-step reasoning instead of providing direct answers. This approach can significantly enhance the model’s ability to handle challenging tasks such as arithmetic and symbolic questions.Generally, the overall effectiveness of CoT relies on the quality of the demonstrations provided. When confronted with no examples but only the prompt “Let’s think step by step”, known as Zero-Shot-CoT Kojima et al. (2022), LLMs struggle with reasoning and encounter hallucination-related issues. While manually designing demonstrations for each question can alleviate such problems Wei et al. (2022b), it comes with a
4. [4]:  Passage ID 4: the mean improvement from CoT across experiments).1 IntroductionChain-of-thought (CoT) (Nye et al., 2022; Wei et al., 2022) has become a widely used prompting technique for eliciting reasoning from language models. CoT can provide human-readable explanations of how problems are solved (Joshi et al., 2023; Lanham et al., 2023), but most frequently it is invoked to improve an LLM’s ability to answer complex questions via intermediate computation (Madaan & Yazdanbakhsh, 2022; Wang et al., 2023a; Dziri et al., 2023). Current post-training schemes for LLMs heavily infuse CoT capabilities into models: systems like ChatGPT or Llama 3.1 default to CoT when given reasoning problems (OpenAI, 2023; Dubey et al., 2024).CoT has seen widespread usage, but it is most heavily explored in the domain of mathematical reasoning (Zhou et al., 2023a; Fu et al., 2023; Chae et al., 2024; Xu et al., 2024b; Qi et al., 2024).In fact, many “reasoning” methods for LLMs are evaluated only in the math
5. [5]:  Passage ID 5: of reasoning completeness. Second, we iteratively refine the CoT prompts to make the reasoning processes more detailed and specific. Finally, we explore the logicality of CoT prompts by interchanging the order of reasoning processes and the final answer. Based on the empirical results, we find that incorporating additional and more specific reasoning steps contributes to a highlycomplete chain-of-thought, resulting in more accurate answers and better performance in mathematical problems.Finally, the logic of reasoning before answering, used in the majority of prior studies, leads to substantial performance improvements compared to the logic of answering then explaining.Based on these findings, we aim to improve upon existing CoT prompting data and fine-tune LLMs on newly generated high-quality CoT data to enhance complex reasoning abilities of LLMs.There are three major technical contributions in our work.First, we propose CoTGenius, a Chain-of-Thought prompting improvement