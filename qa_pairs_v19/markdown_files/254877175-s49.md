# A Survey of Deep Learning for Mathematical Reasoning

## Question

How does Chain-of-thought prompting (CoT) improve the mathematical reasoning capabilities of large language models?

## URLs

1. https://ar5iv.org/html/2201.11903. [2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
2. https://ar5iv.org/html/2411.11984. [2411.11984] Understanding Chain-of-Thought in LLMs through Information Theory
3. https://ar5iv.org/html/2404.14812. [2404.14812] Pattern-Aware Chain-of-Thought Prompting in Large Language Models
4. https://ar5iv.org/html/2405.13004. [2405.13004] MathDivide: Improved mathematical reasoning by large language models
5. https://ar5iv.org/html/2403.14312. [2403.14312] ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting
6. https://ar5iv.org/html/2305.15408. [2305.15408] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
7. https://ar5iv.org/html/2409.12183. [2409.12183] To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning
8. https://ar5iv.org/html/2406.06592. [2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision
9. https://ar5iv.org/html/2410.21333. [2410.21333] Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse

## Answer (I don't know?: ✗)

Chain-of-Thought (CoT) prompting significantly enhances the mathematical reasoning capabilities of Large Language Models (LLMs) by encouraging a systematic breakdown of reasoning tasks into intermediate steps, similar to human cognitive processes. This technique helps LLMs improve their performance on complex reasoning tasks, particularly in areas like arithmetic and symbolic problem-solving [1] [5]. 

CoT prompting works by allowing the LLM to generate a sequence of reasoning steps rather than producing a direct answer. This not only provides clarity in reasoning but also increases the likelihood of arriving at the correct solution through a stepwise approach [3] [5]. It mirrors human reasoning, which typically involves breaking down a problem into manageable components. Studies show that when LLMs are prompted with a CoT approach, they can exhibit significantly better performance in reasoning tasks compared to those that do not use this prompting strategy [2] [4]. 

However, the effectiveness of CoT prompting depends considerably on the quality of demonstrations provided to the models. In scenarios where only a prompt suggesting "Let’s think step by step" is given, termed as Zero-Shot-CoT, models often struggle with reasoning and may produce erroneous outputs, known as hallucinations [5]. This indicates that the model requires high-quality examples to maximize the benefits of CoT prompting.

Additionally, it has been noted that specific decoding strategies can further enhance the performance of CoT. For instance, the self-consistency decoding strategy proposed by Wang et al. (2023) leverages multiple reasoning paths and aggregates them to derive a voted answer. This approach counters the limitations of the traditional greedy decoding strategy, which can restrict performance [3] [4]. 

Furthermore, theoretical exploration into the mechanics of CoT prompts suggests that certain types of model architectures, such as bounded-depth Transformers, may not be capable of directly deriving correct answers for complex tasks unless they are significantly large, as established using circuit complexity theory [1] [2]. In contrast, autoregressive Transformers of constant size can prove effective in generating correct outputs when equipped with CoT mechanisms, showcasing the adaptability of LLMs in solving variably complex problems.

In conclusion, Chain-of-Thought prompting enhances the mathematical reasoning capabilities of LLMs by structuring thought processes to mirror human reasoning, allowing for complex multi-step problem-solving while also highlighting the necessity of quality prompts for optimal performance. The evolution of decoding strategies alongside the application of CoT can further leverage these models' computational strengths.

1. [1]:  https://ar5iv.org/html/2305.15408, [2305.15408] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
2. [2]:  https://ar5iv.org/html/2305.15408, [2305.15408] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
3. [3]:  https://ar5iv.org/html/2406.06592, [2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision
4. [4]:  https://ar5iv.org/html/2406.06592, [2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision
5. [5]:  https://ar5iv.org/html/2404.14812, [2404.14812] Pattern-Aware Chain-of-Thought Prompting in Large Language Models
---
1. [1]:  Passage ID 1: studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. By using circuit complexity theory, we first give impossibility results showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a
2. [2]:  Passage ID 2: studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. By using circuit complexity theory, we first give impossibility results showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a
3. [3]:  Passage ID 3: cost-effective compared to existing methods.1 IntroductionDespite significant progress in various large language model (LLM) benchmarks achieved through simply scaling up the model (Wei et al., 2022a), the development of complex reasoning abilities, particularly in tasks like mathematical problem-solving and code generation, necessitates a deeper understanding and remains an active research frontier.Chain-of-Thought (CoT) Prompting (Wei et al., 2022b) was proposed to guide the LLM to break down a reasoning task into a sequence of intermediate steps, similar to a human’s reasoning process. CoT boosts the performance of LLMs on many reasoning tasks, but the greedy decoding strategy limits its performance. To address that, Wang et al. (2023) proposed the self-consistency decoding strategy, leveraging multiple reasoning paths to reach a voted answer. Besides these prompting focused work, fine-tuning LLM with question and CoT solution pairs (Perez et al., 2021; Ouyang et al., 2022)
4. [4]:  Passage ID 4: cost-effective compared to existing methods.1 IntroductionDespite significant progress in various large language model (LLM) benchmarks achieved through simply scaling up the model (Wei et al., 2022a), the development of complex reasoning abilities, particularly in tasks like mathematical problem-solving and code generation, necessitates a deeper understanding and remains an active research frontier.Chain-of-Thought (CoT) Prompting (Wei et al., 2022b) was proposed to guide the LLM to break down a reasoning task into a sequence of intermediate steps, similar to a human’s reasoning process. CoT boosts the performance of LLMs on many reasoning tasks, but the greedy decoding strategy limits its performance. To address that, Wang et al. (2023) proposed the self-consistency decoding strategy, leveraging multiple reasoning paths to reach a voted answer. Besides these prompting focused work, fine-tuning LLM with question and CoT solution pairs (Perez et al., 2021; Ouyang et al., 2022)
5. [5]:  Passage ID 5: of Automation, Chinese Academy of Sciences{yufeng.zhang, xuepeng.wang}@ia.ac.cn{lingxiang.wu, jinqiao.wang}@nlpr.ia.ac.cn1 IntroductionLarge language models (LLMs) have been proven highly effective in solving complex reasoning tasks. One technique contributing to their success is the chain-of-thought (CoT) prompting Wei et al. (2022b), which motivates the LLMs to perform multi-step reasoning instead of providing direct answers. This approach can significantly enhance the model’s ability to handle challenging tasks such as arithmetic and symbolic questions.Generally, the overall effectiveness of CoT relies on the quality of the demonstrations provided. When confronted with no examples but only the prompt “Let’s think step by step”, known as Zero-Shot-CoT Kojima et al. (2022), LLMs struggle with reasoning and encounter hallucination-related issues. While manually designing demonstrations for each question can alleviate such problems Wei et al. (2022b), it comes with a