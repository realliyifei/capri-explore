# A Survey of Deep Learning for Mathematical Reasoning

## Question

How does Chain-of-thought prompting (CoT) improve the mathematical reasoning capabilities of large language models?

## URLs

1. https://ar5iv.org/html/2201.11903. [2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
2. https://ar5iv.org/html/2411.11984. [2411.11984] Understanding Chain-of-Thought in LLMs through Information Theory
3. https://ar5iv.org/html/2404.14812. [2404.14812] Pattern-Aware Chain-of-Thought Prompting in Large Language Models
4. https://ar5iv.org/html/2405.13004. [2405.13004] MathDivide: Improved mathematical reasoning by large language models
5. https://ar5iv.org/html/2403.14312. [2403.14312] ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting
6. https://ar5iv.org/html/2305.15408. [2305.15408] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
7. https://ar5iv.org/html/2409.12183. [2409.12183] To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning
8. https://ar5iv.org/html/2406.06592. [2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision
9. https://ar5iv.org/html/2410.21333. [2410.21333] Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse

## Answer (I don't know?: ✗)

Chain-of-Thought (CoT) prompting enhances the mathematical reasoning capabilities of large language models (LLMs) by guiding them to break down complex reasoning tasks into a series of intermediate steps, akin to human reasoning processes. This methodology addresses the inherent limitations of LLMs, especially when faced with tasks that require multi-step reasoning or intricate mathematical calculations.

Firstly, CoT prompting has been shown to significantly boost LLM performance in reasoning tasks by encouraging a structural approach to problem-solving. Instead of jumping to conclusions or providing immediate answers, LLMs that utilize CoT approach problems in a stepwise fashion. This not only enhances their ability to cope with complex tasks—like arithmetic and symbolic reasoning—but also reduces the likelihood of producing incorrect or "hallucinated" outputs that often arise when models are pressured to deliver instant answers without detailed reasoning [1][5].

In addition to improving performance, the implementation of CoT helps to address the limitations posed by greedy decoding strategies traditionally used in LLMs. While these strategies are effective in many instances, they tend to restrict the model's overall reasoning capability. To overcome this, recent advancements such as self-consistency decoding have been proposed, allowing the model to consider multiple reasoning paths before converging on a final answer. This strategy further solidifies the effectiveness of CoT in driving more reliable conclusions from LLMs [3][4].

Moreover, the studies indicate that the impact of CoT is context-sensitive; the quality of demonstrations provided plays a crucial role in determining the effectiveness of CoT prompting. In situations where LLMs are exposed to well-constructed examples that highlight stepwise reasoning, they tend to generate more accurate outputs. Conversely, when faced with insufficient examples but only minimal prompts like "Let’s think step by step," LLMs often struggle, exhibiting issues with reasoning and increased errors due to hallucinations. This illustrates the dependency of CoT's success on properly structured demonstrations [5].

Further supporting the rationale for CoT's efficacy, comparative analysis using circuit complexity theory has shown that bounded-depth Transformers struggle to directly produce correct answers for straightforward mathematical tasks unless the model's size is extraordinarily large. Conversely, autoregressive Transformers of constant size can successfully handle these tasks through CoT derivations, indicating that the structural breakdown provided by CoT enables LLMs to sidestep these computational limitations [1][2].

In conclusion, CoT improves the mathematical reasoning capabilities of LLMs by promoting a methodical approach to problem-solving, enhancing reasoning accuracy through structured prompting, and overcoming traditional decoding limitations. Its effectiveness is significantly influenced by the quality of examples and demonstrations that accompany the prompts, ultimately allowing LLMs to perform complex reasoning tasks with greater efficacy.

1. [1]:  https://ar5iv.org/html/2305.15408, [2305.15408] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
2. [2]:  https://ar5iv.org/html/2305.15408, [2305.15408] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
3. [3]:  https://ar5iv.org/html/2406.06592, [2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision
4. [4]:  https://ar5iv.org/html/2406.06592, [2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision
5. [5]:  https://ar5iv.org/html/2404.14812, [2404.14812] Pattern-Aware Chain-of-Thought Prompting in Large Language Models
---
1. [1]:  Passage ID 1: studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. By using circuit complexity theory, we first give impossibility results showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a
2. [2]:  Passage ID 2: studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. By using circuit complexity theory, we first give impossibility results showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a
3. [3]:  Passage ID 3: cost-effective compared to existing methods.1 IntroductionDespite significant progress in various large language model (LLM) benchmarks achieved through simply scaling up the model (Wei et al., 2022a), the development of complex reasoning abilities, particularly in tasks like mathematical problem-solving and code generation, necessitates a deeper understanding and remains an active research frontier.Chain-of-Thought (CoT) Prompting (Wei et al., 2022b) was proposed to guide the LLM to break down a reasoning task into a sequence of intermediate steps, similar to a human’s reasoning process. CoT boosts the performance of LLMs on many reasoning tasks, but the greedy decoding strategy limits its performance. To address that, Wang et al. (2023) proposed the self-consistency decoding strategy, leveraging multiple reasoning paths to reach a voted answer. Besides these prompting focused work, fine-tuning LLM with question and CoT solution pairs (Perez et al., 2021; Ouyang et al., 2022)
4. [4]:  Passage ID 4: cost-effective compared to existing methods.1 IntroductionDespite significant progress in various large language model (LLM) benchmarks achieved through simply scaling up the model (Wei et al., 2022a), the development of complex reasoning abilities, particularly in tasks like mathematical problem-solving and code generation, necessitates a deeper understanding and remains an active research frontier.Chain-of-Thought (CoT) Prompting (Wei et al., 2022b) was proposed to guide the LLM to break down a reasoning task into a sequence of intermediate steps, similar to a human’s reasoning process. CoT boosts the performance of LLMs on many reasoning tasks, but the greedy decoding strategy limits its performance. To address that, Wang et al. (2023) proposed the self-consistency decoding strategy, leveraging multiple reasoning paths to reach a voted answer. Besides these prompting focused work, fine-tuning LLM with question and CoT solution pairs (Perez et al., 2021; Ouyang et al., 2022)
5. [5]:  Passage ID 5: of Automation, Chinese Academy of Sciences{yufeng.zhang, xuepeng.wang}@ia.ac.cn{lingxiang.wu, jinqiao.wang}@nlpr.ia.ac.cn1 IntroductionLarge language models (LLMs) have been proven highly effective in solving complex reasoning tasks. One technique contributing to their success is the chain-of-thought (CoT) prompting Wei et al. (2022b), which motivates the LLMs to perform multi-step reasoning instead of providing direct answers. This approach can significantly enhance the model’s ability to handle challenging tasks such as arithmetic and symbolic questions.Generally, the overall effectiveness of CoT relies on the quality of the demonstrations provided. When confronted with no examples but only the prompt “Let’s think step by step”, known as Zero-Shot-CoT Kojima et al. (2022), LLMs struggle with reasoning and encounter hallucination-related issues. While manually designing demonstrations for each question can alleviate such problems Wei et al. (2022b), it comes with a