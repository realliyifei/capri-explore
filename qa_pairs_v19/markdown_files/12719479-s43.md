# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

What detailed information should researchers provide when reporting reliability data in computational linguistics studies?

## URLs

1. https://ar5iv.org/html/2410.23186
2. https://ar5iv.org/html/1901.02543
3. https://ar5iv.org/html/2410.22954
4. https://ar5iv.org/html/2406.13415
5. https://ar5iv.org/html/2412.03854
6. https://ar5iv.org/html/2303.04526
7. https://ar5iv.org/html/2407.13578
8. https://ar5iv.org/html/2301.10684
9. https://ar5iv.org/html/2409.10713
10. https://ar5iv.org/html/2402.17649

## Answer

## I don't know?: False

In reporting reliability data in computational linguistics studies, researchers should adhere to established best practices that enhance the credibility and reproducibility of their findings. Based on the provided context, several key aspects should be considered.

1. **Validation Against Benchmarks**: Researchers should evaluate their results against standard benchmarks and existing implementations. This practice helps establish the consistency of results, which is essential for the credibility of their findings ([1][4]). Evidence indicates that many papers in the NLP field lack this comparative validation, raising questions about their scientific rigor ([1]).

2. **Reproducibility of Results**: It is crucial that researchers document their methodologies in such a way that allows others to replicate their work easily. This includes detailing experimental designs, data sources, and implementation specifics. Previous studies have highlighted significant difficulties in replicating results in NLP, indicating that a failure to provide clear methodologies can compromise reliability ([1]).

3. **Detailed CodeCorrection Reports**: Researchers should consider the attributes studied in software quality assurance (SQA) and report on code correctness. The passage mentions that code correctness has been largely neglected in NLP, which leads to potentially misleading findings ([4]). Demonstrating attention to code quality, including bug detection and resolution, is vital for reinforcing the reliability of reported outcomes.

4. **Source Documentation**: To address the trust deficit from users, it is recommended that researchers provide reliable sources for their outputs. This includes attaching citations and referencing relevant documents in their training data. By including sources and reasoning steps, researchers can substantiate the claims made by their models ([2]). Such practices not only enhance transparency but also allow for verification of results.

5. **Sample Efficiency Metrics**: Given the ongoing push towards data-efficient language learning, researchers should report metrics relating to sample efficiency. Establishing baselines for sample-efficient language learning can provide a clearer understanding of progress within the NLP community, offering insights into the performance of models under constrained data conditions ([3]).

6. **Human Evaluation Components**: In addition to quantitative metrics, researchers are encouraged to incorporate qualitative assessments through human evaluations. These evaluations provide an additional layer of scrutiny and can highlight aspects of model performance that automated metrics might overlook ([5]).

In conclusion, researchers in computational linguistics should comprehensively report on validation measures, reproducibility, code correctness, source documentation, sample efficiency, and human evaluation. Implementing these practices can significantly contribute to enhancing the reliability and accountability of findings in the NLP community.

[1]: https://ar5iv.org/html/2303.16166, No Title
[2]: https://ar5iv.org/html/2305.12544, No Title
[3]: https://ar5iv.org/html/2305.12544, No Title
[4]: https://ar5iv.org/html/2303.16166, No Title
[5]: https://ar5iv.org/html/2410.04981, No Title

[1]: Passage ID 1: improving research software quality within the NLP community.††\twemojilight bulbDenotes equal contributions.1 IntroductionIn the field of natural language processing (NLP), as well as in broader contexts, the validity and soundness of research findings are typically upheld by “establishing consistency of results versus existing implementations, standard benchmarks, or sanity checks via statistically significant experimental results” (Rozier and Rozier, 2014). Nevertheless, recent evidence indicates that several of these aspects are absent in many papers (Raff, 2019), questioning the scientific credibility of NLP research.On one hand, many scientists have reported difficulties in replicating the work of others, or even their own, (Prinz et al., 2011; Gundersen and Kjensmo, 2018; Wieling et al., 2018a; Chen et al., 2019; Gundersen, 2019), also in the specific context of NLP (Wieling et al., 2018b; Belz et al., 2021a; Marie et al., 2021; Narang et al., 2021; Gehrmann et al.,
[2]: Passage ID 2: a lack of trust from the users. A promising solution is to provide reliable sources for the facts output by a model, by attaching references and showing any additional reasoning steps. For example, citations can be included along with its bibliography, or pointers to documents in the training data (or a document database) can be attached to the output. Such a system should evaluate the extent to which these sources back up the claims made by the model.12 Efficient NLPBackground.Efficient NLP is a research direction aiming to optimize the use of resources for NLP models. This objective arises from theneed to address the challenges posed by the increasing scale of language models and their growing resource consumption present new challenges for NLP advances (Touvron et al., 2023b; Zhang et al., 2023).Indeed, it is widely acknowledged that scaling up is an essential approach for achieving state-of-the-art performance on NLP tasks, especially those skills emerged with the
[3]: Passage ID 3: can stay focused on the experiment and follow guidelines. Additionally, it is difficult to control confounding variables when you have no control over the subjects of the experiment.Research Directions.1.Sample-efficient language learning. This is an area ripe with opportunities to advance our understanding of language and develop more data efficient NLP tools. There is a great need for fundamental and theoretical research into sample-efficient language learning. Computational theories and algorithms for achieving state-of-the-art on smaller data regimes are an exciting area for researchers interested in core NLP, and the pursuit of the state-of-the-art performance may soon be rerouted to data-efficiency scores.Related to this direction is the goal of establishing baselines for sample-efficient lamnguage learning. Having a lower-bound goal (e.g. X hours of interaction achieving Y score) can enable the NLP community to have a more accurate understanding of progress in terms
[4]: Passage ID 4: the foundation for further research, ultimately resulting in unreliable and potentially misleading findings (McCullough et al., 2008).In light of the above considerations, this study is a call to action, underpinned by empirical evidence, to enhance the dependability of published NLP findings. In particular, our contributions are:1.We examine the extent to which research works consider the attributes studied in the field of software quality assurance (Buckley and Poston, 1984; Tripathy and Naik, 2011), or SQA (§2), and show that code correctness has been neglected by the NLP community thus far (§3);2.Through a case study on open-source implementations of the widespread Conformer architecture (Gulati et al., 2020), we prove that:-At least one impactful bug is present in all the analyzed implementations (§4.2);-Bugs do notprevent from achievinggood andreproducible results that outperformother architecturesin speech recognition and translation across
[5]: Passage ID 5: to do human evaluation: A brief introduction to user studies in nlp.Natural Language Engineering, 29(5):1199–1222.SciScore (2024)SciScore. 2024.The best methods review tool for scientific research.https://sciscore.com/.Accessed: 12 June 2024.Semmelrock et al. (2023)Harald Semmelrock, Simone Kopeinik, Dieter Theiler, Tony Ross-Hellauer, and Dominik Kowald. 2023.Reproducibility in machine learning-driven research.arXiv preprint arXiv:2307.10320.Soliman and Siponen (2022)Wael Soliman and Mikko Siponen. 2022.What do we really mean by rigor in information systems research?Su et al. (2023)Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2023.One embedder, any task: Instruction-finetuned text embeddings.In Findings of the Association for Computational Linguistics: ACL 2023, pages 1102–1121, Toronto, Canada. Association for Computational