# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

What detailed information should researchers provide when reporting reliability data in computational linguistics studies?

## URLs

1. https://ar5iv.org/html/2410.23186. [2410.23186] Reliability of Topic Modeling
2. https://ar5iv.org/html/1901.02543. [1901.02543] Computational Register Analysis and Synthesis
3. https://ar5iv.org/html/2410.22954. [2410.22954] Retrieval-Augmented Generation with Estimation of Source Reliability
4. https://ar5iv.org/html/2406.13415. [2406.13415] Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators
5. https://ar5iv.org/html/2412.03854. [2412.03854] What Do Machine Learning Researchers Mean by "Reproducible"?
6. https://ar5iv.org/html/2303.04526. [2303.04526] Student’s t-Distribution: On Measuring the Inter-Rater Reliability When the Observations are Scarce
7. https://ar5iv.org/html/2407.13578. [2407.13578] Large Language Models as Reliable Knowledge Bases?
8. https://ar5iv.org/html/2301.10684. [2301.10684] Consistency is Key: Disentangling Label Variation in Natural Language Processing with Intra-Annotator Agreement
9. https://ar5iv.org/html/2409.10713. [2409.10713] “The Data Says Otherwise” – Towards Automated Fact-checking and Communication of Data Claims
10. https://ar5iv.org/html/2402.17649. [2402.17649] Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs

## Answer (I don't know?: ✗)

When researchers report reliability data in computational linguistics studies, they should include several detailed aspects to enhance the transparency and trustworthiness of their findings. Firstly, it is critical for researchers to report their Intra-Rater Reliability (IRR) scores along with confidence intervals. This practice should be encouraged as it makes the evaluations more meaningful and reliable [1]. The implementation of statistical methods, such as Student’s t-Distribution, when providing these reliability metrics, can further justify the findings [1].

Additionally, the extent to which intra-annotator agreement is reported is notably low in the field, with only about 0.07% of over 80,000 papers in the ACL Anthology including this measurement [2]. Despite the acknowledgment that intra-annotator agreement may be irrelevant for many papers, its importance in those that do suggest it should not be overlooked. Consequently, researchers should strive to document how often they calculate and report intra-annotator agreement to highlight its significance in ensuring data quality.

Moreover, a systematic comparison of different estimators of factual confidence in Large Language Models (LLMs) is recommended. Researchers have expressed a lack of clarity regarding how various estimation methods perform relative to one another [3]. Therefore, they should describe the experimental framework, including the methods used for comparison, the contexts within which the data was collected, and specific metrics applied to evaluate reliability.

When using topic models (such as LDA), researchers should remember that increasing the number of topics may enhance feature representation but could also impair the model's reliability in corpus-based analysis. The inherent randomness in topic models can introduce significant variability [5]. Thus, researchers are encouraged to incorporate reliability measures as a standard part of their analysis, ensuring that their findings are robust and interpretable. This can involve presenting reliability metrics analogous to standard errors in statistical modeling, thus providing a clear picture of the reliability of subsequent analyses [5].

In conclusion, by reporting IRR scores with confidence intervals, providing context on their methodology, ensuring the systematic comparison of estimators, and emphasizing the reliability of models used, researchers can significantly strengthen the credibility of their findings in computational linguistics studies.

1. [1]:  https://ar5iv.org/html/2303.04526, [2303.04526] Student’s t-Distribution: On Measuring the Inter-Rater Reliability When the Observations are Scarce
2. [2]:  https://ar5iv.org/html/2301.10684, [2301.10684] Consistency is Key: Disentangling Label Variation in Natural Language Processing with Intra-Annotator Agreement
3. [3]:  https://ar5iv.org/html/2406.13415, [2406.13415] Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators
4. [4]:  https://ar5iv.org/html/2406.13415, [2406.13415] Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators
5. [5]:  https://ar5iv.org/html/2410.23186, [2410.23186] Reliability of Topic Modeling
---
1. [1]:  Passage ID 1: if only one extra observation.We encourage practitioners and researchers to report their IRR scores and confidence intervals in all evaluations, e.g. using Student’s t-Distribution method whenever possible; thus making the NLP evaluation more meaningful, transparent, and trustworthy.1 IntroductionHuman evaluations have been always the gold standard to judge the quality of natural language processing (NLP) system’s outputs Han et al. (2021); Freitag et al. (2021); Gladkoff and Han (2022).This applies to many sub-tasks including machine translation (MT) Han et al. (2020); Han (2022a); Charalampidou and Gladkoff (2022); MILAD (2022),text summarisation Bhandari et al. (2020); Latif et al. (2009), question answering Alrdahi et al. (2020), information extraction Wu et al. (2022); Nenadic et al. (2004), and prediction Yang et al. (2009), as well as domain applications such as social media, biomedical and clinical domains knowledge representation Milošević et al. (2019); Yang et al.
2. [2]:  Passage ID 2: Anthology of the Association for Computational Linguistics (ACL).222https://aclanthology.org/Here, we wish to discover for which tasks and what purposes NLP researchers collect and report on repeat annotations and evidence for how and when repeat items should be presented to annotators.Full details of the review methodology are available in Appendix A.To what extent and why is intra-annotator agreement reported in NLP?When we conducted our study, the search and filtering process returned only 56 relevant publications out of more than 80,000 papers listed in the Anthology.In other words, a tiny fraction (around 0.07%) of computational linguistics and NLP publications in the repository report measurement of intra-annotator agreement.333We acknowledge that intra-annotator agreement is irrelevant to a large proportion of papers, but highlight that the number of publications which report it is nevertheless extremely low.Publication dates of included papers range from 2005 to 2022,
3. [3]:  Passage ID 3: } else { localStorage.setItem("ar5iv_theme", "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Factual Confidence of LLMs: on Reliability and Robustness of Current EstimatorsMatéo Mahaut Universitat Pompeu Fabramateo.mahaut@upf.edu\ANDLaura Aina  Paula Czarnowska  Momchil Hardalov Thomas Müller  Lluís MàrquezAWS AI Labs {eailaura, czarpaul, momchilh, thomul, lluismv}@amazon.com Work conducted during an internship at AWS AI Labs.AbstractLarge Language Models (LLMs) tend to be unreliable in the factuality of their answers.To address this problem, NLP researchers have proposed a range of techniques to estimate LLM’s confidence over facts. However, due to the lack of a systematic comparison, it is not clear how the different methods compare to one another. To fill this gap, we present a survey and empirical comparison of estimators of factual confidence. We define an experimental framework allowing for fair comparison,
4. [4]:  Passage ID 4: on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP ’19, pages 4396–4406, Hong Kong, China.Wang et al. (2023a)Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, et al. 2023a.Survey on factuality in large language models: Knowledge, retrieval and domain-specificity.arXiv preprint arXiv:2310.07521.Wang et al. (2023b)Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b.Self-consistency improves chain of thought reasoning in language models.In The Eleventh International Conference on Learning Representations, ICLR ’23, Kigali, Rwanda.Weidinger et al. (2021)Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021.Ethical
5. [5]:  Passage ID 5: accuracy, and reliability. While increasing the number of topics in an LDA model can enhance feature representation for predictive tasks, it can also compromise the model’s reliability when used for corpus-based analysis. Furthermore, inherent randomness in topic models can introduce significant variability, making results less interpretable and reliable.Our reliability method provides a vital tool that should be a standard component of any topic model-based research, serving as a cornerstone for assessing robustness of topic models and ensuring reliability of subsequent analyses. This method can be extended to alternative topic modeling methods like NMF and BERTopic, a planned direction for our future work (Lee and Seung, 1999; Grootendorst, 2022). Researchers should prioritize reliability as a fundamental component of topic model analysis, akin to the significance of standard errors in statistical modeling. By reporting reliability measures, researchers can present a more