# "Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking

## Question

How is the text-to-text paradigm being utilized in Dialogue State Tracking (DST)?

## URLs

1. https://ar5iv.org/html/2305.13721
2. https://ar5iv.org/html/2311.06345
3. https://ar5iv.org/html/2402.10466
4. https://ar5iv.org/html/2402.01828
5. https://ar5iv.org/html/2207.14627
6. https://ar5iv.org/html/2305.13710
7. https://ar5iv.org/html/2203.08568
8. https://ar5iv.org/html/2404.14809
9. https://ar5iv.org/html/2407.16347

## Answer

The text-to-text paradigm is increasingly being utilized in Dialogue State Tracking (DST) as a method to reformulate the traditional DST tasks into structured text generation or question-answering formats. This approach enhances the adaptability and scalability of DST systems, while making the process more interpretable. 

Historically, DST was primarily based on slot-filling tasks, which involved predicting specific slot values corresponding to predefined categories in a dialogue, such as hotel rating or location [2]. However, researchers have started to frame DST as a structured text generation task. For instance, one way to interpret a user request like "Find me a 3 star hotel" is to deduce that the user wants the slot "hotel-star" to equal 3 [2]. By recasting these slot predictions into question-answering formats, models can ask questions such as "What is the hotel star rating the user wants?" and simply predict the answer, which in this case would be 3. This method leverages natural language, making it easier for models to learn from dialogues in a way that is more conducive to continual learning, as it emphasizes tasks that are fundamentally understandable through natural language [2].

Moreover, the transition to this text-to-text paradigm offers enhancements through the incorporation of neural network models, particularly transformer-based architectures like BERT, which have demonstrated marked improvements in DST performance [1]. Such models process and analyze sentences to understand context more effectively and generate appropriate responses accordingly. [4] mentions that advancements in DST—including the introduction of models like the Slot-utterance Matching Belief Tracker (SUMBT)—capitalize on attention mechanisms, significant for efficiently estimating conversational context. These mechanisms allow systems to focus on relevant parts of the input sequence, thereby improving context retention and enabling more accurate slot predictions.

The shift towards using powerful language models, including Codex and ChatGPT, signifies an embrace of this text-to-text approach, which is crucial for tackling more complex dialogue states without heavily relying on domain-specific ontologies [5]. Traditional lexicon-dependent methods faced scalability challenges when dealing with longer and more intricate dialogues, but transforming DST into a question-answering framework mitigates the need for labor-intensive lexicon crafting, making the overall process more agile and scalable [5].

In summary, the text-to-text paradigm in DST is being utilized through the conversion of slot-filling tasks into question-answering formats, providing a natural language interface that enhances interpretability, decreases reliance on structured ontology, and facilitates the application of advanced neural architectures, ultimately aiming for better dialogue management and user interaction [2] [4] [5].

[1]: https://ar5iv.org/html/2311.15623, No Title
[2]: https://ar5iv.org/html/2305.13721, [2305.13721] Continual Dialogue State Tracking via Example-Guided Question Answering
[3]: https://ar5iv.org/html/2311.15623, No Title
[4]: https://ar5iv.org/html/2410.22767, No Title
[5]: https://ar5iv.org/html/2404.08559, No Title

[1]: Passage ID 1: Engineering Management,the Chinese University of Hong Kong{xhfeng, wuxx, hmmeng}@se.cuhk.edu.hk                      Helen Meng1 IntroductionDialogue State Tracking (DST) serves as a pivotal component in task-oriented dialogue systems, typically representing dialogue states as slot-value pairs bounded by the application domain’s ontology. DST’s role is to predict the ongoing dialogue state, utilizing it to generate system responses to user queries.While neural network models, particularly those employing transformer-based architectures like BERT Devlin et al. (2018), have achieved substantial performance enhancements in DST (Balaraman et al., 2021), surpassing traditional heuristic-based approaches, they also present challenges. These models, due to their intricate architectures, offer limited visibility and control during training and inference processes. The cascading non-linear transformations from dialogue data to dialogue state are intricate and obscure. The
[2]: Passage ID 2: presents an illustrated overview.2.1 DST as question answeringDialogue state tracking (DST) is defined as estimating the beliefs of a user’s goals at every turn in a dialogue.It was traditionally formulated as a slot-filling task Wu et al. (2020); Heck et al. (2020), and more recently as a structured text generation task Hosseini-Asl et al. (2020); Peng et al. (2021); Su et al. (2022), shown in (0) in Figure 2.If a user were to say “Find me a 3 star hotel.", the goal is to deduce hotel-star = 3.However, we can also indirectly achieve the same predictions by reformulating DST as a collection of per-slot questions to answer Gao et al. (2019); Lin et al. (2021a).Given the same user request, we can ask our model to answer “What is the hotel star rating the user wants?" and have it predict 3.We hypothesize that this question answering approach is more conducive to continual learning because it leverages a general skill that is understandable through natural language.We only need
[3]: Passage ID 3: studies, for simplicity, often avoided using external contextual knowledge, instead, relying on transformations of latent representations in PLMs as sources of contextual knowledge. Although these latent representations are context-aware, they remain susceptible to interpretability issues. Some solutions, such as using a specialized neural component to extract contextualized word representations Peters et al. (2018) Liu et al. (2019), require additional fine-tuning.In contrast, our approach employs a computationally efficient algorithm to capture semantic patterns in the entire training corpus and generates features that guide attention to semantically important tokens in the input sequence. This process and the resulting features can be easily analyzed and understood.3 Model3.1 Task: Dialogue State TrackingDialogue State Tracking (DST) involves receiving the dialogue history, the current dialogue turn, and dialogue state history as inputs and yields the updated dialogue
[4]: Passage ID 4: systems, or chatbots, have become increasingly popular due to their ability to automate user tasks and provide accurate answers. These systems process and understand natural language, track conversation context through a conversation manager, and generate appropriate responses [32].To understand natural language and predict dialogue states, DST uses deep neural Network. [3] proposed a dialogue state generator using a copying mechanism to address dependency on domain ontology and lack of cross-domain knowledge sharing. [15] introduced the Slot-utterance Matching Belief Tracker (SUMBT) with an Attention mechanism, and [33] used attention mechanisms to efficiently estimate conversational context.Advances in Generation-Based Chatbots and DST Generation-based chatbots operate across various domains, with research focused on reducing dependence on ontology. [34] introduced a Neural Belief Tracking (NBT) framework using CNNs, and [35] applied a BERT-style model for non-categorical
[5]: Passage ID 5: ChatGPT and Codex.2.   Related WorkDialogue State TrackingDST plays a crucial role in natural language understanding within task-oriented dialogue systems.In the early years, DST methods (Lee et al., 2019; Zhang et al., 2020) heavily relied on manually crafted lexicons to capture dialogue states.However, this approach faced challenges in scaling up to longer and more intricate dialogues.This difficulty arose from the need for more high-quality annotated data in emerging domains and the reliance on labor-intensive, hand-crafted lexicons.To address these limitations, Wu et al. (2019) and Le et al. (2020) shifted their focus to open vocabulary DST research.This transition aimed to diminish the reliance on manually crafted lexicons, offering a more adaptable and scalable approach.With the widespread adoption of large language models, Hu et al. (2022) and Heck et al. (2023) have turned to powerful language models like Codex-Davinci-002 and ChatGPT to tackle the DST