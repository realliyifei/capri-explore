# "Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking

## Question

How is the text-to-text paradigm being utilized in Dialogue State Tracking (DST)?

## URLs

1. https://ar5iv.org/html/2305.13721. [2305.13721] Continual Dialogue State Tracking via Example-Guided Question Answering
2. https://ar5iv.org/html/2311.06345. [2311.06345] Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking
3. https://ar5iv.org/html/2402.10466. [2402.10466] Large Language Models as Zero-shot Dialogue State Tracker through Function Calling
4. https://ar5iv.org/html/2402.01828. [2402.01828] Retrieval Augmented End-to-End Spoken Dialog Models
5. https://ar5iv.org/html/2207.14627. [2207.14627] "Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking
6. https://ar5iv.org/html/2305.13710. [2305.13710] Using Textual Interface to Align External Knowledge for End-to-End Task-Oriented Dialogue Systems
7. https://ar5iv.org/html/2203.08568. [2203.08568] In-Context Learning for Few-Shot Dialogue State Tracking
8. https://ar5iv.org/html/2404.14809. [2404.14809] A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications
9. https://ar5iv.org/html/2407.16347. [2407.16347] FactTrack: Time-Aware World State Tracking in Story Outlines

## Answer (I don't know?: ‚úó)

The text-to-text paradigm is increasingly being utilized in Dialogue State Tracking (DST) to enhance the model's ability to understand and predict user goals throughout a conversation. Traditionally, DST has been approached as a slot-filling task, where specific information is extracted from user inputs to update the dialogue state. However, recent advancements have seen a shift towards treating DST as a question-answering task focused on understanding user intent expressed in natural language. 

This innovative approach is exemplified in the reformulation of DST as a collection of per-slot questions to answer based on user inputs. For instance, instead of simply filling a slot like "hotel-star = 3" when the user states, "Find me a 3 star hotel," models are now designed to ask, "What is the hotel star rating the user wants?" and predict the response of 3. This method leverages a more general skill‚Äînatural language understanding‚Äîmaking it more interpretable and flexible [1][2].

The text-to-text paradigm aligns well with the continuous learning objectives of DST. By reformulating the DST task into fine-grained question-answering tasks that are inherently understandable, models can continuously adapt and improve their predictions based on dialogue history rather than being restricted to predefined templates or structures. For example, techniques that utilize example-guided learning enhance the model's ability to answer questions concerning the dialogue state by providing relevant historical context, thereby improving overall performance in continual learning contexts [1][3][5].

Moreover, the advantages of adopting this paradigm include the capacity to develop more robust and flexible dialogue systems. Traditional DST models have faced challenges, such as limited adaptability to user queries and variability in conversation flow. By implementing a text-to-text framework, the models can more effectively update and maintain a comprehensive representation of the dialogue state, responding more adeptly to dynamic user interactions [2][4].

In conclusion, the text-to-text paradigm in DST facilitates a shift towards more flexible, natural language-driven interactions, allowing models to engage in continual learning and robustly handle various dialogue scenarios. This modern approach not only improves the accuracy of dialogue state representations but also enhances user experience through more intuitive and responsive conversational agents [3][5]. Thus, leveraging the text-to-text methodology in DST is establishing a new standard for developing sophisticated task-oriented dialogue systems in NLP.

1. [1]:  https://ar5iv.org/html/2305.13721, [2305.13721] Continual Dialogue State Tracking via Example-Guided Question Answering
2. [2]:  https://ar5iv.org/html/2207.14627, [2207.14627] "Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking
3. [3]:  https://ar5iv.org/html/2207.14627, [2207.14627] "Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking
4. [4]:  https://ar5iv.org/html/2305.13710, [2305.13710] Using Textual Interface to Align External Knowledge for End-to-End Task-Oriented Dialogue Systems
5. [5]:  https://ar5iv.org/html/2305.13721, [2305.13721] Continual Dialogue State Tracking via Example-Guided Question Answering
---
1. [1]:  Passage ID 1: presents an illustrated overview.2.1 DST as question answeringDialogue state tracking (DST) is defined as estimating the beliefs of a user‚Äôs goals at every turn in a dialogue.It was traditionally formulated as a slot-filling task¬†Wu et¬†al. (2020); Heck et¬†al. (2020), and more recently as a structured text generation task¬†Hosseini-Asl et¬†al. (2020); Peng et¬†al. (2021); Su et¬†al. (2022), shown in (0) in Figure¬†2.If a user were to say ‚ÄúFind me a 3 star hotel.", the goal is to deduce hotel-star = 3.However, we can also indirectly achieve the same predictions by reformulating DST as a collection of per-slot questions to answer¬†Gao et¬†al. (2019); Lin et¬†al. (2021a).Given the same user request, we can ask our model to answer ‚ÄúWhat is the hotel star rating the user wants?" and have it predict 3.We hypothesize that this question answering approach is more conducive to continual learning because it leverages a general skill that is understandable through natural language.We only need
2. [2]:  Passage ID 2: to a more robust downstream policy van Niekerk et¬†al. (2021).These studies are rare in their kind and call for more similar work.5 ConclusionDialogue state tracking is a crucial component of a conversational agent to identify the user‚Äôs needs at each turn of the conversation.A growing body of work is addressing this task and we have outlined the latest developments.After giving an overview of the task and the different datasets available, we have categorized modern neural approaches according to the inference of the dialogue state.Despite encouraging results on benchmarks such as MultiWOZ, these systems lack flexibility and robustness, which are critical skills for a dialogue system.In recent years, many works have sought to address these limitations and we have summarized the advances.However, there are still significant challenges to be addressed in the future.There are many interesting avenues and we have proposed three key features of DST models to guide future
3. [3]:  Passage ID 3: These systems are typically implemented through a modular architecture that provides more control and allows interaction with a database, desirable features for a commercial application.Among other components, dialogue state tracking (DST) seeks to update a representation of the user‚Äôs needs at each turn, taking into account the dialogue history.It is a key component of the system: the downstream dialogue policy uses this representation to predict the next action to be performed (e.g. asking for clarification). A response is then generated based on this action.Beyond processing isolated turns, DST must be able to accurately accumulate information during a conversation and adjust its prediction according to the observations to provide a summary of the dialogue so far.Figure 1: Dialogue state tracking (DST) example taken from MultiWOZ. DST seeks to update the dialogue state at each turn as (slot, value) pairs. Current DST models struggle to parse U3subscriptùëà3U_{3}, which requires
4. [4]:  Passage ID 4: and the corresponding database entity. Meanwhile, the textual interface in our pipeline demonstrates its effectiveness as a front-end for superior knowledge representation.More details are in Section¬†3.Task-oriented dialogue systems have been extensively studied for various applications that involve natural language interactions between users and machines.These systems are designed to accomplish specific tasks, such as booking a hotel, ordering food, or scheduling an appointment.The traditional paradigm for building such systems is to use a modularized design Pieraccini et¬†al. (1992); Young (2006); Young et¬†al. (2013); Zhao and Esk√©nazi (2016), where a dialogue state is maintained across modules to track the progress of the conversation and to interact with external databases.It generally incorporates Dialogue State Tracking (DST), database query or API calls, Natural Language Generation (NLG), and lexicalization to create the final system response.However, the traditional
5. [5]:  Passage ID 5: reformulation for dialogue state tracking (DST), a key component of dialogue systems that estimates the user‚Äôs goal as a conversation proceeds.We restructure DST to eliminate service-specific structured text and unify data from all services by decomposing each DST sample to a bundle of fine-grained example-guided question answering tasks.Our reformulation encourages a model to learn the general skill of learning from an in-context example to correctly answer a natural language question that corresponds to a slot in a dialogue state.With a retriever trained to find examples that introduce similar updates to dialogue states, we find that our method can significantly boost continual learning performance, even for a model with just 60M parameters.When combined with dialogue-level memory replay, our approach attains state-of-the-art performance on continual learning metrics without relying on any complex regularization or parameter expansion methods.1 IntroductionFigure 1: Left: