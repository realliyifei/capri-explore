# "Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking

## Question

How is the text-to-text paradigm being utilized in Dialogue State Tracking (DST)?

## URLs

1. https://ar5iv.org/html/2305.13721. [2305.13721] Continual Dialogue State Tracking via Example-Guided Question Answering
2. https://ar5iv.org/html/2311.06345. [2311.06345] Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking
3. https://ar5iv.org/html/2402.10466. [2402.10466] Large Language Models as Zero-shot Dialogue State Tracker through Function Calling
4. https://ar5iv.org/html/2402.01828. [2402.01828] Retrieval Augmented End-to-End Spoken Dialog Models
5. https://ar5iv.org/html/2207.14627. [2207.14627] "Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking
6. https://ar5iv.org/html/2305.13710. [2305.13710] Using Textual Interface to Align External Knowledge for End-to-End Task-Oriented Dialogue Systems
7. https://ar5iv.org/html/2203.08568. [2203.08568] In-Context Learning for Few-Shot Dialogue State Tracking
8. https://ar5iv.org/html/2404.14809. [2404.14809] A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications
9. https://ar5iv.org/html/2407.16347. [2407.16347] FactTrack: Time-Aware World State Tracking in Story Outlines

## Answer (I don't know?: ✗)

The text-to-text paradigm in Dialogue State Tracking (DST) is leveraged through a structured text generation approach, which reformulates traditional slot-filling tasks into a question-answering format. This shift allows the DST system to infer user goals more flexibly and intuitively. Traditionally, DST was viewed as a task focused on filling specific slots, where the model would extract values for predefined properties based on user input. For example, if a user requests "Find me a 3-star hotel," the traditional approach would involve directly associating the slot "hotel-star" with the value "3" [2].

In contrast, the new paradigm presents DST as a series of natural language questions directed at the model. Using the same example request, the model could be prompted with a question such as, “What is the hotel star rating the user wants?” which it could answer with "3" [2]. This method not only aids in capturing the user's intents but also aligns more closely with natural conversational flow, making it conducive to continual learning and adaptation as it utilizes general skills that are typically more understandable through natural language interactions [2].

Moreover, the text-to-text approach allows for the integration of sophisticated models that enhance semantic understanding and contextual awareness. For instance, while traditional methods depended heavily on manually crafted lexicons—which posed significant scalability issues—research has shifted toward open vocabulary DST that can dynamically adapt to variations in dialogue without relying on predefined lists [5]. Recent advancements have incorporated powerful language models such as Codex and ChatGPT, which utilize this flexibility to improve their dialogue management capabilities [5].

These large language models operate on the principle that both input and output are treated as text sequences, enabling the system to utilize a richer understanding of context and semantics [4]. This accommodates various domains and reduces the dependency on ontology, thus allowing for more seamless interaction across different conversational contexts [4][5]. Furthermore, the computational efficiency achieved through the text-to-text paradigm helps in identifying and highlighting significant tokens within dialogue sequences, enabling the system to dynamically adjust its focus based on the dialogue history and current turn[3] [4].

In summary, the text-to-text paradigm in DST redefines the process of understanding and managing dialogues by transforming slot-filling into a question-answering model that enhances interpretability, scalability, and the ability to navigate various conversational contexts, ultimately leading to more effective task-oriented dialogues [2][4][5].

1. [1]:  https://ar5iv.org/html/2311.15623, No Title
2. [2]:  https://ar5iv.org/html/2305.13721, [2305.13721] Continual Dialogue State Tracking via Example-Guided Question Answering
3. [3]:  https://ar5iv.org/html/2311.15623, No Title
4. [4]:  https://ar5iv.org/html/2410.22767, No Title
5. [5]:  https://ar5iv.org/html/2404.08559, No Title
---
1. [1]:  Passage ID 1: Engineering Management,the Chinese University of Hong Kong{xhfeng, wuxx, hmmeng}@se.cuhk.edu.hk                      Helen Meng1 IntroductionDialogue State Tracking (DST) serves as a pivotal component in task-oriented dialogue systems, typically representing dialogue states as slot-value pairs bounded by the application domain’s ontology. DST’s role is to predict the ongoing dialogue state, utilizing it to generate system responses to user queries.While neural network models, particularly those employing transformer-based architectures like BERT Devlin et al. (2018), have achieved substantial performance enhancements in DST (Balaraman et al., 2021), surpassing traditional heuristic-based approaches, they also present challenges. These models, due to their intricate architectures, offer limited visibility and control during training and inference processes. The cascading non-linear transformations from dialogue data to dialogue state are intricate and obscure. The
2. [2]:  Passage ID 2: presents an illustrated overview.2.1 DST as question answeringDialogue state tracking (DST) is defined as estimating the beliefs of a user’s goals at every turn in a dialogue.It was traditionally formulated as a slot-filling task Wu et al. (2020); Heck et al. (2020), and more recently as a structured text generation task Hosseini-Asl et al. (2020); Peng et al. (2021); Su et al. (2022), shown in (0) in Figure 2.If a user were to say “Find me a 3 star hotel.", the goal is to deduce hotel-star = 3.However, we can also indirectly achieve the same predictions by reformulating DST as a collection of per-slot questions to answer Gao et al. (2019); Lin et al. (2021a).Given the same user request, we can ask our model to answer “What is the hotel star rating the user wants?" and have it predict 3.We hypothesize that this question answering approach is more conducive to continual learning because it leverages a general skill that is understandable through natural language.We only need
3. [3]:  Passage ID 3: studies, for simplicity, often avoided using external contextual knowledge, instead, relying on transformations of latent representations in PLMs as sources of contextual knowledge. Although these latent representations are context-aware, they remain susceptible to interpretability issues. Some solutions, such as using a specialized neural component to extract contextualized word representations Peters et al. (2018) Liu et al. (2019), require additional fine-tuning.In contrast, our approach employs a computationally efficient algorithm to capture semantic patterns in the entire training corpus and generates features that guide attention to semantically important tokens in the input sequence. This process and the resulting features can be easily analyzed and understood.3 Model3.1 Task: Dialogue State TrackingDialogue State Tracking (DST) involves receiving the dialogue history, the current dialogue turn, and dialogue state history as inputs and yields the updated dialogue
4. [4]:  Passage ID 4: systems, or chatbots, have become increasingly popular due to their ability to automate user tasks and provide accurate answers. These systems process and understand natural language, track conversation context through a conversation manager, and generate appropriate responses [32].To understand natural language and predict dialogue states, DST uses deep neural Network. [3] proposed a dialogue state generator using a copying mechanism to address dependency on domain ontology and lack of cross-domain knowledge sharing. [15] introduced the Slot-utterance Matching Belief Tracker (SUMBT) with an Attention mechanism, and [33] used attention mechanisms to efficiently estimate conversational context.Advances in Generation-Based Chatbots and DST Generation-based chatbots operate across various domains, with research focused on reducing dependence on ontology. [34] introduced a Neural Belief Tracking (NBT) framework using CNNs, and [35] applied a BERT-style model for non-categorical
5. [5]:  Passage ID 5: ChatGPT and Codex.2.   Related WorkDialogue State TrackingDST plays a crucial role in natural language understanding within task-oriented dialogue systems.In the early years, DST methods (Lee et al., 2019; Zhang et al., 2020) heavily relied on manually crafted lexicons to capture dialogue states.However, this approach faced challenges in scaling up to longer and more intricate dialogues.This difficulty arose from the need for more high-quality annotated data in emerging domains and the reliance on labor-intensive, hand-crafted lexicons.To address these limitations, Wu et al. (2019) and Le et al. (2020) shifted their focus to open vocabulary DST research.This transition aimed to diminish the reliance on manually crafted lexicons, offering a more adaptable and scalable approach.With the widespread adoption of large language models, Hu et al. (2022) and Heck et al. (2023) have turned to powerful language models like Codex-Davinci-002 and ChatGPT to tackle the DST