# Through the Lens of Core Competency: Survey on Evaluation of Large Language Models

## Question

How is the planning ability of large language models evaluated using automated methods?

## URLs

1. https://ar5iv.org/html/2406.10479. [2406.10479] Unlocking Large Language Model’s Planning Capabilities with Maximum Diversity Fine-tuning
2. https://ar5iv.org/html/2406.13094. [2406.13094] Exploring and Benchmarking the Planning Capabilities of Large Language Models
3. https://ar5iv.org/html/2409.01806. [2409.01806] LASP: Surveying the State-of-the-Art in Large Language Model-Assisted AI Planning
4. https://ar5iv.org/html/2405.06650. [2405.06650] Large Language Models as Planning Domain Generators
5. https://ar5iv.org/html/2310.00194. [2310.00194] A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models
6. https://ar5iv.org/html/2311.18760. [2311.18760] TaskBench: Benchmarking Large Language Models for Task Automation
7. https://ar5iv.org/html/2408.13890. [2408.13890] Making Large Language Models Better Planners with Reasoning-Decision Alignment
8. https://ar5iv.org/html/2312.16127. [2312.16127] LLM-SAP: Large Language Model Situational Awareness Based Planning
9. https://ar5iv.org/html/2310.10632. [2310.10632] BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology

## Answer (I don't know?: ✗)

The evaluation of the planning ability of large language models (LLMs) through automated methods involves several key approaches aimed at enhancing and assessing their performance in automated planning tasks.

First, researchers have focused on using frameworks that enable the automated generation of planning domain models from textual descriptions. This approach reduces the manual labor traditionally required in AI planning by allowing LLMs to produce domain models directly from simple text inputs. The effectiveness of this method is measured by comparing the sets of plans derived from the LLM-generated domains across various planning instances, thereby assessing the accuracy and utility of the generated models [3].

Additionally, the research explores the implementation of rigorous and systematic evaluations of LLM performance in planning tasks. One significant aspect of this evaluation process includes investigating the use of in-context learning (ICL). This approach examines the impact of increasing context length on planning performance, with an expectation that providing more context will lead to improved decision-making capabilities by the model [4]. 

Furthermore, to enhance the planning capabilities of LLMs, researchers have experimented with fine-tuning approaches that optimize the models for generating more effective planning paths. Fine-tuning can greatly improve the model’s performance by training it on specific datasets related to planning tasks, thus allowing it to adapt and generalize better to varying problem scenarios [4].

To grasp the generalization abilities of LLMs in automated planning, studies show that while they can achieve favorable outcomes with appropriate model selection, data preparation, and fine-tuning, they still face limitations in generalizing plans across unknown domains or when dealing with variability in object naming and plan lengths. This indicates that the evaluation methods must also consider these aspects, examining not only the success of generated plans but also the model's adaptability to new and unseen challenges [5].

In summary, the evaluation of LLM planning capabilities through automated methods involves the generation of domain models from text descriptions, systematic performance assessments using frameworks like ICL and fine-tuning techniques, and an examination of generalization across diverse planning contexts. This integrated approach is critical for advancing the utility and reliability of LLMs in automated planning applications [1][3][4][5].

1. [1]:  https://ar5iv.org/html/2406.10479, [2406.10479] Unlocking Large Language Model’s Planning Capabilities with Maximum Diversity Fine-tuning
2. [2]:  https://ar5iv.org/html/2311.18760, [2311.18760] TaskBench: Benchmarking Large Language Models for Task Automation
3. [3]:  https://ar5iv.org/html/2405.06650, [2405.06650] Large Language Models as Planning Domain Generators
4. [4]:  https://ar5iv.org/html/2406.13094, [2406.13094] Exploring and Benchmarking the Planning Capabilities of Large Language Models
5. [5]:  https://ar5iv.org/html/2409.01806, [2409.01806] LASP: Surveying the State-of-the-Art in Large Language Model-Assisted AI Planning
---
1. [1]:  Passage ID 1: (LLMs) have demonstrated impressive capabilities in a variety of applications beyond traditional natural language processing (NLP) tasks. For instance, they can accomplish complex tasks involving planning in environments like Minecraft [21, 25] and household settings [10, 23], and solve math problems using reasoning [22, 24]. Despite these impressive capabilities, concerns have been raised about their proficiency in planning tasks. Numerous studies [19, 14, 11, 7] have shown that LLMs, on their own, often struggle to generate valid plans. As a result, enhancing the planning abilities of LLMs has emerged as a prominent research area.In this paper, we focus on enhancing the planning capabilities of Large Language Models (LLMs) within rigorous planning settings, often referred to as "automated planning". Despite extensive criticism, there has been little concrete progress in fundamentally improving the planning capabilities of LLMs within the field of automated planning.In this paper,
2. [2]:  Passage ID 2: answering with external tools.CoRR, abs/2306.13304, 2023.Appendix A AppendixA.1 Related WorksLarge language models (ChatGPT (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), LLAMA (Touvron et al., 2023a; b), Bard (Anil et al., 2023)) have drawn the development of autonomous agents (e.g., AutoGPT (Gravitas, 2023), HuggingGPT (Shen et al., 2023), BabyAGI (Nakajima, 2023)). These applications can be considered as a form of task automation, which uses LLMs as the controller to analyze user instructions and search for the most suitable solution (e.g., external models) to obtain answers. Despite these advances in this area, it still lacks a systematic and standardized benchmark to measure the capability of LLMs in automation tasks. Traditional benchmarks like GLUE (Wang et al., 2019b) or SuperGLUE (Wang et al., 2019a) can only evaluate the capability of pre-trained models in a single task. To further explore the capability of LLMs, some benchmarks (AlpacaEval (Li et al., 2023b),
3. [3]:  Passage ID 3: "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Large Language Models as Planning Domain GeneratorsJames Oswald1,Kavitha Srinivas2,Harsha Kokel2,Junkyu Lee2,Michael Katz2,Shirin Sohrabi2AbstractDeveloping domain models is one of the few remaining places that require manual human labor in AI planning. Thus, in order to make planning more accessible, it is desirable to automate the process of domain model generation. To this end, we investigate if large language models (LLMs) can be used to generate planning domain models from simple textual descriptions. Specifically, we introduce a framework for automated evaluation of LLM-generated domains by comparing the sets of plans for domain instances. Finally, we perform an empirical analysis of 7 large language models, including coding and chat models across 9 different planning domains, and under three classes of natural language domain descriptions. Our results indicate that
4. [4]:  Passage ID 4: rigorous and systematic evaluation of LLM performance.Second, we investigate the use of in-context learning (ICL) to enhance LLM planning, exploring the direct relationship between increased context length and improved planning performance.Third, we demonstrate the positive impact of fine-tuning LLMs on optimal planning paths, as well as the effectiveness of incorporating model-driven search procedures.Finally, we investigate the performance of the proposed methods in out-of-distribution scenarios, assessing the ability to generalize to novel and unseen planning challenges.Exploring and Benchmarking the Planning Capabilities of Large Language ModelsBernd Bohnet††thanks: Equal contribution                      Azade Nova11footnotemark: 1                      Aaron T Parisi                      Kevin Swersky                      Katayoon GoshvadiHanjun Dai                      Dale Schuurmans                      Noah
5. [5]:  Passage ID 5: (4) Are LLMs capable of generalizing plans? They show favorable results of LLMs for automated planning with appropriate selection of the LLM, data preparation, and fine-tuning. However, in terms of generalization, LLMs exhibit limited capabilities, whether for planning in unknown domains, handling randomly named objects, or generalizing across different plan lengths.5.2 Robustness of LLM-Assisted PlanningLLM planners based on GPT-4 and other large-scale enterprise models can be unreliable due to potential behavior changes (snapshot updates) & expensive pricing. Recent research suggests that groups of smaller language models, such as those with 6 billion and 13 billion parameters, can perform similarly to much larger counterparts like the GPT-3.5, which has roughly 175 billion parameters. According to Lu et al. (2024), using responses randomly selected from these smaller models—each conditioned on previous outputs from its counterparts—allows these systems to work together