# Through the Lens of Core Competency: Survey on Evaluation of Large Language Models

## Question

How is the planning ability of large language models evaluated using automated methods?

## URLs

1. https://ar5iv.org/html/2406.10479
2. https://ar5iv.org/html/2406.13094
3. https://ar5iv.org/html/2409.01806
4. https://ar5iv.org/html/2405.06650
5. https://ar5iv.org/html/2310.00194
6. https://ar5iv.org/html/2311.18760
7. https://ar5iv.org/html/2408.13890
8. https://ar5iv.org/html/2312.16127
9. https://ar5iv.org/html/2310.10632

## Answer

## I don't know?: False

The evaluation of planning abilities of large language models (LLMs) using automated methods involves generating planning domain models from textual descriptions and comparing the sets of plans generated for various instances of those domains. Specifically, researchers are investigating how LLMs can autonomously generate planning domain models, which traditionally require significant manual input from humans [3].

One of the approaches includes implementing a framework for automated evaluation of LLM-generated domains, whereby the plans produced by different LLMs are systematically compared. This comparison is essential because it provides insights into how well these models can create valid and coherent plans based on given descriptions. The evaluation process takes into account not just the accuracy of the plans generated but also how well the LLMs understand the domains they are supposed to model [3].

Moreover, the study involves empirical analysis across several LLMs (including coding and chat models), addressing various planning domains and utilizing multiple classes of natural language domain descriptions [3]. This broad analysis is crucial for assessing the overall capability and robustness of LLMs in automated planning tasks.

Despite ongoing developments, the field still faces challenges. There lacks a systematic and standardized benchmark to measure the capabilities of LLMs specifically in automation tasks, which raises questions about the reliability of the evaluation methods currently in use. Traditional benchmarks, like GLUE and SuperGLUE, have proven inadequate for comprehensive assessments of LLMs in the context of autonomous planning, as they typically evaluate performance on single tasks rather than the complexity involved in generating plans across various domains [4].

To enhance the automation of planning tasks using LLMs, ongoing research efforts are focused on improving their ability to generate more valid and nuanced plans [1]. As researchers push towards more rigorous evaluation methodologies, the identification of effective benchmarks tailored for the specific challenges of planning and task automation becomes increasingly critical for advancing the state of the art in this area [4]. 

In conclusion, the evaluation of LLM planning abilities via automated methods involves generating domain models from text, comparing plans, and establishing robust benchmarks, all while grappling with the need for improved systematic evaluation in the rapidly evolving field of LLMs and automated planning [3][4].

[1]: https://ar5iv.org/html/2406.10479, [2406.10479] Unlocking Large Language Model’s Planning Capabilities with Maximum Diversity Fine-tuning
[2]: https://ar5iv.org/html/2409.16202, No Title
[3]: https://ar5iv.org/html/2405.06650, [2405.06650] Large Language Models as Planning Domain Generators
[4]: https://ar5iv.org/html/2311.18760, [2311.18760] TaskBench: Benchmarking Large Language Models for Task Automation
[5]: https://ar5iv.org/html/2407.15186, No Title

[1]: Passage ID 1: (LLMs) have demonstrated impressive capabilities in a variety of applications beyond traditional natural language processing (NLP) tasks. For instance, they can accomplish complex tasks involving planning in environments like Minecraft [21, 25] and household settings [10, 23], and solve math problems using reasoning [22, 24]. Despite these impressive capabilities, concerns have been raised about their proficiency in planning tasks. Numerous studies [19, 14, 11, 7] have shown that LLMs, on their own, often struggle to generate valid plans. As a result, enhancing the planning abilities of LLMs has emerged as a prominent research area.In this paper, we focus on enhancing the planning capabilities of Large Language Models (LLMs) within rigorous planning settings, often referred to as "automated planning". Despite extensive criticism, there has been little concrete progress in fundamentally improving the planning capabilities of LLMs within the field of automated planning.In this paper,
[2]: Passage ID 2: human-like text, answer complex questions, and perform a wide array of natural language processing (NLP) tasks. Models such as GPT (Achiam et al., 2023), LLaMA (Touvron et al., 2023), Baichuan (Yang et al., 2023), Qwen (Bai et al., 2023), GLM (Du et al., 2022), and DeepSeek-V2 (DeepSeek-AI, 2024) have set new standards, demonstrating exceptional performance across various benchmarks. These models have been widely adopted in both academic and industrial settings, highlighting their versatility and potential. However, the evaluation of LLMs is a multifaceted challenge that extends beyond mere accuracy in answering questions. It involves assessing their ability to understand context, generate coherent and relevant responses, and apply specialized knowledge effectively.Evaluation BenchmarksThe evaluation of LLMs has traditionally relied on established NLP benchmark datasets, such as SQuAD (Rajpurkar et al., 2016), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019), which focus
[3]: Passage ID 3: "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Large Language Models as Planning Domain GeneratorsJames Oswald1,Kavitha Srinivas2,Harsha Kokel2,Junkyu Lee2,Michael Katz2,Shirin Sohrabi2AbstractDeveloping domain models is one of the few remaining places that require manual human labor in AI planning. Thus, in order to make planning more accessible, it is desirable to automate the process of domain model generation. To this end, we investigate if large language models (LLMs) can be used to generate planning domain models from simple textual descriptions. Specifically, we introduce a framework for automated evaluation of LLM-generated domains by comparing the sets of plans for domain instances. Finally, we perform an empirical analysis of 7 large language models, including coding and chat models across 9 different planning domains, and under three classes of natural language domain descriptions. Our results indicate that
[4]: Passage ID 4: answering with external tools.CoRR, abs/2306.13304, 2023.Appendix A AppendixA.1 Related WorksLarge language models (ChatGPT (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), LLAMA (Touvron et al., 2023a; b), Bard (Anil et al., 2023)) have drawn the development of autonomous agents (e.g., AutoGPT (Gravitas, 2023), HuggingGPT (Shen et al., 2023), BabyAGI (Nakajima, 2023)). These applications can be considered as a form of task automation, which uses LLMs as the controller to analyze user instructions and search for the most suitable solution (e.g., external models) to obtain answers. Despite these advances in this area, it still lacks a systematic and standardized benchmark to measure the capability of LLMs in automation tasks. Traditional benchmarks like GLUE (Wang et al., 2019b) or SuperGLUE (Wang et al., 2019a) can only evaluate the capability of pre-trained models in a single task. To further explore the capability of LLMs, some benchmarks (AlpacaEval (Li et al., 2023b),
[5]: Passage ID 5: concern, autonomous agents, complex schema, benchmarks and domain knowledge.Our intention is that this survey will serve as a valuable resource for newcomers to this field and offer valuable insights for researchers.2. Background2.1. Large Language ModelsLarge Language Models(LLMs)(Chang et al., 2024) mainly refer to transformer-based(Vaswani et al., 2017) neural language models which contain billions of parameters and are pre-trained on large-scale corpus. LLMs like GPT-4(Achiam et al., 2023) and LLaMA(Touvron et al., 2023a) have shown an impressive effect on a wide range of natural language tasks such as translation, summary, intent detection, coding, etc. LLMs’ success is due to scaling law(Kaplan et al., 2020) and the emergent capability(Wei et al., 2022a). Scaling law(Kaplan et al., 2020) points out that language models’ performance on one specific downstream task has a power law relationship with three factors: the size of parameters, dataset size, and training