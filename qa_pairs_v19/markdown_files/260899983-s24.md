# Through the Lens of Core Competency: Survey on Evaluation of Large Language Models

## Question

How is the planning ability of large language models evaluated using automated methods?

## URLs

1. https://ar5iv.org/html/2406.10479. [2406.10479] Unlocking Large Language Model’s Planning Capabilities with Maximum Diversity Fine-tuning
2. https://ar5iv.org/html/2406.13094. [2406.13094] Exploring and Benchmarking the Planning Capabilities of Large Language Models
3. https://ar5iv.org/html/2409.01806. [2409.01806] LASP: Surveying the State-of-the-Art in Large Language Model-Assisted AI Planning
4. https://ar5iv.org/html/2405.06650. [2405.06650] Large Language Models as Planning Domain Generators
5. https://ar5iv.org/html/2310.00194. [2310.00194] A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models
6. https://ar5iv.org/html/2311.18760. [2311.18760] TaskBench: Benchmarking Large Language Models for Task Automation
7. https://ar5iv.org/html/2408.13890. [2408.13890] Making Large Language Models Better Planners with Reasoning-Decision Alignment
8. https://ar5iv.org/html/2312.16127. [2312.16127] LLM-SAP: Large Language Model Situational Awareness Based Planning
9. https://ar5iv.org/html/2310.10632. [2310.10632] BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology

## Answer (I don't know?: ✗)

The evaluation of the planning ability of large language models (LLMs) through automated methods is currently an area of active research, with a focus on enhancing their performance in structured planning tasks. The context outlines several key approaches and considerations involved in evaluating LLM planning capabilities.

One critical aspect discussed is the need for rigorous, systematic evaluation benchmarks specific to automated planning. Traditional NLP benchmarks like GLUE and SuperGLUE can assess LLMs in single-task scenarios but do not adequately measure the multifaceted capabilities required for planning tasks [3] [4]. Therefore, there is a growing call for the development of specialized benchmarks that can evaluate LLMs' performance across a variety of automated planning scenarios, thereby enabling a more comprehensive assessment of their capabilities [3] [5].

In addition to establishing benchmarks, the role of in-context learning (ICL) is emphasized as a way to improve planning performance. Studies have suggested a direct correlation between increased context length during training and enhanced planning outcomes. By providing more extensive contextual information, LLMs can better understand the nuances of planning tasks, leading to more effective solution generation [4]. Moreover, fine-tuning LLMs on optimal planning paths has been shown to positively impact their performance, indicating that tailored training strategies can be beneficial [4].

Additionally, exploring model-driven search procedures is presented as another method to enhance planning capabilities. This involves using LLMs as controllers that analyze user instructions, which can help automate the search for suitable solutions by leveraging external models [3]. This automation can significantly streamline the problem-solving process within planning contexts.

Furthermore, there is an emphasis on evaluating LLM performance in out-of-distribution scenarios. This is critical as it allows researchers to assess how well these models can generalize to novel and unseen planning challenges. Such evaluations are vital for understanding the robustness and versatility of LLMs in real-world applications [4].

Overall, the evaluation process is underscored as multifaceted, integrating the development of targeted benchmarks, the application of in-context learning, fine-tuning approaches, and assessments in diverse scenarios. This comprehensive framework aims to push forward the understanding and enhancement of LLM planning abilities in automated methods [1] [4] [5]. 

In summary, the evaluation of LLMs' planning capabilities in automated contexts involves the establishment of specialized benchmarks, leveraging in-context learning and fine-tuning techniques, and testing the models in a variety of standard and unusual scenarios to ensure their effectiveness across different planning tasks.

1. [1]:  https://ar5iv.org/html/2406.10479, [2406.10479] Unlocking Large Language Model’s Planning Capabilities with Maximum Diversity Fine-tuning
2. [2]:  https://ar5iv.org/html/2409.16202, No Title
3. [3]:  https://ar5iv.org/html/2311.18760, [2311.18760] TaskBench: Benchmarking Large Language Models for Task Automation
4. [4]:  https://ar5iv.org/html/2406.13094, [2406.13094] Exploring and Benchmarking the Planning Capabilities of Large Language Models
5. [5]:  https://ar5iv.org/html/2407.15186, No Title
---
1. [1]:  Passage ID 1: (LLMs) have demonstrated impressive capabilities in a variety of applications beyond traditional natural language processing (NLP) tasks. For instance, they can accomplish complex tasks involving planning in environments like Minecraft [21, 25] and household settings [10, 23], and solve math problems using reasoning [22, 24]. Despite these impressive capabilities, concerns have been raised about their proficiency in planning tasks. Numerous studies [19, 14, 11, 7] have shown that LLMs, on their own, often struggle to generate valid plans. As a result, enhancing the planning abilities of LLMs has emerged as a prominent research area.In this paper, we focus on enhancing the planning capabilities of Large Language Models (LLMs) within rigorous planning settings, often referred to as "automated planning". Despite extensive criticism, there has been little concrete progress in fundamentally improving the planning capabilities of LLMs within the field of automated planning.In this paper,
2. [2]:  Passage ID 2: human-like text, answer complex questions, and perform a wide array of natural language processing (NLP) tasks. Models such as GPT (Achiam et al., 2023), LLaMA (Touvron et al., 2023), Baichuan (Yang et al., 2023), Qwen (Bai et al., 2023), GLM (Du et al., 2022), and DeepSeek-V2 (DeepSeek-AI, 2024) have set new standards, demonstrating exceptional performance across various benchmarks. These models have been widely adopted in both academic and industrial settings, highlighting their versatility and potential. However, the evaluation of LLMs is a multifaceted challenge that extends beyond mere accuracy in answering questions. It involves assessing their ability to understand context, generate coherent and relevant responses, and apply specialized knowledge effectively.Evaluation BenchmarksThe evaluation of LLMs has traditionally relied on established NLP benchmark datasets, such as SQuAD (Rajpurkar et al., 2016), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019), which focus
3. [3]:  Passage ID 3: answering with external tools.CoRR, abs/2306.13304, 2023.Appendix A AppendixA.1 Related WorksLarge language models (ChatGPT (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), LLAMA (Touvron et al., 2023a; b), Bard (Anil et al., 2023)) have drawn the development of autonomous agents (e.g., AutoGPT (Gravitas, 2023), HuggingGPT (Shen et al., 2023), BabyAGI (Nakajima, 2023)). These applications can be considered as a form of task automation, which uses LLMs as the controller to analyze user instructions and search for the most suitable solution (e.g., external models) to obtain answers. Despite these advances in this area, it still lacks a systematic and standardized benchmark to measure the capability of LLMs in automation tasks. Traditional benchmarks like GLUE (Wang et al., 2019b) or SuperGLUE (Wang et al., 2019a) can only evaluate the capability of pre-trained models in a single task. To further explore the capability of LLMs, some benchmarks (AlpacaEval (Li et al., 2023b),
4. [4]:  Passage ID 4: rigorous and systematic evaluation of LLM performance.Second, we investigate the use of in-context learning (ICL) to enhance LLM planning, exploring the direct relationship between increased context length and improved planning performance.Third, we demonstrate the positive impact of fine-tuning LLMs on optimal planning paths, as well as the effectiveness of incorporating model-driven search procedures.Finally, we investigate the performance of the proposed methods in out-of-distribution scenarios, assessing the ability to generalize to novel and unseen planning challenges.Exploring and Benchmarking the Planning Capabilities of Large Language ModelsBernd Bohnet††thanks: Equal contribution                      Azade Nova11footnotemark: 1                      Aaron T Parisi                      Kevin Swersky                      Katayoon GoshvadiHanjun Dai                      Dale Schuurmans                      Noah
5. [5]:  Passage ID 5: concern, autonomous agents, complex schema, benchmarks and domain knowledge.Our intention is that this survey will serve as a valuable resource for newcomers to this field and offer valuable insights for researchers.2. Background2.1. Large Language ModelsLarge Language Models(LLMs)(Chang et al., 2024) mainly refer to transformer-based(Vaswani et al., 2017) neural language models which contain billions of parameters and are pre-trained on large-scale corpus. LLMs like GPT-4(Achiam et al., 2023) and LLaMA(Touvron et al., 2023a) have shown an impressive effect on a wide range of natural language tasks such as translation, summary, intent detection, coding, etc. LLMs’ success is due to scaling law(Kaplan et al., 2020) and the emergent capability(Wei et al., 2022a). Scaling law(Kaplan et al., 2020) points out that language models’ performance on one specific downstream task has a power law relationship with three factors: the size of parameters, dataset size, and training