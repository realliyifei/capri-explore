# Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

How do statistical machine translation systems determine the most probable translations during the decoding process?

## URLs

1. https://ar5iv.org/html/2205.00978. [2205.00978] Quality-Aware Decoding for Neural Machine Translation
2. https://ar5iv.org/html/2104.05336. [2104.05336] Machine Translation Decoding beyond Beam Search
3. https://ar5iv.org/html/2310.06707. [2310.06707] Quality Control at Your Fingertips: Quality- Aware Translation Models
4. https://ar5iv.org/html/2005.10283. [2005.10283] Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation
5. https://ar5iv.org/html/2210.05193. [2210.05193] Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation
6. https://ar5iv.org/html/2405.18922. [2405.18922] Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective
7. https://ar5iv.org/html/1808.10006. [1808.10006] Correcting Length Bias in Neural Machine Translation
8. https://ar5iv.org/html/2405.18348. [2405.18348] Can Automatic Metrics Assess High-Quality Translations?
9. https://ar5iv.org/html/2409.17943. [2409.17943] On Translating Technical Terminology: A Translation Workflow for Machine-Translated Acronyms

## Answer (I don't know?: ✗)

Statistical machine translation (SMT) systems determine the most probable translations during the decoding process primarily through the use of maximum a posteriori (MAP) decoding. This approach involves selecting the hypothesis that maximizes the conditional probability of a given translation based on the input sentence. However, due to the computational complexities associated with the vast number of possible translations, exact maximization is often impractical, necessitating the use of approximative decoding techniques such as beam search or greedy decoding to find the best hypothesis efficiently [1][3].

Recent studies have highlighted significant issues with MAP decoding, specifically its tendency to yield suboptimal results. For instance, Eikema and Aziz (2020) demonstrated that the MAP decoding method may not align well with the actual distributions of translations, pointing out that the probability distributions often lead to the assignment of lower probabilities to human translations compared to those generated by the decoder, which suggests significant calibration issues within NMT models [1][2]. This misalignment indicates that while traditional methods aim to identify the highest-scoring translation, they may overlook critical aspects of the translation space that are better represented in the sampling of various hypotheses.

Moreover, researchers have indicated that the use of MAP decoding can result in common pathologies and biases in neural machine translation (NMT) outputs. This includes the phenomenon where the most likely translations accumulate so little probability mass that the interpretations can be deemed essentially arbitrary [2][5]. Consequently, the recommended alternative is the use of minimum Bayes risk (MBR) decoding. MBR decoding emphasizes a self-consistency approach by sampling from the model's distribution and prioritizing hypotheses that show greater similarity to all other potential outputs. This approach aims for a more holistic consideration of the translation distribution, rather than solely focusing on maximizing a single point estimate [1][2][5].

A critical enhancement noted in the literature involves integrating quality estimation signals during the decoding process. For example, using an internal quality estimate as a guiding prompt can substantially boost translation quality and efficiency, leading to improvements in inference speed by reducing the hypothesis space [3]. Additionally, more research into how NMT models handle specific elements such as acronyms has also indicated gaps in current methodologies, particularly emphasizing that traditional SMT processes can struggle significantly when dealing with specialized language features [4]. 

In conclusion, while SMT systems conventionally rely on MAP decoding to derive the most probable translations, the recognition of its limitations has prompted a shift towards using more holistic approaches like MBR decoding, supplemented by quality estimation to enhance performance and reliability in translations.

1. [1]:  https://ar5iv.org/html/2310.06707, [2310.06707] Quality Control at Your Fingertips: Quality- Aware Translation Models
2. [2]:  https://ar5iv.org/html/2005.10283, [2005.10283] Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation
3. [3]:  https://ar5iv.org/html/2310.06707, [2310.06707] Quality Control at Your Fingertips: Quality- Aware Translation Models
4. [4]:  https://ar5iv.org/html/2409.17943, [2409.17943] On Translating Technical Terminology: A Translation Workflow for Machine-Translated Acronyms
5. [5]:  https://ar5iv.org/html/2005.10283, [2005.10283] Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation
---
1. [1]:  Passage ID 1: intractable due to the huge number of possible sentences, typically beam search or greedy decoding are used to approximate the search for the best hypothesis.Machine translation (MT) is a prominent example of these types of models, where the system is trained to generate a sentence in a target language given a source sentence in another language.Nonetheless, Eikema and Aziz, (2020) have demonstrated that MAP decoding methods may be suboptimal due to the presence of misaligned probability distributions.Moreover,NMT models often assign human translations lower probabilities than their own beam search outputs due to calibration issues (Ott et al.,, 2018; Freitag et al.,, 2020).Eikema and Aziz, (2020, 2022) applied MBR decoding for NMT models as an alternative generation approach.MBR decoding follows a self-consistency approach by sampling from the model distribution and giving precedence to hypotheses that exhibit greater similarity to all other hypotheses.In contrast to MAP
2. [2]:  Passage ID 2: rule aimed at identifying the highest-scoring translation, i.e. the mode. We argue that the evidence corroborates the inadequacy of MAP decoding more than casts doubt on the model and its training algorithm. In this work, we show that translation distributions do reproduce various statistics of the data well, but that beam search strays from such statistics. We show that some of the known pathologies and biases of NMT are due to MAP decoding and not to NMT’s statistical assumptions nor MLE. In particular, we show that the most likely translations under the model accumulate so little probability mass that the mode can be considered essentially arbitrary. We therefore advocate for the use of decision rules that take into account the translation distribution holistically. We show that an approximation to minimum Bayes risk decoding gives competitive results confirming that NMT models do capture important aspects of translation well in expectation.00footnotetext:  This work is licensed
3. [3]:  Passage ID 3: for a separate quality estimation model.Moreover, we show that using this quality signal as a prompt during MAP decoding can significantly improve translation quality.When using the internal quality estimate to prune the hypothesis space during MBR decoding, we can not only further improve translation quality, but also reduce inference speed by two orders of magnitude.1 IntroductionMost state-of-the-art models for Natural Language Processing (NLP) tasks are probabilistic, with the most frequent parameterization being based on neural networks.Once these models are trained, the prevailing decoding strategy for Natural Language Generation (NLG) is MAP decoding, i.e. select the hypothesis that maximizes the conditional probability given an input.As an exact maximization is computationally intractable due to the huge number of possible sentences, typically beam search or greedy decoding are used to approximate the search for the best hypothesis.Machine translation (MT) is a
4. [4]:  Passage ID 4: language processing (NLP) do – predict the next word in a series of words. While high-resource languages like English and French are reported to achieve near human parity using common metrics for measurement such as BLEU and COMET, we find that an important step is being missed: the translation of technical terms, specifically acronyms. Some state-of-the art machine translation systems like Google Translate which are publicly available can be erroneous when dealing with acronyms – as much as 50% in our findings. This article addresses acronym disambiguation for MT systems by proposing an additional step to the SL–TL (FR–EN) translation workflow where we first offer a new acronym corpus for public consumption and then experiment with a search-based thresholding algorithm that achieves nearly 10% increase when compared to Google Translate and OpusMT.1 IntroductionWith the myriad of artificial intelligence tools available for professional translators, it can be hard for translators
5. [5]:  Passage ID 5: to minimum Bayes risk decoding gives competitive results confirming that NMT models do capture important aspects of translation well in expectation.00footnotetext:  This work is licensed under a Creative CommonsAttribution 4.0 International Licence.Licence details:http://creativecommons.org/licenses/by/4.0/.1 IntroductionRecent findings in neural machine translation (NMT) suggest that modern translation systems have some serious flaws. This is based on observations such as: i) translations produced via beam search typically under-estimate sequence length [Sountsov andSarawagi (2016, Koehn and Knowles (2017], the length bias; ii) translation quality generally deteriorates with better approximate search [Koehn and Knowles (2017, Murray andChiang (2018, Ott et al. (2018, Kumar andSarawagi (2019], the beam search curse;iii) the true most likely translation under the model (i.e., the mode of the distribution) is empty in many cases [Stahlberg and Byrne (2019] and a general