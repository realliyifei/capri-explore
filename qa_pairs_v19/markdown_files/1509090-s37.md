# Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

How do statistical machine translation systems determine the most probable translations during the decoding process?

## URLs

1. https://ar5iv.org/html/2205.00978
2. https://ar5iv.org/html/2104.05336
3. https://ar5iv.org/html/2310.06707
4. https://ar5iv.org/html/2005.10283
5. https://ar5iv.org/html/2210.05193
6. https://ar5iv.org/html/2405.18922
7. https://ar5iv.org/html/1808.10006
8. https://ar5iv.org/html/2405.18348
9. https://ar5iv.org/html/2409.17943

## Answer

## I don't know?: False

Statistical machine translation (SMT) systems primarily determine the most probable translations during the decoding process by utilizing approaches that prioritize the estimation of posterior probabilities of translation hypotheses. The common method employed is Maximum A Posteriori (MAP) decoding, where the system seeks to select the hypothesis with the highest estimated probability. This process typically involves the use of beam search, which is a heuristic search algorithm that balances computational efficiency with search quality by exploring a limited number of the most promising hypotheses at each step of decoding [1] [2].

However, it has been shown that the reliability of estimated probabilities as a proxy for translation quality may be overstated. Evidence indicates that there isn’t always a positive correlation between the estimated probabilities returned by the decoding process and the actual quality of the translations produced. For instance, research has demonstrated instances where a human reference translation could have a lower estimated probability than a generated hypothesis, even in cases where the generated translation is of poor quality [2]. This phenomenon highlights the inadequacies of MAP decoding, suggesting that the approach may result in suboptimal translation outputs.

The statistical foundation of SMT relies heavily on the principle that translation distributions should replicate the underlying statistics of the training data closely. Yet, studies have shown that the current decoding methods, such as beam search, can deviate significantly from these statistical characteristics. This can lead to biases and pathologies in translations, confirming that the issue tends to stem from the MAP decoding itself rather than the statistical assumptions made by the NMT models or traditional maximum likelihood estimation (MLE) processes [3]. These findings advocate for a more holistic decision-making approach that considers the entire translation distribution.

An alternative proposed to address these limitations is Minimum Bayes Risk (MBR) decoding, which operates on different principles. Instead of solely relying on individual estimates, MBR decoding samples from the model distribution and favors those hypotheses that exhibit higher similarity to the entire range of output possibilities. This self-consistency approach helps in mitigating the limitations associated with MAP decoding and can yield competitive translation results by holistically considering translation outputs [1]. By focusing on the overall distribution rather than the single most likely hypothesis, MBR potentially captures richer contextual information necessary for producing quality translations.

In summary, while statistical machine translation systems often rely on MAP decoding supported by beam search to estimate the most probable translations, there are significant challenges in correlating these statistical estimates with translation quality. Alternative strategies, like MBR decoding, offer promising paths forward for improving the efficacy of translation outputs [3].

1. [1]:  https://ar5iv.org/html/2310.06707, [2310.06707] Quality Control at Your Fingertips: Quality- Aware Translation Models
2. [2]:  https://ar5iv.org/html/2406.11632, [2406.11632] Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding for Neural Machine Translation
3. [3]:  https://ar5iv.org/html/2005.10283, [2005.10283] Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation
4. [4]:  https://ar5iv.org/html/2408.06124, No Title
5. [5]:  https://ar5iv.org/html/1807.10854, No Title
---
1. [1]:  Passage ID 1: intractable due to the huge number of possible sentences, typically beam search or greedy decoding are used to approximate the search for the best hypothesis.Machine translation (MT) is a prominent example of these types of models, where the system is trained to generate a sentence in a target language given a source sentence in another language.Nonetheless, Eikema and Aziz, (2020) have demonstrated that MAP decoding methods may be suboptimal due to the presence of misaligned probability distributions.Moreover,NMT models often assign human translations lower probabilities than their own beam search outputs due to calibration issues (Ott et al.,, 2018; Freitag et al.,, 2020).Eikema and Aziz, (2020, 2022) applied MBR decoding for NMT models as an alternative generation approach.MBR decoding follows a self-consistency approach by sampling from the model distribution and giving precedence to hypotheses that exhibit greater similarity to all other hypotheses.In contrast to MAP
2. [2]:  Passage ID 2: the Power of Source: Source-based Minimum Bayes Risk Decoding for Neural Machine Translation1 IntroductionNeural Machine Translation (NMT) models typically aim to select a hypothesis with the highest estimated posterior probability during decoding, an approach known as Maximum A Posteriori (MAP) decoding. Beam search (Graves, 2012; Sutskever et al., 2014), which balances computational cost and search accuracy, has become the standard approximate decoding method for MAP.However, the underlying assumption of beam search - that estimated probability is a good proxy for translation quality - has been challenged by evidence showing that estimated probability and quality do not always correlate positively (Ott et al., 2018; Freitag et al., 2021). For example, Fig 1 illustrates a case where a human reference translation has a lower estimated probability than the hypothesis generated by beam search, and even lower than that of a poor translation. Furthermore, the true MAP output
3. [3]:  Passage ID 3: rule aimed at identifying the highest-scoring translation, i.e. the mode. We argue that the evidence corroborates the inadequacy of MAP decoding more than casts doubt on the model and its training algorithm. In this work, we show that translation distributions do reproduce various statistics of the data well, but that beam search strays from such statistics. We show that some of the known pathologies and biases of NMT are due to MAP decoding and not to NMT’s statistical assumptions nor MLE. In particular, we show that the most likely translations under the model accumulate so little probability mass that the mode can be considered essentially arbitrary. We therefore advocate for the use of decision rules that take into account the translation distribution holistically. We show that an approximation to minimum Bayes risk decoding gives competitive results confirming that NMT models do capture important aspects of translation well in expectation.00footnotetext:  This work is licensed
4. [4]:  Passage ID 4: in Section V before making conclusions in Section VI.II Literature ReviewThe current popular approach in machine translation involves utilizing modern neural networks, which are trained on extensive datasets containing millions to billions of parameters. This approach has proven to achieve substantial quality improvements. At the same time, traditional methods are now less commonly used due to their limitations in dealing with new domains and expensive cost and language pairs with significantly different word orders [2].Many works on neural machine translation rely on an encoder-decoder architecture [3]. Cho et al. [4] introduced the RNN Encoder-Decoder with two RNN networks to improve phrase representation using conditional probabilities. This model captures semantically and syntactically meaningful representations of linguistic phrases. Sutskever et al. [5] created a sequence-to-sequence network using multilayered LSTMs to encode input sequences into fixed-dimensional
5. [5]:  Passage ID 5: the top performers in classifying types of questions using the TREC database [146].Between their requirement for such understanding and their ease of examination due to the typical encoder–decoder structure they use,neural machine translation (NMT) systems (Section IV-G) are splendid testbeds for researching internal semantic representations. Poliak et al. [147] trained encoders on four different language pairs: English and Arabic, English and Spanish, English and Chinese, and English and German. The decoding classifiers were trained on four distinct datasets: Multi-NLI [148], which is an expanded version of SNLI [149], as well as three recast datasets from the JHU Decompositional Semantics Initiative [150] (FrameNet Plus or FN+ [151], Definite Pronoun Resolution or DPR [152], and Semantic Proto-Roles or SPR [153]). None of the results were particularly strong, although they were strongest in SPR. This led to the conclusion that NMT models do a poor job of capturing paraphrased