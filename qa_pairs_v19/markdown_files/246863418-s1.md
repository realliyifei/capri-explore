# A Survey on Dynamic Neural Networks for Natural Language Processing

## Question

How do skipping and early stopping techniques enhance the efficiency of dynamic neural networks in natural language processing?

## URLs

1. https://ar5iv.org/html/2202.07101. [2202.07101] A Survey on Dynamic Neural Networks for Natural Language Processing
2. https://ar5iv.org/html/2311.15436. [2311.15436] Learning to Skip for Language Modeling
3. https://ar5iv.org/html/2102.04906. [2102.04906] Dynamic Neural Networks: A Survey
4. https://ar5iv.org/html/2403.17561. [2403.17561] A Survey on Deep Learning and State-of-the-art Applications
5. https://ar5iv.org/html/2412.00238. [2412.00238] Twisted Convolutional Networks (TCNs): Enhancing Feature Interactions for Non-Spatial Data Classification
6. https://ar5iv.org/html/2106.05022. [2106.05022] Adaptive Inference through Early-Exit Networks: Design, Challenges and Directions
7. https://ar5iv.org/html/2312.09494. [2312.09494] No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models
8. https://ar5iv.org/html/2410.20022. [2410.20022] Dynamic layer selection in decoder-only transformers
9. https://ar5iv.org/html/2001.01647. [2001.01647] Are skip connections necessary for biologically plausible learning rules?

## Answer (I don't know?: ‚úó)

Skipping and early stopping techniques enhance the efficiency of dynamic neural networks in natural language processing (NLP) by optimizing the computational resources based on the relevance of the input data. These techniques allow models to bypass unimportant information and focus on critical parts of the input, leading to faster inference speeds and improved performance in tasks requiring the understanding of long sequences.

1. **Skipping Mechanisms**: The skipping technique is designed to improve efficiency by allowing the neural network to ignore irrelevant information within a sequence. Specifically, models can skip processing certain tokens that provide little value for the task at hand. For instance, LSTM-Jump is an example of a skipping mechanism that computes a ‚Äújumping softmax‚Äù at each step to determine how far to advance in the sequence and whether to stop reading altogether [3]. This means that unnecessary computations on less important segments of the input can be avoided, enabling the model to allocate its resources more effectively.

2. **Early Stopping**: Early stopping complements the skipping technique by enabling the model to halt processing before exhausting the full input. This means the model can make inferences using a subset of the input that has been identified as sufficient for making a decision. Both skipping and early stopping lead to a reduction in the computational workload, particularly for long sequences, which is often a significant concern in NLP tasks due to their complexity and resource demands [3] [4].

3. **Dynamic Adjustment**: Both techniques function within the broader framework of dynamic neural networks, which adapt their computational paths based on the input [1] [2]. This adaptability is crucial since not all inputs require the same amount of processing power. For example, simple tasks like categorizing a positive sentiment do not necessitate the full capacity of a deep Transformer model, which contains multiple layers [2]. By dynamically adjusting how much of the network is engaged for a given input, these techniques contribute to more efficient computations.

4. **Capacity and Resource Management**: The implementations of skimming, including skipping and early stopping, are grounded in the need to strike a balance between model capacity and resource management [1] [2]. They allow for the allocation of computation only where it is most impactful, which is vital in a landscape where models are growing in complexity and size, often reaching trillions of parameters. This dynamic approach helps in mitigating the issues of computational complexity and memory consumption that can hinder the scalability of NLP models [2].

In summary, skipping and early stopping techniques enhance the efficiency of dynamic neural networks in NLP by focusing computational effort on the most important inputs, dynamically adjusting to the needs of the task, and effectively managing the extensive resources required for processing complex language tasks. These strategies ultimately lead to faster inference and improved handling of long-term dependencies in language data [3] [4].

1. [1]:  https://ar5iv.org/html/2202.07101, [2202.07101] A Survey on Dynamic Neural Networks for Natural Language Processing
2. [2]:  https://ar5iv.org/html/2202.07101, [2202.07101] A Survey on Dynamic Neural Networks for Natural Language Processing
3. [3]:  https://ar5iv.org/html/2202.07101, [2202.07101] A Survey on Dynamic Neural Networks for Natural Language Processing
4. [4]:  https://ar5iv.org/html/2410.20022, [2410.20022] Dynamic layer selection in decoder-only transformers
5. [5]:  https://ar5iv.org/html/2410.20022, [2410.20022] Dynamic layer selection in decoder-only transformers
---
1. [1]:  Passage ID 1: For example, categorizing ‚ÄúI love you‚Äù as a positive sentencedoes not require a model containing dozens of Transformer layers. To resolve the aforementioned problems, dynamic neural networks havebeen a significant thrust of recent research in NLP. Dynamic networks can adjust their computational path based on the input for better efficiency, making it possible to train models with trillions of parameters and accelerate models in a low-resource setting.In this survey, we review the latest state of research on three types of dynamic neural networks that have been adopted in NLP: skimming, mixtures of experts (MoE), and early exit, as illustrated in Figure¬†1. These three types of techniques share a common idea of dynamically adjusting computation with respect to input, to save computation through bypassing unnecessary modules in a large neural network. However, they implement the goal via different approaches. Skimming was well-researched in the era of recurrent neural networks (RNN).
2. [2]:  Passage ID 2: with trillions of parameters and faster inference on mobile devices.In this survey, wesummarize the progress of three types of dynamic neural networks in NLP:skimming, mixture of experts, and early exit.We also highlight current challenges in dynamic neural networks and directions for future research.1 IntroductionScaling upmodel capacity is an obvious yet effective approach for better performance in natural language processing (NLP) tasks¬†(Brown et¬†al., 2020; Kaplan et¬†al., 2020; Ghorbani et¬†al., 2021; Zhou et¬†al., 2020b). However,theresultingincrease in computational complexity and memory consumption becomes a bottleneck forscaling, making these models hard to train and use. On the other hand, itis not necessaryto allocate the same amount of computation to all instances. For example, categorizing ‚ÄúI love you‚Äù as a positive sentencedoes not require a model containing dozens of Transformer layers. To resolve the aforementioned problems, dynamic neural networks
3. [3]:  Passage ID 3: read text and extract information from it¬†(Li et¬†al., 2019). By emphasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies. The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN, corresponding with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only.Skipping and Early StoppingSkipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early. LSTM-Jump¬†(Yu et¬†al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU). At each step, the current states are used to compute a ‚Äújumping softmax‚Äù, which decides how many steps to jump forward and whether to stop reading. LSTM-Jump employs
4. [4]:  Passage ID 4: for layer skipping. For all experiments, we only consider the computation performed on the autoregressively generated tokens ùê≤1:Tsubscriptùê≤:1ùëá\mathbf{y}_{1:T}, as is commonly done [5, 6, 11, 12]. The prompt is propagated through the full network since the prompt tokens can be efficiently processed once and in parallel and provide important context for NLG [12]. In general, we aim to achieve the best ROUGE metric [13] for the lowest computational cost cùëêc.3 Empirical Studies, Results, and Analysis3.1 Is layer skipping more effective than early exiting?We hypothesize that for NLG tasks and a pre-trained model without fine-tuning, layer skipping is more effective than early exit. Previous empirical results show that residual connections in decoder blocks contribute a greater portion of the total block‚Äôs output than the MLP and attention blocks¬†[7]. Consequently, skipping a layer‚Äôs execution should result in limited hidden state drift. We assess ‚Äúeffectiveness‚Äù by two methods:
5. [5]:  Passage ID 5: (b) filling missing KV cache entries with approximations [5, 6]. Despite some empirical studies [7, 8], the existing literature fails to unequivocally answer whether early exit is better than layer skipping at minimizing the error in the model‚Äôs hidden states.Second, the large output vocabulary space in NLG renders the evaluation of the classification head at intermediate layers prohibitively costly. Instead, lightweight controllers route a token using the hidden state¬†[5, 6, 9]. It is not clear that existing token-level layer skipping controllers derive a genuine benefit from processing the hidden state. Third, per-token adaptation adds overhead to each autoregressive pass, which accumulates over long generations. Per-token controllers must therefore be lightweight networks to mitigate this effect [5, 6, 9, 10]. We investigate in Section 3.3 whether it is possible to reduce computation, with a limited performance drop, by making sequence-level decisions that only need to be