# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How does annotator bias affect the interpretation of agreement coefficients like π and κ in computational linguistics?

## URLs

1. https://ar5iv.org/html/2408.15204
2. https://ar5iv.org/html/2411.15634
3. https://ar5iv.org/html/2406.17947
4. https://ar5iv.org/html/2111.07997
5. https://ar5iv.org/html/2104.00640
6. https://ar5iv.org/html/2404.16958
7. https://ar5iv.org/html/2411.04424
8. https://ar5iv.org/html/2409.09412
9. https://ar5iv.org/html/2210.14531

## Answer

Annotator bias can significantly affect the interpretation of agreement coefficients such as π (pi) and κ (kappa) in computational linguistics. In the context of natural language processing (NLP), annotator bias refers to the tendency of human evaluators to make decisions based on their experiences, perspectives, and interpretations, leading to inconsistencies in labeling tasks. This bias can introduce variability in the data, complicating the assessment of agreement among annotators.

Firstly, the presence of ambiguity in linguistic data can lead to annotator disagreement, which in turn affects the calculation of agreement coefficients. When annotators encounter ambiguous content, their interpretations may vary widely. This inherent ambiguity in language means that annotator disagreement should be accounted for when evaluating models using agreement measures like κ, which is designed to correct for chance agreement [2] [3]. If these discrepancies are not incorporated, the agreement coefficients may present a misleadingly high level of consensus, failing to reflect the true level of agreement among annotators.

Moreover, specific linguistic features can exacerbate annotator bias. For example, the presence of negation, vagueness, or unclear quantifiers can confuse annotators and affect their scoring of responses [2]. This confusion is likely to lower agreement scores, as annotators struggle to consistently interpret the same pieces of text. The study findings indicate that when annotators assess claims laden with non-explicitly communicated content or ambiguous claims, their disagreement levels tend to rise, thereby lowering agreement metrics such as κ [3]. This high level of annotator disagreement can suggest that the agreed-upon interpretations do not adequately characterize the complexities of the linguistic data involved.

Furthermore, the calculation of chance significance in agreement coefficients can also be skewed by annotator bias. If annotators have differing thresholds for what constitutes acceptable responses, this subjectivity may lead to inflated or deflated κ values that don't accurately mirror the performance of the examined NLP models. In this regard, coefficients like κ are sensitive to the distribution of responses and the biases inherent in the annotators' backgrounds and understanding of the task at hand [1] [5].

Lastly, addressing annotator bias is essential for improving annotator training, which can help enhance the reliability of agreement measures. The methodologies employed in annotating data can impose certain biases themselves; to minimize this, careful consideration of annotator selection and the anonymity of responses can play a crucial role [4]. For instance, ensuring that annotators do not have prior familiarity with the candidate models they are scoring can help reduce potential biases that could affect the outcomes of agreement scores.

In conclusion, annotator bias has critical implications for the interpretation of agreement coefficients like π and κ in NLP. A failure to account for the variability introduced by individual subjectivity can lead to misinterpretations of model performance and overall agreement in linguistic tasks. Thus, integrating bias mitigation strategies is necessary to ensure the validity of the assessments derived from these agreement metrics.

[1]: https://ar5iv.org/html/2407.11371, No Title
[2]: https://ar5iv.org/html/2104.00640, [2104.00640] AmbiFC: Fact-Checking Ambiguous Claims with Evidence
[3]: https://ar5iv.org/html/2104.00640, [2104.00640] AmbiFC: Fact-Checking Ambiguous Claims with Evidence
[4]: https://ar5iv.org/html/2408.09235, No Title
[5]: https://ar5iv.org/html/2404.02340, No Title

[1]: Passage ID 1: literature. We then explain our methodology, and evaluate it first through a simulation study, and then through application to real-world corpora. Finally, we conclude with discussions of limitations, ethical considerations, and future research.2 Theoretical Foundation and MotivationEstimation of chance agreement is a key element in the evaluation of classification tasks. However, though the field of NLP features a wide variety of span detection and labeling tasks, there is a lack of widely adopted chance-corrected metrics for them.In classification tasks, the Kappa coefficient is one of the most popular chance-corrected inter-annotator agreement measures (Komagata, 2002; Artstein and Poesio, 2008; Eugenio and Glass, 2004; Hripcsak and Rothschild, 2005; Powers, 2015).The Kappa coefficient is defined as (Ao−Ae)/(1−Ae)subscript𝐴𝑜subscript𝐴𝑒1subscript𝐴𝑒(A_{o}-A_{e})/(1-A_{e}), where Aosubscript𝐴𝑜A_{o} is the observed agreement without chance agreement correction, and
[2]: Passage ID 2: regarding polarity. This corroborates the results of previous work, showing that ambiguity is inherent to linguistic data and therefore annotator disagreement on labels should be incorporated in NLP models.Independent variablecoefficientP-valueImplicature-0.29890.000*Presupposition-0.38000.012*Coreference-0.29610.009*Vagueness-0.64690.000*Underspecification-0.61110.000*Probabilistic reasoning-0.55900.000*Evidence length-0.03560.966Claim length1.26420.600Negation in claim-0.22090.001*Quantifier in claim0.12550.119Table 5: Results of ANOVA showing linguistic inference effects on Krippendorff’s α𝛼\alpha in AmbiFC. The significant effects are marked with an asterisk.6 Experiments6.1 Evidence Selection (Ev.)The system is tasked with identifying the sentences in a given Wikipedia passage P=[s1,…,sn]𝑃subscript𝑠1…subscript𝑠𝑛P=[s_{1},...,s_{n}] that serve as evidence si∈E⊆Psubscript𝑠𝑖𝐸𝑃s_{i}\in E\subseteq P for a claim
[3]: Passage ID 3: input length might negatively impact annotation quality, while quantifiers could make the claims clearer to the annotators.ResultsThe variance analysis showed an R2superscript𝑅2R^{2} value of 0.367, indicating that a significant portion of the variation in annotator disagreement could be explained by annotators’ sensitivity to non-explicitly communicated content in claims or evidence as captured by the independent variables. Table 5 presents the significant effects observed in the correlation between the presence of inference types and the level of disagreement. The coefficients in the table reveal that ambiguous content is significantly linked to agreement scores, with the presence of negation in the claim also having a significant effect, likely due to confusion regarding polarity. This corroborates the results of previous work, showing that ambiguity is inherent to linguistic data and therefore annotator disagreement on labels should be incorporated in NLP
[4]: Passage ID 4: We recruit three graduate students from our academic network, all specialized in natural language processing, to serve as annotators. We provide the input given to the candidates, reference answers, and candidate responses. This format, while similar, is distinct from the judge models’ prompts which additionally require formatted decisions. The human annotators focus solely on the accuracy and relevance of the responses. To ensure impartial evaluations, we anonymize the origin of responses. Annotators do not know which candidate model generated such responses, reducing potential bias linked to model familiarity or reputation. We asked the annotators to score the candidate LLMs outputs on a binary scale: ‘1’ for ‘True’ and ‘0’ for ‘False’ based on alignment with the reference answer and contextual relevance.To ensure a rigorous evaluation, each of the three annotators independently assesses the entire set of outputs generated by each candidate model across all datasets. Specifically,
[5]: Passage ID 5: ConclusionAs research on subjective tasks in natural language processing has grown, the importance of modeling annotators and minority opinions has become more apparent. With the recent growth in work on annotator modeling and data perspectivism, it is becoming important to understand how the properties of our datasets and methods impact the effectiveness of annotator modeling methods. We examined seven corpora to better understand recent methods and introduced our method, the composite embeddings. We found that when annotator agreement on a dataset is low (K-α<0.4𝛼0.4\alpha<0.4), the user token embedding was most effective. When the annotator agreement was higher (K-α>0.5𝛼0.5\alpha>0.5), our new composite embedding gave the best performance. Surprisingly, the user token and composite embeddings, which are simple and efficient to implement, outperformed the multi-tasking model that was the highest performing model in prior work. Importantly, we also note that the number of