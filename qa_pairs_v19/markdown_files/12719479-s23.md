# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How does annotator bias affect the interpretation of agreement coefficients like π and κ in computational linguistics?

## URLs

1. https://ar5iv.org/html/2408.15204. [2408.15204] Can Unconfident LLM Annotations Be Used for Confident Conclusions?
2. https://ar5iv.org/html/2411.15634. [2411.15634] "All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations
3. https://ar5iv.org/html/2406.17947. [2406.17947] Untitled Document
4. https://ar5iv.org/html/2111.07997. [2111.07997] Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection
5. https://ar5iv.org/html/2104.00640. [2104.00640] AmbiFC: Fact-Checking Ambiguous Claims with Evidence
6. https://ar5iv.org/html/2404.16958. [2404.16958] A Closer Look at Classification Evaluation Metrics and a Critical Reflection of Common Evaluation Practice
7. https://ar5iv.org/html/2411.04424. [2411.04424] Bayesian Calibration of Win Rate Estimation with LLM Evaluators
8. https://ar5iv.org/html/2409.09412. [2409.09412] Label Convergence: Defining an Upper Performance Bound in Object Recognition through Contradictory Annotations
9. https://ar5iv.org/html/2210.14531. [2210.14531] Unifying Data Perspectivism and Personalization: An Application to Social Norms

## Answer (I don't know?: ✗)

Annotator bias significantly affects the interpretation of agreement coefficients such as π (pi) and κ (kappa) in computational linguistics by introducing variability and ambiguity into the annotations provided by human annotators. This leads to a misrepresentation of the true levels of agreement on labels assigned to linguistic data.

The presence of ambiguity in linguistic data is well-documented, indicating that annotator disagreement on labels—stemming from their sensitivity to non-explicitly communicated content—should be integrated into NLP models to improve their robustness and accuracy [1][2]. For instance, factors such as implicature, presupposition, and vagueness can lead to substantial variance in annotator views, which in turn impacts the calculated agreement coefficients. Studies have shown that the inherent ambiguity often linked to non-explicit content in claims or evidence is a critical contributor to this annotator disagreement, as clearly indicated by the significant effects captured in variance analysis [2].

Furthermore, the characteristics of the claims made by annotators, including the use of negation or quantifiers, can further complicate agreement assessments. For example, negation in claims correlates with confusion regarding polarity, making it a significant factor in the disagreement levels noted among annotators [1]. As disagreement increases due to such linguistic intricacies, the values of agreements like π and κ may not accurately reflect the underlying coherence or validity of the data, thus potentially misleading researchers regarding the reliability of the annotations.

In contexts where crowdsourcing is employed to gather human annotations, the approach can introduce additional biases, as crowd workers may rely on large language models (LLMs) to aid in their task, which could skew their responses further [4]. This reliance can amplify biases related to the interpretation of linguistic constructs, resulting in annotations that reflect the LLM's characteristics rather than the annotators' authentic judgments.

Moreover, while agreement coefficients like π and κ aim to quantify the level of annotation concordance, they may fail to account for the complexity of human interpretation influenced by the intrinsic characteristics of the language and the context of the data being annotated. Therefore, when evaluating these coefficients, one must consider the significant effects of linguistic ambiguity, the influences of LLMs on human annotations, and other relevant factors that contribute to annotator bias [4][5].

In conclusion, annotator bias affects the interpretation of agreement coefficients in computational linguistics by introducing variability and ambiguity into annotations. This necessitates a careful approach to the evaluation of agreement metrics, ensuring that models can adequately account for the nuances of human language interpretation and the implications of linguistic variability on the assessment of agreement.

1. [1]:  https://ar5iv.org/html/2104.00640, [2104.00640] AmbiFC: Fact-Checking Ambiguous Claims with Evidence
2. [2]:  https://ar5iv.org/html/2104.00640, [2104.00640] AmbiFC: Fact-Checking Ambiguous Claims with Evidence
3. [3]:  https://ar5iv.org/html/2408.15204, [2408.15204] Can Unconfident LLM Annotations Be Used for Confident Conclusions?
4. [4]:  https://ar5iv.org/html/2408.15204, [2408.15204] Can Unconfident LLM Annotations Be Used for Confident Conclusions?
5. [5]:  https://ar5iv.org/html/2408.15204, [2408.15204] Can Unconfident LLM Annotations Be Used for Confident Conclusions?
---
1. [1]:  Passage ID 1: regarding polarity. This corroborates the results of previous work, showing that ambiguity is inherent to linguistic data and therefore annotator disagreement on labels should be incorporated in NLP models.Independent variablecoefficientP-valueImplicature-0.29890.000*Presupposition-0.38000.012*Coreference-0.29610.009*Vagueness-0.64690.000*Underspecification-0.61110.000*Probabilistic reasoning-0.55900.000*Evidence length-0.03560.966Claim length1.26420.600Negation in claim-0.22090.001*Quantifier in claim0.12550.119Table 5: Results of ANOVA showing linguistic inference effects on Krippendorff’s α𝛼\alpha in AmbiFC. The significant effects are marked with an asterisk.6 Experiments6.1 Evidence Selection (Ev.)The system is tasked with identifying the sentences in a given Wikipedia passage P=[s1,…,sn]𝑃subscript𝑠1…subscript𝑠𝑛P=[s_{1},...,s_{n}] that serve as evidence si∈E⊆Psubscript𝑠𝑖𝐸𝑃s_{i}\in E\subseteq P for a claim
2. [2]:  Passage ID 2: input length might negatively impact annotation quality, while quantifiers could make the claims clearer to the annotators.ResultsThe variance analysis showed an R2superscript𝑅2R^{2} value of 0.367, indicating that a significant portion of the variation in annotator disagreement could be explained by annotators’ sensitivity to non-explicitly communicated content in claims or evidence as captured by the independent variables. Table 5 presents the significant effects observed in the correlation between the presence of inference types and the level of disagreement. The coefficients in the table reveal that ambiguous content is significantly linked to agreement scores, with the presence of negation in the claim also having a significant effect, likely due to confusion regarding polarity. This corroborates the results of previous work, showing that ambiguity is inherent to linguistic data and therefore annotator disagreement on labels should be incorporated in NLP
3. [3]:  Passage ID 3: b) that uses active data collection for improved efficiency. Furthermore, we make use of power tuning Angelopoulos et al. (2023b), a technique that ensures that incorporating LLM annotations into the estimation can never be worse than ignoring them completely.3 Methods3.1 Problem SetupWe have a text corpus consisting of n𝑛n independent and identically distributed (i.i.d.) instances T1,…,Tnsubscript𝑇1…subscript𝑇𝑛T_{1},\dots,T_{n}. We wish to estimate a quantity of interest θ∗superscript𝜃\theta^{*}, such as the prevalence of political bias in the corpus or the causal effect of using certain linguistic markers on the perceived sentiment. To perform the estimation, we require human annotations H1,…,Hnsubscript𝐻1…subscript𝐻𝑛H_{1},\dots,H_{n} corresponding to T1,…,Tnsubscript𝑇1…subscript𝑇𝑛T_{1},\dots,T_{n}. For example, Hisubscript𝐻𝑖H_{i} might indicate whether Tisubscript𝑇𝑖T_{i} contains political bias, or assess the perceived politeness of Tisubscript𝑇𝑖T_{i}. In addition to
4. [4]:  Passage ID 4: explore ways to account for variability and bias in human annotations.Human annotations are often obtained through crowdsourcing, which may itself be influenced by LLMs, as crowd workers might use LLMs to increase productivity Veselovsky et al. (2023). Although we use datasets collected before the widespread availability of LLMs, detecting AI-generated text remains a challenge Verma et al. (2024); Weber-Wulff et al. (2023).This work only conducted experiments on estimation tasks within CSS datasets and only in English. However, Confidence-Driven Inference is generalizable to other types of text-based datasets, and it would be valuable to see more diverse applications in future research.Lastly, the presented experiments do not address causal effects. For instance, in the context of politeness, to identify the causal effect of hedging on perceived politeness, it would be necessary to compare texts that are otherwise identical but differ only in their use of hedging.
5. [5]:  Passage ID 5: achieves a  negative gain. Confidence-driven and non-adaptive approaches achieve near 90% coverage, or higher. In contrast, LLM-only coverage is often poor. Gain in effective sample size is not estimated for the LLM-only approach as it does not leverage human annotations. Errors show a standard deviation over 100 trials.Politeness.Texts from online requests posted on Stack Exchange and Wikipedia (n=5,480𝑛5480n=5,480) can be seen as polite or impolite. Politeness annotations help understand how linguistic devices impact perceived politeness Danescu-Niculescu-Mizil et al. (2013).In this estimation task, θ∗superscript𝜃\theta^{*} corresponds to the logistic regression coefficient βhedgesubscript𝛽hedge\beta_{\mathrm{hedge}} measuring the impact of a linguistic feature such as hedging on the perceived politeness,