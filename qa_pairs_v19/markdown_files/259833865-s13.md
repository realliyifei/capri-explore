# A Survey of Evaluation Methods of Generated Medical Textual Reports

## Question

How is the evaluation of medical concept coverage and negation correctness performed in generated medical reports?

## URLs

1. https://ar5iv.org/html/2009.08666. [2009.08666] Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures
2. https://ar5iv.org/html/2406.11143. [2406.11143] Scorecards for Synthetic Medical Data Evaluation and Reporting
3. https://ar5iv.org/html/2409.00250. [2409.00250] Medical Report Generation Is A Multi-label Classification Problemâ€  Equal Contribution
4. https://ar5iv.org/html/2406.16845. [2406.16845] RaTEScore: A Metric for Radiology Report Generation
5. https://ar5iv.org/html/2412.04606. [2412.04606] Semantic Consistency-Based Uncertainty Quantification for Factuality in Radiology Report Generation
6. https://ar5iv.org/html/2406.15963. [2406.15963] Effectiveness of ChatGPT in explaining complex medical reports to patients
7. https://ar5iv.org/html/2405.00982. [2405.00982] On the Evaluation of Machine-Generated Reports
8. https://ar5iv.org/html/2110.07356. [2110.07356] Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization
9. https://ar5iv.org/html/2408.14397. [2408.14397] Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs
10. https://ar5iv.org/html/2405.12833. [2405.12833] A Survey of Deep Learning-based Radiology Report Generation Using Multimodal Data

## Answer (I don't know?: âœ—)

The evaluation of medical concept coverage and negation correctness in generated medical reports involves several steps and metrics, as outlined in the provided context.

1. **Medical Concept Coverage**:
   The evaluation process begins with assessing the coverage of medical concepts in the output summary generated by the model, compared to a ground truth reference summary. Specifically, a set of metrics is proposed to measure how well the medical terms are encapsulated in the model's output. This involves defining two sets: 
   - ğ’, which represents the set of medical concepts in the reference summary, and 
   - ğ’Ì‚, which signifies the set of medical concepts present in the summary generated by the model. 
   The concept recall is calculated using the formula: 

   \[
   \text{Concept recall} = \frac{\sum_{n=1}^{N} |\hat{\mathcal{C}}^{(n)} \cap \mathcal{C}^{(n)}|}{\sum_{n=1}^{N} |\mathcal{C}^{(n)}|}
   \]
   
   This formula helps in determining how many of the relevant medical concepts from the reference summary are included in the modelâ€™s summary, thus providing a quantitative measure of coverage [3] [5].

2. **Negation Correctness**:
   The evaluation of negation correctness is another crucial aspect. This entails assessing whether the model accurately identifies negated medical concepts. The effectiveness of the model in determining the negated status of medical concepts is gauged using a tool called Negex (Harkema et al., 2009), which serves to detect negated concepts within the decoded summary. The precision and recall of these detected negations are computed to derive a Negation F1 score. The evaluation process checks if the negations in the summary match the intent expressed in the original conversation snippets, where incorrectly identified negations are recorded as false positives or negatives [1] [2].

3. **Doctor Evaluations**:
   Additionally, two doctors evaluate the summaries produced by the models based on their clinical relevance and factual correctness. They assess summaries based on how many relevant concepts from the dialogues are accurately represented, giving ratings such as "All" (100%), "Most" (at least 75%), "Some" (at least 1 but less than 75%), and "None" (0%). This human-centered evaluation is essential to validate the automated metrics and ensure that the summaries are useful from a medical perspective [4] [5].

In summary, the evaluation of medical concept coverage and negation correctness in generated medical reports is a systematic process that combines automated metrics and expert doctor assessments to ensure both the precision of medical information and the accuracy of negated concepts. This multi-faceted approach is crucial for enhancing the usability of summaries in a healthcare context.

1. [1]:  https://ar5iv.org/html/2009.08666, [2009.08666] Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures
2. [2]:  https://ar5iv.org/html/2110.07356, [2110.07356] Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization
3. [3]:  https://ar5iv.org/html/2009.08666, [2009.08666] Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures
4. [4]:  https://ar5iv.org/html/2110.07356, [2110.07356] Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization
5. [5]:  https://ar5iv.org/html/2110.07356, [2110.07356] Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization
---
1. [1]:  Passage ID 1: conversation would be false positives and vice versa for false negatives.Negation Correctness: To measure the effectiveness of the model to identify the negated status of medical concepts,we use Negex Harkema etÂ al. (2009) to determine negated concepts. Of the concepts present in the decoded summary, we evaluate precision and recall on whether the decoded negations were accurate for the decoded concepts and compute a Negation F1.4.2 Doctor EvaluationWe also had two doctors, who serve patients on our telehealth platform, evaluate the summaries produced by the models. Given the local dialogue snippets and the generated summary, we asked them to evaluate the extent to which the summary captured factually correct and medically relevant information from the snippet. Depending on what percentage of the concepts were correctly mentioned in the decoded summary of the provided snippet, the doctors graded the summaries with All (100%), Most (at least 75%), Some (at least 1 fact but
2. [2]:  Passage ID 2: a Concept F1666Note if there are no concepts detected in the snippet and summary by the entity extractor, then a conservative F1 score of 0 is given for that example. We use an in-house medical entity extractor to extract medical concepts in the summary. Medical concepts in the decoded summary that werenâ€™t present in the original conversation would be false positives and vice versa for false negatives.Negation Correctness: To measure the effectiveness of the model to identify the negated status of medical concepts,we use Negex Harkema etÂ al. (2009) to determine negated concepts. Of the concepts present in the decoded summary, we evaluate precision and recall on whether the decoded negations were accurate for the decoded concepts and compute a negation F166{}^{\ref{footnote f1}}.7.2 Doctor EvaluationWe also had doctors, who serve patients on our telehealth platform, evaluate the summaries produced by the models. Given the local dialogue snippets and the generated summary, we
3. [3]:  Passage ID 3: it is important for doctors to evaluate the quality of the output.4.1 Automated metricsWhile we measure model performance on standard metrics of ROUGE Lin (2004), we also wanted to specifically measure a modelâ€™s effectiveness in capturing the medical concepts that are of importance, and the negations. Therefore, we propose a new set of automated metrics that directly measure medically relevant information in the summary.Medical Concept Coverage: The concept coverage set of metrics captures the encapsulation of the medical terms in the modelâ€™s output summary to the ground truth reference. In particular, let ğ’ğ’\mathcal{C} be the set of medical concepts in the reference summary and ğ’^^ğ’\hat{\mathcal{C}} be the set of concepts in the summary output by the model. Then,Concept recall=âˆ‘n=1N|ğ’^(n)âˆ©ğ’(n)|âˆ‘n=1N|ğ’(n)|Concept recallsuperscriptsubscriptğ‘›1ğ‘superscript^ğ’ğ‘›superscriptğ’ğ‘›superscriptsubscriptğ‘›1ğ‘superscriptğ’ğ‘›\textrm{Concept
4. [4]:  Passage ID 4: Doctor EvaluationWe also had doctors, who serve patients on our telehealth platform, evaluate the summaries produced by the models. Given the local dialogue snippets and the generated summary, we asked them to evaluate the extent to which the summary captured factually correct and medically relevant information from the snippet. Depending on what percentage of the concepts were correctly mentioned in the decoded summary of the provided snippet, the doctors graded the summaries with All (100%), Most (at least 75%), Some (at least 1 fact but less than 75%), None (0%) labels.We also formulated a comparison task where given summaries generated by different models and the associated dialogue, they were asked which summary was the "best" from a usability perspective. Usability was defined as whether the summary could stand in as a replacement for reading the dialogue snippet i.e. whether it captures the correct concepts from the snippet and whether the negations are accurate. The
5. [5]:  Passage ID 5: always correlate well to human judgments as they may not fully capture coherent sentence structure and semantics Roller etÂ al. (2020); KryÅ›ciÅ„ski etÂ al. (2019). Since medical dialogue summarization would be used to assist health care, it is important for doctors to evaluate the quality of the output.7.1 Automated metricsWhile we measure model performance on standard metrics of ROUGE Lin (2004) 555We use the following package with default configuration: https://github.com/google-research/google-research/tree/master/rouge, we also measure a modelâ€™s effectiveness in capturing the medical concepts that are of importance, and their negations Joshi etÂ al. (2020)Medical Concept Coverage: The concept coverage set of metrics captures the coverage of medical terms in the modelâ€™s output summary with respect to the ground truth. In particular, let ğ’ğ’\mathcal{C} be the set of medical concepts in the reference summary and ğ’^^ğ’\hat{\mathcal{C}} be the set of concepts in the summary output by