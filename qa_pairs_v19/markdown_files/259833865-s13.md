# A Survey of Evaluation Methods of Generated Medical Textual Reports

## Question

How is the evaluation of medical concept coverage and negation correctness performed in generated medical reports?

## URLs

1. https://ar5iv.org/html/2009.08666. [2009.08666] Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures
2. https://ar5iv.org/html/2406.11143. [2406.11143] Scorecards for Synthetic Medical Data Evaluation and Reporting
3. https://ar5iv.org/html/2409.00250. [2409.00250] Medical Report Generation Is A Multi-label Classification Problem† Equal Contribution
4. https://ar5iv.org/html/2406.16845. [2406.16845] RaTEScore: A Metric for Radiology Report Generation
5. https://ar5iv.org/html/2412.04606. [2412.04606] Semantic Consistency-Based Uncertainty Quantification for Factuality in Radiology Report Generation
6. https://ar5iv.org/html/2406.15963. [2406.15963] Effectiveness of ChatGPT in explaining complex medical reports to patients
7. https://ar5iv.org/html/2405.00982. [2405.00982] On the Evaluation of Machine-Generated Reports
8. https://ar5iv.org/html/2110.07356. [2110.07356] Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization
9. https://ar5iv.org/html/2408.14397. [2408.14397] Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs
10. https://ar5iv.org/html/2405.12833. [2405.12833] A Survey of Deep Learning-based Radiology Report Generation Using Multimodal Data

## Answer (I don't know?: ✗)

The evaluation of medical concept coverage and negation correctness in generated medical reports is performed through a structured methodology that includes both automated metrics and subjective evaluations by medical professionals.

1. **Medical Concept Coverage**: The evaluation begins by assessing how well the model's output captures relevant medical concepts present in the reference summaries. This is quantified using a metric called "concept recall." The concept coverage metrics compute the intersection of the medical concepts found in the model's output summary with those in the reference summary. Specifically, the concept recall is calculated using the formula: 
   \[
   \text{Concept recall} = \frac{\sum_{n=1}^{N} |\hat{\mathcal{C}}^{(n)} \cap \mathcal{C}^{(n)}|}{\sum_{n=1}^{N} |\mathcal{C}^{(n)}|}
   \]
   where \(\mathcal{C}\) represents the set of medical concepts in the reference summary, and \(\hat{\mathcal{C}}\) is the set of concepts in the model's output [1] [5]. This ensures a quantitative measure of how much of the medically relevant information has been accurately represented in the generated report.

2. **Negation Correctness**: To evaluate how well the model identifies negated statuses of medical concepts, a system based on the Negex algorithm (Harkema et al., 2009) is employed. This algorithm helps determine which medical concepts should be considered as negated. The evaluation of negation correctness involves calculating both precision and recall concerning the identified negated concepts. An overall Negation F1 score is computed, allowing for an integrated measure of both precision and recall in identifying negations accurately within the medical context [1] [4].

3. **Doctor Evaluation**: Complementing the automated metrics, doctors who are actively serving patients on a telehealth platform also evaluate the generated summaries. They assess the factual accuracy and medical relevance of the content. Doctors compare the local dialogue snippets—with the generated summary—and grade according to the percentage of concepts correctly mentioned. Grading options range from “All” (100% accuracy) to “Some” (at least 1 fact) [1] [4]. This subjective evaluation provides a real-world check on the effectiveness of the summary in a clinical context.

In summary, the evaluation of medical concept coverage and negation correctness is a dual approach involving precise automated metrics to provide quantitative assessments, along with comprehensive evaluations by healthcare professionals to ensure clinical relevance and factual accuracy in medical reports [1] [5]. This combination fosters an effective method for assessing the capabilities of NLP models in producing medically accurate summaries.

1. [1]:  https://ar5iv.org/html/2009.08666, [2009.08666] Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures
2. [2]:  https://ar5iv.org/html/2302.02291, No Title
3. [3]:  https://ar5iv.org/html/2302.02291, No Title
4. [4]:  https://ar5iv.org/html/2110.07356, [2110.07356] Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization
5. [5]:  https://ar5iv.org/html/2009.08666, [2009.08666] Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures
---
1. [1]:  Passage ID 1: conversation would be false positives and vice versa for false negatives.Negation Correctness: To measure the effectiveness of the model to identify the negated status of medical concepts,we use Negex Harkema et al. (2009) to determine negated concepts. Of the concepts present in the decoded summary, we evaluate precision and recall on whether the decoded negations were accurate for the decoded concepts and compute a Negation F1.4.2 Doctor EvaluationWe also had two doctors, who serve patients on our telehealth platform, evaluate the summaries produced by the models. Given the local dialogue snippets and the generated summary, we asked them to evaluate the extent to which the summary captured factually correct and medically relevant information from the snippet. Depending on what percentage of the concepts were correctly mentioned in the decoded summary of the provided snippet, the doctors graded the summaries with All (100%), Most (at least 75%), Some (at least 1 fact but
2. [2]:  Passage ID 2: Roni Romano,and Oded Maimon. 2008.Negation recognition in medical narrative reports.Information Retrieval 11,6 (2008), 499–538.Tixier et al. (2016)Antoine J-P Tixier,Matthew R Hallowell, Balaji Rajagopalan,and Dean Bowman. 2016.Automated content analysis for construction safety:A natural language processing system to extract precursors and outcomes fromunstructured injury reports.Automation in Construction62 (2016), 45–56.Topal et al. (2021)M Onat Topal, Anil Bas,and Imke van Heerden. 2021.Exploring transformers in natural languagegeneration: Gpt, bert, and xlnet.arXiv preprint arXiv:2102.08036(2021).Udebuana et al. (2019)Okpala Izunna Udebuana,Ijioma Patricia Ngozi, andEmejulu Augustine Obiajulu.2019.Analysis of Evaluated Sentiments; aPseudo-Linguistic Approach and Online Acceptability Index for Decision-Makingwith Data: Nigerian Election in View.Computing 7,2 (2019), 39–44.Verma et al. (2011)Sudha Verma,
3. [3]:  Passage ID 3: Roni Romano,and Oded Maimon. 2008.Negation recognition in medical narrative reports.Information Retrieval 11,6 (2008), 499–538.Tixier et al. (2016)Antoine J-P Tixier,Matthew R Hallowell, Balaji Rajagopalan,and Dean Bowman. 2016.Automated content analysis for construction safety:A natural language processing system to extract precursors and outcomes fromunstructured injury reports.Automation in Construction62 (2016), 45–56.Topal et al. (2021)M Onat Topal, Anil Bas,and Imke van Heerden. 2021.Exploring transformers in natural languagegeneration: Gpt, bert, and xlnet.arXiv preprint arXiv:2102.08036(2021).Udebuana et al. (2019)Okpala Izunna Udebuana,Ijioma Patricia Ngozi, andEmejulu Augustine Obiajulu.2019.Analysis of Evaluated Sentiments; aPseudo-Linguistic Approach and Online Acceptability Index for Decision-Makingwith Data: Nigerian Election in View.Computing 7,2 (2019), 39–44.Verma et al. (2011)Sudha Verma,
4. [4]:  Passage ID 4: a Concept F1666Note if there are no concepts detected in the snippet and summary by the entity extractor, then a conservative F1 score of 0 is given for that example. We use an in-house medical entity extractor to extract medical concepts in the summary. Medical concepts in the decoded summary that weren’t present in the original conversation would be false positives and vice versa for false negatives.Negation Correctness: To measure the effectiveness of the model to identify the negated status of medical concepts,we use Negex Harkema et al. (2009) to determine negated concepts. Of the concepts present in the decoded summary, we evaluate precision and recall on whether the decoded negations were accurate for the decoded concepts and compute a negation F166{}^{\ref{footnote f1}}.7.2 Doctor EvaluationWe also had doctors, who serve patients on our telehealth platform, evaluate the summaries produced by the models. Given the local dialogue snippets and the generated summary, we
5. [5]:  Passage ID 5: it is important for doctors to evaluate the quality of the output.4.1 Automated metricsWhile we measure model performance on standard metrics of ROUGE Lin (2004), we also wanted to specifically measure a model’s effectiveness in capturing the medical concepts that are of importance, and the negations. Therefore, we propose a new set of automated metrics that directly measure medically relevant information in the summary.Medical Concept Coverage: The concept coverage set of metrics captures the encapsulation of the medical terms in the model’s output summary to the ground truth reference. In particular, let 𝒞𝒞\mathcal{C} be the set of medical concepts in the reference summary and 𝒞^^𝒞\hat{\mathcal{C}} be the set of concepts in the summary output by the model. Then,Concept recall=∑n=1N|𝒞^(n)∩𝒞(n)|∑n=1N|𝒞(n)|Concept recallsuperscriptsubscript𝑛1𝑁superscript^𝒞𝑛superscript𝒞𝑛superscriptsubscript𝑛1𝑁superscript𝒞𝑛\textrm{Concept