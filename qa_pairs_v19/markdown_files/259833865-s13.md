# A Survey of Evaluation Methods of Generated Medical Textual Reports

## Question

How is the evaluation of medical concept coverage and negation correctness performed in generated medical reports?

## URLs

1. https://ar5iv.org/html/2009.08666. [2009.08666] Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures
2. https://ar5iv.org/html/2406.11143. [2406.11143] Scorecards for Synthetic Medical Data Evaluation and Reporting
3. https://ar5iv.org/html/2409.00250. [2409.00250] Medical Report Generation Is A Multi-label Classification Problem† Equal Contribution
4. https://ar5iv.org/html/2406.16845. [2406.16845] RaTEScore: A Metric for Radiology Report Generation
5. https://ar5iv.org/html/2412.04606. [2412.04606] Semantic Consistency-Based Uncertainty Quantification for Factuality in Radiology Report Generation
6. https://ar5iv.org/html/2406.15963. [2406.15963] Effectiveness of ChatGPT in explaining complex medical reports to patients
7. https://ar5iv.org/html/2405.00982. [2405.00982] On the Evaluation of Machine-Generated Reports
8. https://ar5iv.org/html/2110.07356. [2110.07356] Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization
9. https://ar5iv.org/html/2408.14397. [2408.14397] Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs
10. https://ar5iv.org/html/2405.12833. [2405.12833] A Survey of Deep Learning-based Radiology Report Generation Using Multimodal Data

## Answer (I don't know?: ✗)

The evaluation of medical concept coverage and negation correctness in generated medical reports involves several systematic approaches focused on measuring how accurately the summaries capture medically relevant information.

First, to assess medical concept coverage, an in-house medical entity extractor is employed to detect medical concepts within the summaries produced by the models. The evaluation specifically involves comparing the set of medical concepts present in the model-generated summary against those in a ground truth reference. This is quantified through a set of automated metrics that measure concept recall, defined as the proportion of correctly identified medical concepts relative to the total concepts in the reference summary. Formally, if \( \mathcal{C} \) represents the set of concepts in the reference summary and \( \hat{\mathcal{C}} \) denotes the set of concepts extracted by the model, the concept recall is calculated as:

\[
\text{Concept Recall} = \frac{\sum_{n=1}^{N} |\hat{\mathcal{C}}^{(n)} \cap \mathcal{C}^{(n)}|}{\sum_{n=1}^{N} |\mathcal{C}^{(n)}|}
\]

This formula measures the intersection of detected concepts across several examples and divides by the total number of concepts in the references, allowing for a clear assessment of the model's effectiveness in encapsulating critical medical terms [1][5].

In addition to concept coverage, negation correctness is another crucial metric for evaluating generated summaries. To determine the effectiveness of the model in identifying the negated status of medical concepts, the Negex tool, as described by Harkema et al. (2009), is utilized to identify negated concepts within the summaries. The model's performance on negation is assessed using precision and recall based on whether the negated states of medical concepts are accurately captured in the summaries. Subsequently, this information is synthesized to produce a negation F1 score, which serves as a holistic measure of the model’s ability to handle negated concepts correctly [1][4].

Further validating the outputs, the generated summaries are also evaluated by practicing doctors on a telehealth platform. The physicians are tasked with reviewing the summaries and determining the extent to which they align with factual correctness and medical relevance. The grading system employed by the doctors classifies the summaries based on the percentage of accurately reported concepts, classifying them into categories such as "All" (100%), "Most" (at least 75%), and "Some" (at least 1 fact) [1][4].

In summary, the evaluation of medical concept coverage and negation correctness in generated medical reports is a multifaceted process involving automated metrics for recall and precision, as well as manual reviews by medical professionals to ensure that the summaries are both relevant and accurate. This multi-tiered approach helps enhance the reliability of natural language processing applications in healthcare contexts.

1. [1]:  https://ar5iv.org/html/2009.08666, [2009.08666] Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures
2. [2]:  https://ar5iv.org/html/2302.02291, No Title
3. [3]:  https://ar5iv.org/html/2302.02291, No Title
4. [4]:  https://ar5iv.org/html/2110.07356, [2110.07356] Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization
5. [5]:  https://ar5iv.org/html/2009.08666, [2009.08666] Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures
---
1. [1]:  Passage ID 1: conversation would be false positives and vice versa for false negatives.Negation Correctness: To measure the effectiveness of the model to identify the negated status of medical concepts,we use Negex Harkema et al. (2009) to determine negated concepts. Of the concepts present in the decoded summary, we evaluate precision and recall on whether the decoded negations were accurate for the decoded concepts and compute a Negation F1.4.2 Doctor EvaluationWe also had two doctors, who serve patients on our telehealth platform, evaluate the summaries produced by the models. Given the local dialogue snippets and the generated summary, we asked them to evaluate the extent to which the summary captured factually correct and medically relevant information from the snippet. Depending on what percentage of the concepts were correctly mentioned in the decoded summary of the provided snippet, the doctors graded the summaries with All (100%), Most (at least 75%), Some (at least 1 fact but
2. [2]:  Passage ID 2: Roni Romano,and Oded Maimon. 2008.Negation recognition in medical narrative reports.Information Retrieval 11,6 (2008), 499–538.Tixier et al. (2016)Antoine J-P Tixier,Matthew R Hallowell, Balaji Rajagopalan,and Dean Bowman. 2016.Automated content analysis for construction safety:A natural language processing system to extract precursors and outcomes fromunstructured injury reports.Automation in Construction62 (2016), 45–56.Topal et al. (2021)M Onat Topal, Anil Bas,and Imke van Heerden. 2021.Exploring transformers in natural languagegeneration: Gpt, bert, and xlnet.arXiv preprint arXiv:2102.08036(2021).Udebuana et al. (2019)Okpala Izunna Udebuana,Ijioma Patricia Ngozi, andEmejulu Augustine Obiajulu.2019.Analysis of Evaluated Sentiments; aPseudo-Linguistic Approach and Online Acceptability Index for Decision-Makingwith Data: Nigerian Election in View.Computing 7,2 (2019), 39–44.Verma et al. (2011)Sudha Verma,
3. [3]:  Passage ID 3: Roni Romano,and Oded Maimon. 2008.Negation recognition in medical narrative reports.Information Retrieval 11,6 (2008), 499–538.Tixier et al. (2016)Antoine J-P Tixier,Matthew R Hallowell, Balaji Rajagopalan,and Dean Bowman. 2016.Automated content analysis for construction safety:A natural language processing system to extract precursors and outcomes fromunstructured injury reports.Automation in Construction62 (2016), 45–56.Topal et al. (2021)M Onat Topal, Anil Bas,and Imke van Heerden. 2021.Exploring transformers in natural languagegeneration: Gpt, bert, and xlnet.arXiv preprint arXiv:2102.08036(2021).Udebuana et al. (2019)Okpala Izunna Udebuana,Ijioma Patricia Ngozi, andEmejulu Augustine Obiajulu.2019.Analysis of Evaluated Sentiments; aPseudo-Linguistic Approach and Online Acceptability Index for Decision-Makingwith Data: Nigerian Election in View.Computing 7,2 (2019), 39–44.Verma et al. (2011)Sudha Verma,
4. [4]:  Passage ID 4: a Concept F1666Note if there are no concepts detected in the snippet and summary by the entity extractor, then a conservative F1 score of 0 is given for that example. We use an in-house medical entity extractor to extract medical concepts in the summary. Medical concepts in the decoded summary that weren’t present in the original conversation would be false positives and vice versa for false negatives.Negation Correctness: To measure the effectiveness of the model to identify the negated status of medical concepts,we use Negex Harkema et al. (2009) to determine negated concepts. Of the concepts present in the decoded summary, we evaluate precision and recall on whether the decoded negations were accurate for the decoded concepts and compute a negation F166{}^{\ref{footnote f1}}.7.2 Doctor EvaluationWe also had doctors, who serve patients on our telehealth platform, evaluate the summaries produced by the models. Given the local dialogue snippets and the generated summary, we
5. [5]:  Passage ID 5: it is important for doctors to evaluate the quality of the output.4.1 Automated metricsWhile we measure model performance on standard metrics of ROUGE Lin (2004), we also wanted to specifically measure a model’s effectiveness in capturing the medical concepts that are of importance, and the negations. Therefore, we propose a new set of automated metrics that directly measure medically relevant information in the summary.Medical Concept Coverage: The concept coverage set of metrics captures the encapsulation of the medical terms in the model’s output summary to the ground truth reference. In particular, let 𝒞𝒞\mathcal{C} be the set of medical concepts in the reference summary and 𝒞^^𝒞\hat{\mathcal{C}} be the set of concepts in the summary output by the model. Then,Concept recall=∑n=1N|𝒞^(n)∩𝒞(n)|∑n=1N|𝒞(n)|Concept recallsuperscriptsubscript𝑛1𝑁superscript^𝒞𝑛superscript𝒞𝑛superscriptsubscript𝑛1𝑁superscript𝒞𝑛\textrm{Concept