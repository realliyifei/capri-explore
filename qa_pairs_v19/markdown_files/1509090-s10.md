# Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

How do linguistic properties such as collocation and non-compositionality contribute to methods for discovering multiword expressions?

## URLs

1. https://ar5iv.org/html/2401.15393. [2401.15393] Semantics of Multiword Expressions in Transformer-Based Models: A Survey
2. https://ar5iv.org/html/2405.08497. [2405.08497] Is Less More? Quality, Quantity and Context in Idiom Processing with Natural Language Models
3. https://ar5iv.org/html/2204.10050. [2204.10050] SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding
4. https://ar5iv.org/html/2109.04413. [2109.04413] AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models
5. https://ar5iv.org/html/2405.02861. [2405.02861] Revisiting a Pain in the Neck: Semantic Phrase Processing Benchmark for Language Models
6. https://ar5iv.org/html/2110.10064. [2110.10064] Idiomatic Expression Identification using Semantic Compatibility
7. https://ar5iv.org/html/1908.05490. No content available

## Answer (I don't know?: ✗)

Linguistic properties such as collocation and non-compositionality play significant roles in the identification and understanding of multiword expressions (MWEs) in Natural Language Processing (NLP). Collocation refers to the tendency of words to co-occur with greater frequency than would be expected by chance, forming predictable combinations that typically convey particular meanings. Non-compositionality, on the other hand, relates to the phenomenon where the meaning of a multiword expression cannot be derived directly from the meanings of its individual components.

The detection of collocations is essential for discovering MWEs, as it allows models to identify phrases that are frequently used together in natural language. For instance, expressions like "make a decision" or "strong coffee" often appear together and carry distinct meanings that differ from their component words when considered in isolation. The ability to pinpoint collocational patterns helps improve the accuracy of MWE identification in computational contexts [3]. Researchers have developed various techniques, such as unsupervised learning methods, that capture these collocational relationships, thus enhancing machine comprehension of language [4].

Non-compositionality significantly complicates the processing of MWEs. Understanding that certain phrases do not conform to standard compositional semantics is crucial for effective language modeling. For example, the idiomatic expression "kick the bucket" cannot be understood literally, and its meaning must be learned from usage patterns rather than derived from the individual words involved. This aspect challenges existing vector representation models since traditional approaches may fail to recognize such idiomaticity and treat MWEs like any other phrase, leading to misinterpretations [1] [2].

Moreover, modern word representation models have begun addressing these challenges. By incorporating both collocational and non-compositional features, they can achieve more nuanced and context-aware embeddings that reflect not just individual word meanings but also the collective usage and semantic significance of phrases. Techniques such as contextual embeddings—exemplified by models like BERT—leverage large corpuses to capture the relational properties of language, making them better suited for identifying and analyzing MWEs in various applications, from text classification to sentiment analysis [3] [4].

Research has shown that incorporating both collocational cues and non-compositionality insights into computational frameworks leads to substantial improvements in the extraction and treatment of MWEs. This dual focus allows for the creation of more effective vector representations, which are then employed in various NLP tasks, from machine translation to information retrieval, ultimately enhancing the performance of natural language understanding systems [3] [4].

In summary, collocation aids in identifying frequently occurring word combinations that form MWEs, while non-compositionality influences how these expressions are interpreted. The integration of these linguistic properties into NLP methods enhances the detection, understanding, and application of multiword expressions, revealing the complexity of natural language and improving machine comprehension.

1. [1]:  https://ar5iv.org/html/2204.10050, [2204.10050] SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding
2. [2]:  https://ar5iv.org/html/2401.15393, [2401.15393] Semantics of Multiword Expressions in Transformer-Based Models: A Survey
3. [3]:  https://ar5iv.org/html/2010.15036, No Title
4. [4]:  https://ar5iv.org/html/2109.04413, [2109.04413] AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models
5. [5]:  https://ar5iv.org/html/2405.02861, [2405.02861] Revisiting a Pain in the Neck: Semantic Phrase Processing Benchmark for Language Models
---
1. [1]:  Passage ID 1: Linguistics.McCarthy et al. (2003)Diana McCarthy, Bill Keller, and John Carroll. 2003.Detecting acontinuum of compositionality in phrasal verbs.In Proceedings of the ACL 2003 Workshop on MultiwordExpressions: Analysis, Acquisition and Treatment, pages 73–80, Sapporo,Japan. Association for Computational Linguistics.Nandakumar et al. (2019)Navnita Nandakumar, Timothy Baldwin, and Bahar Salehi. 2019.How well do embeddingmodels capture non-compositionality? a view from multiword expressions.In Proceedings of the 3rd Workshop on Evaluating Vector SpaceRepresentations for NLP, pages 27–34, Minneapolis, USA. Association forComputational Linguistics.Navigli and Ponzetto (2010)Roberto Navigli and Simone Paolo Ponzetto. 2010.Babelnet: Building a very large multilingual semantic network.In Proceedings of the 48th annual meeting of the associationfor computational linguistics, pages 216–225.Pereira and Kobayashi (2022)Lis Pereira and Ichiro
2. [2]:  Passage ID 2: editors,Multiword Expressions in Lexical Resources. Linguistic, Lexicographicand Computational Perspectives, Phraseology and Multiword Expressions.Language Science Press, Berlin.Seo et al. (2017)Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017.Bidirectionalattention flow for machine comprehension.In Proceedings of the Fifth International Conference onLearning Representations (ICLR 2017).Shwartz and Dagan (2019)Vered Shwartz and Ido Dagan. 2019.Still a pain in theneck: Evaluating text representations on lexical composition.Transactions of the Association for Computational Linguistics,7:403–419.Sinha et al. (2021)Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, andDouwe Kiela. 2021.Maskedlanguage modeling and the distributional hypothesis: Order word matterspre-training for little.In Proceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing, pages 2888–2913,
3. [3]:  Passage ID 3: language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP related tasks. In the end, this survey briefly discusses the commonly used ML and DL based classifiers, evaluation metrics and the applications of these word embeddings in different NLP tasks.Text Mining, Natural Language Processing, Word representation, Language
4. [4]:  Passage ID 4: Multiwordexpression processing: A Survey.Computational Linguistics, 43(4):837–892.Cook et al. (2008)Paul Cook, Afsaneh Fazly, and Suzanne Stevenson. 2008.The VNCTokens Dataset.In In proceedings of the MWE workshop. ACL.Cordeiro et al. (2019)Silvio Cordeiro, Aline Villavicencio, Marco Idiart, and Carlos Ramisch. 2019.Unsupervisedcompositionality prediction of nominal compounds.Computational Linguistics, 45(1):1–57.Devlin et al. (2019)Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.BERT: Pre-training ofdeep bidirectional transformers for language understanding.In Proceedings of the 2019 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers), pages 4171–4186,Minneapolis, Minnesota. Association for Computational Linguistics.Farahmand et al. (2015)Meghdad Farahmand, Aaron Smith, and Joakim Nivre. 2015.A multiword
5. [5]:  Passage ID 5: Linguistics.Rodríguez (2003)María A Barrios Rodríguez. 2003.The domain of the lexical functions fact0, causfact0 and real1.learning, page 64.Rodríguez-Fernández et al. (2016)Sara Rodríguez-Fernández, Luis Espinosa-Anke, Roberto Carlini, and Leo Wanner. 2016.Semantics-driven recognition of collocations using word embeddings.In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 499–505, Berlin, Germany. Association for Computational Linguistics.Sag et al. (2002)Ivan A Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002.Multiword expressions: A pain in the neck for nlp.In Computational Linguistics and Intelligent Text Processing: Third International Conference, CICLing 2002 Mexico City, Mexico, February 17–23, 2002 Proceedings 3, pages 1–15. Springer.Sailer and Markantonatou (2018)Manfred Sailer and Stella Markantonatou. 2018.Multiword expressions: