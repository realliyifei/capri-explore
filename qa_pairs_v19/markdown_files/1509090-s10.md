# Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

How do linguistic properties such as collocation and non-compositionality contribute to methods for discovering multiword expressions?

## URLs

1. https://ar5iv.org/html/2401.15393. [2401.15393] Semantics of Multiword Expressions in Transformer-Based Models: A Survey
2. https://ar5iv.org/html/2405.08497. [2405.08497] Is Less More? Quality, Quantity and Context in Idiom Processing with Natural Language Models
3. https://ar5iv.org/html/2204.10050. [2204.10050] SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding
4. https://ar5iv.org/html/2109.04413. [2109.04413] AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models
5. https://ar5iv.org/html/2405.02861. [2405.02861] Revisiting a Pain in the Neck: Semantic Phrase Processing Benchmark for Language Models
6. https://ar5iv.org/html/2110.10064. [2110.10064] Idiomatic Expression Identification using Semantic Compatibility
7. https://ar5iv.org/html/1908.05490. No content available

## Answer (I don't know?: ✗)

Linguistic properties, particularly collocation and non-compositionality, play crucial roles in the discovery and processing of multiword expressions (MWEs) within natural language processing (NLP) frameworks. These properties inform how models recognize and evaluate the relationships between words in phrases.

1. **Collocation**: Collocation refers to the habitual juxtaposition of words that frequently occur together, forming a cohesive phrase. Recognizing collocational patterns can greatly enhance the identification of MWEs. For instance, phrases like "make a decision" or "strong tea" are examples where the words have a specific syntactic and semantic relationship that is not purely additive. Understanding these relationships is essential for NLP tasks such as semantic analysis and text generation. Studies have shown that incorporating collocation into MWE detection increases a model's capability to discern these expressions from more generic word combinations [4], [5]. The alignment of linguistic behavior among MWEs, indicated by their distinct syntactic and semantic characteristics compared to generic phrases, emphasizes the importance of collocations in modeling [4].

2. **Non-compositionality**: This concept refers to MWEs whose meanings cannot be deduced from the meanings of their constituent words individually. For example, "kick the bucket" means "to die," which cannot be inferred from the meanings of "kick" or "bucket." Non-compositionality poses significant challenges for NLP systems, which often rely on semantic compositionality to understand language. Detecting non-compositionality is fundamental in establishing which phrases should be treated as MWEs. Recent research highlights that embedding models struggle to effectively capture this non-compositionality, indicating that linguistic models must incorporate more contextual and semantic information to enhance MWE recognition [1], [2]. The ability to distinguish idiomatic expressions—which are inherently non-compositional—from compositional phrases can significantly impact the efficacy of paraphrasing and translation tasks in NLP [4].

In summary, both collocation and non-compositionality are integral to methods for discovering MWEs in NLP. They influence how models process and interpret language, affecting tasks that require sophisticated understanding of phrase meanings and structural relationships. Addressing the nuances of these properties through enhanced contextual embeddings and tailored algorithms continues to be a focal point of research aimed at improving the accuracy and efficiency of MWE processing in various linguistic applications [1] [5].

1. [1]:  https://ar5iv.org/html/2204.10050, [2204.10050] SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding
2. [2]:  https://ar5iv.org/html/2401.15393, [2401.15393] Semantics of Multiword Expressions in Transformer-Based Models: A Survey
3. [3]:  https://ar5iv.org/html/2109.04413, [2109.04413] AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models
4. [4]:  https://ar5iv.org/html/2204.10050, [2204.10050] SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding
5. [5]:  https://ar5iv.org/html/2401.15393, [2401.15393] Semantics of Multiword Expressions in Transformer-Based Models: A Survey
---
1. [1]:  Passage ID 1: Linguistics.McCarthy et al. (2003)Diana McCarthy, Bill Keller, and John Carroll. 2003.Detecting acontinuum of compositionality in phrasal verbs.In Proceedings of the ACL 2003 Workshop on MultiwordExpressions: Analysis, Acquisition and Treatment, pages 73–80, Sapporo,Japan. Association for Computational Linguistics.Nandakumar et al. (2019)Navnita Nandakumar, Timothy Baldwin, and Bahar Salehi. 2019.How well do embeddingmodels capture non-compositionality? a view from multiword expressions.In Proceedings of the 3rd Workshop on Evaluating Vector SpaceRepresentations for NLP, pages 27–34, Minneapolis, USA. Association forComputational Linguistics.Navigli and Ponzetto (2010)Roberto Navigli and Simone Paolo Ponzetto. 2010.Babelnet: Building a very large multilingual semantic network.In Proceedings of the 48th annual meeting of the associationfor computational linguistics, pages 216–225.Pereira and Kobayashi (2022)Lis Pereira and Ichiro
2. [2]:  Passage ID 2: editors,Multiword Expressions in Lexical Resources. Linguistic, Lexicographicand Computational Perspectives, Phraseology and Multiword Expressions.Language Science Press, Berlin.Seo et al. (2017)Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017.Bidirectionalattention flow for machine comprehension.In Proceedings of the Fifth International Conference onLearning Representations (ICLR 2017).Shwartz and Dagan (2019)Vered Shwartz and Ido Dagan. 2019.Still a pain in theneck: Evaluating text representations on lexical composition.Transactions of the Association for Computational Linguistics,7:403–419.Sinha et al. (2021)Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, andDouwe Kiela. 2021.Maskedlanguage modeling and the distributional hypothesis: Order word matterspre-training for little.In Proceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing, pages 2888–2913,
3. [3]:  Passage ID 3: Multiwordexpression processing: A Survey.Computational Linguistics, 43(4):837–892.Cook et al. (2008)Paul Cook, Afsaneh Fazly, and Suzanne Stevenson. 2008.The VNCTokens Dataset.In In proceedings of the MWE workshop. ACL.Cordeiro et al. (2019)Silvio Cordeiro, Aline Villavicencio, Marco Idiart, and Carlos Ramisch. 2019.Unsupervisedcompositionality prediction of nominal compounds.Computational Linguistics, 45(1):1–57.Devlin et al. (2019)Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.BERT: Pre-training ofdeep bidirectional transformers for language understanding.In Proceedings of the 2019 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers), pages 4171–4186,Minneapolis, Minnesota. Association for Computational Linguistics.Farahmand et al. (2015)Meghdad Farahmand, Aaron Smith, and Joakim Nivre. 2015.A multiword
4. [4]:  Passage ID 4: at identifying whether a sentence contains an idiomatic expression, and (b) a task based on semantic text similarity which requires the model to adequately represent potentially idiomatic expressions in context. Each Subtask includes different settings regarding the amount of training data. Besides the task description, this paper introduces the datasets in English, Portuguese, and Galician and their annotation procedure, the evaluation metrics, and a summary of the participant systems and their results. The task had close to 100 registered participants organised into twenty five teams making over 650 and 150 submissions in the practice and evaluation phases respectively.1 IntroductionMultiword Expressions (MWEs) are a challenge for natural language processing (NLP), as their linguistic behaviour (e.g., syntactic, semantic) differs from that of generic word combinations Baldwin and Kim (2010); Ramisch and Villavicencio (2018). Moreover, MWEs are pervasive in all domains Biber
5. [5]:  Passage ID 5: performance on individual items whose topic distribution is similar to that of the full dataset (Nedumpozhimana et al., 2022).Summary. A wide array of experimental settings unequivocally show that any increase in contextual information enables better MWE representations. Linguistic context affects the behavior of model structures and is a beneficial source of information on multiple tasks – including those which are not readily reduced to comparisons of target expressions and surrounding context.5 Impact of linguistic propertiesMWE representations may be affected by properties of the target expressions themselves.We now provide a breakdown of the reported effects.Individual expressions vary in terms of their inherent predictive properties, as shown in work on the usefulness of individual idioms when training an idiomaticity classifier.Nedumpozhimana et al. (2022) note a positive effect of informativeness, measured by training a classifier on one idiom and evaluating it