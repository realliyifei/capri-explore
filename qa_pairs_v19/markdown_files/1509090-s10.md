# Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

How do linguistic properties such as collocation and non-compositionality contribute to methods for discovering multiword expressions?

## URLs

1. https://ar5iv.org/html/2401.15393
2. https://ar5iv.org/html/2405.08497
3. https://ar5iv.org/html/2204.10050
4. https://ar5iv.org/html/2109.04413
5. https://ar5iv.org/html/2405.02861
6. https://ar5iv.org/html/2110.10064
7. https://ar5iv.org/html/1908.05490

## Answer

## I don't know?: False

Linguistic properties such as collocation and non-compositionality play significant roles in discovering multiword expressions (MWEs), influencing the methods and techniques used in natural language processing (NLP).

Collocation refers to the way certain words tend to occur together more frequently than would be expected by chance. This phenomenon can be leveraged in identifying MWEs because the frequency of co-occurrence serves as a strong indicator of a semantic or functional unit in language. By employing statistical measures and language models that capture these co-occurrence patterns, researchers can effectively identify phrases that maintain specific meanings when used together, demonstrating collocational strength. For instance, language models can analyze large corpora to extract common collocations, thereby informing the development of resources that classify MWEs automatically [1] [2].

Non-compositionality pertains to the relationship between the meanings of the individual components of a multiword expression versus the meaning of the entire expression. For example, in the phrase "kick the bucket," the literal meanings of the individual words do not contribute to the overall meaning, which pertains to dying. Understanding non-compositionality is crucial for developing accurate text representations and models because these expressions can mislead traditional methods that rely on straightforward compositional semantics. Thus, recognizing non-compositional phrases is essential for creating more refined language models that can differentiate between literal and idiomatic meanings [1] [3].

The integration of both collocation and non-compositionality into machine learning algorithms enhances the detection and understanding of MWEs. For example, models that leverage embedding techniques need to accurately identify and represent these expressions to capture their unique semantics effectively. Research has shown that embedding models can struggle with non-compositional phrases, thus motivating the development of specialized embeddings designed to capture the nuances of such expressions [2] [4].

Furthermore, advances in NLP, such as the use of bidirectional attention mechanisms, enable models to better understand the context in which MWEs appear. This contextualization helps identify and represent MWEs more accurately, making it easier to detect non-compositional phrases that might otherwise be overlooked in a less contextually aware model [2] [5]. 

Overall, the interplay between collocation and non-compositionality is crucial for improving methods of discovering multiword expressions in NLP. By leveraging statistical techniques alongside contextual and semantic analyses, researchers can enhance the capabilities of language models, leading to better performance in various NLP applications, including machine translation, sentiment analysis, and more [3] [4].

[1]: https://ar5iv.org/html/2204.10050, [2204.10050] SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding
[2]: https://ar5iv.org/html/2401.15393, [2401.15393] Semantics of Multiword Expressions in Transformer-Based Models: A Survey
[3]: https://ar5iv.org/html/2010.15036, No Title
[4]: https://ar5iv.org/html/2010.15036, No Title
[5]: https://ar5iv.org/html/2109.04413, [2109.04413] AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models

[1]: Passage ID 1: Linguistics.McCarthy et al. (2003)Diana McCarthy, Bill Keller, and John Carroll. 2003.Detecting acontinuum of compositionality in phrasal verbs.In Proceedings of the ACL 2003 Workshop on MultiwordExpressions: Analysis, Acquisition and Treatment, pages 73–80, Sapporo,Japan. Association for Computational Linguistics.Nandakumar et al. (2019)Navnita Nandakumar, Timothy Baldwin, and Bahar Salehi. 2019.How well do embeddingmodels capture non-compositionality? a view from multiword expressions.In Proceedings of the 3rd Workshop on Evaluating Vector SpaceRepresentations for NLP, pages 27–34, Minneapolis, USA. Association forComputational Linguistics.Navigli and Ponzetto (2010)Roberto Navigli and Simone Paolo Ponzetto. 2010.Babelnet: Building a very large multilingual semantic network.In Proceedings of the 48th annual meeting of the associationfor computational linguistics, pages 216–225.Pereira and Kobayashi (2022)Lis Pereira and Ichiro
[2]: Passage ID 2: editors,Multiword Expressions in Lexical Resources. Linguistic, Lexicographicand Computational Perspectives, Phraseology and Multiword Expressions.Language Science Press, Berlin.Seo et al. (2017)Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017.Bidirectionalattention flow for machine comprehension.In Proceedings of the Fifth International Conference onLearning Representations (ICLR 2017).Shwartz and Dagan (2019)Vered Shwartz and Ido Dagan. 2019.Still a pain in theneck: Evaluating text representations on lexical composition.Transactions of the Association for Computational Linguistics,7:403–419.Sinha et al. (2021)Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, andDouwe Kiela. 2021.Maskedlanguage modeling and the distributional hypothesis: Order word matterspre-training for little.In Proceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing, pages 2888–2913,
[3]: Passage ID 3: language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP related tasks. In the end, this survey briefly discusses the commonly used ML and DL based classifiers, evaluation metrics and the applications of these word embeddings in different NLP tasks.Text Mining, Natural Language Processing, Word representation, Language
[4]: Passage ID 4: language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP related tasks. In the end, this survey briefly discusses the commonly used ML and DL based classifiers, evaluation metrics and the applications of these word embeddings in different NLP tasks.Text Mining, Natural Language Processing, Word representation, Language
[5]: Passage ID 5: Multiwordexpression processing: A Survey.Computational Linguistics, 43(4):837–892.Cook et al. (2008)Paul Cook, Afsaneh Fazly, and Suzanne Stevenson. 2008.The VNCTokens Dataset.In In proceedings of the MWE workshop. ACL.Cordeiro et al. (2019)Silvio Cordeiro, Aline Villavicencio, Marco Idiart, and Carlos Ramisch. 2019.Unsupervisedcompositionality prediction of nominal compounds.Computational Linguistics, 45(1):1–57.Devlin et al. (2019)Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.BERT: Pre-training ofdeep bidirectional transformers for language understanding.In Proceedings of the 2019 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers), pages 4171–4186,Minneapolis, Minnesota. Association for Computational Linguistics.Farahmand et al. (2015)Meghdad Farahmand, Aaron Smith, and Joakim Nivre. 2015.A multiword