# Transformers for Tabular Data Representation: A Survey of Models and Applications

## Question

What is the structure of the vanilla transformer architecture, and what are its key components?

## URLs

1. https://ar5iv.org/html/2311.12351. [2311.12351] Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey
2. https://ar5iv.org/html/2407.06677. [2407.06677] Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules
3. https://ar5iv.org/html/1910.10488. [1910.10488] Injecting Hierarchy with U-Net Transformers
4. https://ar5iv.org/html/2403.01994. [2403.01994] Vanilla Transformers are Transfer Capability Teachers
5. https://ar5iv.org/html/2205.01138. [2205.01138] Transformers in Time-series Analysis: A Tutorial
6. https://ar5iv.org/html/2206.06488. [2206.06488] Multimodal Learning with Transformers: A Survey
7. https://ar5iv.org/html/2311.17633. [2311.17633] Introduction to Transformers: an NLP Perspective
8. https://ar5iv.org/html/2409.16653. [2409.16653] The Credibility Transformer
9. https://ar5iv.org/html/2104.09079. [2104.09079] A novel time‚Äìfrequency Transformer based on self‚Äìattention mechanism and its application in fault diagnosis of rolling bearings
10. https://ar5iv.org/html/2408.04723. [2408.04723] Survey: Transformer-based Models in Data Modality Conversion

## Answer (I don't know?: ‚úó)

The vanilla Transformer architecture, proposed by Vaswani et al. in 2017, is characterized by its unique structure that primarily relies on attention mechanisms rather than traditional recurrent or convolutional models. This architecture enables efficient processing of sequential input data, making it particularly effective for natural language processing (NLP) tasks.

The core structure of the Transformer consists of an encoder-decoder framework, where the encoder's primary role is to process the input and the decoder is responsible for generating the output. However, many applications focus exclusively on the encoder part, which serves as a feature extractor for various tasks [2] [3].

The encoder itself is composed of a stack of several identical layers, with each layer containing two main components: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network [2]. The self-attention mechanism plays a crucial role by assigning different weights to the input representations, allowing the model to focus on various relevant segments of the data [3] [5]. The effectiveness of this attention mechanism comes from its ability to evaluate how well each input (the "query") aligns with its corresponding contextual inputs (the "keys"), producing weighted sums that emphasize the most relevant information [5].

To facilitate the training of deep networks, each of these sub-layers in the encoder incorporates residual connections that help ease the flow of gradients during backpropagation [2]. Following these residual connections, layer normalization is applied to stabilize activation distributions, further enhancing the training process [2] [3].

Additionally, the position-wise feed-forward network, which operates independently on each position of the input sequence, consists of two linear transformations with a ReLU activation in between [2]. This structure allows the model to learn complex mappings between the input and output sequences.

Overall, the combination of the multi-head self-attention mechanism, position-wise feed-forward networks, residual connections, and layer normalization are key components that establish the architecture of the vanilla Transformer [1] [2] [3]. This innovative design has significantly impacted NLP, enabling advanced capabilities in understanding and generating natural language. The attention-based structure allows the model to capture global dependencies effectively, improving performance and efficiency compared to previous architectures that relied on sequential processing [3] [5].

1. [1]:  https://ar5iv.org/html/2408.03130, No Title
2. [2]:  https://ar5iv.org/html/2011.06727, No Title
3. [3]:  https://ar5iv.org/html/2311.13755, No Title
4. [4]:  https://ar5iv.org/html/2104.09079, [2104.09079] A novel time‚Äìfrequency Transformer based on self‚Äìattention mechanism and its application in fault diagnosis of rolling bearings
5. [5]:  https://ar5iv.org/html/2311.13755, No Title
---
1. [1]:  Passage ID 1: of the categorization and to the respective section for a more detailed look at the discussed literature in each category.II PreliminariesII-A Transformers for Language ModelingIn recent years, the transformer has become the primary architecture for Natural Language Processing (NLP) tasks [7] [8] [9] [10]. The prominence is because of the attention mechanism, which enables the transformer to efficiently focus on different text parts and learn complex language structures [11] [12]. The mechanism aims to determine the significance of various elements in a sequence about a specific element [11] [12].These learned structures enable the transformer to solve complex NLP problems. In addition to that, the transformer typically comprises multiple transformer blocks, each containing an attention module and a feed-forward module [11] [13]. The feed-forward model facilitates the learning of mappings between the input and output and can be used in the encoder or decoder blocks [11]
2. [2]:  Passage ID 2: performance for Neural Machine Translation (NMT) tasks. The overall architecture is based solely on attention mechanisms to draw global dependencies between input, dispensing with recurrence and convolutions entirely. The initial proposed Transformer employs a sequence to sequence structure that comprises the encoder and decoder. But the subsequent research work often adopt the encoder part to serve as the feature extractor, thus our introduction here is limited to it.The encoder is composed of a stack of several identical layers, which includes a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. It employs a residual connection¬†[30] around each of the two sub-layers to ease the training of deep neural network. And layer normalization¬†[53] is applied after the residual connection to stabilize the activations of model.Due to its superior performance, the Transformer is widely used in various NLP tasks and has achieved excellent results.
3. [3]:  Passage ID 3: foundation for the widespread use of pre-trained language models. As a result, it has significantly amplified the effectiveness and range of text analytic tasks within the realm of deep learning [46].3.2 TRANSFORMERSThe Transformer architecture, as described in the influential work by Vaswani et¬†al. (2017), presents a unique framework for transferring weighted knowledge between different neural components. Unlike traditional approaches that rely on recurrent or convolutional structures, the Transformer exclusively utilizes attention mechanisms. At the heart of its effectiveness is the attention mechanism, which assigns weights to each input representation and learns to focus on important segments of the data. The output is then computed by taking a weighted sum of these values, with the weights determined through evaluating how well the query matches with its corresponding key [47]. The innovative design of the Transformer has not only advanced NLP but also made significant
4. [4]:  Passage ID 4: PreliminariesThis section will briefly introduce the vanilla Transformer proposed by Vaswani et al. [26] in 2017. Variants of Transformer applicated in the fields of natural language processing (NLP) and computer vision (CV) will also be reviewed.2.1 TransformerRecurrent models have shown a good capability to process sequence input in the form of [x1,x2,‚Ä¶,xt]subscriptùë•1subscriptùë•2‚Ä¶subscriptùë•ùë°\left[x_{1},x_{2},\dots,x_{t}\right]. Along the direction of the input sequence, they generate a sequence of hidden states htsubscript‚Ñéùë°h_{t}, as a function of the previous hidden state ht‚àí1subscript‚Ñéùë°1h_{t-1} and the input token xtsubscriptùë•ùë°x_{t} for position tùë°t. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths. Vaswani et al. [26] proposed Transformer, a new architecture
5. [5]:  Passage ID 5: with the weights determined through evaluating how well the query matches with its corresponding key [47]. The innovative design of the Transformer has not only advanced NLP but also made significant progress in computer vision and spatio-temporal modelling [48, 49]. By efficiently processing sequential data like sentences, the Transformer has improved model performance through enhanced parallelization, reducing training times [50]. Its attention-centric mechanism enables the architecture to capture global dependencies between input and output, pushing the boundaries of what can be achieved in NLP and related fields. Fig. 2 represents the Transformers architecture.BERT (Bidirectional Encoder Representations from Transformers), however, is a major breakthrough in the field of deep language understanding. Its architecture, which utilizes the powerful Transformer model, particularly its encoder component, has revolutionized our ability to comprehend natural language. BERT‚Äôs