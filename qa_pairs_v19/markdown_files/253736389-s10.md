# Transformers for Tabular Data Representation: A Survey of Models and Applications

## Question

What is the structure of the vanilla transformer architecture, and what are its key components?

## URLs

1. https://ar5iv.org/html/2311.12351
2. https://ar5iv.org/html/2407.06677
3. https://ar5iv.org/html/1910.10488
4. https://ar5iv.org/html/2403.01994
5. https://ar5iv.org/html/2205.01138
6. https://ar5iv.org/html/2206.06488
7. https://ar5iv.org/html/2311.17633
8. https://ar5iv.org/html/2409.16653
9. https://ar5iv.org/html/2104.09079
10. https://ar5iv.org/html/2408.04723

## Answer

The vanilla transformer architecture, introduced by Vaswani et al. in 2017, represents a significant innovation in the field of Natural Language Processing (NLP). It fundamentally differs from traditional models by utilizing attention mechanisms rather than recurrent or convolutional structures. The architecture is typically composed of an encoder-decoder framework, where each part consists of multiple layers of identical blocks [5].

### Key Components of the Vanilla Transformer Architecture:

1. **Encoder-Decoder Structure**: The transformer consists of an encoder that processes the input data and a decoder that generates the output. The encoder transforms input text into a sequence of continuous representations, whereas the decoder generates text in an autoregressive manner, predicting one symbol at a time based on previously generated symbols [5].

2. **Attention Mechanisms**: Central to the effectiveness of the transformer is the attention mechanism, which allows the model to learn which parts of the input data to focus on when making predictions. Specifically, each input representation is assigned a weight through the attention mechanism, enabling the model to compute a weighted sum of the inputs and focus on significant segments of the data [4]. 

3. **Multi-Head Attention**: The architecture employs a multi-head attention component that allows the model to jointly attend to information from different representation subspaces at different positions. This enhances the model's ability to capture complex relationships within the input data [5].

4. **Feed-Forward Neural Networks**: Each encoder and decoder block contains a feed-forward neural network. This network operates independently on each position of the input and is used to transform the data obtained from the attention layers into a format suitable for further processing. These feed-forward networks are typically comprised of two linear transformations with a ReLU activation in between [1] [3].

5. **Positional Encoding**: As transformers do not process the input sequentially (unlike RNNs), they require a mechanism to incorporate the order of the sequence. This is achieved through positional encoding, which adds information about the position of each token in the sequence to the input embeddings [5].

6. **Normalization and Residual Connections**: Layer normalization is applied after each sub-layer (attention and feed-forward) within both the encoder and decoder. Additionally, residual connections help to mitigate the vanishing gradient problem by allowing gradients to flow through the network more effectively [3].

In summary, the vanilla transformer architecture is characterized by its encoder-decoder structure, reliance on attention mechanisms for effective data representation, the inclusion of feed-forward neural networks, the use of positional encodings for order representation, and incorporation of normalization and residual connections to enhance training stability and model performance [4] [5]. This architecture has proven instrumental in achieving state-of-the-art results in various NLP tasks, marking a paradigm shift from previous methods [3] [4].

[1]: https://ar5iv.org/html/2408.03130, No Title
[2]: https://ar5iv.org/html/2312.05589, No Title
[3]: https://ar5iv.org/html/2408.04723, [2408.04723] Survey: Transformer-based Models in Data Modality Conversion
[4]: https://ar5iv.org/html/2311.13755, No Title
[5]: https://ar5iv.org/html/2104.09079, [2104.09079] A novel time‚Äìfrequency Transformer based on self‚Äìattention mechanism and its application in fault diagnosis of rolling bearings

[1]: Passage ID 1: of the categorization and to the respective section for a more detailed look at the discussed literature in each category.II PreliminariesII-A Transformers for Language ModelingIn recent years, the transformer has become the primary architecture for Natural Language Processing (NLP) tasks [7] [8] [9] [10]. The prominence is because of the attention mechanism, which enables the transformer to efficiently focus on different text parts and learn complex language structures [11] [12]. The mechanism aims to determine the significance of various elements in a sequence about a specific element [11] [12].These learned structures enable the transformer to solve complex NLP problems. In addition to that, the transformer typically comprises multiple transformer blocks, each containing an attention module and a feed-forward module [11] [13]. The feed-forward model facilitates the learning of mappings between the input and output and can be used in the encoder or decoder blocks [11]
[2]: Passage ID 2: Machine Translation, Question Answering, Text Classification, Generation, Speech Recognition, Summarization, and Language Modeling. The paper systematically introduces each task, delineates key architectures from Recurrent Neural Networks (RNNs) to Transformer-based models like BERT, and evaluates their performance, challenges, and computational demands. The adaptability of ensemble techniques is emphasized, highlighting their capacity to enhance various NLP applications. Challenges in implementation, including computational overhead, overfitting, and model interpretation complexities, are addressed, alongside the trade-off between interpretability and performance. Serving as a concise yet invaluable guide, this review synthesizes insights into tasks, architectures, and challenges, offering a holistic perspective for researchers and practitioners aiming to advance language-driven applications through ensemble deep learning in NLP.1 IntroductionNatural Language Processing (NLP)
[3]: Passage ID 3: ProcessingIn the rapid development of NLP, Pretrained Language Models (PLMs) (Hu et¬†al., 2023) have established new benchmarks in performance across a range of linguistic tasks. In this section, we will first illustrate the architecture of NLP transformers and highlight prominent models. Subsequently, we will study various downstream tasks in NLP. Finally, we will show the application of TB models in converting textual data to visual or speech modalities.4.1. TB Architecture in NLPIn NLP, three types of TB models namely Encoder-only, Decoder-only, and Encoder-Decoder, which will be explained.4.1.1. Encoder-onlyEncoder-only architectures within the domain of PLMs endeavor to encapsulate the entirety of semantic and contextual data present within a text corpus, subsequently transforming this information into a condensed feature vector representation. The most popular encoder-only architecture is BERT, proposed by Devlin et al. (Devlin et¬†al., 2018). It represents a paradigm
[4]: Passage ID 4: foundation for the widespread use of pre-trained language models. As a result, it has significantly amplified the effectiveness and range of text analytic tasks within the realm of deep learning [46].3.2 TRANSFORMERSThe Transformer architecture, as described in the influential work by Vaswani et¬†al. (2017), presents a unique framework for transferring weighted knowledge between different neural components. Unlike traditional approaches that rely on recurrent or convolutional structures, the Transformer exclusively utilizes attention mechanisms. At the heart of its effectiveness is the attention mechanism, which assigns weights to each input representation and learns to focus on important segments of the data. The output is then computed by taking a weighted sum of these values, with the weights determined through evaluating how well the query matches with its corresponding key [47]. The innovative design of the Transformer has not only advanced NLP but also made significant
[5]: Passage ID 5: PreliminariesThis section will briefly introduce the vanilla Transformer proposed by Vaswani et al. [26] in 2017. Variants of Transformer applicated in the fields of natural language processing (NLP) and computer vision (CV) will also be reviewed.2.1 TransformerRecurrent models have shown a good capability to process sequence input in the form of [x1,x2,‚Ä¶,xt]subscriptùë•1subscriptùë•2‚Ä¶subscriptùë•ùë°\left[x_{1},x_{2},\dots,x_{t}\right]. Along the direction of the input sequence, they generate a sequence of hidden states htsubscript‚Ñéùë°h_{t}, as a function of the previous hidden state ht‚àí1subscript‚Ñéùë°1h_{t-1} and the input token xtsubscriptùë•ùë°x_{t} for position tùë°t. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths. Vaswani et al. [26] proposed Transformer, a new architecture