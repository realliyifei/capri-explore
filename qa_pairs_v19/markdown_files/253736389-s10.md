# Transformers for Tabular Data Representation: A Survey of Models and Applications

## Question

What is the structure of the vanilla transformer architecture, and what are its key components?

## URLs

1. https://ar5iv.org/html/2311.12351. [2311.12351] Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey
2. https://ar5iv.org/html/2407.06677. [2407.06677] Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules
3. https://ar5iv.org/html/1910.10488. [1910.10488] Injecting Hierarchy with U-Net Transformers
4. https://ar5iv.org/html/2403.01994. [2403.01994] Vanilla Transformers are Transfer Capability Teachers
5. https://ar5iv.org/html/2205.01138. [2205.01138] Transformers in Time-series Analysis: A Tutorial
6. https://ar5iv.org/html/2206.06488. [2206.06488] Multimodal Learning with Transformers: A Survey
7. https://ar5iv.org/html/2311.17633. [2311.17633] Introduction to Transformers: an NLP Perspective
8. https://ar5iv.org/html/2409.16653. [2409.16653] The Credibility Transformer
9. https://ar5iv.org/html/2104.09079. [2104.09079] A novel time‚Äìfrequency Transformer based on self‚Äìattention mechanism and its application in fault diagnosis of rolling bearings
10. https://ar5iv.org/html/2408.04723. [2408.04723] Survey: Transformer-based Models in Data Modality Conversion

## Answer (I don't know?: ‚úó)

The vanilla Transformer architecture, introduced by Vaswani et al. in 2017, is designed to process sequences of data while relying entirely on an attention mechanism to capture dependencies globally between inputs and outputs, as opposed to the traditional recurrent structures found in previous models [4] [5]. This innovative design facilitates parallelization during training, which is particularly advantageous for handling longer sequences [4] [5].

The core structure of the vanilla Transformer consists of several key components:

1. **Transformer Blocks**: The architecture is primarily composed of stacked blocks known as Transformer blocks. Each block operates through a combination of layers and mechanisms that enhance the model's ability to process sequences effectively [5].

2. **Multi-Head Self-Attention Mechanism**: At the heart of each Transformer block is the multi-head self-attention mechanism. This allows the model to attend to different parts of the input sequence simultaneously, facilitating the capture of relationships and dependencies across all tokens in the input [5].

3. **Position-wise Feed-Forward Network**: Following the attention mechanism, each block includes a position-wise feed-forward network. This component applies a linear transformation to each position (or token) independently, thereby enabling the model to process features across various dimensions [5].

4. **Layer Normalization**: Each Transformer block incorporates layer normalization, which helps stabilize and speed up the training by normalizing the inputs to each layer. This process mitigates issues related to internal covariate shift, enhancing the learning dynamics [5].

5. **Residual Connections**: Residual connections are employed within the model to facilitate the flow of gradients. These connections allow information from earlier layers to be added directly to the output of later layers, which helps in training deeper networks by addressing vanishing gradient issues [5].

6. **Positional Encoding**: Since the Transformer architecture does not utilize recurrent or convolutional layers that inherently encode the order of input tokens, it includes a positional encoding mechanism. This component adds information about the position of each token in the sequence to the input embeddings, which allows the model to maintain an understanding of the token sequence order [4].

Overall, the vanilla Transformer‚Äôs architecture significantly deviates from traditional sequential models, providing a powerful framework that is scalable and capable of managing complex relationships within data, making it a crucial component of modern Natural Language Processing tasks [5].

1. [1]:  https://ar5iv.org/html/2311.17633, [2311.17633] Introduction to Transformers: an NLP Perspective
2. [2]:  https://ar5iv.org/html/2311.17633, [2311.17633] Introduction to Transformers: an NLP Perspective
3. [3]:  https://ar5iv.org/html/2408.04723, [2408.04723] Survey: Transformer-based Models in Data Modality Conversion
4. [4]:  https://ar5iv.org/html/2104.09079, [2104.09079] A novel time‚Äìfrequency Transformer based on self‚Äìattention mechanism and its application in fault diagnosis of rolling bearings
5. [5]:  https://ar5iv.org/html/2104.09079, [2104.09079] A novel time‚Äìfrequency Transformer based on self‚Äìattention mechanism and its application in fault diagnosis of rolling bearings
---
1. [1]:  Passage ID 1: "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Introduction to Transformers: an NLP Perspective¬†\nameTong Xiao \emailxiaotong@mail.neu.edu.cn \addrNLP Lab., Northeastern University, Shenyang, ChinaNiuTrans Research, Shenyang, China\AND\nameJingbo Zhu \emailzhujingbo@mail.neu.edu.cn \addrNLP Lab., Northeastern University, Shenyang, ChinaNiuTrans Research, Shenyang, ChinaAbstractTransformers have dominated empirical machine learning models of natural language processing. In this paper, we introduce basic concepts of Transformers and present key techniques that form the recent advances of these models. This includes a description of the standard Transformer architecture, a series of model refinements, and common applications. Given that Transformers and related deep learning techniques might be evolving in ways we have never seen, we cannot dive into all the model details or cover all the technical areas. Instead, we
2. [2]:  Passage ID 2: work. For example, the inductive biases used in our model design can be thought of as some structural prior, while NLP models can also learn the underlying structure of problems by themselves. In this sub-section we will discuss some of these issues. We will focus on the methods of introducing linguistic structure into Transformer models. As Transformer can be applied to many NLP tasks, which differ much in their input and output formats, we will primarily discuss modifications to Transformer encoders (call them syntax-aware Transformer encoders). Our discussion, however, is general, and the methods can be easily extended to Transformer decoders.3.1 Syntax-aware Input and OutputOne of the simplest methods of incorporating structure into NLP systems is to modify the input sequence, leaving the system unchanged. As a simple example, consider a sentence where each word xjsubscriptùë•ùëóx_{j} is assigned a set of Œ∫ùúÖ\kappa syntactic labels
3. [3]:  Passage ID 3: ProcessingIn the rapid development of NLP, Pretrained Language Models (PLMs) (Hu et¬†al., 2023) have established new benchmarks in performance across a range of linguistic tasks. In this section, we will first illustrate the architecture of NLP transformers and highlight prominent models. Subsequently, we will study various downstream tasks in NLP. Finally, we will show the application of TB models in converting textual data to visual or speech modalities.4.1. TB Architecture in NLPIn NLP, three types of TB models namely Encoder-only, Decoder-only, and Encoder-Decoder, which will be explained.4.1.1. Encoder-onlyEncoder-only architectures within the domain of PLMs endeavor to encapsulate the entirety of semantic and contextual data present within a text corpus, subsequently transforming this information into a condensed feature vector representation. The most popular encoder-only architecture is BERT, proposed by Devlin et al. (Devlin et¬†al., 2018). It represents a paradigm
4. [4]:  Passage ID 4: PreliminariesThis section will briefly introduce the vanilla Transformer proposed by Vaswani et al. [26] in 2017. Variants of Transformer applicated in the fields of natural language processing (NLP) and computer vision (CV) will also be reviewed.2.1 TransformerRecurrent models have shown a good capability to process sequence input in the form of [x1,x2,‚Ä¶,xt]subscriptùë•1subscriptùë•2‚Ä¶subscriptùë•ùë°\left[x_{1},x_{2},\dots,x_{t}\right]. Along the direction of the input sequence, they generate a sequence of hidden states htsubscript‚Ñéùë°h_{t}, as a function of the previous hidden state ht‚àí1subscript‚Ñéùë°1h_{t-1} and the input token xtsubscriptùë•ùë°x_{t} for position tùë°t. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths. Vaswani et al. [26] proposed Transformer, a new architecture
5. [5]:  Passage ID 5: This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths. Vaswani et al. [26] proposed Transformer, a new architecture relying entirely on attention mechanism to draw global dependencies between input and output. Transformer completely abandons the traditional recurrent structure to realize the parallel calculation of sequence input. In addition, convolution operation which is difficult to globally model the relationship between local features is eliminated from in Transformer [21].Fig. 1: Architecture of the vanilla Transformer [26].Transformer is a multilayer structure by stacking Transformer blocks, whose vanilla form is shown in Fig. 1. The Transformer block is characterized by a multi-head self‚Äìattention mechanism, a position-wise feed-forward network, layer normalization [34] module and residual connector [35]. The input to the Transformer is often a tensor of shape