# Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models

## Question

How do direct optimization methods improve the effectiveness of prompts when probing pre-trained language models?

## URLs

1. https://ar5iv.org/html/2404.01077. [2404.01077] Efficient Prompting Methods for Large Language Models: A Survey
2. https://ar5iv.org/html/2212.09611. [2212.09611] Optimizing Prompts for Text-to-Image Generation
3. https://ar5iv.org/html/2405.17346. [2405.17346] Prompt Optimization with Human Feedback
4. https://ar5iv.org/html/2404.10357. [2404.10357] Optimization of Prompt Learning via Multi-Knowledge Representation for Vision-Language Models
5. https://ar5iv.org/html/2310.01691. [2310.01691] Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models
6. https://ar5iv.org/html/2402.14688. [2402.14688] Q-Probe: A Lightweight Approach to Reward Maximization for Language Models
7. https://ar5iv.org/html/2406.03843. [2406.03843] POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models
8. https://ar5iv.org/html/2411.12508. [2411.12508] Probe-Me-Not: Protecting Pre-trained Encoders from Malicious Probing
9. https://ar5iv.org/html/2309.05950. [2309.05950] Language Models as Black-Box Optimizers for Vision-Language Models
10. https://ar5iv.org/html/2410.14716. [2410.14716] A Systematic Survey on Large Language Models for Algorithm Design

## Answer (I don't know?: âœ—)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable