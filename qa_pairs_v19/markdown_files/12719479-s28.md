# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

How is the significance of agreement coefficients established beyond testing against chance agreement?

## URLs

1. https://ar5iv.org/html/2303.12502. [2303.12502] Measuring Agreement Among Several Raters Classifying Subjects Into One-Or-More (Hierarchical) Nominal Categories. A Generalisation of Fleiss’ kappa
2. https://ar5iv.org/html/2407.11371. [2407.11371] Estimating Agreement by Chance for Sequence Annotation
3. https://ar5iv.org/html/1909.10140. [1909.10140] A new coefficient of correlation
4. https://ar5iv.org/html/2404.09053. [2404.09053] ALICE: Combining Feature Selection and Inter-Rater Agreeability for Machine Learning Insights
5. https://ar5iv.org/html/2411.16797. [2411.16797] Enhancing Answer Reliability Through Inter-Model Consensus of Large Language Models
6. https://ar5iv.org/html/2107.11449. [2107.11449] Applying Inter-rater Reliability and Agreement in Grounded Theory Studies in Software Engineering
7. https://ar5iv.org/html/2401.12990. [2401.12990] Topic Modelling: Going Beyond Token Outputs
8. https://ar5iv.org/html/2410.05291. [2410.05291] Liberal-Conservative Hierarchies of Intercoder Reliability Estimators
9. https://ar5iv.org/html/2411.08410. [2411.08410] The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense
10. https://ar5iv.org/html/2101.00433. [2101.00433] Untitled Document

## Answer (I don't know?: ✗)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable