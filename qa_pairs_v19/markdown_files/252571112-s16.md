# SHAP-Based Explanation Methods: A Review for NLP Interpretability

## Question

What are the main criticisms of using Shapley values for feature importance in model explanations?

## URLs

1. https://ar5iv.org/html/2002.11097. [2002.11097] Problems with Shapley-value-based explanations as feature importance measures
2. https://ar5iv.org/html/2302.08160. [2302.08160] The Inadequacy of Shapley Values for Explainability
3. https://ar5iv.org/html/2310.00416. [2310.00416] Refutation of Shapley Values for XAI – Additional Evidence
4. https://ar5iv.org/html/2404.13522. [2404.13522] Error Analysis of Shapley Value-Based Model Explanations: An Informative Perspective
5. https://ar5iv.org/html/2303.05981. [2303.05981] Feature Importance: A Closer Look at Shapley Values and LOCO
6. https://ar5iv.org/html/2409.12578. [2409.12578] CLE-SH: Comprehensive Literal Explanation package for SHapley values by statistical validity
7. https://ar5iv.org/html/2410.06815. [2410.06815] shap-select: Lightweight Feature Selection Using SHAP Values and Regression
8. https://ar5iv.org/html/2111.11946. [2111.11946] Is Shapley Explanation for a model unique?
9. https://ar5iv.org/html/2206.08204. [2206.08204] Inherent Inconsistencies of Feature Importance

## Answer (I don't know?: ✗)

The main criticisms of using Shapley values for feature importance in model explanations revolve around their mathematical properties and their alignment with human-centric expectations. These critiques unfold in two primary dimensions: formal mathematical issues and human interpretability concerns.

Firstly, applying Shapley values to feature importance introduces mathematical challenges that can obscure their effectiveness as explanatory tools. The formalization of Shapley values is based on cooperative game theory, where features are viewed as players who contribute to the outcome of a game (the model's prediction) [3]. However, when utilized for feature importance, these mathematical properties may not align with intuitive expectations from users who seek clear and understandable explanations [1]. The complexities introduced in resolving these mathematical issues often necessitate additional layers of interpretation, such as the requirement for causal reasoning, which can complicate the explanation further [2].

Secondly, from a human-centric perspective, the expectations people have regarding explanations of model decisions are often not met by Shapley-value-based interpretations. Research indicates that while Shapley values possess desirable mathematical properties, they do not necessarily fulfill the goals of explainability that users desire [2]. This mismatch highlights that explanations derived from Shapley values may lack clarity or fail to provide actionable insights, which are critical in contexts where decision-making is impacted — particularly in high-stakes scenarios like legal or medical decisions.

Furthermore, the increasing complexity of modern machine learning models, such as deep learning and tree ensembles, exacerbates the difficulty in understanding feature contributions through Shapley values. With such models, analyzing model parameters directly to ascertain feature roles becomes nearly impossible, thereby prompting researchers to seek more sophisticated methods of feature importance, including Shapley values [4]. Nevertheless, when it comes to using these values in practical settings, their application can remain narrowly constrained and may not provide universal interpretability [1].

Finally, while alternative metrics for feature importance exist, challenges such as computational feasibility and the exponential complexity of enumerating explanations pose additional barriers [5]. This raises a significant concern: despite the theoretical appeal of Shapley values, they may not be the most practical or effective solution for explaining feature importance across diverse scenarios.

In summary, the critical views on Shapley values for feature importance highlight their mathematical limitations, the risks of complexity in human interpretation, the mismatch with user expectations for explainability, and the challenges arising from the intricate nature of modern machine learning models. Thus, while Shapley principles are embedded in numerous research efforts, caution is recommended in their application without careful consideration of these critiques.

1. [1]:  https://ar5iv.org/html/2002.11097, [2002.11097] Problems with Shapley-value-based explanations as feature importance measures
2. [2]:  https://ar5iv.org/html/2002.11097, [2002.11097] Problems with Shapley-value-based explanations as feature importance measures
3. [3]:  https://ar5iv.org/html/2002.11097, [2002.11097] Problems with Shapley-value-based explanations as feature importance measures
4. [4]:  https://ar5iv.org/html/2207.07605, No Title
5. [5]:  https://ar5iv.org/html/2302.08160, [2302.08160] The Inadequacy of Shapley Values for Explainability
---
1. [1]:  Passage ID 1: that Shapley-value-based explanations for feature importance fail to serve their desired purpose in general. We make this argument in two parts. Firstly, we show that applying the Shapley value to the problem of feature importance introduces mathematically formalizable properties which may not align with what we would expect from an explanation. Secondly, taking a human-centric perspective, we evaluate Shapley-value-based explanations through established frameworks of what people expect from explanations, and find them wanting. We find that the game theoretic problem formulation of Shapley-value-based explanations do not match the proposed use cases for its solution, and thus caution against their usage except in narrowly constrained settings where they admit a clear interpretation.We describe the different Shapley-value-based explanation frameworks in Section 2, and present our two-part critique in Sections 3 and 4. We discuss these results and provide some suggestions in
2. [2]:  Passage ID 2: show that mathematical problems arise when Shapley values are used for feature importance, and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values are not a natural solution to the human-centric goals of explainability.SHAP, Shapley values, explainable machine learning1 IntroductionMachine learning models are increasingly being used to replace human decision-making for tasks involving some kind of prediction. As state-of-the-art predictive machine learning models become increasingly inscrutable, there has been an increase in concern that the black-box nature of these systems can obscure undesirable properties of the decision algorithm, such as illegal bias or signals accidentally learned from artifacts irrelevant to the task at hand. More recently, attempts have been made to “explain” the output of a complicated function in terms of its inputs to
3. [3]:  Passage ID 3: } else { localStorage.setItem("ar5iv_theme", "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Problems with Shapley-value-based explanations as feature importance measuresI. Elizabeth Kumar  Suresh Venkatasubramanian  Carlos Scheidegger  Sorelle A. FriedlerAbstractGame-theoretic formulations of feature importance have become popular as a way to “explain” machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game’s unique Shapley values. Justification for these methods rests on two pillars: their desirable mathematical properties, and their applicability to specific motivations for explanations. We show that mathematical problems arise when Shapley values are used for feature importance, and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal
4. [4]:  Passage ID 4: used today, including tree ensembles and deep learning models, their large number of operations prevents us from understanding each feature’s role by examining the model parameters. These flexible, non-linear models can capture more patterns in data, but they require us to develop more sophisticated and generalizable notions of feature importance.Thus, many researchershaverecentlybegun turning toShapley value explanationsto summarize important features (Figure 1b), surface non-linear effects (Figure 1c), and provide individualized explanations (Figure 1d) in an axiomatic manner (Figure 2b).3 Shapley valuesFigure 2: (a) Defining terms related to the Shapley value. Players either participate or abstain from the coalitional game, and the gamemaps from any subset of participating players to a scalar value. Shapley values are a solution concept to allocatecredit to each player ina coalitional game. (b) A sufficient, but not exhaustive set of axioms that uniquely define the
5. [5]:  Passage ID 5: of Shapley values cannot be fixed, or otherwise it isunlikely that it can be computed in polynomial-time, unless of coursethat P=NPPNP\textnormal{P}=\textnormal{NP}.An alternative measure of feature importance is to enumerate all theAXp’s of an explanation problem, and then rank the features by theiroccurrence in explanations, giving more weight to the smallerexplanations. Such a measure respects feature irrelevancy inthat irrelevant features will have a score of 0.A drawback of such a measure of feature importance is that it hingeson the enumeration of all abductive explanations, and their number isworst-case exponential. However, complete (or partial) enumeration ispossible in some cases, and in those cases, the proposed measure offeature importance could be used.It is left for future work the identification of other possibledrawbacks.7 ExperimentsThe experiments are organized in two main parts. The first part,in Section 7.1, highlights the limitations of SHAP