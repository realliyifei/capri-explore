# SHAP-Based Explanation Methods: A Review for NLP Interpretability

## Question

What are the main criticisms of using Shapley values for feature importance in model explanations?

## URLs

1. https://ar5iv.org/html/2002.11097. [2002.11097] Problems with Shapley-value-based explanations as feature importance measures
2. https://ar5iv.org/html/2302.08160. [2302.08160] The Inadequacy of Shapley Values for Explainability
3. https://ar5iv.org/html/2310.00416. [2310.00416] Refutation of Shapley Values for XAI – Additional Evidence
4. https://ar5iv.org/html/2404.13522. [2404.13522] Error Analysis of Shapley Value-Based Model Explanations: An Informative Perspective
5. https://ar5iv.org/html/2303.05981. [2303.05981] Feature Importance: A Closer Look at Shapley Values and LOCO
6. https://ar5iv.org/html/2409.12578. [2409.12578] CLE-SH: Comprehensive Literal Explanation package for SHapley values by statistical validity
7. https://ar5iv.org/html/2410.06815. [2410.06815] shap-select: Lightweight Feature Selection Using SHAP Values and Regression
8. https://ar5iv.org/html/2111.11946. [2111.11946] Is Shapley Explanation for a model unique?
9. https://ar5iv.org/html/2206.08204. [2206.08204] Inherent Inconsistencies of Feature Importance

## Answer (I don't know?: ✗)

The main criticisms of using Shapley values for feature importance in model explanations can be grouped into mathematical and human-centric concerns.

Firstly, on the mathematical side, it has been asserted that applying Shapley values to the problem of feature importance introduces complexities that can misalign with our expectations from explanations. Specifically, the mathematical problems that arise include the inadequacy of Shapley values to handle the inherent complexities of non-linear models effectively. These models, such as tree ensembles and deep learning architectures, necessitate more sophisticated definitions of feature importance than Shapley values can provide [1][2] [3]. It has been noted that the solutions required to mitigate these issues can further complicate the application of Shapley values, potentially requiring causal reasoning that may exceed the original scope of their intended use [2].

Moreover, it has been found that Shapley values can assign significant importance to irrelevant features, leading to incorrect attributions that could misguide human decision-makers. This point is critical, highlighted in experiments showing that irrelevant features were assigned Shapley values that inaccurately portrayed them as crucial to predictions [4]. The discrepancy between the attributes assigned by Shapley values and the actual relevance of certain features highlights a fundamental issue with their reliability in explainability contexts.

Another substantial concern revolves around the idea that Shapley values do not fulfill the human-centric goals of explainability. The critique posits that the game-theoretic formulation of Shapley values fails to resonate with what users typically seek from explanations. Explanations should ideally aid in understanding and building trust in the model’s operations; however, Shapley values often fall short of meeting these human-centric expectations [2][3].

Lastly, there's an acknowledgment of the computational complexity of deriving exact Shapley values. Many studies approximate these values due to their high computational demands, leading to inadequately tested assumptions about their utility for explainability. The complexity involved in computing Shapley values raises concerns about their practical applicability in real-world scenarios where timely and interpretable model insights are necessary [4][5].

In conclusion, using Shapley values for feature importance poses significant mathematical challenges, potential misinformation due to the misattribution of feature relevance, and a lack of alignment with user expectations for explanation clarity and utility. These criticisms suggest that while Shapley values have been a prominent method for feature attribution in explainable machine learning, they may not be suitable for all contexts where model transparency is crucial [1][2][4].

1. [1]:  https://ar5iv.org/html/2002.11097, [2002.11097] Problems with Shapley-value-based explanations as feature importance measures
2. [2]:  https://ar5iv.org/html/2002.11097, [2002.11097] Problems with Shapley-value-based explanations as feature importance measures
3. [3]:  https://ar5iv.org/html/2207.07605, No Title
4. [4]:  https://ar5iv.org/html/2302.08160, [2302.08160] The Inadequacy of Shapley Values for Explainability
5. [5]:  https://ar5iv.org/html/2302.08160, [2302.08160] The Inadequacy of Shapley Values for Explainability
---
1. [1]:  Passage ID 1: that Shapley-value-based explanations for feature importance fail to serve their desired purpose in general. We make this argument in two parts. Firstly, we show that applying the Shapley value to the problem of feature importance introduces mathematically formalizable properties which may not align with what we would expect from an explanation. Secondly, taking a human-centric perspective, we evaluate Shapley-value-based explanations through established frameworks of what people expect from explanations, and find them wanting. We find that the game theoretic problem formulation of Shapley-value-based explanations do not match the proposed use cases for its solution, and thus caution against their usage except in narrowly constrained settings where they admit a clear interpretation.We describe the different Shapley-value-based explanation frameworks in Section 2, and present our two-part critique in Sections 3 and 4. We discuss these results and provide some suggestions in
2. [2]:  Passage ID 2: show that mathematical problems arise when Shapley values are used for feature importance, and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values are not a natural solution to the human-centric goals of explainability.SHAP, Shapley values, explainable machine learning1 IntroductionMachine learning models are increasingly being used to replace human decision-making for tasks involving some kind of prediction. As state-of-the-art predictive machine learning models become increasingly inscrutable, there has been an increase in concern that the black-box nature of these systems can obscure undesirable properties of the decision algorithm, such as illegal bias or signals accidentally learned from artifacts irrelevant to the task at hand. More recently, attempts have been made to “explain” the output of a complicated function in terms of its inputs to
3. [3]:  Passage ID 3: used today, including tree ensembles and deep learning models, their large number of operations prevents us from understanding each feature’s role by examining the model parameters. These flexible, non-linear models can capture more patterns in data, but they require us to develop more sophisticated and generalizable notions of feature importance.Thus, many researchershaverecentlybegun turning toShapley value explanationsto summarize important features (Figure 1b), surface non-linear effects (Figure 1c), and provide individualized explanations (Figure 1d) in an axiomatic manner (Figure 2b).3 Shapley valuesFigure 2: (a) Defining terms related to the Shapley value. Players either participate or abstain from the coalitional game, and the gamemaps from any subset of participating players to a scalar value. Shapley values are a solution concept to allocatecredit to each player ina coalitional game. (b) A sufficient, but not exhaustive set of axioms that uniquely define the
4. [4]:  Passage ID 4: explanation).The experiments also demonstrated that irrelevant features can beassigned Shapley values which incorrectly give those features crucialimportance to the prediction.In light of the results presented in this section, we conclude thatfeature attribution based on exact Shapley values (as well as anyother approach that correlates in practice with exact Shapley values)will almost surely mislead human decision makers in some use cases.8 ConclusionsFor more than a decade Shapley values have represented one of the mostvisible approaches for feature attribution in explainability.Almost without exception, and motivated by its computationalcomplexity [29, 8, 7, 30],existing work approximates the computation of exact Shapley values.This means that the adequacy of Shapley values for explainability hasnot been investigated with rigor.This paper demonstrates that exact Shapley values can attributeincorrect importance to features. Concretely, the paper
5. [5]:  Passage ID 5: of Shapley values cannot be fixed, or otherwise it isunlikely that it can be computed in polynomial-time, unless of coursethat P=NPPNP\textnormal{P}=\textnormal{NP}.An alternative measure of feature importance is to enumerate all theAXp’s of an explanation problem, and then rank the features by theiroccurrence in explanations, giving more weight to the smallerexplanations. Such a measure respects feature irrelevancy inthat irrelevant features will have a score of 0.A drawback of such a measure of feature importance is that it hingeson the enumeration of all abductive explanations, and their number isworst-case exponential. However, complete (or partial) enumeration ispossible in some cases, and in those cases, the proposed measure offeature importance could be used.It is left for future work the identification of other possibledrawbacks.7 ExperimentsThe experiments are organized in two main parts. The first part,in Section 7.1, highlights the limitations of SHAP