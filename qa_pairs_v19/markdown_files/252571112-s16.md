# SHAP-Based Explanation Methods: A Review for NLP Interpretability

## Question

What are the main criticisms of using Shapley values for feature importance in model explanations?

## URLs

1. https://ar5iv.org/html/2002.11097. [2002.11097] Problems with Shapley-value-based explanations as feature importance measures
2. https://ar5iv.org/html/2302.08160. [2302.08160] The Inadequacy of Shapley Values for Explainability
3. https://ar5iv.org/html/2310.00416. [2310.00416] Refutation of Shapley Values for XAI – Additional Evidence
4. https://ar5iv.org/html/2404.13522. [2404.13522] Error Analysis of Shapley Value-Based Model Explanations: An Informative Perspective
5. https://ar5iv.org/html/2303.05981. [2303.05981] Feature Importance: A Closer Look at Shapley Values and LOCO
6. https://ar5iv.org/html/2409.12578. [2409.12578] CLE-SH: Comprehensive Literal Explanation package for SHapley values by statistical validity
7. https://ar5iv.org/html/2410.06815. [2410.06815] shap-select: Lightweight Feature Selection Using SHAP Values and Regression
8. https://ar5iv.org/html/2111.11946. [2111.11946] Is Shapley Explanation for a model unique?
9. https://ar5iv.org/html/2206.08204. [2206.08204] Inherent Inconsistencies of Feature Importance

## Answer (I don't know?: ✗)

The main criticisms of using Shapley values for feature importance in model explanations revolve around their mathematical properties and their alignment with human-centric expectations of explainability.

Firstly, one significant issue is that applying Shapley values to the problem of feature importance introduces mathematical complexities that do not align with intuitive understandings of explanations. It has been argued that these mathematical problems can lead to misleading attributions of importance to features, including irrelevant features, which could incorrectly be assigned crucial roles in predictions. This misattribution occurs because exact Shapley values may give skewed insights into the contributions of features, as irrelevant features can be erroneously assigned high importance values [1][4]. The paper critiques these points and concludes that feature attribution based on exact Shapley values often misleads human decision-makers in various use cases, emphasizing the inadequacy of Shapley values for achieving meaningful interpretability in practice [4].

Moreover, there are inherent challenges in computational feasibility, as the computation of exact Shapley values is known to be complex and is unlikely to be computed in polynomial time unless major theoretical breakthroughs occur in computer science frameworks, specifically relating to P vs NP problems [5]. This complexity raises concerns about the reliability and practicality of employing Shapley values as a standard for feature importance, arguing for the need for alternative metrics that can accommodate for unwanted complexity in explanations [2][3].

From a human-centric perspective, established frameworks for evaluating explanatory power indicate that Shapley values often fall short. The theoretical formulation of Shapley values does not adequately match the expectations people have when seeking explanations for model predictions [1][2]. In essence, while Shapley values have been prominent in the field of explainable machine learning, their application does not always translate into effective explanations that resonate with users and stakeholders involved in decision-making processes.

In summary, the critical points against the use of Shapley values for feature importance include their tendency to misattribute importance to irrelevant features [4], computational inefficiencies that hinder their practical applicability [5], and a general failure to meet the human-centric goals of explainability [1][2][3]. These aspects suggest caution in using Shapley values outside narrowly defined contexts where their limitations can be adequately controlled.

1. [1]:  https://ar5iv.org/html/2002.11097, [2002.11097] Problems with Shapley-value-based explanations as feature importance measures
2. [2]:  https://ar5iv.org/html/2002.11097, [2002.11097] Problems with Shapley-value-based explanations as feature importance measures
3. [3]:  https://ar5iv.org/html/2002.11097, [2002.11097] Problems with Shapley-value-based explanations as feature importance measures
4. [4]:  https://ar5iv.org/html/2302.08160, [2302.08160] The Inadequacy of Shapley Values for Explainability
5. [5]:  https://ar5iv.org/html/2302.08160, [2302.08160] The Inadequacy of Shapley Values for Explainability
---
1. [1]:  Passage ID 1: that Shapley-value-based explanations for feature importance fail to serve their desired purpose in general. We make this argument in two parts. Firstly, we show that applying the Shapley value to the problem of feature importance introduces mathematically formalizable properties which may not align with what we would expect from an explanation. Secondly, taking a human-centric perspective, we evaluate Shapley-value-based explanations through established frameworks of what people expect from explanations, and find them wanting. We find that the game theoretic problem formulation of Shapley-value-based explanations do not match the proposed use cases for its solution, and thus caution against their usage except in narrowly constrained settings where they admit a clear interpretation.We describe the different Shapley-value-based explanation frameworks in Section 2, and present our two-part critique in Sections 3 and 4. We discuss these results and provide some suggestions in
2. [2]:  Passage ID 2: show that mathematical problems arise when Shapley values are used for feature importance, and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values are not a natural solution to the human-centric goals of explainability.SHAP, Shapley values, explainable machine learning1 IntroductionMachine learning models are increasingly being used to replace human decision-making for tasks involving some kind of prediction. As state-of-the-art predictive machine learning models become increasingly inscrutable, there has been an increase in concern that the black-box nature of these systems can obscure undesirable properties of the decision algorithm, such as illegal bias or signals accidentally learned from artifacts irrelevant to the task at hand. More recently, attempts have been made to “explain” the output of a complicated function in terms of its inputs to
3. [3]:  Passage ID 3: } else { localStorage.setItem("ar5iv_theme", "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Problems with Shapley-value-based explanations as feature importance measuresI. Elizabeth Kumar  Suresh Venkatasubramanian  Carlos Scheidegger  Sorelle A. FriedlerAbstractGame-theoretic formulations of feature importance have become popular as a way to “explain” machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game’s unique Shapley values. Justification for these methods rests on two pillars: their desirable mathematical properties, and their applicability to specific motivations for explanations. We show that mathematical problems arise when Shapley values are used for feature importance, and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal
4. [4]:  Passage ID 4: explanation).The experiments also demonstrated that irrelevant features can beassigned Shapley values which incorrectly give those features crucialimportance to the prediction.In light of the results presented in this section, we conclude thatfeature attribution based on exact Shapley values (as well as anyother approach that correlates in practice with exact Shapley values)will almost surely mislead human decision makers in some use cases.8 ConclusionsFor more than a decade Shapley values have represented one of the mostvisible approaches for feature attribution in explainability.Almost without exception, and motivated by its computationalcomplexity [29, 8, 7, 30],existing work approximates the computation of exact Shapley values.This means that the adequacy of Shapley values for explainability hasnot been investigated with rigor.This paper demonstrates that exact Shapley values can attributeincorrect importance to features. Concretely, the paper
5. [5]:  Passage ID 5: of Shapley values cannot be fixed, or otherwise it isunlikely that it can be computed in polynomial-time, unless of coursethat P=NPPNP\textnormal{P}=\textnormal{NP}.An alternative measure of feature importance is to enumerate all theAXp’s of an explanation problem, and then rank the features by theiroccurrence in explanations, giving more weight to the smallerexplanations. Such a measure respects feature irrelevancy inthat irrelevant features will have a score of 0.A drawback of such a measure of feature importance is that it hingeson the enumeration of all abductive explanations, and their number isworst-case exponential. However, complete (or partial) enumeration ispossible in some cases, and in those cases, the proposed measure offeature importance could be used.It is left for future work the identification of other possibledrawbacks.7 ExperimentsThe experiments are organized in two main parts. The first part,in Section 7.1, highlights the limitations of SHAP