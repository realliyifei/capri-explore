# The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities

## Question

How does the frequency of terms in pre-training data influence numerical reasoning performance in few-shot settings?

## URLs

1. https://ar5iv.org/html/2202.07206. [2202.07206] Impact of Pretraining Term Frequencies on Few-Shot Reasoning
2. https://ar5iv.org/html/2411.12580. [2411.12580] Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models
3. https://ar5iv.org/html/2307.02477. [2307.02477] Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks
4. https://ar5iv.org/html/2404.04125. [2404.04125] No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance
5. https://ar5iv.org/html/2311.12337. [2311.12337] Do Smaller Language Models Answer Contextualised Questions Through Memorisation Or Generalisation?
6. https://ar5iv.org/html/2304.01665. [2304.01665] Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks
7. https://ar5iv.org/html/2402.14903. [2402.14903] 1 Illustrating the dependence of frontier model arithmetic performance on tokenization. We show how using commas can enforce right-to-left (R2L) tokenization for the same addition problem. R2L tokenization leads to improved model performance on both GPT-3.5 and GPT-4 (March 2023 models), which we show is due to tokenization alignment between addends and answer through various controls and error analyses.
8. https://ar5iv.org/html/2407.14985. [2407.14985] 1 Introduction

## Answer (I don't know?: ✗)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable