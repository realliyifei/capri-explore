# The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities

## Question

How does the frequency of terms in pre-training data influence numerical reasoning performance in few-shot settings?

## URLs

1. https://ar5iv.org/html/2202.07206. [2202.07206] Impact of Pretraining Term Frequencies on Few-Shot Reasoning
2. https://ar5iv.org/html/2411.12580. [2411.12580] Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models
3. https://ar5iv.org/html/2307.02477. [2307.02477] Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks
4. https://ar5iv.org/html/2404.04125. [2404.04125] No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance
5. https://ar5iv.org/html/2311.12337. [2311.12337] Do Smaller Language Models Answer Contextualised Questions Through Memorisation Or Generalisation?
6. https://ar5iv.org/html/2304.01665. [2304.01665] Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks
7. https://ar5iv.org/html/2402.14903. [2402.14903] 1 Illustrating the dependence of frontier model arithmetic performance on tokenization. We show how using commas can enforce right-to-left (R2L) tokenization for the same addition problem. R2L tokenization leads to improved model performance on both GPT-3.5 and GPT-4 (March 2023 models), which we show is due to tokenization alignment between addends and answer through various controls and error analyses.
8. https://ar5iv.org/html/2407.14985. [2407.14985] 1 Introduction

## Answer (I don't know?: ✗)

The frequency of terms in pre-training data significantly influences numerical reasoning performance in few-shot settings, particularly in the context of pretrained language models (LMs). This influence stems from the models’ ability to generalize reasoning through exposure to various term frequencies during their training phase. 

Research has shown that the performance of LMs on numerical reasoning tasks correlates positively with the frequency of the terms appearing in the pretraining data. Specifically, Razeghi et al. indicate that they investigated how the performance on test instances relates to the frequency of terms from those instances in the pretraining data for various GPT-based models trained on the Pile dataset [1]. This correlation is critical because it suggests that LMs struggle with reasoning involving terms that were less frequently encountered during pretraining.

Furthermore, previous studies, such as those by Kassner et al. (2020) and Wei et al. (2021), performed controlled experiments to directly assess the impact of pretraining data characteristics on LMs' abilities to memorize facts and reason about them. They concluded that frequency plays a crucial role in whether a model can effectively recall or apply specific knowledge, including syntactic rules and factual information [3]. This assertion aligns with the findings that LMs exhibit varied performance levels on numerical reasoning tasks based on the occurrence rate of terms in the pretraining corpus [2] [3].

Additionally, the analysis from the research highlighted that methods, such as evaluating models while controlling for training data overlap with test instances, often fail to capture the full extent of the issue. For instance, prior evaluations that attempted to minimize the shared data scope (measured by 13-gram overlaps) still revealed substantial performance gaps based on term frequency considerations [4]. This indicates that term frequency is a substantial factor independent of simple data overlap metrics.

In summary, the frequency of terms in pretraining data appears to critically determine the efficacy of few-shot learning in numerical reasoning tasks. Models trained with frequent exposure to specific terms are more adept at performing numerical reasoning tasks involving those terms, as they have had the chance to internalize relevant patterns and relationships. Conversely, terms that were less frequently encountered tend to lead to decreased reasoning performance, highlighting a critical area for improving model training and evaluation strategies in NLP [5].

1. [1]:  https://ar5iv.org/html/2202.07206, [2202.07206] Impact of Pretraining Term Frequencies on Few-Shot Reasoning
2. [2]:  https://ar5iv.org/html/2411.12580, [2411.12580] Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models
3. [3]:  https://ar5iv.org/html/2202.07206, [2202.07206] Impact of Pretraining Term Frequencies on Few-Shot Reasoning
4. [4]:  https://ar5iv.org/html/2202.07206, [2202.07206] Impact of Pretraining Term Frequencies on Few-Shot Reasoning
5. [5]:  https://ar5iv.org/html/2404.04125, [2404.04125] No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance
---
1. [1]:  Passage ID 1: } else { localStorage.setItem("ar5iv_theme", "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Impact of Pretraining Term Frequencies on Few-Shot ReasoningYasaman Razeghi  Robert L. Logan IV  Matt Gardner  Sameer SinghAbstractPretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings.However, the extent to which this extrapolation relies on robust reasoning is unclear.In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data.In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data.We measure the strength of this correlationfor a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit
2. [2]:  Passage ID 2: Logan IV, Matt Gardner, and Sameer Singh.Impact of pretraining term frequencies on few-shot numerical reasoning.In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, pp.  840–854, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.doi: 10.18653/v1/2022.findings-emnlp.59.URL https://aclanthology.org/2022.findings-emnlp.59.Singh et al. (2024)Aaditya K Singh, Ted Moskovitz, Felix Hill, Stephanie C.Y. Chan, and Andrew M Saxe.What needs to go right for an induction head? a mechanistic study of in-context learning circuits and their formation.In Forty-first International Conference on Machine Learning, 2024.URL https://openreview.net/forum?id=O8rrXl71D5.Templeton et al. (2024)Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham,
3. [3]:  Passage ID 3: 2019; Weir et al., 2020; Lin et al., 2020), mathematical (Saxton et al., 2019), and other NLP task-related (Radford et al., 2019; Shin et al., 2020) knowledge LMs acquire during pretraining.In this work, we focus on the in-context learning setup of Brown et al. (2020), who use prompts that include training examples to diagnose LMs’ few-shot learning capabilities.Impact of Frequency on LM PerformanceKassner et al. (2020) and Wei et al. (2021) perform controlled experiments varying pretraining data to characterize the extent pretraining affects LMs’ ability to learn to memorize and reason with facts as well as learn generalizable syntax rules.In line with our results, both of these works find that frequency is a distinguishing factor in whether or not the model memorizes a particular fact or syntactic rule for a verb form.Sinha et al. (2021) further demonstrate that shuffling word order during pretraining has minimal impact on an LMs’ accuracy on downstream tasks, and,
4. [4]:  Passage ID 4: models (Spithourakis & Riedel, 2018; Wallace et al., 2019).Recently, Geva et al. (2020) and Zhou et al. (2020) have proposed training schemes to help improve LMs’ temporal and numerical reasoning capabilities.Patel et al. (2021) also showed that NLP math solvers rely on simple heuristics to answer math questions.We expect that the performance gap metric proposed in this work will be useful to better understand the impact of such schemes.6 Discussion and Future WorkIn this work, we consider how to conduct few-shot evaluations in light of the analysis with respect to the pretraining data. Prior work has attempted to control for overlap between training or pretraining data and the test instances, but as we have seen, those methods are insufficient. For example, Brown et al. (2020) measure the impact of removing instances from evaluation datasets that share 13-gram overlap with their pretraining data on GPT-3’s accuracy, and also argue that the low occurrence of exact phrases
5. [5]:  Passage ID 5: on Machine Learning (ICML), 2021.Razeghi et al. [2022a]Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh.Impact of pretraining term frequencies on few-shot numerical reasoning.In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 840–854, 2022a.Razeghi et al. [2022b]Yasaman Razeghi, Raja Sekhar Reddy Mekala, Robert L Logan Iv, Matt Gardner, and Sameer Singh.Snoopy: An online interface for exploring the effect of pretraining term frequencies on few-shot lm performance.In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 389–395, 2022b.Rombach et al. [2022]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.High-resolution image synthesis with latent diffusion models.In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.Saehoon et al. [2021]Kim Saehoon, Cho Sanghun, Kim Chiheon, Doyup Lee,