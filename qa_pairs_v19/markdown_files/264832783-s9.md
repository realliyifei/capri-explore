# The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities

## Question

How does the frequency of terms in pre-training data influence numerical reasoning performance in few-shot settings?

## URLs

1. https://ar5iv.org/html/2202.07206. [2202.07206] Impact of Pretraining Term Frequencies on Few-Shot Reasoning
2. https://ar5iv.org/html/2411.12580. [2411.12580] Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models
3. https://ar5iv.org/html/2307.02477. [2307.02477] Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks
4. https://ar5iv.org/html/2404.04125. [2404.04125] No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance
5. https://ar5iv.org/html/2311.12337. [2311.12337] Do Smaller Language Models Answer Contextualised Questions Through Memorisation Or Generalisation?
6. https://ar5iv.org/html/2304.01665. [2304.01665] Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks
7. https://ar5iv.org/html/2402.14903. [2402.14903] 1 Illustrating the dependence of frontier model arithmetic performance on tokenization. We show how using commas can enforce right-to-left (R2L) tokenization for the same addition problem. R2L tokenization leads to improved model performance on both GPT-3.5 and GPT-4 (March 2023 models), which we show is due to tokenization alignment between addends and answer through various controls and error analyses.
8. https://ar5iv.org/html/2407.14985. [2407.14985] 1 Introduction

## Answer (I don't know?: ✗)

The frequency of terms in pre-training data significantly impacts the numerical reasoning performance of language models (LMs) in few-shot settings. The research conducted by Razeghi et al. demonstrates that language models can extrapolate numerical reasoning from few examples, but this capability is closely tied to how common or rare the terms encountered in the test instances were during pre-training [1]. 

The core investigation revolves around the correlation between model performance on various numerical deduction tasks (like arithmetic) and the frequency of terms in the pre-training corpus. Higher frequencies of relevant terms are associated with enhanced model performance, indicating that frequent terms are more readily memorized and understood by LMs, which leads to better reasoning and application in few-shot scenarios [1] [5]. This is consistent with findings from previous studies by Kassner et al. (2020) and Wei et al. (2021), who found a clear relationship between term frequency and the model's ability to either memorize facts or generalize syntactic rules needed for reasoning tasks [3]. 

Moreover, the overall structure of few-shot evaluations is influenced by this term frequency; prior work has shown that controlling for the overlap between training and test instances is vital yet sometimes inadequate. For instance, Brown et al. (2020) investigated the performance of models like GPT-3 when certain common instances were excluded from the evaluation set, noting a resultant change in accuracy that reflected the significance of specific terms even when they shared minimal overlap with pre-training data [4]. 

In more practical terms, if certain numerical or contextual terms appeared less frequently in the pre-training dataset, the models exhibited a decreased ability to reason accurately with these terms when presented in few-shot tasks. This suggests that for effective training, it is crucial to ensure a diverse and representative frequency distribution of terms across various types of reasoning tasks in the pre-training phase [1] [2]. 

In summary, the influence of term frequency in pre-training directly affects how well language models can deduce numerical information in few-shot contexts, revealing a dependency on the robustness of their training data to facilitate effective reasoning capabilities.

1. [1]:  https://ar5iv.org/html/2202.07206, [2202.07206] Impact of Pretraining Term Frequencies on Few-Shot Reasoning
2. [2]:  https://ar5iv.org/html/2411.12580, [2411.12580] Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models
3. [3]:  https://ar5iv.org/html/2202.07206, [2202.07206] Impact of Pretraining Term Frequencies on Few-Shot Reasoning
4. [4]:  https://ar5iv.org/html/2202.07206, [2202.07206] Impact of Pretraining Term Frequencies on Few-Shot Reasoning
5. [5]:  https://ar5iv.org/html/2404.04125, [2404.04125] No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance
---
1. [1]:  Passage ID 1: } else { localStorage.setItem("ar5iv_theme", "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Impact of Pretraining Term Frequencies on Few-Shot ReasoningYasaman Razeghi  Robert L. Logan IV  Matt Gardner  Sameer SinghAbstractPretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings.However, the extent to which this extrapolation relies on robust reasoning is unclear.In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data.In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data.We measure the strength of this correlationfor a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit
2. [2]:  Passage ID 2: Logan IV, Matt Gardner, and Sameer Singh.Impact of pretraining term frequencies on few-shot numerical reasoning.In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, pp.  840–854, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.doi: 10.18653/v1/2022.findings-emnlp.59.URL https://aclanthology.org/2022.findings-emnlp.59.Singh et al. (2024)Aaditya K Singh, Ted Moskovitz, Felix Hill, Stephanie C.Y. Chan, and Andrew M Saxe.What needs to go right for an induction head? a mechanistic study of in-context learning circuits and their formation.In Forty-first International Conference on Machine Learning, 2024.URL https://openreview.net/forum?id=O8rrXl71D5.Templeton et al. (2024)Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham,
3. [3]:  Passage ID 3: 2019; Weir et al., 2020; Lin et al., 2020), mathematical (Saxton et al., 2019), and other NLP task-related (Radford et al., 2019; Shin et al., 2020) knowledge LMs acquire during pretraining.In this work, we focus on the in-context learning setup of Brown et al. (2020), who use prompts that include training examples to diagnose LMs’ few-shot learning capabilities.Impact of Frequency on LM PerformanceKassner et al. (2020) and Wei et al. (2021) perform controlled experiments varying pretraining data to characterize the extent pretraining affects LMs’ ability to learn to memorize and reason with facts as well as learn generalizable syntax rules.In line with our results, both of these works find that frequency is a distinguishing factor in whether or not the model memorizes a particular fact or syntactic rule for a verb form.Sinha et al. (2021) further demonstrate that shuffling word order during pretraining has minimal impact on an LMs’ accuracy on downstream tasks, and,
4. [4]:  Passage ID 4: models (Spithourakis & Riedel, 2018; Wallace et al., 2019).Recently, Geva et al. (2020) and Zhou et al. (2020) have proposed training schemes to help improve LMs’ temporal and numerical reasoning capabilities.Patel et al. (2021) also showed that NLP math solvers rely on simple heuristics to answer math questions.We expect that the performance gap metric proposed in this work will be useful to better understand the impact of such schemes.6 Discussion and Future WorkIn this work, we consider how to conduct few-shot evaluations in light of the analysis with respect to the pretraining data. Prior work has attempted to control for overlap between training or pretraining data and the test instances, but as we have seen, those methods are insufficient. For example, Brown et al. (2020) measure the impact of removing instances from evaluation datasets that share 13-gram overlap with their pretraining data on GPT-3’s accuracy, and also argue that the low occurrence of exact phrases
5. [5]:  Passage ID 5: on Machine Learning (ICML), 2021.Razeghi et al. [2022a]Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh.Impact of pretraining term frequencies on few-shot numerical reasoning.In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 840–854, 2022a.Razeghi et al. [2022b]Yasaman Razeghi, Raja Sekhar Reddy Mekala, Robert L Logan Iv, Matt Gardner, and Sameer Singh.Snoopy: An online interface for exploring the effect of pretraining term frequencies on few-shot lm performance.In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 389–395, 2022b.Rombach et al. [2022]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.High-resolution image synthesis with latent diffusion models.In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.Saehoon et al. [2021]Kim Saehoon, Cho Sanghun, Kim Chiheon, Doyup Lee,