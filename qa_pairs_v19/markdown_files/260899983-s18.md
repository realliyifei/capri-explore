# Through the Lens of Core Competency: Survey on Evaluation of Large Language Models

## Question

How is social bias assessed in large language models using datasets like CrowS-Pairs and StereoSet?

## URLs

1. https://ar5iv.org/html/2304.06861. [2304.06861] Evaluation of Social Biases in Recent Large Pre-Trained Models
2. https://ar5iv.org/html/2309.00770. [2309.00770] Bias and Fairness in Large Language Models: A Survey
3. https://ar5iv.org/html/2312.06315. [2312.06315] GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models
4. https://ar5iv.org/html/2308.10684. [2308.10684] Systematic Offensive Stereotyping (SOS) Bias in Language Models
5. https://ar5iv.org/html/2010.00133. [2010.00133] CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models
6. https://ar5iv.org/html/2311.09090. [2311.09090] Social Bias Probing: Fairness Benchmarking for Language Models
7. https://ar5iv.org/html/2312.05662. [2312.05662] Understanding the Effect of Model Compression on Social Bias in Large Language Models
8. https://ar5iv.org/html/2408.07665. [2408.07665] Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models
9. https://ar5iv.org/html/2406.13551. [2406.13551] Mitigating Social Biases in Language Models through Unlearning

## Answer (I don't know?: ✗)

Social bias in large language models is assessed using datasets like CrowS-Pairs and StereoSet through structured methodologies that involve the evaluation of model responses to constructed sentence pairs that embody stereotypes and anti-stereotypes.

CrowS-Pairs is specifically designed to assess social biases in masked language models (MLMs). It contains 1,508 examples covering nine bias types, such as race, religion, and age [3]. The dataset consists of pairs of sentences—one contains a stereotype and the other an anti-stereotype. This allows evaluators to examine how language models respond to differing representations of social groups, providing insights into the biases present in these models [3] [4]. The methodology includes crowdsourced annotations that highlight bias levels, facilitating a detailed analysis of how various pre-trained language models, like BERT, exhibit social biases [1] [4].

StereoSet, in contrast, focuses not only on the intrasentence level but also on the intersentence level. It encompasses four types of biases—gender, occupation, race, and religion, while measuring both the language modeling abilities and stereotypical biases [1] [4]. The process involves creating a context sentence featuring a biased target group, followed by potential continuations that are stereotypical, anti-stereotypical, or irrelevant. The model's performance is evaluated based on the likelihood of selecting each type of continuation, which provides a metric for assessing biases [2].

Both datasets utilize a crowdsourcing approach to ensure diverse representation and a comprehensive challenge to stereotype-based responses. This rigorous methodology enables the datasets to effectively capture a wide range of social biases, thus allowing for comparisons across different language models [4]. For example, a benchmark analysis against these datasets can reveal trends in bias reduction as new models are developed, where improvements in model architecture and training data can be correlated with observable declines in bias [1].

To summarize, the assessment of social bias in large language models via datasets like CrowS-Pairs and StereoSet employs answer generation for sentence pairs that incorporate biases, measurement of model responses to stereotypical vs. anti-stereotypical contexts, and rigorous evaluation frameworks that involve crowdsourced input. This approach not only highlights existing prejudices in models but also informs ongoing efforts to mitigate these biases as part of ethical NLP development.

1. [1]:  https://ar5iv.org/html/2304.06861, [2304.06861] Evaluation of Social Biases in Recent Large Pre-Trained Models
2. [2]:  https://ar5iv.org/html/2408.07665, [2408.07665] Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models
3. [3]:  https://ar5iv.org/html/2402.11512, No Title
4. [4]:  https://ar5iv.org/html/2304.06861, [2304.06861] Evaluation of Social Biases in Recent Large Pre-Trained Models
5. [5]:  https://ar5iv.org/html/2408.07665, [2408.07665] Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models
---
1. [1]:  Passage ID 1: Pairs, CrowS-Pairs (Nangia et al., 2020). StereoSet accounts for four different biases that are based on- gender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance.We aim to answer the question: as technology advances and newer pre-trained language models are released, are they being done responsibly such that their inherent social biases have been reduced? We believe that while making improvements to speed, size, efficiency and performance of models is at the forefront of research, equal importance needs to be given to mitigating their biases given their impact in the real world.The three newer models are run against these benchmarks and compared to the baseline of BERT. This gives us a general trend in whether bias is reduced as advancements in language models are being made. We find that all the models exhibit bias, but the three recent models perform better on the benchmarks than
2. [2]:  Passage ID 2: of performing multiple tasks without additional training. As more speech large language models have been developed, the analysis of biases within them remains unexplored. Consequently, we propose Spoken StereoSet to measure the extent of social biases in speech large language models. To our knowledge, this is the first study to focus specifically on assessing biases in speech large language models.3 Methodology3.1 MotivationInspired by the Intersentence split of Stereoset, we developed our dataset using a similar format. Stereoset evaluates bias and language modeling capabilities in discourse-level reasoning. In Stereoset, the author first creates a context sentence that includes a biased target group. Then, crowd workers write three possible continuations: one stereotypical, one anti-stereotypical, and one irrelevant. The bias and language modeling capability of language models is measured by identifying which continuation the models are most likely to choose.Speech
3. [3]:  Passage ID 3: popular models like BERT Devlin et al. (2019), GPT-2 Radford et al. (2019), RoBERTa Liu et al. (2019), and XLNet Yang et al. (2020). The dataset’s creation involved a rigorous crowdsourcing process, ensuring a wide coverage of stereotypes and anti-stereotypes to challenge and evaluate the fairness of NLP systems. In our words we use the stereoset dataset to benchmark our debiasing method against all the other debiasing methods. To accomplish this, we compute the average sentence vector for all sentences and their respective contexts. We then compare these vectors against each other to derive the stereotype score.2.3 CrowS-PairsCrowS-Pairs Nangia et al. (2020), designed to assess social biases in masked language models (MLMs), comprises 1,508 examples covering nine bias types, including race, religion, and age. It contrasts sentences related to historically disadvantaged and advantaged groups in the U.S., with annotations from crowd workers highlighting the degree of
4. [4]:  Passage ID 4: three bias benchmarks (StereoSet, CrowS-Pairs and Sentence Encoder Association Test (May et al., 2019). They report which technique is best at debiasing and whether these techniques hamper a model’s language modeling ability and performance on downstream Natural Language Understanding tasks.Crowdsourced Stereotype Pairs, CrowS-Pairs (Nangia et al., 2020) is a crowdsourced bias evaluation dataset that deals with nine different types of social bias. Each sample consists of a pair of sentences- a stereotype and an anti-stereotype. This high quality dataset provides us a good way to gain insights on varying types of bias in recent models.An earlier bias evaluation benchmark StereoSet (Nadeem et al., 2021) deals with four types of biases but on a larger scale. They introduce the Context Association Test that "measures the language modeling ability as well as the stereotypical bias of pretrained language models". This work provides us with a large amount of data, though it does not
5. [5]:  Passage ID 5: the existence of social bias in pre-trained models within the natural language processing (NLP) domain. Bolukbasi et al. [15] demonstrated the presence of gender stereotypes in word embeddings. Building on these findings, Manzini et al. [16] revealed that word embeddings also exhibit social biases related to race and religion. Subsequently, May et al. [17] extended this research to measure biases in sentence encoders, such as ELMo and BERT, thereby exploring biases at the sentence level. Later, Nangia et al. [18] introduced the CrowS-Pairs dataset to assess a wide range of social biases in masked language models at the intrasentence level. Concurrently, StereoSet [19] was developed to measure biases at both intrasentence and intersentence levels. The BBQ [20] dataset was later constructed in a question-answering (QA) format to investigate the manifestation of social biases in the QA outputs of pre-trained language models. Nonetheless, these studies predominantly focus on bias analysis