# Through the Lens of Core Competency: Survey on Evaluation of Large Language Models

## Question

How is social bias assessed in large language models using datasets like CrowS-Pairs and StereoSet?

## URLs

1. https://ar5iv.org/html/2304.06861
2. https://ar5iv.org/html/2309.00770
3. https://ar5iv.org/html/2312.06315
4. https://ar5iv.org/html/2308.10684
5. https://ar5iv.org/html/2010.00133
6. https://ar5iv.org/html/2311.09090
7. https://ar5iv.org/html/2312.05662
8. https://ar5iv.org/html/2408.07665
9. https://ar5iv.org/html/2406.13551

## Answer

## I don't know?: False

Social bias in large language models is assessed using datasets like CrowS-Pairs and StereoSet through systematic methodologies that involve contrasting biased and non-biased sentence pairs, allowing researchers to evaluate the models' tendencies towards stereotype reinforcement. 

CrowS-Pairs, developed by Nangia et al. (2020), serves as a focused evaluation dataset that targets nine types of social bias, including race, religion, and age. Each entry in CrowS-Pairs includes a stereotype and its counterpoint, the anti-stereotype, enabling a direct comparison [3]. This contrast allows researchers to categorize and measure how language models react to these pairs, providing insights into the extent of bias those models carry [4]. The dataset is built on a rigorous crowdsourcing process, ensuring that a diverse range of biases is represented accurately, thus contributing to an extensive evaluation framework [3]. 

StereoSet, on the other hand, is designed to evaluate biases at both intrasentence and intersentence levels. It broadly analyzes four types of biases while examining the language modeling capabilities of models [1], [4]. In this evaluation framework, the method involves creating context sentences that include biased target groups. Crowd workers are then tasked with providing three possible continuations for each context: one that is stereotypical, one that is anti-stereotypical, and one that is irrelevant. The model's bias is assessed based on the likelihood of its selection from these continuations, thereby measuring both its bias and its capacity for language understanding [2].

Through both of these methodologies, researchers can derive stereotype scores by comparing the models’ responses across different scenarios. This approach illuminates the inherent biases and evaluates improvements in debiasing techniques implemented in newer language models [1][3][4]. 

In summary, datasets like CrowS-Pairs and StereoSet play vital roles in not only identifying biases inherent within large language models but also in benchmarking the effectiveness of debiasing efforts. The systematic contrasting of stereotypical and anti-stereotypical pairs, along with the crowd-sourced creation of biased contexts, serves as the foundation for accurate and meaningful evaluations of social bias within NLP models [1][3][4].

1. [1]:  https://ar5iv.org/html/2304.06861, [2304.06861] Evaluation of Social Biases in Recent Large Pre-Trained Models
2. [2]:  https://ar5iv.org/html/2408.07665, [2408.07665] Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models
3. [3]:  https://ar5iv.org/html/2402.11512, No Title
4. [4]:  https://ar5iv.org/html/2304.06861, [2304.06861] Evaluation of Social Biases in Recent Large Pre-Trained Models
5. [5]:  https://ar5iv.org/html/2408.07665, [2408.07665] Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models
---
1. [1]:  Passage ID 1: Pairs, CrowS-Pairs (Nangia et al., 2020). StereoSet accounts for four different biases that are based on- gender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance.We aim to answer the question: as technology advances and newer pre-trained language models are released, are they being done responsibly such that their inherent social biases have been reduced? We believe that while making improvements to speed, size, efficiency and performance of models is at the forefront of research, equal importance needs to be given to mitigating their biases given their impact in the real world.The three newer models are run against these benchmarks and compared to the baseline of BERT. This gives us a general trend in whether bias is reduced as advancements in language models are being made. We find that all the models exhibit bias, but the three recent models perform better on the benchmarks than
2. [2]:  Passage ID 2: of performing multiple tasks without additional training. As more speech large language models have been developed, the analysis of biases within them remains unexplored. Consequently, we propose Spoken StereoSet to measure the extent of social biases in speech large language models. To our knowledge, this is the first study to focus specifically on assessing biases in speech large language models.3 Methodology3.1 MotivationInspired by the Intersentence split of Stereoset, we developed our dataset using a similar format. Stereoset evaluates bias and language modeling capabilities in discourse-level reasoning. In Stereoset, the author first creates a context sentence that includes a biased target group. Then, crowd workers write three possible continuations: one stereotypical, one anti-stereotypical, and one irrelevant. The bias and language modeling capability of language models is measured by identifying which continuation the models are most likely to choose.Speech
3. [3]:  Passage ID 3: popular models like BERT Devlin et al. (2019), GPT-2 Radford et al. (2019), RoBERTa Liu et al. (2019), and XLNet Yang et al. (2020). The dataset’s creation involved a rigorous crowdsourcing process, ensuring a wide coverage of stereotypes and anti-stereotypes to challenge and evaluate the fairness of NLP systems. In our words we use the stereoset dataset to benchmark our debiasing method against all the other debiasing methods. To accomplish this, we compute the average sentence vector for all sentences and their respective contexts. We then compare these vectors against each other to derive the stereotype score.2.3 CrowS-PairsCrowS-Pairs Nangia et al. (2020), designed to assess social biases in masked language models (MLMs), comprises 1,508 examples covering nine bias types, including race, religion, and age. It contrasts sentences related to historically disadvantaged and advantaged groups in the U.S., with annotations from crowd workers highlighting the degree of
4. [4]:  Passage ID 4: three bias benchmarks (StereoSet, CrowS-Pairs and Sentence Encoder Association Test (May et al., 2019). They report which technique is best at debiasing and whether these techniques hamper a model’s language modeling ability and performance on downstream Natural Language Understanding tasks.Crowdsourced Stereotype Pairs, CrowS-Pairs (Nangia et al., 2020) is a crowdsourced bias evaluation dataset that deals with nine different types of social bias. Each sample consists of a pair of sentences- a stereotype and an anti-stereotype. This high quality dataset provides us a good way to gain insights on varying types of bias in recent models.An earlier bias evaluation benchmark StereoSet (Nadeem et al., 2021) deals with four types of biases but on a larger scale. They introduce the Context Association Test that "measures the language modeling ability as well as the stereotypical bias of pretrained language models". This work provides us with a large amount of data, though it does not
5. [5]:  Passage ID 5: the existence of social bias in pre-trained models within the natural language processing (NLP) domain. Bolukbasi et al. [15] demonstrated the presence of gender stereotypes in word embeddings. Building on these findings, Manzini et al. [16] revealed that word embeddings also exhibit social biases related to race and religion. Subsequently, May et al. [17] extended this research to measure biases in sentence encoders, such as ELMo and BERT, thereby exploring biases at the sentence level. Later, Nangia et al. [18] introduced the CrowS-Pairs dataset to assess a wide range of social biases in masked language models at the intrasentence level. Concurrently, StereoSet [19] was developed to measure biases at both intrasentence and intersentence levels. The BBQ [20] dataset was later constructed in a question-answering (QA) format to investigate the manifestation of social biases in the QA outputs of pre-trained language models. Nonetheless, these studies predominantly focus on bias analysis