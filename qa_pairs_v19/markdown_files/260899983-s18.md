# Through the Lens of Core Competency: Survey on Evaluation of Large Language Models

## Question

How is social bias assessed in large language models using datasets like CrowS-Pairs and StereoSet?

## URLs

1. https://ar5iv.org/html/2304.06861. [2304.06861] Evaluation of Social Biases in Recent Large Pre-Trained Models
2. https://ar5iv.org/html/2309.00770. [2309.00770] Bias and Fairness in Large Language Models: A Survey
3. https://ar5iv.org/html/2312.06315. [2312.06315] GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models
4. https://ar5iv.org/html/2308.10684. [2308.10684] Systematic Offensive Stereotyping (SOS) Bias in Language Models
5. https://ar5iv.org/html/2010.00133. [2010.00133] CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models
6. https://ar5iv.org/html/2311.09090. [2311.09090] Social Bias Probing: Fairness Benchmarking for Language Models
7. https://ar5iv.org/html/2312.05662. [2312.05662] Understanding the Effect of Model Compression on Social Bias in Large Language Models
8. https://ar5iv.org/html/2408.07665. [2408.07665] Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models
9. https://ar5iv.org/html/2406.13551. [2406.13551] Mitigating Social Biases in Language Models through Unlearning

## Answer (I don't know?: ✗)

Social bias in large language models is assessed using datasets like CrowS-Pairs and StereoSet through carefully designed methodologies that evaluate the extent to which these models reproduce or perpetuate stereotypes across various social categories.

CrowS-Pairs consists of a collection of paired sentences, each presenting a stereotype and its corresponding anti-stereotype. This dataset specifically targets nine different types of social bias, allowing researchers to gain insights into the biases present in recent models. The assessment process involves measuring how likely a language model is to select the stereotypical continuation over the anti-stereotypical one when given a context sentence that incorporates a biased target group. This methodology provides a clear indicator of the models' propensity to reflect societal stereotypes in their outputs [1][3].

On the other hand, StereoSet is designed to evaluate biases at both intrasentence and intersentence levels, broadening the scope of bias assessment. Similar to CrowS-Pairs, StereoSet also involves creating context sentences with a biased target group, followed by crowd-generated continuations: a stereotypical, an anti-stereotypical, and an irrelevant one. The model's performance is measured based on its ability to appropriately select among these options, thereby assessing not just the bias but also the language modeling capabilities in contextually complex situations [1][2][4]. 

Both datasets serve as benchmarks for evaluating the degree of bias in pre-trained language models. They facilitate comparative analysis between newer models and older ones, such as BERT, to determine whether advancements in technology also lead to a reduction in social biases. Findings from recent studies indicate that while newer models still exhibit bias, they generally perform better on these benchmarks compared to older models, suggesting an evolving understanding and mitigation of such biases in the development of language models [1][5].

Moreover, the existence of social biases is further illustrated through their manifestation in real-world scenarios, where these biases can lead to significant errors in classification tasks or harmful content generation associated with sensitive identities. Thus, the inclusion of thorough bias evaluation mechanisms in the model training process is crucial, as biases embedded in training datasets can propagate through to downstream applications [3][5].

In conclusion, social bias assessment in large language models using datasets like CrowS-Pairs and StereoSet involves a combination of targeted sentence-pair evaluations and comparative performance analysis, ultimately aiming to enhance the fairness and reliability of NLP applications while acknowledging the complexities of bias in language processing.

1. [1]:  https://ar5iv.org/html/2304.06861, [2304.06861] Evaluation of Social Biases in Recent Large Pre-Trained Models
2. [2]:  https://ar5iv.org/html/2408.07665, [2408.07665] Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models
3. [3]:  https://ar5iv.org/html/2304.06861, [2304.06861] Evaluation of Social Biases in Recent Large Pre-Trained Models
4. [4]:  https://ar5iv.org/html/2408.07665, [2408.07665] Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models
5. [5]:  https://ar5iv.org/html/2311.09090, [2311.09090] Social Bias Probing: Fairness Benchmarking for Language Models
---
1. [1]:  Passage ID 1: Pairs, CrowS-Pairs (Nangia et al., 2020). StereoSet accounts for four different biases that are based on- gender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance.We aim to answer the question: as technology advances and newer pre-trained language models are released, are they being done responsibly such that their inherent social biases have been reduced? We believe that while making improvements to speed, size, efficiency and performance of models is at the forefront of research, equal importance needs to be given to mitigating their biases given their impact in the real world.The three newer models are run against these benchmarks and compared to the baseline of BERT. This gives us a general trend in whether bias is reduced as advancements in language models are being made. We find that all the models exhibit bias, but the three recent models perform better on the benchmarks than
2. [2]:  Passage ID 2: of performing multiple tasks without additional training. As more speech large language models have been developed, the analysis of biases within them remains unexplored. Consequently, we propose Spoken StereoSet to measure the extent of social biases in speech large language models. To our knowledge, this is the first study to focus specifically on assessing biases in speech large language models.3 Methodology3.1 MotivationInspired by the Intersentence split of Stereoset, we developed our dataset using a similar format. Stereoset evaluates bias and language modeling capabilities in discourse-level reasoning. In Stereoset, the author first creates a context sentence that includes a biased target group. Then, crowd workers write three possible continuations: one stereotypical, one anti-stereotypical, and one irrelevant. The bias and language modeling capability of language models is measured by identifying which continuation the models are most likely to choose.Speech
3. [3]:  Passage ID 3: three bias benchmarks (StereoSet, CrowS-Pairs and Sentence Encoder Association Test (May et al., 2019). They report which technique is best at debiasing and whether these techniques hamper a model’s language modeling ability and performance on downstream Natural Language Understanding tasks.Crowdsourced Stereotype Pairs, CrowS-Pairs (Nangia et al., 2020) is a crowdsourced bias evaluation dataset that deals with nine different types of social bias. Each sample consists of a pair of sentences- a stereotype and an anti-stereotype. This high quality dataset provides us a good way to gain insights on varying types of bias in recent models.An earlier bias evaluation benchmark StereoSet (Nadeem et al., 2021) deals with four types of biases but on a larger scale. They introduce the Context Association Test that "measures the language modeling ability as well as the stereotypical bias of pretrained language models". This work provides us with a large amount of data, though it does not
4. [4]:  Passage ID 4: the existence of social bias in pre-trained models within the natural language processing (NLP) domain. Bolukbasi et al. [15] demonstrated the presence of gender stereotypes in word embeddings. Building on these findings, Manzini et al. [16] revealed that word embeddings also exhibit social biases related to race and religion. Subsequently, May et al. [17] extended this research to measure biases in sentence encoders, such as ELMo and BERT, thereby exploring biases at the sentence level. Later, Nangia et al. [18] introduced the CrowS-Pairs dataset to assess a wide range of social biases in masked language models at the intrasentence level. Concurrently, StereoSet [19] was developed to measure biases at both intrasentence and intersentence levels. The BBQ [20] dataset was later constructed in a question-answering (QA) format to investigate the manifestation of social biases in the QA outputs of pre-trained language models. Nonetheless, these studies predominantly focus on bias analysis
5. [5]:  Passage ID 5: through language. Social biases are featured in training datasets and propagated in downstream NLP applications, where it becomes evident when the model exhibits significant errors in classification settings for specific minorities or generates harmful content when prompted with sensitive identities (Nozza et al., 2021).Fairness Datasets and ScoresRecent work Blodgett et al. (2021) has pointed out relevant concerns regarding stereotype framing and data reliability of benchmark collections explicitly designed to analyze biases in language models, such as CrowS-Pairs Nangia et al. (2020) and StereoSet Nadeem et al. (2021). Consequently, the effectiveness and soundness of the resulting fairness auditing is partly comprised.The scores proposed in the contributions presenting the datasets are highly dependent on the form of the resource they propose and therefore they are hardly generalizable to other datasets to conduct a more general comparative analysis. Specifically, Nangia et al.