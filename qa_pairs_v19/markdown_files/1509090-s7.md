# Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

What is MWE discovery, and how does it differ from MWE identification methods?

## URLs

1. https://ar5iv.org/html/2403.02009. [2403.02009] Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic?
2. https://ar5iv.org/html/2211.05201. [2211.05201] HilMeMe: A Human-in-the-Loop Machine Translation Evaluation Metric Looking into Multi-Word Expressions
3. https://ar5iv.org/html/2208.07832. [2208.07832] BERT(s) to Detect Multiword Expressions
4. https://ar5iv.org/html/2110.10064. [2110.10064] Idiomatic Expression Identification using Semantic Compatibility
5. https://ar5iv.org/html/2007.11381. [2007.11381] To Be or Not To Be a Verbal Multiword Expression: A Quest for Discriminating Features
6. https://ar5iv.org/html/2110.01295. [2110.01295] SPAR.txt, a cheap Shallow Parsing approach for Regulatory texts
7. https://ar5iv.org/html/2303.06623. [2303.06623] MWE as WSD: Solving Multiword Expression Identification with Word Sense Disambiguation
8. https://ar5iv.org/html/2306.10419. [2306.10419] Untitled Document
9. https://ar5iv.org/html/2011.03783. [2011.03783] AlphaMWE: Construction of Multilingual Parallel Corpora with MWE Annotations

## Answer (I don't know?: ✗)

Multiword expressions (MWEs) refer to combinations of words that function as single units in language, such as idioms or phrasal verbs. The process of handling these expressions requires two main subtasks: MWE discovery and MWE identification. 

MWE discovery focuses on identifying new multiword expressions from text. This involves exploring texts to find instances of MWEs that may not have been previously recognized or documented [4]. The goal is to expand the lexicon of MWEs available for computational processing by discovering novel combinations that fulfill the criteria of being non-compositional and having specific meanings.

On the other hand, MWE identification involves the automatic annotation of known multiword expressions within a body of text. This subtask is concerned with recognizing and marking existing MWEs in running text, associating them with established types of MWEs [4]. MWE identification typically employs various techniques, including machine learning approaches, rule-based systems, or leveraging lexicons and annotated datasets. It essentially interprets text by identifying which segments correspond to recognized MWEs.

In summary, the key difference lies in their objectives: MWE discovery is proactive, seeking to find and categorize new units from text, while MWE identification is reactive, focusing on the detection and annotation of known MWEs within a text [4]. Both tasks are crucial for advancing the understanding and processing of language in natural language processing applications, but they require different methodologies and data handling techniques.

1. [1]:  https://ar5iv.org/html/2208.07832, [2208.07832] BERT(s) to Detect Multiword Expressions
2. [2]:  https://ar5iv.org/html/2303.06623, [2303.06623] MWE as WSD: Solving Multiword Expression Identification with Word Sense Disambiguation
3. [3]:  https://ar5iv.org/html/2208.07832, [2208.07832] BERT(s) to Detect Multiword Expressions
4. [4]:  https://ar5iv.org/html/2403.02009, [2403.02009] Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic?
5. [5]:  https://ar5iv.org/html/2208.07832, [2208.07832] BERT(s) to Detect Multiword Expressions
---
1. [1]:  Passage ID 1: (MT) [17, 16], which depends on a clear distinction between word tokens and phrases, has to be re-thought to accommodate MWEs [8, 29]. The usual approach in these applications is to identify MWEs first, and then treat them accordingly. Therefore, detecting MWEs is a key research area in NLP.In recent years, the identification of MWEs has been modelled as a supervised machine learning task where the machine learning models are trained on an annotated dataset. As we explain in Section 2, several datasets have been released to train these machine learning models. Furthermore shared tasks such as SemEval-2016 Task 10 [28] and PARSEME [27] have contributed to develop datasets. In recent years, neural network-based models, and in particular architectures incorporating RecurrentNeural Networks (RNNs) such as Long Short Term Memory (LSTM) and Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in MWE identification tasks [27]. Usually, these models utilise
2. [2]:  Passage ID 2: 2019).The task of MWE identification is locating these MWEs in a given body of text. Common approaches to solving MWE identification include rule-based systems (Foufi et al., 2017; Pasquer et al., 2020), CRF-based systems Liu et al. (2021), and token tagging systems (Rohanian et al., 2019). Rule-based systems remain competitive with neural models in this task, and many systems including ours use MWE lexicons in order to identify MWEs, which Savary et al. (2019) argue are critical to making progress in MWE identification. Kurfalı and Östling (2020) and Kanclerz and Piasecki (2022) are similar to our work in that they frame the task of MWE identification as a classification problem, although neither use gloss information.Among all the types of MWEs, verbal MWEs are particularly difficult to identify due to their surface variability — constituents can be conjugated or separated so that they become discontinuous (Pasquer et al., 2020). Much work on verbal MWE identification,
3. [3]:  Passage ID 3: (RNNs) such as Long Short Term Memory (LSTM) and Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in MWE identification tasks [27]. Usually, these models utilise pre-trained word embedding models such as word2vec [15] and glove [22]. We describe these models in Section 2. However, these traditional word embeddings provide the same embedding for polysemous words [21] [20]. Therefore, non-substitutability and the ambiguous nature of the MWEs can cause complications with traditional word embeddings.A possible solution is to utilise neural architectures such as transformers that incorporate context more into the learning process. However, as far as we know, there has not been any research done to compare the performance of different transformer models in the MWE identification task. In this research, we empirically evaluate several transformer models in detecting MWEs to fill this gap. The findings of this research can be beneficial for many NLP
4. [4]:  Passage ID 4: should be predicted.Constant et al. (2017) did a detailed survey on multiword expression (MWE) processing by dividing it into two subtasks: MWE (type) discovery and MWE (token) identification. The MWE (type) discovery subtask is focused on identifying new MWEs from text and the MWE (token) identification subtask involves automatically annotating multiword expressions in running text by associating them with known multiword expression types. Our general idiom token identification task has aspects of both of these tasks in it. On the one hand, we are interested in identifying whether a particular piece of text contains a non-compositional usage from a given category of MWEs. So from this perspective, our task is similar to MWE (token) identification in that we are annotating text, although in our case the annotation is a binary label applied to the entire sentence rather than an annotation at the token level. However, because the models we train are in principle able to identify new
5. [5]:  Passage ID 5: [25].The most popular method to detect MWEs are based on recurrent neural network variants such as LSTMs and gated recurrent units (GRUs) [25]. [18] use a LSTM model with Conditional random field (CRF) to detect MWEs. Furthermore, they incorporate dependency parse information to improve the results. Graph convolutional neural networks (GCNs) [13] have also been applied to MWE identification. [25] incorporate multi-head self-attention to improve the performance of GCN in MWE detection. Transformers have also been used to detect MWEs[5],[12]; however, the research has been limited to a few transformer models. Therefore, in this research, we fill this gap by empirically evaluating multiple transformers in the task of MWE identification.3 DataThe dataset we used was from the 2016 SemEval shared task 10333SemEval 2016 shared task description: http://dimsum16.github.io/ [28]. The shared task was designed to predict both minimal semantic units and semantic classes (supersenses). The