# English Machine Reading Comprehension Datasets: A Survey

## Question

How do evaluation metrics vary for extractive, generative, and cloze machine reading comprehension tasks?

## URLs

1. https://ar5iv.org/html/2405.18682. [2405.18682] Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical Machine Reading Comprehension
2. https://ar5iv.org/html/2402.01512. [2402.01512] Distractor Generation for Multiple-Choice Questions: A Survey of Methods, Datasets, and Evaluation
3. https://ar5iv.org/html/2109.12264. [2109.12264] More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering
4. https://ar5iv.org/html/2410.04266. [2410.04266] Constructing Cloze Questions Generatively
5. https://ar5iv.org/html/2404.19316. [2404.19316] QLSC: A Query Latent Semantic Calibrator for Robust Extractive Question Answering † Equal contribution. ✉ Corresponding author: Xulong Zhang (zhangxulong@ieee.org).
6. https://ar5iv.org/html/2411.05338. [2411.05338] SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers
7. https://ar5iv.org/html/2310.19736. [2310.19736] Evaluating Large Language Models: A Comprehensive Survey
8. https://ar5iv.org/html/2001.01582. [2001.01582] A Survey on Machine Reading Comprehension Systems
9. https://ar5iv.org/html/1907.01118. [1907.01118] Neural Machine Reading Comprehension: Methods and Trends
10. https://ar5iv.org/html/2101.00178. [2101.00178] UnitedQA: A Hybrid Approach for Open Domain Question Answering

## Answer (I don't know?: ✓)

I don't know. The provided context does not offer specific details regarding how evaluation metrics differ among extractive, generative, and cloze machine reading comprehension tasks. While some information on the general evaluation of NLP models is given, including metrics like BLEU, ROUGE, and METEOR, as well as discussions about the complexity of evaluating large language models (LLMs) [1][2], there is no explicit comparison or analysis of how these metrics are applied or vary specifically for the three types of tasks mentioned (extractive, generative, and cloze). The context primarily focuses on the challenges of evaluating model-generated text without delving into differentiated methodologies for specific task types [1][2][3][4][5]. Therefore, I cannot provide a precise answer to your question based on the information given.

1. [1]:  https://ar5iv.org/html/2408.09235, No Title
2. [2]:  https://ar5iv.org/html/2409.16202, No Title
3. [3]:  https://ar5iv.org/html/2209.12617, No Title
4. [4]:  https://ar5iv.org/html/2209.12617, No Title
5. [5]:  https://ar5iv.org/html/2209.12617, No Title
---
1. [1]:  Passage ID 1: Language Processing (NLP) forward. With their widespread applications, the need for reliable evaluation methods has become increasingly critical. Such evaluations are essential to ensure these models meet quality standards, align with human expectations, and maintain safety and reliability in various applications (Chang et al., 2024).Conventional automated metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) have long been employed to evaluate the performance of model generated text. However, these metrics primarily focus on surface-form similarity and often fail to account for semantically equivalent lexical and compositional diversity (Zhu et al., 2023; Chen et al., 2021; Zhang et al., 2020). Moreover, automated metrics struggle in evaluating open-ended generation or free-form text, where a wide range of acceptable outputs exists. This limitation becomes particularly evident when assessing instruction-tuned chat models, which tend
2. [2]:  Passage ID 2: human-like text, answer complex questions, and perform a wide array of natural language processing (NLP) tasks. Models such as GPT (Achiam et al., 2023), LLaMA (Touvron et al., 2023), Baichuan (Yang et al., 2023), Qwen (Bai et al., 2023), GLM (Du et al., 2022), and DeepSeek-V2 (DeepSeek-AI, 2024) have set new standards, demonstrating exceptional performance across various benchmarks. These models have been widely adopted in both academic and industrial settings, highlighting their versatility and potential. However, the evaluation of LLMs is a multifaceted challenge that extends beyond mere accuracy in answering questions. It involves assessing their ability to understand context, generate coherent and relevant responses, and apply specialized knowledge effectively.Evaluation BenchmarksThe evaluation of LLMs has traditionally relied on established NLP benchmark datasets, such as SQuAD (Rajpurkar et al., 2016), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019), which focus
3. [3]:  Passage ID 3: Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence (AI) concerned with text understanding and text generation. The former subject is studied in the sub-branch natural language understanding (NLU) 1; 2; 3 and the latter in natural language generation (NLG) 4; 5. Over the years, both fields, i.e., NLU and NLG developed enormously with an extensive literature which requires nowadays a dedicated discussion of specialized subtasks when presenting approaches or methods thereof despite the fact that a systems understanding of NLP can only be achieved holistically.In this paper, we focus on subtasks of NLP centered around question answering (QA). The task of a QA system is to find an answer (output) in the form of a natural language for a given question (input) usually presented in form of a sentence. While our focus is on QA systems and their
4. [4]:  Passage ID 4: Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence (AI) concerned with text understanding and text generation. The former subject is studied in the sub-branch natural language understanding (NLU) 1; 2; 3 and the latter in natural language generation (NLG) 4; 5. Over the years, both fields, i.e., NLU and NLG developed enormously with an extensive literature which requires nowadays a dedicated discussion of specialized subtasks when presenting approaches or methods thereof despite the fact that a systems understanding of NLP can only be achieved holistically.In this paper, we focus on subtasks of NLP centered around question answering (QA). The task of a QA system is to find an answer (output) in the form of a natural language for a given question (input) usually presented in form of a sentence. While our focus is on QA systems and their
5. [5]:  Passage ID 5: Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence (AI) concerned with text understanding and text generation. The former subject is studied in the sub-branch natural language understanding (NLU) 1; 2; 3 and the latter in natural language generation (NLG) 4; 5. Over the years, both fields, i.e., NLU and NLG developed enormously with an extensive literature which requires nowadays a dedicated discussion of specialized subtasks when presenting approaches or methods thereof despite the fact that a systems understanding of NLP can only be achieved holistically.In this paper, we focus on subtasks of NLP centered around question answering (QA). The task of a QA system is to find an answer (output) in the form of a natural language for a given question (input) usually presented in form of a sentence. While our focus is on QA systems and their