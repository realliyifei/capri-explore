# English Machine Reading Comprehension Datasets: A Survey

## Question

How do evaluation metrics vary for extractive, generative, and cloze machine reading comprehension tasks?

## URLs

1. https://ar5iv.org/html/2405.18682. [2405.18682] Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical Machine Reading Comprehension
2. https://ar5iv.org/html/2402.01512. [2402.01512] Distractor Generation for Multiple-Choice Questions: A Survey of Methods, Datasets, and Evaluation
3. https://ar5iv.org/html/2109.12264. [2109.12264] More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering
4. https://ar5iv.org/html/2410.04266. [2410.04266] Constructing Cloze Questions Generatively
5. https://ar5iv.org/html/2404.19316. [2404.19316] QLSC: A Query Latent Semantic Calibrator for Robust Extractive Question Answering † Equal contribution. ✉ Corresponding author: Xulong Zhang (zhangxulong@ieee.org).
6. https://ar5iv.org/html/2411.05338. [2411.05338] SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers
7. https://ar5iv.org/html/2310.19736. [2310.19736] Evaluating Large Language Models: A Comprehensive Survey
8. https://ar5iv.org/html/2001.01582. [2001.01582] A Survey on Machine Reading Comprehension Systems
9. https://ar5iv.org/html/1907.01118. [1907.01118] Neural Machine Reading Comprehension: Methods and Trends
10. https://ar5iv.org/html/2101.00178. [2101.00178] UnitedQA: A Hybrid Approach for Open Domain Question Answering

## Answer (I don't know?: ✗)

Evaluation metrics for extractive, generative, and cloze machine reading comprehension (MRC) tasks vary significantly based on the nature of the tasks and the output being evaluated.

For **extractive tasks**, where the answers are expected to be spans taken directly from a given text, the most common evaluation metrics include accuracy, Exact Match (EM), and F1 score. Accuracy reflects the proportion of correctly answered questions against ground-truth answers, while EM measures whether an output exactly matches one of the expected spans. The F1 score, which is the harmonic mean of precision and recall, is particularly useful for assessing performance when there are multiple correct spans, as it provides a more nuanced view of how well the model extracts information from the text [5].

In contrast, **generative tasks** require models to produce new, unbounded answers, which often necessitates different evaluation metrics. For generative MRC, BLEU and ROUGE are widely used, as they support a broader range of possible answers. BLEU measures the overlap of n-grams between the generated answer and the reference answer, while ROUGE assesses recall by comparing the generated output to the reference answers through the same n-gram comparison but emphasizes the coverage of the generated content [5]. Additionally, evaluation for generative answers often incorporates metrics like BERT-based cosine similarity, which measures the semantic similarity between the generated and the reference answers, capturing the quality of responses beyond mere lexical similarity [3].

**Cloze tasks**, which often involve fill-in-the-blank formats, primarily rely on accuracy and apply metrics traditionally used in multiple-choice tasks as well. Accuracy here indicates how many of the blanks were correctly filled based on a provided context. Metric assessments may also include reliability and plausibility evaluations, which check the logical consistency of the generated words or phrases in filling the gaps in a sentence [3] [5].

Overall, the choice of evaluation metrics reflects the fundamental differences in output types across these MRC tasks. Extractive tasks emphasize precision in selecting text spans, generative tasks value creative and contextually relevant outputs, and cloze tasks focus on accurate completions, often intersecting with features of both extractive and generative evaluations [1][4]. This differentiated approach helps provide a comprehensive view of a model's performance across different reading comprehension challenges.

1. [1]:  https://ar5iv.org/html/2310.19736, [2310.19736] Evaluating Large Language Models: A Comprehensive Survey
2. [2]:  https://ar5iv.org/html/2411.05338, [2411.05338] SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers
3. [3]:  https://ar5iv.org/html/2402.01512, [2402.01512] Distractor Generation for Multiple-Choice Questions: A Survey of Methods, Datasets, and Evaluation
4. [4]:  https://ar5iv.org/html/2310.19736, [2310.19736] Evaluating Large Language Models: A Comprehensive Survey
5. [5]:  https://ar5iv.org/html/1907.01118, [1907.01118] Neural Machine Reading Comprehension: Methods and Trends
---
1. [1]:  Passage ID 1: languages.Except for AGIEval (Zhong et al., 2023), which incorporates fill-in-the-blank questions, all the aforementioned benchmarks primarily rely on multiple-choice questions as their main evaluation format, with accuracy serving as the key performance metric. Consequently, these benchmarks tend to overlook the inclusion of open-ended questions.In contrast, LucyEval (Zeng et al., 2023b) pioneers a more diverse evaluation approach by introducing three categories of subjective questions: conceptual explanations, short answer questions, and computational questions. Additionally, LucyEval (Zeng et al., 2023b) introduces a novel evaluation metric known as GScore. For the assessment of short answer questions and conceptual explanations, GScore aggregates a variety of metrics, including BLEU-4, ROUGE-2, ChrF, and Semantic Similarity, through a weighted combination. This holistic approach offers a relatively comprehensive yet straightforward means of evaluating subjective
2. [2]:  Passage ID 2: dataset across instance categories. The average score (R-1, R-2, R-L, BL, BS) of human and GPT-4 generated answers are grouped by preference.Prompt for LLM JudgeYou are an expert evaluator tasked with assessing the quality of a model-generated answer compared to a gold standard correct answer in a long-form question-answering context. Your goal is to provide a quantified evaluation across multiple dimensions. Please follow these steps:Carefully read the original question, the model-generated answer, and the gold correct answer. Evaluate the model-generated answer on the following dimensions, providing a score from 1-10 for each (where 1 is poor and 10 is excellent): a) Relevance (1-10): How well does the answer address the specific question asked? b) Accuracy (1-10): To what extent is the information provided correct and aligned with the gold answer? c) Completeness (1-10): How thoroughly does the answer cover all aspects of the question compared to the gold answer? d) Conciseness
3. [3]:  Passage ID 3: comprehension key metricsinclude BLEU Papineni et al. (2002), ROUGE Lin (2004), METEOR Lavie and Denkowski (2009), BERT Cosine Similarity (BERT-CS) Maurya and Desarkar (2020), Self-BLEU Caccia et al. (2019), and embedding-based metrics Liu et al. (2016a) like Greedy Matching Rus and Lintean (2012), Embedding Average John et al. (2016), and Vector Extrema Forgues et al. (2014). Two studies Xie et al. (2021); Shuai et al. (2021) employ the accuracy score Lan et al. (2019a) in conjunction with the BERT Kenton and Toutanova (2019) and ALBERT Lan et al. (2019b) as reading comprehension systems to assess the generated distractors.5.2 Human EvaluationIn the cloze task, reliability and plausibility are the most common evaluation methods used for assessing the quality of generated candidates. After selecting suitable participants for evaluation, developers will prepare list of generated candidates with list of ground truth distractors. In most cases, the chosen participant will use a
4. [4]:  Passage ID 4: and NLG, benchmarks for knowledge and reasoning, and benchmarks for holistic evaluation.7.1 Benchmarks for NLU and NLGUnderstanding and generating language represent the core ability of human linguistic competence. Consequently, natural language understanding (NLU) and natural language generation (NLG) are the two key areas in natural language processing. The evaluation of models’ understanding and generation capabilities typically employs classic tasks from NLU and NLG, such as question answering and reading comprehension, among others. Typically, the tasks selected for evaluation are intentionally designed to be challenging while remaining solvable by the majority of human participants (Wang et al., 2019b). Each subtask has its own automatic evaluation metrics.GLUE (Wang et al., 2019b) is a widely adopted benchmark in NLU, comprising nine tasks with different categories and a diagnostic dataset. These categories encompass single-sentence tasks, similarity, paraphrase tasks,
5. [5]:  Passage ID 5: Baidu Zhidao (question-answering community). Answers are human-generated instead of spans of original context. What makes DuReader different is that it provides new question types such as yes/no and opinion. Compared to factoid questions, these questions sometimes require summaries over multiple parts of documents, which opens an opportunity for the research community.4.2 Evaluation MetricsFor different MRC tasks, there are various evaluation metrics. To evaluate cloze tests and multiple-choice tasks, the most common metric is accuracy. In terms of span extraction, exact match (EM), a variant of accuracy, and F1 score are computed to measure model performance. Considering that answers for free answering tasks are not limited to the original context, ROUGE-L and BLEU are widely used. In the following part, we will give detailed descriptions of these evaluation metrics.-AccuracyAccuracy with respect to ground-truth answers is usually applied to evaluate cloze tests and