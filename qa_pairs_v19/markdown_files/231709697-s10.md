# English Machine Reading Comprehension Datasets: A Survey

## Question

How do evaluation metrics vary for extractive, generative, and cloze machine reading comprehension tasks?

## URLs

1. https://ar5iv.org/html/2405.18682
2. https://ar5iv.org/html/2402.01512
3. https://ar5iv.org/html/2109.12264
4. https://ar5iv.org/html/2410.04266
5. https://ar5iv.org/html/2404.19316
6. https://ar5iv.org/html/2411.05338
7. https://ar5iv.org/html/2310.19736
8. https://ar5iv.org/html/2001.01582
9. https://ar5iv.org/html/1907.01118
10. https://ar5iv.org/html/2101.00178

## Answer

The evaluation metrics for extractive, generative, and cloze machine reading comprehension tasks vary significantly, reflecting the distinct characteristics and challenges posed by each task type.

1. **Extractive Tasks**: In extractive reading comprehension, the goal is to select a span of text from a given document in response to a question. Traditional evaluation methods for these tasks often rely on metrics such as accuracy and F1 score, which measure the overlap between the model’s selected response and the ground truth answer. These metrics can sometimes be inadequate, as they may not consider the semantic richness of the outputs or address the issue of missing or extraneous information [1] [4]. Qualitative evaluations, such as manual reviews of output for inconsistencies and completeness, are also employed to ensure a broader understanding of the model's performance [4].

2. **Generative Tasks**: When it comes to generative tasks, where the model is expected to generate text that can vary widely in form and content, traditional metrics like BLEU and ROUGE are more commonly applied. However, these metrics primarily evaluate surface-level similarity and can overlook semantically equivalent outputs [2]. To address these limitations, newer evaluation approaches that incorporate human-like understanding are being developed. For instance, LucyEval introduces a more nuanced evaluation framework that includes various categories of subjective assessments, thereby providing a richer analysis of generative outputs [3]. 

3. **Cloze Tasks**: Cloze tasks, which often involve fill-in-the-blank questions, typically use accuracy as the main performance metric. While some benchmarks, like AGIEval, cater specifically to this format, the primary focus remains on choosing the correct answer from multiple options [3]. This focus on discrete answers means that the evaluation is somewhat constrained, and may not capture the richness and variability of generative or open-ended outputs.

Overall, while extractive tasks emphasize precision in identifying spans of text, generative tasks require a broader evaluation philosophy that accommodates semantic variability. Cloze tasks, on the other hand, tend to simplify the evaluation to largely discrete answer selection. The evolution towards more comprehensive metrics that assess semantic understanding and generation capabilities is ongoing, suggesting a move away from rigid, surface-level benchmarks toward a more nuanced evaluation landscape [1] [2] [3]. 

This variety in evaluation metrics illustrates the ongoing challenges in NLP, particularly in effectively measuring and interpreting complex outputs generated by modern models [1].

[1]: https://ar5iv.org/html/2405.13622, No Title
[2]: https://ar5iv.org/html/2408.09235, No Title
[3]: https://ar5iv.org/html/2310.19736, [2310.19736] Evaluating Large Language Models: A Comprehensive Survey
[4]: https://ar5iv.org/html/2305.16326, No Title
[5]: https://ar5iv.org/html/2305.16326, No Title

[1]: Passage ID 1: of evaluation in Natural Language Processing (NLP) has transitioned from classical, task-specific benchmarks, like BLEU scores for machine translation (Papineni, 2002), to more nuanced metrics (Es et al., 2023; Zheng et al., 2023; Hoshi et al., 2023; Saad-Falcon et al., 2023), like Answer Equivalence (Bulian et al., 2022), as the complexity of outputs has increased. This shift is exemplified by the work on GPT-3 which challenges traditional evaluation methods with its task-agnostic capabilities (Brown et al., 2020; Bender & Koller, 2020). Notwithstanding these advances, the field continues to confront well documented challenges (Deutsch et al., 2021; Bowman & Dahl, 2021; Bulian et al., 2022; Novikova et al., 2017; Fabbri et al., 2021), in particular to accurately measure models’ understanding of nuanced human concepts.When evaluating retrieval-augmented generation models, the difficulties are compounded by the multiplicity of components involved. (Gao et al., 2023b) offers a survey
[2]: Passage ID 2: Language Processing (NLP) forward. With their widespread applications, the need for reliable evaluation methods has become increasingly critical. Such evaluations are essential to ensure these models meet quality standards, align with human expectations, and maintain safety and reliability in various applications (Chang et al., 2024).Conventional automated metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) have long been employed to evaluate the performance of model generated text. However, these metrics primarily focus on surface-form similarity and often fail to account for semantically equivalent lexical and compositional diversity (Zhu et al., 2023; Chen et al., 2021; Zhang et al., 2020). Moreover, automated metrics struggle in evaluating open-ended generation or free-form text, where a wide range of acceptable outputs exists. This limitation becomes particularly evident when assessing instruction-tuned chat models, which tend
[3]: Passage ID 3: languages.Except for AGIEval (Zhong et al., 2023), which incorporates fill-in-the-blank questions, all the aforementioned benchmarks primarily rely on multiple-choice questions as their main evaluation format, with accuracy serving as the key performance metric. Consequently, these benchmarks tend to overlook the inclusion of open-ended questions.In contrast, LucyEval (Zeng et al., 2023b) pioneers a more diverse evaluation approach by introducing three categories of subjective questions: conceptual explanations, short answer questions, and computational questions. Additionally, LucyEval (Zeng et al., 2023b) introduces a novel evaluation metric known as GScore. For the assessment of short answer questions and conceptual explanations, GScore aggregates a variety of metrics, including BLEU-4, ROUGE-2, ChrF, and Semantic Similarity, through a weighted combination. This holistic approach offers a relatively comprehensive yet straightforward means of evaluating subjective
[4]: Passage ID 4: 12 BioNLP datasets covering six applications (named entity recognition, relation extraction, multi-label document classification, question answering, text summarization, and text simplification). The evaluation is conducted under four settings: zero-shot, static few-shot, dynamic K-nearest few-shot, and fine-tuning. We compare these models against state-of-the-art (SOTA) approaches that fine-tune (domain-specific) BERT or BART models, which are well-established methods in BioNLP tasks. The evaluation covers both quantitative and qualitative evaluations, where the latter involves manually reviewing collectively hundreds of thousands of LLM outputs for inconsistencies, missing information, and hallucinations in extractive and classification tasks. The qualitative review also examines accuracy, 1 completeness, and readability in text summarization tasks. Additionally, a cost analysis of closed-source GPT models is conducted. Subjects:Computation and Language (cs.CL); Artificial
[5]: Passage ID 5: 12 BioNLP datasets covering six applications (named entity recognition, relation extraction, multi-label document classification, question answering, text summarization, and text simplification). The evaluation is conducted under four settings: zero-shot, static few-shot, dynamic K-nearest few-shot, and fine-tuning. We compare these models against state-of-the-art (SOTA) approaches that fine-tune (domain-specific) BERT or BART models, which are well-established methods in BioNLP tasks. The evaluation covers both quantitative and qualitative evaluations, where the latter involves manually reviewing collectively hundreds of thousands of LLM outputs for inconsistencies, missing information, and hallucinations in extractive and classification tasks. The qualitative review also examines accuracy, 1 completeness, and readability in text summarization tasks. Additionally, a cost analysis of closed-source GPT models is conducted. Subjects:Computation and Language (cs.CL); Artificial