# English Machine Reading Comprehension Datasets: A Survey

## Question

How do evaluation metrics vary for extractive, generative, and cloze machine reading comprehension tasks?

## URLs

1. https://ar5iv.org/html/2405.18682. [2405.18682] Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical Machine Reading Comprehension
2. https://ar5iv.org/html/2402.01512. [2402.01512] Distractor Generation for Multiple-Choice Questions: A Survey of Methods, Datasets, and Evaluation
3. https://ar5iv.org/html/2109.12264. [2109.12264] More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering
4. https://ar5iv.org/html/2410.04266. [2410.04266] Constructing Cloze Questions Generatively
5. https://ar5iv.org/html/2404.19316. [2404.19316] QLSC: A Query Latent Semantic Calibrator for Robust Extractive Question Answering † Equal contribution. ✉ Corresponding author: Xulong Zhang (zhangxulong@ieee.org).
6. https://ar5iv.org/html/2411.05338. [2411.05338] SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers
7. https://ar5iv.org/html/2310.19736. [2310.19736] Evaluating Large Language Models: A Comprehensive Survey
8. https://ar5iv.org/html/2001.01582. [2001.01582] A Survey on Machine Reading Comprehension Systems
9. https://ar5iv.org/html/1907.01118. [1907.01118] Neural Machine Reading Comprehension: Methods and Trends
10. https://ar5iv.org/html/2101.00178. [2101.00178] UnitedQA: A Hybrid Approach for Open Domain Question Answering

## Answer (I don't know?: ✗)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable