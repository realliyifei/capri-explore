# A Survey on Intelligent Poetry Generation: Languages, Features, Techniques, Reutilisation and Evaluation

## Question

How do English poetry generation systems typically handle metre and rhyme?

## URLs

1. https://ar5iv.org/html/2406.15267
2. https://ar5iv.org/html/2410.15299
3. https://ar5iv.org/html/2305.08493
4. https://ar5iv.org/html/2406.18906
5. https://ar5iv.org/html/2406.03450
6. https://ar5iv.org/html/2210.13669
7. https://ar5iv.org/html/1711.00938
8. https://ar5iv.org/html/2002.02511
9. https://ar5iv.org/html/1908.08861
10. https://ar5iv.org/html/2103.03775

## Answer

English poetry generation systems typically employ various machine learning methods to effectively handle metre and rhyme, with advancements seen particularly in neural network architectures. These systems often use encoder-decoder models, specifically recurrent neural networks (RNNs) like LSTMs (Long Short-Term Memory networks) that can account for temporal sequences in text [2].

One notable approach in managing rhyming constraints is the strategic generation of words. For instance, some systems generate the last word of each line before constructing the rest, allowing for the alignment of rhyme and rhythm within predefined patterns [2]. These techniques highlight the distinction from traditional phrase-based systems, which may lack the flexibility necessary for poetic forms. 

In terms of metre, advanced systems have shown the capability to learn and replicate specific rhythmic structures by being trained on targeted poetry corpuses. Research by Hopkins and Kiela (2017) demonstrated that networks could generate rhythmic poetry when trained with sufficient examples of a single type of poetry, albeit with evaluations suggesting that these outputs were rated lower than human-generated texts [3]. However, they managed to create poetry that was indistinguishable from human work in assessments, indicating a level of proficiency in mimicking poetic structure.

Further developments like the Deep-speare model highlight an integration of multiple neural networks, each focusing on different aspects of poetry generation: one for rhythm, another for rhyme, and yet another for word choice [3]. This modular approach enables the system to adhere to the intricate demands of English poetry's formal characteristics such as iambic pentameter in sonnets.

In recent research, systems such as ByGPT5 have emerged, capable of learning style constraints, including rhyme schemes, without intensive human engineering during the model design phase. This increase in autonomy reflects a significant leap in the functionality of poetry generation systems in automatically learning from human-created datasets [5]. The collaborative model proposed by Chakrabarty et al. (2022) also illustrates how human input can guide the process while assessing creativity through crowd evaluation mechanisms. This indicates a nuanced understanding of both constraints and the creative essence required in poetry writing [5].

Moreover, while early systems were heavily hand-engineered, modern approaches heavily lean on the statistical advantages derived from large datasets of human poetry, highlighting a transformative shift towards data-driven learning strategies in this field [5].

In summary, English poetry generation systems leverage a range of sophisticated neural architectures and training techniques to effectively manage metre and rhyme, employing targeted generation methods, specialized neural networks, and learning from extensive collections of existing poetry. These advancements point towards increasingly autonomous systems that can generate high-quality poetic forms while preserving essential stylistic features.

[1]: https://ar5iv.org/html/2406.03450, [2406.03450] What is the Best Way for ChatGPT to Translate Poetry?
[2]: https://ar5iv.org/html/2103.03775, [2103.03775] There Once Was a Really Bad Poet, It Was Automated but You Didn’t Know It
[3]: https://ar5iv.org/html/2002.02511, [2002.02511] Introducing Aspects of Creativity in Automatic Poetry Generation
[4]: https://ar5iv.org/html/2406.18906, [2406.18906] Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets
[5]: https://ar5iv.org/html/2406.15267, [2406.15267] Evaluating Diversity in Automatic Poetry Generation

[1]: Passage ID 1: (2010). This research explored how statistical machine translation systems could produce translations that adhere to constraints such as length, rhythm, and rhyme.Subsequently, Greene et al. (2010) used statistical methods to translate rhymed poetry, achieving the translation of Italian poetry into English. Through unsupervised learning, they identified word stress patterns in an original poetry corpus, which were then utilized in generating English poetry.Ghazvininejad et al. (2018) introduced the first neural poetry translation system, capable of translating French poetry into English while adhering to user-specified target rhythm and rhyme patterns. This system demonstrated greater flexibility than phrase-based machine translation (PBMT) systems, consistently generating translations that fit any specified scheme.Chakrabarty et al. (2021) conducted an empirical study on poetry translation, highlighting a crucial yet often overlooked issue: advanced machine translation systems,
[2]: Passage ID 2: 2019). However, their large-scale neural network models take advantage of the Chinese quatrain database, which has more than 150k training examples. In contrast, LimGen uses less than 300 limericks. Most modern poetry-generation systems are encoder-decoder style recurrent networks (e.g. character-level and word-level LSTMs) with modifications such as various forms of attention mechanisms. Lau et al. (2018) integrated these techniques and proposed Deep-speare, which represents the state-of-the-art for Shakespearean sonnet generation. In our experiments, we have adapted and re-trained Deep-speare for limerick generation. Empirically, it cannot compete with LimGen.For handling rhyming constraints, unlike Ghazvininejad et al. (2016) and Benhart et al. (2018) who generate the last word of each line before generating the rest of the line, our proposed Storyline algorithm selects a probability distribution for the last word of each line.Beyond poetry generation, templates are often used
[3]: Passage ID 3: controlling for style in Chinese poetry. They found that with enough training data, adequate results could be achieved. Problems related to poetic structure were addressed by Hopkins and Kiela (2017). They generated rhythmic poetry by training the network on only a single type of poetry to ensure produced poems adhered to a single rhythmic structure. It was found in human evaluations that while the poems produced were rated to be of lower quality than human produced poems, they were indistinguishable from human produced poems. Lau et al. (2018) took the LSTM approach one step further with the Deepspeare model by employing an attention mechanism to model interactions among generated words. They also use three neural networks, one for rhythm, one for rhyming and another for word choice in their quest to generate Shakespeare-like sonnets.Vaswani et al. (2017) developed a deep neural architecture called the Transformer that did away with any sort of need for recurrence. The Transformer
[4]: Passage ID 4: a task to evaluate how well LLMs can identify poetic form for more than 20 poetic forms and formal elements in the English language.This is a challenging task because poetic form is determined by a combination of factors: rhyme scheme, meter, repetition, number of lines, and/or subject matter.But what do LLMs really know about poetry?What can they know about poetry?Prior research has focused on computational poetry generation (zhang-lapata-2014-chinese), summarization (mahbub-etal-2023-unveiling) and detection of individual forms abdibayev_automating_2021, but we need broader evaluation of a wider range of poetic forms and features, and updated audits of LLM capacities and knowledge.Poetic features uniquely combine verbal, aural, and visual elements; the substance, sound, and (in written poetry) appearance of words on the page (e.g., white space) all matter.What’s more, poetry often communicates deep emotion and meaning in non-literal, ambiguous ways, employing figurative
[5]: Passage ID 5: While early modern systems were heavily hand-engineered Gervás (2001), more recent approaches are all trained on collections of human poetry Lau et al. (2018); Jhamtani et al. (2019); Agarwal and Kann (2020) but still extensively utilize human guidance e.g. to enforce formal characteristics of poetry such as rhyming Wöckener et al. (2021). Belouadi and Eger (2023) have recently released a character-level decoder-only LLM (ByGPT5) capable of learning style-constraints such as rhyming without human involvement in model design.Chakrabarty et al. (2022) propose a collaborative system for poetry,which can follow human instructions to write poems. Theymeasure creativity of the generated poems via crowd workers, who decidewhich of two poems is more creative. While Chakrabarty et al. (2022) do not define creativity, it could be considered as generating novel poems outside the training data set; in contrast, we measure diversity by assessing whether poetry generation systems generate