# A Survey on Intelligent Poetry Generation: Languages, Features, Techniques, Reutilisation and Evaluation

## Question

How do English poetry generation systems typically handle metre and rhyme?

## URLs

1. https://ar5iv.org/html/2406.15267. [2406.15267] Evaluating Diversity in Automatic Poetry Generation
2. https://ar5iv.org/html/2410.15299. [2410.15299] Does ChatGPT Have a Poetic Style?
3. https://ar5iv.org/html/2305.08493. [2305.08493] Creative Data Generation: A Review Focusing on Text and Poetry
4. https://ar5iv.org/html/2406.18906. [2406.18906] Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets
5. https://ar5iv.org/html/2406.03450. [2406.03450] What is the Best Way for ChatGPT to Translate Poetry?
6. https://ar5iv.org/html/2210.13669. [2210.13669] Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing
7. https://ar5iv.org/html/1711.00938. No content available
8. https://ar5iv.org/html/2002.02511. [2002.02511] Introducing Aspects of Creativity in Automatic Poetry Generation
9. https://ar5iv.org/html/1908.08861. [1908.08861] Neural Poetry: Learning to Generate Poems using SyllablesThis is a post-peer-review, pre-copyedit version of an article published in LNCS, volume 11730. The final authenticated version is available online at: https://doi.org/10.1007/978-3-030-30490-4_26
10. https://ar5iv.org/html/2103.03775. [2103.03775] There Once Was a Really Bad Poet, It Was Automated but You Didn’t Know It

## Answer (I don't know?: ✗)

English poetry generation systems employ various approaches to handle metre and rhyme, utilizing advancements in machine learning and statistical methods.

Initially, statistical machine translation systems were found to effectively manage constraints such as length, rhythm, and rhyme. For instance, Greene et al. (2010) successfully translated Italian poetry into English by leveraging statistical methods that focused on word stress patterns identified through unsupervised learning. This method allowed for the generation of English poetry that adhered to the stylistic requirements of the original [1]. Moreover, Ghazvininejad et al. (2018) introduced a neural poetry translation system which demonstrated the ability to translate French poetry into English while adhering to specified rhythm and rhyme patterns, outperforming traditional phrase-based machine translation systems in flexibility [1].

In the realm of modern poetry generation, systems typically utilize encoder-decoder architectures, particularly recurrent neural networks (RNNs) such as LSTMs, often enhanced by attention mechanisms. An example of this is Lau et al. (2018) who developed "Deep-speare," a state-of-the-art model specifically designed for generating Shakespearean sonnets. Their architecture incorporates multiple neural networks, each focusing on aspects like rhythm, rhyme, and word choice, effectively addressing the complex nature of poetic generation [2][4]. 

Different systems employ various techniques for managing rhyming constraints. For instance, rather than generating a full line of poetry before addressing rhyme, some approaches (like those noted by the authors of the Storyline algorithm) select a probability distribution for the last word of each line to maintain the desired rhyme scheme [2]. This stands in contrast to other methods where the last word is generated prior to the rest of the line [2].

Further, the abundance of training data plays a crucial role in the performance of these systems. Research indicates that greater quantities and diversity of poetry data can enhance the quality of the generated output. However, challenges remain, particularly in ensuring that the generated poetry not only adheres to structural rules but also resonates with aesthetic quality, as evidenced by the findings of Hopkins and Kiela (2017), who noted that while their generated rhythmic poetry was rated lower than human-produced examples, it was often indistinguishable from them [4].

Overall, effective English poetry generation hinges on a combination of sophisticated neural architectures, adequate training data, and nuanced algorithms capable of balancing the intricate demands of metre and rhyme across diverse poetic forms. This complexity illustrates the ongoing evolution and challenges in reaching human-like poetic expression in machine-generated outputs.

1. [1]:  https://ar5iv.org/html/2406.03450, [2406.03450] What is the Best Way for ChatGPT to Translate Poetry?
2. [2]:  https://ar5iv.org/html/2103.03775, [2103.03775] There Once Was a Really Bad Poet, It Was Automated but You Didn’t Know It
3. [3]:  https://ar5iv.org/html/2406.18906, [2406.18906] Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets
4. [4]:  https://ar5iv.org/html/2002.02511, [2002.02511] Introducing Aspects of Creativity in Automatic Poetry Generation
5. [5]:  https://ar5iv.org/html/2406.18906, [2406.18906] Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets
---
1. [1]:  Passage ID 1: (2010). This research explored how statistical machine translation systems could produce translations that adhere to constraints such as length, rhythm, and rhyme.Subsequently, Greene et al. (2010) used statistical methods to translate rhymed poetry, achieving the translation of Italian poetry into English. Through unsupervised learning, they identified word stress patterns in an original poetry corpus, which were then utilized in generating English poetry.Ghazvininejad et al. (2018) introduced the first neural poetry translation system, capable of translating French poetry into English while adhering to user-specified target rhythm and rhyme patterns. This system demonstrated greater flexibility than phrase-based machine translation (PBMT) systems, consistently generating translations that fit any specified scheme.Chakrabarty et al. (2021) conducted an empirical study on poetry translation, highlighting a crucial yet often overlooked issue: advanced machine translation systems,
2. [2]:  Passage ID 2: 2019). However, their large-scale neural network models take advantage of the Chinese quatrain database, which has more than 150k training examples. In contrast, LimGen uses less than 300 limericks. Most modern poetry-generation systems are encoder-decoder style recurrent networks (e.g. character-level and word-level LSTMs) with modifications such as various forms of attention mechanisms. Lau et al. (2018) integrated these techniques and proposed Deep-speare, which represents the state-of-the-art for Shakespearean sonnet generation. In our experiments, we have adapted and re-trained Deep-speare for limerick generation. Empirically, it cannot compete with LimGen.For handling rhyming constraints, unlike Ghazvininejad et al. (2016) and Benhart et al. (2018) who generate the last word of each line before generating the rest of the line, our proposed Storyline algorithm selects a probability distribution for the last word of each line.Beyond poetry generation, templates are often used
3. [3]:  Passage ID 3: that more poetry data is available in the training of closed models like GPT-4 than is available in Dolma, and memorization is an issue that can be partly but not fully addressed by current open resources.4 Methods4.1 Form ClassificationSonnetLimerickHaikuBalladmodelf1precisionrecallf1precisionrecallf1precisionrecallf1precisionrecallGPT-3.5All of the poems have been tagged by experts as one of these forms. You must pick one of these options.Please also provide an elaborated rationale for why you think the poem is in this form, a one-word summary rationale, and a score ranking your confidence in your answer from 0 to 1.Please report the single poetic form, elaborated rationale, one-word rationale, and multiple forms in the following format.1. Poetic Form: Ballad2. Elaborated Rationale: This poem is a ballad because…3. One-Word Summary: Meter4. Confidence Score : 0.911. Poetic Form: Sonnet2. Elaborated Rationale:
4. [4]:  Passage ID 4: controlling for style in Chinese poetry. They found that with enough training data, adequate results could be achieved. Problems related to poetic structure were addressed by Hopkins and Kiela (2017). They generated rhythmic poetry by training the network on only a single type of poetry to ensure produced poems adhered to a single rhythmic structure. It was found in human evaluations that while the poems produced were rated to be of lower quality than human produced poems, they were indistinguishable from human produced poems. Lau et al. (2018) took the LSTM approach one step further with the Deepspeare model by employing an attention mechanism to model interactions among generated words. They also use three neural networks, one for rhythm, one for rhyming and another for word choice in their quest to generate Shakespeare-like sonnets.Vaswani et al. (2017) developed a deep neural architecture called the Transformer that did away with any sort of need for recurrence. The Transformer
5. [5]:  Passage ID 5: a task to evaluate how well LLMs can identify poetic form for more than 20 poetic forms and formal elements in the English language.This is a challenging task because poetic form is determined by a combination of factors: rhyme scheme, meter, repetition, number of lines, and/or subject matter.But what do LLMs really know about poetry?What can they know about poetry?Prior research has focused on computational poetry generation (zhang-lapata-2014-chinese), summarization (mahbub-etal-2023-unveiling) and detection of individual forms abdibayev_automating_2021, but we need broader evaluation of a wider range of poetic forms and features, and updated audits of LLM capacities and knowledge.Poetic features uniquely combine verbal, aural, and visual elements; the substance, sound, and (in written poetry) appearance of words on the page (e.g., white space) all matter.What’s more, poetry often communicates deep emotion and meaning in non-literal, ambiguous ways, employing figurative