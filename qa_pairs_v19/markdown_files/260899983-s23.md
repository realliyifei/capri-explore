# Through the Lens of Core Competency: Survey on Evaluation of Large Language Models

## Question

What are the methods used to assess the quality of text generated with sentiment in NLP tasks?

## URLs

1. https://ar5iv.org/html/2408.01527. [2408.01527] Analyzing LLMs’ Capabilities to Establish Implicit User Sentiment of Software Desirability* *The authors wish to thank Creighton University and Dakota State University for their generous financial support of this research.
2. https://ar5iv.org/html/2310.05312. [2310.05312] Quality Assurance of A GPT-based Sentiment Analysis System: Adversarial Review Data Generation and Detection
3. https://ar5iv.org/html/2405.09770. [2405.09770] Optimization Techniques for Sentiment Analysis Based on LLM (GPT-3)
4. https://ar5iv.org/html/2404.10503. [2404.10503] A Sentiment Analysis of Medical Text Based on Deep Learning
5. https://ar5iv.org/html/2409.17885. [2409.17885] Sentiment Analysis of ML Projects: Bridging Emotional Intelligence and Code Quality
6. https://ar5iv.org/html/2407.13069. [2407.13069] Dynamic Sentiment Analysis with Local Large Language Models using Majority Voting: A Study on Factors Affecting Restaurant Evaluation
7. https://ar5iv.org/html/2311.11250. [2311.11250] A Comprehensive Review on Sentiment Analysis: Tasks, Approaches and Applications
8. https://ar5iv.org/html/2405.04053. [2405.04053] Evaluating Text Summaries Generated by Large Language Models Using OpenAI’s GPT
9. https://ar5iv.org/html/2309.02045. [2309.02045] Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies
10. https://ar5iv.org/html/2205.06439. [2205.06439] AEON: A Method for Automatic Evaluation of NLP Test Cases

## Answer (I don't know?: ✗)

The assessment of the quality of text generated with sentiment in NLP tasks involves several methods, which can be broadly categorized into supervised and unsupervised approaches, as well as specialized techniques such as negation detection.

1. **Supervised Learning**: This method relies on annotated datasets, where the text is labeled with specific sentiment categories (e.g., positive, negative, neutral). A prediction model is built using a feature vector constructed from the text, which could include aspects like word frequency. The model is trained on this labeled data to learn how to classify unseen data [2][4]. Techniques from previous research, such as those by Turney and Littman (2003), have shown high precision results in sentiment analysis contexts, indicating the effectiveness of supervised approaches.

2. **Unsupervised Learning**: Unlike the supervised approach, unsupervised methods do not require labeled datasets. Instead, they often utilize dictionary-based methods or linguistic dictionaries to match frequent keywords to sentiment categories. For example, the Linguistic Inquiry and Word Count (LIWC) dictionary is a notable tool in this regard [4]. These methods can facilitate sentiment classification, especially in customized contexts where predefined categories are not readily available.

3. **Negation Handling**: A specific challenge in sentiment analysis is the handling of negation, which can significantly alter the sentiment conveyed in a sentence. Research has explored various ways to detect negation and its effect on sentiment scores. Automated systems have been developed, such as the one proposed by Jurek et al. (2015), which reverse the polarity of sentiments associated with negated words. This technique involves identifying negated elements in the text and applying a polarity reversal to accurately assess the underlying sentiment [5]. More advanced disambiguation functions using antonyms and other linguistic features may further enhance sentiment analysis by providing a context-based understanding of how negation influences sentiment [5].

4. **Manual Label Comparison**: In addition to the above methodologies, evaluating how well NLP models recover manual sentiment labels can serve as a crucial quality assessment measure. This involves comparing model predictions against established human-coded labels, allowing researchers to gauge model effectiveness comprehensively [1].

5. **Benchmarking Techniques**: Finally, the use of benchmark datasets, as highlighted by Pang and Lee (2008), is essential in sentiment analysis research. These datasets facilitate the comparison of various models and methods, providing a standard for assessing quality in sentiment generation [2][3].

In conclusion, the quality of text generated in NLP sentiment tasks can be evaluated through a combination of supervised and unsupervised learning methods, effective negation detection strategies, and rigorous manual label comparison against established benchmarks, ensuring robust sentiment classification capabilities.

1. [1]:  https://ar5iv.org/html/2405.02454, No Title
2. [2]:  https://ar5iv.org/html/2311.11250, [2311.11250] A Comprehensive Review on Sentiment Analysis: Tasks, Approaches and Applications
3. [3]:  https://ar5iv.org/html/2311.11250, [2311.11250] A Comprehensive Review on Sentiment Analysis: Tasks, Approaches and Applications
4. [4]:  https://ar5iv.org/html/2302.03490, No Title
5. [5]:  https://ar5iv.org/html/2302.02291, No Title
---
1. [1]:  Passage ID 1: of the text analysis task, and then the model executes the task based on the description. However, the ambiguous nature of sentiment outlined above raises an important question: How do large language models understand the task of “sentiment analysis” when prompted to do it? If the model’s understanding of sentiment is not consistent with the researcher’s, they may not be measuring what they think they are. Further, if sentiment is a confounded measurement, would text analysis improve if we disambiguate the term and measure more precise concepts? In the following sections I present answers to these questions.2 Data and MethodsI approach these questions by using LLMs to classify documents three different times: once for sentiment, once for emotional valence, and a final time for stance (opinion). I then evaluate which of the classification approaches is best able to recover manual labels from two data sets, and examine how well sentiment classifications correlate with both opinion
2. [2]:  Passage ID 2: Turney and Littman, 2003; Wiebe et al, 2000) worked on sentiments analysis and opinions mining. Nasukawa et al. (Nasukawa and Yi, 2003) showed the high precision result on customer reviews and news articles available over web pages. They classified the specific subjects from a document in positive or negative polarity. This paper’s result rise in interest to other researchers in this domain. The influential 2008 review of Pang and Lee (Pang and Lee, 2008) covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems on benchmark datasets in recent research. Here, we discuss sentiment analysis in NLP, including its different methods, such as supervised and unsupervised.2.1.1 Supervised ApproachIt is based on the annotated dataset (labeled data) to build a prediction model. This approach builds a feature vector of the text, either aspect or word frequency, then the model learns (training) on the dataset and gives prediction for unseen
3. [3]:  Passage ID 3: Turney and Littman, 2003; Wiebe et al, 2000) worked on sentiments analysis and opinions mining. Nasukawa et al. (Nasukawa and Yi, 2003) showed the high precision result on customer reviews and news articles available over web pages. They classified the specific subjects from a document in positive or negative polarity. This paper’s result rise in interest to other researchers in this domain. The influential 2008 review of Pang and Lee (Pang and Lee, 2008) covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems on benchmark datasets in recent research. Here, we discuss sentiment analysis in NLP, including its different methods, such as supervised and unsupervised.2.1.1 Supervised ApproachIt is based on the annotated dataset (labeled data) to build a prediction model. This approach builds a feature vector of the text, either aspect or word frequency, then the model learns (training) on the dataset and gives prediction for unseen
4. [4]:  Passage ID 4: 2020).A well-known subtask of text classification is sentiment classification (also known as sentiment analysis, or opinion mining), which aims to distinguish the subjective information in the text, such as positive or negative sentiment (Pang and Lee, 2007).However, the existing tools only do well in categories that are easy to predict. If the categorization is customized and very specific to a study context, then there are two common solutions. One is to use dictionary-based methods, by a list of frequent keywords that correspond to a certain category (Albaugh et al., 2013) or using general linguistic dictionaries such as the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker et al., 2001).The second way is to adopt the data-driven pipeline, which requires human hand coding of documents into a predetermined set of categories, then train an NLP model to learn the text classification task (Sun et al., 2019), and verify the performance of the NLP model on a held-out
5. [5]:  Passage ID 5: tool, we answered the following two questions:•RQ1: How do we detect negation in a given text?•RQ2: How can a automated system apply negation to appropriate words to improve sentiment analysis?NLP was used mainly because it guides the extraction of information from texts in their natural form. The most frequent lexicon-based technique for negation is to reverse the polarity of the object that is affected by the negator in a sentence (Jurek et al., 2015). This study presents an alternative approach by developing a disambiguation function that can be used to average out the polarity scores of the negated word antonyms using five dictionaries. Beyond sentiment detection, we propose using antonyms to construct a human-readable semantic construction of the entire sentence, taking a context-based approach into consideration. First, detecting negation in a sentence requires the use of an automated decontraction system, as some negations may be in contraction mode e.g.,