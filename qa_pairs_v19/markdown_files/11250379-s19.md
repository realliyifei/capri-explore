# A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin

## Question

What challenges and limitations are associated with using decoupled approaches in Mandarin speech recognition systems?

## URLs

1. https://ar5iv.org/html/2311.07062. [2311.07062] Decoupling and Interacting Multi-Task Learning Network for Joint Speech and Accent Recognition
2. https://ar5iv.org/html/2403.01255. [2403.01255] Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey
3. https://ar5iv.org/html/2412.00721. [2412.00721] A Comparative Study of LLM-based ASR and Whisper in Low Resource and Code Switching Scenario
4. https://ar5iv.org/html/2010.14798. [2010.14798] decoupling pronunciation and language for end-to-end code-switching automatic speech recognition
5. https://ar5iv.org/html/2308.13345. [2308.13345] Decoupled Structure for Improved Adaptability of End-to-End Models
6. https://ar5iv.org/html/2208.04641. [2208.04641] ASR Error Correction with Constrained Decoding on Operation Prediction
7. https://ar5iv.org/html/2409.09554. [2409.09554] ASR Error Correction using Large Language Models

## Answer (I don't know?: ✓)

The context provided does not contain specific information regarding the challenges and limitations associated with using decoupled approaches in Mandarin speech recognition systems. While there are several references to various studies and methodologies in Automatic Speech Recognition (ASR), including those discussing Mandarin ASR and deep learning techniques, the excerpts focus primarily on advancements, methodologies, and performance comparisons without detailing the challenges or limitations of decoupling recognition and transcription processes.

For instance, references to various techniques, such as syllable-based models, modular training, and the application of large language models (LLMs), indicate the exploration of these technologies but do not explicitly mention potential drawbacks or difficulties related to decoupling methods specifically in Mandarin ASR systems [1][2][3][4][5]. These aspects—such as integration complexities, potential decreases in efficiency, or issues with context understanding—are not addressed in the provided context.

Furthermore, while the authors discuss performance comparisons and experimental results in various ASR scenarios, the narrative does not cover the obstacles that may arise from implementing decoupled recognition strategies or the impact on system outputs when these strategies are employed.

Therefore, due to the lack of specific details about the challenges and limitations of decoupled approaches in Mandarin speech recognition systems, I conclude that I don't know. The context is insufficient to provide a comprehensive answer to the question.

1. [1]:  https://ar5iv.org/html/2311.07062, [2311.07062] Decoupling and Interacting Multi-Task Learning Network for Joint Speech and Accent Recognition
2. [2]:  https://ar5iv.org/html/2311.07062, [2311.07062] Decoupling and Interacting Multi-Task Learning Network for Joint Speech and Accent Recognition
3. [3]:  https://ar5iv.org/html/2308.13345, [2308.13345] Decoupled Structure for Improved Adaptability of End-to-End Models
4. [4]:  https://ar5iv.org/html/2412.00721, [2412.00721] A Comparative Study of LLM-based ASR and Whisper in Low Resource and Code Switching Scenario
5. [5]:  https://ar5iv.org/html/2412.00721, [2412.00721] A Comparative Study of LLM-based ASR and Whisper in Low Resource and Code Switching Scenario
---
1. [1]:  Passage ID 1: Annual Conference of the International SpeechCommunication Association (INTERSPEECH), 2018, pp. 791–795.[39]J. Yuan, X. Cai, D. Gao, R. Zheng, L. Huang, and K. Church, “Decouplingrecognition and transcription in Mandarin ASR,” in IEEE AutomaticSpeech Recognition and Understanding Workshop (ASRU), 2021, pp. 1019–1025.[40]X. Wang, Z. Yao, X. Shi, and L. Xie, “Cascade RNN-transducer: Syllablebased streaming on-device Mandarin speech recognition with asyllable-to-character converter,” in IEEE Spoken Language TechnologyWorkshop (SLT), 2021, pp. 15–21.[41]S. Zhang, J. Yi, Z. Tian, Y. Bai, J. Tao et al., “Decouplingpronunciation and language for end-to-end code-switching automatic speechrecognition,” in IEEE International Conference on Acoustics, Speechand Signal Processing (ICASSP), 2021, pp. 6249–6253.[42]Y. Yang, B. Du, and Y. Li, “Multi-level modeling units for end-to-endMandarin speech recognition,” arXiv preprint
2. [2]:  Passage ID 2: character-pinyin training,” in Proceedings of theAnnual Conference of the International Speech Communication Association(INTERSPEECH), 2016, pp. 3404–3408.[36]S. Zhou, L. Dong, S. Xu, and B. Xu, “A comparison of modeling units insequence-to-sequence speech recognition with the Transformer on MandarinChinese,” in International Conference on Neural InformationProcessing (ICONIP), 2018, pp. 210–220.[37]Z. Chen, Q. Liu, H. Li, and K. Yu, “On modular training of neuralacoustics-to-word model for LVCSR,” in IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 4754–4758.[38]S. Zhou, L. Dong, S. Xu, and B. Xu, “Syllable-based sequence-to-sequencespeech recognition with the Transformer in Mandarin Chinese,” inProceedings of the Annual Conference of the International SpeechCommunication Association (INTERSPEECH), 2018, pp. 791–795.[39]J. Yuan, X. Cai, D. Gao, R. Zheng, L. Huang, and K. Church, “Decouplingrecognition
3. [3]:  Passage ID 3: D., Qian, Y., Xie,L., 2021.The accented English speech recognition challenge2020: Open datasets, tracks, baselines, results and methods, in:Proc. ICASSP, Toronto, ON, Canada.Sriram et al. (2018)Sriram, A., Jun, H.,Satheesh, S., Coates, A.,2018.Cold Fusion: Training seq2seq models together withlanguage models, in: Proc. Interspeech,Hyderabad, India.Tsunoo et al. (2022)Tsunoo, E., Kashiwagi, Y.,Narisetty, C.P., Watanabe, S.,2022.Residual language model for end-to-end speechrecognition, in: Proc. Interspeech,Incheon, Korea.Variani et al. (2020)Variani, E., Rybach, D.,Allauzen, C., Riley, M.,2020.Hybrid autoregressive transducer (HAT), in:Proc. ICASSP, Barcelona, Spain.Vaswani et al. (2017)Vaswani, A., Shazeer, N.,Parmar, N., Uszkoreit, J.,Jones, L., Gomez, A.N.,Kaiser, L., Polosukhin, I.,2017.Attention is all you need, in:Proc. NeurIPS, Long Beach, CA, USA.Wang et al. (2019)Wang, D., Wang, X., Lv,S., 2019.An overview
4. [4]:  Passage ID 4: Speech Recognition (ASR) field. Previous works mainly concentrated on leveraging LLMs for speech recognition in English and Chinese. However, their potential for addressing speech recognition challenges in low resource settings remains underexplored. Hence, in this work, we aim to explore the capability of LLMs in low resource ASR and Mandarin-English code switching ASR. We also evaluate and compare the recognition performance of LLM-based ASR systems against Whisper model. Extensive experiments demonstrate that LLM-based ASR yields a relative gain of 12.8\% over the Whisper model in low resource ASR while Whisper performs better in Mandarin-English code switching ASR. We hope that this study could shed light on ASR for low resource scenarios.  Comments:This work hasn't been finished yetSubjects:Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS)Cite as:arXiv:2412.00721 [cs.AI] (or arXiv:2412.00721v2
5. [5]:  Passage ID 5: Science > Artificial IntelligencearXiv:2412.00721 (cs) This paper has been withdrawn by Zheshu Song [Submitted on 1 Dec 2024 (v1), last revised 4 Dec 2024 (this version, v2)]Title:A Comparative Study of LLM-based ASR and Whisper in Low Resource and Code Switching ScenarioAuthors:Zheshu Song, Ziyang Ma, Yifan Yang, Jianheng Zhuo, Xie Chen View a PDF of the paper titled A Comparative Study of LLM-based ASR and Whisper in Low Resource and Code Switching Scenario, by Zheshu Song and Ziyang Ma and Yifan Yang and Jianheng Zhuo and Xie ChenNo PDF available, click to view other formatsAbstract:Large Language Models (LLMs) have showcased exceptional performance across diverse NLP tasks, and their integration with speech encoder is rapidly emerging as a dominant trend in the Automatic Speech Recognition (ASR) field. Previous works mainly concentrated on leveraging LLMs for speech recognition in English and Chinese. However, their potential for addressing speech recognition