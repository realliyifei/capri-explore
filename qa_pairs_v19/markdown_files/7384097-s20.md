# A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena

## Question

How do BLEU and METEOR metrics evaluate word order, and what are their limitations in this context?

## URLs

1. https://ar5iv.org/html/2109.14895. [2109.14895] Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems
2. https://ar5iv.org/html/1601.02789. [1601.02789] Comparison and Adaptation of Automatic Evaluation Metrics for Quality Assessment of Re-Speaking
3. https://ar5iv.org/html/2306.13041. [2306.13041] Towards Explainable Evaluation Metrics for Machine Translation
4. https://ar5iv.org/html/2401.06760. [2401.06760] Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies
5. https://ar5iv.org/html/2204.09533. [2204.09533] Evaluating Commit Message Generation: To BLEU Or Not To BLEU?
6. https://ar5iv.org/html/2403.11473. [2403.11473] Word Order‚Äôs Impacts: Insights from Reordering and Generation Analysis
7. https://ar5iv.org/html/2403.08103. [2403.08103] Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data
8. https://ar5iv.org/html/2403.09727. [2403.09727] Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems
9. https://ar5iv.org/html/2208.03133. [2208.03133] Out of the BLEU: How Should We Assess Quality of the Code Generation Models?

## Answer (I don't know?: ‚úó)

BLEU and METEOR are two widely used metrics for evaluating translation quality in natural language processing (NLP), but they approach the evaluation of word order differently and have inherent limitations.

BLEU (Bilingual Evaluation Understudy) primarily uses a lexical match-based scoring system. It evaluates translation quality by calculating the overlap of n-grams (sequences of n words) between candidate translations and reference translations, with a typical n ranging from 1 to 4. The proximity between candidate and reference translations for BLEU is measured through these overlapping n-grams, and the scores are combined in a geometric mean [3]. However, BLEU's focus on mere surface form similarity means that it does not effectively account for word order variations; it treats all matching n-grams equally without considering their positioning context. This can lead to situations where translations that represent different word orders may still receive favorable scores, effectively masking significant errors in word order [2] [3]. 

On the other hand, METEOR (Metric for Evaluation of Translation with Explicit ORdering) attempts to address some of these limitations through its algorithm, which incorporates various forms of word matching, including exact matches, stemmed matches, and synonym matches [5]. An essential feature of METEOR is its fragmentation penalty, which applies when a hypothesis translation is restructured to align with the reference translation [1]. This fragmentation penalty quantitatively captures the minimum number of jumps a reader would need to make to find the correct word order, which corresponds to a better assessment of word order fidelity [1]. However, METEOR has its own limitations as it does not differentiate between short and long-range reordering errors, which can result in misleading evaluations when significant structural shifts occur in longer sentences [1].

Both metrics also struggle with evaluating open-ended generations or free-form text, which is a growing area of application with many potential acceptable outputs. This limitation becomes particularly pronounced when assessing instruction-tuned chat models, as the variety and flexibility of outputs can be difficult to adequately evaluate with these traditional methods [2]. Moreover, while METEOR incorporates semantic information, the reliance on synonymy modules and stemming can sometimes lead to inadequate representation of nuanced meanings, particularly in more complex sentence structures that are sensitive to word order [5].

In summary, while BLEU assesses word order concerns through n-gram matching‚Äîwhich may be insufficient in many cases‚ÄîMETEOR introduces a fragmentation penalty to address word order but still falls short of resolving the issue entirely, especially with its inability to distinguish between short and long-range errors. Both metrics are valuable, yet they have critical limitations that necessitate ongoing research for more robust evaluation models in NLP [2] [3].

1. [1]:  https://ar5iv.org/html/1502.04938, No Title
2. [2]:  https://ar5iv.org/html/2408.09235, No Title
3. [3]:  https://ar5iv.org/html/1502.04938, No Title
4. [4]:  https://ar5iv.org/html/2204.09533, [2204.09533] Evaluating Commit Message Generation: To BLEU Or Not To BLEU?
5. [5]:  https://ar5iv.org/html/2109.14895, [2109.14895] Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems
---
1. [1]:  Passage ID 1: captured, but only their proximity within a small context.The former issue is addressed to some extent by METEOR [\citenameBanerjee and Lavie2005], which relies on language-specific stemmers and synonymy modules to go beyond the surface-level similarity.As for word order, METEOR treats it separately with a fragmentation penalty proportional to the smallest number of chunks that the hypothesis must be divided into to align with the reference translation.This quantity can be interpreted as the number of times that a human reader would have to ‚Äòjump‚Äôbetween words to recover the correct translation order.However, no distinction is made between short and long-range reordering errors.The weakness of BLEU and METEOR with respect to word order was demonstrated by \nameciteBirch:10with a significant example that we report in Table¬†2.For simplicity, the example assumes that the reference order is monotonic and thathypotheses and reference translations contain exactly the same
2. [2]:  Passage ID 2: Language Processing (NLP) forward. With their widespread applications, the need for reliable evaluation methods has become increasingly critical. Such evaluations are essential to ensure these models meet quality standards, align with human expectations, and maintain safety and reliability in various applications¬†(Chang et¬†al., 2024).Conventional automated metrics such as BLEU¬†(Papineni et¬†al., 2002), ROUGE¬†(Lin, 2004), and METEOR¬†(Banerjee and Lavie, 2005) have long been employed to evaluate the performance of model generated text. However, these metrics primarily focus on surface-form similarity and often fail to account for semantically equivalent lexical and compositional diversity¬†(Zhu et¬†al., 2023; Chen et¬†al., 2021; Zhang et¬†al., 2020). Moreover, automated metrics struggle in evaluating open-ended generation or free-form text, where a wide range of acceptable outputs exists. This limitation becomes particularly evident when assessing instruction-tuned chat models, which tend
3. [3]:  Passage ID 3: guide, at least in part, their steps towards improvement.Besides, fast evaluation metrics are used to automatically tune SMT feature weights on a development corpus, for instance by means of minimum error rate training procedures [\citenameOch2003].The design of MT evaluation metrics correlating with human judgements is an active research area.Here we briefly survey two widely used general-purpose metrics, BLEU and METEOR, and then describe in more detail a number of reordering-specific metrics.3.1 General-purpose metricsBLEU [\citenamePapineni et al.2001] is a lexical match based score that represents the de facto standard for SMT evaluation.Here, proximity between candidate and reference translations is measured in terms of overlapping word nùëõn-grams, with nùëõn typically ranging from 1 to 4.For each order nùëõn a modified precision score (see \namecitePapineni:01 for details) is computed on the whole test set and combined in a geometric mean. The resulting score is then
4. [4]:  Passage ID 4: order to handle ordering, a simple bag of words representation is not suitable. Metrics such as METEOR¬†(Banerjee and Lavie, 2005) and METEOR-NEXT¬†(Denkowski and Lavie, 2010) account for the alignment of matched words in the predicted and reference sentences. The pair ‚ÄúUpdate change‚Äù and ‚ÄúUpdated changes‚Äù essentially convey the same meaning, although exact-word matchers in BLEU and ROUGE would give a score of zero instead of one. Hence, in addition to exact matches, stemmed and paraphrased matchings are also considered by METEOR and METEOR-NEXT. Banerjee and Lavie¬†(Banerjee and Lavie, 2005) convert text to lower case as a part of preprocessing in the implementation of METEOR. Punctuations are often treated as separate words by the default parsers and matchers in various programming languages which heavily affects the scores. BLEU4 geometrically averages the n-gram matches, thereby, giving zero score for pairs like ‚Äùadded chain of responsibility class diagram‚Äù and ‚Äùadded chain diagram‚Äù
5. [5]:  Passage ID 5: the Goodreads example above). Online built-in MT tools have been shown to frequently transfer the exact opposite sentiment word for some dialectical expressions in UGC translated into English (Saadany and Orasan, 2020). Therefore, the BLEU evaluation of an MT performance would be misleadingly over-permissive in such cases where only one or two sentiment-critical words are mistranslated.METEORMETEOR (Banerjee and Lavie, 2005) incorporates semantic information as it evaluates translation by calculating either exact match, stem match, or synonymy match. For synonym matching, it utilises WordNet synsets (Pedersen et¬†al., 2004). More recent versions (METEOR 1.5 and METEOR++2.0) apply also importance weighting by giving smaller weight to function words (Denkowski and Lavie, 2014; Guo and Hu, 2019). The METEOR score ranges from 0 (worst translation) to 1 (best translation). There are two shortcomings to the METEOR metric which do not make it a robust solution for evaluating sentiment