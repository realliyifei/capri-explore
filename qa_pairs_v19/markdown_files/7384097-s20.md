# A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena

## Question

How do BLEU and METEOR metrics evaluate word order, and what are their limitations in this context?

## URLs

1. https://ar5iv.org/html/2109.14895. [2109.14895] Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems
2. https://ar5iv.org/html/1601.02789. [1601.02789] Comparison and Adaptation of Automatic Evaluation Metrics for Quality Assessment of Re-Speaking
3. https://ar5iv.org/html/2306.13041. [2306.13041] Towards Explainable Evaluation Metrics for Machine Translation
4. https://ar5iv.org/html/2401.06760. [2401.06760] Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies
5. https://ar5iv.org/html/2204.09533. [2204.09533] Evaluating Commit Message Generation: To BLEU Or Not To BLEU?
6. https://ar5iv.org/html/2403.11473. [2403.11473] Word Order’s Impacts: Insights from Reordering and Generation Analysis
7. https://ar5iv.org/html/2403.08103. [2403.08103] Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data
8. https://ar5iv.org/html/2403.09727. [2403.09727] Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems
9. https://ar5iv.org/html/2208.03133. [2208.03133] Out of the BLEU: How Should We Assess Quality of the Code Generation Models?

## Answer (I don't know?: ✗)

The BLEU and METEOR metrics evaluate word order in different ways, with significant implications for their effectiveness in natural language processing tasks.

**BLEU (Bilingual Evaluation Understudy)** primarily focuses on n-gram precision, which inherently considers word order by comparing n-grams in the generated text against reference translations. However, BLEU gives no specific score to outputs that may have the same meaning but differ in word order. For example, if the reference contains "the large canine" and the generated output is "one large canine," BLEU returns a score of 0, even though the phrase is semantically similar. This demonstrates a limitation of BLEU, as it fails to capture the semantic equivalence of phrases that differ in ordering [1] [2].

**METEOR** addresses some of BLEU's weaknesses by incorporating a more sophisticated alignment process that matches generated words to reference translations using exact matches, stemming, synonyms, and paraphrase matches [2] [3]. In this process, METEOR scores the alignment based on unigram precision, recall, and an explicit ordering measure, allowing it to evaluate word order more effectively than BLEU. This means that while METEOR also considers word order, it does not do so in isolation. Instead, it calculates scores based on how many words align across various layers of matching [2] [3].

However, METEOR is not without limitations. Although it incorporates semantic matches using resources like WordNet for synonym tracking, it still struggles in certain contexts, particularly with sentiment analysis. For instance, if a few sentiment-critical words in a translation are misinterpreted, the overall scoring may not accurately reflect the translation's quality, leading to potential misinterpretation of meaning [3] [4]. Furthermore, recent iterations of METEOR, while improving some aspects, still do not fully resolve the issues related to measuring deeper semantic similarities and contextual understanding [3].

In summary, while BLEU evaluates word order through n-gram precision without considering semantic equivalence, METEOR approaches this evaluation more holistically but still faces challenges in capturing the full semantic richness of natural language. Both metrics exhibit inherent limitations that can affect their reliability in accurately assessing the quality of machine translations and generated texts, raising concerns within the NLP community regarding their continued use in evaluating state-of-the-art models [1] [2] [4].

1. [1]:  https://ar5iv.org/html/2408.09235, No Title
2. [2]:  https://ar5iv.org/html/2212.00138, No Title
3. [3]:  https://ar5iv.org/html/2109.14895, [2109.14895] Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems
4. [4]:  https://ar5iv.org/html/2306.13041, [2306.13041] Towards Explainable Evaluation Metrics for Machine Translation
5. [5]:  https://ar5iv.org/html/2306.13041, [2306.13041] Towards Explainable Evaluation Metrics for Machine Translation
---
1. [1]:  Passage ID 1: Language Processing (NLP) forward. With their widespread applications, the need for reliable evaluation methods has become increasingly critical. Such evaluations are essential to ensure these models meet quality standards, align with human expectations, and maintain safety and reliability in various applications (Chang et al., 2024).Conventional automated metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) have long been employed to evaluate the performance of model generated text. However, these metrics primarily focus on surface-form similarity and often fail to account for semantically equivalent lexical and compositional diversity (Zhu et al., 2023; Chen et al., 2021; Zhang et al., 2020). Moreover, automated metrics struggle in evaluating open-ended generation or free-form text, where a wide range of acceptable outputs exists. This limitation becomes particularly evident when assessing instruction-tuned chat models, which tend
2. [2]:  Passage ID 2: is “one large canine”. BLEU gives a score of 0, even though the phrases are essentially semantically equivalent.METEOR Lavie and Agarwal (2007) is an automated evaluation metric for evaluating machine translation that addresses these shortcomings of BLEU. It performs scoring by aligning hypotheses to one or more reference translations. A monolingual word aligner aligns words and phrases in successive stages: exact match, by stem, by synonym, and by paraphrase. METEOR then scores the computed alignment using unigram precision, recall, and a measure of explicit ordering. METEOR correlates well with human judgment at the sentence-level, and is another commonly-used metric for evaluating machine translation.8 ConclusionLet us conclude by recapping what we have covered in this tutorial. We began by formalizing the tasks of word alignment and statistical machine translation, finding that they are intrinsically linked. We the turned to neural machine translation, taking a detour
3. [3]:  Passage ID 3: the Goodreads example above). Online built-in MT tools have been shown to frequently transfer the exact opposite sentiment word for some dialectical expressions in UGC translated into English (Saadany and Orasan, 2020). Therefore, the BLEU evaluation of an MT performance would be misleadingly over-permissive in such cases where only one or two sentiment-critical words are mistranslated.METEORMETEOR (Banerjee and Lavie, 2005) incorporates semantic information as it evaluates translation by calculating either exact match, stem match, or synonymy match. For synonym matching, it utilises WordNet synsets (Pedersen et al., 2004). More recent versions (METEOR 1.5 and METEOR++2.0) apply also importance weighting by giving smaller weight to function words (Denkowski and Lavie, 2014; Guo and Hu, 2019). The METEOR score ranges from 0 (worst translation) to 1 (best translation). There are two shortcomings to the METEOR metric which do not make it a robust solution for evaluating sentiment
4. [4]:  Passage ID 4: multiple high-quality evaluation metrics in recent years (Zhao et al. 2019; Zhang et al. 2020a; Rei et al. 2020; Sellam et al. 2020; Yuan et al. 2021), the Natural Language Processing (NLP) community appears hesitant to adopt them for assessing NLG systems (Marie et al. 2021; Gehrmann et al. 2023). Empirical investigations of Marie et al. (2021) indicate that the majority of MT papers relies on surface-level evaluation metrics such as BLEU and METEOR (Papineni et al. 2002; Banerjee and Lavie 2005), which were created two decades ago, a trend that may allegedly have worsened in recent times.These surface-level metrics cannot (even) measure semantic similarity of their inputs and are thus fundamentally flawed, particularly when it comes to assessing the quality of recent state-of-the-art MT systems (e.g. Peyrard 2019; Freitag et al. 2022),raising concerns about the credibility of the scientific field.We argue that the potential reasons for this neglect of recent high-quality metrics
5. [5]:  Passage ID 5: multiple high-quality evaluation metrics in recent years (Zhao et al. 2019; Zhang et al. 2020a; Rei et al. 2020; Sellam et al. 2020; Yuan et al. 2021), the Natural Language Processing (NLP) community appears hesitant to adopt them for assessing NLG systems (Marie et al. 2021; Gehrmann et al. 2023). Empirical investigations of Marie et al. (2021) indicate that the majority of MT papers relies on surface-level evaluation metrics such as BLEU and METEOR (Papineni et al. 2002; Banerjee and Lavie 2005), which were created two decades ago, a trend that may allegedly have worsened in recent times.These surface-level metrics cannot (even) measure semantic similarity of their inputs and are thus fundamentally flawed, particularly when it comes to assessing the quality of recent state-of-the-art MT systems (e.g. Peyrard 2019; Freitag et al. 2022),raising concerns about the credibility of the scientific field.We argue that the potential reasons for this neglect of recent high-quality metrics