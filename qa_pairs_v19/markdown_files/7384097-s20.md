# A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena

## Question

How do BLEU and METEOR metrics evaluate word order, and what are their limitations in this context?

## URLs

1. https://ar5iv.org/html/2109.14895. [2109.14895] Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems
2. https://ar5iv.org/html/1601.02789. [1601.02789] Comparison and Adaptation of Automatic Evaluation Metrics for Quality Assessment of Re-Speaking
3. https://ar5iv.org/html/2306.13041. [2306.13041] Towards Explainable Evaluation Metrics for Machine Translation
4. https://ar5iv.org/html/2401.06760. [2401.06760] Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies
5. https://ar5iv.org/html/2204.09533. [2204.09533] Evaluating Commit Message Generation: To BLEU Or Not To BLEU?
6. https://ar5iv.org/html/2403.11473. [2403.11473] Word Order’s Impacts: Insights from Reordering and Generation Analysis
7. https://ar5iv.org/html/2403.08103. [2403.08103] Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data
8. https://ar5iv.org/html/2403.09727. [2403.09727] Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems
9. https://ar5iv.org/html/2208.03133. [2208.03133] Out of the BLEU: How Should We Assess Quality of the Code Generation Models?

## Answer (I don't know?: ✗)

BLEU and METEOR are both automated evaluation metrics commonly used in natural language processing (NLP) to assess the quality of machine-generated translations or text. They each measure different aspects of translation quality, including how they handle word order.

**BLEU Evaluation of Word Order**: BLEU (Bilingual Evaluation Understudy) evaluates the overlap of n-grams between generated text and reference text. This means it looks for sequences of n words in the output that match those in the reference. The scoring of BLEU relies heavily on exact matches, which can lead to a zero score for sentences that may convey similar meanings but have different word orders. For example, the phrases "added chain of responsibility class diagram" and "added chain diagram" would receive a low BLEU score despite their similar semantic content because of differences in word arrangement and n-gram overlap, thus emphasizing its limitations in capturing the importance of word order in conveying meaning [1] [4].

**METEOR Evaluation of Word Order**: In contrast, METEOR (Metric for Evaluation of Translation with Explicit ORdering) addresses some of the limitations of BLEU by integrating multiple forms of matching, including exact matches, stem matches (which consider different forms of a word), and synonym matches. METEOR evaluates the alignment of matched words more effectively compared to BLEU. It performs this by aligning words based on their positions and accounting for variations in word forms. However, while METEOR provides a more nuanced approach to evaluating translations, it still exhibits limitations; notably, the potential neglect of semantic similarities and the quality of matches can introduce inaccuracies. The authors mention that METEOR does not robustly evaluate sentiment, particularly for cases where sentiment-critical words are mismatched in translation, indicating that its evaluation can still be surface-level and flawed [1] [2] [3].

**Limitations of Both Metrics**: Despite advancements in evaluating word order and semantic representations, both BLEU and METEOR rely heavily on surface-level metrics. They cannot adequately measure semantic similarity or contextual relevance, which is essential for modern state-of-the-art machine translation systems [3]. This fundamental flaw raises concerns about the robustness and reliability of using these metrics in advanced NLP applications where deeper understanding of language is required [3] [5]. 

In summary, while BLEU and METEOR offer different mechanisms for evaluating word order—BLEU through n-gram matching and METEOR through a broader semantic lens—their limitations in handling nuanced meanings and contexts highlight the ongoing challenges in developing reliable evaluation metrics in the field of NLP.

1. [1]:  https://ar5iv.org/html/2204.09533, [2204.09533] Evaluating Commit Message Generation: To BLEU Or Not To BLEU?
2. [2]:  https://ar5iv.org/html/2109.14895, [2109.14895] Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems
3. [3]:  https://ar5iv.org/html/2306.13041, [2306.13041] Towards Explainable Evaluation Metrics for Machine Translation
4. [4]:  https://ar5iv.org/html/2403.08103, [2403.08103] Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data
5. [5]:  https://ar5iv.org/html/2204.09533, [2204.09533] Evaluating Commit Message Generation: To BLEU Or Not To BLEU?
---
1. [1]:  Passage ID 1: order to handle ordering, a simple bag of words representation is not suitable. Metrics such as METEOR (Banerjee and Lavie, 2005) and METEOR-NEXT (Denkowski and Lavie, 2010) account for the alignment of matched words in the predicted and reference sentences. The pair “Update change” and “Updated changes” essentially convey the same meaning, although exact-word matchers in BLEU and ROUGE would give a score of zero instead of one. Hence, in addition to exact matches, stemmed and paraphrased matchings are also considered by METEOR and METEOR-NEXT. Banerjee and Lavie (Banerjee and Lavie, 2005) convert text to lower case as a part of preprocessing in the implementation of METEOR. Punctuations are often treated as separate words by the default parsers and matchers in various programming languages which heavily affects the scores. BLEU4 geometrically averages the n-gram matches, thereby, giving zero score for pairs like ”added chain of responsibility class diagram” and ”added chain diagram”
2. [2]:  Passage ID 2: the Goodreads example above). Online built-in MT tools have been shown to frequently transfer the exact opposite sentiment word for some dialectical expressions in UGC translated into English (Saadany and Orasan, 2020). Therefore, the BLEU evaluation of an MT performance would be misleadingly over-permissive in such cases where only one or two sentiment-critical words are mistranslated.METEORMETEOR (Banerjee and Lavie, 2005) incorporates semantic information as it evaluates translation by calculating either exact match, stem match, or synonymy match. For synonym matching, it utilises WordNet synsets (Pedersen et al., 2004). More recent versions (METEOR 1.5 and METEOR++2.0) apply also importance weighting by giving smaller weight to function words (Denkowski and Lavie, 2014; Guo and Hu, 2019). The METEOR score ranges from 0 (worst translation) to 1 (best translation). There are two shortcomings to the METEOR metric which do not make it a robust solution for evaluating sentiment
3. [3]:  Passage ID 3: multiple high-quality evaluation metrics in recent years (Zhao et al. 2019; Zhang et al. 2020a; Rei et al. 2020; Sellam et al. 2020; Yuan et al. 2021), the Natural Language Processing (NLP) community appears hesitant to adopt them for assessing NLG systems (Marie et al. 2021; Gehrmann et al. 2023). Empirical investigations of Marie et al. (2021) indicate that the majority of MT papers relies on surface-level evaluation metrics such as BLEU and METEOR (Papineni et al. 2002; Banerjee and Lavie 2005), which were created two decades ago, a trend that may allegedly have worsened in recent times.These surface-level metrics cannot (even) measure semantic similarity of their inputs and are thus fundamentally flawed, particularly when it comes to assessing the quality of recent state-of-the-art MT systems (e.g. Peyrard 2019; Freitag et al. 2022),raising concerns about the credibility of the scientific field.We argue that the potential reasons for this neglect of recent high-quality metrics
4. [4]:  Passage ID 4: validation, and test sets, allowing us to evaluate the performance of our model and ensure it generalizes well to unseen data.5 Experiments5.1 MetricsWe used two metrics to evaluate our models: BLEU and METEOR. These metrics are commonly used to assess the quality of generated text by comparing it to a reference text.5.1.1 BLEUBLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of machine-generated translations. It measures the overlap of n-grams between the generated text and the reference text. The BLEU score ranges from 0 to 1, with 1 being the best score.Formula:B​L​E​U=min⁡(1,l​e​n​(p​r​e​d​i​c​t​i​o​n)l​e​n​(r​e​f​e​r​e​n​c​e))×exp⁡(∑n=1Nwn​log⁡pn)𝐵𝐿𝐸𝑈1𝑙𝑒𝑛𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛𝑙𝑒𝑛𝑟𝑒𝑓𝑒𝑟𝑒𝑛𝑐𝑒superscriptsubscript𝑛1𝑁subscript𝑤𝑛subscript𝑝𝑛BLEU=\min\left(1,\frac{len(prediction)}{len(reference)}\right)\times\exp\left(\sum_{n=1}^{N}w_{n}\log p_{n}\right)where l​e​n​(p​r​e​d​i​c​t​i​o​n)𝑙𝑒𝑛𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛len(prediction) is the length of the predicted
5. [5]:  Passage ID 5: simple and easy to compute automated evaluation metrics such as BLEU4 or its variants. The advances in the field of Machine Translation (MT) indicate several weaknesses of BLEU4 and its variants. They also propose several other metrics for evaluating Natural Language Generation (NLG) tools. In this work, we discuss the suitability of various MT metrics for the CMG task. Based on the insights from our experiments, we propose a new variant specifically for evaluating the CMG task. We re-evaluate the state of the art CMG tools on our new metric. We believe that our work fixes an important gap that exists in the understanding of evaluation metrics for CMG research.BLEU, METEOR, Commit Message Generation††journalyear: 2022††copyright: acmcopyright††conference: New Ideas and Emerging Results ; May 21–29, 2022; Pittsburgh, PA, USA††booktitle: New Ideas and Emerging Results (ICSE-NIER’22), May 21–29, 2022, Pittsburgh, PA, USA††price: 15.00††doi: 10.1145/3510455.3512790††isbn: