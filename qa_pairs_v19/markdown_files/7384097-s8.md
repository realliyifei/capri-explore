# A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena

## Question

How do syntax-based Statistical Machine Translation (SMT) approaches utilize tree structures differently?

## URLs

1. https://ar5iv.org/html/1704.04743. [1704.04743] Towards String-to-Tree Neural Machine Translation
2. https://ar5iv.org/html/2103.11089. [2103.11089] Untitled Document
3. https://ar5iv.org/html/2009.13398. [2009.13398] Aspects of Terminological and Named Entity Knowledge within Rule-Based Machine Translation Models for Under-Resourced Neural Machine Translation Scenarios
4. https://ar5iv.org/html/1809.01854. [1809.01854] Top-down Tree Structured Decoding with Syntactic Connections for Neural Machine Translation and Parsing
5. https://ar5iv.org/html/1502.01682. [1502.01682] Use of Modality and Negation in Semantically-Informed Syntactic MT
6. https://ar5iv.org/html/2302.03908. [2302.03908] Syntax and Domain Aware Model for Unsupervised Program Translation
7. https://ar5iv.org/html/1608.04631. [1608.04631] Neural versus Phrase-Based Machine Translation Quality: a Case Study
8. https://ar5iv.org/html/2412.05159. [2412.05159] Enhancing Cross-Language Code Translation via Task-Specific Embedding Alignment in Retrieval-Augmented Generation

## Answer (I don't know?: ✗)

Syntax-based Statistical Machine Translation (SMT) approaches, notably string-to-tree (s2t) models, leverage tree structures to ensure that translations maintain valid syntactic forms while allowing for flexible reordering between the source and target languages. This is distinct from traditional methods, where syntax may not be explicitly represented.

1. **Usage of Trees**: In s2t SMT, a source-language string is translated into a target-language tree. The primary advantage of this approach is that it inherently preserves the syntactic structure of the target language, which is not typically prioritized in standard neural machine translation methods. This helps generate more syntactically sound translations compared to those produced by models that do not utilize explicit syntactic representations [1][5].

2. **Modeling and Training**: The s2t model is trained on parallel corpora where the target sentences are automatically parsed into syntactic trees. These trees allow statistical models to encode complex relationships between the source and target languages at a structural level, enabling the model to consider hierarchical relationships within sentences rather than simply processing them linearly [4][5]. Synchronous transduction grammar (STSG) rules, such as those derived from GHKM rules, further enhance this by enabling reordering at all levels of the parse tree [5].

3. **Representation Differences**: While traditional phrase-based SMT may use phrases as units of translation, s2t approaches involve translating into structured representations that account for the grammatical hierarchy of sentences. For example, while a standard model may merely translate word by word or phrase by phrase, the s2t approach emphasizes the construction of trees that represent the grammatical structure of sentences, facilitating a more nuanced understanding of both syntax and semantics [1][5].

4. **Flexibility and Naturalness**: The flexibility provided by the tree structures in s2t SMT contrasts with more rigid translation mechanisms. By allowing for various forms of syntactic rearrangement, these models can better handle the diverse ways that languages express similar ideas, thus aiming to produce translations that sound more natural in the target language [4][5].

In summary, syntax-based SMT approaches, especially string-to-tree models, adopt a fundamentally different framework by emphasizing tree structures that capture syntactic relationships and foster flexible reordering, embarking on a more linguistically-informed translation strategy compared to traditional phrase-based methods. This contrasts with the more straightforward, often linear nature of representations found in standard NMT systems.

1. [1]:  https://ar5iv.org/html/1704.04743, [1704.04743] Towards String-to-Tree Neural Machine Translation
2. [2]:  https://ar5iv.org/html/2009.13398, [2009.13398] Aspects of Terminological and Named Entity Knowledge within Rule-Based Machine Translation Models for Under-Resourced Neural Machine Translation Scenarios
3. [3]:  https://ar5iv.org/html/2302.03908, [2302.03908] Syntax and Domain Aware Model for Unsupervised Program Translation
4. [4]:  https://ar5iv.org/html/1704.04743, [1704.04743] Towards String-to-Tree Neural Machine Translation
5. [5]:  https://ar5iv.org/html/1704.04743, [1704.04743] Towards String-to-Tree Neural Machine Translation
---
1. [1]:  Passage ID 1: baseline. A small-scale human evaluation also showed an advantage to the syntax-aware system.1 Introduction and ModelNeural Machine Translation (NMT)Kalchbrenner and Blunsom (2013); Sutskever et al. (2014); Bahdanau et al. (2014) has recently became the state-of-the-art approach to machine translation Bojar et al. (2016), while being much simpler than the previously dominant phrase-based statistical machine translation (SMT) approaches Koehn (2010). NMT models usually do not make explicit use of syntactic information about the languages at hand. However, a large body of work was dedicated to syntax-based SMT Williams et al. (2016). One prominent approach to syntax-based SMT is string-to-tree (s2t) translation Yamada and Knight (2001, 2002), in which a source-language string is translated into a target-language tree. s2t approaches to SMT help to ensure the resulting translations have valid syntactic structure, while also mediating flexible reordering between the source and
2. [2]:  Passage ID 2: NMT models trained with Stanford or IXA POS tags as additional information.3.2 Leveraging Syntactic Tree InformationIn addition to the direct use of the linguistic knowledge for the lexicon entries, the grammars (monolingual and bilingual lexicons) were indirectly used by exploring the results of each internal intermediate stage of the translation process, which Lucy LT expresses as annotated trees.For example, the sentence parsed in Figure 2,I own the house down the streetis encoded as\lParen\lParen\lParenI own \lParen\lParen\lParenthe house\rParen\rParen\rParen \lParen\lParen\lParendown \lParen\lParen\lParenthe street\rParen​\rParen​\rParen\rParen\rParen\rParen\rParen\rParen\rParen.222To avoid collisions with parenthesis in the text, we used the left (\lParen\lParen\lParen, U+2985) and right (\rParen\rParen\rParen, U+2986) white parenthesis.We use this representation as source text when training the NMT models, as sequence-to-sequence deep neural network models do not
3. [3]:  Passage ID 3: 28, 8, 21]. Based on the naturalness theory of programming languages, i.e., source code is repetitive and predictable, statistical machine learning methods have been used for modeling programs and code migration. Some previous work borrowed the idea from the natural language translation field and applied statistical machine translation (SMT) approaches [20] for code migration [27, 28]. Among these studies, researchers build statistical models and optimize the parameters by analyzing the statistical relationships between bilingual data. Then they employ these statistical models to generate translations.With the development of deep learning technologies, in recent years, researchers have started to tackle program translation task using supervised sequence-to-sequence neural machine translation (NMT) methods [6], which need a large amount of parallel code resources to train. However, parallel resources are much more scarce in the programming language domain than in natural language.It
4. [4]:  Passage ID 4: tree.Note that the linearized trees we predict are different in their structure from those in Vinyals et al. (2015) as instead of having part of speech tags as terminals, they contain the words of the translated sentence. We intentionally omit the POS information as including it would result in significantly longer sequences. The s2t model is trained on parallel corpora in which the target sentences are automatically parsed. Since this modeling keeps the form of a sequence-to-sequence learning task, we can employ the conventional attention-based sequence to sequence paradigm Bahdanau et al. (2014) as-is, while enriching the output with syntactic information.Related Work Some recent works did propose to incorporate syntactic or other linguistic knowledge into NMT systems, although mainly on the source side: Eriguchi et al. (2016a, b) replace the encoder in an attention-based model with a Tree-LSTM Tai et al. (2015) over a constituency parse tree; Bastings et al. (2017) encoded
5. [5]:  Passage ID 5: translated into a target-language tree. s2t approaches to SMT help to ensure the resulting translations have valid syntactic structure, while also mediating flexible reordering between the source and target languages. The main formalism driving current s2t SMT systems is GHKM rules Galley et al. (2004, 2006), which are synchronous transduction grammar (STSG) fragments, extracted from word-aligned sentence pairs with syntactic trees on one side. The GHKM translation rules allow flexible reordering on all levels of the parse-tree.We suggest that NMT can also benefit from the incorporation of syntactic knowledge, and propose a simple method of performing string-to-tree neural machine translation. Our method is inspired by recent works in syntactic parsing, which model trees as sequences Vinyals et al. (2015); Choe and Charniak (2016). Namely, we translate a source sentence into a linearized, lexicalized constituency tree, as demonstrated in Figure 2. Figure 1 shows a translation from