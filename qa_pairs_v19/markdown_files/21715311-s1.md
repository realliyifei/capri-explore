# A Survey on Automatically-Constructed WordNets and their Evaluation: Lexical and Word Embedding-based Approaches

## Question

Why is the expansion approach often preferred for constructing WordNets, and how does it facilitate automatic synset construction?

## URLs

1. https://ar5iv.org/html/2403.20215. [2403.20215] Advancing the Arabic WordNet: Elevating Content Quality
2. https://ar5iv.org/html/2412.02138. [2412.02138] Misalignment of Semantic Relation Knowledge between WordNet and Human Intuition
3. https://ar5iv.org/html/2410.23133. [2410.23133] Crowdsourcing Lexical Diversity
4. https://ar5iv.org/html/2211.12328. [2211.12328] A survey on knowledge-enhanced multimodal learning
5. https://ar5iv.org/html/2404.12698. [2404.12698] Neural Semantic Parsing with Extremely Rich Symbolic Meaning Representations
6. https://ar5iv.org/html/2402.12011. [2402.12011] A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change Submitted to NAACL 2024

## Answer (I don't know?: ✗)

The expansion approach is often preferred for constructing WordNets because it allows for the localization and translation of existing synsets from a foundational resource, such as the English Princeton WordNet (PWN), into the target language. This method is particularly advantageous as it helps maintain semantic consistency while addressing linguistic diversity across languages. In this model, synsets from PWN are not merely copied; instead, they are adapted to fit the lexical and cultural context of the target language, which facilitates a more accurate representation of meaning [2][5].

One key advantage of the expansion approach is its reliance on the pre-existing structure and comprehensiveness of the PWN, which serves as a backbone for constructing WordNets in other languages. By using English synsets as a starting point, the expansion method allows linguists and language experts to systematically translate and adapt these synsets into the target language [2]. This is exemplified by the construction of the Polish WordNet, which employed the expansion method, exhibiting how existing resources can be localized effectively [3]. Furthermore, this methodology can help identify and fill lexical gaps—concepts that may not have direct translations but are culturally or contextually significant.

Automatic synset construction is facilitated through the expansion approach by leveraging existing tools and technologies that can handle translations and language mappings. As noted, the methodology benefits from the input and validation from language experts who can ensure that the translated synsets accurately reflect both lexical meaning and cultural connotations [1]. By incorporating expert judgments, the expansion approach enhances the quality of the resulting lexicon, addressing issues of correctness and completeness that can arise when using solely automated methods for lexical resource construction [3][4].

Moreover, the expansion model plays a crucial role in establishing a structured framework for automatic construction. With a repository of well-defined synsets in PWN, tools can automatically generate equivalent synsets for target languages, which are then reviewed and refined by language experts, thereby balancing efficiency with linguistic accuracy [1][5]. This dual approach not only enables broad scalability in developing WordNets across various languages but also supports the creation of language-specific resources that are culturally aware and contextually relevant [4]. 

In summary, the expansion approach is favored due to its ability to leverage existing lexical structures while allowing for cultural localization and expert validation. This method enhances the efficacy of automatic synset construction and ensures the development of more nuanced and accurate lexical resources in computational linguistics [1][2][4].

1. [1]:  https://ar5iv.org/html/2410.23133, [2410.23133] Crowdsourcing Lexical Diversity
2. [2]:  https://ar5iv.org/html/2403.20215, [2403.20215] Advancing the Arabic WordNet: Elevating Content Quality
3. [3]:  https://ar5iv.org/html/2403.20215, [2403.20215] Advancing the Arabic WordNet: Elevating Content Quality
4. [4]:  https://ar5iv.org/html/2410.23133, [2410.23133] Crowdsourcing Lexical Diversity
5. [5]:  https://ar5iv.org/html/2410.23133, [2410.23133] Crowdsourcing Lexical Diversity
---
1. [1]:  Passage ID 1: For example, in Freihat et al. (2024), we adopted a language expert-based methodology involving two translators and an Arabic language expert to improve the quality of the Arabic wordnet. This approach addressed multiple aspects of lexico-semantic resource quality (Khalilia et al., 2021) by adding missing information and correcting errors, with a focus on handling language diversity and untranslatability. Specifically, we expanded the structure of the wordnet to include phrasets, which provide approximate phrase-level translations, and lexical gaps, representing untranslatable concepts.Our methodology was validated on the Arabic wordnet, resulting in AWN V3, which includes 9,576 synsets, 236 gaps, and 701 phrasets.Similarly, Bella et al. (2020) applied the expert-sourced expansion methodology in the development of the Scottish Gaelic wordnet. A total of 10,583 English synsets from the Princeton WordNet were translated and validated by Gaelic language experts, and then merged with
2. [2]:  Passage ID 2: such as machine translation Poibeau (2017), information retrieval Nie (2022), or word sense disambiguation Navigli (2009).The English Princeton WordNet (PWN) (Miller, 1995), as the first wordnet, has been adapted and employed as a foundation for constructing wordnets in other languages.In general, WordNets are constructed using either the merge or the expand model Vossen (1998). In the merge model, synsets are initially created from pre-existing resources (e.g., dictionaries) in a language. Then, for translability into other languages, the synsets have to be aligned with equivalent English synsets in PWN. For example, the IndoWordNet (Bhattacharyya, 2010) was built following this model. In the expand model, PWN synsets are ‘localized’ or ‘translated’ into target languages. For example, the Polish WordNet (Piasecki et al., 2009) was constructed using this model. In either case, when mapping across languages, the PWN synsets (and thus the English language) are usually used as a
3. [3]:  Passage ID 3: the Polish WordNet (Piasecki et al., 2009) was constructed using this model. In either case, when mapping across languages, the PWN synsets (and thus the English language) are usually used as a pivot when translating words across languages.Wordnets often suffer from quality issues, in a large part due to the use of automated and semi-automated methods for building them Khalilia et al. (2021a, b). In addition, mistakes can be hard to detect as most wordnets do not contain glosses or example sentences. The above are true of the existing Arabic wordnets. The first Arabic wordnet (AWN V1) was built following the expand model (Elkateb et al., 2006) and includes 9,618 synsets translated from PWN to modern standard Arabic. Its second version (AWN V2) (Regragui et al., 2016) extended AWN V1 to 11,269 synsets and was developed using a semi-automatic method and the expand model. As we show in our paper, both wordnets suffer from correctness and completeness issues, and lack glosses and
4. [4]:  Passage ID 4: discusses the results and presents the resulting resource. Finally, we provide conclusions in Section 7.2. Related WorkLarge-scale lexical databases in computational linguistics and NLP have traditionally been developed using three primary approaches: expert-sourced, automated, and crowdsourcing methods. While the expert-sourced and automated methods are key for building diversity-aware lexicons (as discussed in Section 2.1), the crowdsourcing approach is also employed to develop linguistic resources (detailed in Section 2.2).2.1. Development of Diversity-Aware Lexical DatabasesDespite the significant costs and time constraints, the expert-sourced approach remains the most established and widely used in lexical semantics research for constructing culturally aware lexical resources. For example, in Freihat et al. (2024), we adopted a language expert-based methodology involving two translators and an Arabic language expert to improve the quality of the Arabic wordnet. This
5. [5]:  Passage ID 5: and large language models in particular, lexical resources continue to be used as integral parts of NLP applications such as machine translation, word sense disambiguation, or information retrieval (Katsuta and Yamamoto, 2020; Campolungo et al., 2022; Loureiro and Jorge, 2019; Barbouch et al., 2021).Due to its dominance, the English language and, in particular, the English Princeton WordNet (PWN) (Miller, 1995), has played a distinguished role in the construction of lexical databases for many languages. The English lexicon has been widely adopted as a pivot representation of lexical meaning across languages, but also as the source language for translation-based lexicon development (Bond and Foster, 2013).Relying on the English lexicon as a “standard” meaning inventory, however, leads to the creation of resources that suffer from language modeling bias, due to deep-running linguistic and cultural differences across speaker communities (Giunchiglia et al., 2017, 2023; Bella et al.,