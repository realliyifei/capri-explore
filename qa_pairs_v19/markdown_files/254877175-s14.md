# A Survey of Deep Learning for Mathematical Reasoning

## Question

How do recent studies assess the effectiveness of subword tokenization techniques in handling numeracy within language models?

## URLs

1. https://ar5iv.org/html/2406.11687. [2406.11687] Tokenization Falling Short: hTe Cusre of Tkoeniaztion
2. https://ar5iv.org/html/2403.11130. [2403.11130] Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models
3. https://ar5iv.org/html/2310.06204. [2310.06204] Estimating Numbers without Regression
4. https://ar5iv.org/html/2310.11628. [2310.11628] Learn Your Tokens: Word-Pooled Tokenization for Language Modeling
5. https://ar5iv.org/html/2402.14903. [2402.14903] 1 Illustrating the dependence of frontier model arithmetic performance on tokenization. We show how using commas can enforce right-to-left (R2L) tokenization for the same addition problem. R2L tokenization leads to improved model performance on both GPT-3.5 and GPT-4 (March 2023 models), which we show is due to tokenization alignment between addends and answer through various controls and error analyses.
6. https://ar5iv.org/html/2410.14166. [2410.14166] LLM The Genius Paradox: A Linguistic and Math Expert’s Struggle with Simple Word-based Counting Problems
7. https://ar5iv.org/html/2402.18376. [2402.18376] Tokenization Is More Than Compression

## Answer (I don't know?: ✗)

Recent studies have critically assessed the effectiveness of subword tokenization techniques in addressing numeracy challenges within language models (LMs). Traditional subword tokenization, while standard, has been found to lead to inadequate numerical understanding. This inadequacy is highlighted by research analyzing different approaches to number representation, which include notational changes, vocabulary adjustments, and architectural modifications [4] [5].

One of the significant findings from these studies is that the conventional tokenization methods, typically based on subword units, do not adequately capture numerical representations. Instead, they propose that representing numbers using a tokenization schema aligned with a log-scaled number line can substantially enhance LMs' ability to work with numerical data. This method has shown noteworthy improvements in approximate numeracy, achieving results comparable to or exceeding current state-of-the-art techniques with minimal modifications to the LMs themselves [3] [4]. This represents a shift from prior beliefs that suggested maintaining the architecture of the LMs was essential for enhancing their numeracy capabilities [5].

Furthermore, researchers have noted the benefits of using specific encoding strategies, such as exponent embeddings, which also leverage techniques from existing frameworks to make predictions about numbers more effectively. For instance, tokenizing numbers on a number line facilitates better performance in masked word prediction tasks and supports approximate numeracy enhancements without necessitating an architecture overhaul [3] [4]. This is particularly relevant as it provides a feasible approach for improving numeracy that does not depend heavily on users’ access to the model’s internal workings or extensive fine-tuning regimes [5].

Moreover, studies by Thawani et al. (2021) emphasize the need to evaluate language models not only on their exact arithmetic competencies but also on their ability to approximate numeracy, especially in contexts like Fermi Problems where estimations play a pivotal role [2] [3]. This dual focus on approximate and exact numeracy allows researchers and practitioners to better assess and utilize LMs in applications that require numerical reasoning.

In summary, recent studies indicate a paradigm shift in tackling numeracy within LMs through innovative tokenization methods that prioritize numerical representation accuracy. By steering away from traditional subword techniques towards more sophisticated methods, such as number line tokenization, researchers are discovering more effective strategies that enhance the capacity of language models to understand and manipulate numerical data. These findings advocate for more attention to model training and comprehensive capability evaluations during performance assessments of NLP models [1] [4].

1. [1]:  https://ar5iv.org/html/2410.14166, [2410.14166] LLM The Genius Paradox: A Linguistic and Math Expert’s Struggle with Simple Word-based Counting Problems
2. [2]:  https://ar5iv.org/html/2310.11628, [2310.11628] Learn Your Tokens: Word-Pooled Tokenization for Language Modeling
3. [3]:  https://ar5iv.org/html/2310.06204, [2310.06204] Estimating Numbers without Regression
4. [4]:  https://ar5iv.org/html/2310.06204, [2310.06204] Estimating Numbers without Regression
5. [5]:  https://ar5iv.org/html/2310.06204, [2310.06204] Estimating Numbers without Regression
---
1. [1]:  Passage ID 1: et al., 2023; Chen et al., 2024), etc. We also show inability of specialized math or coding LLMs to transfer advanced capabilities to much simpler tasks, calling for more attention and research in model capability acquisition during training and comprehensive capability evaluation during benchmarking. Lastly, we find effectiveness of reasoning strategies to help elicit knowledge and problem-solving capabilities from LLMs, highlighting importance of cultivating consciousness of reasoning during model pretraining.2 BackgroundWe introduce related work in Section A.1.2.1 TokenizationWord-based tokenization algorithms used in earlier non-transformer models such as Word2Vec (Mikolov, 2013), FastText (Bojanowski et al., 2017) and GloVe (Pennington et al., 2014), split texts into words (probably with some extra rules) and find numerical representation for each of them. Words that are unseen in the training corpus or ignored due to limited vocabulary size are typically represented
2. [2]:  Passage ID 2: the literacy of language models.In Proceedings of the 2021 Conference on Empirical Methods inNatural Language Processing, pages 6960–6967, Online and Punta Cana,Dominican Republic. Association for Computational Linguistics.Thawani et al. (2021b)Avijit Thawani, Jay Pujara, Filip Ilievski, and Pedro Szekely.2021b.Representingnumbers in NLP: a survey and a vision.In Proceedings of the 2021 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human LanguageTechnologies, pages 644–656, Online. Association for ComputationalLinguistics.Wallace et al. (2019)Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019.Do NLP models knownumbers? probing numeracy in embeddings.In Proceedings of the 2019 Conference on Empirical Methods inNatural Language Processing and the 9th International Joint Conference onNatural Language Processing (EMNLP-IJCNLP), pages 5307–5315, Hong Kong,China. Association for
3. [3]:  Passage ID 3: changes to the underlying language model.5 Related workWe restrict our analysis to the task of approximately decoding numbers in MNP setting, which requires different methods and metrics from the tasks that instead evaluate their exact arithmetic skills (Thawani et al., 2021b).The method we highlight in this paper i.e. change of vocabulary to tokenize numbers on a log-scaled number line, has been previously used in different settings.Others have shown the benefits of using such exponent embeddings as number encoders for language models, whether it be for the task of masked number prediction (Berg-Kirkpatrick andSpokoyny, 2020) or masked word prediction (Thawani et al., 2021a). Our work extends these results with further evidence of the representational power gained by simply tokenizing numbers on the number line.Our simple intervention to improve approximate numeracy in LMs is also related to other work Chen et al. (2022) which aims to improve exact numeracy of LMs
4. [4]:  Passage ID 4: tokenizing numbers on the number line.Our simple intervention to improve approximate numeracy in LMs is also related to other work Chen et al. (2022) which aims to improve exact numeracy of LMs without any architecture change.6 ConclusionSubword tokenization, the standard approach to representing numbers leads to inaccurate numerical understanding.In this work, we analyze number representation approaches that make notational (e.g. scientific vs. decimal), vocabulary (i.e. tokenizing on the number line), and architectural changes (i.e. regressing to the number).We find that tokenization on the number line achieves near or better than state-of-the-artresults while requiring minimal intervention to the language model.This is a negative insight against recent results in the community which suggest that language models must be architecturally modified to gain numeracy.It will allow language models to conveniently improve their numeracy, including cases where users may not
5. [5]:  Passage ID 5: which suggest that language models must be architecturally modified to gain numeracy.It will allow language models to conveniently improve their numeracy, including cases where users may not have access to the model’s architecture and are only provided a typical finetuning regime with small changes to the tokenizer’s vocabulary.Finally, we find similar trends in the challenging setting of numerical fact estimation for solving Fermi Problems – indicating that vocabulary-change is sufficient to represent approximate numbers effectively with minimal effort.7 AcknowledgementsThis work was funded by the Defense Advanced Research Projects Agency with award N660011924033.We would like to thank Peter Clark (AI2) for insightful discussions on the project, and theanonymous reviewers at EACL 2023 and Negative Insights workshop for helping us refine earlier versions of this paper.8 Ethics and LimitationsOur findings and recommendations may not apply beyond the English language