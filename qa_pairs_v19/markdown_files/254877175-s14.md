# A Survey of Deep Learning for Mathematical Reasoning

## Question

How do recent studies assess the effectiveness of subword tokenization techniques in handling numeracy within language models?

## URLs

1. https://ar5iv.org/html/2406.11687
2. https://ar5iv.org/html/2403.11130
3. https://ar5iv.org/html/2310.06204
4. https://ar5iv.org/html/2310.11628
5. https://ar5iv.org/html/2402.14903
6. https://ar5iv.org/html/2410.14166
7. https://ar5iv.org/html/2402.18376

## Answer

## I don't know?: True

I don't know. The context provided does not contain any information directly related to the assessment of subword tokenization techniques in handling numeracy within language models. The passages discuss subword tokenization in relation to general language modeling, the efficiency of data usage in models, and the structure of natural language processing, but they do not explicitly address numeracy or how subword tokenization techniques are evaluated in that context. Therefore, I can't provide a precise answer based on the given information.

[1]: https://ar5iv.org/html/2410.17094, No Title
[2]: https://ar5iv.org/html/2410.17094, No Title
[3]: https://ar5iv.org/html/2409.13057, No Title
[4]: https://ar5iv.org/html/2409.13057, No Title
[5]: https://ar5iv.org/html/2310.11628, [2310.11628] Learn Your Tokens: Word-Pooled Tokenization for Language Modeling

[1]: Passage ID 1: subword tokenization into three key aspects: Frequency, Compositionality and Unknown words. Although subword tokenization has been proved effective, few people have explored which part of advantages plays the most important role. Additionally, it is also difficult to evaluate different methods of tokenization without investigating their performance on downstream tasks.Recently, transformer-based large language models (LLMs) have established their unchallengeable status in every field of NLP. It is widely accepted that a language model with a more parameters trained on a vast mount of data will perform better. However, compared to LLMs which require billions of word tokens to learn languages, children who are exposed to fewer than 100 million word tokens by age 13 are more efficient language learners (Gilkerson et al., 2017). Here people come across the question, whether we can build a more data efficient language model, so that it can achieve relatively good performance with a
[2]: Passage ID 2: language learners (Gilkerson et al., 2017). Here people come across the question, whether we can build a more data efficient language model, so that it can achieve relatively good performance with a smaller data consumption. With a data efficient LM, it is easier for people to explore the influence of subword tokenization, as a small size of training data is more sensitive to the method of tokenization.This paper describes my submission to the SIGMORPHON 2024 Subword Tokenization shared task. Participants are asked to develop a subword tokenization system and use it to pretrain a language model on the 100M word tokens dataset from the BabyLM challenge (Warstadt et al., 2023). The performance of pretrained model is evaluated by model’s predictions after fine-tuning it on three different subtasks: Word and Definition, Word and Morphology and Word and Word. In this paper, I introduce two subword tokenization systems with their variants: one based on a statistics-based morphological
[3]: Passage ID 3: as OpenAI’s ChatGPT 18, 19, Anthropic’s Claude 20, and Microsoft’s Bing Copilot 21. NLP has been used broadly to summarize text documents, deduce author sentiment, solve symbolic math problems, and even generate programming code 22, 23, 24, 25. The effectiveness of NLP is predicated on the view that (human) languages consist of a structured symbolic syntax with a defined set of assembly rules of basic units of a language known as ”tokens” (e.g., characters, words, or punctuation) that can be pieced together to form higher/̄order constructs such as sentences or paragraphs. The structured output of such a system reflect the grammar, conventions, and styles of the associated languages. Tokens, which are represented computationally in NLP models as mathematical vectors, are manipulated and processed to encode ’meanings’, such that tokens of similar ”meaning” are mathematically closer together in vector space. By analyzing a large collection of data, NLP methods are used to infer emergent
[4]: Passage ID 4: NLP/̄based PLI prediction studies over the past five years across each of these categories. Although these studies could be categorized in other ways, for example by the ML model used (neural network, decision tree, etc.) or by the predictive task type (classification vs. regression), we have chosen to emphasize a categorization based on input data type since the computational methods used for sequence text and structural data comprise a major difference.5.2 5.2. Extraction of EmbeddingsNLP approaches deconstruct text into individual tokens or units of “meaning” prior to use in computational operations and inferences—a process referred to as ”tokenization”. Schema for tokenization, aside from character/̄based and word/̄based forms, can also be done using sub/̄words. Sub/̄word tokenization is the breaking down of texts into units smaller than words with the goal of creating a wider vocabulary. Sub/̄words are commonly selected in cases where the definition of a ”word” is unclear,
[5]: Passage ID 5: of 30! We extensively study the language modeling setup for all three categories of tokenizers and theoretically analyze how our end-to-end models can also be a strong trade-off in efficiency and robustness. Code on Github.1 IntroductionAlmost all natural language processing (NLP) begins with tokenization Mielke et al. (2021). Sequences of characters are (mostly deterministically) segmented into discrete tokens, each of which has a lookup embedding in an enormous vocabulary matrix. Statistical NLP methods, similar to other forms of machine learning at the time, relied on feature extraction from these tokens, in the form of n-gram occurrences or part-of-speech tags or other representations of syntax. All of these pipelines have over time been replaced with end-to-end learning using recurrent neural networks (RNNs) or transformers, however the tokenization schemes remain static, deterministic, and manually engineered.State-of-the-art approaches include subword tokenization