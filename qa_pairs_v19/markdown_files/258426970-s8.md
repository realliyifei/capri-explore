# Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation

## Question

What are the primary techniques for optimizing model parameters using human feedback, and how are these techniques categorized?

## URLs

1. https://ar5iv.org/html/2410.19878. [2410.19878] Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies
2. https://ar5iv.org/html/2311.13231. [2311.13231] Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model
3. https://ar5iv.org/html/2312.15997. [2312.15997] Aligning Large Language Models with Human Preferences through Representation Engineering
4. https://ar5iv.org/html/2403.07708. [2403.07708] Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards
5. https://ar5iv.org/html/2410.05116. [2410.05116] Human-Feedback Efficient Reinforcement Learning for Online Diffusion Model Finetuning
6. https://ar5iv.org/html/2307.15217. [2307.15217] Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback
7. https://ar5iv.org/html/2406.11191. [2406.11191] A Survey on Human Preference Learning for Large Language Models
8. https://ar5iv.org/html/2408.13296. [2408.13296] The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities (Version 1.0)
9. https://ar5iv.org/html/2409.11564. [2409.11564] Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey

## Answer (I don't know?: ✗)

The primary techniques for optimizing model parameters using human feedback can be categorized into three major approaches: reinforcement learning, contrastive learning, and hindsight instruction relabeling. Each of these methods serves to align large language models (LLMs) more closely with user intent and improve their outputs.

1. **Reinforcement Learning (RL)**: This technique involves using feedback from humans to guide the learning process of the model. The model generates outputs, which are then evaluated based on human preferences. If a response aligns well with human intent, the model is rewarded, encouraging similar future outputs. Existing studies highlight this methodology as a key area for improvement in LLMs, helping to refine their safety, performance, and controllability [3].

2. **Contrastive Learning**: This approach focuses on teaching the model to distinguish between preferred and non-preferred outputs. By comparing pairs of outputs, the model learns to optimize responses based on which are chosen by human evaluators, effectively enabling it to prioritize more desirable outcomes over less favorable ones [3].

3. **Hindsight Instruction Relabeling**: This innovative technique adjusts the instructions according to the outcomes achieved, allowing the model to learn retrospectively from previous mistakes or suboptimal responses. It aims to model user preferences more effectively by refining the instructions based on human feedback [3].

The application of these techniques is crucial in the fine-tuning process of LLMs. The overall framework for fine-tuning models begins with data preparation and extends through to model deployment, where it is essential to evaluate and calibrate the various components used in these techniques. The report emphasizes considering factors such as data collection strategies and hyperparameter tuning, which are pivotal for enhancing the effectiveness of the optimization process [2]. 

Moreover, the evaluation of aligned LLMs, particularly in terms of how they respond to human preferences, is addressed by summarizing advantages and disadvantages of different modeling methods and feedback sources [4]. 

In addition, the effectiveness of these optimization components remains a topic of ongoing research, where challenges such as hyperparameter sensitivity and biases in pre-training data are scrutinized [5]. These issues can introduce complexities into the optimization process, making it essential to conduct comparative evaluations under controlled conditions to assess the true performance of the various methods [5]. 

In summary, the optimization of LLM parameters through human feedback primarily involves reinforcement learning, contrastive learning, and hindsight instruction relabeling, categorized based on their methodological approaches and implications for model responses.

1. [1]:  https://ar5iv.org/html/2408.13296, [2408.13296] The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities (Version 1.0)
2. [2]:  https://ar5iv.org/html/2408.13296, [2408.13296] The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities (Version 1.0)
3. [3]:  https://ar5iv.org/html/2312.15997, [2312.15997] Aligning Large Language Models with Human Preferences through Representation Engineering
4. [4]:  https://ar5iv.org/html/2406.11191, [2406.11191] A Survey on Human Preference Learning for Large Language Models
5. [5]:  https://ar5iv.org/html/2409.11564, [2409.11564] Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey
---
1. [1]:  Passage ID 1: limitations such as rare word handling, overfitting, and capturing complex linguistic patterns. Notable examples, such as GPT-3 and GPT-4 [2], leverage the self-attention mechanism within Transformer architectures to efficiently manage sequential data and understand long-range dependencies. Key advancements include in-context learning for generating coherent text from prompts and Reinforcement Learning from Human Feedback (RLHF) [3] for refining models using human responses. Techniques like prompt engineering, question-answering, and conversational interactions have significantly advanced the field of natural language processing (NLP) [4].1.2 Historical Development and Key MilestonesLanguage models are fundamental to natural language processing (NLP), leveraging mathematical techniques to generalise linguistic rules and knowledge for tasks involving prediction and generation. Over several decades, language modelling has evolved from early statistical language models (SLMs) to
2. [2]:  Passage ID 2: process of fine-tuning Large Language Models (LLMs), integrating theoretical insights and practical applications. It begins by tracing the historical development of LLMs, emphasising their evolution from traditional Natural Language Processing (NLP) models and their pivotal role in modern AI systems. The analysis differentiates between various fine-tuning methodologies, including supervised, unsupervised, and instruction-based approaches, underscoring their respective implications for specific tasks.A structured seven-stage pipeline for LLM fine-tuning is introduced, covering the complete lifecycle from data preparation to model deployment. Key considerations include data collection strategies, handling of imbalanced datasets, model initialisation, and optimisation techniques, with a particular focus on hyperparameter tuning. The report also highlights parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA) and Half Fine-Tuning, which balance resource constraints
3. [3]:  Passage ID 3: in training.2 Related WorkTuning large language models to elicit desired responses and behavior from their extensive knowledge and capabilities is essential in the development of chat agents, such as ChatGPT Brown et al. (2020), LLaMA Touvron et al. (2023) and GPT-4 Bubeck et al. (2023), characterized by safety, performance, and controllability.The enlargement of the size of language models only does not inherently enhance their ability to follow a user’s intent.For example, LLMs may still generate outputs that are untruthful, toxic, or simply not helpful to the user.Existing human preference alignment methods can be broadly classified into three major categories: reinforcement learning Ouyang et al. (2022); Ramamurthy et al. (2023), contrastive learning Rafailov et al. (2023); Zhao et al. (2023); Yuan et al. (2023), and Hindsight instruction relabeling Zhang et al. (2023); Liu et al. (2023).Extensive research has devoted into the exploration of RL from human feedback
4. [4]:  Passage ID 4: perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs.We first categorize the human feedback according to data sources and formats.We then summarize techniques for human preferences modeling and compare the advantages and disadvantages of different schools of models.Moreover, we present various preference usage methods sorted by the objectives to utilize human preference signals.Finally, we summarize some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discuss our outlooks on the human intention alignment for LLMs.Index Terms: Large language models, preference learning, human feedback, preference modeling, instruction following.††publicationid: pubid: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. I
5. [5]:  Passage ID 5: we describe topics related to human preferences that are either underexplored or still in their early stages. We also discuss potential future research areas that could be highly beneficial for advancing the field.8.1 Discussion8.1.1 Effectiveness of Optimization ComponentsIn the literature on preference tuning, the comparative performance of different methods remains unclear, particularly when comparisons are not conducted under fair conditions. This is largely because RL is highly sensitive to changes in hyper-parameters, and running multiple hyper-parameter configurations is very costly. For instance, when a new method is proposed, the baseline may not be fully optimized, resulting in weaker baselines. Another issue in automatic evaluation using LLMs as judges is the bias introduced by the pre-training data. A model might prefer predictions generated by a similar type of model. For example, a GPT-4 model may favor outputs from its own model family over those from other