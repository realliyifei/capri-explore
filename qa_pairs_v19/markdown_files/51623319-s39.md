# Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

What are the three approaches to annotating non-nominal antecedents, and what challenges do they face?

## URLs

1. https://ar5iv.org/html/2411.12829
2. https://ar5iv.org/html/1706.02256
3. https://ar5iv.org/html/2405.01972
4. https://ar5iv.org/html/2107.12920
5. https://ar5iv.org/html/2108.01204
6. https://ar5iv.org/html/2112.09742
7. https://ar5iv.org/html/2210.07188
8. https://ar5iv.org/html/2109.05250
9. https://ar5iv.org/html/2003.03666
10. https://ar5iv.org/html/2311.09174

## Answer

## I don't know?: True

I don't know. The context provided does not mention non-nominal antecedents or any specific approaches to annotating them. It focuses on the general annotation process in natural language processing (NLP) using large language models (LLMs) and the challenges associated with manual and LLM-based annotations, but it does not detail any methodologies specifically for non-nominal antecedents or enumerate three approaches related to that topic. Consequently, the information is insufficient to answer your question.

1. [1]:  https://ar5iv.org/html/2306.15766, No Title
2. [2]:  https://ar5iv.org/html/2407.03895, No Title
3. [3]:  https://ar5iv.org/html/2407.03895, No Title
4. [4]:  https://ar5iv.org/html/1605.04278, No Title
5. [5]:  https://ar5iv.org/html/2405.02861, No Title
---
1. [1]:  Passage ID 1: 1).A common solution in all these cases is to collect more labelled data distinct from the distribution of training data, but labelling (or annotating) data is an expensive and manual process. To address this issue, prior work suggests using large language models (LLMs, Ouyang et al. (2022); Brown et al. (2020)) to annotate data. LLMs like GPT-3 obtain promising accuracy for annotating data for a variety of NLP tasks including sentiment classification Ding et al. (2022), keyword relevance Choi et al. (2023); Gilardi et al. (2023) and question answering Gilardi et al. (2023). However, LLM-based annotations can be noisy and due to efficiency reasons, we cannot deploy LLM models directly.In this paper, we take the natural next step and ask whether annotations from LLMs can be used to enhance generalization of existing NLP models. Given a corpus of unlabelled data, we find that a naive application of LLMs (annotating inputs at random) provides only marginal gains on total accuracy
2. [2]:  Passage ID 2: years showed significant advancements [98, 7, 19] in natural language processing (NLP): Large language models (LLMs) emerged [8], facilitating new methodologies by describing tasks in natural language without a strong formalism. Because resource-intensive LLMs are not always superior [112], smaller, supervised learning-based models are still highly relevant for specialized domains or use cases that require rapid inference or are constrained by hardware limitations (such as mobile devices or offline scenarios) [34].One of these domains is entity recognition [73]. Entity recognition  (ER) describes the task of assigning a label to a sequence of words (e.g. to extract a person, a date or any other predefined label). To apply supervised learning to ER, data must be annotated. The manual annotation process, in which humans annotate data points with these predefined labels, is time-intensive and expensive [106]. Its output is an annotated dataset, which is also called corpus (pl. corpora)
3. [3]:  Passage ID 3: years showed significant advancements [98, 7, 19] in natural language processing (NLP): Large language models (LLMs) emerged [8], facilitating new methodologies by describing tasks in natural language without a strong formalism. Because resource-intensive LLMs are not always superior [112], smaller, supervised learning-based models are still highly relevant for specialized domains or use cases that require rapid inference or are constrained by hardware limitations (such as mobile devices or offline scenarios) [34].One of these domains is entity recognition [73]. Entity recognition  (ER) describes the task of assigning a label to a sequence of words (e.g. to extract a person, a date or any other predefined label). To apply supervised learning to ER, data must be annotated. The manual annotation process, in which humans annotate data points with these predefined labels, is time-intensive and expensive [106]. Its output is an annotated dataset, which is also called corpus (pl. corpora)
4. [4]:  Passage ID 4: undergraduates, three are linguisticsmajors and two are engineering majors with a linguistic minor.The graduate student is a linguist specializing in syntax. Anadditional graduate student in NLP participated in the final debuggingof the dataset.Prior to annotating the treebank sentences, the annotators were trained for about8 weeks. During the training, the annotators attendedtutorials on dependency grammars, and learned the English UDguidelines444http://universaldependencies.org/#en,the Penn Treebank POS guidelines [Santorini, 1990], the grammatical error annotationscheme of the FCE [Nicholls, 2003], as well as the ESLguidelines described in section 5 and in the annotation manual.Furthermore, the annotators completed six annotation exercises, in which theywere required to annotate POS tags and dependencies for practice sentences from scratch.The exercises were done individually, and were followed by group meetings in whichannotation disagreements were discussed and
5. [5]:  Passage ID 5: graduate students who majored in linguistics to annotate 100 random examples sampled from the test set of each task in LexBench. We adopt a two-stage approach, wherein an annotator undergoes a brief training phase before advancing to the annotation phase. This process is inspired by the methodology employed by SuperGLUE Sarlin et al. (2020). Our results (cf. Table 3,5) show that humans only dominate in three out of ten tasks, with models exhibiting superior performance in the rest. Therefore, we conclude that the LM’s ability to understand semantic phrases has reached parity with humans and exceeded it in most tasks.5.3 Semantic Category Scaling with In-Context LearningTo investigate the semantic understanding capacity of LMs regarding general phrases (i.e., lexical collocation), we further explored the categorization performance under varying numbers of semantic category combinations. In detail, we pick classes of lexical relations from LexBench with at least 40 cases each and