# Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

What are the three approaches to annotating non-nominal antecedents, and what challenges do they face?

## URLs

1. https://ar5iv.org/html/2411.12829. [2411.12829] Human-Robot Dialogue Annotation for Multi-Modal Common Ground
2. https://ar5iv.org/html/1706.02256. [1706.02256] A Mention-Ranking Model for Abstract Anaphora Resolution
3. https://ar5iv.org/html/2405.01972. [2405.01972] A quantitative and typological study of Early Slavic participle clauses and their competition
4. https://ar5iv.org/html/2107.12920. [2107.12920] Emotion Stimulus Detection in German News Headlines
5. https://ar5iv.org/html/2108.01204. [2108.01204] The RareDis corpus: a corpus annotated with rare diseases, their signs and symptoms
6. https://ar5iv.org/html/2112.09742. [2112.09742] Opinion Piece: Can we Fix the Scope for Coreference? Problems and Solutions for Benchmarks beyond OntoNotes
7. https://ar5iv.org/html/2210.07188. [2210.07188] ezCoref: Towards Unifying Annotation Guidelines for Coreference Resolution
8. https://ar5iv.org/html/2109.05250. [2109.05250] Towards Evaluation of Cross-document Coreference Resolution Models Using Datasets with Diverse Annotation Schemes
9. https://ar5iv.org/html/2003.03666. [2003.03666] Multitask Learning-Based Neural Bridging Reference Resolution
10. https://ar5iv.org/html/2311.09174. [2311.09174] AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph

## Answer (I don't know?: ✓)

I don't know. The context provided does not contain specific information about three approaches to annotating non-nominal antecedents or the challenges they face. While there are mentions of methodologies related to anaphora resolution and coreference in general, detailed discussions on non-nominal antecedents, the specific approaches to their annotation, or their associated challenges are absent from the text. Thus, I cannot provide a well-supported answer based on the available context.

1. [1]:  https://ar5iv.org/html/1706.02256, [1706.02256] A Mention-Ranking Model for Abstract Anaphora Resolution
2. [2]:  https://ar5iv.org/html/2210.07188, [2210.07188] ezCoref: Towards Unifying Annotation Guidelines for Coreference Resolution
3. [3]:  https://ar5iv.org/html/2311.09174, [2311.09174] AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph
4. [4]:  https://ar5iv.org/html/2112.09742, [2112.09742] Opinion Piece: Can we Fix the Scope for Coreference? Problems and Solutions for Benchmarks beyond OntoNotes
5. [5]:  https://ar5iv.org/html/2411.12829, [2411.12829] Human-Robot Dialogue Annotation for Multi-Modal Common Ground
---
1. [1]:  Passage ID 1: the anaphor embedded in the anaphoric sentence and its (typicallynon-nominal) antecedent.We propose a mention-ranking model that learns how abstract anaphors relate to their antecedentswith an LSTM-Siamese Net.We overcome the lack of training data by generatingartificial anaphoric sentence–antecedent pairs. Our model outperforms state-of-the-art results on shell noun resolution. We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus.This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors anda greater range of confounders. We found model variants that outperform the baselines for nominal anaphors, without training on individual anaphor data, but still lag behind for pronominal anaphors.Our model selects syntactically plausible candidates and – if disregarding syntax – discriminates candidates using deeper features.1 IntroductionCurrent research in anaphora (or coreference) resolution is focused on
2. [2]:  Passage ID 2: Secondly, as a result of resource constraints, we limited ourselves to one set of guidelines and compared crowd annotations under these guidelines with expert annotations. Understanding the effects of various guidelines on annotator behavior is left for future research. Thirdly, even the best automatic mention detection algorithm could have errors, especially when tested out-of-domain. Some of the proposed solutions are to directly crowdsource mentions or verify the automatically identified mentions via crowdsourcing Madge et al. (2019b), which can be utilized for future collection of high-quality corpora.8 Ethics StatementThe data collection protocol was approved by the coauthors’ institutional review board. All annotators were presented with a consent form (mentioned below) prior to the annotation. They were also informed that only satisfactory performance on the screening example will allow them to take part in the annotation task. All data collected during the tutorial and
3. [3]:  Passage ID 3: the abstraction knowledge of Noun-Entail and Verb-Entail involves identifying nouns and verbs from events. In ASER, each word in the syntactic pattern is classified into word types according to their POS tags, including noun, verb, be, and preposition. We use those word types to identify the nouns and verbs. For example, the pattern subject-verb-object has word types noun, verb, and noun for each word. Also, we identify modifiers to complete each noun by collecting all words dependent on the noun in the dependency parsing graph, such as “fluffy” in ”fluffy cat.”We also take care of some special cases where eventualities contain some transparent nouns Meyers , such as “I have a lot of food.” In this case, we identify “food” as an instance instead of “lot.” Verbs also have similar constructions, such as “I am going to sleep.” In this example, we identify “sleep” as an instance instead of “going.”A.3 Prompts for Collecting DataWe provide the prompt template used in collecting
4. [4]:  Passage ID 4: which this would mean much work, and a detailed examination of how coreference interacts with existing scope annotation schemes is certainly in order. But assuming we can take the position that (serious) scope problems are fairly rare, then I think the right starting point before moving on to an adequate representation for these cases is to include the phenomena missing in OntoNotes as an ideal target, and offer scope as a topic for further advanced research on coreference, similarly to bridging anaphora or split antecedents, whose absence in benchmarks like OntoNotes is well understood, but does not interfere with progress on the most common types of coreferentiality in corpora.5 ConclusionIf this opinion piece falls short of changing any coreference annotation practices, then I hope it at least serves one purpose: to make researchers aware of the limitations of ON-style coreference, and by extension most NLP tools for coreference resolution. My experience has been that these
5. [5]:  Passage ID 5: how these annotations have, independently or in a combined fashion, been put to use, and how we envision enabling theinterpretation of languagethat accesses the different dimensions of common ground, meaning, and architectural components of a dialogue system and robot sensors dynamically (Section 6). The paper ends with an overview of related work, our conclusions, and brief notes on future work.2 Problem Space: Human-Robot Dialogue for Search & NavigationRobots are being increasingly used in disaster relief and robot rescue as robotics technology improves [44, 42, 51]; however, there remain many challenges for supporting natural language communication between a human and their remotely located robot teammate. The first challenge is one of practicality: reliable internet connectivity may not be available to stream high-fidelity video from a robot’s sensors due to the nature of the incident (e.g., search and rescue in heavily forested areas, or in cities with broken power lines