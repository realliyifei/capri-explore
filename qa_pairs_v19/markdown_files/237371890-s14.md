# Survey of Low-Resource Machine Translation

## Question

How have recent studies explored the choice of parent language in transfer learning for neural machine translation (NMT)?

## URLs

1. https://ar5iv.org/html/2404.08259
2. https://ar5iv.org/html/2212.04262
3. https://ar5iv.org/html/2001.01622
4. https://ar5iv.org/html/2205.11277
5. https://ar5iv.org/html/1809.00357
6. https://ar5iv.org/html/2106.15115
7. https://ar5iv.org/html/2009.07715
8. https://ar5iv.org/html/1909.10955
9. https://ar5iv.org/html/2304.07869
10. https://ar5iv.org/html/2202.03629

## Answer

## I don't know?: True

The context provided does not offer specific information about recent studies exploring the choice of parent language in transfer learning for neural machine translation (NMT). While it discusses the concept of transfer learning in NMT and its benefits—such as reducing the required size of training data and improving performance [5]—it does not detail how different studies have investigated or established criteria for selecting the parent language for transfer learning applications.

Additionally, the passages focus broadly on machine translation, the dominance of neural machine translation, and some strategies in NMT but lack concrete data or examples on the choice of languages used in practice. Therefore, given this insufficiency, I cannot provide a precise answer to your question. 

In summary, I don't know how recent studies have explored the choice of parent language in transfer learning for NMT because the context did not include any relevant findings or discussions regarding this specific topic.

[1]: https://ar5iv.org/html/2107.04239, No Title
[2]: https://ar5iv.org/html/2107.04239, No Title
[3]: https://ar5iv.org/html/2107.04239, No Title
[4]: https://ar5iv.org/html/2304.07869, [2304.07869] Neural Machine Translation For Low Resource Languages
[5]: https://ar5iv.org/html/2106.15115, [2106.15115] Neural Machine Translation for Low-Resource Languages: A Survey

[1]: Passage ID 1: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
[2]: Passage ID 2: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
[3]: Passage ID 3: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
[4]: Passage ID 4: task due to the inherent complex nature and the fluidity that natural languages bring. Nonetheless, in recent years, it has achieved state-of-the-art performance in several language pairs. Although, a lot of traction can be seen in the areas of multilingual neural machine translation (MNMT) in the recent years, there are no comprehensive survey done to identify what approaches work well. The goal of this paper is to investigate the realm of low resource languages and build a Neural Machine Translation model to achieve state-of-the-art results. The paper looks to build upon the mBART.CC25 [1] language model and explore strategies to augment it with various NLP and Deep Learning techniques like back translation and transfer learning. This implementation tries to unpack the architecture of the NMT application and determine the different components which offers us opportunities to amend the said application within the purview of the low resource languages problem space.1
[5]: Passage ID 5: of languages.3.6. Transfer Learning in NMTTransfer learning is a sub-area in Machine Learning that reuses (i.e. transfers or adapts) knowledge that is gained from solving one particular task, problem, or model (parent) by applying it to a different but related one (child) (Pan and Yang, 2009). Zophet al. (2016) first introduced the viability of transfer learning for NMT. In NMT, the parent model is first trained on a large corpus of parallel data from a high-resource language pair (or pairs), which is then used to initialize the parameters of a child model that is trained on a relatively smaller parallel corpus of the LRL pair (Figure 4).The advantages of transferring knowledge from the parent model to the child model include i) reducing the size requirement on child training data, ii) improving the performance of the child task, and iii) faster convergence compared to child models trained from scratch.The transfer process in NMT models can be broadly categorised as