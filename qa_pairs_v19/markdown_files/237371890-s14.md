# Survey of Low-Resource Machine Translation

## Question

How have recent studies explored the choice of parent language in transfer learning for neural machine translation (NMT)?

## URLs

1. https://ar5iv.org/html/2404.08259. [2404.08259] Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024
2. https://ar5iv.org/html/2212.04262. [2212.04262] ConsistTL: Modeling Consistency in Transfer Learning for Low-Resource Neural Machine Translation
3. https://ar5iv.org/html/2001.01622. [2001.01622] Untitled Document
4. https://ar5iv.org/html/2205.11277. [2205.11277] When does Parameter-Efficient Transfer Learning Work for Machine Translation?
5. https://ar5iv.org/html/1809.00357. [1809.00357] Trivial Transfer Learning for Low-Resource Neural Machine Translation
6. https://ar5iv.org/html/2106.15115. [2106.15115] Neural Machine Translation for Low-Resource Languages: A Survey
7. https://ar5iv.org/html/2009.07715. [2009.07715] Knowledge Graphs for Multilingual Language Translation and Generation
8. https://ar5iv.org/html/1909.10955. [1909.10955] Efficiently Reusing Old Models Across Languages via Transfer Learning
9. https://ar5iv.org/html/2304.07869. [2304.07869] Neural Machine Translation For Low Resource Languages
10. https://ar5iv.org/html/2202.03629. [2202.03629] Survey of Hallucination in Natural Language Generation

## Answer (I don't know?: ✗)

Recent studies have explored the choice of parent language in transfer learning for neural machine translation (NMT) by examining various configurations of parent and child models, particularly in relation to resource availability and similarity among languages. Transfer learning in NMT involves leveraging knowledge from a "parent" model trained on high-resource language pairs to improve the performance of a "child" model trained on low-resource language pairs [3][4]. 

Zoph et al. (2016) were among the first to highlight the effectiveness of transfer learning in enhancing NMT capabilities, allowing models to benefit from previously acquired knowledge [3]. In many cases, the parent and child models use the same target language, which can streamline the learning process and improve translation accuracy. For instance, the evidence shows a preference for using bilingual parent models, where the same target language is maintained [4]. 

Additionally, some studies have investigated configurations where the parent and child models may not share a common target language. Kocmi and Bojar (2018) and Maimaiti et al. (2019) demonstrated that transferring knowledge does not always depend on shared languages, indicating the flexibility in choosing parent languages that might be linguistically or structurally different from the child languages [4]. This approach can be beneficial in scenarios where the target languages are less related, but a significant amount of knowledge can still be transferred from a more resourceful model. 

Moreover, variation in the choice of parent languages has also been guided by the need to balance the available bilingual resources for specific languages. For example, recent efforts in translating Indian languages have involved training multilingual models using bilingual data as well as additional data from related languages to enhance overall performance [5]. 

In summary, recent studies indicate a strategic approach to selecting parent languages in transfer learning within NMT, balancing factors such as language similarity, resource availability, and the specific capabilities required for low-resource languages. This nuanced understanding helps in designing effective translation systems across diverse linguistic landscapes.

1. [1]:  https://ar5iv.org/html/2107.04239, No Title
2. [2]:  https://ar5iv.org/html/2107.04239, No Title
3. [3]:  https://ar5iv.org/html/2106.15115, [2106.15115] Neural Machine Translation for Low-Resource Languages: A Survey
4. [4]:  https://ar5iv.org/html/2106.15115, [2106.15115] Neural Machine Translation for Low-Resource Languages: A Survey
5. [5]:  https://ar5iv.org/html/2409.15879, No Title
---
1. [1]:  Passage ID 1: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
2. [2]:  Passage ID 2: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
3. [3]:  Passage ID 3: of languages.3.6. Transfer Learning in NMTTransfer learning is a sub-area in Machine Learning that reuses (i.e. transfers or adapts) knowledge that is gained from solving one particular task, problem, or model (parent) by applying it to a different but related one (child) (Pan and Yang, 2009). Zophet al. (2016) first introduced the viability of transfer learning for NMT. In NMT, the parent model is first trained on a large corpus of parallel data from a high-resource language pair (or pairs), which is then used to initialize the parameters of a child model that is trained on a relatively smaller parallel corpus of the LRL pair (Figure 4).The advantages of transferring knowledge from the parent model to the child model include i) reducing the size requirement on child training data, ii) improving the performance of the child task, and iii) faster convergence compared to child models trained from scratch.The transfer process in NMT models can be broadly categorised as
4. [4]:  Passage ID 4: the first step in transfer learning is to train a parent model, which could be either bilingual or multilingual (note that the source and target in multi-NMT models can be many-to-one, one-to-many, or many-to-many. A special case of multi-NMT based transfer learning is fine-tuning large-scale multilingual language models such as mBART using small amounts of parallel data (Cooper Stickland et al., 2021), as already mentioned in Section 3.5). However, the bilingual parent model is more common. The majority of the time, the parent and child have the same target language  (Zophet al., 2016; Dabreet al., 2017; Nguyen and Chiang, 2017; Murthy et al., 2019; Kim et al., 2019a; Ajiet al., 2020; Maimaitiet al., 2020), while others use the same source language for both the parent and child  (Kocmi and Bojar, 2020). However, it is also possible for the parent and child not to have shared languages in common (Kocmi and Bojar, 2018; Maimaitiet al., 2019; Luoet al., 2019). Often, multi-NMT
5. [5]:  Passage ID 5: previous research. However, training NMT models requires large amounts of data, which presents a significant challenge when dealing with low-resource languages. To tackle this challenge, we employed transfer learning, a well-established approach that enhances model performance by transferring knowledge gained from one task to other related tasks. To improve translation capabilities for low-resource languages, we faced the challenge of limited bilingual resources for Indian languages. To overcome this issue, we trained a multilingual model using not only all the bilingual data provided for the task but also additional Bengali data. Additionally, we examined the languages supported by the existing IndicTrans2Gala et al. (2023) open-source model and conducted a comparative analysis. Based on our findings, we selected different baseline models for knowledge transfer depending on the language pair: for Assamese and Manipuri, we used the IndicTrans2 model as the baseline, while for Khasi and