# Survey of Low-Resource Machine Translation

## Question

How have recent studies explored the choice of parent language in transfer learning for neural machine translation (NMT)?

## URLs

1. https://ar5iv.org/html/2404.08259. [2404.08259] Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024
2. https://ar5iv.org/html/2212.04262. [2212.04262] ConsistTL: Modeling Consistency in Transfer Learning for Low-Resource Neural Machine Translation
3. https://ar5iv.org/html/2001.01622. [2001.01622] Untitled Document
4. https://ar5iv.org/html/2205.11277. [2205.11277] When does Parameter-Efficient Transfer Learning Work for Machine Translation?
5. https://ar5iv.org/html/1809.00357. [1809.00357] Trivial Transfer Learning for Low-Resource Neural Machine Translation
6. https://ar5iv.org/html/2106.15115. [2106.15115] Neural Machine Translation for Low-Resource Languages: A Survey
7. https://ar5iv.org/html/2009.07715. [2009.07715] Knowledge Graphs for Multilingual Language Translation and Generation
8. https://ar5iv.org/html/1909.10955. [1909.10955] Efficiently Reusing Old Models Across Languages via Transfer Learning
9. https://ar5iv.org/html/2304.07869. [2304.07869] Neural Machine Translation For Low Resource Languages
10. https://ar5iv.org/html/2202.03629. [2202.03629] Survey of Hallucination in Natural Language Generation

## Answer (I don't know?: ✗)

Recent studies in neural machine translation (NMT) have highlighted the significance of choosing an appropriate parent language for effective transfer learning, particularly in the context of low-resource languages (LRLs). Transfer learning in NMT involves training a parent model with a rich parallel corpus from a high-resource language and then applying that knowledge to a child model that operates on a smaller corpus from an LRL. This process aims to improve the performance of translations and reduce the data requirements for training child models [2] [3].

Early research emphasized the common practice of using bilingual parent models in transfer learning. Typically, the parent and child models share the same target language, which enhances the transfer effectiveness [3]. However, studies have also shown that it is feasible for parent and child models to not share any common languages [3], indicating flexibility in language selection during the transfer learning process.

Research has indicated several advantages of this approach, including improved performance for the child task and faster convergence rates compared to models that are trained from scratch [2]. Furthermore, it has been noted that using a multi-NMT framework can provide beneficial results by allowing one or more parent models to assist multiple child models, potentially leading to better outcomes than when training child pairs independently [4].

However, the literature reveals that the effectiveness of specific parent language choices remains an area needing more comprehensive investigation. Some studies conducted prior to the development of more advanced multi-NMT models have suggested that transfer learning could outperform traditional methods [4]. Yet, these findings were based on limited language sets, and the applicability of such observations with contemporary multi-NMT models that encompass a broader array of languages is still under exploration.

Overall, the choice of parent language in transfer learning for NMT is critical and subject to ongoing research to investigate how various factors—including the paring of languages and the characteristics of both high-resource and low-resource languages—affect transfer learning outcomes [4][5]. This ongoing scrutiny underscores the need for additional empirical studies to ascertain best practices for leveraging transfer learning effectively across diverse languages within the NMT landscape.

1. [1]:  https://ar5iv.org/html/2304.07869, [2304.07869] Neural Machine Translation For Low Resource Languages
2. [2]:  https://ar5iv.org/html/2106.15115, [2106.15115] Neural Machine Translation for Low-Resource Languages: A Survey
3. [3]:  https://ar5iv.org/html/2106.15115, [2106.15115] Neural Machine Translation for Low-Resource Languages: A Survey
4. [4]:  https://ar5iv.org/html/2106.15115, [2106.15115] Neural Machine Translation for Low-Resource Languages: A Survey
5. [5]:  https://ar5iv.org/html/2106.15115, [2106.15115] Neural Machine Translation for Low-Resource Languages: A Survey
---
1. [1]:  Passage ID 1: task due to the inherent complex nature and the fluidity that natural languages bring. Nonetheless, in recent years, it has achieved state-of-the-art performance in several language pairs. Although, a lot of traction can be seen in the areas of multilingual neural machine translation (MNMT) in the recent years, there are no comprehensive survey done to identify what approaches work well. The goal of this paper is to investigate the realm of low resource languages and build a Neural Machine Translation model to achieve state-of-the-art results. The paper looks to build upon the mBART.CC25 [1] language model and explore strategies to augment it with various NLP and Deep Learning techniques like back translation and transfer learning. This implementation tries to unpack the architecture of the NMT application and determine the different components which offers us opportunities to amend the said application within the purview of the low resource languages problem space.1
2. [2]:  Passage ID 2: of languages.3.6. Transfer Learning in NMTTransfer learning is a sub-area in Machine Learning that reuses (i.e. transfers or adapts) knowledge that is gained from solving one particular task, problem, or model (parent) by applying it to a different but related one (child) (Pan and Yang, 2009). Zophet al. (2016) first introduced the viability of transfer learning for NMT. In NMT, the parent model is first trained on a large corpus of parallel data from a high-resource language pair (or pairs), which is then used to initialize the parameters of a child model that is trained on a relatively smaller parallel corpus of the LRL pair (Figure 4).The advantages of transferring knowledge from the parent model to the child model include i) reducing the size requirement on child training data, ii) improving the performance of the child task, and iii) faster convergence compared to child models trained from scratch.The transfer process in NMT models can be broadly categorised as
3. [3]:  Passage ID 3: the first step in transfer learning is to train a parent model, which could be either bilingual or multilingual (note that the source and target in multi-NMT models can be many-to-one, one-to-many, or many-to-many. A special case of multi-NMT based transfer learning is fine-tuning large-scale multilingual language models such as mBART using small amounts of parallel data (Cooper Stickland et al., 2021), as already mentioned in Section 3.5). However, the bilingual parent model is more common. The majority of the time, the parent and child have the same target language  (Zophet al., 2016; Dabreet al., 2017; Nguyen and Chiang, 2017; Murthy et al., 2019; Kim et al., 2019a; Ajiet al., 2020; Maimaitiet al., 2020), while others use the same source language for both the parent and child  (Kocmi and Bojar, 2020). However, it is also possible for the parent and child not to have shared languages in common (Kocmi and Bojar, 2018; Maimaitiet al., 2019; Luoet al., 2019). Often, multi-NMT
4. [4]:  Passage ID 4: some research has shown that transfer learning is better than training a child pair (or a set of pairs ) with one or more parent pairs in a multi-NMT manner (Lakew et al., 2018b; Kim et al., 2019a; Maimaitiet al., 2019, 2020). However, that research has been conducted against an early multi-NMT model (Johnsonet al., 2017a), considering very few languages. Whether the same observation would hold if a more novel multi-NMT model (discussed in Section 3.5) is used along with a large number of language pairs should be subject to more research. On the other hand, transfer learning using pre-trained multi-NMT parent models has received only limited attention (Neubig and Hu, 2018; Guet al., 2018a; Goyalet al., 2020). As mentioned above, multiple factors affect the success of transfer learning. Thus the impact of these factors should be evaluated extensively to determine their exact impact on LRL-NMT. Zero-shot translation adds an extra condition to the cold-start scenario, meaning that
5. [5]:  Passage ID 5: Scientific Instruments OrganisationSector-30CChandigarhIndia160030Abstract.Neural Machine Translation (NMT) has seen a tremendousspurt of growth in less than ten years, and has already entered a mature phase. While considered as the most widely used solution for Machine Translation, its performance on low-resource language pairs still remains sub-optimal compared to the high-resource counterparts, due to the unavailability of large parallel corpora. Therefore, the implementation of NMT techniques for low-resource language pairs has been receiving the spotlight in the recent NMT research arena, thus leading to a substantial amount of research reported on this topic. This paper presents a detailed survey of research advancements in low-resource language NMT (LRL-NMT), along with a quantitative analysis aimed at identifying the most popular solutions. Based on our findings from reviewing previous work, this survey paper provides a set of guidelines to select the possible NMT