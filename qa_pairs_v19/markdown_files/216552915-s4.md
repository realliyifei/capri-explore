# When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?

## Question

What are the key steps and considerations when using word embeddings to analyze beliefs about social dimensions?

## URLs

1. https://ar5iv.org/html/2111.00086. [2111.00086] Measuring a Texts Fairness Dimensions Using Machine Learning Based on Social Psychological Factors
2. https://ar5iv.org/html/2211.03634. [2211.03634] No Word Embedding Model Is Perfect: Evaluating the Representation Accuracy for Social Bias in the Media
3. https://ar5iv.org/html/2107.10413. [2107.10413] Theoretical foundations and limits of word embeddings: what types of meaning can they capture?
4. https://ar5iv.org/html/2007.04508. [2007.04508] Cultural Cartography with Word Embeddings
5. https://ar5iv.org/html/2006.03950. [2006.03950] Instructions for ACL-IJCNLP 2021 Proceedings
6. https://ar5iv.org/html/2402.11512. [2402.11512] From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings
7. https://ar5iv.org/html/1812.08769. [1812.08769] What are the biases in my word embedding?
8. https://ar5iv.org/html/2402.17389. [2402.17389] FairBelief – Assessing Harmful Beliefs in Language Models

## Answer (I don't know?: ✗)

To analyze beliefs about social dimensions using word embeddings in natural language processing (NLP), several key steps and considerations must be taken into account. These steps ensure the effective evaluation and quantification of social bias encoded in models trained on text corpora.

1. **Understanding Social Bias in Word Embeddings**: The first step is to recognize that word embeddings can encode social biases prevalent in their training data. This implies efforts should be directed towards evaluating the extent of this bias within the embeddings [1][4]. Evaluating social bias involves analyzing the models to understand how accurately they represent the intended concepts.

2. **Choosing the Right Algorithms**: Selecting an appropriate embedding algorithm is crucial. Different algorithms can impact the accuracy of bias measurement. Recent approaches suggest that specific embedding algorithms may better capture social dimensions and mitigate representation issues associated with low-resource settings and token frequency differences. Research indicates that while existing methods reduce the gap in bias measurement, they may still not fully align with established psychological insights [3][5].

3. **Employing Established Tests**: Utilizing established methodologies such as the Implicit Association Test (IAT) and the Word Embedding Association Test (WEAT) is vital. These tests assess shared associations across various non-discriminatory social group words and help quantify attitudes, which can be particularly useful for sentiment classification and analyzing specific linguistic patterns such as hate speech or targeted language [2]. These methodologies lend credibility to the analysis and help ensure a rigorous evaluation of biases.

4. **Corpus Evaluation**: A careful analysis of the text corpus used for training the embeddings is equally important. The corpus should be representative and diverse to capture a wide spectrum of political opinions and social biases inherent in language. In recent studies, for example, a collection of 500,000 articles was utilized to review expected social bias, illustrating the importance of comprehensive data in quantifying social bias accurately [3].

5. **Addressing Ethical Considerations**: Ethical implications of deploying biased word embeddings in real-world applications need to be critically assessed. This encompasses understanding how biases can impact generalization performance negatively and the potential harmful consequences, especially in AI applications [5]. It is essential to maintain accountability in both the methods used and the applications derived from these embeddings.

6. **Performance Evaluation**: Following the development of methods like DeepSoftDebias, it is critical to exhaustively evaluate the performance of these debiasing approaches across various state-of-the-art datasets and accuracy metrics. Comparative analysis against existing methods can yield insights into the effectiveness of these approaches in reducing biases across different social dimensions [4].

In summary, analyzing beliefs about social dimensions using word embeddings requires a multifaceted approach that integrates understanding the influences of biases, selecting appropriate algorithms, employing established statistical tests, evaluating the training corpus rigorously, considering ethical implications, and ensuring robust performance evaluations. These considerations collectively contribute to a more equitable and transparent application of word embeddings in NLP.

1. [1]:  https://ar5iv.org/html/2211.03634, [2211.03634] No Word Embedding Model Is Perfect: Evaluating the Representation Accuracy for Social Bias in the Media
2. [2]:  https://ar5iv.org/html/2006.03950, [2006.03950] Instructions for ACL-IJCNLP 2021 Proceedings
3. [3]:  https://ar5iv.org/html/2211.03634, [2211.03634] No Word Embedding Model Is Perfect: Evaluating the Representation Accuracy for Social Bias in the Media
4. [4]:  https://ar5iv.org/html/2402.11512, [2402.11512] From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings
5. [5]:  https://ar5iv.org/html/2211.03634, [2211.03634] No Word Embedding Model Is Perfect: Evaluating the Representation Accuracy for Social Bias in the Media
---
1. [1]:  Passage ID 1: wider spectrum of political opinions.Our main goal is to deepen the understanding of the social bias in word embeddings for different training algorithms.3 MethodThis paper studies how to best evaluate a text corpus for social bias, harnessing the ability of word embeddings to encode direct contexts.In particular, we quantify the social bias encoded in models trained on a corpus.The models are thus used as a proxy from which we derive the social bias in the original corpus.In the following, we present our evaluation method, discuss potential issues, and describe the employed embedding algorithms.3.1 Evaluating Social Bias in EmbeddingsWe seek to analyze to what extent word embedding models encode the social bias of training data.For further insights, we investigate the models quality.Word SimilarityThe quality of the semantic space of word embedding models benefits from larger datasets Pennington et al. (2014).Since most social bias measures rely on this space,
2. [2]:  Passage ID 2: regularities in text corpora provides another layer of transparency into what word embeddings are learning during their training process.9 Ethical ConsiderationsThis work uses expert research in social psychology and computer and information science, specifically the Implicit Association Test (IAT) and the Word Embedding Association Test (WEAT), and applies it to the NLP domain in order to discover widely shared associations of non-discriminatory non-social group words Greenwaldet al. (1998); Caliskanet al. (2017). Prior NLP applications of the WEAT focus mainly on social group biases, since studying potentially harmful features of machine learning and artificial intelligence (AI) are important for fair and ethical implementations of AI. Our application investigates valence (pleasant/unpleasant) associations that quantify attitudes, which can be used to analyze sentiment classification or for a more specific use case of detecting targeted language (information operations/hate
3. [3]:  Passage ID 3: models trained on respective data.Recent work has relied on word embedding bias measures, such as Weat.However, several representation issues of embeddings can harm the measures’ accuracy, including low-resource settings and token frequency differences.In this work, we study what kind of embedding algorithm serves best to accurately measure types of social bias known to exist in US online news articles.To cover the whole spectrum of political bias in the US, we collect 500k articles and review psychology literature with respect to expected social bias.We then quantify social bias using Weat along with embedding algorithms that account for the aforementioned issues.We compare how models trained with the algorithms on news articles represent the expected social bias.Our results suggest that the standard way to quantify bias does not align well with knowledge from psychology.While the proposed algorithms reduce the gap, they still do not fully match the literature.Accepted
4. [4]:  Passage ID 4: language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform ‘soft debiasing’. We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.1 IntroductionWord embeddings are a foundational element in the architecture of large language models (LLMs). They act as the basis for these models to understand and subsequently, generate human-like language. However, it has been shown that these models rely on word embeddings, which themselves may reflect or
5. [5]:  Passage ID 5: et al. (2013) or as a knowledge source Slonim et al. (2021).For example, bias present in the articles may be learned and amplified by word embeddings if not explicitly accounted for.This impacts generalization performance negatively Shah et al. (2020) and may have harmful consequences in practical applications Bender et al. (2021); Joseph and Morgan (2020).So far, one hurdle to mitigate these problems is the limited reliability of common measures of social bias present in a corpus Spliethöver and Wachsmuth (2021), stemming from embedding training algorithms not tailored to low-resource situations Knoche et al. (2019); Spinde et al. (2021).In this paper, we investigate how to assess social bias more reliably while empirically studying the interaction of social bias and political bias in US online news outlets.In particular, we identify low-resource settings and token frequency differences as two main issues with existing embedding-based bias measures.We consider social bias