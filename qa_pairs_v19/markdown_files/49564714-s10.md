# Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing

## Question

How can morphosyntactic annotation be effectively projected to resource-lean languages, and what techniques are employed for feature prediction?

## URLs

1. https://ar5iv.org/html/2406.05424. [2406.05424] Recent advancements in computational morphology : A comprehensive survey
2. https://ar5iv.org/html/2403.06399. [2403.06399] GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing
3. https://ar5iv.org/html/2401.16078. [2401.16078] Understanding the effects of word-level linguistic annotations in under-resourced neural machine translation
4. https://ar5iv.org/html/2105.14214. [2105.14214] Predictive Representation Learning for Language Modeling
5. https://ar5iv.org/html/2404.05839. [2404.05839] ÚFAL LatinPipe at EvaLatin 2024: Morphosyntactic Analysis of Latin
6. https://ar5iv.org/html/2302.00407. [2302.00407] On the Role of Morphological Information for Contextual Lemmatization
7. https://ar5iv.org/html/2402.17608. [2402.17608] Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)
8. https://ar5iv.org/html/2405.05966. [2405.05966] Natural Language Processing RELIES on Linguistics
9. https://ar5iv.org/html/2403.04507. [2403.04507] NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems

## Answer (I don't know?: ✗)

Morphosyntactic annotation can be effectively projected to resource-lean languages through various strategies focused on leveraging existing linguistic resources, employing transfer learning, and utilizing syntactic features. One primary approach involves the application of multilingual models that facilitate cross-lingual transfer from high-resource languages to low-resource languages. This method has shown promising results, especially when utilized with models designed to handle multiple languages simultaneously [4].

To specifically address the feature prediction for morphosyntactic tasks, researchers employ both rule-based and machine learning-based methods. Rule-based methods require a solid understanding of linguistic rules, which entails creating suffix tables and other language-specific characteristics [3]. On the other hand, machine learning-based approaches, particularly those leveraging deep neural networks, often do not necessitate manual feature engineering. Instead, they automatically learn relevant features from the data, making them more adaptable to varying amounts of training data [4]. However, in low-resource settings where data may be sparse, employing statistical approaches such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) can yield reasonable results, even though these methods require extensive feature engineering [4].

Another effective technique is the use of morpho-syntactic description tags. These tags enhance the grammaticality of the output in tasks such as neural machine translation (NMT), even under conditions where available parallel corpora for language pairs are significantly limited. Linguistic annotations can provide valuable context for both the source and target languages, improving overall translation performance and allowing for better feature predictions during the training of NMT systems [5]. 

Ultimately, while resource-lean languages present unique challenges, the combination of linguistic insight—gained through morphosyntactic annotations and multilingual transfer methods—along with advanced machine learning techniques enables researchers to predict features more effectively in these contexts. Therefore, a holistic approach that incorporates linguistic expertise alongside cutting-edge machine learning methodologies is crucial for successfully projecting morphosyntactic annotation in low-resource settings.

1. [1]:  https://ar5iv.org/html/2405.05966, [2405.05966] Natural Language Processing RELIES on Linguistics
2. [2]:  https://ar5iv.org/html/2403.04507, [2403.04507] NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems
3. [3]:  https://ar5iv.org/html/2406.05424, [2406.05424] Recent advancements in computational morphology : A comprehensive survey
4. [4]:  https://ar5iv.org/html/2406.05424, [2406.05424] Recent advancements in computational morphology : A comprehensive survey
5. [5]:  https://ar5iv.org/html/2401.16078, [2401.16078] Understanding the effects of word-level linguistic annotations in under-resourced neural machine translation
---
1. [1]:  Passage ID 1: contemporary NLP as well as their relationship. In the sections that follow, we tackle each of the six facets of NLP research which make up the mnemonic “RELIES,” for which we argue that linguistics has enduring relevance.3 ResourcesThe field of NLP is committed to an empirical methodology wherein machine learning models are trained and evaluated on language data. This paradigm requires linguistic expertise on several fronts. Resources are supported by various degrees of linguistic knowledge—ranging from proficiency in a language to formal training in linguistics.Creation of resources for general NLP tasks.By “general NLP tasks” we mean tasks that closely relate to applications in widespread demand, such as machine translation (MT), entity linking, and sentiment classification. Studying these tasks in an empirical way requires corpus resources. Even if we can do without some of these resources in the training of NLP systems, they remain relevant for testing and studying
2. [2]:  Passage ID 2: but not limited to sentiment analysis Sun et al. (2019), relation extraction Zhang et al. (2018); Vashishth et al. (2018); Guo et al. (2019), semantic role labelling Wang et al. (2019); Kasai et al. (2019), question answering Khashabi et al. (2018), or machine translation Chen et al. (2017); Zhang et al. (2019). These underlying tasks may therefore be referred to as natural language preprocessing (NLPre) tasks, as they precede the advanced NLP tasks. Since the quality of morphosyntactic predictions has a crucial impact on the performance of downstream tasks Sachan et al. (2021), it is prudent to employ the best existing NLPre tools to predict the proper linguistic features. We are equipped with various NLPre methods, ranging from rule-based tools with hand-crafted grammars (e.g. Crouch et al., 2011), through statistical systems (e.g. Nivre, 2009; McDonald et al., 2005; Straka et al., 2016), neural systems supported by pre-trained language models (e.g. Qi et al., 2020; Nguyen et al.,
3. [3]:  Passage ID 3: models may not perform effectively in low resource environments due to a lack of labeled example data. In [115], authors state that the primary objective is to determine how well cross-lingual transfer works for the task of lemmatization. The authors’ LSTM-based encoder-decoder model employs a two-step attention procedure. They note that a monolingual model trained on roughly 1000 training samples provides competitive accuracy for the majority of Indian languages. They also note that having POS tags present is a feature that helps with training.6 Datasets for Morphological processingAs surveyed in the above sections, the methods for creating morphological analyzer are either rule based or machine learning based. For the rule based methods, knowledge is required in the form of linguistic rules, suffix tables, and other language-specific characteristics. For the machine learning based methods focused on deep neural network architectures, they do not need any manual feature
4. [4]:  Passage ID 4: which is expensive and time-consuming process [69]. Statistical approaches such as SVM and CRF require heavy feature engineering while the neural approaches do not require any feature engineering. It has been shown by many authors that the neural approaches can be incorporated in different statistical machine learning methods [111] [135]. From the results persented in various works, we observe that the recent approaches based on deep learning gives promising results given the availability of large training data. In case of unavailability of the large amount of data, often the rule based or statistical methods perform reasonable well.For the low resource languages, building morphological analyzers can be a challenging task due to scarcity of the resources such as training data required for the deep neural network models [5]. Multilingual models have shown promising results for cross lingual transfer from high resource languages to low-resource languages [139] [80] [105] [106]. In
5. [5]:  Passage ID 5: the use of morpho-syntactic description tags improves the grammaticality of the output. We provide a detailed analysis of the reasons behind this result.1 Introduction00footnotetext:  This work is licensed under a Creative CommonsAttribution 4.0 International Licence.Licence details:http://creativecommons.org/licenses/by/4.0/.Training neural machine translation (NMT) systems for under-resourced language pairs, for which the amount of parallel corpora is orders of magnitude smaller than those available for prevailing language pairs, may be challenging. Recently, ?) have shown that even under these circumstances, NMT surpasses classical approaches such as phrase-based statistical MT [Koehn, 2010]. In these under-resourced scenarios, the use of relevant linguistic word-level annotations has proved to improve translation performance [Sennrich andHaddow, 2016, Nadejde et al., 2017].Linguistic annotations can be used to label source-language (SL) or target-language (TL)