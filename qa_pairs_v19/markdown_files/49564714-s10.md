# Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing

## Question

How can morphosyntactic annotation be effectively projected to resource-lean languages, and what techniques are employed for feature prediction?

## URLs

1. https://ar5iv.org/html/2406.05424. [2406.05424] Recent advancements in computational morphology : A comprehensive survey
2. https://ar5iv.org/html/2403.06399. [2403.06399] GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing
3. https://ar5iv.org/html/2401.16078. [2401.16078] Understanding the effects of word-level linguistic annotations in under-resourced neural machine translation
4. https://ar5iv.org/html/2105.14214. [2105.14214] Predictive Representation Learning for Language Modeling
5. https://ar5iv.org/html/2404.05839. [2404.05839] √öFAL LatinPipe at EvaLatin 2024: Morphosyntactic Analysis of Latin
6. https://ar5iv.org/html/2302.00407. [2302.00407] On the Role of Morphological Information for Contextual Lemmatization
7. https://ar5iv.org/html/2402.17608. [2402.17608] Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)
8. https://ar5iv.org/html/2405.05966. [2405.05966] Natural Language Processing RELIES on Linguistics
9. https://ar5iv.org/html/2403.04507. [2403.04507] NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems

## Answer (I don't know?: ‚úì)

I don't know. The context provided does not contain specific information about morphosyntactic annotation's projection to resource-lean languages or the techniques employed for feature prediction in that scenario. While the passages discuss various aspects of Natural Language Processing (NLP), such as language resources, machine translation, and feature relevance, they do not explicitly address morphosyntactic annotation methods or techniques tailored for languages with limited resources. Therefore, without further details or relevant passages, I cannot provide a precise answer to your question.

1. [1]:  https://ar5iv.org/html/2405.05966, [2405.05966] Natural Language Processing RELIES on Linguistics
2. [2]:  https://ar5iv.org/html/2105.05975, No Title
3. [3]:  https://ar5iv.org/html/2407.03895, No Title
4. [4]:  https://ar5iv.org/html/2407.03895, No Title
5. [5]:  https://ar5iv.org/html/2401.11972, No Title
---
1. [1]:  Passage ID 1: contemporary NLP as well as their relationship. In the sections that follow, we tackle each of the six facets of NLP research which make up the mnemonic ‚ÄúRELIES,‚Äù for which we argue that linguistics has enduring relevance.3 ResourcesThe field of NLP is committed to an empirical methodology wherein machine learning models are trained and evaluated on language data. This paradigm requires linguistic expertise on several fronts. Resources are supported by various degrees of linguistic knowledge‚Äîranging from proficiency in a language to formal training in linguistics.Creation of resources for general NLP tasks.By ‚Äúgeneral NLP tasks‚Äù we mean tasks that closely relate to applications in widespread demand, such as machine translation (MT), entity linking, and sentiment classification. Studying these tasks in an empirical way requires corpus resources. Even if we can do without some of these resources in the training of NLP systems, they remain relevant for testing and studying
2. [2]:  Passage ID 2: As opposed to the majority of multilingual NLP literature, we don‚Äôt only train on English, but on a group of almost 30 languages. We show that looking at particular syntactic features is 2-4 times more helpful in predicting the performance than an aggregated syntactic similarity. We find out that the importance of syntactic features strongly differs depending on the downstream task - no single feature is a good performance predictor for all NLP tasks. As a result, one should not expect that for a target language L1subscriptùêø1L_{1} there is a single language L2subscriptùêø2L_{2} that is the best choice for any NLP task (for instance, for Bulgarian, the best source language is French on POS tagging, Russian on NER and Thai on NLI). We discuss the most important linguistic features affecting the transfer quality using statistical and machine learning methods.1 IntroductionA vast majority of currently available NLP datasets is in English. However, in many applications around the world
3. [3]:  Passage ID 3: years showed significant advancements [98, 7, 19] in natural language processing (NLP): Large language models (LLMs) emerged [8], facilitating new methodologies by describing tasks in natural language without a strong formalism. Because resource-intensive LLMs are not always superior [112], smaller, supervised learning-based models are still highly relevant for specialized domains or use cases that require rapid inference or are constrained by hardware limitations (such as mobile devices or offline scenarios) [34].One of these domains is entity recognition [73]. Entity recognition ¬†(ER) describes the task of assigning a label to a sequence of words (e.g. to extract a person, a date or any other predefined label). To apply supervised learning to ER, data must be annotated. The manual annotation process, in which humans annotate data points with these predefined labels, is time-intensive and expensive [106]. Its output is an annotated dataset, which is also called corpus (pl. corpora)
4. [4]:  Passage ID 4: years showed significant advancements [98, 7, 19] in natural language processing (NLP): Large language models (LLMs) emerged [8], facilitating new methodologies by describing tasks in natural language without a strong formalism. Because resource-intensive LLMs are not always superior [112], smaller, supervised learning-based models are still highly relevant for specialized domains or use cases that require rapid inference or are constrained by hardware limitations (such as mobile devices or offline scenarios) [34].One of these domains is entity recognition [73]. Entity recognition ¬†(ER) describes the task of assigning a label to a sequence of words (e.g. to extract a person, a date or any other predefined label). To apply supervised learning to ER, data must be annotated. The manual annotation process, in which humans annotate data points with these predefined labels, is time-intensive and expensive [106]. Its output is an annotated dataset, which is also called corpus (pl. corpora)
5. [5]:  Passage ID 5: (Lopez, 2008). Initially, rule-based approaches and statistical approaches were prevalent in this field and later neural machine translation (NMT) turned out to be a key milestone in the current era. Compared to other NLG tasks, machine translation requires less information from external sources as it is enforced to preserve the content during the conversion from the source language to the target language. However, enhancing the input to NMT with linguistic features such as morphological analysis, part-of-speech tags, and dependency labels is shown to improve the quality of the task (Sennrich and Haddow, 2016; Chen et¬†al., 2018).Bastings et¬†al. (2017) extended this idea by applying a graph convolution network on the dependency trees to obtain a dense vector representation for the sentence structure. Apart from utilizing the linguistic features,Chen et¬†al. (2018) aided the translation using search engines by extracting similar source sentences and their corresponding translation.