# An Empirical Survey of Data Augmentation for Limited Data Learning in NLP

## Question

How is data augmentation used in consistency training for semi-supervised learning in natural language processing (NLP)?

## URLs

1. https://ar5iv.org/html/2306.10153. [2306.10153] Semi-supervised Relation Extraction via Data Augmentation and Consistency-training
2. https://ar5iv.org/html/2101.06967. [2101.06967] On Data-Augmentation and Consistency-Based Semi-Supervised Learning
3. https://ar5iv.org/html/2410.22124. [2410.22124] RankUp: Boosting Semi-Supervised Regression with an Auxiliary Ranking Classifier
4. https://ar5iv.org/html/1904.12848. [1904.12848] Unsupervised Data Augmentation for Consistency Training
5. https://ar5iv.org/html/2302.11412. [2302.11412] Data Augmentation for Neural NLP
6. https://ar5iv.org/html/2205.12461. [2205.12461] Augmentation-induced Consistency Regularization for Classification
7. https://ar5iv.org/html/2107.03158. [2107.03158] A Survey on Data Augmentation for Text Classification
8. https://ar5iv.org/html/2010.01764. [2010.01764] How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?
9. https://ar5iv.org/html/2106.07499. [2106.07499] An Empirical Survey of Data Augmentation for Limited Data Learning in NLP
10. https://ar5iv.org/html/2202.03629. [2202.03629] Survey of Hallucination in Natural Language Generation

## Answer (I don't know?: ✗)

Data augmentation plays a crucial role in consistency training for semi-supervised learning in natural language processing (NLP) by generating enriched data that complements existing labeled and unlabeled datasets. Consistency training hinges on the principle that a model should produce similar outputs for data points that have been perturbed or augmented, which helps improve generalization and robustness.

In the context of semi-supervised learning, data augmentation generates new training samples by applying specific transformations to existing data. These transformations can be applied to both labeled data to create supplementary examples for supervised learning and to unlabeled data for semi-supervised learning by enforcing consistency among models under perturbations [2]. According to the context, one common strategy involves manipulating data through augmentation methods such as back-translation and language-model-based augmentations [4]. By leveraging these techniques, the model is trained not only on the original data but also on these augmented samples, ensuring that it learns to maintain stable predictions despite variations in input [1][5].

The importance of data augmentation in consistency training is further highlighted by the use of various perturbation techniques, which can encompass both the input data and the model parameters. Self-ensembling methods, for example, promote prediction consistency through data variations, effectively bridging the gap between labeled and unlabeled data by applying augmentations like noise or alterations to the input [5]. This approach aids in addressing the lack of extensive labeled datasets commonly faced in NLP tasks and helps reduce the risk of overfitting to the limited training data.

Moreover, the empirical studies referenced in the context suggest that while different augmentation methods can improve model performance in semi-supervised settings, their efficacy can vary across different tasks [1][3]. Thus, selecting appropriate data augmentation strategies based on the task requirements is critical. For instance, sentence-level augmentations tend to work particularly well for semi-supervised learning, exemplifying how different levels of data manipulation can produce diverse training advantages [1].

To summarize, data augmentation in the context of consistency training for semi-supervised learning in NLP enhances the model's ability to generalize by providing diverse and consistent training instances. This technique mitigates issues related to overfitting while capitalizing on the available data, whether labeled or unlabeled, to strengthen predictive performance across various NLP tasks [1][2][5].

1. [1]:  https://ar5iv.org/html/2106.07499, [2106.07499] An Empirical Survey of Data Augmentation for Limited Data Learning in NLP
2. [2]:  https://ar5iv.org/html/2106.07499, [2106.07499] An Empirical Survey of Data Augmentation for Limited Data Learning in NLP
3. [3]:  https://ar5iv.org/html/2106.07499, [2106.07499] An Empirical Survey of Data Augmentation for Limited Data Learning in NLP
4. [4]:  https://ar5iv.org/html/2306.10153, [2306.10153] Semi-supervised Relation Extraction via Data Augmentation and Consistency-training
5. [5]:  https://ar5iv.org/html/2306.10153, [2306.10153] Semi-supervised Relation Extraction via Data Augmentation and Consistency-training
---
1. [1]:  Passage ID 1: settings on 11 NLP tasks, and (3) discuss current challenges and future directions of data augmentation, as well as learning with limited data in NLP more broadly.Our experimental results allow us to conclude that no single augmentation works best for every task, but (i) token-level augmentations work well for supervised learning, (ii) sentence-level augmentation usually works the best for semi-supervised learning, and (iii) augmentation methods can sometimes hurt performance, even in the semi-supervised setting.Related Surveys.Recently, several surveys also explore the data augmentation techniques for NLP Hedderich et al. (2020); Feng et al. (2021). Hedderich et al. (2020) provide a broad overview of techniques for NLP in low resource scenarios and briefly cover data augmentation as one of several techniques. In contrast, we focus on data augmentation and provide a more comprehensive review on recent data augmentation methods in this work. While Feng et al. (2021) also survey
2. [2]:  Passage ID 2: amount of research towards learning with limited labeled data for various tasks in the NLP community.One common approach for mitigating the need for labeled data is data augmentation.Data augmentation Feng et al. (2021) generates new data by modifying existing data points through transformations that are designed based on prior knowledge about the problem’s structure Yang (2015); Wei and Zou (2019).This augmented data can be generated from labeled data, and then directly used in supervised learning Wei and Zou (2019), or in semi-supervised learning for unlabeled data through consistency regularization Xie et al. (2020) (“consistency training”). While various approaches have been proposed to tackle learning with limited labeled data — including unsupervised pre-training Peters et al. (2018); Devlin et al. (2019); Raffel et al. (2020), multi-task learning Glorot et al. (2011); Liu et al. (2017); Augenstein et al. (2018), semi-supervised learning Zhu (2005); Chapelle et al. (2009);
3. [3]:  Passage ID 3: as one of several techniques. In contrast, we focus on data augmentation and provide a more comprehensive review on recent data augmentation methods in this work. While Feng et al. (2021) also survey task-specific data augmentation approaches for NLP, our work summarizes recent data augmentation methods in a more fine-grained categorization. We also focus on their application to learning from limited data by providing an empirical study over different augmentation methods on various benchmark datasets in both supervised and semi-supervised settings, so as to hint data augmentation selections in future research.2 Data Augmentation for NLPData augmentation increases both the amount (the number of data points) and the diversity (the variety of data) of a given dataset Cubuk et al. (2019). Limited labeled data often leads to overfitting on the training setand data augmentation works to alleviate this issue by manipulating data either automatically or manually to create additional
4. [4]:  Passage ID 4: which are labelled the same by both models. Several works have developed on this idea of improving the pseudo-label quality via various strategies like meta-learning Hu et al. (2021a) or reinforcement learning Hu et al. (2021b).These set of methods constitute our baselines.Data augmentation for NLPIn this work, we concentrate on two major classes of data augmentation techniques for NLP – sentence-level data augmentation and latent space augmentations. Sentence-level data augmentation techniques include back-translation Edunov et al. (2018), language-model based augmentations Anaby-Tavor et al. (2020), and word-replacement strategies Zhang et al. (2015). We adapt the back-translation techniques to the RE task.In latent space augmentations one generates more samples by interpolating between pairs of given data points in their latent space.This was originally introduced for image classification Zhang et al. (2018); Verma et al. (2019); Yun et al. (2019) as a data augmentation
5. [5]:  Passage ID 5: augmentation and consistency training for semi-supervised relation extraction task.2 Related workSemi-supervised learning for NLPSemi-supervised learning algorithms can be categorized into two broad classes–1) self-ensembling methods and 2) self-training methods.Self-ensembling methods leverage the smoothness and cluster/low-density assumptions of the latent space Chapelle and Zien (2005).They train the models to make consistent predictions under various kinds of perturbations to either a) the data Miyato et al. (2019); Xie et al. (2020), or b) the model parameters themselves Tarvainen and Valpola (2017). The former methods are broadly referred to as consistency training methods and have resulted in state-of-the-art performances for several semi-supervised NLP tasks.Sachan et al. (2019) add adversarial noise to both labelled and unlabelled data and train models to make consistent predictions on the original and the corresponding noisy data-point.Many recent methods