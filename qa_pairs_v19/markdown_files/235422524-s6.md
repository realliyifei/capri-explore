# An Empirical Survey of Data Augmentation for Limited Data Learning in NLP

## Question

How is data augmentation used in consistency training for semi-supervised learning in natural language processing (NLP)?

## URLs

1. https://ar5iv.org/html/2306.10153. [2306.10153] Semi-supervised Relation Extraction via Data Augmentation and Consistency-training
2. https://ar5iv.org/html/2101.06967. [2101.06967] On Data-Augmentation and Consistency-Based Semi-Supervised Learning
3. https://ar5iv.org/html/2410.22124. [2410.22124] RankUp: Boosting Semi-Supervised Regression with an Auxiliary Ranking Classifier
4. https://ar5iv.org/html/1904.12848. [1904.12848] Unsupervised Data Augmentation for Consistency Training
5. https://ar5iv.org/html/2302.11412. [2302.11412] Data Augmentation for Neural NLP
6. https://ar5iv.org/html/2205.12461. [2205.12461] Augmentation-induced Consistency Regularization for Classification
7. https://ar5iv.org/html/2107.03158. [2107.03158] A Survey on Data Augmentation for Text Classification
8. https://ar5iv.org/html/2010.01764. [2010.01764] How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?
9. https://ar5iv.org/html/2106.07499. [2106.07499] An Empirical Survey of Data Augmentation for Limited Data Learning in NLP
10. https://ar5iv.org/html/2202.03629. [2202.03629] Survey of Hallucination in Natural Language Generation

## Answer (I don't know?: ✓)

I don't know. The provided context does not specifically discuss how data augmentation is applied in consistency training for semi-supervised learning within the field of Natural Language Processing (NLP). While the passages describe various methods of data augmentation and their benefits in improving model performance, they do not make any explicit connections to consistency training or the semi-supervised learning framework. Without information directly relating to these concepts, it is not possible to formulate a precise answer.

1. [1]:  https://ar5iv.org/html/2404.03938, No Title
2. [2]:  https://ar5iv.org/html/2302.13007, No Title
3. [3]:  https://ar5iv.org/html/2306.10153, [2306.10153] Semi-supervised Relation Extraction via Data Augmentation and Consistency-training
4. [4]:  https://ar5iv.org/html/2306.14377, No Title
5. [5]:  https://ar5iv.org/html/2302.11412, [2302.11412] Data Augmentation for Neural NLP
---
1. [1]:  Passage ID 1: a challenging task in Natural Language Processing (NLP). This study aims to provide MWP solvers with a more diverse training set, ultimately improving their ability to solve various math problems. We propose several methods for data augmentation by modifying the problem texts and equations, such as synonym replacement, rule-based: question replacement, and rule based: reversing question methodologies over two English MWP datasets. This study extends by introducing a new in-context learning augmentation method, employing the Llama-7b language model. This approach involves instruction-based prompting for rephrasing the math problem texts. Performance evaluations are conducted on 9 baseline models, revealing that augmentation methods outperform baseline models. Moreover, concatenating examples generated by various augmentation methods further improves performance.keywords: Question Answering, Math Word Problem Solving, Data Augmentation, In-Context Learning, Llama-7b1
2. [2]:  Passage ID 2: processing (NLP) tasks. This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely-used strategy to mitigate such challenges is to perform data augmentation to better capture the data invariance and increase the sample size. However, current text data augmentation methods either can’t ensure the correct labeling of the generated data (lacking faithfulness) or can’t ensure sufficient diversity in the generated data (lacking compactness), or both. Inspired by the recent success of large language models, especially the development of ChatGPT, which demonstrated improved language comprehension abilities, in this work, we propose a text data augmentation approach based on ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples into multiple conceptually similar but semantically different samples. The augmented samples can then be used in downstream
3. [3]:  Passage ID 3: which are labelled the same by both models. Several works have developed on this idea of improving the pseudo-label quality via various strategies like meta-learning Hu et al. (2021a) or reinforcement learning Hu et al. (2021b).These set of methods constitute our baselines.Data augmentation for NLPIn this work, we concentrate on two major classes of data augmentation techniques for NLP – sentence-level data augmentation and latent space augmentations. Sentence-level data augmentation techniques include back-translation Edunov et al. (2018), language-model based augmentations Anaby-Tavor et al. (2020), and word-replacement strategies Zhang et al. (2015). We adapt the back-translation techniques to the RE task.In latent space augmentations one generates more samples by interpolating between pairs of given data points in their latent space.This was originally introduced for image classification Zhang et al. (2018); Verma et al. (2019); Yun et al. (2019) as a data augmentation
4. [4]:  Passage ID 4: on synthetic data.Machine Learning, ICML1 IntroductionData-centric AI research has been actively conducted in natural language processing (NLP) to improve model performance without the need for significant cost and model modification. Several data-centric AI methods have been developed to achieve this goal, such as data management (Choi & Park, 2023), data filtering (Koehn et al., 2020), noise injection (perturbation) (Sarp et al., 2021; Partovyan et al., 2018), and data augmentation (Shorten & Khoshgoftaar, 2019). Among these methods, the use of synthetic data (Nikolenko, 2019) has gained increasing interest with the development of the large language models (LLMs), such as GPT3 (Brown et al., 2020), ChatGPT111https://chat.openai.com/, and LaMDA (Thoppilan et al., 2022),These LLMs have demonstrated the potential for generating high-quality synthetic data (Chen et al., 2023; Wang et al., 2021) and the possibility of replacing the need for human-annotated data with synthetic
5. [5]:  Passage ID 5: data diversity by injecting prior knowledge without changing the model [9], and balance an imbalanced dataset by adding examples with not frequently occurring classes [7]. Some studies have found that data augmentation is a preferred regularization method [10] as it improves generalization without degrading the model’s representational capacity or retuning other hyperparameters. Secondly, data augmentation strategies can help make the model more robust by securing it against adversarial examples [11] and simulating potential distribution shifts [12]. Transformer-based models tend to suffer from out-of-domain problem [4], which can be alleviated by simulating domain shifts with data augmentation. Furthermore, data augmentation can help mitigate model bias [13], [14] and substitute real-world data to remove personally identifiable information and protect peoples’ privacy.While data augmentation methods can be model agnostic, we will focus on the ones that work well with neural NLP