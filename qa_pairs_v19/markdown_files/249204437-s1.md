# How well do real-time machine translation apps perform in practice? Insights from a literature review

## Question

Why are traditional machine translation (MT) quality metrics often considered inadequate for evaluating real-time MT in synchronous dialogues?

## URLs

1. https://ar5iv.org/html/2209.12617. [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
2. https://ar5iv.org/html/2401.15042. [2401.15042] ProxyQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models
3. https://ar5iv.org/html/2012.15515. [2012.15515] Neural Machine Translation: A Review of Methods, Resources, and Tools
4. https://ar5iv.org/html/2411.13577. [2411.13577] WavChat: A Survey of Spoken Dialogue Models
5. https://ar5iv.org/html/2212.06933. [2212.06933] Paraphrase Identification with Deep Learning: A Review of Datasets and Methods
6. https://ar5iv.org/html/2402.09748. [2402.09748] Model Compression and Efficient Inference for Large Language Models: A Survey
7. https://ar5iv.org/html/2411.00027. [2411.00027] Personalization of Large Language Models: A Survey
8. https://ar5iv.org/html/2305.16816. [2305.16816] Songs Across Borders: Singable and Controllable Neural Lyric Translation
9. https://ar5iv.org/html/2410.15885. [2410.15885] How to Build a Pre-trained Multimodal model for Simultaneously Chatting and Decision-making?
10. https://ar5iv.org/html/2403.17297. [2403.17297] InternLM2 Technical Report

## Answer (I don't know?: ✗)

Traditional machine translation (MT) quality metrics are often regarded as inadequate for evaluating real-time MT in synchronous dialogues primarily due to their lack of correlation with human judgments and their inability to effectively capture the contextual nuances involved in such scenarios.

Firstly, many traditional metrics, such as BLEU and METEOR, which have previously been utilized to assess translation quality, focus primarily on optimizing lexical precision rather than reflecting the fluency and adequacy that are critical in real-time settings. Research has indicated that these lexical metrics have limited correlation with human evaluations, meaning they may not accurately reflect the quality of translations as perceived by actual users [5]. This is particularly important in synchronous dialogues, where immediate human interaction is necessary, and the quality of communication is paramount.

Furthermore, the nature of synchronous dialogues entails the necessity for contextual understanding and rapid adaptability to changing conversational flows. Traditional MT systems typically struggle in such environments due to their reliance on pre-defined datasets and fixed inputs, which do not account for the dynamic and often unpredictable nature of dialogues [1] [2]. In essence, they may fail to handle idiomatic expressions, context-dependent meanings, and linguistic nuances effectively, leading to translations that can be misleading or unclear in real-time conversations [3].

In addition, previous evaluations of MT systems have confirmed that neural metrics show higher correlation with human judgments compared to traditional non-neural metrics [4]. This suggests a need for a shift towards more nuanced evaluation criteria that can better assess the quality of translations in a live setting. Ultimately, better metrics must capture not only translation accuracy but also the interaction dynamics and contextual adaptability that characterize effective communication in synchronous dialogue environments.

In summary, traditional MT quality metrics are often seen as inadequate for synchronous dialogue evaluation because they primarily optimize for lexical precision and lack correlation with human judgments while failing to address the dynamic nature of real-time communication [4] [5]. More advanced evaluation methods are needed to accurately reflect the complexities of human interaction and contextual understanding inherent in these scenarios.

1. [1]:  https://ar5iv.org/html/2401.14559, No Title
2. [2]:  https://ar5iv.org/html/2401.14559, No Title
3. [3]:  https://ar5iv.org/html/2404.08661, No Title
4. [4]:  https://ar5iv.org/html/2407.03277, No Title
5. [5]:  https://ar5iv.org/html/2205.00978, No Title
---
1. [1]:  Passage ID 1: common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations (Axelrod et al.,, 2011; Haddow and Koehn,, 2012).Recent advances in language modelling techniques in general and large-scale language models (LLMs) in particular have shown significant potential in improving a wide range of NLP tasks. Inspired by this idea, this research aims to answer two major Research Questions (RQ):RQ1In scenarios involving human interaction and continuous feedback, can we employ language models to improve the quality of adaptive MT at inference time? In the subsequent sections, I will be referring to this question as “Adaptive and Interactive MT”.RQ2In the absence of sufficient in-domain data, can we use pre-trained LLMs to improve the process of NMT domain adaptation? In the following sections, I will be referring to this question as “Domain-specific Text Generation for MT”.Figure 1.1:
2. [2]:  Passage ID 2: I examined how to leverage language modelling techniques in general and LLMs in particular to improve translation features that involve human interaction and continuous feedback, such as adaptive MT, terminology-constrained MT, domain-aware automatic post-editing, auto-suggestion and auto-completion. In the second research question, I addressed a common scenario in the translation industry, namely receiving highly specialised projects, where there is hardly any parallel in-domain data. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. One way to address this question is through domain-specific text generation with LLMs and then fine-tuning an MT model using the generated bilingual in-domain synthetic data. Furthermore, this question overlaps with the first question, since real-time adaptive MT with LLMs can serve as an alternative approach to address in-domain data
3. [3]:  Passage ID 3: highly automation and efficiency are its two main problems. (2) A corpus-based system with statistical machine translation (SMT) or phrase-based machine translation (PB- SMT) as the primary method uses machine learning algorithms by using a large amount of parallel corpus as input, overcoming the problem of using labor work to boost efficiency automatically (Lopez, 2008); however, it has issues such as with translation for idiomatic expressions, compound words that have to be translated by more than one word, long dependencies, and ambiguous words with different meanings depending on contexts (Nießen, 2000). (3) The currently predominant neural machine translation (NMT) system has gained the most extensive popularity in machine translation domain. In contrast to more established system SMT, NMT makes use of its architecture and capacity to capture complex sentence dependencies, which suggests that it has a great deal of potential to become a new trend in machine translation.
4. [4]:  Passage ID 4: evaluation is resource-intensive and time-consuming, and the number of translation systems included in a meta-evaluation tends to be relatively small. In this study, we explore the use of commercial machine translations, collected weekly over a period of 6 years for 12 translation directions, for the evaluation of MT metrics. Given the common use of human A/B testing Tang et al. (2010); Caswell and Liang (2020), our base assumption is that commercial systems show real improvements over time and that we can assess metrics as to whether they prefer more recent MT outputs. Using our dataset, we revisit a number of recent findings in MT metrics research, and find that our dataset supports these.Freitag et al. (2022, 2023) revealed that neural metrics exhibit significantly higher correlation with human judgments compared to non-neural ones. In our experiments, we analyze metric scores over time and evaluate metrics’ ability to accurately rank MT systems. Our findings demonstrate that
5. [5]:  Passage ID 5: Nakayama, 2017; Eikema and Aziz, 2021; Müller and Sennrich, 2021).While this previous work has exhibited promising results, it has mostly focused on optimizing lexical metrics such as BLEU or METEOR (Papineni et al., 2002; Lavie and Denkowski, 2009), which have limited correlation with human judgments Mathur et al. (2020a); Freitag et al. (2021a).Moreover, a rigorous apples-to-apples comparison among this suite of techniques and their variants is still missing, even though they share similar building blocks.Our work fills these gaps by asking the question:“Can we leverage recent advances in MT quality evaluation to generate better translations? If so, how can we most effectively do so?”To answer this question, we systematically explore NMT decoding using a suite of ranking procedures. We take advantage of recent state-of-the-art learnable metrics, both reference-based, such as COMET and BLEURT Rei et al. (2020a); Sellam et al. (2020), and reference-free (also known as quality