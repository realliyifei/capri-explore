# How well do real-time machine translation apps perform in practice? Insights from a literature review

## Question

Why are traditional machine translation (MT) quality metrics often considered inadequate for evaluating real-time MT in synchronous dialogues?

## URLs

1. https://ar5iv.org/html/2209.12617
2. https://ar5iv.org/html/2401.15042
3. https://ar5iv.org/html/2012.15515
4. https://ar5iv.org/html/2411.13577
5. https://ar5iv.org/html/2212.06933
6. https://ar5iv.org/html/2402.09748
7. https://ar5iv.org/html/2411.00027
8. https://ar5iv.org/html/2305.16816
9. https://ar5iv.org/html/2410.15885
10. https://ar5iv.org/html/2403.17297

## Answer

Traditional machine translation (MT) quality metrics often fall short in evaluating real-time MT within synchronous dialogues due to their reliance on static criteria, which do not adequately capture the dynamics of live interaction and the nuances of context. These metrics, such as BLEU and METEOR, primarily focus on lexical similarity and have demonstrated limited correlation with human judgments in real-time scenarios [1] [4]. 

One significant drawback of these conventional metrics is their tendency to assess translations against a fixed reference, which disregards the evolving nature of conversation and the interactive feedback that can occur in real-time settings. In continuous dialogue, the context can shift rapidly based on prior exchanges, making the evaluation of translations based solely on static reference texts insufficient [2]. For instance, when translations happen in adaptive MT systems, which involve human interaction and immediate feedback, the translations need to be evaluated in context rather than just through isolated lexical metrics [2].

Moreover, the lack of adequate specialized datasets and terminology in real-time dialogues compounds the issue. There can be scenarios where there are no parallel in-domain data, making it challenging to produce consistent translations that accurately reflect the ongoing context of the communication. As mentioned, adaptive MT and domain-specific text generation with large language models (LLMs) are areas where traditional metrics struggle, as they do not account for the real-time adaptation needed for high-quality dialogue translations [1] [2].

Additionally, the evaluation process itself is often resource-intensive and time-consuming, limiting the number of systems that can be assessed within a meta-evaluation framework. This adds to the challenge of capturing the state of the art in real-time MT, as conventional metrics may not evolve at the same pace as the technologies themselves [3]. 

In summary, the inadequacy of traditional MT quality metrics in real-time synchronous dialogues arises from their static evaluation criteria, limited correlation with human assessments in dynamic contexts, and challenges posed by insufficient contextualized data. As the field of NLP progresses, there is a need for more adaptive evaluation frameworks that reflect the intricacies of interactive and adaptive MT scenarios [1] [4].

[1]: https://ar5iv.org/html/2401.14559, No Title
[2]: https://ar5iv.org/html/2401.14559, No Title
[3]: https://ar5iv.org/html/2407.03277, No Title
[4]: https://ar5iv.org/html/2205.00978, No Title
[5]: https://ar5iv.org/html/2107.04239, No Title

[1]: Passage ID 1: common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations (Axelrod et al.,, 2011; Haddow and Koehn,, 2012).Recent advances in language modelling techniques in general and large-scale language models (LLMs) in particular have shown significant potential in improving a wide range of NLP tasks. Inspired by this idea, this research aims to answer two major Research Questions (RQ):RQ1In scenarios involving human interaction and continuous feedback, can we employ language models to improve the quality of adaptive MT at inference time? In the subsequent sections, I will be referring to this question as “Adaptive and Interactive MT”.RQ2In the absence of sufficient in-domain data, can we use pre-trained LLMs to improve the process of NMT domain adaptation? In the following sections, I will be referring to this question as “Domain-specific Text Generation for MT”.Figure 1.1:
[2]: Passage ID 2: I examined how to leverage language modelling techniques in general and LLMs in particular to improve translation features that involve human interaction and continuous feedback, such as adaptive MT, terminology-constrained MT, domain-aware automatic post-editing, auto-suggestion and auto-completion. In the second research question, I addressed a common scenario in the translation industry, namely receiving highly specialised projects, where there is hardly any parallel in-domain data. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. One way to address this question is through domain-specific text generation with LLMs and then fine-tuning an MT model using the generated bilingual in-domain synthetic data. Furthermore, this question overlaps with the first question, since real-time adaptive MT with LLMs can serve as an alternative approach to address in-domain data
[3]: Passage ID 3: evaluation is resource-intensive and time-consuming, and the number of translation systems included in a meta-evaluation tends to be relatively small. In this study, we explore the use of commercial machine translations, collected weekly over a period of 6 years for 12 translation directions, for the evaluation of MT metrics. Given the common use of human A/B testing Tang et al. (2010); Caswell and Liang (2020), our base assumption is that commercial systems show real improvements over time and that we can assess metrics as to whether they prefer more recent MT outputs. Using our dataset, we revisit a number of recent findings in MT metrics research, and find that our dataset supports these.Freitag et al. (2022, 2023) revealed that neural metrics exhibit significantly higher correlation with human judgments compared to non-neural ones. In our experiments, we analyze metric scores over time and evaluate metrics’ ability to accurately rank MT systems. Our findings demonstrate that
[4]: Passage ID 4: Nakayama, 2017; Eikema and Aziz, 2021; Müller and Sennrich, 2021).While this previous work has exhibited promising results, it has mostly focused on optimizing lexical metrics such as BLEU or METEOR (Papineni et al., 2002; Lavie and Denkowski, 2009), which have limited correlation with human judgments Mathur et al. (2020a); Freitag et al. (2021a).Moreover, a rigorous apples-to-apples comparison among this suite of techniques and their variants is still missing, even though they share similar building blocks.Our work fills these gaps by asking the question:“Can we leverage recent advances in MT quality evaluation to generate better translations? If so, how can we most effectively do so?”To answer this question, we systematically explore NMT decoding using a suite of ranking procedures. We take advantage of recent state-of-the-art learnable metrics, both reference-based, such as COMET and BLEURT Rei et al. (2020a); Sellam et al. (2020), and reference-free (also known as quality
[5]: Passage ID 5: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the