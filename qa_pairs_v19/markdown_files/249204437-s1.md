# How well do real-time machine translation apps perform in practice? Insights from a literature review

## Question

Why are traditional machine translation (MT) quality metrics often considered inadequate for evaluating real-time MT in synchronous dialogues?

## URLs

1. https://ar5iv.org/html/2209.12617. [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
2. https://ar5iv.org/html/2401.15042. [2401.15042] ProxyQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models
3. https://ar5iv.org/html/2012.15515. [2012.15515] Neural Machine Translation: A Review of Methods, Resources, and Tools
4. https://ar5iv.org/html/2411.13577. [2411.13577] WavChat: A Survey of Spoken Dialogue Models
5. https://ar5iv.org/html/2212.06933. [2212.06933] Paraphrase Identification with Deep Learning: A Review of Datasets and Methods
6. https://ar5iv.org/html/2402.09748. [2402.09748] Model Compression and Efficient Inference for Large Language Models: A Survey
7. https://ar5iv.org/html/2411.00027. [2411.00027] Personalization of Large Language Models: A Survey
8. https://ar5iv.org/html/2305.16816. [2305.16816] Songs Across Borders: Singable and Controllable Neural Lyric Translation
9. https://ar5iv.org/html/2410.15885. [2410.15885] How to Build a Pre-trained Multimodal model for Simultaneously Chatting and Decision-making?
10. https://ar5iv.org/html/2403.17297. [2403.17297] InternLM2 Technical Report

## Answer (I don't know?: ✗)

Traditional machine translation (MT) quality metrics are often considered inadequate for evaluating real-time MT in synchronous dialogues due to several inherent limitations. Firstly, many of these metrics, such as BLEU, METEOR, and ROUGE, primarily focus on lexical similarity between generated outputs and reference translations. These metrics are more suited for tasks like text summarization or standard translation, where there is a fixed reference (or references), rather than for dynamic interactions in dialogue systems which can take many conversational forms [1] [2].

A significant factor is the nature of dialogues themselves. Unlike static text, dialogues are interactive and context-dependent, meaning that the same underlying message can be conveyed in numerous ways. This variability poses challenges for traditional MT metrics, which often fail to capture semantic meaning effectively due to their reliance on direct text comparisons. For instance, passages 1 and 2 note that one meaning can be expressed in various forms, and traditional metrics might miss this nuance, thus providing a misleading quality assessment [1] [2]. Additionally, these metrics tend to overlook critical aspects of speech quality that impact the user experience in real-time settings, such as fluency, expressiveness, and the ability to maintain context over a series of exchanges [1].

Furthermore, the evolving nature of conversation in dialogues often involves open-ended questions and dynamic responses, where traditional metrics struggle to provide relevant evaluations. In real-time settings, the accuracy of the translation operation is not merely about matching words, but also about how well the translation serves the communicative intent within the flow of dialogue [2]. In many cases, due to their inability to handle standard answers, transitional metrics may not provide sufficient insights into the effectiveness of the responses, particularly in contexts where creative language or idiomatic expressions are used [2].

The inadequacy of traditional metrics in capturing human judgment is further illustrated by research indicating that newer metrics, such as those leveraging semantic evaluation methods like BertScore, have been shown to give outcomes that align more closely with human preferences, particularly in the context of evaluating interaction quality in conversational AI [1]. Thus, as the focus shifts towards improving user interactions in dialogue systems, there's a growing necessity to adopt evaluation methodologies that encompass not just linguistic accuracy but also contextual appropriateness and engagement quality.

Overall, the traditional MT metrics are limited due to their rigid structure, focus on lexical similarities, and inability to adapt to the fluid and context-rich nature of dialogues, which requires a broader, more diversified approach to evaluation. The field is gradually moving towards incorporating metrics that assess semantic understanding and human-like response quality, marking a significant shift in how we evaluate real-time machine translation in dialogues [1] [2].

1. [1]:  https://ar5iv.org/html/2411.13577, [2411.13577] WavChat: A Survey of Spoken Dialogue Models
2. [2]:  https://ar5iv.org/html/2411.13577, [2411.13577] WavChat: A Survey of Spoken Dialogue Models
3. [3]:  https://ar5iv.org/html/2305.16816, [2305.16816] Songs Across Borders: Singable and Controllable Neural Lyric Translation
4. [4]:  https://ar5iv.org/html/2401.15042, [2401.15042] ProxyQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models
5. [5]:  https://ar5iv.org/html/2209.12617, [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
---
1. [1]:  Passage ID 1: [13], ROUGE [127]). We collectively refer to these evaluation metrics as MT-Metrics. However, these metrics have certain limitations since one meaning has many different ways to convey. So there are some metrics like BertScore [248] focus on evaluating the semantic similarity between two sentences. And there are also been some methods utilizing LLM to judge the effectiveness of the responses which focusing on human preference [253, 139]. The results of these large model-based especially GPT4o-based ratings of evaluation metrics demonstrated a high degree of correlation with human.Speech Quality. Speech quality is one of the fundamental aspects for evaluating the performance of spoken dialogue systems, as it is closely tied to the experience of users. There are two common dimensions for assessing speech quality: the clarity and naturalness (expressiveness and prosody) of the generated audio, and the robustness of the generated speech, such as the presence of missing or extra words.
2. [2]:  Passage ID 2: questions are close-ended questions with short answers, so that they can have good generalization ability, any model that can generate text answers can be evaluated with these benchmarks and accuracy and F-Score can be easily adopted as the evaluation metrics.∙∙\bullet MT-Metrics. With the development of the LLMs, LLMs can follow instructions to accomplish many complex problems, so the scope of the evaluation was further expanded to include open-ended questions. These open-ended questions often lack standard answers, therefore it’s difficult to measure them by common ACC-Metrics. A common approach is to measure the grammatical similarity between generated and reference utterances using the metrics used to measure grammatical similarity in mechanical translation (e.g. BLEU [162], METEOR [13], ROUGE [127]). We collectively refer to these evaluation metrics as MT-Metrics. However, these metrics have certain limitations since one meaning has many different ways to convey. So there are
3. [3]:  Passage ID 3: techniques from translatology literature to prompt-driven NMT approaches, exploring better adaptation methods, and instantiating them to an English-Chinese lyric translation system. Our model achieves 99.85%, 99.00%, and 95.52% on length accuracy, rhyme accuracy, and word boundary recall. In our subjective evaluation, our model shows a 75% relative enhancement on overall quality, compared against naive fine-tuning111Code available at https://github.com/Sonata165/ControllableLyricTranslation.1 IntroductionFigure 1: Translation comparison of a general-domain NMT system (2nd row), already been adapted with parallel lyric data, versus a singable translation (3rd row).With the globalization of entertainment, it is becoming increasingly common for people to appreciate songs in foreign languages. Meanwhile, artists are internationalizing their work and building territories worldwide. Nevertheless, an unfriendly barrier exists between the artists and the audience: most commercial
4. [4]:  Passage ID 4: New Bing and GPT-4-turbo generate more concise results conveyed with essential contents. Specifically, critical hallucination can be observed in the output of the LLaMA2-13B-Chat, while none of the contents can be used to answer the proxy-questions. However, New Bing covers as much points as it can, providing a comprehensive overview of the given topic, while results of GPT-4-turbo is well-structured and offers detailed and helpful introduction to some of the critical contents.Question: Data parallelism, model parallelism, and pipeline parallelism play a vital role in the training of large-scale language models. What are the representative works and frameworks among these technologies? Please introduce these technologies and frameworks in detail.LLaMA2-13B-Chat-Page 1Introduction:The training of large-scale language models has become increasingly popular in recent years due to their wide range of applications in natural language processing (NLP) tasks.
5. [5]:  Passage ID 5: The task of a QA system is to find an answer (output) in the form of a natural language for a given question (input) usually presented in form of a sentence. While our focus is on QA systems and their quantitative evaluations, we discuss also evaluation scores introduced in related fields. The reason for this is that machine translation (MT), text summarization (TS) and dialogue systems (DS) (also called conversational agents) also have the problem of evaluating various forms of input-output text pairs. Hence, despite the fact that there are considerable differences between question answering, machine translation, text summarization, and dialogue systems several evaluation scores are typically used across those tasks.For QA many methods and techniques have been introduced varying from simple rule-based systems to advanced and complex machine learning techniques 8; 9; 10; 11. In recent years, deep neural network-based approaches to realizing different forms of data-driven