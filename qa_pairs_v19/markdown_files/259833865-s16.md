# A Survey of Evaluation Methods of Generated Medical Textual Reports

## Question

How is medical correctness assessed in the generation of medical reports, and what are the criteria involved?

## URLs

1. https://ar5iv.org/html/2410.17357
2. https://ar5iv.org/html/2408.13988
3. https://ar5iv.org/html/2412.02971
4. https://ar5iv.org/html/2406.11143
5. https://ar5iv.org/html/2401.13267
6. https://ar5iv.org/html/2404.17778
7. https://ar5iv.org/html/2410.23769
8. https://ar5iv.org/html/2405.10842
9. https://ar5iv.org/html/2411.17301
10. https://ar5iv.org/html/2410.00379

## Answer

## I don't know?: False

Medical correctness in the generation of medical reports by large language models (LLMs) is primarily assessed through human evaluation, where medical experts consider several key criteria. The evaluation focuses on ensuring that the generated content is coherent, carries sufficient key information, maintains information correctness, and adheres to professionalism standards.

1. **Coherence**: This criterion assesses the contextual relevance and rationality of the information presented. It requires that the questions or reports produced by the LLMs form a clear and easily understandable text, enhancing the overall readability of the generated document [1] [4].

2. **Sufficiency of Key Information**: The ability of LLMs to extract and abstract the important aspects from input medical reports is evaluated here. This measures whether the generated questions provide enough critical information for an examinee to formulate appropriate responses. Notably, some LLMs scored lower in this criterion, with one scoring below 3.5, indicating a gap in their performance concerning key information sufficiency [2] [4].

3. **Information Correctness**: This criterion evaluates the reliability of the information included in the generated questions or reports. The output must accurately reflect medical knowledge and practices, ensuring that the generated content is trustworthy for clinical decision-making [4].

4. **Professionalism**: This aspect focuses on the overall appropriateness of the generated content concerning medical standards and ethical considerations. It ensures that the questions and reports uphold a professional tone appropriate for medical contexts [2] [4].

Human medical experts review the outputs based on these criteria to assess the quality of the LLMs' performance. For instance, while many models achieve high scores in coherence and information correctness, the average score for the sufficiency of key information tends to be relatively lower, highlighting a critical area for improvement [2]. This evaluation is vital as it directly relates to the effectiveness of LLMs in generating useful and clinically relevant content, which is currently an ongoing area of research in NLP for healthcare [3].

Overall, the assessment of medical correctness in AI-generated reports involves thorough evaluation against these specified criteria, which are crucial for ensuring the suitability of LLM outputs in real-world medical applications.

[1]: https://ar5iv.org/html/2410.23769, [2410.23769] The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams
[2]: https://ar5iv.org/html/2410.23769, [2410.23769] The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams
[3]: https://ar5iv.org/html/2305.12544, No Title
[4]: https://ar5iv.org/html/2410.23769, [2410.23769] The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams
[5]: https://ar5iv.org/html/2310.19736, No Title

[1]: Passage ID 1: in determining whether the questions are professionally appropriate, the LLMs still incur a loss of critical information during the process of extracting and abstracting information from the input reports.Furthermore, human medical experts evaluated the AI-generated answers based on four criteria: coherence, factual consistency, evidence of statement, and professionalism. Figure 2 depicts the average scores of different LLMs’ answers toward a same sampled set of AI-generated questions. It can be observed that the average score for question answering is lower than that for question generation, with LLMs’ ratings hovering around 3.5 across all evaluation metrics. This indicates a significant gap between the performance of LLMs and the critical requirements of human experts for medical open-ended question answering. Moreover, with the task of question answering in the specific domain of elderly chronic diseases under prompting of limited references, identifying and improving strategies
[2]: Passage ID 2: of the AI-generated questions by human medical experts, focusing on four criteria: coherence, sufficiency of key information, information correctness, and professionalism. Figure 1 visualizes the average rating results of human experts on the question generation of different LLMs based on a same sampled set of elderly chronic disease admission reports. It can be noted that the majority of LLMs achieve scores over 4 in terms of coherence and information correctness, and achieve scores nearly 4 in terms of professionalism, whereas the average score for sufficient of key information is relatively lower, with one scoring below 3.5 in general. This phenomenon may indicate that human experts were satisfied with the contextual semantic correctness and readability of AI-generated questions. However, in determining whether the questions are professionally appropriate, the LLMs still incur a loss of critical information during the process of extracting and abstracting information from the input
[3]: Passage ID 3: as work to date has primarily focused on English or other high-resource languages Mondal et al. (2022) but devoted less efforts towards minority languages. Additionally, the lack of human evaluation of NLP-based health systems has made it challenging to measure their effectiveness in the real world. Current automatic evaluation metrics do not necessarily speak to patient outcomes. Hence, human-centric studies must be conducted in evaluating the efficacy of NLP-powered tools in healthcare.Research Directions.1.Healthcare benchmark construction. Although the documentation of recent LLMs reports very high performance for various medical question answering benchmarks, or medical licensing texts, there are many other tasks in healthcare that lack the data required to achieve similarly good performance. Access to medical datasets is often limited because of privacy issues, and therefore other approaches may be required to compile such benchmarks. Synthetic datasets are one such
[4]: Passage ID 4: Human evaluationWe evaluated the performance of LLMs on producing qualification examinations for medical education through human medical expert assessment. Specifically, we focused on two tasks: generating open-ended questions based on admission reports and generating corresponding open-ended answers to the examination questions. (1) For the generation of examination questions, we assessed from four perspectives: coherence, sufficiency of key information, information correctness, and professionalism. Coherence refers to the contextual relevance and rationality between the information, viewpoints, and sentences, forming a clear and easily comprehensible text. Sufficient key information refers to the ability of LLMs to extract, abstract, and organize important and complete information from the input reports, which is adequate for the examinee to formulate a reasonable response based on this background information. Information correctness indicates whether the question is reliable and
[5]: Passage ID 5: Medical LLMs are also evaluated in their potential application scenarios. PubMedQA (Jin et al., 2019) measures LLMs’ question-answering ability on medical scientific literature while LiveQA (Abacha et al., 2017) evaluates LLMs as consultation robot using commonly asked questions scraped from medical websites. MultiMedQA (Singhal et al., 2022) integrates six existing datasets and further augments them with curated commonly searched health queries. Similarly, Ayers et al. (2023) compare ChatGPT’s ability to produce quality and empathetic responses to patient questions on a social media forum with that of physicians. Goodwin & Demner-Fushman (2022) propose a standard clinical language understanding benchmark based on disease staging, clinical phenotyping, mortality prediction, and remaining length-of-stay prediction, enabling direct comparison between different models. Other testing scenarios include medical evidence summarization (Tang et al., 2023b), diagnosis and triage (Levine et al.,