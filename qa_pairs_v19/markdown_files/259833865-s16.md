# A Survey of Evaluation Methods of Generated Medical Textual Reports

## Question

How is medical correctness assessed in the generation of medical reports, and what are the criteria involved?

## URLs

1. https://ar5iv.org/html/2410.17357. [2410.17357] Image-aware Evaluation of Generated Medical Reports
2. https://ar5iv.org/html/2408.13988. [2408.13988] Automatic Medical Report Generation: Methods and Applications
3. https://ar5iv.org/html/2412.02971. [2412.02971] MedAutoCorrect: Image-Conditioned Autocorrection in Medical Reporting
4. https://ar5iv.org/html/2406.11143. [2406.11143] Scorecards for Synthetic Medical Data Evaluation and Reporting
5. https://ar5iv.org/html/2401.13267. [2401.13267] Dual-modal Dynamic Traceback Learning for Medical Report Generation
6. https://ar5iv.org/html/2404.17778. [2404.17778] MRScore: Evaluating Radiology Report Generation with LLM-based Reward System
7. https://ar5iv.org/html/2410.23769. [2410.23769] The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams
8. https://ar5iv.org/html/2405.10842. [2405.10842] Automated Radiology Report Generation: A Review of Recent Advances
9. https://ar5iv.org/html/2411.17301. [2411.17301] ER2Score: LLM-based Explainable and Customizable Metric for Assessing Radiology Reports with Reward-Control Loss
10. https://ar5iv.org/html/2410.00379. [2410.00379] CXPMRG-Bench: Pre-training and Benchmarking for X-ray Medical Report Generation on CheXpert Plus Dataset

## Answer (I don't know?: ✗)

The assessment of medical correctness in the generation of medical reports, particularly in the context of large language models (LLMs), involves several criteria that are evaluated by human medical experts. The primary criteria include coherence, sufficiency of key information, information correctness, and professionalism.

1. **Coherence**: This criterion measures the contextual relevance and rationality of the generated content. It assesses whether the information presented flows logically and forms a clear and comprehensible narrative. Human evaluators examine if the generated questions or reports appropriately connect different pieces of information [2][4].

2. **Sufficiency of Key Information**: This refers to the LLM's ability to extract, abstract, and organize vital information from input reports. Sufficient key information must allow the examinee or user to formulate reasonable responses based on the essential background information provided. In the evaluation, LLMs have been noted to occasionally fail to capture critical information, leading to lower scores in this area [1][4].

3. **Information Correctness**: This aspect ensures the reliability of the content generated. It assesses whether the information aligns with established medical knowledge and practices. The ability to produce correct and factually consistent information is crucial, particularly when LLMs are tasked with generating responses to medical inquiries [2][4].

4. **Professionalism**: This criterion evaluates whether the language, tone, and style of the generated content align with professional medical standards. It is vital for maintaining the trustworthiness and appropriateness of the generated reports, especially in sensitive fields such as healthcare [1][2].

Overall, assessments of the correctness in medical report generation by LLMs suggest a significant gap between the models' outputs and the expectations of human experts, particularly in the realm of open-ended question answering related to medical topics [1]. The evaluation results highlight that while LLMs often excel in coherence and information correctness, they face challenges in ensuring sufficient key information and exhibiting professionalism consistently [2]. 

These criteria serve to guide the ongoing development and refinement of LLMs, ensuring they can better meet the critical needs of medical reporting and inquiry while supporting the healthcare domain effectively [4][5].

1. [1]:  https://ar5iv.org/html/2410.23769, [2410.23769] The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams
2. [2]:  https://ar5iv.org/html/2410.23769, [2410.23769] The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams
3. [3]:  https://ar5iv.org/html/2310.12489, No Title
4. [4]:  https://ar5iv.org/html/2410.23769, [2410.23769] The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams
5. [5]:  https://ar5iv.org/html/2405.01769, No Title
---
1. [1]:  Passage ID 1: in determining whether the questions are professionally appropriate, the LLMs still incur a loss of critical information during the process of extracting and abstracting information from the input reports.Furthermore, human medical experts evaluated the AI-generated answers based on four criteria: coherence, factual consistency, evidence of statement, and professionalism. Figure 2 depicts the average scores of different LLMs’ answers toward a same sampled set of AI-generated questions. It can be observed that the average score for question answering is lower than that for question generation, with LLMs’ ratings hovering around 3.5 across all evaluation metrics. This indicates a significant gap between the performance of LLMs and the critical requirements of human experts for medical open-ended question answering. Moreover, with the task of question answering in the specific domain of elderly chronic diseases under prompting of limited references, identifying and improving strategies
2. [2]:  Passage ID 2: of the AI-generated questions by human medical experts, focusing on four criteria: coherence, sufficiency of key information, information correctness, and professionalism. Figure 1 visualizes the average rating results of human experts on the question generation of different LLMs based on a same sampled set of elderly chronic disease admission reports. It can be noted that the majority of LLMs achieve scores over 4 in terms of coherence and information correctness, and achieve scores nearly 4 in terms of professionalism, whereas the average score for sufficient of key information is relatively lower, with one scoring below 3.5 in general. This phenomenon may indicate that human experts were satisfied with the contextual semantic correctness and readability of AI-generated questions. However, in determining whether the questions are professionally appropriate, the LLMs still incur a loss of critical information during the process of extracting and abstracting information from the input
3. [3]:  Passage ID 3: submission process is straightforward and requires users to provide relevant details about their concern, such as symptoms, medical history, and any additional information that may help diagnose their problems. The platform emphasizes the importance of accurate and complete information to facilitate accurate responses. Once a question is submitted, users can expect to receive responses from certified doctors within a reasonable time frame. The quality and thoroughness of the responses depend in large part on the complexity of the question and the availability of Doctors specializing in the relevant field.The medical question/answer dataset was obtained from the QD website as a valuable resource for researchers and developers in the field of healthcare and NLP. The dataset is accessible through the GitHub link [35]. The repository includes medical questions and answers collected from various websites. For this analysis, the QD website dataset was used. The questions in the dataset
4. [4]:  Passage ID 4: Human evaluationWe evaluated the performance of LLMs on producing qualification examinations for medical education through human medical expert assessment. Specifically, we focused on two tasks: generating open-ended questions based on admission reports and generating corresponding open-ended answers to the examination questions. (1) For the generation of examination questions, we assessed from four perspectives: coherence, sufficiency of key information, information correctness, and professionalism. Coherence refers to the contextual relevance and rationality between the information, viewpoints, and sentences, forming a clear and easily comprehensible text. Sufficient key information refers to the ability of LLMs to extract, abstract, and organize important and complete information from the input reports, which is adequate for the examinee to formulate a reasonable response based on this background information. Information correctness indicates whether the question is reliable and
5. [5]:  Passage ID 5: This task involves inputting a medical query in a free-text format into the model, which then generates a corresponding response. For instance, if a user inputs, ‘Discuss the four major types of leukocytes and their roles in the human immune system in bullet point format,’ the model will produce an informed answer based on its internal medical knowledge. This task serves to measure both the medical knowledge capacity and the instruction-following capability of the model. iCliniq (Li et al., 2023i; Chen et al., 2024) contains 10k real online conversations between patients and doctors to evaluate models’ medical instruction-following ability in the dialog scenario. MedInstruct-test (Zhang et al., 2023i) contains 217 clinical craft free-form instructions to evaluate the medical capacity and instruction-following ability of models across different medical settings such as treatment recommendation, medical education, disease classification, etc. Evaluating the instruction-following capacity