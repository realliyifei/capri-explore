# A Survey of Evaluation Methods of Generated Medical Textual Reports

## Question

How is extrinsic human evaluation used to assess the quality of generated medical summaries?

## URLs

1. https://ar5iv.org/html/2311.08303
2. https://ar5iv.org/html/2407.00908
3. https://ar5iv.org/html/2409.19507
4. https://ar5iv.org/html/2212.01669
5. https://ar5iv.org/html/2309.12444
6. https://ar5iv.org/html/2305.14341
7. https://ar5iv.org/html/2410.17357
8. https://ar5iv.org/html/2406.11289
9. https://ar5iv.org/html/2204.00617

## Answer

## I don't know?: False

Extrinsic human evaluation is crucial for assessing the quality of generated medical summaries, particularly in determining their factual accuracy and relevance to patient care. In the reviewed studies, medical professionals, including doctors who use telehealth platforms, are employed to evaluate the generated summaries against original dialogue snippets. This process includes several steps and criteria for meaningful assessment.

Firstly, the doctors assess how well the summaries capture the essential information from the original dialogues. They evaluate summaries based on the percentage of concepts that are accurately represented, grading them with labels such as "All" (100% correct), "Most" (at least 75% correct), "Some" (at least 1 fact but less than 75%), and "None" (0% correct) [2]. This structured grading system allows researchers to quantify the accuracy and relevance of the summaries, providing a clear metric of performance from a medical perspective.

Additionally, an important aspect of the evaluation is a comparative task where doctors review summaries generated by different models alongside the corresponding dialogue. Here, physicians determine which summary is "best" based on usability. Usability is defined in terms of whether the summary serves as a sufficient substitute for reading the original dialogue snippet, meaning it must accurately capture the correct concepts and maintain the integrity of negations within the context [2]. 

Furthermore, the evaluation is not solely reliant on subjective assessments; it is complemented by various automatic evaluation metrics which provide a baseline comparison. These include lexical similarity measures such as ROUGE and semantic similarity assessments through BERTScore, which help in evaluating the overlap and contextual meaning of the summaries compared to standard references [3]. While these automated metrics are useful, they do not fully capture the nuance necessary for effective medical summarization, which underscores the importance of human evaluation.

In summary, extrinsic human evaluation of medical summaries involves structured feedback from medical professionals regarding the factual correctness and usability of the summaries, supplemented by automated metrics to ensure a comprehensive assessment of the summary quality. This dual approach facilitates a robust understanding of how well NLP models perform in real-world medical contexts, driving improvements in automated healthcare communication tools [1][2][3].

[1]: https://ar5iv.org/html/2305.12544, No Title
[2]: https://ar5iv.org/html/2110.07356, No Title
[3]: https://ar5iv.org/html/2408.14418, No Title
[4]: https://ar5iv.org/html/2110.07356, No Title
[5]: https://ar5iv.org/html/2404.15777, No Title

[1]: Passage ID 1: as work to date has primarily focused on English or other high-resource languages Mondal et al. (2022) but devoted less efforts towards minority languages. Additionally, the lack of human evaluation of NLP-based health systems has made it challenging to measure their effectiveness in the real world. Current automatic evaluation metrics do not necessarily speak to patient outcomes. Hence, human-centric studies must be conducted in evaluating the efficacy of NLP-powered tools in healthcare.Research Directions.1.Healthcare benchmark construction. Although the documentation of recent LLMs reports very high performance for various medical question answering benchmarks, or medical licensing texts, there are many other tasks in healthcare that lack the data required to achieve similarly good performance. Access to medical datasets is often limited because of privacy issues, and therefore other approaches may be required to compile such benchmarks. Synthetic datasets are one such
[2]: Passage ID 2: Doctor EvaluationWe also had doctors, who serve patients on our telehealth platform, evaluate the summaries produced by the models. Given the local dialogue snippets and the generated summary, we asked them to evaluate the extent to which the summary captured factually correct and medically relevant information from the snippet. Depending on what percentage of the concepts were correctly mentioned in the decoded summary of the provided snippet, the doctors graded the summaries with All (100%), Most (at least 75%), Some (at least 1 fact but less than 75%), None (0%) labels.We also formulated a comparison task where given summaries generated by different models and the associated dialogue, they were asked which summary was the "best" from a usability perspective. Usability was defined as whether the summary could stand in as a replacement for reading the dialogue snippet i.e. whether it captures the correct concepts from the snippet and whether the negations are accurate. The
[3]: Passage ID 3: doctor-patient dialogues and their text transcripts written by human annotators. For experiments involving fine-tuning we use the NoteChat-1000 dataset (Wang et al. 2024) for training, which includes 1000 synthetic doctor-patient dialogue transcripts generated by multiple LLMs in a cooperative roleplay setting, conditioned on clinical notes.Evaluation Metrics:We utilized three types of evaluation metrics to assess the performance of the summarization models. For lexical similarity, we used the ROUGE metrics (Lin 2004), specifically focusing on ROUGE-L, which measures the longest common subsequence overlap between generated and reference summaries. To capture the semantic similarity, we employed BERTScore (Zhang et al. 2020), which uses embeddings from pre-trained BERT models to compare the contextual meaning of the texts. Additionally, recognizing the importance of accurately identifying medical terminology in summaries, we assessed the overlap of domain-specific named entities
[4]: Passage ID 4: for summarization require large amounts of labeled data which is especially hard to obtain. We present an algorithm to create synthetic training data with an explicit focus on capturing medically relevant information. We utilize GPT-3 as the backbone of our algorithm and scale 210 human labeled examples to yield results comparable to using 6400 human labeled examples (∼similar-to\sim30x) leveraging low-shot learning and an ensemble method. In detailed experiments, we show that this approach produces high quality training data that can further be combined with human labeled data to get summaries that are strongly preferable to those produced by models trained on human data alone both in terms of medical accuracy and coherency.1 IntroductionWith increasing usage of telehealth platforms Mann et al. (2020), large scale ecosystems of providers and patients have become apparent. This has exacerbated the need for comprehensive visit summaries of the medical dialogues by the attending
[5]: Passage ID 5: and Text Summarization. Their performance was measured across 26 datasets encompassing various types of medical data such as clinical notes, patient electronic health records (EHRs), and medical research articles. The evaluation focused on metrics like Precision, Recall, F1 scores for NER and RE tasks; Recall@1 for Entity Linking; F1 and Accuracy for Text Classification and Question Answering; alongside ROUGE and BERTScore for Text Summarization. The findings revealed LLMs’ strong zero-shot capabilities, particularly in tasks with smaller training datasets, sometimes even outperforming state-of-the-art models that were fine-tuned specifically for those tasks. However, their performance varied across different tasks and datasets, indicating no single LLM consistently outperformed others in all evaluations.In a retrospective cohort study[22] focused on processing patient-initiated electronic health record (EHR) messages, a natural language processing (NLP) framework was developed