# A Survey of Evaluation Methods of Generated Medical Textual Reports

## Question

How is extrinsic human evaluation used to assess the quality of generated medical summaries?

## URLs

1. https://ar5iv.org/html/2311.08303. [2311.08303] MED-OMIT: Extrinsically-Focused Evaluation of Omissions in Medical Summarization
2. https://ar5iv.org/html/2407.00908. [2407.00908] FineSurE: Fine-grained Summarization Evaluation using LLMs
3. https://ar5iv.org/html/2409.19507. [2409.19507] A Critical Look at Meta-evaluating Summarisation Evaluation Metrics
4. https://ar5iv.org/html/2212.01669. [2212.01669] A Survey on Medical Document Summarization
5. https://ar5iv.org/html/2309.12444. [2309.12444] Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI
6. https://ar5iv.org/html/2305.14341. [2305.14341] \scalerel*X APPLS: Evaluating Evaluation Metrics for Plain Language Summarization
7. https://ar5iv.org/html/2410.17357. [2410.17357] Image-aware Evaluation of Generated Medical Reports
8. https://ar5iv.org/html/2406.11289. [2406.11289] A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models
9. https://ar5iv.org/html/2204.00617. [2204.00617] Visual explanations for polyp detection: How medical doctors assess intrinsic versus extrinsic explanations

## Answer (I don't know?: ✗)

The context provided includes insights into the evaluation metrics and methodologies related to natural language processing (NLP) in the healthcare domain, particularly concerning the generation of medical summaries. Extrinsic human evaluation is a crucial aspect of assessing the quality of these generated summaries, especially given the shortcomings of automatic evaluation metrics.

First, it is noted that current automatic evaluation methods are inadequate for capturing the specific nuances associated with medical summaries [2]. These metrics may not accurately reflect patient outcomes or the clinical applicability of generated summaries. Thus, there is a pressing need for human-centric evaluations to effectively measure the performance and relevance of NLP-powered tools in healthcare [1].

Extrinsic human evaluation in this context typically involves human evaluators assessing the quality of summaries based on various criteria such as medical accuracy, coherence, and overall utility. This process contrasts with intrinsic evaluations, which focus on the properties of the summary itself without engaging with its practical use [2]. Human evaluation can help to address the inconsistencies found in current assessment methodologies, where different researchers may evaluate different aspects of summaries, leading to a lack of standardization [2]. 

Further enriching the human evaluation process, studies have demonstrated the effectiveness of using synthetic training data alongside human-labeled data. For instance, an algorithm was utilized to generate medical summaries that captured relevant information, which were then combined with human-labeled examples. The results showed that this hybrid dataset produced summaries that were preferable in terms of medical accuracy and coherence compared to summaries generated solely from human data [4]. This points to the necessity of having human intervention not only in the evaluation stage but also in the synthesis of training data that informs the models.

In practical terms, extrinsic human evaluation can be conducted through tasks such as rating the relevance and clarity of generated summaries in the context of patient care or clinical documentation. Metrics such as Precision, Recall, and F1 scores play a role in quantifying the performance but should be supplemented with human judgment to ensure that the generated summaries meet healthcare practitioners' and patients' needs [5].

In conclusion, extrinsic human evaluation is pivotal in anchoring the effectiveness of generated medical summaries in real-world applications, thereby aligning automatic evaluations with the actual clinical requirements. As the field progresses, emphasizing the integration of human evaluations and synthetic data will be essential for enhancing the quality and relevance of NLP tools in healthcare.

1. [1]:  https://ar5iv.org/html/2305.12544, No Title
2. [2]:  https://ar5iv.org/html/2212.01669, [2212.01669] A Survey on Medical Document Summarization
3. [3]:  https://ar5iv.org/html/2212.01669, [2212.01669] A Survey on Medical Document Summarization
4. [4]:  https://ar5iv.org/html/2110.07356, No Title
5. [5]:  https://ar5iv.org/html/2404.15777, No Title
---
1. [1]:  Passage ID 1: as work to date has primarily focused on English or other high-resource languages Mondal et al. (2022) but devoted less efforts towards minority languages. Additionally, the lack of human evaluation of NLP-based health systems has made it challenging to measure their effectiveness in the real world. Current automatic evaluation metrics do not necessarily speak to patient outcomes. Hence, human-centric studies must be conducted in evaluating the efficacy of NLP-powered tools in healthcare.Research Directions.1.Healthcare benchmark construction. Although the documentation of recent LLMs reports very high performance for various medical question answering benchmarks, or medical licensing texts, there are many other tasks in healthcare that lack the data required to achieve similarly good performance. Access to medical datasets is often limited because of privacy issues, and therefore other approaches may be required to compile such benchmarks. Synthetic datasets are one such
2. [2]:  Passage ID 2: works on the basis of input, output, and method used. It is evident that most of the current work revolves around single documents as input, abstractive summaries as outputs, and deep learning or transformer as their base model. Some of the works also use external Knowledge bases in the form of medical databases or knowledge graphs to further improve the performance. There are also few that focus on specific medical domains such as COVID articles or Radiology reports. Recently, the community is also moving towards a hybrid of extractive and abstractive summarization approaches (Extract-then-Abstract) to improve the faithfulness of generated summaries.After conducting a comprehensive study of evaluation metrics, we can make the following observations: (1) Standard evaluation metrics are insufficient to capture the special aspects of medical summaries. (2) There are a lot of inconsistencies in the human evaluation process as different researchers measure different aspects of the
3. [3]:  Passage ID 3: works on the basis of input, output, and method used. It is evident that most of the current work revolves around single documents as input, abstractive summaries as outputs, and deep learning or transformer as their base model. Some of the works also use external Knowledge bases in the form of medical databases or knowledge graphs to further improve the performance. There are also few that focus on specific medical domains such as COVID articles or Radiology reports. Recently, the community is also moving towards a hybrid of extractive and abstractive summarization approaches (Extract-then-Abstract) to improve the faithfulness of generated summaries.After conducting a comprehensive study of evaluation metrics, we can make the following observations: (1) Standard evaluation metrics are insufficient to capture the special aspects of medical summaries. (2) There are a lot of inconsistencies in the human evaluation process as different researchers measure different aspects of the
4. [4]:  Passage ID 4: for summarization require large amounts of labeled data which is especially hard to obtain. We present an algorithm to create synthetic training data with an explicit focus on capturing medically relevant information. We utilize GPT-3 as the backbone of our algorithm and scale 210 human labeled examples to yield results comparable to using 6400 human labeled examples (∼similar-to\sim30x) leveraging low-shot learning and an ensemble method. In detailed experiments, we show that this approach produces high quality training data that can further be combined with human labeled data to get summaries that are strongly preferable to those produced by models trained on human data alone both in terms of medical accuracy and coherency.1 IntroductionWith increasing usage of telehealth platforms Mann et al. (2020), large scale ecosystems of providers and patients have become apparent. This has exacerbated the need for comprehensive visit summaries of the medical dialogues by the attending
5. [5]:  Passage ID 5: and Text Summarization. Their performance was measured across 26 datasets encompassing various types of medical data such as clinical notes, patient electronic health records (EHRs), and medical research articles. The evaluation focused on metrics like Precision, Recall, F1 scores for NER and RE tasks; Recall@1 for Entity Linking; F1 and Accuracy for Text Classification and Question Answering; alongside ROUGE and BERTScore for Text Summarization. The findings revealed LLMs’ strong zero-shot capabilities, particularly in tasks with smaller training datasets, sometimes even outperforming state-of-the-art models that were fine-tuned specifically for those tasks. However, their performance varied across different tasks and datasets, indicating no single LLM consistently outperformed others in all evaluations.In a retrospective cohort study[22] focused on processing patient-initiated electronic health record (EHR) messages, a natural language processing (NLP) framework was developed