# A Survey of Evaluation Methods of Generated Medical Textual Reports

## Question

How is extrinsic human evaluation used to assess the quality of generated medical summaries?

## URLs

1. https://ar5iv.org/html/2311.08303. [2311.08303] MED-OMIT: Extrinsically-Focused Evaluation of Omissions in Medical Summarization
2. https://ar5iv.org/html/2407.00908. [2407.00908] FineSurE: Fine-grained Summarization Evaluation using LLMs
3. https://ar5iv.org/html/2409.19507. [2409.19507] A Critical Look at Meta-evaluating Summarisation Evaluation Metrics
4. https://ar5iv.org/html/2212.01669. [2212.01669] A Survey on Medical Document Summarization
5. https://ar5iv.org/html/2309.12444. [2309.12444] Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI
6. https://ar5iv.org/html/2305.14341. [2305.14341] \scalerel*X APPLS: Evaluating Evaluation Metrics for Plain Language Summarization
7. https://ar5iv.org/html/2410.17357. [2410.17357] Image-aware Evaluation of Generated Medical Reports
8. https://ar5iv.org/html/2406.11289. [2406.11289] A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models
9. https://ar5iv.org/html/2204.00617. [2204.00617] Visual explanations for polyp detection: How medical doctors assess intrinsic versus extrinsic explanations

## Answer (I don't know?: ✗)

Extrinsic human evaluation is an important aspect of assessing the quality of generated medical summaries, particularly due to the limitations of automatic metrics. According to the context provided, human evaluations are critical for accurately assessing the generated summaries as conventional reference-based metrics, such as ROUGE, often exhibit weak correlation with actual human judgments [3]. This suggests that while these metrics can provide baseline measurements, they may fail to capture the nuanced qualities of medical summaries that humans can discern.

The need for extrinsic evaluation arises from the complexities and specific nuances present in the medical domain. Medical summaries require a high degree of accuracy, readability, and faithfulness to the source material, elements that are better judged through human analysis [3]. Human evaluators can assess various aspects including clarity, relevance, and adherence to medical facts, which are essential for maintaining the quality and safety of medical information. Passages indicate that even though automated methods for evaluation are being developed, the reliance on human judgment is still paramount due to the intricate nature of medical data processing and communication [3].

Furthermore, the context highlights that current evaluation methods using automated metrics fall short in adequately addressing critical elements like faithfulness and semantic overlaps in summaries. For instance, many existing metrics evaluate at a syntactic level without capturing the deeper semantic relationships inherent in medical texts, which may lead to inaccuracies if these aspects are overlooked [5]. Human evaluators can bridge this gap by providing fine-grained feedback that considers the specific medical context and potential implications of the generated summaries.

Moreover, the need for diverse evaluation benchmarks and prompts tailored for the medical domain is emphasized, as biases or inaccuracies in evaluation prompts can impact the quality of human assessments [4]. Therefore, in extrinsic evaluation, humans not only evaluate the outputs directly but may also need to adapt their evaluation criteria based on the nuances of the medical content being summarized [4].

In summary, extrinsic human evaluation in the context of medical summarization is essential for capturing quality attributes that automated metrics may fail to address. It involves detailed assessments of generated summaries by human experts who can evaluate factors such as accuracy, relevance, and faithfulness—crucial elements for ensuring that medical summaries are reliable and informative. While efforts to enhance automatic evaluators are ongoing, they underscore the persistent necessity for human input in the evaluation process, especially within the context of critical fields like healthcare [3] [4].

1. [1]:  https://ar5iv.org/html/2212.01669, [2212.01669] A Survey on Medical Document Summarization
2. [2]:  https://ar5iv.org/html/2212.01669, [2212.01669] A Survey on Medical Document Summarization
3. [3]:  https://ar5iv.org/html/2407.00908, [2407.00908] FineSurE: Fine-grained Summarization Evaluation using LLMs
4. [4]:  https://ar5iv.org/html/2407.00908, [2407.00908] FineSurE: Fine-grained Summarization Evaluation using LLMs
5. [5]:  https://ar5iv.org/html/2212.01669, [2212.01669] A Survey on Medical Document Summarization
---
1. [1]:  Passage ID 1: and fluency of generated summaries. (5) Fact-based Evaluation: To measure the accuracy of medical facts generated by the model in output summary, Enarvi et al. (Enarvi et al., 2020) utilizes a machine-learning clinical factextractor module that is capable of extracting medical facts such as treatment or diagnosis and fine-grained attributes such as body part or medications. They use this model to extract facts from the generated summary and reference summary and calculate the F1 score for both extracted facts. Similar type of factual correctness metrics has been used in several other works (Hu et al., 2021; Zhang et al., 2019b; Chintagunta et al., 2021b). Enarvi et al. (Enarvi et al., 2020) used Negex metric (Harkema et al., 2009) to capture the capability of model to evaluate the negative status of medical concepts by computing the negations in generated summary and calculating whether the negations were accurate for the medical facts present in the generated summary. (6)
2. [2]:  Passage ID 2: works on the basis of input, output, and method used. It is evident that most of the current work revolves around single documents as input, abstractive summaries as outputs, and deep learning or transformer as their base model. Some of the works also use external Knowledge bases in the form of medical databases or knowledge graphs to further improve the performance. There are also few that focus on specific medical domains such as COVID articles or Radiology reports. Recently, the community is also moving towards a hybrid of extractive and abstractive summarization approaches (Extract-then-Abstract) to improve the faithfulness of generated summaries.After conducting a comprehensive study of evaluation metrics, we can make the following observations: (1) Standard evaluation metrics are insufficient to capture the special aspects of medical summaries. (2) There are a lot of inconsistencies in the human evaluation process as different researchers measure different aspects of the
3. [3]:  Passage ID 3: by LLMs, the development of automated methods for evaluation remains a challenge Kryściński et al. (2020); Maynez et al. (2020).Conventional reference-based metrics, such as ROUGE Lin (2004), have exhibited a weak correlation with actual human judgments Liu et al. (2023). Consequently, human evaluation remains an essential step for accurately assessing the quality of generated summaries, even considering its inherent costs and time-consuming nature.Recently, the need for better automatic evaluators has become an important research topic, aiming to streamline evaluation processes and ease manual efforts in model development Gao et al. (2023). This effort provides valuable insights into whether generated summaries align with predefined quality standards, including aspects like faithfulness. Various approaches have been explored, including approaches based on neural language inference (NLI) Laban et al. (2022) and question-answering (QA) Fabbri et al. (2022); Zhong et al. (2022). In
4. [4]:  Passage ID 4: fine-grained human annotations. We emphasize the critical importance of constructing a high-quality benchmark dataset with high diversity in input domains, length, and types. Also, the prompts for evaluation may need to be tuned if a different summary is expected like the summary from the medical domain. Lastly, other aspects can be considered for text summarization, such as toxicity and social bias. We leave these challenges as future work.Ethics StatementThis paper focuses on designing an automatic evaluator using LLMs for text summarization. Therefore, we do not anticipate any negative ethical and social impact.AcknowledgementsThe first author, Hwanjun Song, was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2024-00334343) and Artificial Intelligence industrial convergence cluster development project funded by the Ministry of Science and ICT (MSIT, Korea) & Gwangju Metropolitan City (No.
5. [5]:  Passage ID 5: procedure. But these are very expensive and time-consuming processes making them unscalable for larger datasets. Many attempts have been made to formulate evaluation metrics that are capable of measuring medical summaries specific aspects such as readability, faithfulness, fact-based evaluation, etc. However, all these metrics suffer from their own limitations making a room for the scope of improvement which are discussed as follows: (1) All of these metrics work at a syntactic level which makes them unsuitable to capture semantic overlaps. (2) Most of these metrics are over-dependent on reference summary. (3) There are no metrics that are designed to work in multimodal settings. (4) There is also a need for metrics that are capable of measuring the correctness of the relationship between different generated medical concepts by designing medical domain-aware metrics. We need to overcome all these limitations in order to improve overall medical summarization systems.Multimodality: