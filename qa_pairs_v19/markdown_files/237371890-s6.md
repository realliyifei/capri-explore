# Survey of Low-Resource Machine Translation

## Question

How does the FLORES-101 benchmark improve upon previous test sets for low-resource languages?

## URLs

1. https://ar5iv.org/html/2106.03193
2. https://ar5iv.org/html/2404.13813
3. https://ar5iv.org/html/2408.11457
4. https://ar5iv.org/html/2204.04306
5. https://ar5iv.org/html/2407.05975
6. https://ar5iv.org/html/2401.16313
7. https://ar5iv.org/html/2301.08745
8. https://ar5iv.org/html/2305.11550
9. https://ar5iv.org/html/2409.00626

## Answer

The FLORES-101 benchmark significantly enhances previous evaluation datasets for low-resource languages through several key aspects: increased coverage, higher quality, and a focus on many-to-many multilingual translation capabilities.

Firstly, FLORES-101 boasts a broader range of topics and better coverage of low-resource languages compared to prior benchmarks. It consists of 3001 sentences sourced from English Wikipedia and translated into 101 languages by professional translators [1] [2]. This stands in contrast to previous benchmarks, which often lacked sufficient representation of low-resource languages or were limited to restricted domains. By encompassing a variety of topics, FLORES-101 addresses the need for diverse data that can facilitate more comprehensive assessments of machine translation (MT) systems across different languages [1] [2].

Secondly, the quality of the translations included in FLORES-101 is notably superior due to a carefully controlled translation process and stringent quality control measures. The dataset was created following a rigorous annotation process where a 20% sample of the translations was reviewed by language-specific evaluators to ensure they met acceptable quality standards [4]. This approach aims to mitigate the low quality often found in datasets constructed using semi-automatic methods, which have been criticized for their inadequate oversight and verification [1]. 

Additionally, FLORES-101 is designed for many-to-many evaluation scenarios. This capability allows seamless assessment of various language pairs, thereby facilitating the evaluation of popular multilingual MT systems as well as regionally relevant pairs [2]. This is particularly important as previous datasets often required pivoting through a high-resource language, such as English, which can introduce biases and reduce the accuracy of translations [4]. FLORES-101's design allows for direct evaluation between numerous low-resource languages and their high-resource counterparts, fostering more equitable assessments across diverse language pairs.

Lastly, the comprehensive documentation of the annotation process in FLORES-101 aids the NLP community in building institutional knowledge about constructing high-quality MT datasets [2]. This transparency is essential for encouraging the development of further robust datasets, improving an area that has historically struggled with both quality and consistency.

In summary, FLORES-101 improves upon previous evaluation test sets for low-resource languages through expansive topic coverage, enhanced translation quality, many-to-many evaluation capabilities, and thorough documentation of its construction process. These improvements aim to foster significant progress in the field of machine translation, particularly for low-resource languages [1] [2] [4].

[1]: https://ar5iv.org/html/2106.03193, [2106.03193] The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation
[2]: https://ar5iv.org/html/2106.03193, [2106.03193] The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation
[3]: https://ar5iv.org/html/2404.13813, [2404.13813] From LLM to NMT: Advancing Low-Resource Machine Translation with Claude
[4]: https://ar5iv.org/html/2409.00626, [2409.00626] Correcting FLORES Evaluation Dataset for Four African Languages
[5]: https://ar5iv.org/html/2408.11457, [2408.11457] Expanding FLORES+ Benchmark for more Low-Resource Settings: Portuguese-Emakhuwa Machine Translation Evaluation

[1]: Passage ID 1: evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures.In this work, we introduce the Flores-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains.These sentences have been translated in 101 languages by professional translators through a carefully controlled process.The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are multilingually aligned.By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.1 IntroductionMachine translation (MT) isone of the most successful applications in natural language processing, as
[2]: Passage ID 2: scarce availability, and because it is non-trivial to check thequality of their work Guzmán et al. (2019).We present the Flores-101 benchmark, consisting of 3001 sentences sampled from English Wikipedia and professionally translated in 101 languages.With this dataset, we make several contributions. First, we provide the community with a high-quality benchmark that has much larger breadth of topics and coverage of low resource languages than any other existing dataset (§4). Second, Flores-101 is suitable for many-to-many evaluation, meaning that it enables seamless evaluation of 10,100 language pairs.This enables the evaluation of popular multilingual MT systems as well as the evaluation of regionally-relevant language pairs like Spanish-Aymara and Vietnamese-Thai, for example.Third, we thoroughly document the annotation process we followed (§3), helping the community build institutional knowledge about how to construct MT datasets. Fourth, we release not only sentences with
[3]: Passage ID 3: between 100k and 1m bitexts, and high-resource if they have more than 1m bitexts, according to Team et al. (2022)222See https://tinyurl.com/535f7ust..We experiment on English and a selection of 36 other languages, of which 15 are high-resource, 17 are low-resource, and 4 are very low-resource. All languages are supported by Google Translate, NLLB-200 (Guzmán et al., 2019), and are included in the FLORES-200 dataset. Every language is evaluated in both the eng->xxx and xxx->eng directions. We do not conduct experiments on non-English-centric language pairs. The full list of languages is provided in Table LABEL:tab:language-table.3.2 DatasetsWe benchmark our model against the following datasets.3.2.1 FLORES-200FLORES-200 is a high-quality evaluation dataset containing human-curated translations between English and 204 different languages (Guzmán et al., 2019; Goyal et al., 2021; Team et al., 2022). The dataset serves as a universal benchmark for which to evaluate
[4]: Passage ID 4: corrected datasets for future evaluation tasks.2 The FLORES Evaluation DatasetThe FLORES evaluation dataset consists of the first FLORES-101 Goyal et al. (2021) and the subsequent more expanded FLORES-200 NLLB Team et al. (2022) that included more languages.FLORES-101:This was the original evaluation data and it was created by translating English dataset collected from Wikipedia, consisting of several topics and domains, to 101 mostly low-resource languages. The dataset was the first available evaluation benchmark for several low-resource languages and it enabled the evaluation of many-to-many translation systems without pivoting through another high-resource language such as English. Several quality control mechanisms were put in place to ensure that the final dataset was of acceptable quality. To determine if translations are good enough for inclusion in FLORES-101, a 20% sample of the dataset were reviewed by language-specific evaluators who assess the quality using a
[5]: Passage ID 5: Krishnan, Marc’Aurelio Ranzato, Francisco Guzman, and Angela Fan. 2021.The flores-101 evaluation benchmark for low-resource and multilingual machine translation.Guzmán et al. (2019)Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume Lample, Philipp Koehn, Vishrav Chaudhary, and Marc’Aurelio Ranzato. 2019.The FLORES evaluation datasets for low-resource machine translation: Nepali–English and Sinhala–English.In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6098–6111, Hong Kong, China. Association for Computational Linguistics.Kingma and Ba (2014)Diederik P. Kingma and Jimmy Ba. 2014.Adam: A method for stochastic optimization.CoRR, abs/1412.6980.Klein et al. (2017)Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. 2017.OpenNMT: Open-source toolkit for neural machine