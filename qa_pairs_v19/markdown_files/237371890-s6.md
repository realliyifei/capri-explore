# Survey of Low-Resource Machine Translation

## Question

How does the FLORES-101 benchmark improve upon previous test sets for low-resource languages?

## URLs

1. https://ar5iv.org/html/2106.03193. [2106.03193] The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation
2. https://ar5iv.org/html/2404.13813. [2404.13813] From LLM to NMT: Advancing Low-Resource Machine Translation with Claude
3. https://ar5iv.org/html/2408.11457. [2408.11457] Expanding FLORES+ Benchmark for more Low-Resource Settings: Portuguese-Emakhuwa Machine Translation Evaluation
4. https://ar5iv.org/html/2204.04306. [2204.04306] MMTAfrica: Multilingual Machine Translation for African Languages
5. https://ar5iv.org/html/2407.05975. [2407.05975] LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages
6. https://ar5iv.org/html/2401.16313. [2401.16313] Machine Translation Meta Evaluation through Translation Accuracy Challenge Sets
7. https://ar5iv.org/html/2301.08745. [2301.08745] Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine
8. https://ar5iv.org/html/2305.11550. [2305.11550] Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens
9. https://ar5iv.org/html/2409.00626. [2409.00626] Correcting FLORES Evaluation Dataset for Four African Languages

## Answer (I don't know?: ✗)

The FLORES-101 benchmark addresses several limitations present in previous test sets for low-resource languages by offering enhanced quality, breadth, and utility, specifically designed to benefit the machine translation (MT) community. Here are the key improvements:

1. **High-Quality Translations**: Unlike many existing datasets that suffer from quality issues due to semi-automatic construction methods, FLORES-101 features translations that are professionally crafted by human translators. This controlled process ensures a level of quality that is often absent in low-resource language datasets [1][2]. Additionally, a quality control mechanism was implemented, where a 20% sample of the dataset was reviewed by language-specific evaluators, further ensuring that only high-quality translations are included [4].

2. **Broad Coverage of Topics and Languages**: FLORES-101 consists of 3001 sentences sampled from English Wikipedia, covering a diverse array of topics and domains. This broad representation contrasts with prior benchmarks that may have focused solely on restricted domains or lacked coverage of low-resource languages. FLORES-101 encompasses 101 languages, providing a more extensive linguistic and thematic range than existing datasets [1][2].

3. **Many-to-Many Evaluation Capability**: FLORES-101 supports many-to-many evaluation, allowing seamless assessment of 10,100 language pairs. This feature is significant for evaluating multilingual MT systems and for translating regionally relevant language pairs, which previous benchmarks might not have facilitated as effectively. Such capability is pivotal for comprehensive assessments in multi-language translation scenarios, where systems often need to operate across various language pairs [2][4].

4. **Institutional Knowledge Documentation**: The FLORES-101 benchmark includes thorough documentation of the annotation process used to create the dataset. This documentation provides insights into how to construct MT datasets efficiently and effectively, building institutional knowledge that can guide future efforts in this area [2].

5. **Focus on Low-Resource Languages**: FLORES-101 is specifically designed to provide the first extensive evaluation benchmark for several low-resource languages. This important focus enables more accurate assessments of translation models on these languages, which are often neglected in standard datasets that tend to prioritize high-resource languages [4]. By addressing these low-resource languages, FLORES-101 helps to alleviate the imbalance in the MT field.

In summary, FLORES-101 represents a significant advancement over previous test sets for low-resource languages by offering high-quality, well-annotated translations spread across numerous languages and topics while enabling robust assessment capabilities for multilingual systems. This development is crucial for driving progress in the machine translation community, particularly concerning underrepresented languages.

1. [1]:  https://ar5iv.org/html/2106.03193, [2106.03193] The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation
2. [2]:  https://ar5iv.org/html/2106.03193, [2106.03193] The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation
3. [3]:  https://ar5iv.org/html/2404.13813, [2404.13813] From LLM to NMT: Advancing Low-Resource Machine Translation with Claude
4. [4]:  https://ar5iv.org/html/2409.00626, [2409.00626] Correcting FLORES Evaluation Dataset for Four African Languages
5. [5]:  https://ar5iv.org/html/2408.11457, [2408.11457] Expanding FLORES+ Benchmark for more Low-Resource Settings: Portuguese-Emakhuwa Machine Translation Evaluation
---
1. [1]:  Passage ID 1: evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures.In this work, we introduce the Flores-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains.These sentences have been translated in 101 languages by professional translators through a carefully controlled process.The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are multilingually aligned.By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.1 IntroductionMachine translation (MT) isone of the most successful applications in natural language processing, as
2. [2]:  Passage ID 2: scarce availability, and because it is non-trivial to check thequality of their work Guzmán et al. (2019).We present the Flores-101 benchmark, consisting of 3001 sentences sampled from English Wikipedia and professionally translated in 101 languages.With this dataset, we make several contributions. First, we provide the community with a high-quality benchmark that has much larger breadth of topics and coverage of low resource languages than any other existing dataset (§4). Second, Flores-101 is suitable for many-to-many evaluation, meaning that it enables seamless evaluation of 10,100 language pairs.This enables the evaluation of popular multilingual MT systems as well as the evaluation of regionally-relevant language pairs like Spanish-Aymara and Vietnamese-Thai, for example.Third, we thoroughly document the annotation process we followed (§3), helping the community build institutional knowledge about how to construct MT datasets. Fourth, we release not only sentences with
3. [3]:  Passage ID 3: between 100k and 1m bitexts, and high-resource if they have more than 1m bitexts, according to Team et al. (2022)222See https://tinyurl.com/535f7ust..We experiment on English and a selection of 36 other languages, of which 15 are high-resource, 17 are low-resource, and 4 are very low-resource. All languages are supported by Google Translate, NLLB-200 (Guzmán et al., 2019), and are included in the FLORES-200 dataset. Every language is evaluated in both the eng->xxx and xxx->eng directions. We do not conduct experiments on non-English-centric language pairs. The full list of languages is provided in Table LABEL:tab:language-table.3.2 DatasetsWe benchmark our model against the following datasets.3.2.1 FLORES-200FLORES-200 is a high-quality evaluation dataset containing human-curated translations between English and 204 different languages (Guzmán et al., 2019; Goyal et al., 2021; Team et al., 2022). The dataset serves as a universal benchmark for which to evaluate
4. [4]:  Passage ID 4: corrected datasets for future evaluation tasks.2 The FLORES Evaluation DatasetThe FLORES evaluation dataset consists of the first FLORES-101 Goyal et al. (2021) and the subsequent more expanded FLORES-200 NLLB Team et al. (2022) that included more languages.FLORES-101:This was the original evaluation data and it was created by translating English dataset collected from Wikipedia, consisting of several topics and domains, to 101 mostly low-resource languages. The dataset was the first available evaluation benchmark for several low-resource languages and it enabled the evaluation of many-to-many translation systems without pivoting through another high-resource language such as English. Several quality control mechanisms were put in place to ensure that the final dataset was of acceptable quality. To determine if translations are good enough for inclusion in FLORES-101, a 20% sample of the dataset were reviewed by language-specific evaluators who assess the quality using a
5. [5]:  Passage ID 5: Krishnan, Marc’Aurelio Ranzato, Francisco Guzman, and Angela Fan. 2021.The flores-101 evaluation benchmark for low-resource and multilingual machine translation.Guzmán et al. (2019)Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume Lample, Philipp Koehn, Vishrav Chaudhary, and Marc’Aurelio Ranzato. 2019.The FLORES evaluation datasets for low-resource machine translation: Nepali–English and Sinhala–English.In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6098–6111, Hong Kong, China. Association for Computational Linguistics.Kingma and Ba (2014)Diederik P. Kingma and Jimmy Ba. 2014.Adam: A method for stochastic optimization.CoRR, abs/1412.6980.Klein et al. (2017)Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. 2017.OpenNMT: Open-source toolkit for neural machine