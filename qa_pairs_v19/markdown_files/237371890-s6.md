# Survey of Low-Resource Machine Translation

## Question

How does the FLORES-101 benchmark improve upon previous test sets for low-resource languages?

## URLs

1. https://ar5iv.org/html/2106.03193. [2106.03193] The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation
2. https://ar5iv.org/html/2404.13813. [2404.13813] From LLM to NMT: Advancing Low-Resource Machine Translation with Claude
3. https://ar5iv.org/html/2408.11457. [2408.11457] Expanding FLORES+ Benchmark for more Low-Resource Settings: Portuguese-Emakhuwa Machine Translation Evaluation
4. https://ar5iv.org/html/2204.04306. [2204.04306] MMTAfrica: Multilingual Machine Translation for African Languages
5. https://ar5iv.org/html/2407.05975. [2407.05975] LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages
6. https://ar5iv.org/html/2401.16313. [2401.16313] Machine Translation Meta Evaluation through Translation Accuracy Challenge Sets
7. https://ar5iv.org/html/2301.08745. [2301.08745] Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine
8. https://ar5iv.org/html/2305.11550. [2305.11550] Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens
9. https://ar5iv.org/html/2409.00626. [2409.00626] Correcting FLORES Evaluation Dataset for Four African Languages

## Answer (I don't know?: ✗)

The FLORES-101 benchmark improves upon previous test sets for low-resource languages through several key contributions aimed at enhancing quality, coverage, and usability for multilingual machine translation (MT) systems. 

First, FLORES-101 provides significantly greater coverage of low-resource languages compared to prior datasets, offering a benchmark that includes sentences translated into 101 languages sampled from a wide variety of topics and domains within the English Wikipedia [1] [2]. This stands in contrast to other existing benchmarks, which often either lack extensive coverage of low-resource languages or are limited to specific, narrow domains [1]. The FLORES-101 dataset is specifically designed to enable evaluation across many language pairs, facilitating the assessment of both popular multilingual translation systems and more regionally relevant language pairs, such as Spanish-Aymara and Vietnamese-Thai [2].

Secondly, FLORES-101 allows for many-to-many evaluation without the need for pivoting through high-resource languages like English [4]. This feature is particularly beneficial for assessing MT systems that perform directly between low-resource language pairs, which was not possible with earlier benchmarks that often relied on English as an intermediary. The dataset enables seamless evaluation across 10,100 language pairs, making it potentially more versatile and comprehensive than prior datasets that lacked such depth [2].

Another critical improvement is the robust quality control mechanisms integrated into the dataset creation process. FLORES-101 employs a careful and professional translation process, where 20% of the dataset is reviewed by language-specific evaluators to ensure the quality of translations meets acceptable standards [4]. This focus on meticulous quality assessment is often absent in earlier datasets, which may have been developed using semi-automatic procedures leading to a lesser quality of translations [1].

Moreover, FLORES-101 offers thorough documentation of the annotation process followed during dataset construction, thereby helping to establish a framework for building future high-quality MT datasets. This institutional knowledge is crucial for the ongoing improvement of testing benchmarks in the field [2]. 

Finally, by intentionally incorporating low-resource languages and ensuring high-quality translations, FLORES-101 advances the state of the art in machine translation evaluation, providing a critical resource that reflects the challenges of real-world translation tasks in diverse linguistic contexts [1] [4]. 

In summary, FLORES-101 enhances previous test sets for low-resource languages through broader language coverage, many-to-many evaluation capabilities, stringent quality control, and detailed documentation, all of which collectively support the advancement of research and development in multilingual machine translation.

1. [1]:  https://ar5iv.org/html/2106.03193, [2106.03193] The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation
2. [2]:  https://ar5iv.org/html/2106.03193, [2106.03193] The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation
3. [3]:  https://ar5iv.org/html/2404.13813, [2404.13813] From LLM to NMT: Advancing Low-Resource Machine Translation with Claude
4. [4]:  https://ar5iv.org/html/2409.00626, [2409.00626] Correcting FLORES Evaluation Dataset for Four African Languages
5. [5]:  https://ar5iv.org/html/2408.11457, [2408.11457] Expanding FLORES+ Benchmark for more Low-Resource Settings: Portuguese-Emakhuwa Machine Translation Evaluation
---
1. [1]:  Passage ID 1: evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures.In this work, we introduce the Flores-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains.These sentences have been translated in 101 languages by professional translators through a carefully controlled process.The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are multilingually aligned.By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.1 IntroductionMachine translation (MT) isone of the most successful applications in natural language processing, as
2. [2]:  Passage ID 2: scarce availability, and because it is non-trivial to check thequality of their work Guzmán et al. (2019).We present the Flores-101 benchmark, consisting of 3001 sentences sampled from English Wikipedia and professionally translated in 101 languages.With this dataset, we make several contributions. First, we provide the community with a high-quality benchmark that has much larger breadth of topics and coverage of low resource languages than any other existing dataset (§4). Second, Flores-101 is suitable for many-to-many evaluation, meaning that it enables seamless evaluation of 10,100 language pairs.This enables the evaluation of popular multilingual MT systems as well as the evaluation of regionally-relevant language pairs like Spanish-Aymara and Vietnamese-Thai, for example.Third, we thoroughly document the annotation process we followed (§3), helping the community build institutional knowledge about how to construct MT datasets. Fourth, we release not only sentences with
3. [3]:  Passage ID 3: between 100k and 1m bitexts, and high-resource if they have more than 1m bitexts, according to Team et al. (2022)222See https://tinyurl.com/535f7ust..We experiment on English and a selection of 36 other languages, of which 15 are high-resource, 17 are low-resource, and 4 are very low-resource. All languages are supported by Google Translate, NLLB-200 (Guzmán et al., 2019), and are included in the FLORES-200 dataset. Every language is evaluated in both the eng->xxx and xxx->eng directions. We do not conduct experiments on non-English-centric language pairs. The full list of languages is provided in Table LABEL:tab:language-table.3.2 DatasetsWe benchmark our model against the following datasets.3.2.1 FLORES-200FLORES-200 is a high-quality evaluation dataset containing human-curated translations between English and 204 different languages (Guzmán et al., 2019; Goyal et al., 2021; Team et al., 2022). The dataset serves as a universal benchmark for which to evaluate
4. [4]:  Passage ID 4: corrected datasets for future evaluation tasks.2 The FLORES Evaluation DatasetThe FLORES evaluation dataset consists of the first FLORES-101 Goyal et al. (2021) and the subsequent more expanded FLORES-200 NLLB Team et al. (2022) that included more languages.FLORES-101:This was the original evaluation data and it was created by translating English dataset collected from Wikipedia, consisting of several topics and domains, to 101 mostly low-resource languages. The dataset was the first available evaluation benchmark for several low-resource languages and it enabled the evaluation of many-to-many translation systems without pivoting through another high-resource language such as English. Several quality control mechanisms were put in place to ensure that the final dataset was of acceptable quality. To determine if translations are good enough for inclusion in FLORES-101, a 20% sample of the dataset were reviewed by language-specific evaluators who assess the quality using a
5. [5]:  Passage ID 5: Krishnan, Marc’Aurelio Ranzato, Francisco Guzman, and Angela Fan. 2021.The flores-101 evaluation benchmark for low-resource and multilingual machine translation.Guzmán et al. (2019)Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume Lample, Philipp Koehn, Vishrav Chaudhary, and Marc’Aurelio Ranzato. 2019.The FLORES evaluation datasets for low-resource machine translation: Nepali–English and Sinhala–English.In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6098–6111, Hong Kong, China. Association for Computational Linguistics.Kingma and Ba (2014)Diederik P. Kingma and Jimmy Ba. 2014.Adam: A method for stochastic optimization.CoRR, abs/1412.6980.Klein et al. (2017)Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. 2017.OpenNMT: Open-source toolkit for neural machine