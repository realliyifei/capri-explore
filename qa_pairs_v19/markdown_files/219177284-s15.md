# Conversational Machine Comprehension: a Literature Review

## Question

What are the two main approaches to integrating conversational history in machine comprehension models?

## URLs

1. https://ar5iv.org/html/2006.00671. [2006.00671] Conversational Machine Comprehension: a Literature Review
2. https://ar5iv.org/html/1810.06683. [1810.06683] FlowQA: Grasping Flow in History for Conversational Machine Comprehension
3. https://ar5iv.org/html/2104.11394. [2104.11394] BERT-CoQAC: BERT-based Conversational Question Answering in Context
4. https://ar5iv.org/html/1905.05412. [1905.05412] BERT with History Answer Embedding for Conversational Question Answering
5. https://ar5iv.org/html/2405.13003. [2405.13003] A Survey on Recent Advances in Conversational Data Generation
6. https://ar5iv.org/html/1909.10743. [1909.10743] An Empirical Study of Content Understanding in Conversational Question Answering
7. https://ar5iv.org/html/2401.12576. [2401.12576] LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools
8. https://ar5iv.org/html/2407.12363. [2407.12363] Conversational Query Reformulation with the Guidance of Retrieved Documents

## Answer (I don't know?: ✗)

The two main approaches to integrating conversational history in machine comprehension models, as highlighted in the provided context, involve utilizing a pre-trained language model, specifically BERT, and implementing a history selection mechanism.

1. **Integration of a Pre-trained Language Model (BERT)**: The first approach introduces a framework based on BERT to incorporate conversational history into the neural machine comprehension system. BERT, a state-of-the-art language representation model, serves as the foundation for enhancing the comprehension capabilities of machine learning models when responding to questions in a conversational context. By leveraging the strengths of BERT, researchers aim to directly integrate the historical turns of conversation, thus improving the system's ability to generate contextually relevant responses [1][4].

2. **History Selection Mechanism**: The second approach proposes a history selection mechanism that identifies and utilizes the conversational turns that are deemed most relevant to the current question. This mechanism is essential as it mitigates the potential noise and unnecessary information caused by using entire conversational histories. By focusing on the most pertinent pieces of historical data, the model can enhance its responses while maintaining a succinct conversational context, resulting in improved performance in model outputs [1][4]. Experimentation and results demonstrate that carefully selecting relevant historical turns contributes significantly to the accuracy and coherence of the responses generated by the system [4].

These approaches are part of ongoing efforts to refine Conversational Question Answering (CQA) systems, which have traditionally struggled with understanding context over multiple turns in conversations. The implementation of these strategies helps to bridge the gap in addressing the challenges posed by conversational history in natural language processing tasks [1][5].

In summary, the integration of a pre-trained model like BERT and the implementation of a history selection mechanism represent the two main strategies for effectively managing conversational history in machine comprehension models. These approaches aim to enhance the quality and relevance of responses in multi-turn conversations, addressing prior limitations in conventional QA systems.

1. [1]:  https://ar5iv.org/html/2104.11394, [2104.11394] BERT-CoQAC: BERT-based Conversational Question Answering in Context
2. [2]:  https://ar5iv.org/html/2411.06284, No Title
3. [3]:  https://ar5iv.org/html/2411.06284, No Title
4. [4]:  https://ar5iv.org/html/2104.11394, [2104.11394] BERT-CoQAC: BERT-based Conversational Question Answering in Context
5. [5]:  https://ar5iv.org/html/2104.10810, No Title
---
1. [1]:  Passage ID 1: QA systems has always been a challenging task in natural language processing and used as a benchmark to evaluate machine’s ability of natural language understanding. However, such systems often struggle when the question answering is carried out in multiple turns by the users to seek more information based on what they have already learned, thus, giving rise to another complicated form called Conversational Question Answering (CQA). CQA systems are often criticized for not understanding or utilizing the previous context of the conversation when answering the questions. To address the research gap, in this paper, we explore how to integrate the conversational history into the neural machine comprehension system. On one hand, we introduce a framework based on publicly available pre-trained language model called BERT for incorporating history turns into the system. On the other hand, we propose a history selection mechanism that selects the turns that are relevant and contributes the most
2. [2]:  Passage ID 2: answering.The integration of multimodal data has opened up new possibilities for AI applications. MLLMs can generate detailed descriptions of images, providing valuable assistance in fields like accessibility and content creation. These models can answer questions about images, demonstrating their ability to understand and reason about visual content. MLLMs also enable the creation of rich multimedia content, combining text, images, and audio to produce engaging and informative outputs.The field of NLP has undergone significant transformations over the past few decades, driven by advancements in computational power, the availability of large-scale datasets, and breakthroughs in machine learning algorithms. This journey can be broadly categorized into several key phases:1.Rule-based systems (1950s-1980s): Early NLP approaches relied heavily on hand-crafted rules and linguistic knowledge bases. While these systems could perform well in narrow domains, they struggled with the
3. [3]:  Passage ID 3: answering.The integration of multimodal data has opened up new possibilities for AI applications. MLLMs can generate detailed descriptions of images, providing valuable assistance in fields like accessibility and content creation. These models can answer questions about images, demonstrating their ability to understand and reason about visual content. MLLMs also enable the creation of rich multimedia content, combining text, images, and audio to produce engaging and informative outputs.The field of NLP has undergone significant transformations over the past few decades, driven by advancements in computational power, the availability of large-scale datasets, and breakthroughs in machine learning algorithms. This journey can be broadly categorized into several key phases:1.Rule-based systems (1950s-1980s): Early NLP approaches relied heavily on hand-crafted rules and linguistic knowledge bases. While these systems could perform well in narrow domains, they struggled with the
4. [4]:  Passage ID 4: model called BERT for incorporating history turns into the system. On the other hand, we propose a history selection mechanism that selects the turns that are relevant and contributes the most to answer the current question. Experimentation results revealed that our framework is comparable in performance with the state-of-the-art models on the QuAC111http://quac.ai/ leader board. We also conduct a number of experiments to show the side effects of using entire context information which brings unnecessary information and noise signals resulting in a decline in the model’s performance.Keywords: Machine comprehension Information retrieval Deep Learning Deep Learning Applications1 IntroductionThe field of conversational AI can be divided in to three categories namely, goal-oriented dialogue systems, chat-oriented dialogue systems, and question answering (QA) dialogue systems.The former two have been very researched upon topics, resulting in a number of successful dialogue agents
5. [5]:  Passage ID 5: question answering systems (Gaoet al., 2018). Task-oriented dialogue systems are designed to complete a specific task on the user’s behalf such as booking hotels, making a restaurant reservation or finding products. The second category mainly focuses on carrying out a conversation with the user on open-domain topics, and question answering bots are designed to find an appropriate answer to user’s query using all its available knowledge and resources. Though, these systems have come a long way in terms of progress but conversing with such models for even a short amount of time quickly unveils the inconsistency in generated responses.A different number of strategies have been introduced over a period to address this issue. One of the standard methods of designing an NLP based project is to utilize word embeddings, pre-trained on a huge amount of unlabelled data using distributed word representations such as GloVe and Word2Vec, to initialize the first layer of the neural network. The