# Conversational Machine Comprehension: a Literature Review

## Question

What are the two main approaches to integrating conversational history in machine comprehension models?

## URLs

1. https://ar5iv.org/html/2006.00671. [2006.00671] Conversational Machine Comprehension: a Literature Review
2. https://ar5iv.org/html/1810.06683. [1810.06683] FlowQA: Grasping Flow in History for Conversational Machine Comprehension
3. https://ar5iv.org/html/2104.11394. [2104.11394] BERT-CoQAC: BERT-based Conversational Question Answering in Context
4. https://ar5iv.org/html/1905.05412. [1905.05412] BERT with History Answer Embedding for Conversational Question Answering
5. https://ar5iv.org/html/2405.13003. [2405.13003] A Survey on Recent Advances in Conversational Data Generation
6. https://ar5iv.org/html/1909.10743. [1909.10743] An Empirical Study of Content Understanding in Conversational Question Answering
7. https://ar5iv.org/html/2401.12576. [2401.12576] LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools
8. https://ar5iv.org/html/2407.12363. [2407.12363] Conversational Query Reformulation with the Guidance of Retrieved Documents

## Answer (I don't know?: ✗)

The two main approaches to integrating conversational history in machine comprehension models, as discussed in the context, involve the use of a framework based on the pre-trained language model BERT and a history selection mechanism. 

Firstly, the integration of conversational history is achieved by implementing a framework that utilizes BERT, a powerful pre-trained language model. This framework is tailored to incorporate the history turns from previous interactions into the neural machine comprehension system effectively. Doing so enhances the system's ability to understand and respond to questions based on the ongoing conversation, thereby addressing challenges faced in traditional QA systems that struggle with multi-turn interactions [1] [2].

Secondly, a critical component of this integration is the proposed history selection mechanism. This mechanism focuses on selecting the conversational turns that are deemed relevant and which contribute the most to the answer for the current question. By filtering out unnecessary context information, the history selection mechanism helps minimize noise in the system's inputs, which, as indicated by experimentation results, can otherwise lead to a decline in performance [5]. Thus, the combination of the BERT-based framework and the history selection mechanism represents a comprehensive approach to enhancing conversational question answering systems in machine comprehension tasks [1] [5]. 

Overall, these two approaches aim to improve the understanding of conversational context in machine comprehension models, ultimately leading to more accurate and contextually relevant answers during multi-turn interactions.

1. [1]:  https://ar5iv.org/html/2104.11394, [2104.11394] BERT-CoQAC: BERT-based Conversational Question Answering in Context
2. [2]:  https://ar5iv.org/html/2104.11394, [2104.11394] BERT-CoQAC: BERT-based Conversational Question Answering in Context
3. [3]:  https://ar5iv.org/html/2411.06284, No Title
4. [4]:  https://ar5iv.org/html/2411.06284, No Title
5. [5]:  https://ar5iv.org/html/2104.11394, [2104.11394] BERT-CoQAC: BERT-based Conversational Question Answering in Context
---
1. [1]:  Passage ID 1: QA systems has always been a challenging task in natural language processing and used as a benchmark to evaluate machine’s ability of natural language understanding. However, such systems often struggle when the question answering is carried out in multiple turns by the users to seek more information based on what they have already learned, thus, giving rise to another complicated form called Conversational Question Answering (CQA). CQA systems are often criticized for not understanding or utilizing the previous context of the conversation when answering the questions. To address the research gap, in this paper, we explore how to integrate the conversational history into the neural machine comprehension system. On one hand, we introduce a framework based on publicly available pre-trained language model called BERT for incorporating history turns into the system. On the other hand, we propose a history selection mechanism that selects the turns that are relevant and contributes the most
2. [2]:  Passage ID 2: QA systems has always been a challenging task in natural language processing and used as a benchmark to evaluate machine’s ability of natural language understanding. However, such systems often struggle when the question answering is carried out in multiple turns by the users to seek more information based on what they have already learned, thus, giving rise to another complicated form called Conversational Question Answering (CQA). CQA systems are often criticized for not understanding or utilizing the previous context of the conversation when answering the questions. To address the research gap, in this paper, we explore how to integrate the conversational history into the neural machine comprehension system. On one hand, we introduce a framework based on publicly available pre-trained language model called BERT for incorporating history turns into the system. On the other hand, we propose a history selection mechanism that selects the turns that are relevant and contributes the most
3. [3]:  Passage ID 3: answering.The integration of multimodal data has opened up new possibilities for AI applications. MLLMs can generate detailed descriptions of images, providing valuable assistance in fields like accessibility and content creation. These models can answer questions about images, demonstrating their ability to understand and reason about visual content. MLLMs also enable the creation of rich multimedia content, combining text, images, and audio to produce engaging and informative outputs.The field of NLP has undergone significant transformations over the past few decades, driven by advancements in computational power, the availability of large-scale datasets, and breakthroughs in machine learning algorithms. This journey can be broadly categorized into several key phases:1.Rule-based systems (1950s-1980s): Early NLP approaches relied heavily on hand-crafted rules and linguistic knowledge bases. While these systems could perform well in narrow domains, they struggled with the
4. [4]:  Passage ID 4: answering.The integration of multimodal data has opened up new possibilities for AI applications. MLLMs can generate detailed descriptions of images, providing valuable assistance in fields like accessibility and content creation. These models can answer questions about images, demonstrating their ability to understand and reason about visual content. MLLMs also enable the creation of rich multimedia content, combining text, images, and audio to produce engaging and informative outputs.The field of NLP has undergone significant transformations over the past few decades, driven by advancements in computational power, the availability of large-scale datasets, and breakthroughs in machine learning algorithms. This journey can be broadly categorized into several key phases:1.Rule-based systems (1950s-1980s): Early NLP approaches relied heavily on hand-crafted rules and linguistic knowledge bases. While these systems could perform well in narrow domains, they struggled with the
5. [5]:  Passage ID 5: model called BERT for incorporating history turns into the system. On the other hand, we propose a history selection mechanism that selects the turns that are relevant and contributes the most to answer the current question. Experimentation results revealed that our framework is comparable in performance with the state-of-the-art models on the QuAC111http://quac.ai/ leader board. We also conduct a number of experiments to show the side effects of using entire context information which brings unnecessary information and noise signals resulting in a decline in the model’s performance.Keywords: Machine comprehension Information retrieval Deep Learning Deep Learning Applications1 IntroductionThe field of conversational AI can be divided in to three categories namely, goal-oriented dialogue systems, chat-oriented dialogue systems, and question answering (QA) dialogue systems.The former two have been very researched upon topics, resulting in a number of successful dialogue agents