# Which *BERT? A Survey Organizing Contextualized Encoders

## Question

How are text representations evaluated in relation to downstream natural language understanding benchmarks?

## URLs

1. https://ar5iv.org/html/2410.18529. [2410.18529] A Systematic Survey on Instructional Text: From Representation Formats to Downstream NLP Tasks
2. https://ar5iv.org/html/2305.12641. [2305.12641] A Comprehensive Survey of Sentence Representations: From the BERT Epoch to the ChatGPT Era and Beyond
3. https://ar5iv.org/html/2406.01607. [2406.01607] Recent advances in universal text embeddings: A Comprehensive Review of Top-Performing Methods on the MTEB Benchmark
4. https://ar5iv.org/html/2409.02384. [2409.02384] STAB: Speech Tokenizer Assessment Benchmark
5. https://ar5iv.org/html/2409.16202. [2409.16202] CJEval: A Benchmark for Assessing Large Language Models Using Chinese Junior High School Exam Data
6. https://ar5iv.org/html/2411.12056. [2411.12056] Benchmarking pre-trained text embedding models in aligning built asset information
7. https://ar5iv.org/html/2410.11020. [2410.11020] Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning
8. https://ar5iv.org/html/2401.04842. [2401.04842] Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers
9. https://ar5iv.org/html/2408.04628. [2408.04628] LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP

## Answer (I don't know?: ✗)

The evaluation of text representations in relation to downstream natural language understanding (NLU) benchmarks involves several systematic approaches that focus on the performance of large language models (LLMs) in generating relevant answers or responses. One prominent technique is measuring the similarity between the embeddings of generated answers and the embeddings of known, relevant passages from retrieval benchmarks. This approach helps assess how effectively LLMs can produce contextually appropriate and semantically meaningful outputs [3] [4].

To implement this evaluation, researchers leverage various established benchmarks, such as the MS MACRO development set, the TREC Deep Learning datasets from 2019 and 2020. These benchmarks provide a foundation for examining the generative capabilities of different models, including various GPT-based and open-source LLMs. By employing prompts—some specifically designed to elicit reasonable but incorrect answers (termed "liar" prompts)—the models’ responses can be systematically assessed against the benchmarks [3].

Furthermore, an innovative evaluation framework has emerged that adapts existing retrieval benchmarks to assess the generated answers by LLMs. This framework allows for comparative analysis across different LLMs and prompts, facilitating a structured means of evaluating performance in question-answering tasks even when no standardized evaluation methods exist. It comprises two main approaches for empirical validation: one that assesses the similarity of generated answers to relevance-judged passages and another that compares the generated answers to the top results retrieved by a diverse set of retrieval models [4] [5].

The underlying premise of using retrieval benchmarks rests on their reliability as anchors for evaluating generated answers. They allow researchers to quantify improvements in generation models without heavily relying on resource-intensive human judgments. The similarity is measured through embedded representations of both generated answers and retrieved passages, reflecting a high correlation of performance concerning relevance judgments in information retrieval [4] [5].

In summary, the evaluation of text representations in NLU is centered around the comparative analysis of generated responses against established retrieval benchmarks. This evaluation framework ensures a structured approach to understanding the contextual coherence, relevance, and semantic accuracy of the outputs produced by various LLMs within complex NLP tasks [2] [5].

1. [1]:  https://ar5iv.org/html/2409.16202, [2409.16202] CJEval: A Benchmark for Assessing Large Language Models Using Chinese Junior High School Exam Data
2. [2]:  https://ar5iv.org/html/2401.04842, [2401.04842] Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers
3. [3]:  https://ar5iv.org/html/2401.04842, [2401.04842] Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers
4. [4]:  https://ar5iv.org/html/2401.04842, [2401.04842] Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers
5. [5]:  https://ar5iv.org/html/2401.04842, [2401.04842] Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers
---
1. [1]:  Passage ID 1: human-like text, answer complex questions, and perform a wide array of natural language processing (NLP) tasks. Models such as GPT (Achiam et al., 2023), LLaMA (Touvron et al., 2023), Baichuan (Yang et al., 2023), Qwen (Bai et al., 2023), GLM (Du et al., 2022), and DeepSeek-V2 (DeepSeek-AI, 2024) have set new standards, demonstrating exceptional performance across various benchmarks. These models have been widely adopted in both academic and industrial settings, highlighting their versatility and potential. However, the evaluation of LLMs is a multifaceted challenge that extends beyond mere accuracy in answering questions. It involves assessing their ability to understand context, generate coherent and relevant responses, and apply specialized knowledge effectively.Evaluation BenchmarksThe evaluation of LLMs has traditionally relied on established NLP benchmark datasets, such as SQuAD (Rajpurkar et al., 2016), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019), which focus
2. [2]:  Passage ID 2: answering: Learning to answer the whole question. In: International Conference on Learning Representations (2018)[30]Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., et al.: Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022)[31]Lin, C.Y.: Rouge: A package for automatic evaluation of summaries. In: Text summarization branches out. pp. 74–81 (2004)[32]Lin, J., Ma, X.: A few brief notes on deepimpact, coil, and a conceptual framework for information retrieval techniques. arXiv preprint arXiv:2106.14807 (2021)[33]Lin, J., Nogueira, R.F., Yates, A.: Pretrained transformers for text ranking: BERT and beyond. CoRR abs/2010.06467 (2020), https://arxiv.org/abs/2010.06467[34]Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., et al.: Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688 (2023)[35]Liu, Y.,
3. [3]:  Passage ID 3: human judgments.In both cases, we measure the similarity between an embedded representation of the generated answer and an embedded representation of a known, or assumed, relevant passage from the retrieval benchmark.In our experiments, we evaluate a range of generative models,including several GPT-based variants and open-source large language models using a variety of prompts,including “liar” prompts intended to produce reasonable but incorrect answers.For retrieval benchmarks, we use the MS MACRO dev set, the TREC Deep Learning 2019 dataset,and the TREC Deep Learning 2020 dataset.Our experimental results support the adaption of standard benchmarks to the evaluation of generated answers.1 IntroductionIn the evolving landscape of Natural Language Processing (NLP), Large Language Models (LLMs) have gained significant attention [8, 11]. These models have empowered numerous applications, offering capabilities that span from conversational systems to complex textual generation
4. [4]:  Passage ID 4: quality of retrieved answers.As models evolve and prompts are modified, we have no systematic way to measure improvements without resorting to expensive human judgments.To address this problem we adapt standard retrieval benchmarks to evaluate answers generated by large language models.Inspired by the BERTScore metric for summarization, we explore two approaches. In the first, we base our evaluation on the benchmark relevance judgments. We empirically run experiments on how information retrieval relevance judgments can be utilized as an anchor to evaluating the generated answers.In the second, we compare generated answers to the top results retrieved by a diverse set of retrieval models,ranging from traditional approaches to advanced methods,allowing us to measure improvements without human judgments.In both cases, we measure the similarity between an embedded representation of the generated answer and an embedded representation of a known, or assumed, relevant passage from the
5. [5]:  Passage ID 5: retrieval benchmarks serve as a reliable anchor for assessing the quality of generated answers.7 Concluding RemarksThis paper addresses the challenge of evaluating the quality of answers generated by large language models, specifically for question-answering tasks and in the absence of established evaluation methods. We propose and empirically validate an evaluation framework that leverages existing retrieval benchmarks to assess the generated answers. This framework allows for the comparison of different LLMs and prompts, providing a structured approach to evaluating generative question answering.We conduct empirical studies on two main approaches for evaluation. First, we measure the similarity between generated answers and relevance-judged passages from retrieval benchmarks, considering different relevance levels. We show the similarity of the embedding representation of generated/retrieved answers with relevance judgements in IR benchmark not only shows a high correlation