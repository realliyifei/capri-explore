# Grammatical Error Correction: A Survey of the State of the Art

## Question

How are errors classified and corrected in grammatical error correction, and what challenges arise with annotation schemes?

## URLs

1. https://ar5iv.org/html/2211.05166
2. https://ar5iv.org/html/2206.02280
3. https://ar5iv.org/html/2302.06407
4. https://ar5iv.org/html/2411.08344
5. https://ar5iv.org/html/2409.14335
6. https://ar5iv.org/html/2411.01544
7. https://ar5iv.org/html/2410.18889
8. https://ar5iv.org/html/1804.00540
9. https://ar5iv.org/html/2408.09688
10. https://ar5iv.org/html/2210.12484

## Answer

## I don't know?: False

Errors in grammatical error correction (GEC) are classified and corrected through various schemes that categorize them based on characteristics such as automatically detectable errors versus those requiring human assistance. Different researchers have developed classification systems for GEC errors, identifying several specific types. For instance, Naber (2003) describes four error types: spelling, style, grammar (syntax), and semantic errors, while Wagner et al. (2007) report on agreement errors and word-related issues such as missing words or extra words. Lee and Seneff (2008) focus on syntax and semantic errors, and Yuan (2017) identifies five categories including lexical, syntactic, semantic, discourse, and pragmatic errors [2]. 

The classification of errors is crucial, as it assists researchers in targeting correction efforts effectively by identifying the frequency and type of errors occurring in the text [1]. This leads to a more focused approach for GEC systems, enabling them to improve accuracy and efficiency in correcting prevalent mistakes. 

The correction of these errors often relies on shared task corpora that utilize various annotation schemes to create error-free texts from errorful ones, often through minimal modifications to retain perceived meaning. However, resources like the Lang-8 online language learning platform indicate that some corrections may involve extensive rewrites [3]. This raises questions about the reliability and completeness of the corrected outputs, particularly given that many errors considered by GEC systems include spelling and discourse-level mistakes, not strictly grammatical [3].

Challenges with annotation schemes also arise in GEC. The existing systems and definitions of errors may not comprehensively capture the nuances found in natural language use, especially in work from non-native speakers [4]. Furthermore, the presumed 'cleanliness' of training and test corpora often does not hold true, as noise—including grammatical errors—frequently appears in real-world data [5]. This variability, combined with the reliance on human effort in creating annotated datasets, amplifies the complexity of developing robust GEC tools.

Moreover, advanced machine learning models, particularly large language models (LLMs), have shown promise in detecting errors efficiently, even outpacing human annotators regarding accuracy and consistency [4]. As the capabilities of these models evolve, integrating their error detection strategies into GEC systems could further enhance the correction processes and overall NLP functionality.

In summary, errors in GEC are classified based on various characteristics, leading to tailored strategies for correction, but challenges persist with the adequacy of error definitions, the variability of natural language input, and the overhead associated with manual annotation tasks. Addressing these issues is essential for the advancement of effective grammatical error correction methodologies in NLP.

1. [1]:  https://ar5iv.org/html/1804.00540, [1804.00540] A Systematic Review of Automated Grammar Checking in English Language
2. [2]:  https://ar5iv.org/html/1804.00540, [1804.00540] A Systematic Review of Automated Grammar Checking in English Language
3. [3]:  https://ar5iv.org/html/2211.05166, [2211.05166] Grammatical Error Correction: A Survey of the State of the Art
4. [4]:  https://ar5iv.org/html/2410.18889, [2410.18889] Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance
5. [5]:  https://ar5iv.org/html/2005.05683, No Title
---
1. [1]:  Passage ID 1: are best suited for language learning but rule designing is a laborious task. Machine learning alleviates this labor but it is dependent on the size and type of the corpus used. Hybrid technique combines the best of both techniques but each part of the hybrid technique should be implemented according to its suitability.In this paper, we have also presented an error classification scheme which identifies five types of errors namely sentence structure errors, punctuation errors, spelling errors, syntax errors, and semantic errors. These errors are further subcategorized. This classification scheme would help the researchers and developers in following ways: (1) identifying the most frequent errors would tell what type of errors must be targeted for correction, (2) identifying the level of the error would tell what length of text should be examined to detect any error, (3) identifying the cause of invalid text would help in finding a solution to write a valid text. This simplifies the
2. [2]:  Passage ID 2: researchers have classified the errors in the corpus based on whether they are automatically detectable or needs human assistance. Naber(Naber, 2003) classifies various errors into four types namely spelling errors, style errors, grammar (syntax) errors and semantic errors. Wagner et al(Wagneret al., 2007) reports four types of errors namely agreement errors, real word spelling errors(contextual errors), missing word errors and extra word errors. Lee et al(Lee and Seneff, 2008) reports two types of errors namely syntax errors and semantic errors. Z Yuan in her doctoral thesis(Yuan, 2017) states five types of errors namely lexical errors, syntactic errors, semantic errors, discourse errors and pragmatic errors. Other than this, there is no general classification of grammar errors to the best of our knowledge. However an overview of major types of errors can be found in many web articles. Thus, we are highly motivated to suggest an error classification scheme. Please see figures 2 and
3. [3]:  Passage ID 3: practice, the task has increasingly been defined in terms of whatcorrections are annotated in corpora used for the shared tasks. Theseuse a variety of annotation schemes but all tend to adopt minimalmodifications of errorful texts to create error-free text with thesame perceived meaning. Other sources of annotated data, such as thatsourced from the online language learning platform Lang-8 (Mizumoto et al., 2012; Tajiri, Komachi, and Matsumoto, 2012), often contain much moreextensive rewrites of entire paragraphs of text. Given thisresource-derived definition of the task, systems are evaluated ontheir ability to correct all kinds of mistakes in text, including spelling and discourse level errors that have no or little grammatical reflex. The term ‘Grammatical’ Error Correction is thus something of a misnomer, but is nevertheless now commonly understood to encompass errors that are not always strictly grammatical in nature. A more descriptive term is Language Error
4. [4]:  Passage ID 4: that LLMs, particularly when highly confident, can effectively detect these errors, outperforming crowd workers in accuracy, consistency, and cost-efficiency. As LLM capabilities advance, their role in refining data quality will become central to improving NLP benchmarks. Future work could explore applying LLM-based error detection to a broader range of datasets and tasks, as well as refining methods for optimizing label correction strategies. We encourage researchers to adopt our methods and critically evaluate existing datasets to drive more robust, reliable results in the field.AcknowledgementsThis research is a collaboration between the Technion and Google Research, supported by the Google Cloud Research Credits program with the award GCP19980904.Ethics StatementWe address several ethical considerations related to human annotators and the research community.First, we recognize the significant human effort and cost involved in creating the datasets used in this study.
5. [5]:  Passage ID 5: facilitating various downstream natural language processing (NLP) tasks (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019b). However, they usually assume training and test corpora are clean and it is unclear how the models behave when confronted with noisy input. Grammatical error is an important type of noise since it naturally and frequently occurs in natural language, especially in spoken and written materials from non-native speakers. Dealing with such a noise reflects model robustness in representing language and grammatical knowledge. It would also have a positive social impact if language encoders can model texts from non-native speakers appropriately.Recent work on evaluating model’s behaviors against grammatical errors employs various methods, including (1) manually constructing minimal edited pairs on specific linguistic phenomena (Marvin and Linzen, 2018; Goldberg, 2019; Warstadt et al., 2019a, b); (2) labeling or creating acceptability judgment resources