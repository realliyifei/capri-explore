# Grammatical Error Correction: A Survey of the State of the Art

## Question

How are errors classified and corrected in grammatical error correction, and what challenges arise with annotation schemes?

## URLs

1. https://ar5iv.org/html/2211.05166. [2211.05166] Grammatical Error Correction: A Survey of the State of the Art
2. https://ar5iv.org/html/2206.02280. [2206.02280] Annotation Error Detection: Analyzing the Past and Present for a More Coherent Future
3. https://ar5iv.org/html/2302.06407. [2302.06407] Correcting Real-Word Spelling Errors: A New Hybrid Approach
4. https://ar5iv.org/html/2411.08344. [2411.08344] Bangla Grammatical Error Detection Leveraging Transformer-based Token Classification
5. https://ar5iv.org/html/2409.14335. [2409.14335] MQM-APE: Toward High-Quality Error Annotation Predictors with Automatic Post-Editing in LLM Translation Evaluators
6. https://ar5iv.org/html/2411.01544. [2411.01544] Building the Self-Improvement Loop: Error Detection and Correction in Goal-Oriented Semantic Communications
7. https://ar5iv.org/html/2410.18889. [2410.18889] Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance
8. https://ar5iv.org/html/1804.00540. [1804.00540] A Systematic Review of Automated Grammar Checking in English Language
9. https://ar5iv.org/html/2408.09688. [2408.09688] Recording for Eyes, Not Echoing to Ears: Contextualized Spoken-to-Written Conversion of ASR Transcripts
10. https://ar5iv.org/html/2210.12484. [2210.12484] SynGEC: Syntax-Enhanced Grammatical Error Correction with a Tailored GEC-Oriented Parser

## Answer (I don't know?: ✗)

In the field of grammatical error correction (GEC), errors are classified in various ways, primarily based on their characteristics and the methodologies employed to detect and correct them. Researchers have developed several classification schemes for understanding the nature of these errors.

One approach classifies errors based on the type of assistance needed for detection, distinguishing between those that can be automatically detected and those requiring human intervention. For instance, Naber (2003) categorizes errors into four types: spelling errors, style errors, grammar (syntax) errors, and semantic errors. Wagner et al. (2007) identify types such as agreement errors and missing or extra word errors, while Lee and Seneff (2008) focus on syntax and semantic errors. In her doctoral thesis, Yuan (2017) presents a broader categorization that includes lexical, syntactic, semantic, discourse, and pragmatic errors. However, it is noted that there is no widely accepted general classification of grammar errors [1][2].

The annotation process for these errors is also crucial in GEC. Annotated corpora serve as foundational data for developing and evaluating correction systems. These corpora can vary significantly; some utilize extensive rewrites (for example, from Lang-8) which include corrections of entire text passages, while others adopt minimal modifications to transform error-ridden texts into grammatically correct forms, thus maintaining the original meaning [3][4]. Systems are evaluated on their capacity to correct various mistake types, including those that might not conventionally be classified as grammatical but still impact text clarity, such as spelling and discourse-level errors [3].

Additionally, challenges arise from the inherent complexity of language and the required annotation schemes for training models effectively. Many existing NLP models assume that training and test data are clean, which is rarely the case in real-world applications, especially when the inputs come from non-native speakers. This makes the handling of grammatical errors a significant challenge, as these errors act as noise in the data [4].

Moreover, the development of effective error correction systems relies heavily on the size and type of corpus used. While machine learning techniques have advanced this field by alleviating labor-intensive tasks, their success is often dependent on the robustness of the training data [5]. Another notable challenge in GEC is reflecting the diversity of errors encountered in natural language use, as traditional categories often do not encompass the full range of grammatical errors present in different contexts [4][5].

In conclusion, while various error classification schemes exist, they face significant challenges related to annotation consistency and the nuanced nature of language. More research is necessary to address these challenges, refine classification systems, and improve the effectiveness of grammar checking tools in computational linguistics [1][5].

1. [1]:  https://ar5iv.org/html/1804.00540, [1804.00540] A Systematic Review of Automated Grammar Checking in English Language
2. [2]:  https://ar5iv.org/html/1804.00540, [1804.00540] A Systematic Review of Automated Grammar Checking in English Language
3. [3]:  https://ar5iv.org/html/2211.05166, [2211.05166] Grammatical Error Correction: A Survey of the State of the Art
4. [4]:  https://ar5iv.org/html/2005.05683, No Title
5. [5]:  https://ar5iv.org/html/1804.00540, [1804.00540] A Systematic Review of Automated Grammar Checking in English Language
---
1. [1]:  Passage ID 1: are best suited for language learning but rule designing is a laborious task. Machine learning alleviates this labor but it is dependent on the size and type of the corpus used. Hybrid technique combines the best of both techniques but each part of the hybrid technique should be implemented according to its suitability.In this paper, we have also presented an error classification scheme which identifies five types of errors namely sentence structure errors, punctuation errors, spelling errors, syntax errors, and semantic errors. These errors are further subcategorized. This classification scheme would help the researchers and developers in following ways: (1) identifying the most frequent errors would tell what type of errors must be targeted for correction, (2) identifying the level of the error would tell what length of text should be examined to detect any error, (3) identifying the cause of invalid text would help in finding a solution to write a valid text. This simplifies the
2. [2]:  Passage ID 2: researchers have classified the errors in the corpus based on whether they are automatically detectable or needs human assistance. Naber(Naber, 2003) classifies various errors into four types namely spelling errors, style errors, grammar (syntax) errors and semantic errors. Wagner et al(Wagneret al., 2007) reports four types of errors namely agreement errors, real word spelling errors(contextual errors), missing word errors and extra word errors. Lee et al(Lee and Seneff, 2008) reports two types of errors namely syntax errors and semantic errors. Z Yuan in her doctoral thesis(Yuan, 2017) states five types of errors namely lexical errors, syntactic errors, semantic errors, discourse errors and pragmatic errors. Other than this, there is no general classification of grammar errors to the best of our knowledge. However an overview of major types of errors can be found in many web articles. Thus, we are highly motivated to suggest an error classification scheme. Please see figures 2 and
3. [3]:  Passage ID 3: practice, the task has increasingly been defined in terms of whatcorrections are annotated in corpora used for the shared tasks. Theseuse a variety of annotation schemes but all tend to adopt minimalmodifications of errorful texts to create error-free text with thesame perceived meaning. Other sources of annotated data, such as thatsourced from the online language learning platform Lang-8 (Mizumoto et al., 2012; Tajiri, Komachi, and Matsumoto, 2012), often contain much moreextensive rewrites of entire paragraphs of text. Given thisresource-derived definition of the task, systems are evaluated ontheir ability to correct all kinds of mistakes in text, including spelling and discourse level errors that have no or little grammatical reflex. The term ‘Grammatical’ Error Correction is thus something of a misnomer, but is nevertheless now commonly understood to encompass errors that are not always strictly grammatical in nature. A more descriptive term is Language Error
4. [4]:  Passage ID 4: facilitating various downstream natural language processing (NLP) tasks (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019b). However, they usually assume training and test corpora are clean and it is unclear how the models behave when confronted with noisy input. Grammatical error is an important type of noise since it naturally and frequently occurs in natural language, especially in spoken and written materials from non-native speakers. Dealing with such a noise reflects model robustness in representing language and grammatical knowledge. It would also have a positive social impact if language encoders can model texts from non-native speakers appropriately.Recent work on evaluating model’s behaviors against grammatical errors employs various methods, including (1) manually constructing minimal edited pairs on specific linguistic phenomena (Marvin and Linzen, 2018; Goldberg, 2019; Warstadt et al., 2019a, b); (2) labeling or creating acceptability judgment resources
5. [5]:  Passage ID 5: Conclusions and Future ResearchGrammar checking is a major part of Natural Language Processing (NLP) whose applications ranges from proofreading to language learning. Much work has been done for the development of grammar checking tools in the past decade. However, fewer efforts are made for surveying the existing literature. Thus, we present a comprehensive study of English grammar checking techniques highlighting the capabilities and challenges associated with them. Also, we systematically selected, examined and reviewed 12 approaches of Grammar checking. The 12 approaches can be classified into three categories namely (1) Rule based technique, (2) Machine learning based technique, and (3) Hybrid technique. Each technique has its own advantages and limitations. Rule based techniques are best suited for language learning but rule designing is a laborious task. Machine learning alleviates this labor but it is dependent on the size and type of the corpus used. Hybrid technique