# Abstractive Meeting Summarization: A Survey

## Question

How do hierarchical transformers process long meeting transcripts using different levels of representation?

## URLs

1. https://ar5iv.org/html/2407.08330. [2407.08330] HDT: Hierarchical Document Transformer
2. https://ar5iv.org/html/2308.13191. [2308.13191] Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers
3. https://ar5iv.org/html/2311.04292. [2311.04292] Aspect-based Meeting Transcript Summarization: A Two-Stage Approach with Weak Supervision on Sentence Classification
4. https://ar5iv.org/html/2403.20262. [2403.20262] ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models
5. https://ar5iv.org/html/2111.08210. [2111.08210] Meeting Summarization with Pre-training and Clustering Methods
6. https://ar5iv.org/html/2307.15793. [2307.15793] Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system
7. https://ar5iv.org/html/2404.11124. [2404.11124] What’s under the hood: Investigating Automatic Metrics on Meeting Summarization
8. https://ar5iv.org/html/2004.02016. [2004.02016] A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining
9. https://ar5iv.org/html/2403.06872. [2403.06872] Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents
10. https://ar5iv.org/html/2410.05903. [2410.05903] Automatic Summarization of Long Documents

## Answer (I don't know?: ✓)

I don't know. The context provided does not include specific information about hierarchical transformers or their processing mechanisms related to long meeting transcripts. The passages focus on dialog act recognition, multi-label classification, and various aspects of speech recognition and classification but do not describe how hierarchical transformers handle long meeting transcripts specifically or their operational levels of representation in that context. Without direct references or relevant information pertaining to hierarchical transformers and their processing of meeting transcripts, it is impossible to formulate a comprehensive answer.

1. [1]:  https://ar5iv.org/html/2410.11627, No Title
2. [2]:  https://ar5iv.org/html/1907.12316, No Title
3. [3]:  https://ar5iv.org/html/1907.12316, No Title
4. [4]:  https://ar5iv.org/html/1907.12316, No Title
5. [5]:  https://ar5iv.org/html/2305.05862, No Title
---
1. [1]:  Passage ID 1: NLP.Nature Machine Intelligence, 5(10):1161–1174.Hupkes et al. (2018)Dieuwke Hupkes, Sara Veldhoen, and Willem Zuidema. 2018.Visualisation and’diagnostic classifiers’ reveal how recurrent andrecursive neural networks process hierarchical structure.Journal of Artificial Intelligence Research, 61:907–926.Kempe and Brooks (2008)Vera Kempe and Patricia J Brooks. 2008.Second language learning of complex inflectional systems.Language Learning, 58(4):703–746.Kim et al. (2016)Yoon Kim, Yacine Jernite, David Sontag, and Alexander Rush. 2016.Character-aware neural language models.In Proceedings of the AAAI conference on artificialintelligence, volume 30.Kudo (2018)Taku Kudo. 2018.Subword regularization:Improving neural network translation models with multiple subwordcandidates.In Proceedings of the 56th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers), pages 66–75,Melbourne, Australia. Association
2. [2]:  Passage ID 2: automatic recognition is important for a dialog system trying to understand its conversational partner. The study presented in this article approaches that task on the DIHANA corpus, whose three-level dialog act annotation scheme poses problems which have not been explored in recent studies. In addition to the hierarchical problem, the two lower levels pose multi-label classification problems. Furthermore, each level in the hierarchy refers to a different aspect concerning the intention of the speaker both in terms of the structure of the dialog and the task. Also, since its dialogs are in Spanish, it allows us to assess whether the state-of-the-art approaches on English data generalize to a different language. More specifically, we compare the performance of different segment representation approaches focusing on both sequences and patterns of words and assess the importance of the dialog history and the relations between the multiple levels of the hierarchy. Concerning the
3. [3]:  Passage ID 3: it is important to assess whether segment representation approaches based on character-level tokenization are able to capture additional information for predicting the task-specific labels. It would also be interesting to explore means to perform the hierarchical classification of the multiple levels using a single network instead of three independent classifiers. Finally, it is important to assess the decay in performance in a real scenario. That is, one in which the dialog system is not simulated and, thus, must deal with problems related to Automatic Speech Recognition (ASR) and use automatically predicted labels as context information.AcknowledgementsThis work was supported by national funds through Fundação para a Ciência e a Tecnologia (FCT) with reference UID/CEC/50021/2019 and by Universidade de Lisboa.We would like to thank the editorial board of Linguamática for allowing us to distribute this translated version of the article. When referring to this study, please
4. [4]:  Passage ID 4: approaches focusing on both sequences and patterns of words and assess the importance of the dialog history and the relations between the multiple levels of the hierarchy. Concerning the single-label classification problem posed by the top level, we show that the conclusions drawn on English data also hold on Spanish data. Furthermore, we show that the approaches can be adapted to multi-label scenarios. Finally, by hierarchically combining the best classifiers for each level, we achieve the best results reported for this corpus.Keywords Dialog Act Recognition, Hierarchical Classification, Multi-Label Classification, DIHANA Corpus.ALActive LearningAMIAMI Meeting CorpusASRAutomatic Speech RecognitionCHSCallHome SpanishCNNConvolutional Neural NetworkDNNDeep Neural NetworkDRLMDiscourse Relation Language ModelDSTC4Dialog State Tracking Challenge 4FCTFundação para a Ciência e a TecnologiaGloVeGlobal Vectors for Word RepresentationGPUGraphics Processing
5. [5]:  Passage ID 5: as named entity recognition (NER) and relation extraction (RE). The latter often requires more understanding of syntax and semantics in the input contexts as well as the interactions of labels in the output space as the structured prediction problems. Compared to sentiment analysis and text classification, question answering (QA) is often thought of as being harder as it often requires a model to understand the embedded internal logic and numerical operation/reasoning. Regarding financial knowledge, the existing classification and sentiment analysis datasets are sourced from daily news and social media. On the other hand, IE and QA data are often from professional documents like financial filings and reports, which usually require more domain knowledge to comprehend.Models.We test the representative state-of-the-art LLMs, ChatGPT and GPT-4 models. Specifically, we use gpt-3.5-turbo and GPT-4 (8k) for most of the experiments, except FinQA few-shot experiments, where the input