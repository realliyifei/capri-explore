# Abstractive Meeting Summarization: A Survey

## Question

How do hierarchical transformers process long meeting transcripts using different levels of representation?

## URLs

1. https://ar5iv.org/html/2407.08330
2. https://ar5iv.org/html/2308.13191
3. https://ar5iv.org/html/2311.04292
4. https://ar5iv.org/html/2403.20262
5. https://ar5iv.org/html/2111.08210
6. https://ar5iv.org/html/2307.15793
7. https://ar5iv.org/html/2404.11124
8. https://ar5iv.org/html/2004.02016
9. https://ar5iv.org/html/2403.06872
10. https://ar5iv.org/html/2410.05903

## Answer

I don't know. The context provided does not contain any information about hierarchical transformers or their specific application to processing long meeting transcripts through different levels of representation. The passages primarily discuss the evolution of deep learning models in natural language processing (NLP), particularly focusing on transformers like BERT and GPT, and their capabilities in various NLP tasks. However, they do not address the architecture or operational details of hierarchical transformers or any methodologies that relate specifically to meeting transcripts. Therefore, there is insufficient context to accurately answer the question.

[1]: https://ar5iv.org/html/2405.20585, No Title
[2]: https://ar5iv.org/html/2311.12351, No Title
[3]: https://ar5iv.org/html/2410.00427, No Title
[4]: https://ar5iv.org/html/2410.00427, No Title
[5]: https://ar5iv.org/html/2010.15036, No Title

[1]: Passage ID 1: Neural Networks (CNNs) [15] and Recurrent Neural Networks (RNNs) [16] were employed in early deep learning applications for NLP. They struggled with processing long-term dependencies and contextual information in large text sequences [17]. However, transformer architectures, notably the Bidirectional Encoder Representations from Transformers (BERT) [18], have recently set a new standard in NLP. These models are distinguished by their self-attention mechanism [17], which efficiently processes the relative significance of each word in a sentence, enhancing understanding of context and relationships within the text. This capability has led to transformers overperforming other models in various NLP tasks. For instance, in NER [19, 20], key entities in the text were identified and categorized, such as names of people, organizations, or locations; relation extraction transformers [21, 22, 23] discern and extract relationships between entities within a text; sentence similarity tasks [24, 25,
[2]: Passage ID 2: Additionally, we have established a repository where we curate relevant literature with real-time updates at https://github.com/Strivin0311/long-llms-learning.1 IntroductionIn recent years, leveraging techniques from deep learning [93], especially the surge of Transformer-based models like BERT [45], GPT [134, 135, 17] and their variants [97, 105, 137], Natural Language Processing (NLP) has significantly advanced, empowering machines to understand and generate human language [170, 98], thus revolutionizing numerous tasks in Natural Language Understanding (NLU) like sentiment analysis [206], Natural Language Generation (NLG) like document summarization [51], as well as other domains like computer vision [81] and autonomous driving [67]. Furthermore, in the wake of ChatGPT [121], PaLM [36], GPT4 [123, 122], etc, the Transformer-based Large Language Models (LLMs) which scale up to 1B∼similar-to\sim100B parameters to empower emergence abilities [183], have shown a new exhilarating
[3]: Passage ID 3: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
[4]: Passage ID 4: ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the laws of thermodynamics?” Following a quality inspection of the synthetically produced questions, we assembled a training dataset of 1601 examples, consisting of 120 questions for each of the 12 topics and 161 general questions. We also derived a test dataset containing 364 examples with a balanced class distribution similar to the training dataset.In our experiments, we evaluated three classification approaches: vector similarity search, prompting a LLM (GPT-3.5-Turbo), and few-shot fine-tuning of a transformer model with the SetFit framework Tunstall et al. (2022). Concerning the vector search approach with the SPECTER2 model, we measured the cosine similarity to compare vectors of embedded user queries with paper embeddings in our vector database to retrieve the 100 most similar
[5]: Passage ID 5: language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP related tasks. In the end, this survey briefly discusses the commonly used ML and DL based classifiers, evaluation metrics and the applications of these word embeddings in different NLP tasks.Text Mining, Natural Language Processing, Word representation, Language