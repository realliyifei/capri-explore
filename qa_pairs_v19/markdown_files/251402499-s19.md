# Abstractive Meeting Summarization: A Survey

## Question

How do hierarchical transformers process long meeting transcripts using different levels of representation?

## URLs

1. https://ar5iv.org/html/2407.08330. [2407.08330] HDT: Hierarchical Document Transformer
2. https://ar5iv.org/html/2308.13191. [2308.13191] Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers
3. https://ar5iv.org/html/2311.04292. [2311.04292] Aspect-based Meeting Transcript Summarization: A Two-Stage Approach with Weak Supervision on Sentence Classification
4. https://ar5iv.org/html/2403.20262. [2403.20262] ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models
5. https://ar5iv.org/html/2111.08210. [2111.08210] Meeting Summarization with Pre-training and Clustering Methods
6. https://ar5iv.org/html/2307.15793. [2307.15793] Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system
7. https://ar5iv.org/html/2404.11124. [2404.11124] What‚Äôs under the hood: Investigating Automatic Metrics on Meeting Summarization
8. https://ar5iv.org/html/2004.02016. [2004.02016] A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining
9. https://ar5iv.org/html/2403.06872. [2403.06872] Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents
10. https://ar5iv.org/html/2410.05903. [2410.05903] Automatic Summarization of Long Documents

## Answer (I don't know?: ‚úó)

Hierarchical transformers process long meeting transcripts by employing a two-level structure that enables effective handling of lengthy input and facilitates understanding of both individual turns and the overall context of the meeting. This approach is particularly useful given the typical characteristics of meeting transcripts, which are often comprised of thousands of tokens organized into multiple turns of dialogue.

1. **Two-Level Hierarchical Structure**: The first level is a word-level transformer that processes the token sequence of each individual turn (or utterance) made by a participant. Each token in a turn is associated with a representation obtained from a trainable embedding matrix, which captures both the syntactic and semantic aspects of the tokens through additional embedding matrices for part-of-speech (POS) and entity (ENT) tags [1][2]. The second level is a turn-level transformer, which aggregates the representations from all turns in the meeting to understand the context across the complete transcript. This hierarchical structure allows for detailed analysis at each level ‚Äî token-level within each turn and turn-level across the meeting [3].

2. **Role Vectors**: To further enhance the representation of the participants in the meeting, a role vector is employed for each participant during the encoding process. This role vector is appended to the turn-level representation to enrich the decoding stage, ensuring that the model accounts for who is speaking and the role they play in the conversation [1][4][5]. This addition helps to maintain the contextual integrity of the summaries generated, as different speakers may focus on different aspects of the discussion.

3. **Mitigating Computational Challenges**: Standard transformer architectures often struggle with very long sequences due to their computational intensity. To address this, the hierarchical design significantly reduces the computational burden associated with applying mechanisms like multi-head self-attention directly on large transcripts. By structuring the data hierarchically, the model can efficiently perform attention operations at both the token and turn levels, thereby minimizing memory overflow issues while processing complex and lengthy transcripts [3][4].

4. **Pretraining on Diverse Data**: Recognizing the potential scarcity of meeting summarization data, the model can be pretrained on larger datasets, such as news summaries. This pretraining enhances the model's capability to generate coherent summaries by exposing it to a broader range of language patterns and summarization techniques [4][5]. Empirical results show that such strategies lead to significant performance improvements, evidenced by higher ROUGE scores when evaluated against previous methods [5].

In summary, the use of hierarchical transformers in summarizing long meeting transcripts encapsulates a structured approach that combines token-level and turn-level understanding, participant role acknowledgment, and strategic pretraining, thereby yielding state-of-the-art performance in generating concise and meaningful meeting summaries [4][5].

1. [1]:  https://ar5iv.org/html/2111.08210, [2111.08210] Meeting Summarization with Pre-training and Clustering Methods
2. [2]:  https://ar5iv.org/html/2004.02016, [2004.02016] A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining
3. [3]:  https://ar5iv.org/html/2004.02016, [2004.02016] A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining
4. [4]:  https://ar5iv.org/html/2004.02016, [2004.02016] A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining
5. [5]:  https://ar5iv.org/html/2004.02016, [2004.02016] A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining
---
1. [1]:  Passage ID 1: to meeting summarization: 1) employ a two-level hierarchical structure: a word-level transformer to process the token sequence of one turn (i.e., an utterance of a participant) in the meeting, and a turn-level transformer to process the information of all m turns in a meeting; 2) train a role vector for each meeting participant during encoding, and append this role vector to the turn-level representation for later decoding.Zhang et¬†al. (2021) perform a comprehensive study on long dialogue summarization by investigating strategies to address the lengthy input problem. The authors investigate the following models: 1) a retrieve-then-summarize pipelineFeng et¬†al. (2020); 2) end-to-end summarization models that include BART (a transformer-based encoder-decoder model), HMNet (a hierarchical type of model)Zhu et¬†al. (2020), and Longformer (an extended transformer model). The authors compare performance of these models in generating summaries for long dialogue and find that the
2. [2]:  Passage ID 2: handle very long sequences, e.g. 5,000 tokens. However, meeting transcripts are usually fairly long, consisting of thousands of tokens.We note that meetings come with a natural multi-turn structure with a reasonable number of turns, e.g. 289 turns per meeting on average in AMI dataset. And the number of tokens in a turn is much less than that in the whole meeting. Therefore, we employ a two-level transformer structure to encode the meeting transcript.Word-level Transformer. The word-level transformer processes the token sequence of one turn in the meeting. We encode each token in one turn using a trainable embedding matrix ùíüùíü\mathcal{D}.Thus, the jùëój-th token in the iùëñi-th turn, wi,jsubscriptùë§ùëñùëów_{i,j}, is associated with a uniform length vector ùíü‚Äã(wi,j)=gi,jùíüsubscriptùë§ùëñùëósubscriptùëîùëñùëó\mathcal{D}(w_{i,j})=g_{i,j}. To incorporate syntactic and semantic information, we also train two embedding matrices to represent the part-of-speech (POS) and entity (ENT) tags. Therefore, the token
3. [3]:  Passage ID 3: architecture Vaswani et¬†al. (2017) to produce abstractive summaries based on meeting transcripts. To adapt the structure to meeting summarization, we propose two major design improvements.First, as meeting transcripts are usually lengthy, a direct application of the canonical transformer structure may not be feasible. For instance, conducting the multi-head self-attention mechanism on a transcript with thousands of tokens is very time consuming and may cause memory overflow problem. Therefore, we leverage a hierarchical structure to reduce the burden of computing. As a meeting consists of utterances from different participants, it forms a natural multi-turn hierarchy. Thus, the hierarchical structure carries out both token-level understanding within each turn and turn-level understanding across the whole meeting. During summary generation, HMNet applies attention to both levels of understanding to ensure that each part of the summary stems from different portions of the transcript
4. [4]:  Passage ID 4: mechanism on word-level and turn-level in the encoder-decoder structure for abstractive document summarization.7 ConclusionIn this paper, we present an end-to-end hierarchical neural network, HMNet, for abstractive meeting summarization. We employ a two-level hierarchical structure to adapt to the long meeting transcript, and a role vector to represent each participant. We also alleviate the data scarcity problem by pretraining on news summarization data. Experiments show that HMNet achieves state-of-the-art performance in both automatic metrics and human evaluation. Through an ablation study, we show that the role vector, hierarchical architecture, and pretraining all contribute to the model‚Äôs performance.For future work, we plan to utilize organizational chart, knowledge graph and topic modeling to generate better meeting summaries, which can better capture salient information from the transcript.AcknowledgementWe thank William Hinthorn for proof-reading this paper.
5. [5]:  Passage ID 5: structure and styles of meeting transcripts are quite different from articles and conversations. In this paper, we propose a novel abstractive summary network that adapts to the meeting scenario. We design a hierarchical structure to accommodate long meeting transcripts and a role vector to depict the difference among speakers. Furthermore, due to the inadequacy of meeting summary data, we pretrain the model on large-scale news summary data.Empirical results show that our model outperforms previous approaches in both automatic metrics and human evaluation. For example, on ICSI dataset, the ROUGE-1 score increases from 34.66% to 46.28%.1 IntroductionMeetings are a very common forum where people exchange ideas, make plans, and share information. With the ubiquity of automatic speech recognition systems come vast amounts of meeting transcripts. Therefore, the need to succinctly summarize the content of a meeting naturally arises.Meeting Transcript (163 turns)‚Ä¶PM: ‚Ä¶