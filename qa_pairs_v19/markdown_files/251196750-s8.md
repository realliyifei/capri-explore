# "Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking

## Question

How does adapting pre-trained language models (PLMs) to dialogues and specific target domains improve performance in dialogue state tracking (DST) tasks?

## URLs

1. https://ar5iv.org/html/2403.04656. [2403.04656] Chain of Thought Explanation for Dialogue State Tracking
2. https://ar5iv.org/html/2311.06345. [2311.06345] Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking
3. https://ar5iv.org/html/2405.12468. [2405.12468] Leveraging Diverse Data Generation for Adaptable Zero-Shot Dialogue State Tracking
4. https://ar5iv.org/html/2405.09744. [2405.09744] Many Hands Make Light Work: Task-Oriented Dialogue System with Module-Based Mixture-of-Experts
5. https://ar5iv.org/html/2311.15623. [2311.15623] Injecting linguistic knowledge into BERT for Dialogue State Tracking
6. https://ar5iv.org/html/2411.12307. [2411.12307] Balancing Accuracy and Efficiency in Multi-Turn Intent Classification for LLM-Powered Dialog Systems in Production
7. https://ar5iv.org/html/2401.16862. [2401.16862] State Value Generation with Prompt Learning and Self-Training for Low-Resource Dialogue State Tracking
8. https://ar5iv.org/html/2305.12077. [2305.12077] Few-Shot Dialogue Summarization via Skeleton-Assisted Prompt Transfer

## Answer (I don't know?: ✗)

Adapting pre-trained language models (PLMs) to dialogues and specific target domains can significantly enhance performance in dialogue state tracking (DST) tasks through several mechanisms:

1. **Understanding User Goals**: DST's primary function is to determine user goals based on the dialogue history. Adapting PLMs allows for better extraction and representation of these goals, often articulated as slot-value pairs linked to a specific domain schema. For example, in a "Find Flight" service, goals may include details such as (DepartureDate, next Wednesday) and (DestinationCity, LAX) [1] [2]. This understanding helps the dialogue system respond more effectively to user intents and provide pertinent information.

2. **Fine-Tuning Capabilities**: PLMs can be fine-tuned on domain-specific datasets, which helps them adapt their generic knowledge to the peculiarities and requirements of specific domains. Previous studies demonstrate that PLMs fine-tuned with a relevant corpus can achieve remarkable performance in DST tasks by learning to navigate the complexities involved in understanding user intentions within that context [1] [4]. This adaptation process leverages substantial training data, which improves the models' ability to generalize across varied user input while maintaining domain specificity.

3. **Incorporation of Contextual Knowledge**: Although some studies have avoided using external contextual knowledge, recent approaches employ computational methods to capture semantic patterns in the training corpus. This allows for the extraction of features that can guide attention to semantically important tokens in the dialogue, yielding a more contextually aware and efficient DST process [3]. Understanding contextual nuances can drastically enhance accuracy in interpreting user goals and constraints.

4. **Leveraging In-Context Learning**: The deployment of LLMs (Large Language Models) in DST has enabled advancements in zero-shot and few-shot learning approaches. These models can utilize prompt-based methods that include human-authored task descriptions or exemplars from context that improve their performance [4]. This flexibility is particularly useful in dynamically shifting dialogues where the user’s intent may evolve within a single conversation, making the model adaptable and responsive.

5. **Incorporation of Reasoning**: Recent work suggests that integrating reasoning explanations into generative dialogue models could bolster performance in DST tasks. By providing logical explanations or chain-of-thought reasoning, these models can better understand and explain the underlying reasoning for chosen slot-values, which aids in navigating more complex user queries and improves their generalization capacities [5]. This enhancement helps models to not only predict user goals but also justify their responses, thereby improving user trust and interaction quality.

In summary, adapting PLMs to dialogues and specific domains enhances DST performance through better user goal extraction, fine-tuning on relevant data, contextual understanding, and the inclusion of reasoning mechanisms. This comprehensive approach enables dialogue systems to operate more effectively and responsively within their specific application areas.

1. [1]:  https://ar5iv.org/html/2311.06345, [2311.06345] Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking
2. [2]:  https://ar5iv.org/html/2311.06345, [2311.06345] Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking
3. [3]:  https://ar5iv.org/html/2311.15623, [2311.15623] Injecting linguistic knowledge into BERT for Dialogue State Tracking
4. [4]:  https://ar5iv.org/html/2311.09758, No Title
5. [5]:  https://ar5iv.org/html/2403.04656, [2403.04656] Chain of Thought Explanation for Dialogue State Tracking
---
1. [1]:  Passage ID 1: systems, dialogue state tracking (DST) determines the user goals based on the previous dialogue turns, i.e. dialogue history. User goals are the tasks and purposes the user wants to accomplish through the dialogue and are typically represented as a set of pre-defined slot-value pairs corresponding to a domain-specific schema that consists of the required information to query the dialogue system. For instance, a typical “Find Flight” service has a set of airline names and their relevant characteristics (e.g., Price, DepartureTime, DepartureDate) as the dialogue constraints. The representations of user goals are a set of (slot, value) pairs, such as (DepartureDate, next Wednesday) and (DestinationCity, LAX) for the “Find Flight” service.Figure 1: Illustration of prompt-based adaptation methods: natural language prompt, in-context learning, and schema graph-induced prompt.Pre-trained language models (PLMs) achieved remarkable performance in task-oriented DST by fine-tuning with
2. [2]:  Passage ID 2: systems, dialogue state tracking (DST) determines the user goals based on the previous dialogue turns, i.e. dialogue history. User goals are the tasks and purposes the user wants to accomplish through the dialogue and are typically represented as a set of pre-defined slot-value pairs corresponding to a domain-specific schema that consists of the required information to query the dialogue system. For instance, a typical “Find Flight” service has a set of airline names and their relevant characteristics (e.g., Price, DepartureTime, DepartureDate) as the dialogue constraints. The representations of user goals are a set of (slot, value) pairs, such as (DepartureDate, next Wednesday) and (DestinationCity, LAX) for the “Find Flight” service.Figure 1: Illustration of prompt-based adaptation methods: natural language prompt, in-context learning, and schema graph-induced prompt.Pre-trained language models (PLMs) achieved remarkable performance in task-oriented DST by fine-tuning with
3. [3]:  Passage ID 3: studies, for simplicity, often avoided using external contextual knowledge, instead, relying on transformations of latent representations in PLMs as sources of contextual knowledge. Although these latent representations are context-aware, they remain susceptible to interpretability issues. Some solutions, such as using a specialized neural component to extract contextualized word representations Peters et al. (2018) Liu et al. (2019), require additional fine-tuning.In contrast, our approach employs a computationally efficient algorithm to capture semantic patterns in the entire training corpus and generates features that guide attention to semantically important tokens in the input sequence. This process and the resulting features can be easily analyzed and understood.3 Model3.1 Task: Dialogue State TrackingDialogue State Tracking (DST) involves receiving the dialogue history, the current dialogue turn, and dialogue state history as inputs and yields the updated dialogue
4. [4]:  Passage ID 4: dialogues involving structured data typically rely on Dialogue State Tracking (DST), where user intent is extracted from the dialogue history between a user and the agent in the form of slot values associated with a predefined schema. Pre-trained language models that are fine-tuned have been used in DST for a few years, including both autoregressive LMs Ham et al. (2020); Hosseini-Asl et al. (2020); Peng et al. (2020) and sequence-to-sequence models Lee et al. (2021); Su et al. (2022); Bang et al. (2023); Imrattanatrai and Fukuda (2023); Wang et al. (2023). These methods generally rely on the availability of a substantial training corpus to achieve good performance.LLMs enable zero-shot or few-shot learning Brown et al. (2020). For DST, Xie et al. (2022); Hudeček and Dušek (2023) prompts LLM with human-authored task descriptions or in-context exemplars. Hu et al. (2022) improves the in-context learning for DST performance by incorporating a retriever to fetch useful exemplars.
5. [5]:  Passage ID 5: the limited generalization capacity of these models when applied to complex scenarios in DST.This work is organized around the following three research questions:Q1: Can introducing reasoning explanations improve performance?Generative dialogue state tracking with pre-trained language (PLM) models has been proven to have decent performance Lee et al. (2021a); Shin et al. (2022) owing to PLMs’ inherent ability to understand the text-based slot names and corresponding slot schema. However, these methods still lack generalization ability according to Figure 2. We further claim that adding logical explanations could enhance generative DST models with actual reasoning ability. Chain-of-thought (CoT) Wei et al. (2022b) as a common practice used in PLMs to handle intricate tasks Chen et al. (2022b, a) by providing Reasoning Suzgun et al. (2022); Chen (2022); Zhou et al. (2022) or Explanantions Stacey et al. (2021); Marasovic et al. (2022); Lu et al. (2022). In this endeavor, we