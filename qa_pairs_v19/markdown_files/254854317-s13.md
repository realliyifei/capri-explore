# The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges

## Question

What statistical methods are commonly employed for text classification and sequence labeling in code-switching research?

## URLs

1. https://ar5iv.org/html/2212.09660. [2212.09660] The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges
2. https://ar5iv.org/html/2406.11289. [2406.11289] A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models
3. https://ar5iv.org/html/2410.02521. [2410.02521] Methods for Automatic Matrix Language Determination of Code-Switched Speech
4. https://ar5iv.org/html/2403.04872. [2403.04872] Code-Mixed Probes Show How Pre-Trained Models Generalise On Code-Switched Text
5. https://ar5iv.org/html/1904.08067. [1904.08067] 1 Introduction
6. https://ar5iv.org/html/2406.13361. [2406.13361] Improving Zero-Shot Cross-Lingual Transfer via Progressive Code-Switching
7. https://ar5iv.org/html/2107.06483. [2107.06483] From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text
8. https://ar5iv.org/html/2305.14534. [2305.14534] Detecting Propaganda Techniques in Code-Switched Social Media Text
9. https://ar5iv.org/html/2005.04322. [2005.04322] LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation

## Answer (I don't know?: ✗)

In code-switching research within the field of natural language processing (NLP), several statistical methods are commonly employed for text classification and sequence labeling tasks. These methods include Hidden Markov Models (HMM), Support Vector Machines (SVM), Maximum Entropy Models, and Conditional Random Fields (CRF).

1. **Hidden Markov Models (HMM)**: HMMs are statistical models that are useful in various speech-related tasks, including sequence labeling problems. They describe processes characterized by a set of hidden states and are applied to deal with the sequential nature of text data in code-switching scenarios [4].

2. **Support Vector Machines (SVM)**: SVMs are widely used for classification tasks. In the context of code-switching, they have been applied effectively, particularly in language identification and named entity recognition [2].

3. **Conditional Random Fields (CRF)**: CRFs have gained popularity in modeling sequence data and are particularly effective for tasks like part-of-speech tagging and named entity recognition in code-switched environments. Their strength lies in their ability to model the conditional relationships between variables efficiently, which is critical for sequence labeling [2].

4. **Maximum Entropy Models**: Although not as prominently mentioned in the context of specific studies within code-switching, they are part of the traditional toolbox for tackling classification problems within NLP, allowing for flexible modeling of different features derived from the training data [4].

These methods reflect a foundational reliance on traditional statistical approaches before the growing shift toward neural network-based techniques in NLP research. However, these statistical models remain critical, particularly in scenarios where labeled data is scarce or where interpretability is paramount [1] [3]. 

Overall, code-switching research has witnessed a gradual evolution in techniques from early statistical methods to more advanced neural approaches, but the foundational methods listed above continue to play a significant role in the field [1] [2].

1. [1]:  https://ar5iv.org/html/2005.04322, [2005.04322] LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation
2. [2]:  https://ar5iv.org/html/2212.09660, [2212.09660] The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges
3. [3]:  https://ar5iv.org/html/2212.09660, [2212.09660] The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges
4. [4]:  https://ar5iv.org/html/2011.06727, No Title
5. [5]:  https://ar5iv.org/html/2005.04322, [2005.04322] LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation
---
1. [1]:  Passage ID 1: interest from the NLP community. Many researchers have proposed novel methods to handle code-switched data, showing improvements on core NLP tasks such as language identification (LID), named entity recognition (NER), and part-of-speech (POS) tagging. However, many of these approaches are usually evaluated on a few language pairs and a specific domain, and it is not clear whether these models are exclusive to such scenarios or they can generalize to other tasks, domains, and language pairs.Moreover, research in code-switching currently has a slow process of comparison in which researchers have to replicate previous methods to report scores on different datasets.Furthermore, choosing the best-published model for benchmarking purposes is not an easy task either.These problems exist mainly because 1) there is no official benchmark for general code-switching evaluation that allows direct comparisons across multiple tasks, and 2) methods are usually not comprehensively evaluated across
2. [2]:  Passage ID 2: (SVM) Solorio and Liu (2008b). Conditional Random Field (CRF) Sutton et al. (2012) is also widely seen in the literature for sequence labeling, such as Part-of-Speech (POS) tagging Vyas et al. (2014), Named Entity Recognition (NER), and word-level language identification Lin et al. (2014); Chittaranjan et al. (2014); Jain and Bhat (2014). HMM-based models have been used in speech-related tasks, such as speech recognition Weiner et al. (2012a); Li and Fung (2013) and text synthesis Qian et al. (2008); Shuang et al. (2010); He et al. (2012).5.4 Utilizing Neural Networks(a) *CL(b) ISCAFigure 5: Methods used for code-mixing NLP.Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time. In contrast, the statistical and rule-based approaches are diminishing. Compared to ISCA, we see more adaptation of the pre-training model. This is because ACL work is more text-based focused, where pre-trained LMs are
3. [3]:  Passage ID 3: to develop models. We introduce a comprehensive systematic survey on code-switching research in natural language processing to understand the progress of the past decades and conceptualize the challenges and tasks on the code-switching topic. Finally, we summarize the trends and findings and conclude with a discussion for future direction and open questions for further investigation.1 IntroductionCode-Switching is the linguistic phenomenon where multilingual speakers use more than one language in the same conversation Poplack (1978). The fragment of the worldwide population that can be considered multilingual, i.e., speaks more than one language, far outnumbers monolingual speakers Tucker (2001); Winata et al. (2021a). This alone makes a compelling argument for developing NLP technology that can successfully process code-switched (CSW) data. However, it was not until the last couple of years that CSW-related research became more popular Sitaram et al. (2019); Jose et al. (2020);
4. [4]:  Passage ID 4: other problems such as dependency parsing [105, 60], semantic role labeling [82, 107], answer selection [132, 56], text error detection [93, 92], document summarization [77], constituent parsing [24], sub-event detection [4], emotion detection in dialogues [102] and complex word identification [25].II-B Traditional Machine Learning Based ApproachesThe traditional statistical machine learning techniques are the primary method for early sequence labeling problems. Based on the carefully designed features to represent each training data, the machine learning algorithms are utilized to train the model from example inputs and their expected outputs, learning to make predictions for unseen samples. Common statistical machine learning techniques include Hidden Markov Models (HMM) [21], Support Vector Machines (SVM) [32], Maximum Entropy Models [41] and Conditional Random Fields (CRF) [51].HMM is a statistical model used to describe a Markov process with implicit unknown states.
5. [5]:  Passage ID 5: for Computational Linguistics.Sitaram et al., 2019Sitaram, S., Chandu, K. R., Rallabandi, S. K., and Black, A. W.(2019).A survey of code-switched speech and language processing.arXiv preprint arXiv:1904.00784.Solorio andLiu, 2008Solorio, T. and Liu, Y.(2008).Part-of-speech tagging for english-spanish code-switched text.In Proceedings of the Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ’08, pages 1051–1060, Stroudsburg, PA, USA.Association for Computational Linguistics.Solorio et al., 2014Solorio, T., Blair, E., Maharjan, S., Bethard, S., Diab, M., Ghoneim, M.,Hawwari, A., AlGhamdi, F., Hirschberg, J., Chang, A., and Fung, P.(2014).Overview for the first shared task on language identification incode-switched data.In Proceedings of the First Workshop on Computational Approachesto Code Switching, pages 62–72, Doha, Qatar, October. Association forComputational Linguistics.Soto and Hirschberg, 2017Soto, V.