# The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges

## Question

What statistical methods are commonly employed for text classification and sequence labeling in code-switching research?

## URLs

1. https://ar5iv.org/html/2212.09660. [2212.09660] The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges
2. https://ar5iv.org/html/2406.11289. [2406.11289] A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models
3. https://ar5iv.org/html/2410.02521. [2410.02521] Methods for Automatic Matrix Language Determination of Code-Switched Speech
4. https://ar5iv.org/html/2403.04872. [2403.04872] Code-Mixed Probes Show How Pre-Trained Models Generalise On Code-Switched Text
5. https://ar5iv.org/html/1904.08067. [1904.08067] 1 Introduction
6. https://ar5iv.org/html/2406.13361. [2406.13361] Improving Zero-Shot Cross-Lingual Transfer via Progressive Code-Switching
7. https://ar5iv.org/html/2107.06483. [2107.06483] From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text
8. https://ar5iv.org/html/2305.14534. [2305.14534] Detecting Propaganda Techniques in Code-Switched Social Media Text
9. https://ar5iv.org/html/2005.04322. [2005.04322] LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation

## Answer (I don't know?: ✗)

In the field of Natural Language Processing (NLP), particularly in code-switching (CSW) research, several statistical methods are commonly employed for text classification and sequence labeling tasks. The literature indicates that the adoption of statistical methods has evolved significantly over the years alongside advancements in machine learning techniques.

For text classification in code-switching research, common statistical methods include Naive Bayes and Support Vector Machines (SVM). These methods have been extensively referenced in studies to handle various aspects of code-switched data [2] [5]. The Naive Bayes classifier is a probabilistic model that operates under the assumption of feature independence, making it a straightforward but effective choice for text classification tasks. Meanwhile, SVM has gained popularity due to its effectiveness in high-dimensional spaces and its ability to perform well with various types of data distribution.

In the area of sequence labeling, which includes tasks such as part-of-speech (POS) tagging and named entity recognition (NER), Conditional Random Fields (CRF) are widely utilized. CRFs are powerful for sequence modeling because they consider the relationship between neighboring labels and can capture dependencies, making them particularly suitable for tasks where the order of elements is important [2] [5]. This makes them a preferred choice for handling the complexities inherent in code-switched language data.

The historical trajectory of techniques used also shows a shift from rule-based approaches to statistical methods in code-switching research. Prior to 2006, most methodologies were grounded in rule-based systems, but the integration of statistical methods began to take prominence following advances in machine learning [5]. Thus, we see a broader acceptance of models like CRF, SVM, and statistical classifiers which have since evolved to be more sophisticated with the integration of neural networks and pre-trained models [1] [2].

In summary, the statistical methods that are extensively utilized in code-switching research for text classification include Naive Bayes and SVM, while CRFs are prevalent in sequence labeling tasks. The evolution of these methodologies reflects broader trends in NLP that prioritize data-driven approaches over traditional rule-based techniques.

1. [1]:  https://ar5iv.org/html/2005.04322, [2005.04322] LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation
2. [2]:  https://ar5iv.org/html/2212.09660, [2212.09660] The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges
3. [3]:  https://ar5iv.org/html/2212.09660, [2212.09660] The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges
4. [4]:  https://ar5iv.org/html/2005.04322, [2005.04322] LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation
5. [5]:  https://ar5iv.org/html/2212.09660, [2212.09660] The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges
---
1. [1]:  Passage ID 1: interest from the NLP community. Many researchers have proposed novel methods to handle code-switched data, showing improvements on core NLP tasks such as language identification (LID), named entity recognition (NER), and part-of-speech (POS) tagging. However, many of these approaches are usually evaluated on a few language pairs and a specific domain, and it is not clear whether these models are exclusive to such scenarios or they can generalize to other tasks, domains, and language pairs.Moreover, research in code-switching currently has a slow process of comparison in which researchers have to replicate previous methods to report scores on different datasets.Furthermore, choosing the best-published model for benchmarking purposes is not an easy task either.These problems exist mainly because 1) there is no official benchmark for general code-switching evaluation that allows direct comparisons across multiple tasks, and 2) methods are usually not comprehensively evaluated across
2. [2]:  Passage ID 2: (SVM) Solorio and Liu (2008b). Conditional Random Field (CRF) Sutton et al. (2012) is also widely seen in the literature for sequence labeling, such as Part-of-Speech (POS) tagging Vyas et al. (2014), Named Entity Recognition (NER), and word-level language identification Lin et al. (2014); Chittaranjan et al. (2014); Jain and Bhat (2014). HMM-based models have been used in speech-related tasks, such as speech recognition Weiner et al. (2012a); Li and Fung (2013) and text synthesis Qian et al. (2008); Shuang et al. (2010); He et al. (2012).5.4 Utilizing Neural Networks(a) *CL(b) ISCAFigure 5: Methods used for code-mixing NLP.Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time. In contrast, the statistical and rule-based approaches are diminishing. Compared to ISCA, we see more adaptation of the pre-training model. This is because ACL work is more text-based focused, where pre-trained LMs are
3. [3]:  Passage ID 3: to develop models. We introduce a comprehensive systematic survey on code-switching research in natural language processing to understand the progress of the past decades and conceptualize the challenges and tasks on the code-switching topic. Finally, we summarize the trends and findings and conclude with a discussion for future direction and open questions for further investigation.1 IntroductionCode-Switching is the linguistic phenomenon where multilingual speakers use more than one language in the same conversation Poplack (1978). The fragment of the worldwide population that can be considered multilingual, i.e., speaks more than one language, far outnumbers monolingual speakers Tucker (2001); Winata et al. (2021a). This alone makes a compelling argument for developing NLP technology that can successfully process code-switched (CSW) data. However, it was not until the last couple of years that CSW-related research became more popular Sitaram et al. (2019); Jose et al. (2020);
4. [4]:  Passage ID 4: for Computational Linguistics.Sitaram et al., 2019Sitaram, S., Chandu, K. R., Rallabandi, S. K., and Black, A. W.(2019).A survey of code-switched speech and language processing.arXiv preprint arXiv:1904.00784.Solorio andLiu, 2008Solorio, T. and Liu, Y.(2008).Part-of-speech tagging for english-spanish code-switched text.In Proceedings of the Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ’08, pages 1051–1060, Stroudsburg, PA, USA.Association for Computational Linguistics.Solorio et al., 2014Solorio, T., Blair, E., Maharjan, S., Bethard, S., Diab, M., Ghoneim, M.,Hawwari, A., AlGhamdi, F., Hirschberg, J., Chang, A., and Fung, P.(2014).Overview for the first shared task on language identification incode-switched data.In Proceedings of the First Workshop on Computational Approachesto Code Switching, pages 62–72, Doha, Qatar, October. Association forComputational Linguistics.Soto and Hirschberg, 2017Soto, V.
5. [5]:  Passage ID 5: CSW data, building a pointer-generator model to learn the real distribution of code-switched data Winata et al. (2019c). Chang et al. (2019) propose to generate CSW sentences from monolingual sentences using Generative Adversarial Network (GAN) Goodfellow et al. (2020) and the generator learns to predict CSW points without any linguistic knowledge.5.3 The Era of Statistical MethodsThe research on CSW is also influenced by the progress and development of machine learning. According to Figure 5, starting in 2006, statistical methods have been adapted to CSW research, while before that year, the approaches were mainly rule-based. There are common statistical methods for text classification used in the literature, such as Naive Bayes Solorio and Liu (2008a) and Support Vector Machine (SVM) Solorio and Liu (2008b). Conditional Random Field (CRF) Sutton et al. (2012) is also widely seen in the literature for sequence labeling, such as Part-of-Speech (POS) tagging Vyas et al. (2014),