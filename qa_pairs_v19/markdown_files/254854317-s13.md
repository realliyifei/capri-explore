# The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges

## Question

What statistical methods are commonly employed for text classification and sequence labeling in code-switching research?

## URLs

1. https://ar5iv.org/html/2212.09660
2. https://ar5iv.org/html/2406.11289
3. https://ar5iv.org/html/2410.02521
4. https://ar5iv.org/html/2403.04872
5. https://ar5iv.org/html/1904.08067
6. https://ar5iv.org/html/2406.13361
7. https://ar5iv.org/html/2107.06483
8. https://ar5iv.org/html/2305.14534
9. https://ar5iv.org/html/2005.04322

## Answer

## I don't know?: False

In code-switching research within the field of Natural Language Processing (NLP), several statistical methods are commonly employed for tasks such as text classification and sequence labeling. These methods primarily include Support Vector Machines (SVM), Conditional Random Fields (CRF), and Hidden Markov Models (HMM).

Support Vector Machines (SVM) have been utilized for various sequence labeling tasks, highlighting their effectiveness in handling code-switched data [2]. Researchers have found SVM to be particularly useful due to its robustness in classification problems, which can be applied to tasks such as Part-of-Speech (POS) tagging and Named Entity Recognition (NER) in code-switched contexts [2].

Conditional Random Fields (CRF) are also widely recognized in the NLP literature for their application in sequence labeling [2]. CRFs are particularly effective for modeling sequential data, which makes them suitable for tasks that involve predicting labels in a context-dependent manner, such as POS tagging and NER in mixed-language scenarios. This capability is crucial when dealing with code-switched text where the context can alter the meaning and use of words drastically.

Additionally, Hidden Markov Models (HMM) have been a foundational statistical model used in early research for sequence labeling problems in code-switching [4]. HMMs are designed to represent probability distributions over sequences, making them suitable for tasks featuring dependencies across time or sequential observations, like speech recognition and language identification [2][4].

Moreover, these statistical methods have been part of a broader landscape of techniques in code-switching NLP that has evolved to increasingly incorporate neural network-based approaches, especially in recent years [2]. The transition towards using pre-trained models has gained traction, driven by their effectiveness in handling diverse language tasks beyond traditional statistical methods. However, the methodologies like SVM, CRF, and HMM still play a significant role in the foundational framework of code-switching research [2][4].

In summary, the statistical methods commonly employed in code-switching research for text classification and sequence labeling are predominantly Support Vector Machines (SVM), Conditional Random Fields (CRF), and Hidden Markov Models (HMM). These techniques provide a solid basis for addressing the unique challenges presented by code-switched data in the NLP space.

1. [1]:  https://ar5iv.org/html/2005.04322, [2005.04322] LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation
2. [2]:  https://ar5iv.org/html/2212.09660, [2212.09660] The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges
3. [3]:  https://ar5iv.org/html/2212.09660, [2212.09660] The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges
4. [4]:  https://ar5iv.org/html/2011.06727, No Title
5. [5]:  https://ar5iv.org/html/2005.04322, [2005.04322] LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation
---
1. [1]:  Passage ID 1: interest from the NLP community. Many researchers have proposed novel methods to handle code-switched data, showing improvements on core NLP tasks such as language identification (LID), named entity recognition (NER), and part-of-speech (POS) tagging. However, many of these approaches are usually evaluated on a few language pairs and a specific domain, and it is not clear whether these models are exclusive to such scenarios or they can generalize to other tasks, domains, and language pairs.Moreover, research in code-switching currently has a slow process of comparison in which researchers have to replicate previous methods to report scores on different datasets.Furthermore, choosing the best-published model for benchmarking purposes is not an easy task either.These problems exist mainly because 1) there is no official benchmark for general code-switching evaluation that allows direct comparisons across multiple tasks, and 2) methods are usually not comprehensively evaluated across
2. [2]:  Passage ID 2: (SVM) Solorio and Liu (2008b). Conditional Random Field (CRF) Sutton et al. (2012) is also widely seen in the literature for sequence labeling, such as Part-of-Speech (POS) tagging Vyas et al. (2014), Named Entity Recognition (NER), and word-level language identification Lin et al. (2014); Chittaranjan et al. (2014); Jain and Bhat (2014). HMM-based models have been used in speech-related tasks, such as speech recognition Weiner et al. (2012a); Li and Fung (2013) and text synthesis Qian et al. (2008); Shuang et al. (2010); He et al. (2012).5.4 Utilizing Neural Networks(a) *CL(b) ISCAFigure 5: Methods used for code-mixing NLP.Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time. In contrast, the statistical and rule-based approaches are diminishing. Compared to ISCA, we see more adaptation of the pre-training model. This is because ACL work is more text-based focused, where pre-trained LMs are
3. [3]:  Passage ID 3: to develop models. We introduce a comprehensive systematic survey on code-switching research in natural language processing to understand the progress of the past decades and conceptualize the challenges and tasks on the code-switching topic. Finally, we summarize the trends and findings and conclude with a discussion for future direction and open questions for further investigation.1 IntroductionCode-Switching is the linguistic phenomenon where multilingual speakers use more than one language in the same conversation Poplack (1978). The fragment of the worldwide population that can be considered multilingual, i.e., speaks more than one language, far outnumbers monolingual speakers Tucker (2001); Winata et al. (2021a). This alone makes a compelling argument for developing NLP technology that can successfully process code-switched (CSW) data. However, it was not until the last couple of years that CSW-related research became more popular Sitaram et al. (2019); Jose et al. (2020);
4. [4]:  Passage ID 4: other problems such as dependency parsing [105, 60], semantic role labeling [82, 107], answer selection [132, 56], text error detection [93, 92], document summarization [77], constituent parsing [24], sub-event detection [4], emotion detection in dialogues [102] and complex word identification [25].II-B Traditional Machine Learning Based ApproachesThe traditional statistical machine learning techniques are the primary method for early sequence labeling problems. Based on the carefully designed features to represent each training data, the machine learning algorithms are utilized to train the model from example inputs and their expected outputs, learning to make predictions for unseen samples. Common statistical machine learning techniques include Hidden Markov Models (HMM) [21], Support Vector Machines (SVM) [32], Maximum Entropy Models [41] and Conditional Random Fields (CRF) [51].HMM is a statistical model used to describe a Markov process with implicit unknown states.
5. [5]:  Passage ID 5: for Computational Linguistics.Sitaram et al., 2019Sitaram, S., Chandu, K. R., Rallabandi, S. K., and Black, A. W.(2019).A survey of code-switched speech and language processing.arXiv preprint arXiv:1904.00784.Solorio andLiu, 2008Solorio, T. and Liu, Y.(2008).Part-of-speech tagging for english-spanish code-switched text.In Proceedings of the Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ’08, pages 1051–1060, Stroudsburg, PA, USA.Association for Computational Linguistics.Solorio et al., 2014Solorio, T., Blair, E., Maharjan, S., Bethard, S., Diab, M., Ghoneim, M.,Hawwari, A., AlGhamdi, F., Hirschberg, J., Chang, A., and Fung, P.(2014).Overview for the first shared task on language identification incode-switched data.In Proceedings of the First Workshop on Computational Approachesto Code Switching, pages 62–72, Doha, Qatar, October. Association forComputational Linguistics.Soto and Hirschberg, 2017Soto, V.