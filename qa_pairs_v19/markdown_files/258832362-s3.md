# Beyond Words: A Comprehensive Survey of Sentence Representations

## Question

How did Word2Vec overcome the limitations of traditional word representation methods in natural language processing?

## URLs

1. https://ar5iv.org/html/2404.14631. [2404.14631] Learning Word Embedding with Better Distance Weighting and Window Size Scheduling
2. https://ar5iv.org/html/2107.10413. [2107.10413] Theoretical foundations and limits of word embeddings: what types of meaning can they capture?
3. https://ar5iv.org/html/2010.15036. [2010.15036] A Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation Language Models
4. https://ar5iv.org/html/2110.01804. [2110.01804] A Survey On Neural Word Embeddings
5. https://ar5iv.org/html/2303.07196. [2303.07196] A Comprehensive Empirical Evaluation of Existing Word Embedding Approaches
6. https://ar5iv.org/html/2003.06279. [2003.06279] Using word embeddings to improve the discriminability of co-occurrence text networks
7. https://ar5iv.org/html/1706.02909. [1706.02909] Deriving a Representative Vector for Ontology Classes with Instance Word Vector Embeddings
8. https://ar5iv.org/html/2309.12058. [2309.12058] An Efficient Consolidation of Word Embedding and Deep Learning Techniques for Classifying Anticancer Peptides: FastText+BiLSTM
9. https://ar5iv.org/html/2411.05036. [2411.05036] From Word Vectors to Multimodal Embeddings: Techniques, Applications, and Future Directions For Large Language Models

## Answer (I don't know?: ✗)

Word2Vec overcame the limitations of traditional word representation methods, such as one-hot encoding, by employing continuous word representations that capture semantic and syntactic relationships more effectively. Traditional methods like one-hot encoding, where each word is represented as a high-dimensional vector with a “1” in a position corresponding to the word's index and “0” elsewhere, face significant challenges. Notably, this representation struggles with the curse of dimensionality in large vocabularies, which can reach into millions, and fails to understand relationships among words since all distances in this vector space are equal [1][5].

In contrast, Word2Vec utilizes distributed word representations, where words are expressed as dense vectors in a lower-dimensional space, typically consisting of a few hundred dimensions. This reduction allows for the encoding of more nuanced meanings and relationships between words. By learning word vectors from vast amounts of text data, Word2Vec captures semantic information effectively, enabling models to leverage these embeddings for a variety of natural language processing tasks [1][3][5]. 

Moreover, Word2Vec also addresses some of the shortcomings associated with static word representations. Continuous representation allows for the modeling of relationships through vector arithmetic; for instance, the vector operation that results in "king" - "man" + "woman" approximates "queen". This ability to capture contextual meanings and relationships between words provides a considerable advantage over traditional methods that treat each word independently without reflecting their relationships [2][4].

Despite its advancements, Word2Vec does have limitations, particularly concerning polysemy—where a single word has multiple meanings. Word2Vec represents all instances of a word with the same vector regardless of context, which may lead to inaccuracies when the context significantly changes the meaning of that word [3]. 

In summary, Word2Vec offers a robust alternative to traditional word representation methods by providing continuous, lower-dimensional embeddings that encapsulate the rich semantic and syntactic relationships inherent in language, thereby enhancing performance across various NLP tasks [2][4].

1. [1]:  https://ar5iv.org/html/2404.14631, [2404.14631] Learning Word Embedding with Better Distance Weighting and Window Size Scheduling
2. [2]:  https://ar5iv.org/html/2010.15036, [2010.15036] A Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation Language Models
3. [3]:  https://ar5iv.org/html/2010.15036, [2010.15036] A Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation Language Models
4. [4]:  https://ar5iv.org/html/2110.01804, [2110.01804] A Survey On Neural Word Embeddings
5. [5]:  https://ar5iv.org/html/2411.05036, [2411.05036] From Word Vectors to Multimodal Embeddings: Techniques, Applications, and Future Directions For Large Language Models
---
1. [1]:  Passage ID 1: of LFW and EDWS in enhancing Word2Vec’s performance, surpassing previous state-of-the-art methods.Keywords: Natural language processing Word embedding Word2Vec Learnable weights Window size scheduling1 IntroductionNLP researchers have long aimed to obtain high-quality word vector representations. One traditional approach is one-hot encoding, where a vector has a “1" at the index corresponding to the word and “0"s elsewhere. However, this approach suffers from the curse of dimensionality when dealing with large vocabulary sizes that can reach millions [2]. Additionally, one-hot encoding fails to capture syntactic or semantic properties because all word distances in the vector space are equal.Distributed word representations have been developed to overcome the limitations of one-hot encoding [8]. In this approach, words are represented by lower-dimensional vectors, typically a few hundred dimensions, where each element can take on various values [18]. These distributed word
2. [2]:  Passage ID 2: language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP related tasks. In the end, this survey briefly discusses the commonly used ML and DL based classifiers, evaluation metrics and the applications of these word embeddings in different NLP tasks.Text Mining, Natural Language Processing, Word representation, Language
3. [3]:  Passage ID 3: areas such as image detection, speech recognition, NLP etc. (LeCunet al., 2015). Continuous word representation models like Word2Vec , GloVe and FastText (Mikolov et al., 2013; Manning et al., 2014; Joulinet al., 2016; Bojanowski et al., 2016b) etc. have drastically improved the classification results and overcome shortcomings of categorical representations. It is found that having these continuous word representation of words is more affected as compared to traditional linguistic features because of their ability to capture more semantic and syntactic information of the textual data without losing much information. Despite their success, there are still some limitations which they are not capable of addressing such as they are unable to handle polysemy issues because they assign the same vector to word and ignores its context. Also, models like Word2Vec and GloVe assigns a random vector to a word which they did not encounter during training which means they are unable to handle out
4. [4]:  Passage ID 4: process.Neural word embeddings transformed the whole field of NLP by introducing substantial improvements in all NLP tasks. In this survey, we provide a comprehensive literature review on neural word embeddings. We give theoretical foundations and describe existing work by an interplay between word embeddings and language modeling. We provide broad coverage on neural word embeddings, including early word embeddings, embeddings targeting specific semantic relations, sense embeddings, morpheme embeddings, and finally, contextual representations. Finally, we describe benchmark datasets in word embeddings’ performance evaluation and downstream tasks along with the performance results of/due to word embeddings.1. IntroductionThe recent decade has witnessed a transformation in natural language processing (NLP). This transformation can be attributed to neural language models, their success in representation learning, and the transfer of this knowledge into complex NLP tasks.Before
5. [5]:  Passage ID 5: words. Each word is represented independently, with no intrinsic way to reflect similarities or relationships between words. This is a substantial limitation because capturing semantic and syntactic relationships is crucial for many natural language processing (NLP) tasks [39]. For instance, as discussed in [14], in an n-gram language model, predicting the next word in a sequence relies on word co-occurrence. If certain words never co-occur, the model cannot infer any relationship between them. Thus, one-hot encoding fails to capture the contextual similarity needed for effective language modeling and other NLP tasks, such as semantic search, knowledge base question answering, and machine translation [23, 40].II-B2 Word EmbeddingsWord embeddings address the limitations of one-hot encoding by representing words as dense, low-dimensional vectors learned from large corpora [10]. These vectors capture semantic and syntactic relationships, mapping words into a continuous vector space