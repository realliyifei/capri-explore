# Beyond Words: A Comprehensive Survey of Sentence Representations

## Question

How did Word2Vec overcome the limitations of traditional word representation methods in natural language processing?

## URLs

1. https://ar5iv.org/html/2404.14631
2. https://ar5iv.org/html/2107.10413
3. https://ar5iv.org/html/2010.15036
4. https://ar5iv.org/html/2110.01804
5. https://ar5iv.org/html/2303.07196
6. https://ar5iv.org/html/2003.06279
7. https://ar5iv.org/html/1706.02909
8. https://ar5iv.org/html/2309.12058
9. https://ar5iv.org/html/2411.05036

## Answer

Word2Vec has significantly advanced natural language processing (NLP) by addressing the limitations inherent in traditional word representation methods, such as one-hot encoding. One-hot encoding results in high-dimensional vectors where word similarity cannot be expressed due to equal distances between all words, failing to capture any semantic or syntactic relationships [1]. In contrast, Word2Vec employs a distributed word representation strategy, generating lower-dimensional vectors that can capture meaningful relationships between words through their context.

One of the primary mechanisms through which Word2Vec achieves this is its use of continuous word representations. Instead of assigning a fixed vector for each word that doesn't reflect its contextual usage, Word2Vec uses context-based prediction methods—specifically, the Continuous Bag of Words (CBOW) and Skip-gram models. These methods allow the model to learn word embeddings based on their surrounding context, effectively capturing semantic relationships that traditional methods overlook [5]. The result is that similar words tend to have similar vector representations, which helps in various NLP applications, such as sentiment analysis and language modeling [3].

Additionally, Word2Vec maintains critical semantic information in the representations it generates. This property is crucial since traditional categorical models often lose syntactic and semantic nuances inherent in language [4]. The embeddings generated by Word2Vec—being dense vectors with lower dimensionality—facilitate better performance in machine learning tasks, as these embeddings can be efficiently used to train classifiers and other models for various NLP tasks [2][5].

However, despite its advancements, Word2Vec is not without limitations. A notable drawback is its inability to address polysemy effectively; that is, a word with multiple meanings receives a single vector regardless of the different contexts it appears in. This can lead to misunderstandings in interpretation, as the model does not differentiate between the different senses of a word [3]. 

In summary, Word2Vec overcomes the limitations of traditional word representation methods by utilizing distributed and continuous word embeddings that capture semantic and syntactic information through context-based training. This approach allows for more nuanced and efficient representations of language compared to fixed-size, high-dimensional vectors from methods like one-hot encoding, thus enhancing performance across various NLP tasks [1][2][5].

[1]: https://ar5iv.org/html/2404.14631, [2404.14631] Learning Word Embedding with Better Distance Weighting and Window Size Scheduling
[2]: https://ar5iv.org/html/2010.15036, [2010.15036] A Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation Language Models
[3]: https://ar5iv.org/html/2010.15036, [2010.15036] A Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation Language Models
[4]: https://ar5iv.org/html/2110.01804, [2110.01804] A Survey On Neural Word Embeddings
[5]: https://ar5iv.org/html/2010.15036, [2010.15036] A Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation Language Models

[1]: Passage ID 1: of LFW and EDWS in enhancing Word2Vec’s performance, surpassing previous state-of-the-art methods.Keywords: Natural language processing Word embedding Word2Vec Learnable weights Window size scheduling1 IntroductionNLP researchers have long aimed to obtain high-quality word vector representations. One traditional approach is one-hot encoding, where a vector has a “1" at the index corresponding to the word and “0"s elsewhere. However, this approach suffers from the curse of dimensionality when dealing with large vocabulary sizes that can reach millions [2]. Additionally, one-hot encoding fails to capture syntactic or semantic properties because all word distances in the vector space are equal.Distributed word representations have been developed to overcome the limitations of one-hot encoding [8]. In this approach, words are represented by lower-dimensional vectors, typically a few hundred dimensions, where each element can take on various values [18]. These distributed word
[2]: Passage ID 2: language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP related tasks. In the end, this survey briefly discusses the commonly used ML and DL based classifiers, evaluation metrics and the applications of these word embeddings in different NLP tasks.Text Mining, Natural Language Processing, Word representation, Language
[3]: Passage ID 3: areas such as image detection, speech recognition, NLP etc. (LeCunet al., 2015). Continuous word representation models like Word2Vec , GloVe and FastText (Mikolov et al., 2013; Manning et al., 2014; Joulinet al., 2016; Bojanowski et al., 2016b) etc. have drastically improved the classification results and overcome shortcomings of categorical representations. It is found that having these continuous word representation of words is more affected as compared to traditional linguistic features because of their ability to capture more semantic and syntactic information of the textual data without losing much information. Despite their success, there are still some limitations which they are not capable of addressing such as they are unable to handle polysemy issues because they assign the same vector to word and ignores its context. Also, models like Word2Vec and GloVe assigns a random vector to a word which they did not encounter during training which means they are unable to handle out
[4]: Passage ID 4: process.Neural word embeddings transformed the whole field of NLP by introducing substantial improvements in all NLP tasks. In this survey, we provide a comprehensive literature review on neural word embeddings. We give theoretical foundations and describe existing work by an interplay between word embeddings and language modeling. We provide broad coverage on neural word embeddings, including early word embeddings, embeddings targeting specific semantic relations, sense embeddings, morpheme embeddings, and finally, contextual representations. Finally, we describe benchmark datasets in word embeddings’ performance evaluation and downstream tasks along with the performance results of/due to word embeddings.1. IntroductionThe recent decade has witnessed a transformation in natural language processing (NLP). This transformation can be attributed to neural language models, their success in representation learning, and the transfer of this knowledge into complex NLP tasks.Before
[5]: Passage ID 5: in a deep neural network-based model. These word embedding are pre-trained by predicting a word based on its context without losing semantic and syntactical information. Thus, using these embedding techniques have demonstrated to be helpful in many NLP tasks because It does not lose the order of words and captures the meaning of words (syntactic and semantic information of words). However, the popularity of word representation methods are due to two famous models, Word2Vec (Mikolov et al., 2013) and GloVe (Manning et al., 2014). These famous, along with others, are briefly discussed below.•Word2VecWord2vec is words representation model developed by (Mikolov et al., 2013). This model uses two hidden layers which are used in a shallow neural network to create a vector of each word. The word vectors captured by Continuous Bag of words (CBOW) and Skip-gram models of word2vec are supposed to have semantic and syntactic information of words. To have a better representation of