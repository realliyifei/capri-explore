# The Lifecycle of "Facts": A Survey of Social Bias in Knowledge Graphs

## Question

What is the difference between demographic parity and predictive parity in assessing fairness in link prediction tasks?

## URLs

1. https://ar5iv.org/html/2409.03893
2. https://ar5iv.org/html/2201.11596
3. https://ar5iv.org/html/2402.02663
4. https://ar5iv.org/html/2205.05396
5. https://ar5iv.org/html/2403.17333
6. https://ar5iv.org/html/2411.02414
7. https://ar5iv.org/html/2411.04371
8. https://ar5iv.org/html/2410.09600
9. https://ar5iv.org/html/2409.08658

## Answer

I don't know. The context provided does not contain sufficient information regarding predictive parity. While it thoroughly explains demographic parity's definition, applications, and constraints in the context of link prediction [1][2][3][4][5], there is no mention or explanation of predictive parity. Without details on predictive parity, I cannot compare the two fairness concepts. Therefore, the context is insufficient to address the differences between demographic parity and predictive parity in assessing fairness in link prediction tasks.

[1]: https://ar5iv.org/html/2201.11596, [2201.11596] FairEGM: Fair Link Prediction and Recommendation via Emulated Graph Modification
[2]: https://ar5iv.org/html/2409.08658, [2409.08658] Promoting Fairness in Link Prediction with Graph Enhancement
[3]: https://ar5iv.org/html/2201.11596, [2201.11596] FairEGM: Fair Link Prediction and Recommendation via Emulated Graph Modification
[4]: https://ar5iv.org/html/2201.11596, [2201.11596] FairEGM: Fair Link Prediction and Recommendation via Emulated Graph Modification
[5]: https://ar5iv.org/html/2409.08658, [2409.08658] Promoting Fairness in Link Prediction with Graph Enhancement

[1]: Passage ID 1: focus on the fair link prediction task. We select demographic parity (Gajane and Pechenizkiy, 2017) as our fairness criteria. Informally, demographic parity is satisfied if the output of the model is not dependent on a given sensitive attribute (Mehrabi et al., 2019). Formally, we define the demographic parity fairness criteria on the link prediction problem as follows.Definition 0 (Link Prediction with Demographic Parity Fairness).Given a graph 𝑮=(𝑽,𝑬)𝑮𝑽𝑬\bm{G}=(\bm{V},\bm{E}), a node v𝑣v, and an embedding model M𝑀M, let LM​(v)=(u1,…,un)subscript𝐿𝑀𝑣subscript𝑢1…subscript𝑢𝑛L_{M}(v)=(u_{1},...,u_{n}) be the set of nodes that have the highest likelihood to form a link with node v𝑣v where LM​(v)subscript𝐿𝑀𝑣L_{M}(v) is computed using the model M𝑀M. The link prediction problem with demographic parity fairness for node v𝑣v has the following constraint: D​(PLM​(v),PS)=0𝐷subscript𝑃subscript𝐿𝑀𝑣subscript𝑃𝑆0D(P_{L_{M}(v)},P_{S})=0 where D𝐷D is the distance metric between distributions,
[2]: Passage ID 2: classification as the condition where the predicted label is independent of the sensitive attribute. In the domain of link prediction, which involves estimating the probability of a link between pairs of nodes in a graph, fairness can be extended by ensuring that the estimated probability is independent of the sensitive attributes of the two nodes involved. In this subsection, we introduce two fairness concepts relevant to link prediction: demographic parity and equal opportunity.3.2.1 Demographic ParityDemographic Parity (DP) requires that predictions are independent of the sensitive attribute. It has been extensively applied in previous fair machine learning studies, and by replacing the classification probability with link prediction probability, it can be simply extended in the context of link prediction. It is also named dyadic fairness in previous literature (Li et al., 2020). In the context of link prediction, the DP requires the prediction probabilities should be
[3]: Passage ID 3: parity fairness for node v𝑣v has the following constraint: D​(PLM​(v),PS)=0𝐷subscript𝑃subscript𝐿𝑀𝑣subscript𝑃𝑆0D(P_{L_{M}(v)},P_{S})=0 where D𝐷D is the distance metric between distributions, PLM​(v)subscript𝑃subscript𝐿𝑀𝑣P_{L_{M}(v)} is the distribution of sensitive attributes over the recommended nodes set LM​(v)subscript𝐿𝑀𝑣L_{M}(v), and PSsubscript𝑃𝑆P_{S} is the distribution of sensitive attributes on the overall graph.The link prediction problem with demographic parity fairness states that the distribution of sensitive attributes over recommended nodes (LM​(v)subscript𝐿𝑀𝑣L_{M}(v)) should not be distinguishable from the distribution of sensitive attributes on the overall graph (PSsubscript𝑃𝑆P_{S}). Note that the model can perform link recommendation by performing either K-nearest neighbors or by using a classifier.Problem Statement: We want to learn a graph neural network (GNN) model M𝑀M such that M𝑀M performs well on the link prediction task while satisfying the demographic
[4]: Passage ID 4: proliferation of graph-based recommendation models (Ying et al., 2018), we incorporate fairness constraints through demographic parity (Gajane and Pechenizkiy, 2017). Informally, demographic parity seeks to ensure that each group with a particular sensitive attribute receives the positive outcome at the same rate as other groups with different values for the same sensitive attribute (Gajane and Pechenizkiy, 2017). For example, a professional networking website can utilize a GNN-based recommendation engine to recommend a job opening to a certain number of individuals. Demographic parity based recommendations could ensure that no individuals of a particular race, ethnicity, or gender are less likely to receive such a recommendation. The same can be said for social networking sites; if a recommendation algorithm is not fair under the definition of demographic parity, then the algorithm will suggest individuals with similar demographics to be linked, reinforcing preexisting ties and
[5]: Passage ID 5: all datasets and achieves the best utility on Pubmed, DBLP, and Google+.6 Related Work6.1 Fairness in Machine LearningIn recent years, numerous fairness definitions in machine learning have been proposed. These definitions generally fall into two categories: (1) Group fairness, which aims to ensure that certain statistical measures are approximately equal across protected groups (e.g., racial or gender groups) (Feldman et al., 2015; Hardt et al., 2016); and (2) Individual fairness (Dwork et al., 2012), which does not rely on sensitive attributes but rather on the similarity between individuals. In our experiments, we adopt two widely used definitions of group fairness: demographic parity and equal opportunity. Demographic parity (Feldman et al., 2015) requires that members of different protected classes are represented in the positive class at the same rate, meaning the distribution of protected attributes in the positive class should reflect the overall population