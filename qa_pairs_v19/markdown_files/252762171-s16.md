# The Lifecycle of "Facts": A Survey of Social Bias in Knowledge Graphs

## Question

What is the difference between demographic parity and predictive parity in assessing fairness in link prediction tasks?

## URLs

1. https://ar5iv.org/html/2409.03893
2. https://ar5iv.org/html/2201.11596
3. https://ar5iv.org/html/2402.02663
4. https://ar5iv.org/html/2205.05396
5. https://ar5iv.org/html/2403.17333
6. https://ar5iv.org/html/2411.02414
7. https://ar5iv.org/html/2411.04371
8. https://ar5iv.org/html/2410.09600
9. https://ar5iv.org/html/2409.08658

## Answer

I don't know. The context provided does not contain sufficient information regarding predictive parity. While it thoroughly explains demographic parity's definition, applications, and constraints in the context of link prediction [1][2][3][4][5], there is no mention or explanation of predictive parity. Without details on predictive parity, I cannot compare the two fairness concepts. Therefore, the context is insufficient to address the differences between demographic parity and predictive parity in assessing fairness in link prediction tasks.

[1]: https://ar5iv.org/html/2201.11596, [2201.11596] FairEGM: Fair Link Prediction and Recommendation via Emulated Graph Modification
[2]: https://ar5iv.org/html/2409.08658, [2409.08658] Promoting Fairness in Link Prediction with Graph Enhancement
[3]: https://ar5iv.org/html/2201.11596, [2201.11596] FairEGM: Fair Link Prediction and Recommendation via Emulated Graph Modification
[4]: https://ar5iv.org/html/2201.11596, [2201.11596] FairEGM: Fair Link Prediction and Recommendation via Emulated Graph Modification
[5]: https://ar5iv.org/html/2409.08658, [2409.08658] Promoting Fairness in Link Prediction with Graph Enhancement

[1]: Passage ID 1: focus on the fair link prediction task. We select demographic parity (Gajane and Pechenizkiy, 2017) as our fairness criteria. Informally, demographic parity is satisfied if the output of the model is not dependent on a given sensitive attribute (Mehrabi etÂ al., 2019). Formally, we define the demographic parity fairness criteria on the link prediction problem as follows.Definition 0 (Link Prediction with Demographic Parity Fairness).Given a graph ğ‘®=(ğ‘½,ğ‘¬)ğ‘®ğ‘½ğ‘¬\bm{G}=(\bm{V},\bm{E}), a node vğ‘£v, and an embedding model Mğ‘€M, let LMâ€‹(v)=(u1,â€¦,un)subscriptğ¿ğ‘€ğ‘£subscriptğ‘¢1â€¦subscriptğ‘¢ğ‘›L_{M}(v)=(u_{1},...,u_{n}) be the set of nodes that have the highest likelihood to form a link with node vğ‘£v where LMâ€‹(v)subscriptğ¿ğ‘€ğ‘£L_{M}(v) is computed using the model Mğ‘€M. The link prediction problem with demographic parity fairness for node vğ‘£v has the following constraint: Dâ€‹(PLMâ€‹(v),PS)=0ğ·subscriptğ‘ƒsubscriptğ¿ğ‘€ğ‘£subscriptğ‘ƒğ‘†0D(P_{L_{M}(v)},P_{S})=0 where Dğ·D is the distance metric between distributions,
[2]: Passage ID 2: classification as the condition where the predicted label is independent of the sensitive attribute. In the domain of link prediction, which involves estimating the probability of a link between pairs of nodes in a graph, fairness can be extended by ensuring that the estimated probability is independent of the sensitive attributes of the two nodes involved. In this subsection, we introduce two fairness concepts relevant to link prediction: demographic parity and equal opportunity.3.2.1 Demographic ParityDemographic Parity (DP) requires that predictions are independent of the sensitive attribute. It has been extensively applied in previous fair machine learning studies, and by replacing the classification probability with link prediction probability, it can be simply extended in the context of link prediction. It is also named dyadic fairness in previous literatureÂ (Li etÂ al., 2020). In the context of link prediction, the DP requires the prediction probabilities should be
[3]: Passage ID 3: parity fairness for node vğ‘£v has the following constraint: Dâ€‹(PLMâ€‹(v),PS)=0ğ·subscriptğ‘ƒsubscriptğ¿ğ‘€ğ‘£subscriptğ‘ƒğ‘†0D(P_{L_{M}(v)},P_{S})=0 where Dğ·D is the distance metric between distributions, PLMâ€‹(v)subscriptğ‘ƒsubscriptğ¿ğ‘€ğ‘£P_{L_{M}(v)} is the distribution of sensitive attributes over the recommended nodes set LMâ€‹(v)subscriptğ¿ğ‘€ğ‘£L_{M}(v), and PSsubscriptğ‘ƒğ‘†P_{S} is the distribution of sensitive attributes on the overall graph.The link prediction problem with demographic parity fairness states that the distribution of sensitive attributes over recommended nodes (LMâ€‹(v)subscriptğ¿ğ‘€ğ‘£L_{M}(v)) should not be distinguishable from the distribution of sensitive attributes on the overall graph (PSsubscriptğ‘ƒğ‘†P_{S}). Note that the model can perform link recommendation by performing either K-nearest neighbors or by using a classifier.Problem Statement: We want to learn a graph neural network (GNN) model Mğ‘€M such that Mğ‘€M performs well on the link prediction task while satisfying the demographic
[4]: Passage ID 4: proliferation of graph-based recommendation models (Ying etÂ al., 2018), we incorporate fairness constraints through demographic parity (Gajane and Pechenizkiy, 2017). Informally, demographic parity seeks to ensure that each group with a particular sensitive attribute receives the positive outcome at the same rate as other groups with different values for the same sensitive attribute (Gajane and Pechenizkiy, 2017). For example, a professional networking website can utilize a GNN-based recommendation engine to recommend a job opening to a certain number of individuals. Demographic parity based recommendations could ensure that no individuals of a particular race, ethnicity, or gender are less likely to receive such a recommendation. The same can be said for social networking sites; if a recommendation algorithm is not fair under the definition of demographic parity, then the algorithm will suggest individuals with similar demographics to be linked, reinforcing preexisting ties and
[5]: Passage ID 5: all datasets and achieves the best utility on Pubmed, DBLP, and Google+.6 Related Work6.1 Fairness in Machine LearningIn recent years, numerous fairness definitions in machine learning have been proposed. These definitions generally fall into two categories: (1) Group fairness, which aims to ensure that certain statistical measures are approximately equal across protected groups (e.g., racial or gender groups)Â (Feldman etÂ al., 2015; Hardt etÂ al., 2016); and (2) Individual fairnessÂ (Dwork etÂ al., 2012), which does not rely on sensitive attributes but rather on the similarity between individuals. In our experiments, we adopt two widely used definitions of group fairness: demographic parity and equal opportunity. Demographic parityÂ (Feldman etÂ al., 2015) requires that members of different protected classes are represented in the positive class at the same rate, meaning the distribution of protected attributes in the positive class should reflect the overall population