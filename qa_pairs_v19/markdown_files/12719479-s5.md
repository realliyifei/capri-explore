# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

Why is observed agreement not sufficient for comparing inter-coder agreement across different studies?

## URLs

1. https://ar5iv.org/html/1806.05821. [1806.05821] Measuring intergroup agreement and disagreement
2. https://ar5iv.org/html/2107.11449. [2107.11449] Applying Inter-rater Reliability and Agreement in Grounded Theory Studies in Software Engineering
3. https://ar5iv.org/html/2303.12502. [2303.12502] Measuring Agreement Among Several Raters Classifying Subjects Into One-Or-More (Hierarchical) Nominal Categories. A Generalisation of Fleiss‚Äô kappa
4. https://ar5iv.org/html/1907.10450. [1907.10450] Investigating Correlations of Inter-coder Agreement and Machine Annotation Performance for Historical Video Data
5. https://ar5iv.org/html/2411.15634. [2411.15634] "All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations
6. https://ar5iv.org/html/2308.15243. [2308.15243] Reliability Gaps Between Groups in COMPAS Dataset
7. https://ar5iv.org/html/2008.00977. [2008.00977] Reliability in Software Engineering Qualitative Research through Inter-Coder Agreement A guide using Krippendorff‚Äôs ùõº & Atlas.ti
8. https://ar5iv.org/html/2006.12904. [2006.12904] Min-Mid-Max Scaling, Limits of Agreement, and Agreement Score
9. https://ar5iv.org/html/2401.04122. [2401.04122] From Prompt Engineering to Prompt Science With Human in the Loop

## Answer (I don't know?: ‚úó)

The observed agreement, while important, is not sufficient for comparing inter-coder agreement across different studies due to several factors that affect how agreement is interpreted and measured. 

Firstly, the context in which annotations are made can significantly influence inter-coder agreement. For instance, different studies may utilize different categories or types of annotations, even if they share a superficially similar aim. In the passage, it is mentioned that "judgments of different groups may have different biases" [3]. These inherent biases in coding can lead to inconsistencies in what constitutes agreement, depending on the specific populations or categories involved.

Secondly, the metrics used to assess inter-coder agreement can vary significantly between studies. Various statistical measures exist for calculating agreement, including Krippendorff‚Äôs Œ±, which is noted for its capacity to assess reliability across categories in visual recognition tasks [2]. However, if different studies employ different measures or thresholds for determining what constitutes "adequate" agreement, this discrepancy can make comparison difficult. One study might report high inter-coder reliability based on one metric, while another study might report lower agreement using a different metric or threshold for the same scenario.

Moreover, the quality and expertise of the annotators involved can also introduce variability. As indicated, expert annotators typically yield higher quality annotations than non-experts [1][2] and might not be directly comparable to agreements achieved with non-expert coders. The difference in training, experience, and the specifics of what each coder associates with certain categories might affect how agreement is perceived and scored.

Additionally, the training sizes and the specific datasets used can also influence inter-coder agreement outcomes. Studies like the one mentioned discuss how training data size plays a role in performance prediction, suggesting that larger datasets can mitigate variability in annotations, thus impacting the overall reported agreement [5].

Lastly, research methodologies, including the specific tasks or constructs examined, can cause differences in observed agreement figures, making it challenging to draw direct comparisons. Consequently, observed agreement figures should be contextualized within their specific study designs, methodologies, and operational definitions to allow for meaningful comparisons across studies rather than assuming that agreement statistics can be straightforwardly compared.

In summary, the presence of biases, varying metrics for agreement calculations, the expertise of annotators, differences in training data size, and methodological distinctions all contribute to the inadequacy of observed agreement as a standalone metric for comparing inter-coder agreement across different studies. This complexity necessitates a more nuanced and contextualized approach when interpreting inter-coder agreement results.

1. [1]:  https://ar5iv.org/html/1907.10450, [1907.10450] Investigating Correlations of Inter-coder Agreement and Machine Annotation Performance for Historical Video Data
2. [2]:  https://ar5iv.org/html/1907.10450, [1907.10450] Investigating Correlations of Inter-coder Agreement and Machine Annotation Performance for Historical Video Data
3. [3]:  https://ar5iv.org/html/1907.10450, [1907.10450] Investigating Correlations of Inter-coder Agreement and Machine Annotation Performance for Historical Video Data
4. [4]:  https://ar5iv.org/html/2407.21037, No Title
5. [5]:  https://ar5iv.org/html/1907.10450, [1907.10450] Investigating Correlations of Inter-coder Agreement and Machine Annotation Performance for Historical Video Data
---
1. [1]:  Passage ID 1: results of a comparison between expert vs. non-expert inter-coder agreements. Furthermore, a performance prediction for the task of concept classification is presented based on inter-coder agreement and training data size as input. In Section 5, the impact of expert vs. non-expert image annotations on person identification performance is investigated. Section 6 concludes the paper and outlines areas for future work.2 Related WorkIn this section, we briefly survey related work for inter-annotator studies conducted for natural language as well as image annotation tasks. Snow et al.¬†[8] have evaluated non-expert annotations by means of Mechanical Turk workers for natural language tasks. Among other experiments, they found that annotations of four non-experts are needed to rival the annotation quality of one expert annotator in selected tasks. Furthermore, they have trained machine learning classifiers on expert as well as non-expert annotations and reported better system
2. [2]:  Passage ID 2: higher agreement for experts, but argue that majority voting filters out noise in non-expert annotations closing the gap to expert annotations of higher quality. A more recent study¬†[2] deals with the question whether machines perform better than humans in visual recognition tasks. For assessing human performance the inter-coder reliability by Krippendorff‚Äôs Œ±ùõº\alpha on 20 common categories of the PASCAL VOC benchmark is measured. For the best submission at PASCAL VOC‚Äôs leaderboard an above average human-level performance for visual concept annotation is reported being on a par or better than 19 of 23 participants.3 Annotation Study: Expert vs. Non-Expert Agreement on GDR-specific Concepts and PersonsIn this section, we aim at comparing the reliability of expert vs. non-expert annotations for for historical TV data in terms of inter-coder-agreement. For this purpose, we collected annotations of expert as well as student participants (Section¬†3.2) on a selected set of concept and
3. [3]:  Passage ID 3: experts and students.Earlier in this section we implied that a high agreement is an indicator for an accurate set of annotations. For the categories of concepts, experts and non-experts were found to be equally accurate annotators.In the next section, we analyze to which extent agreement contributes to video indexing performance and investigate the opportunity of performance prediction.We determined that experts are more accurate labelers when it comes to historically relevant persons of the GDR. However, judgments of different groups may have different biases as also reasoned for the concept categories. Therefore, in Section¬†5, we exemplary determine for the set of different personality annotations collected in our study whether high agreement on training images implies higher system performance for person recognition.4 Predicting Concept Classification Performance In this section, we discuss correlations between original average precision results and inter-coder
4. [4]:  Passage ID 4: on negotiation outcomes (Putnam & Jones, 1982[14]). A necessary step in these kinds of studies is to develop a coding scheme, train coders, ensure inter-coder agreement, and have those coders code hundreds or thousands of speech units. This process of human coding can take months, and cost thousands of dollars.Researchers can save time and money by automating this coding process. Additionally, automated coding reduces the risks that are inherent in human coding ‚Äì such as coder fatigue due to coding too many transcripts in a sitting or coder drift in applying codes across a number of transcripts. Indeed, having an automatic coding process may make some projects feasible that previously were not feasible. This study explores whether, and how, LLMs can be used to replace (or supplement) human coders, which should increase both the efficiency and reliability of coding for negotiation research. What we learned in the process of negotiation coding may also be applicable to other
5. [5]:  Passage ID 5: to the performance of video indexing methods based on supervised learning.Moreover, we suppose that inter-coder agreement might form an upper bound for such methods. In this regard, some first experiments are presented to predict average precision using support vector regression based on inter-coder agreement and training data size.Furthermore, the influence of image annotation quality induced by experts vs. non-experts on the person recognition performance is investigated based on annotations acquired during the study.The remainder of the paper is organized as follows. Section 2 discusses related work regarding inter-annotator studies. Section 3 deals with the comprehensive user study including a description of the used dataset, the study participants, the experimental design and the results of a comparison between expert vs. non-expert inter-coder agreements. Furthermore, a performance prediction for the task of concept classification is presented based on inter-coder agreement