# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

Why is observed agreement not sufficient for comparing inter-coder agreement across different studies?

## URLs

1. https://ar5iv.org/html/1806.05821. [1806.05821] Measuring intergroup agreement and disagreement
2. https://ar5iv.org/html/2107.11449. [2107.11449] Applying Inter-rater Reliability and Agreement in Grounded Theory Studies in Software Engineering
3. https://ar5iv.org/html/2303.12502. [2303.12502] Measuring Agreement Among Several Raters Classifying Subjects Into One-Or-More (Hierarchical) Nominal Categories. A Generalisation of Fleiss‚Äô kappa
4. https://ar5iv.org/html/1907.10450. [1907.10450] Investigating Correlations of Inter-coder Agreement and Machine Annotation Performance for Historical Video Data
5. https://ar5iv.org/html/2411.15634. [2411.15634] "All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations
6. https://ar5iv.org/html/2308.15243. [2308.15243] Reliability Gaps Between Groups in COMPAS Dataset
7. https://ar5iv.org/html/2008.00977. [2008.00977] Reliability in Software Engineering Qualitative Research through Inter-Coder Agreement A guide using Krippendorff‚Äôs ùõº & Atlas.ti
8. https://ar5iv.org/html/2006.12904. [2006.12904] Min-Mid-Max Scaling, Limits of Agreement, and Agreement Score
9. https://ar5iv.org/html/2401.04122. [2401.04122] From Prompt Engineering to Prompt Science With Human in the Loop

## Answer (I don't know?: ‚úó)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable