# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

Why is observed agreement not sufficient for comparing inter-coder agreement across different studies?

## URLs

1. https://ar5iv.org/html/1806.05821. [1806.05821] Measuring intergroup agreement and disagreement
2. https://ar5iv.org/html/2107.11449. [2107.11449] Applying Inter-rater Reliability and Agreement in Grounded Theory Studies in Software Engineering
3. https://ar5iv.org/html/2303.12502. [2303.12502] Measuring Agreement Among Several Raters Classifying Subjects Into One-Or-More (Hierarchical) Nominal Categories. A Generalisation of Fleiss‚Äô kappa
4. https://ar5iv.org/html/1907.10450. [1907.10450] Investigating Correlations of Inter-coder Agreement and Machine Annotation Performance for Historical Video Data
5. https://ar5iv.org/html/2411.15634. [2411.15634] "All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations
6. https://ar5iv.org/html/2308.15243. [2308.15243] Reliability Gaps Between Groups in COMPAS Dataset
7. https://ar5iv.org/html/2008.00977. [2008.00977] Reliability in Software Engineering Qualitative Research through Inter-Coder Agreement A guide using Krippendorff‚Äôs ùõº & Atlas.ti
8. https://ar5iv.org/html/2006.12904. [2006.12904] Min-Mid-Max Scaling, Limits of Agreement, and Agreement Score
9. https://ar5iv.org/html/2401.04122. [2401.04122] From Prompt Engineering to Prompt Science With Human in the Loop

## Answer (I don't know?: ‚úì)

I don't know. The context provided does not include specific information about inter-coder agreement, observed agreement, or the reasons why it may not be sufficient for comparison across different studies. The passages focus on other aspects of natural language processing research, such as code-switching evaluation, research software quality, and question-answering systems, without addressing the nuances of inter-coder reliability or agreement. Thus, the information needed to answer your question completely and precisely is lacking in the provided context.

1. [1]:  https://ar5iv.org/html/2005.04322, No Title
2. [2]:  https://ar5iv.org/html/2303.16166, No Title
3. [3]:  https://ar5iv.org/html/2411.05503, No Title
4. [4]:  https://ar5iv.org/html/2303.16166, No Title
5. [5]:  https://ar5iv.org/html/2209.12617, No Title
---
1. [1]:  Passage ID 1: interest from the NLP community. Many researchers have proposed novel methods to handle code-switched data, showing improvements on core NLP tasks such as language identification (LID), named entity recognition (NER), and part-of-speech (POS) tagging. However, many of these approaches are usually evaluated on a few language pairs and a specific domain, and it is not clear whether these models are exclusive to such scenarios or they can generalize to other tasks, domains, and language pairs.Moreover, research in code-switching currently has a slow process of comparison in which researchers have to replicate previous methods to report scores on different datasets.Furthermore, choosing the best-published model for benchmarking purposes is not an easy task either.These problems exist mainly because 1) there is no official benchmark for general code-switching evaluation that allows direct comparisons across multiple tasks, and 2) methods are usually not comprehensively evaluated across
2. [2]:  Passage ID 2: improving research software quality within the NLP community.‚Ä†‚Ä†\twemojilight bulbDenotes equal contributions.1 IntroductionIn the field of natural language processing (NLP), as well as in broader contexts, the validity and soundness of research findings are typically upheld by ‚Äúestablishing consistency of results versus existing implementations, standard benchmarks, or sanity checks via statistically significant experimental results‚Äù (Rozier and Rozier, 2014). Nevertheless, recent evidence indicates that several of these aspects are absent in many papers (Raff, 2019), questioning the scientific credibility of NLP research.On one hand, many scientists have reported difficulties in replicating the work of others, or even their own, (Prinz et¬†al., 2011; Gundersen and Kjensmo, 2018; Wieling et¬†al., 2018a; Chen et¬†al., 2019; Gundersen, 2019), also in the specific context of NLP (Wieling et¬†al., 2018b; Belz et¬†al., 2021a; Marie et¬†al., 2021; Narang et¬†al., 2021; Gehrmann et¬†al.,
3. [3]:  Passage ID 3: success depends on the collective work of interdisciplinary teams.Textbooks are Frequently Cited. Although we did not conduct a detailed citation analysis, textbooks and educational materials are frequently cited, suggesting their value to the field. The development of relevant textbooks, courses, and workshops is in high demand and could facilitate the adoption of modern NLP methods, accelerating field progress.Since we have published the relevant data online111111Bibliography in JSON format: https://kyrgyznlp.github.io/static/bibliography_joined.json., we encourage those interested in formal collaboration analysis to apply Social Network Analysis (SNA) methods¬†[102, 80] to the collaboration graph, labeled bimodal author-paper graph, citation graph (not yet collected), etc. Although the graph sizes are small in this case, comparing indices such as betweenness centrality with other indicators of researchers‚Äô output impact could provide a more conclusive basis to support or
4. [4]:  Passage ID 4: the foundation for further research, ultimately resulting in unreliable and potentially misleading findings (McCullough et¬†al., 2008).In light of the above considerations, this study is a call to action, underpinned by empirical evidence, to enhance the dependability of published NLP findings. In particular, our contributions are:1.We examine the extent to which research works consider the attributes studied in the field of software quality assurance (Buckley and Poston, 1984; Tripathy and Naik, 2011), or SQA (¬ß2), and show that code correctness has been neglected by the NLP community thus far (¬ß3);2.Through a case study on open-source implementations of the widespread Conformer architecture (Gulati et¬†al., 2020), we prove that:-At least one impactful bug is present in all the analyzed implementations (¬ß4.2);-Bugs do notprevent from achievinggood andreproducible results that outperformother architecturesin speech recognition and translation across
5. [5]:  Passage ID 5: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence