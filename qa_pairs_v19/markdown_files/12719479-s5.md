# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

Why is observed agreement not sufficient for comparing inter-coder agreement across different studies?

## URLs

1. https://ar5iv.org/html/1806.05821. [1806.05821] Measuring intergroup agreement and disagreement
2. https://ar5iv.org/html/2107.11449. [2107.11449] Applying Inter-rater Reliability and Agreement in Grounded Theory Studies in Software Engineering
3. https://ar5iv.org/html/2303.12502. [2303.12502] Measuring Agreement Among Several Raters Classifying Subjects Into One-Or-More (Hierarchical) Nominal Categories. A Generalisation of Fleiss‚Äô kappa
4. https://ar5iv.org/html/1907.10450. [1907.10450] Investigating Correlations of Inter-coder Agreement and Machine Annotation Performance for Historical Video Data
5. https://ar5iv.org/html/2411.15634. [2411.15634] "All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations
6. https://ar5iv.org/html/2308.15243. [2308.15243] Reliability Gaps Between Groups in COMPAS Dataset
7. https://ar5iv.org/html/2008.00977. [2008.00977] Reliability in Software Engineering Qualitative Research through Inter-Coder Agreement A guide using Krippendorff‚Äôs ùõº & Atlas.ti
8. https://ar5iv.org/html/2006.12904. [2006.12904] Min-Mid-Max Scaling, Limits of Agreement, and Agreement Score
9. https://ar5iv.org/html/2401.04122. [2401.04122] From Prompt Engineering to Prompt Science With Human in the Loop

## Answer (I don't know?: ‚úó)

Observed inter-coder agreement is often not sufficient for comparing across different studies due to significant variations in factors such as the nature of the tasks, the expertise of the annotators, and the datasets utilized. Firstly, different studies may engage annotators with varying levels of expertise, which affects the quality and reliability of the annotations. For instance, the context indicates that in selected tasks, four non-expert annotators are necessary to match the annotation quality of one expert [1]. The performance and reliability of expertise thus play a central role in comparison; studies comparing expert vs. non-expert annotations show that while experts are generally more accurate, non-expert annotations can still yield useful information when assessed appropriately [2] [3].

Additionally, the types of concepts or categories being annotated can differ significantly between studies. The specific nature of these concepts may introduce biases unique to each group of annotators, which impacts the overall inter-coder agreement observed. For instance, the paper notes that experts performed better specifically on historically relevant persons, yet for broader concept categories, both experts and non-experts were found to have comparable accuracy [3]. As shifts in focus from one category to another can lead to differences in understanding and interpreting the guidelines or prompts, such discrepancies are pivotal in affecting agreement levels.

The methodology for calculating inter-coder agreement might also vary across studies. For example, some studies may utilize simple percentage agreement, while others employ more sophisticated measures like Krippendorff's Œ±, which accounts for chance agreement in a more nuanced manner [2]. The choice of measurement can significantly influence the reported agreement levels, making direct comparisons challenging.

Moreover, factors like the diversity of annotations and the inherent biases of different annotator groups further complicate the comparison of inter-coder agreement. As noted, while non-expert annotations may have more diversity‚Äîwhich can reduce bias‚Äîthey can also create noise in the data. It has been observed that majority voting approaches can help filter this noise, potentially leading to inter-coder agreements that may not reflect true annotator quality across studies [5]. 

Consequently, without a unified framework that takes into account the type of task, the expertise of annotators, the nature and categorization of content, the grading methodology, and other contextual factors, observed agreements cannot be adequately compared or interpreted, limiting their overall utility in drawing generalized conclusions across different studies. Therefore, careful consideration of these factors is necessary for meaningful comparisons of inter-coder agreement in studies of annotation reliability.

1. [1]:  https://ar5iv.org/html/1907.10450, [1907.10450] Investigating Correlations of Inter-coder Agreement and Machine Annotation Performance for Historical Video Data
2. [2]:  https://ar5iv.org/html/1907.10450, [1907.10450] Investigating Correlations of Inter-coder Agreement and Machine Annotation Performance for Historical Video Data
3. [3]:  https://ar5iv.org/html/1907.10450, [1907.10450] Investigating Correlations of Inter-coder Agreement and Machine Annotation Performance for Historical Video Data
4. [4]:  https://ar5iv.org/html/1907.10450, [1907.10450] Investigating Correlations of Inter-coder Agreement and Machine Annotation Performance for Historical Video Data
5. [5]:  https://ar5iv.org/html/1907.10450, [1907.10450] Investigating Correlations of Inter-coder Agreement and Machine Annotation Performance for Historical Video Data
---
1. [1]:  Passage ID 1: results of a comparison between expert vs. non-expert inter-coder agreements. Furthermore, a performance prediction for the task of concept classification is presented based on inter-coder agreement and training data size as input. In Section 5, the impact of expert vs. non-expert image annotations on person identification performance is investigated. Section 6 concludes the paper and outlines areas for future work.2 Related WorkIn this section, we briefly survey related work for inter-annotator studies conducted for natural language as well as image annotation tasks. Snow et al.¬†[8] have evaluated non-expert annotations by means of Mechanical Turk workers for natural language tasks. Among other experiments, they found that annotations of four non-experts are needed to rival the annotation quality of one expert annotator in selected tasks. Furthermore, they have trained machine learning classifiers on expert as well as non-expert annotations and reported better system
2. [2]:  Passage ID 2: higher agreement for experts, but argue that majority voting filters out noise in non-expert annotations closing the gap to expert annotations of higher quality. A more recent study¬†[2] deals with the question whether machines perform better than humans in visual recognition tasks. For assessing human performance the inter-coder reliability by Krippendorff‚Äôs Œ±ùõº\alpha on 20 common categories of the PASCAL VOC benchmark is measured. For the best submission at PASCAL VOC‚Äôs leaderboard an above average human-level performance for visual concept annotation is reported being on a par or better than 19 of 23 participants.3 Annotation Study: Expert vs. Non-Expert Agreement on GDR-specific Concepts and PersonsIn this section, we aim at comparing the reliability of expert vs. non-expert annotations for for historical TV data in terms of inter-coder-agreement. For this purpose, we collected annotations of expert as well as student participants (Section¬†3.2) on a selected set of concept and
3. [3]:  Passage ID 3: experts and students.Earlier in this section we implied that a high agreement is an indicator for an accurate set of annotations. For the categories of concepts, experts and non-experts were found to be equally accurate annotators.In the next section, we analyze to which extent agreement contributes to video indexing performance and investigate the opportunity of performance prediction.We determined that experts are more accurate labelers when it comes to historically relevant persons of the GDR. However, judgments of different groups may have different biases as also reasoned for the concept categories. Therefore, in Section¬†5, we exemplary determine for the set of different personality annotations collected in our study whether high agreement on training images implies higher system performance for person recognition.4 Predicting Concept Classification Performance In this section, we discuss correlations between original average precision results and inter-coder
4. [4]:  Passage ID 4: to the performance of video indexing methods based on supervised learning.Moreover, we suppose that inter-coder agreement might form an upper bound for such methods. In this regard, some first experiments are presented to predict average precision using support vector regression based on inter-coder agreement and training data size.Furthermore, the influence of image annotation quality induced by experts vs. non-experts on the person recognition performance is investigated based on annotations acquired during the study.The remainder of the paper is organized as follows. Section 2 discusses related work regarding inter-annotator studies. Section 3 deals with the comprehensive user study including a description of the used dataset, the study participants, the experimental design and the results of a comparison between expert vs. non-expert inter-coder agreements. Furthermore, a performance prediction for the task of concept classification is presented based on inter-coder agreement
5. [5]:  Passage ID 5: the annotation quality of one expert annotator in selected tasks. Furthermore, they have trained machine learning classifiers on expert as well as non-expert annotations and reported better system performance for non-experts due to high annotation diversity reducing the annotator bias. However, these observations were exclusively made for natural language tasks.Nowak and Rueger¬†[7] presented a study on inter-coder agreement for image annotation from both crowdsourcing and experts. Human annotators had to label 99 images of the ImageCLEF test data (http://www.imageclef.org/) with respect to 22 concept categories. Some of the categories were mutually exclusive (season, time of day, indoor/outdoor/none). The images were assessed by experts as well as by Mechanical Turk workers. They measure higher agreement for experts, but argue that majority voting filters out noise in non-expert annotations closing the gap to expert annotations of higher quality. A more recent study¬†[2] deals with the