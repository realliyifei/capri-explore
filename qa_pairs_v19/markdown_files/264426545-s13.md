# Automatic Pronunciation Assessment -A Review

## Question

How have recent unsupervised methods improved the detection of mispronunciation errors in speech assessment?

## URLs

1. https://ar5iv.org/html/2201.10198
2. https://ar5iv.org/html/2103.03023
3. https://ar5iv.org/html/2207.00774
4. https://ar5iv.org/html/2203.15937
5. https://ar5iv.org/html/2307.16324
6. https://ar5iv.org/html/2101.06396
7. https://ar5iv.org/html/2106.03494
8. https://ar5iv.org/html/2310.13974
9. https://ar5iv.org/html/2205.02670
10. https://ar5iv.org/html/2403.08187

## Answer

Recent unsupervised methods have contributed to the detection of mispronunciation errors in speech assessment, albeit with some limitations in effectiveness. Traditional approaches to mispronunciation detection typically require expert knowledge, manual labeling, or reliable automatic speech recognition (ASR) results, which can be costly [1][2][3][4]. Recent efforts have focused on unsupervised acoustic pattern discovery, which aims to alleviate some of these costs, although the outcomes have often been sub-optimal [1][2][3][4].

One significant advancement in this area has been the work of Lee and Glass (2012), who explored a comparison-based approach to analyze the misalignment between a student’s speech and a teacher’s speech. This foundational study set the stage for further investigations that delved into identifying mispronunciation errors by evaluating acoustic similarities across individual learners’ utterances in later studies by Lee and Glass (2015) and Lee et al. (2016). They proposed a novel n-best filtering method to manage the ambiguity in error candidate hypotheses that emerged from acoustic similarity clustering [1][2][3][4].

Furthermore, Mao et al. (2018) introduced a method utilizing k-means clustering on phoneme-based phonemic posterior-grams (PPGs), which served to expand the phoneme set utilized in the analysis of non-native (L2) speech. This technique aimed to enhance the richness of the acoustic features available for detecting mispronunciation errors [1][2][3][4].

Despite these advances, the effectiveness of current systems is still hampered by data limitations. One key issue is the scarcity of annotated mispronounced speech necessary for the reliable training of pronunciation error detection models. According to more recent findings, existing systems achieve only around 60% precision with varying recall levels of 40%-80% in detecting pronunciation errors [5]. This indicates a significant room for improvement.

Innovative models that leverage generative techniques, including phoneme-to-phoneme (P2P), text-to-speech (T2S), and speech-to-speech (S2S) conversions, have emerged as promising solutions to augment the training data needed for these models. By generating both correctly pronounced and mispronounced synthetic speech, these techniques can enhance the training process and improve the accuracy of machine learning models applied to mispronunciation detection, leading to state-of-the-art results in the field [5].

In conclusion, while recent unsupervised methods have made strides in understanding and detecting mispronunciation errors through acoustic analysis and clustering techniques, their effectiveness is still constrained by data availability. Ongoing innovations in synthetic speech generation may offer a pathway towards overcoming these challenges and further improving the reliability of mispronunciation detection systems.

[1]: https://ar5iv.org/html/2310.13974, [2310.13974] Automatic Pronunciation Assessment - A Review
[2]: https://ar5iv.org/html/2310.13974, [2310.13974] Automatic Pronunciation Assessment - A Review
[3]: https://ar5iv.org/html/2310.13974, [2310.13974] Automatic Pronunciation Assessment - A Review
[4]: https://ar5iv.org/html/2310.13974, [2310.13974] Automatic Pronunciation Assessment - A Review
[5]: https://ar5iv.org/html/2207.00774, [2207.00774] Computer-assisted Pronunciation Training - Speech synthesis is almost all you need

[1]: Passage ID 1: approaches for studying mispronunciation detection typically involve the need for expert knowledge, laborious manual labeling, or dependable ASR results, all of which come with significant costs. In contrast, recent years have witnessed considerable endeavors in unsupervised acoustic pattern discovery, yielding sub-optimal outcomes. Lee and Glass (2012) initially investigated a comparison-based approach that analyzes the extent of misalignment between a student’s speech and a teacher’s speech. In subsequent studies Lee and Glass (2015); Lee et al. (2016), explored the discovery of mispronunciation errors by analyzing the acoustic similarities across individual learners’ utterances, with a proposed n-best filtering method to resolve ambiguous error candidate hypotheses derived from acoustic similarity clustering. Furthermore, Mao et al. (2018) proposed k-means clustering on phoneme-based phonemic posterior-grams (PPGs) to expand the phoneme set in L2 speech. More recently, Sini et al.
[2]: Passage ID 2: approaches for studying mispronunciation detection typically involve the need for expert knowledge, laborious manual labeling, or dependable ASR results, all of which come with significant costs. In contrast, recent years have witnessed considerable endeavors in unsupervised acoustic pattern discovery, yielding sub-optimal outcomes. Lee and Glass (2012) initially investigated a comparison-based approach that analyzes the extent of misalignment between a student’s speech and a teacher’s speech. In subsequent studies Lee and Glass (2015); Lee et al. (2016), explored the discovery of mispronunciation errors by analyzing the acoustic similarities across individual learners’ utterances, with a proposed n-best filtering method to resolve ambiguous error candidate hypotheses derived from acoustic similarity clustering. Furthermore, Mao et al. (2018) proposed k-means clustering on phoneme-based phonemic posterior-grams (PPGs) to expand the phoneme set in L2 speech. More recently, Sini et al.
[3]: Passage ID 3: approaches for studying mispronunciation detection typically involve the need for expert knowledge, laborious manual labeling, or dependable ASR results, all of which come with significant costs. In contrast, recent years have witnessed considerable endeavors in unsupervised acoustic pattern discovery, yielding sub-optimal outcomes. Lee and Glass (2012) initially investigated a comparison-based approach that analyzes the extent of misalignment between a student’s speech and a teacher’s speech. In subsequent studies Lee and Glass (2015); Lee et al. (2016), explored the discovery of mispronunciation errors by analyzing the acoustic similarities across individual learners’ utterances, with a proposed n-best filtering method to resolve ambiguous error candidate hypotheses derived from acoustic similarity clustering. Furthermore, Mao et al. (2018) proposed k-means clustering on phoneme-based phonemic posterior-grams (PPGs) to expand the phoneme set in L2 speech. More recently, Sini et al.
[4]: Passage ID 4: approaches for studying mispronunciation detection typically involve the need for expert knowledge, laborious manual labeling, or dependable ASR results, all of which come with significant costs. In contrast, recent years have witnessed considerable endeavors in unsupervised acoustic pattern discovery, yielding sub-optimal outcomes. Lee and Glass (2012) initially investigated a comparison-based approach that analyzes the extent of misalignment between a student’s speech and a teacher’s speech. In subsequent studies Lee and Glass (2015); Lee et al. (2016), explored the discovery of mispronunciation errors by analyzing the acoustic similarities across individual learners’ utterances, with a proposed n-best filtering method to resolve ambiguous error candidate hypotheses derived from acoustic similarity clustering. Furthermore, Mao et al. (2018) proposed k-means clustering on phoneme-based phonemic posterior-grams (PPGs) to expand the phoneme set in L2 speech. More recently, Sini et al.
[5]: Passage ID 5: are not able to detect pronunciation errors with high accuracy (only 60% precision at 40%-80% recall). One of the key problems is the low availability of mispronounced speech that is needed for the reliable training of pronunciation error detection models. If we had a generative model that could mimic non-native speech and produce any amount of training data, then the task of detecting pronunciation errors would be much easier. We present three innovative techniques based on phoneme-to-phoneme (P2P), text-to-speech (T2S), and speech-to-speech (S2S) conversion to generate correctly pronounced and mispronounced synthetic speech. We show that these techniques not only improve the accuracy of three machine learning models for detecting pronunciation errors but also help establish a new state-of-the-art in the field. Earlier studies have used simple speech generation techniques such as P2P conversion, but only as an additional mechanism to improve the accuracy of pronunciation error