# Automatic Pronunciation Assessment -A Review

## Question

How have recent unsupervised methods improved the detection of mispronunciation errors in speech assessment?

## URLs

1. https://ar5iv.org/html/2201.10198. [2201.10198] Improved Mispronunciation detection system using a hybrid CTC-ATT based approach for L2 English speakers
2. https://ar5iv.org/html/2103.03023. [2103.03023] End-to-End Mispronunciation Detection and Diagnosis From Raw Waveforms
3. https://ar5iv.org/html/2207.00774. [2207.00774] Computer-assisted Pronunciation Training - Speech synthesis is almost all you need
4. https://ar5iv.org/html/2203.15937. [2203.15937] Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment
5. https://ar5iv.org/html/2307.16324. [2307.16324] Mispronunciation detection using self-supervised speech representations
6. https://ar5iv.org/html/2101.06396. [2101.06396] MISPRONUNCIATION DETECTION IN NON-NATIVE (L2) ENGLISH WITH UNCERTAINTY MODELING
7. https://ar5iv.org/html/2106.03494. [2106.03494] Weakly-supervised word-level pronunciation error detection in non-native English speech
8. https://ar5iv.org/html/2310.13974. [2310.13974] Automatic Pronunciation Assessment - A Review
9. https://ar5iv.org/html/2205.02670. [2205.02670] Unsupervised Mismatch Localization in Cross-Modal Sequential Data with Application to Mispronunciations Localization
10. https://ar5iv.org/html/2403.08187. [2403.08187] Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of Speech Sound Disorders in Korean children Preprint. Under review.

## Answer (I don't know?: ✗)

Recent unsupervised methods for detecting mispronunciation errors in speech assessment have primarily revolved around innovative research that leverages acoustic pattern discovery and machine learning techniques. Traditional approaches often entailed the requirement for expert knowledge, manual labeling of data, or reliable automatic speech recognition (ASR) outputs, all of which were resource-intensive and costly [1]. Recent developments, however, have seen efforts to create more automated and less labor-intensive methods.

One of the pathways explored was a comparison-based approach, as investigated by Lee and Glass (2012), which analyzed the misalignment between a learner's speech and a model or teacher's speech [1]. Building upon this, further studies by Lee and Glass (2015) and Lee et al. (2016) sought to identify mispronunciation errors by evaluating the acoustic similarities among learners' utterances. They introduced an n-best filtering method to tackle ambiguities in error candidate hypotheses elicited from acoustic similarity clustering, thus enhancing the quality of mispronunciation detection [1].

In 2018, Mao et al. proposed a method employing k-means clustering on phoneme-based phonemic posterior-grams (PPGs) which expanded the phoneme set in second language (L2) speech. This technique contributed to refining the data upon which mispronunciation detection models are trained [1]. Notably, the challenges regarding the low availability of mispronounced speech data have been recognized as a barrier to achieving high accuracy in these models, with current systems obtaining only about 60% precision [2]. The emergence of generative models, such as those that mimic non-native speech, presents a solution for generating larger datasets required for effective training. Techniques described in the literature including phoneme-to-phoneme (P2P), text-to-speech (T2S), and speech-to-speech (S2S) conversion have shown promise in generating both correctly and incorrectly pronounced synthetic speech, thus overcoming the data scarcity problem [2].

The research demonstrated that these generative techniques notably improved the accuracy of machine learning models for detecting pronunciation errors, establishing new benchmarks within the field [2][3]. Unlike prior studies that utilized simple speech generation methods as auxiliary tools, the latest work considers speech generation as a primary strategy for mispronunciation detection, assessing its effectiveness in detecting pronunciation and lexical stress errors using non-native English speech corpora from diverse language speakers [3]. The best-performing S2S technique significantly boosted accuracy in AUC metrics, showcasing a 41% improvement [3].

In summary, while traditional methods were constrained by high resource demands and data limitations, recent unsupervised methods utilizing advanced generative models and machine learning techniques have significantly moved the needle in the accuracy and efficiency of mispronunciation detection in speech assessment [1][2][3].

1. [1]:  https://ar5iv.org/html/2310.13974, [2310.13974] Automatic Pronunciation Assessment - A Review
2. [2]:  https://ar5iv.org/html/2207.00774, [2207.00774] Computer-assisted Pronunciation Training - Speech synthesis is almost all you need
3. [3]:  https://ar5iv.org/html/2207.00774, [2207.00774] Computer-assisted Pronunciation Training - Speech synthesis is almost all you need
4. [4]:  https://ar5iv.org/html/2207.00774, [2207.00774] Computer-assisted Pronunciation Training - Speech synthesis is almost all you need
5. [5]:  https://ar5iv.org/html/2310.13974, [2310.13974] Automatic Pronunciation Assessment - A Review
---
1. [1]:  Passage ID 1: approaches for studying mispronunciation detection typically involve the need for expert knowledge, laborious manual labeling, or dependable ASR results, all of which come with significant costs. In contrast, recent years have witnessed considerable endeavors in unsupervised acoustic pattern discovery, yielding sub-optimal outcomes. Lee and Glass (2012) initially investigated a comparison-based approach that analyzes the extent of misalignment between a student’s speech and a teacher’s speech. In subsequent studies Lee and Glass (2015); Lee et al. (2016), explored the discovery of mispronunciation errors by analyzing the acoustic similarities across individual learners’ utterances, with a proposed n-best filtering method to resolve ambiguous error candidate hypotheses derived from acoustic similarity clustering. Furthermore, Mao et al. (2018) proposed k-means clustering on phoneme-based phonemic posterior-grams (PPGs) to expand the phoneme set in L2 speech. More recently, Sini et al.
2. [2]:  Passage ID 2: are not able to detect pronunciation errors with high accuracy (only 60% precision at 40%-80% recall). One of the key problems is the low availability of mispronounced speech that is needed for the reliable training of pronunciation error detection models. If we had a generative model that could mimic non-native speech and produce any amount of training data, then the task of detecting pronunciation errors would be much easier. We present three innovative techniques based on phoneme-to-phoneme (P2P), text-to-speech (T2S), and speech-to-speech (S2S) conversion to generate correctly pronounced and mispronounced synthetic speech. We show that these techniques not only improve the accuracy of three machine learning models for detecting pronunciation errors but also help establish a new state-of-the-art in the field. Earlier studies have used simple speech generation techniques such as P2P conversion, but only as an additional mechanism to improve the accuracy of pronunciation error
3. [3]:  Passage ID 3: state-of-the-art in the field. Earlier studies have used simple speech generation techniques such as P2P conversion, but only as an additional mechanism to improve the accuracy of pronunciation error detection. We, on the other hand, consider speech generation to be the first-class method of detecting pronunciation errors. The effectiveness of these techniques is assessed in the tasks of detecting pronunciation and lexical stress errors. Non-native English speech corpora of German, Italian, and Polish speakers are used in the evaluations. The best proposed S2S technique improves the accuracy of detecting pronunciation errors in AUC metric by 41% from 0.528 to 0.749 compared to the state-of-the-art approach.keywords: computer-assisted pronunciation training; automated pronunciation error detection; automated lexical stress error detection; speech synthesis; voice conversion; deep learningGo to the article published in Speech Communication Journal1 IntroductionLanguage
4. [4]:  Passage ID 4: feedback component [12]. The automated pronunciation evaluation component is responsible for detecting pronunciation errors in spoken speech, for example, for detecting words pronounced incorrectly by the speaker. The feedback component informs the speaker about mispronounced words and advises how to pronounce them correctly. This article is devoted to the topic of automated detection of pronunciation errors in non-native speech. This area of CAPT can take advantage of technological advances in machine learning and bring us closer to creating a fully automated assistant based on artificial intelligence for language learning.The research community has long studied the automated detection of pronunciation errors in non-native speech. Existing work has focused on various tasks such as detecting mispronounced phonemes [9] and lexical stress errors [13]. Researcher have given most attention to studying various machine learning models such as Bayesian networks [14, 15] and deep learning
5. [5]:  Passage ID 5: guiding the learner to fix mistakes in their pronunciation.This paper addresses the former – focusing onpronunciation assessment, which aims to automatically score non-native speech-segment and give meaningful feedback.To build such a robust pronunciation assessment system,the following design aspects should be addressed.ModellingMispronunciation detection and diagnosis (MDD), in many cases, are more challenging to model compared to the vanilla automatic speech recognition (ASR) system, which converts speech into text regardless of pronunciation mistakes. Robust ASR should perform well with all variation including dialects and non-native speakers. However, MDD should mark phonetic variations from the learner, which may sometimes be subtle differences Li et al. (2016a).Training ResourcesRecent success in deep learning methods emphasized the need for in-domain training data. Language learners can be divided into two groups: adult secondary (L2) language learners and