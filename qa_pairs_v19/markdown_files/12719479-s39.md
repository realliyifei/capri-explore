# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

What are the proposed solutions to improve intercoder agreement in word sense tagging, and how effective are they?

## URLs

1. https://ar5iv.org/html/2305.17984. [2305.17984] minOffense: Inter-Agreement Hate Terms for Stable Rules, Concepts, Transitivities, and Lattices
2. https://ar5iv.org/html/2312.00100. [2312.00100] Introducing Rhetorical Parallelism Detection: A New Task with Datasets, Metrics, and Baselines
3. https://ar5iv.org/html/1903.12271. [1903.12271] In Search of Meaning: Lessons, Resources and Next Steps for Computational Analysis of Financial Discourse
4. https://ar5iv.org/html/2003.10224. [2003.10224] Unsupervised Word Polysemy Quantification with Multiresolution Grids of Contextual Embeddings
5. https://ar5iv.org/html/2403.18024. [2403.18024] Enriching Word Usage Graphs with Cluster Definitions
6. https://ar5iv.org/html/2407.21037. [2407.21037] An Application of Large Language Models to Coding Negotiation Transcripts
7. https://ar5iv.org/html/1502.05472. [1502.05472] On the Effects of Low-Quality Training Data on Information Extraction from Clinical Reports
8. https://ar5iv.org/html/2209.04502. [2209.04502] A Close Look at a Systematic Method for Analyzing Sets of Security Advice

## Answer (I don't know?: ✓)

I don't know. The context provided does not mention anything about intercoder agreement in word sense tagging or propose any solutions related to this topic. The passages focus on various aspects of Natural Language Processing (NLP), including classification accuracy of certain approaches, synthetic dataset creation, and different NLP tasks, yet none are related to the specific question concerning intercoder agreement in tagging. Therefore, the information is insufficient to form a comprehensive answer.

1. [1]:  https://ar5iv.org/html/1907.12316, No Title
2. [2]:  https://ar5iv.org/html/2410.00427, No Title
3. [3]:  https://ar5iv.org/html/2410.00427, No Title
4. [4]:  https://ar5iv.org/html/1807.02383, No Title
5. [5]:  https://ar5iv.org/html/1807.10854, No Title
---
1. [1]:  Passage ID 1: improved the results by combining the HMMs with NNs using unigrams and Part of Speech (POS) tags as features, achieving 76.1% accuracy. The task was also approached using Latent Semantic Analysis (LSA) in three different studies (Serafin et al., 2003; Serafin & Di Eugenio, 2004; Di Eugenio et al., 2010). The first used both plain LSA and multiple adaptations based on clustering and the incorporation of features concerning the preceding dialog acts. However, there was no improvement over plain LSA, which achieved 65.36% accuracy on the tag set with 37 classes and 68.91% on the compressed set of 10 classes. On the other hand, the remaining studies experimented with multiple syntactic and dialog related features and were able to improve the results of plain LSA, up to 77.74% and 81.27%, respectively. In the last study, these results were further improved to 80.34% and 82.88% by applying an instance-based learning approach, namely k-Nearest Neighbors (k-NN), to the reduced semantic spaces
2. [2]:  Passage ID 2: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
3. [3]:  Passage ID 3: ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the laws of thermodynamics?” Following a quality inspection of the synthetically produced questions, we assembled a training dataset of 1601 examples, consisting of 120 questions for each of the 12 topics and 161 general questions. We also derived a test dataset containing 364 examples with a balanced class distribution similar to the training dataset.In our experiments, we evaluated three classification approaches: vector similarity search, prompting a LLM (GPT-3.5-Turbo), and few-shot fine-tuning of a transformer model with the SetFit framework Tunstall et al. (2022). Concerning the vector search approach with the SPECTER2 model, we measured the cosine similarity to compare vectors of embedded user queries with paper embeddings in our vector database to retrieve the 100 most similar
4. [4]:  Passage ID 4: adjective, adverb and so on (Part of Speech Tagging). At the semantic level, each word is analyzed to get the meaningful representation of the sentence. Hence, the basic task of NLP is to process the unstructured text and to produce a representation of its meaning. The higher level tasks in NLP are Machine Translation (MT), Information Extraction (IE), Information Retrieval (IR), Automatic Text Summarization (ATS), Question-Answering System, Parsing, Sentiment Analysis, Natural Language Understanding (NLU) and Natural Language Generation (NLG). Information Extraction (IE) refers to the use of computational methods to identify relevant pieces of information in document generated for human use and convert this information into a representation suitable for computer based storage, processing, and retrieval (Wimalasuriya and Dua, 2010). The input to IE system is a collection of documents (email, web pages, news groups, news articles, business reports, research papers, blogs, resumes,
5. [5]:  Passage ID 5: the top performers in classifying types of questions using the TREC database [146].Between their requirement for such understanding and their ease of examination due to the typical encoder–decoder structure they use,neural machine translation (NMT) systems (Section IV-G) are splendid testbeds for researching internal semantic representations. Poliak et al. [147] trained encoders on four different language pairs: English and Arabic, English and Spanish, English and Chinese, and English and German. The decoding classifiers were trained on four distinct datasets: Multi-NLI [148], which is an expanded version of SNLI [149], as well as three recast datasets from the JHU Decompositional Semantics Initiative [150] (FrameNet Plus or FN+ [151], Definite Pronoun Resolution or DPR [152], and Semantic Proto-Roles or SPR [153]). None of the results were particularly strong, although they were strongest in SPR. This led to the conclusion that NMT models do a poor job of capturing paraphrased