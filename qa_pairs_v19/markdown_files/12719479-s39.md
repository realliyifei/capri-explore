# Survey Article Inter-Coder Agreement for Computational Linguistics

## Question

What are the proposed solutions to improve intercoder agreement in word sense tagging, and how effective are they?

## URLs

1. https://ar5iv.org/html/2305.17984. [2305.17984] minOffense: Inter-Agreement Hate Terms for Stable Rules, Concepts, Transitivities, and Lattices
2. https://ar5iv.org/html/2312.00100. [2312.00100] Introducing Rhetorical Parallelism Detection: A New Task with Datasets, Metrics, and Baselines
3. https://ar5iv.org/html/1903.12271. [1903.12271] In Search of Meaning: Lessons, Resources and Next Steps for Computational Analysis of Financial Discourse
4. https://ar5iv.org/html/2003.10224. [2003.10224] Unsupervised Word Polysemy Quantification with Multiresolution Grids of Contextual Embeddings
5. https://ar5iv.org/html/2403.18024. [2403.18024] Enriching Word Usage Graphs with Cluster Definitions
6. https://ar5iv.org/html/2407.21037. [2407.21037] An Application of Large Language Models to Coding Negotiation Transcripts
7. https://ar5iv.org/html/1502.05472. [1502.05472] On the Effects of Low-Quality Training Data on Information Extraction from Clinical Reports
8. https://ar5iv.org/html/2209.04502. [2209.04502] A Close Look at a Systematic Method for Analyzing Sets of Security Advice

## Answer (I don't know?: ‚úó)

The context provided does not directly discuss specific solutions for improving intercoder agreement in word sense tagging, nor does it detail their effectiveness. However, it does discuss general considerations related to inter-coder agreement and potential improvements in coding practices. 

One passage outlines that even with structured methods like the SAcoding approach, discrepancies in tagging can arise due to the inherent imprecision in natural language descriptions. The study referenced suggests that while the SAcoding method is designed to minimize subjectivity, it nonetheless leads to varying tags among coders for closely related items. These outcomes highlight the need for methods to reduce non-agreements further. One proposed approach is rewording or clarifying the questions that lead to higher disagreement rates, as well as providing additional explanatory materials and instructions for coders [2][3][4]. 

The passages imply that enhancing coder consistency could potentially involve refining questions and guidelines to ensure clarity and reduce subjective interpretation. However, the effectiveness of these proposed solutions is not directly tested or quantified in the provided context. It emphasizes the expectation that with similar expertise among coders, the potential for agreement could increase, suggesting an indirect relationship between coder experience and intercoder agreement [3][4]. 

Thus, while the context points to some solutions aimed at improving intercoder agreement‚Äîmainly through refining the coding scheme and enhancing coder instructions‚Äîit does not provide explicit evidence on their effectiveness or detailed outcomes from implementing these strategies. Therefore, precise measures or statistical improvements are not outlined in the given information. In summary, while some solutions are identified, their actual effectiveness remains unaddressed in the context provided.

1. [1]:  https://ar5iv.org/html/2403.18024, [2403.18024] Enriching Word Usage Graphs with Cluster Definitions
2. [2]:  https://ar5iv.org/html/2407.21037, [2407.21037] An Application of Large Language Models to Coding Negotiation Transcripts
3. [3]:  https://ar5iv.org/html/2209.04502, [2209.04502] A Close Look at a Systematic Method for Analyzing Sets of Security Advice
4. [4]:  https://ar5iv.org/html/2209.04502, [2209.04502] A Close Look at a Systematic Method for Analyzing Sets of Security Advice
5. [5]:  https://ar5iv.org/html/1502.05472, [1502.05472] On the Effects of Low-Quality Training Data on Information Extraction from Clinical Reports
---
1. [1]:  Passage ID 1: and Massimo Poesio. 2008.Survey article:Inter-coder agreement for computational linguistics.Computational Linguistics, 34(4):555‚Äì596.Bird et¬†al. (2009)Steven Bird, Ewan Klein, and Edward Loper. 2009.Natural language processing with Python: analyzing text withthe natural language toolkit." O‚ÄôReilly Media, Inc.".Blevins and Zettlemoyer (2020)Terra Blevins and Luke Zettlemoyer. 2020.Moving down thelong tail of word sense disambiguation with gloss informed bi-encoders.In Proceedings of the 58th Association for ComputationalLinguistics.Chung et¬†al. (2022)Hyung¬†Won Chung, Le¬†Hou, Shayne Longpre, Barret Zoph, Yi¬†Tay, William Fedus,Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et¬†al. 2022.Scaling instruction-finetuned language models.arXiv preprint arXiv:2210.11416.Conneau et¬†al. (2020)Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, GuillaumeWenzek, Francisco Guzm√°n, Edouard Grave, Myle Ott, Luke
2. [2]:  Passage ID 2: on negotiation outcomes (Putnam & Jones, 1982[14]). A necessary step in these kinds of studies is to develop a coding scheme, train coders, ensure inter-coder agreement, and have those coders code hundreds or thousands of speech units. This process of human coding can take months, and cost thousands of dollars.Researchers can save time and money by automating this coding process. Additionally, automated coding reduces the risks that are inherent in human coding ‚Äì such as coder fatigue due to coding too many transcripts in a sitting or coder drift in applying codes across a number of transcripts. Indeed, having an automatic coding process may make some projects feasible that previously were not feasible. This study explores whether, and how, LLMs can be used to replace (or supplement) human coders, which should increase both the efficiency and reliability of coding for negotiation research. What we learned in the process of negotiation coding may also be applicable to other
3. [3]:  Passage ID 3: results is that when coders disagreed on tags for an item, in many cases the differing tags were nonetheless closely related, e.g., adjacent on the tag continuum of Barrera et al.¬†(Barrera et¬†al., 2023).Despite the design intent of the SAcoding method to reduce subjectivity in assigning tags to items, natural language descriptions of security advice are inherently imprecise. It is thus unsurprising that even using SAcoding (vs.¬†directly assigning tags), the two coders tagged significant numbers of advice items differently. This leads to asking how nonagreements between coders might be further reduced. One idea that we have noted is to explore rewording or clarification of questions that resulted in relatively large numbers of Q-nonagreements, and to provide further explanatory materials and instructions to coders.We expect that if further studies were run, one with two coders of similar expert-level security experience and another with two of vastly different experience levels,
4. [4]:  Passage ID 4: it viable to accomplish with reasonable resources?Q7subscriptùëÑ7Q_{7}Is it intended that the end-user carry out this item?Q8subscriptùëÑ8Q_{8}Is it intended that a security expert carry out this item?Q9subscriptùëÑ9Q_{9}Is it a general policy, general practice, or general procedure?Q10subscriptùëÑ10Q_{10}Is it a broad approach or security property?The publication that introduced SAcoding (Barrera et¬†al., 2023) did not explicitly recommend the number of coders to use, but employed the method using a single coder on a large dataset. Ideally, the SAcoding tree would produce very few or zero tag nonagreements as noted above, with each question in the tree designed to reduce reliance on subjective interpretation by coders‚Äîfor a given advice item, every question having crisp, clear criteria, such that competent coders all answer yes or no in unison. Our main goal herein is to explore the degree to which this occurs, and we find that coder
5. [5]:  Passage ID 5: how much intercoder disagreementgenerates how much loss in extraction accuracy.The rest of the paper is organized as follows. Section2 reviews related work on information extractionfrom clinical documents, and on establishing the relations betweentraining data quality and extraction accuracy. In Sections3 and 4 we describe experimentsthat attempt to quantify the degradation in extraction accuracy thatderives from low-quality training data, with Section3 devoted to spelling out the experimental settingand Section 4 devoted instead to presenting anddiscussing the results. Section 5 concludes,discussing avenues for further research.2 Related Work2.1 Information extraction from clinicaldocumentsMost works on information extraction from clinical documentsrely on methods based on supervised learning, i.e., methods fortraining an information extraction system from manually annotatedexamples. Support vector machines (SVMs ‚Äì[Jiang et¬†al. (2011), Liet¬†al. (2008),