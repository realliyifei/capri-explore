# "Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking

## Question

What are the two primary methods for collecting dialogue datasets, and what are the pros and cons of each?

## URLs

1. https://ar5iv.org/html/2406.07794. [2406.07794] Making Task-Oriented Dialogue Datasets More Natural by Synthetically Generating Indirect User Requests
2. https://ar5iv.org/html/2403.17319. [2403.17319] JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset
3. https://ar5iv.org/html/2405.03480. [2405.03480] Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search
4. https://ar5iv.org/html/2308.10792. [2308.10792] Instruction Tuning for Large Language Models: A Survey
5. https://ar5iv.org/html/2411.13577. [2411.13577] WavChat: A Survey of Spoken Dialogue Models
6. https://ar5iv.org/html/2410.09335. [2410.09335] Rethinking Data Selection at Scale: Random Selection is Almost All You Need
7. https://ar5iv.org/html/2309.13233. [2309.13233] User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue
8. https://ar5iv.org/html/2402.10137. [2402.10137] TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles
9. https://ar5iv.org/html/2409.16490. [2409.16490] Exploring Knowledge Tracing in Tutor-Student Dialogues
10. https://ar5iv.org/html/2205.12662. [2205.12662] DFM: Dialogue Foundation Model for Universal Large-Scale Dialogue-Oriented Task Learning

## Answer (I don't know?: ✗)

The two primary methods for collecting dialogue datasets discussed in the provided passages are expert-driven approaches and large language model (LLM)-guided methods.

1. **Expert-driven approaches**: This traditional method often relies on skilled individuals working in a controlled environment, famously exemplified by the wizard-of-oz setups. In such cases, an expert simulates responses in a dialogue system, creating a dataset that ideally reflects nuanced human interactions. However, this approach has notable downsides. It is challenging to scale due to the requirement for expert involvement, making it difficult to gather large-scale dialogue datasets that encompass multiple sessions and reflect diverse user preferences. As highlighted in Passage 4, "Previous approaches rely on experts in a wizard-of-oz setup that is difficult to scale, particularly for personalized tasks" [4].

2. **LLM-guided methods (LAPS)**: This newer approach utilizes large language models to assist a single human worker in generating personalized dialogues. The LAPS method aims to collect large-scale, human-written, multi-session, and multi-domain conversations effectively. The advantages of this method include increased speed of dataset creation and improved conversation quality. According to Passage 4, LAPS has "proven to speed up the creation process and improve quality" [4]. Furthermore, the naturalness and diversity of dialogues produced through this method have been shown to be comparable to those created by experts, addressing some limitations of fully synthetic methods, thus ensuring the resulting dataset is more aligned with real user preferences.

In summary, while expert-driven methods are beneficial for capturing intricate human interactions, they are not scalable and often lead to limited datasets. In contrast, LLM-guided methods can produce extensive and diverse dialogue datasets quickly but may still require careful oversight to ensure authenticity and relevance to real-world interactions.

1. [1]:  https://ar5iv.org/html/2410.00427, No Title
2. [2]:  https://ar5iv.org/html/2411.13577, [2411.13577] WavChat: A Survey of Spoken Dialogue Models
3. [3]:  https://ar5iv.org/html/2411.13577, [2411.13577] WavChat: A Survey of Spoken Dialogue Models
4. [4]:  https://ar5iv.org/html/2405.03480, [2405.03480] Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search
5. [5]:  https://ar5iv.org/html/1807.10854, No Title
---
1. [1]:  Passage ID 1: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
2. [2]:  Passage ID 2: systems, particularly their duplex, streaming nature, which distinguishes them from text-based dialogue systems. In Section 6, we examine the construction of training datasets and the evaluation methodologies specific to spoken dialogue models. At the end of each section, we include a summary and discussion to reflect on the key insights. Finally, in Section 7, we conclude the survey by summarizing the major findings and discussing open issues for future research. Given the complexity of the technical points, we provide an overview of the structure of this survey in Figure 3.Figure 2: A general overview of current spoken dialogue systems. We categorize these systems into two paradigms, cascaded spoken dialogue models and end-to-end spoken dialogue models, based on whether the core language model can directly understand and generate speech representations. Additionally, we provide a visualization of the input and output methods used in different spoken dialogue systems.2
3. [3]:  Passage ID 3: in full-duplex systems remains a significant challenge. Moreover, enhancing LLMs to effectively handle the speech modality—mastering both speech comprehension and generation—while maintaining robust natural language text processing capabilities, is hindered by the limited size of labeled speech datasets. These datasets are far smaller compared to the vast amounts of pure text data available, which risks diminishing the models’ original text processing capabilities. Thus, building a truly end-to-end conversational model that meets real-world requirements necessitates careful consideration of model architecture, training paradigms, and training data. Overall, we believe that several key aspects are crucial in the training paradigm of spoken dialogue models: aligning speech-text modalities to ensure consistent understanding, designing multi-stage training strategies for gradual adaptation, and optimizing training structures and inference paradigms for efficient performance.4.1
4. [4]:  Passage ID 4: in developing models is the lack of large-scale dialogue datasets that span multiple sessions and reflect real-world user preferences. Previous approaches rely on experts in a wizard-of-oz setup that is difficult to scale, particularly for personalized tasks.Our method, LAPS, addresses this by using large language models (LLMs) to guide a single human worker in generating personalized dialogues. This method has proven to speed up the creation process and improve quality.LAPS can collect large-scale, human-written, multi-session, and multi-domain conversations, including extracting user preferences.When compared to existing datasets, LAPS-produced conversations are as natural and diverse as expert-created ones, which stays in contrast with fully synthetic methods.The collected dataset is suited to train preference extraction and personalized response generation.Our results show that responses generated explicitly using extracted preferences better match user’s actual preferences,
5. [5]:  Passage ID 5: have received. To train a model to perform a certain task well, the last step a practitioner must go through is to use available downloadable task-specific corpora, or build one’s own task-specific corpus. This last training step is usually supervised. It is also recommended that if several tasks are to be performed, multi-task training be used wherever possible.V ConclusionsEarly applications of natural language processing included a well-acclaimed but simpleminded algebra word problem solver program called STUDENT [272], as well as interesting but severely constrained conversational systems such as Eliza, which acted as a “psycho-therapist” [273]), and another that conversed about manipulating blocks in a microworld [274]. Nowadays, highly advanced applications of NLP are ubiquitous. These include Google’s and Microsoft’s machine translators, which translate more or less competently from a language to scores of other languages, as well as a number of devices which process