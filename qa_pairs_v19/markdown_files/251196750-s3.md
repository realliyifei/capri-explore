# "Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking

## Question

What are the two primary methods for collecting dialogue datasets, and what are the pros and cons of each?

## URLs

1. https://ar5iv.org/html/2406.07794. [2406.07794] Making Task-Oriented Dialogue Datasets More Natural by Synthetically Generating Indirect User Requests
2. https://ar5iv.org/html/2403.17319. [2403.17319] JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset
3. https://ar5iv.org/html/2405.03480. [2405.03480] Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search
4. https://ar5iv.org/html/2308.10792. [2308.10792] Instruction Tuning for Large Language Models: A Survey
5. https://ar5iv.org/html/2411.13577. [2411.13577] WavChat: A Survey of Spoken Dialogue Models
6. https://ar5iv.org/html/2410.09335. [2410.09335] Rethinking Data Selection at Scale: Random Selection is Almost All You Need
7. https://ar5iv.org/html/2309.13233. [2309.13233] User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue
8. https://ar5iv.org/html/2402.10137. [2402.10137] TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles
9. https://ar5iv.org/html/2409.16490. [2409.16490] Exploring Knowledge Tracing in Tutor-Student Dialogues
10. https://ar5iv.org/html/2205.12662. [2205.12662] DFM: Dialogue Foundation Model for Universal Large-Scale Dialogue-Oriented Task Learning

## Answer (I don't know?: ✗)

In the field of natural language processing (NLP), the two primary methods for collecting dialogue datasets are traditional crowdsourcing and synthetic data generation. Each method has its own advantages and drawbacks.

1. **Crowdsourcing**: This traditional method involves gathering conversation data from a large group of people, often through platforms where contributors are paid to provide data. 

   - **Pros**: 
     - Crowdsourcing can yield a diverse range of conversational data, potentially covering multiple perspectives and styles of dialogue.
     - It allows for the collection of authentic dialogue that reflects real-world interactions, which can enhance the relatability and applicability of the dataset.

   - **Cons**: 
     - The process is often costly, as it requires compensation for contributors, which can limit scalability. 
     - It is also labor-intensive and time-consuming, as managing many contributors and ensuring quality control can be challenging. This method may result in inconsistencies and noise in the data depending on the contributor's understanding and engagement [3].

2. **Synthetic Data Generation**: This method uses algorithms and models to create dialogue data rather than relying on human contributions. Techniques might include data augmentation or transforming existing textual resources into conversational formats.

   - **Pros**: 
     - Synthetic data generation is generally more scalable and efficient than crowdsourcing; it can produce large volumes of data quickly.
     - It allows for systematic control over the types of dialogue generated, enabling researchers to create specific scenarios or styles of conversation that may not be easily obtained through crowdsourcing [3].

   - **Cons**: 
     - The main drawback is that synthetic data may lack the authenticity and variability found in genuine human interactions, potentially leading to less effective models if they are trained exclusively on such data.
     - The quality of the synthetic data depends heavily on the underlying algorithms and methods used for generation. If the models are not sophisticated enough, the output might not adequately reflect the complexity of human dialogue [3] [5].

In summary, while crowdsourcing and synthetic data generation each serve vital roles in the creation of dialogue datasets, they come with distinct pros and cons. Crowdsourcing offers authenticity and variability but at a high cost and labor intensity, whereas synthetic methods provide scalability and control but risk sacrificing the naturalness and depth of real human conversation. The choice between these methods—or a combination of both—can significantly impact the quality and effectiveness of the resulting dialogue systems.

1. [1]:  https://ar5iv.org/html/2410.00427, No Title
2. [2]:  https://ar5iv.org/html/1807.10854, No Title
3. [3]:  https://ar5iv.org/html/2405.13003, [2405.13003] A Survey on Recent Advances in Conversational Data Generation
4. [4]:  https://ar5iv.org/html/2411.13577, [2411.13577] WavChat: A Survey of Spoken Dialogue Models
5. [5]:  https://ar5iv.org/html/2411.13577, [2411.13577] WavChat: A Survey of Spoken Dialogue Models
---
1. [1]:  Passage ID 1: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
2. [2]:  Passage ID 2: NLP software. Furthermore, there are thousands of languages spoken throughout the world, with at least eighty spoken by more than 10 million people, meaning that current research excludes an immense segment of humankind. Collection and validation of data in under-analyzed languages, as well as testing NLP models using such data, will be a tremendous contribution to not only the field of natural language processing, but to human society as a whole.Due to the small amounts of data available in many languages, the authors do not foresee the complete usurpation of traditional NLP models by deep learning any time in the near future. Deep learning models (and even shallow ANNs) are extremely data-hungry. Contrastingly, many traditional models require only relatively small amounts of training data. However, looking further forward, it can be anticipated that deep learning models will become the norm in computational linguistics, with pre-training and transfer learning playing highly
3. [3]:  Passage ID 3: systems is challenging due to the scarcity of specialized dialogue data. Traditionally, conversational datasets were created through crowdsourcing, but this method has proven costly, limited in scale, and labor-intensive. As a solution, the development of synthetic dialogue data has emerged, utilizing techniques to augment existing datasets or convert textual resources into conversational formats, providing a more efficient and scalable approach to dataset creation.In this survey, we offer a systematic and comprehensive review of multi-turn conversational data generation, focusing on three types of dialogue systems: open domain, task-oriented, and information-seeking. We categorize the existing research based on key components like seed data creation, utterance generation, and quality filtering methods, and introduce a general framework that outlines the main principles of conversation data generation systems. Additionally, we examine the evaluation metrics and methods for assessing
4. [4]:  Passage ID 4: systems, particularly their duplex, streaming nature, which distinguishes them from text-based dialogue systems. In Section 6, we examine the construction of training datasets and the evaluation methodologies specific to spoken dialogue models. At the end of each section, we include a summary and discussion to reflect on the key insights. Finally, in Section 7, we conclude the survey by summarizing the major findings and discussing open issues for future research. Given the complexity of the technical points, we provide an overview of the structure of this survey in Figure 3.Figure 2: A general overview of current spoken dialogue systems. We categorize these systems into two paradigms, cascaded spoken dialogue models and end-to-end spoken dialogue models, based on whether the core language model can directly understand and generate speech representations. Additionally, we provide a visualization of the input and output methods used in different spoken dialogue systems.2
5. [5]:  Passage ID 5: in full-duplex systems remains a significant challenge. Moreover, enhancing LLMs to effectively handle the speech modality—mastering both speech comprehension and generation—while maintaining robust natural language text processing capabilities, is hindered by the limited size of labeled speech datasets. These datasets are far smaller compared to the vast amounts of pure text data available, which risks diminishing the models’ original text processing capabilities. Thus, building a truly end-to-end conversational model that meets real-world requirements necessitates careful consideration of model architecture, training paradigms, and training data. Overall, we believe that several key aspects are crucial in the training paradigm of spoken dialogue models: aligning speech-text modalities to ensure consistent understanding, designing multi-stage training strategies for gradual adaptation, and optimizing training structures and inference paradigms for efficient performance.4.1