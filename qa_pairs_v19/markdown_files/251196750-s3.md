# "Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking

## Question

What are the two primary methods for collecting dialogue datasets, and what are the pros and cons of each?

## URLs

1. https://ar5iv.org/html/2406.07794. [2406.07794] Making Task-Oriented Dialogue Datasets More Natural by Synthetically Generating Indirect User Requests
2. https://ar5iv.org/html/2403.17319. [2403.17319] JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset
3. https://ar5iv.org/html/2405.03480. [2405.03480] Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search
4. https://ar5iv.org/html/2308.10792. [2308.10792] Instruction Tuning for Large Language Models: A Survey
5. https://ar5iv.org/html/2411.13577. [2411.13577] WavChat: A Survey of Spoken Dialogue Models
6. https://ar5iv.org/html/2410.09335. [2410.09335] Rethinking Data Selection at Scale: Random Selection is Almost All You Need
7. https://ar5iv.org/html/2309.13233. [2309.13233] User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue
8. https://ar5iv.org/html/2402.10137. [2402.10137] TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles
9. https://ar5iv.org/html/2409.16490. [2409.16490] Exploring Knowledge Tracing in Tutor-Student Dialogues
10. https://ar5iv.org/html/2205.12662. [2205.12662] DFM: Dialogue Foundation Model for Universal Large-Scale Dialogue-Oriented Task Learning

## Answer (I don't know?: ✗)

The two primary methods for collecting dialogue datasets are the wizard-of-oz approach and the use of large language models (LLMs) to generate synthetic dialogues. 

1. **Wizard-of-Oz Approach**: This method involves human experts simulating a dialogue system's responses, often without the end user knowing that they are interacting with a human rather than a machine. While this approach can yield high-quality data that reflects real-world interactions, it has significant limitations in terms of scalability. The reliance on experts means that collecting large amounts of diverse dialogues can be time-consuming and resource-intensive. Additionally, this method may not accurately capture user preferences over multiple sessions due to its one-off nature [3]. 

2. **LLM-Based Synthetic Dialogues**: More recently, approaches like LAPS (LLM-Augmented Personalized Self-Dialogue) have been introduced, which utilize LLMs to guide human workers in generating personalized dialogues. This method has shown advantages, such as the capability to speed up the data generation process and improve the overall quality of the dialogues produced. When compared to expert-created conversations, those produced by LAPS are reported to be natural and diverse, capturing user preferences more effectively. However, there are concerns regarding the credibility of LLM-generated conversations, as they may not accurately represent real user interactions or preferences, which could undermine their reliability for future applications in personal conversational systems [4] [5].

In summary, the wizard-of-oz method allows for high-quality, nuanced data but at the cost of scalability and efficiency, while LLM-based approaches facilitate rapid and sizeable data generation but raise concerns about the authenticity and applicability of the generated dialogues. These trade-offs must be considered when selecting a method for dialogue dataset collection.

1. [1]:  https://ar5iv.org/html/2411.13577, [2411.13577] WavChat: A Survey of Spoken Dialogue Models
2. [2]:  https://ar5iv.org/html/2411.13577, [2411.13577] WavChat: A Survey of Spoken Dialogue Models
3. [3]:  https://ar5iv.org/html/2405.03480, [2405.03480] Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search
4. [4]:  https://ar5iv.org/html/2405.03480, [2405.03480] Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search
5. [5]:  https://ar5iv.org/html/2411.13577, [2411.13577] WavChat: A Survey of Spoken Dialogue Models
---
1. [1]:  Passage ID 1: systems, particularly their duplex, streaming nature, which distinguishes them from text-based dialogue systems. In Section 6, we examine the construction of training datasets and the evaluation methodologies specific to spoken dialogue models. At the end of each section, we include a summary and discussion to reflect on the key insights. Finally, in Section 7, we conclude the survey by summarizing the major findings and discussing open issues for future research. Given the complexity of the technical points, we provide an overview of the structure of this survey in Figure 3.Figure 2: A general overview of current spoken dialogue systems. We categorize these systems into two paradigms, cascaded spoken dialogue models and end-to-end spoken dialogue models, based on whether the core language model can directly understand and generate speech representations. Additionally, we provide a visualization of the input and output methods used in different spoken dialogue systems.2
2. [2]:  Passage ID 2: in full-duplex systems remains a significant challenge. Moreover, enhancing LLMs to effectively handle the speech modality—mastering both speech comprehension and generation—while maintaining robust natural language text processing capabilities, is hindered by the limited size of labeled speech datasets. These datasets are far smaller compared to the vast amounts of pure text data available, which risks diminishing the models’ original text processing capabilities. Thus, building a truly end-to-end conversational model that meets real-world requirements necessitates careful consideration of model architecture, training paradigms, and training data. Overall, we believe that several key aspects are crucial in the training paradigm of spoken dialogue models: aligning speech-text modalities to ensure consistent understanding, designing multi-stage training strategies for gradual adaptation, and optimizing training structures and inference paradigms for efficient performance.4.1
3. [3]:  Passage ID 3: in developing models is the lack of large-scale dialogue datasets that span multiple sessions and reflect real-world user preferences. Previous approaches rely on experts in a wizard-of-oz setup that is difficult to scale, particularly for personalized tasks.Our method, LAPS, addresses this by using large language models (LLMs) to guide a single human worker in generating personalized dialogues. This method has proven to speed up the creation process and improve quality.LAPS can collect large-scale, human-written, multi-session, and multi-domain conversations, including extracting user preferences.When compared to existing datasets, LAPS-produced conversations are as natural and diverse as expert-created ones, which stays in contrast with fully synthetic methods.The collected dataset is suited to train preference extraction and personalized response generation.Our results show that responses generated explicitly using extracted preferences better match user’s actual preferences,
4. [4]:  Passage ID 4: paradigm of collecting synthetic conversational data through LLMs (Gilardi et al., 2023; Yunxiang et al., 2023; Lee et al., 2022; Xu et al., 2023; Chen et al., 2023; Kim et al., 2023, 2022; Jandaghi et al., 2023) raises concerns regarding the diversity of the generated dialogues (Reif et al., 2023; Chung et al., 2023; Yu et al., 2023; Dell’Acqua et al., 2023; Park et al., 2023).Crucially, LLM-generated conversations do not represent actual user preferences and interactions, which undermines their credibility for the development of future personal conversational systems.The critical question that arises here is RQ1: Can we collect large-scale multi-session human-written conversational datasets that contain user preferences?We address this question by proposing LAPS, an LLM-Augmented Personalized Self-Dialogue method to collect large-scale personal conversations.The method employs an LLM to dynamically generate personal guidance for crowd workers, playing both user and assistant
5. [5]:  Passage ID 5: shape the intelligent development of dialogue systems.To address the limitations of existing training spoken dialogue data and leverage the knowledge and reasoning capabilities of mature text-based models, many approaches involve Continue Training on pre-trained text language models. This training paradigm encompasses nearly all data types required to build a spoken dialogue system. The following sections focus on analyzing data acquisition and processing methods under this training flow, covering the following core stages: Text Language Model Pre-training, Post-Train for Audio Modal Adaption, Post-Train for Dual-Stream Audio Processing, Enhancing Conversational Abilities and Instruction Tuning. We have listed commonly used datasets for training in Table 2. However, current spoken dialogue models lack exploration in music and sound. To support future development in spoken dialogue systems, we provide a list of common music and sound datasets in the appendix A as a