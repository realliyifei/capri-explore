# GPTEval: A Survey on Assessments of ChatGPT and GPT-4

## Question

How effective were ChatGPT and GPT-4 in handling ambiguity in Natural Language Inference tasks?

## URLs

1. https://ar5iv.org/html/2304.03439
2. https://ar5iv.org/html/2404.11972
3. https://ar5iv.org/html/2305.05862
4. https://ar5iv.org/html/2405.12063
5. https://ar5iv.org/html/2411.18337
6. https://ar5iv.org/html/2312.11511
7. https://ar5iv.org/html/2402.17717

## Answer

I don't know. The context provided does not include specific information regarding the effectiveness of ChatGPT and GPT-4 in handling ambiguity in Natural Language Inference (NLI) tasks. While the passages discuss various aspects of the models, such as their performance in deductive reasoning [5], comparisons with traditional Question Answering Systems [1] [4], and the utilization of different evaluation frameworks [2] [3], there are no direct mentions or evaluations of how these models address ambiguous scenarios specifically in NLI. Therefore, the context is insufficient to provide a clear answer to your question.

[1]: https://ar5iv.org/html/2312.07592, No Title
[2]: https://ar5iv.org/html/2401.05561, No Title
[3]: https://ar5iv.org/html/2401.05561, No Title
[4]: https://ar5iv.org/html/2312.07592, No Title
[5]: https://ar5iv.org/html/2304.03439, [2304.03439] Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4

[1]: Passage ID 1: tasks.Omar et al. (2023)Omar et al. (2023) compared traditional question answering systems (i.e. KGQAN and EDGQA) with ChatGPT for knowledge graphs. Several experiments were conducted on four real-world knowledge graphs over different applications. The authors discussed the advantages and disadvantages of each approach and proposed future directions for improving knowledge graph chatbots. The authors prompted ChatGPT in three modes: Default, Follow-up asking the entire list of response and Excel, requesting the answers in structured format. The QAS performance is evaluated in terms of determinism considering precision, recall and F1 score. The chatbot capabilities to produce correct and fluent answers is evaluated in terms of robustness, explainability and question understanding, through human manual assessment process. The paper suggests that ChatGPT provides more robust and explainable conversations compared to traditional QASs, but there are challenges to overcome, such as ensuring
[2]: Passage ID 2: evaluation’s high cost and low efficiency, we’ve integrated a specialized classifier [73] and ChatGPT/GPT-4 into our evaluation framework.For the tasks with ground-truth labels, our evaluation focuses on keyword matching and regular expressions. When the approach fails to assess particular responses accurately, we utilize ChatGPT/GPT-4 to extract keywords in answers before the evaluation process.Regarding generative tasks, they yield various answers, often including reasoning and explanations, making traditional keyword/regex matching ineffective. Recent studies have validated the effectiveness of LLMs in evaluation [372, 373, 73, 374, 375], enabling their use as cost-effective alternatives to human evaluators. Consequently, for complex generative tasks such as “Adversarial Factuality" (§6.4), we employ GPT-4, whereas, for more straightforward generative tasks, ChatGPT (GPT-3.5) is used to ensure cost-effectiveness. Additionally, we employ a previously researched evaluator (i.e.,
[3]: Passage ID 3: evaluation’s high cost and low efficiency, we’ve integrated a specialized classifier [73] and ChatGPT/GPT-4 into our evaluation framework.For the tasks with ground-truth labels, our evaluation focuses on keyword matching and regular expressions. When the approach fails to assess particular responses accurately, we utilize ChatGPT/GPT-4 to extract keywords in answers before the evaluation process.Regarding generative tasks, they yield various answers, often including reasoning and explanations, making traditional keyword/regex matching ineffective. Recent studies have validated the effectiveness of LLMs in evaluation [372, 373, 73, 374, 375], enabling their use as cost-effective alternatives to human evaluators. Consequently, for complex generative tasks such as “Adversarial Factuality" (§6.4), we employ GPT-4, whereas, for more straightforward generative tasks, ChatGPT (GPT-3.5) is used to ensure cost-effectiveness. Additionally, we employ a previously researched evaluator (i.e.,
[4]: Passage ID 4: to this workAbstractIn the current era, a multitude of language models has emerged to cater to user inquiries. Notably, the GPT-3.5 Turbo language model has gained substantial attention as the underlying technology for ChatGPT. Leveraging extensive parameters, this model adeptly responds to a wide range of questions. However, due to its reliance on internal knowledge, the accuracy of responses may not be absolute. This article scrutinizes ChatGPT as a Question Answering System (QAS), comparing its performance to other existing QASs. The primary focus is on evaluating ChatGPT’s proficiency in extracting responses from provided paragraphs, a core QAS capability. Additionally, performance comparisons are made in scenarios without a surrounding passage. Multiple experiments, exploring response hallucination and considering question complexity, were conducted on ChatGPT. Evaluation employed well-known Question Answering (QA) datasets, including SQuAD, NewsQA, and PersianQuAD, across
[5]: Passage ID 5: ChatGPT and GPT-4 in this domain. Previous ChatGPT evaluation indicates that ChatGPT performs better on deductive reasoning than inductive ones Bang et al. (2023) and that ChatGPT performs better on factual inputs Qin et al. (2023). After the release of GPT-4, we will discuss two tasks for evaluating its logical reasoning abilities: multi-choice reading comprehension and natural language inference. Both tasks are reasoning-heavy and serve as a playground for testing models’ reasoning abilities. Several logical reasoning datasets have been released under the two tasks. These benchmarks proved to be difficult to solve for PLMs. We hope this report will shed more light on the logical reasoning ability of ChatGPT and GPT-4. Our contributions are as follows:1. We test ChatGPT and GPT-4 on two logical reasoning tasks: multi-choice reading comprehension and natural language inference. We conduct experiments on multiple logical reasoning benchmarks to analyze the logical reasoning ability