# Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems

## Question

How is the "pre-train, fine-tune extra part of the model" strategy utilized in recommendation systems?

## URLs

1. https://ar5iv.org/html/2302.03735. [2302.03735] Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems

## Answer (I don't know?: ✗)

The "pre-train, fine-tune" strategy, particularly the approach of pre-training large models and then fine-tuning specific components of those models, has gained traction in recommendation systems (RS) due to its effectiveness in improving performance while balancing efficiency. This approach consists of a two-phase training process.

In the first phase, models are pre-trained on extensive datasets, allowing them to learn generalizable features that can be useful for a variety of tasks. Pre-training equips the model with universal knowledge, which enhances its ability to perform well on diverse downstream recommendation tasks [1]. The benefit of this approach is not only better generalization across different recommendations but also faster convergence during fine-tuning [1].

In the second phase, designated fine-tuning is performed, which often involves updating only specific parts of the larger model to adapt to the targeted recommendation task. This selective fine-tuning can include techniques where a partial pre-trained model is fine-tuned or an additional component with significantly fewer parameters is adjusted [2]. By focusing on a smaller subset of the model, this method attempts to maintain both efficiency and performance—addressing the challenges of updating large-scale models without compromising their effectiveness [2].

Research highlights some practical implementations of this strategy. For instance, a study indicates that while fine-tuning only the output layer of a model often leads to subpar performance, fine-tuning the last few layers of the pre-trained model can yield better results, although with varying stability depending on the model and specific tasks [2]. In efforts to further enhance recommendation efficiency, some researchers have suggested compressing large pre-trained models into smaller versions, termed 'student models,' that retain essential capabilities while being easier to manipulate [2].

Additionally, fine-tuning methods such as Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning, and techniques like the Q-LoRA method have gained attention as effective strategies within the realm of fine-tuning pre-trained language models for task-specific applications in RS [3]. This reflects a focus on adapting the model to handle specialized tasks, thus addressing the limitations that pre-trained models may experience in niche domains [3].

In summary, the "pre-train, fine-tune extra part of the model" strategy leverages extensive training on large datasets for general competency and subsequently narrows its focus on particular components for refinement. This dual approach ultimately enables recommendation systems to efficiently use sophisticated models while achieving higher accuracy tailored to specific user needs and contexts [1] [2] [3].

1. [1]:  https://ar5iv.org/html/2302.03735, [2302.03735] Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems
2. [2]:  https://ar5iv.org/html/2302.03735, [2302.03735] Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems
3. [3]:  https://ar5iv.org/html/2411.15382, No Title
4. [4]:  https://ar5iv.org/html/2409.16674, No Title
5. [5]:  https://ar5iv.org/html/2302.03735, [2302.03735] Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems
---
1. [1]:  Passage ID 1: different training efforts on different parts of the recommendation model. This section will go through various training strategies w.r.t. specific recommendation purposes. Figure 2(a) presents the statistics of recent publications of LMRSs grouped by different training strategies and the total number of published research works each year. Figure 2(b) shows the taxonomy and some corresponding representative LMRSs.4.1 Pre-train, fine-tune paradigm for RSThe “pre-train, fine-tune" paradigm attracts increasing attention from researchers in the recommendation field due to several advantages: 1) Pre-training provides a better model initialization, which usually leads to better generalization on different downstream recommendation tasks, improves recommendation performance from various perspectives, and speeds up convergence on the fine-tuning stage; 2) Pre-training on huge source corpus can learn universal knowledge which can be beneficial for the downstream recommenders; 3)
2. [2]:  Passage ID 2: great success of pre-trained models in multiple recommendation tasks, how to maintain and update such complex and large-scale models without affecting the efficiency and accuracy of recommendations in reality needs more attention. Some works have proposed improving model updating efficiency by fine-tuning a partial pre-trained model or an extra part with far fewer parameters than the model’s magnitude. However, Yuan et al. (2020b) empirically found that fine-tuning only the output layer often resulted in poor performance in recommendation scenarios. While properly fine-tuning the last few layers sometimes offered promising performance, the improvements were quite unstable and depended on the pre-trained model and tasks. Yu et al. (2022) proposed compressing large pre-trained language models into student models to improve recommendation efficiency, while Yang et al. (2022b) focused on accelerating the fine-tuning of pre-trained language models and reducing GPU memory footprint for news
3. [3]:  Passage ID 3: creation (Leiker et al., 2023), and storytelling (Xie et al., 2023). Built on transformer-based architectures with billions of parameters, these models undergo extensive training on large-scale datasets, which equips them with capabilities for handling complex natural language processing tasks. However, these pre-trained models often face challenges in domains that demand specialized knowledge, such as medical fields (Jin et al., 2021; Pal et al., 2022) and legal services (Yue et al., 2023). To address these limitations, recent research have focused on fine-tuning pre-trained LLMs to adapt them to specialized tasks. This process involves training a pre-trained model on a smaller task-specific dataset while retaining most of its learned parameters.Common fine-tuning methods such as Reinforcement Learning with Human Feedback (RLHF) (Ouyang et al., 2024), supervised fine-tuning (Mecklenburg et al., 2024; Rafailov et al., 2023), and Q-LoRA method (Dettmers et al., 2023) have been widely
4. [4]:  Passage ID 4: which auxiliary information can be used for a recommender system is not researched thoroughly. Recently, the advent of Large Language Models (LLMs) in Natural Language Processing (NLP) has sparked increased enthusiasm for leveraging the capabilities of these models to elevate and improve recommendation systems.With the thriving of pre-training in NLP, many language models have been pre-trained on large scale unsupervised corpora, and then fine-tuned for downstream tasks. The transformer architecture[3], was introduced in 2017, has become a foundation in LLMs.Based on Transformer architecture, many Pre-trained Language Models (PLMs) have emerged. GPT series[4], developed by OpenAI, and BERT[5], developed by Google, represent two prominent approaches to leverage transformers for PLMs. The key advantage of incorporating PLMs into recommendation systems lies in their ability to extract high-quality representations of textual features and leverage the extensive external knowledge
5. [5]:  Passage ID 5: balancing the recommendation accuracy and training efficiency.Pre-train, fine-tune partial modelSince fine-tuning the whole model is usually time-consuming and less flexible, many LMRSs choose to fine-tune partial parameters of the model to achieve a balance between training overhead and recommendation performance (Hou et al., 2022; Yu et al., 2022; Wu et al., 2022a). For instance, to deal with the domain bias problem that BERT induces a non-smooth anisotropic semantic space for general texts resulting in a large language gap for texts from different domains of items, Hou et al. (2022) applied a linear transformation layer to transform BERT representations of items from different domains followed by an adaptive combination strategy to derive a universal item representation. Meanwhile, considering the seesaw phenomenon that learning from multiple domain-specific behavioural patterns can be a conflict, they proposed sequence-item and sequence-sequence contrastive tasks for multi-task