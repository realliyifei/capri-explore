# Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems

## Question

How is the "pre-train, fine-tune extra part of the model" strategy utilized in recommendation systems?

## URLs

1. https://ar5iv.org/html/2302.03735. [2302.03735] Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems

## Answer (I don't know?: ✗)

The "pre-train, fine-tune extra part of the model" strategy is a specific approach in recommendation systems that focuses on enhancing the performance of models by fine-tuning only additional, task-specific layers in conjunction with pre-trained transformer models (PTMs). This strategy allows for leveraging the rich representations learned by the PTMs while optimizing the model for specific recommendation tasks.

In this context, some researchers have proposed the idea of incorporating a task-specific layer on top of the PTMs. Instead of fine-tuning the entire model—which can be computationally expensive and time-consuming—this strategy opts to optimize only the parameters of the newly added layer that is designed for the recommendation task at hand. This method is particularly advantageous because it streamlines the training process, reduces the training overhead, and increases flexibility without sacrificing the quality of recommendations [3][4].

An example of this approach can be seen in the work of Shang et al. (2019), who pre-trained both GPT and BERT models to learn embeddings related to patient visits. These embeddings were then utilized as inputs for fine-tuning the additional prediction layer specifically for medication recommendations. This approach highlights the effectiveness of using pre-trained models to enhance the information provided by new task-specific layers, allowing the system to better cater to the complexities of the recommendation task while maintaining efficient training [4].

Additionally, by focusing on the extra parts of the model, researchers can adapt to novel recommendation scenarios, such as handling unseen domains or new items. The strategy of fine-tuning only a small proportion of model parameters aids in rapidly adjusting to new contexts, thereby facilitating quicker response times for cold-start problems in recommendations [4].

Overall, the "pre-train, fine-tune extra part of the model" strategy emphasizes the flexibility and efficiency of recommendation systems, allowing them to utilize powerful pre-trained architectures while being tailored for specific tasks through minimal additional training requirements. This method not only improves performance outcomes but also allows for scalability in applying advanced deep learning techniques in practical recommendation applications.

1. [1]:  https://ar5iv.org/html/2302.03735, [2302.03735] Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems
2. [2]:  https://ar5iv.org/html/2302.03735, [2302.03735] Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems
3. [3]:  https://ar5iv.org/html/2302.03735, [2302.03735] Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems
4. [4]:  https://ar5iv.org/html/2302.03735, [2302.03735] Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems
5. [5]:  https://ar5iv.org/html/2302.03735, [2302.03735] Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems
---
1. [1]:  Passage ID 1: different training efforts on different parts of the recommendation model. This section will go through various training strategies w.r.t. specific recommendation purposes. Figure 2(a) presents the statistics of recent publications of LMRSs grouped by different training strategies and the total number of published research works each year. Figure 2(b) shows the taxonomy and some corresponding representative LMRSs.4.1 Pre-train, fine-tune paradigm for RSThe “pre-train, fine-tune" paradigm attracts increasing attention from researchers in the recommendation field due to several advantages: 1) Pre-training provides a better model initialization, which usually leads to better generalization on different downstream recommendation tasks, improves recommendation performance from various perspectives, and speeds up convergence on the fine-tuning stage; 2) Pre-training on huge source corpus can learn universal knowledge which can be beneficial for the downstream recommenders; 3)
2. [2]:  Passage ID 2: great success of pre-trained models in multiple recommendation tasks, how to maintain and update such complex and large-scale models without affecting the efficiency and accuracy of recommendations in reality needs more attention. Some works have proposed improving model updating efficiency by fine-tuning a partial pre-trained model or an extra part with far fewer parameters than the model’s magnitude. However, Yuan et al. (2020b) empirically found that fine-tuning only the output layer often resulted in poor performance in recommendation scenarios. While properly fine-tuning the last few layers sometimes offered promising performance, the improvements were quite unstable and depended on the pre-trained model and tasks. Yu et al. (2022) proposed compressing large pre-trained language models into student models to improve recommendation efficiency, while Yang et al. (2022b) focused on accelerating the fine-tuning of pre-trained language models and reducing GPU memory footprint for news
3. [3]:  Passage ID 3: balancing the recommendation accuracy and training efficiency.Pre-train, fine-tune partial modelSince fine-tuning the whole model is usually time-consuming and less flexible, many LMRSs choose to fine-tune partial parameters of the model to achieve a balance between training overhead and recommendation performance (Hou et al., 2022; Yu et al., 2022; Wu et al., 2022a). For instance, to deal with the domain bias problem that BERT induces a non-smooth anisotropic semantic space for general texts resulting in a large language gap for texts from different domains of items, Hou et al. (2022) applied a linear transformation layer to transform BERT representations of items from different domains followed by an adaptive combination strategy to derive a universal item representation. Meanwhile, considering the seesaw phenomenon that learning from multiple domain-specific behavioural patterns can be a conflict, they proposed sequence-item and sequence-sequence contrastive tasks for multi-task
4. [4]:  Passage ID 4: considering the seesaw phenomenon that learning from multiple domain-specific behavioural patterns can be a conflict, they proposed sequence-item and sequence-sequence contrastive tasks for multi-task learning during the pre-training stage. They found only fine-tuning a small proportion of model parameters could quickly adapt the model to unseen domains with cold-start or new items.Pre-train, fine-tune extra part of the modelWith the increase in the depth of PTMs, the representation captured by them makes the downstream recommendation easier. Apart from the aforementioned two fine-tuning strategies, some works leverage a task-specific layer on top of the PTMs for recommendation tasks. Fine-tuning only goes through such extra parts of the PTMs by optimizing the parameters of the task-specific layer. Shang et al. (2019) pre-trained a GPT and a BERT model to learn patient visit embeddings, which were then used as input to fine-tune the extra prediction layer for medication
5. [5]:  Passage ID 5: a haggingface transformer-based architecture as the base model for next-item prediction and explores four different LM tasks, namely Causal LM, MLM, Permutation LM, and Replacement Token Detection during training. These two models laid the foundation for LM-based recommender systems and have become popular baselines for their successors.Pre-train, fine-tune holistic modelUnder this category, the model is pre-trained and fine-tuned with different data sources, and the fine-tuning process will go through adjusting the whole model parameters. The learning objectives can also vary between the pre-training and fine-tuning stages. Pre-training and fine-tuning with different domains of data sources, also called cross-domain recommendation, can refer to the works of Kang et al. (2021) and Qiu et al. (2021). Kang et al. (2021) pre-trained a GPT model using segmented source API code and fine-tuned it with API code snippets from another library for cross-library recommendation. Wang et al.