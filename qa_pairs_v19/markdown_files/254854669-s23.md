# Let's Negotiate! A Survey of Negotiation Dialogue Systems

## Question

How is supervised learning utilized in negotiation dialogue systems to predict strategies and generate responses?

## URLs

1. https://ar5iv.org/html/2212.09072. [2212.09072] Let’s Negotiate! A Survey of Negotiation Dialogue Systems
2. https://ar5iv.org/html/2402.01097. [2402.01097] Let’s Negotiate! A Survey of Negotiation Dialogue Systems
3. https://ar5iv.org/html/2305.13626. [2305.13626] Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration
4. https://ar5iv.org/html/1808.09637. [1808.09637] Decoupling Strategy and Generation in Negotiation Dialogues
5. https://ar5iv.org/html/2410.01555. [2410.01555] ACE: A LLM-based Negotiation Coaching System
6. https://ar5iv.org/html/2402.13550. [2402.13550] Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues
7. https://ar5iv.org/html/2311.00262. [2311.00262] Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents

## Answer (I don't know?: ✗)

Supervised learning (SL) plays a crucial role in negotiation dialogue systems, primarily by enabling the prediction of strategies and the generation of appropriate responses based on dialogue context. 

In the context of negotiation systems, supervised learning is employed to train models that can mimic human negotiation behavior. For example, the SL approach aims to learn the optimal actions that should be taken by maximizing the likelihood of the training data, effectively trying to reflect average human behavior in negotiations [1] [4]. However, it has limitations as it does not directly optimize for specific outcomes in a negotiation scenario.

To address these limitations, He et al. (2018) propose an enhanced SL framework that optimizes for a particular dialogue reward function, which considers several factors including the utility function for both the buyer and seller, the differences in utilities between the negotiating agents, and the number of utterances throughout the dialogue [1] [4]. This direct optimization allows for a more tailored approach beyond just mimicking past behaviors, which can enhance the effectiveness of the negotiation strategy.

In addition, Zhou et al. (2020) illustrate how SL can be used to predict negotiation strategies within the context of dialogue. Their work involves training a strategy predictor that forecasts whether a certain negotiation strategy will occur in the subsequent utterance based on the current dialogue state [1] [4]. This predictive mechanism is integral, as it lays the groundwork for generating system responses that are not only relevant to the user utterance but also in line with the anticipated negotiation strategies.

However, traditional methods often struggle with dialogue management, resulting in challenges in controlling and interpreting the strategies used [2]. More recent advancements involve the integration of end-to-end neural models that simultaneously learn both dialogue strategy and language realization from human-human dialogues, enhancing the models' ability to generate more fluid and natural responses during negotiations [2]. 

Moreover, the application of SL in training chat agents for negotiation has also been enhanced through the use of pre-existing negotiation data to perform both supervised learning and offline reinforcement learning on the agents' models [3]. This results in agents that can effectively engage in negotiations by learning from prior interactions and optimizing their responses accordingly.

In summary, supervised learning in negotiation dialogue systems is utilized to predict strategies and generate responses by maximizing the probability of successful negotiation behaviors, optimizing dialogue rewards, and leveraging historical negotiation data to train models capable of understanding and navigating complex negotiation dynamics effectively [1] [2] [3].

1. [1]:  https://ar5iv.org/html/2212.09072, [2212.09072] Let’s Negotiate! A Survey of Negotiation Dialogue Systems
2. [2]:  https://ar5iv.org/html/1808.09637, [1808.09637] Decoupling Strategy and Generation in Negotiation Dialogues
3. [3]:  https://ar5iv.org/html/2410.01555, [2410.01555] ACE: A LLM-based Negotiation Coaching System
4. [4]:  https://ar5iv.org/html/2402.01097, [2402.01097] Let’s Negotiate! A Survey of Negotiation Dialogue Systems
5. [5]:  https://ar5iv.org/html/2104.10810, No Title
---
1. [1]:  Passage ID 1: of the training data. However, supervised learning only aims to mimic the average human behavior, so He et al. (2018) propose to finetune the supervised model to directly optimize for a particular dialogue reward function, which is defined as i) the utility function of the final price for the buyer and seller ii) the difference between two agents’ utilities iii) the number of utterances in the dialogue. Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training. The system response would be generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy. In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm. Recently, Dutt et al. (2021b) propose a generalisedframework for identifying resisting strategies in persuasive negotiations using a pre-trained
2. [2]:  Passage ID 2: of that strategy via generation of natural language (e.g.,“I really need a car so I can go to work, but all I have is 6000, any more and I won’t be able to feed my children.”).Most past work in NLP on negotiation focuses on strategy (dialogue management)with either no natural language Cuayáhuitl et al. (2015); Cao et al. (2018)or canned responses Keizer et al. (2017); Traum et al. (2008).Recently, end-to-end neural models Lewis et al. (2017); He et al. (2017) are used tosimultaneously learn dialogue strategy and language realizationfrom human-human dialogues,following the trend of using neural network models on bothgoal-oriented dialogue Wen et al. (2017a); Dhingra et al. (2017)and open-domain dialogue Sordoni et al. (2015); Li et al. (2017); Lowe et al. (2017).However, these models have two problems:(i) it is hard to control and interpret the strategies,and (ii) directly optimizing the agent’s goal through reinforcement learningoften leads to degenerate solutions
3. [3]:  Passage ID 3: judged as helpful by English languages learners and have also had a demonstrable effect on actual learning outcomes Liang et al. (2023).Prior work on developing systems for negotiation training has been fairly limited. These systems can only be interacted with by selecting from a list of pre-written options and deliver “canned” responses as a reply. Despite these limitations, prior work has shown that interacting with virtual agents can improve learners’ understanding of negotiations Gratch et al. (2016).2.2 NegotiationMethods that have applied LLMs to the area of negotiation have been focused on building negotiation chat agents. These methods typically use existing negotiation data to perform supervised learning or offline reinforcement learning on a negotiation model Lewis et al. (2017); He et al. (2018); Verma et al. (2022); Zhan et al. (2024). More recent work has focused on examining and enhancing the negotiation capabilities of prompt-based negotiation agents Schneider
4. [4]:  Passage ID 4: negotiation skills. It allows agents to leverage parameterized DQN to learn a comprehensive negotiation strategy that integrates linguistic communication skills and bidding strategies.3.4.2 Supervised LearningSupervised learning (SL) is another popular paradigm for policy learning. Lewis et al. (2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data. However, supervised learning only aims to mimic the average human behavior, so He et al. (2018) propose to apply a supervised model to directly optimize a particular dialogue reward function, which is characterized by i) the utility function of the final price for the buyer and seller ii) the differences between two agents’ utilities iii) the number of utterances in the dialogue. Zhou et al. (2020) first train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training. Then, the response generation
5. [5]:  Passage ID 5: question answering systems (Gaoet al., 2018). Task-oriented dialogue systems are designed to complete a specific task on the user’s behalf such as booking hotels, making a restaurant reservation or finding products. The second category mainly focuses on carrying out a conversation with the user on open-domain topics, and question answering bots are designed to find an appropriate answer to user’s query using all its available knowledge and resources. Though, these systems have come a long way in terms of progress but conversing with such models for even a short amount of time quickly unveils the inconsistency in generated responses.A different number of strategies have been introduced over a period to address this issue. One of the standard methods of designing an NLP based project is to utilize word embeddings, pre-trained on a huge amount of unlabelled data using distributed word representations such as GloVe and Word2Vec, to initialize the first layer of the neural network. The