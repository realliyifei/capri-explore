# Let's Negotiate! A Survey of Negotiation Dialogue Systems

## Question

How is supervised learning utilized in negotiation dialogue systems to predict strategies and generate responses?

## URLs

1. https://ar5iv.org/html/2212.09072. [2212.09072] Let’s Negotiate! A Survey of Negotiation Dialogue Systems
2. https://ar5iv.org/html/2402.01097. [2402.01097] Let’s Negotiate! A Survey of Negotiation Dialogue Systems
3. https://ar5iv.org/html/2305.13626. [2305.13626] Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration
4. https://ar5iv.org/html/1808.09637. [1808.09637] Decoupling Strategy and Generation in Negotiation Dialogues
5. https://ar5iv.org/html/2410.01555. [2410.01555] ACE: A LLM-based Negotiation Coaching System
6. https://ar5iv.org/html/2402.13550. [2402.13550] Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues
7. https://ar5iv.org/html/2311.00262. [2311.00262] Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents

## Answer (I don't know?: ✗)

Supervised learning (SL) is utilized in negotiation dialogue systems as a method to learn and predict negotiation strategies as well as to generate appropriate responses based on user utterances and dialogue context. This methodology involves training models on existing negotiation data, which can include both human-human dialogues and historical negotiation records. The system aims to maximize the likelihood of actions that should be taken during negotiations, essentially trying to mimic the average behavior observed in the training data [1][4].

One of the primary uses of supervised learning in this context is the training of strategy predictors. For example, Zhou et al. (2020) develop a strategy predictor specifically designed to identify whether a certain negotiation strategy will occur in the next utterance. This prediction is essential for generating system responses that are coherent and aligned with the user's input and the dialogue's context [1][4]. Once the strategies are predicted, responses can be constructed to reflect these strategies, allowing for a more nuanced and effective negotiation interaction.

Additionally, the work by He et al. (2018) emphasizes that supervised learning can be further enhanced by optimizing a specific dialogue reward function. This reward function is characterized by factors such as the utility functions for both buyers and sellers, the differences in utility between negotiating agents, and the number of utterances exchanged [1][4]. This approach enhances the agent’s responsiveness and adaptability to different negotiation scenarios, making exchanges feel more natural and less scripted.

However, it is noted that SL alone has limitations, as it may not adequately control or interpret the negotiation strategies, often leading to degenerate solutions when directly optimizing the agent's goals through reinforcement learning [2][4]. This suggests that while SL provides a foundational framework for strategy prediction and response generation, additional methods may be required to address these challenges adequately, such as incorporating reinforcement learning techniques or fine-tuning based on specific user interactions [2][3].

In summary, supervised learning in negotiation dialogue systems serves as a crucial approach for predicting strategies and generating responses, relying on historical data to guide behavior, while also seeking to optimize the interaction through defined utility outcomes. This dual focus on prediction and response has implications for improving how virtual agents perform in negotiation settings, making them more effective and user-friendly [3][4].

1. [1]:  https://ar5iv.org/html/2212.09072, [2212.09072] Let’s Negotiate! A Survey of Negotiation Dialogue Systems
2. [2]:  https://ar5iv.org/html/1808.09637, [1808.09637] Decoupling Strategy and Generation in Negotiation Dialogues
3. [3]:  https://ar5iv.org/html/2410.01555, [2410.01555] ACE: A LLM-based Negotiation Coaching System
4. [4]:  https://ar5iv.org/html/2402.01097, [2402.01097] Let’s Negotiate! A Survey of Negotiation Dialogue Systems
5. [5]:  https://ar5iv.org/html/2311.00262, [2311.00262] Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents
---
1. [1]:  Passage ID 1: of the training data. However, supervised learning only aims to mimic the average human behavior, so He et al. (2018) propose to finetune the supervised model to directly optimize for a particular dialogue reward function, which is defined as i) the utility function of the final price for the buyer and seller ii) the difference between two agents’ utilities iii) the number of utterances in the dialogue. Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training. The system response would be generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy. In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm. Recently, Dutt et al. (2021b) propose a generalisedframework for identifying resisting strategies in persuasive negotiations using a pre-trained
2. [2]:  Passage ID 2: of that strategy via generation of natural language (e.g.,“I really need a car so I can go to work, but all I have is 6000, any more and I won’t be able to feed my children.”).Most past work in NLP on negotiation focuses on strategy (dialogue management)with either no natural language Cuayáhuitl et al. (2015); Cao et al. (2018)or canned responses Keizer et al. (2017); Traum et al. (2008).Recently, end-to-end neural models Lewis et al. (2017); He et al. (2017) are used tosimultaneously learn dialogue strategy and language realizationfrom human-human dialogues,following the trend of using neural network models on bothgoal-oriented dialogue Wen et al. (2017a); Dhingra et al. (2017)and open-domain dialogue Sordoni et al. (2015); Li et al. (2017); Lowe et al. (2017).However, these models have two problems:(i) it is hard to control and interpret the strategies,and (ii) directly optimizing the agent’s goal through reinforcement learningoften leads to degenerate solutions
3. [3]:  Passage ID 3: judged as helpful by English languages learners and have also had a demonstrable effect on actual learning outcomes Liang et al. (2023).Prior work on developing systems for negotiation training has been fairly limited. These systems can only be interacted with by selecting from a list of pre-written options and deliver “canned” responses as a reply. Despite these limitations, prior work has shown that interacting with virtual agents can improve learners’ understanding of negotiations Gratch et al. (2016).2.2 NegotiationMethods that have applied LLMs to the area of negotiation have been focused on building negotiation chat agents. These methods typically use existing negotiation data to perform supervised learning or offline reinforcement learning on a negotiation model Lewis et al. (2017); He et al. (2018); Verma et al. (2022); Zhan et al. (2024). More recent work has focused on examining and enhancing the negotiation capabilities of prompt-based negotiation agents Schneider
4. [4]:  Passage ID 4: negotiation skills. It allows agents to leverage parameterized DQN to learn a comprehensive negotiation strategy that integrates linguistic communication skills and bidding strategies.3.4.2 Supervised LearningSupervised learning (SL) is another popular paradigm for policy learning. Lewis et al. (2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data. However, supervised learning only aims to mimic the average human behavior, so He et al. (2018) propose to apply a supervised model to directly optimize a particular dialogue reward function, which is characterized by i) the utility function of the final price for the buyer and seller ii) the differences between two agents’ utilities iii) the number of utterances in the dialogue. Zhou et al. (2020) first train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training. Then, the response generation
5. [5]:  Passage ID 5: following their original designs, we adapt these systems into the applications studied in our experiments.Ask-an-Expert (Zhang et al., 2023a)  This method is originally proposed for emotional support dialogues, whose prompt is directly adopted in our experiments.In order to further accommodate the other two applications, we simply change the role in the pre-defined questions for asking the expert LLM, as shown in Table 19.Negotiation DialoguesSystemAssume you are the bargain expert to reach a better deal with the seller. Given the conversation history, answer the question. Please answer with only one short and succinct sentence.UserThe following is the conversation history: [conversation]Question: How did the seller feel? Answer:Assistant[answer1]UserThe following is the conversation history: [conversation]Question: Why did the seller feel that way? Answer:Assistant[answer2]UserThe following is the conversation