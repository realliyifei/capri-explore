# Which *BERT? A Survey Organizing Contextualized Encoders

## Question

What are the pros and cons of using input manipulation for probing tasks in contextualized encoders?

## URLs

1. https://ar5iv.org/html/2409.13385
2. https://ar5iv.org/html/2005.05683
3. https://ar5iv.org/html/2010.00854
4. https://ar5iv.org/html/2406.00053
5. https://ar5iv.org/html/2304.00740
6. https://ar5iv.org/html/2110.07382
7. https://ar5iv.org/html/2306.08193
8. https://ar5iv.org/html/2411.06284
9. https://ar5iv.org/html/2004.14513
10. https://ar5iv.org/html/2310.17271

## Answer

## I don't know?: True

I don't know. The provided context does not specifically address the pros and cons of using input manipulation for probing tasks in contextualized encoders. While it does mention some aspects of NLP tasks, input manipulation, and contextual encoding through various styles, the specific advantages and disadvantages you are asking about are not covered in the context. For a comprehensive understanding of this topic, further details and discussions on input manipulation techniques, their efficacy, and comparative analyses would be necessary.

[1]: https://ar5iv.org/html/2409.13385, [2409.13385] Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey
[2]: https://ar5iv.org/html/2401.05561, No Title
[3]: https://ar5iv.org/html/2403.15453, No Title
[4]: https://ar5iv.org/html/2410.00427, No Title
[5]: https://ar5iv.org/html/2306.05036, No Title

[1]: Passage ID 1: up conscious reasoning for more complex tasks 222procedural learning vs. declarative learning - https://en.wikipedia.org/wiki/Procedural_knowledge. This process is essential for building skills and knowledge, enabling us to tackle increasingly intricate challenges.Researchers in NLP Askell et al. (2021), Snell et al. (2022) are exploring techniques to fine-tune language models, such as context distillation and "Gisting". Context distillation involves generating "practice" questions, having the model reason step-by-step, and fine-tuning it to predict answers from simpler prompts. This helps the model internalize skills, like step-by-step addition (ref Figure 2). "Gisting" Mu et al. (2024) compresses instructions into concise key-value attention prefixes, saving computational resources and generalizing well to new tasks. As depicted in Figure 3, the approach involves learning a gist model by incorporating gist tokens during instruction tuning, enabling the model to handle prompt
[2]: Passage ID 2: trigger LLMs to use external tools to solve the problem. The users’ inputs (tasks) are mostly beyond the LLM’s ability. We randomly extracted 520 samples and had two human experts filter the samples, retaining only the prompts that LLMs cannot answer. These prompts include requests for real-time knowledge (e.g., retrieve the latest news), user interaction requests (e.g., play a game), non-text modality requests (e.g., process image input), and other requests that LLMs cannot answer. Since these requests are related to specific tools (as the queries are generated based on certain tools’ descriptions) and do not have a good generality, we manually wrote 40 general questions that are out of LLMs’ ability (e.g., What time is it now?) and added them to the dataset. Here are some data examples in our final dataset:•Can you find out the most recent tweet from Elon Musk on space travel?•I want to repurpose this written interview into an audio format. Can this AI help with
[3]: Passage ID 3: and definitions of the labels in their context, even on unseen classes or in unknown languages. In contrast, we often find that AI systems on few-shot and zero-shot scenarios still perform much worse than humans [23, 77]. This gap in performance is due to the AI system’s inability to reason about the relationships between the context of the input and the context of the class label. Ongoing work is in this area has aims to properly encode these contexts. For example, previous work in encoding entity descriptions as search query targets has shown some ability to retrieve relevant entity candidates [103, 54]. However, these relationships are nuanced; this research gap has not been fully explored and a wide gap remains.Most modern information extraction systems are built atop token-based pre-trained models. They work by tying together token features to identify spans. However, as pre-trained LLMs like GPT-4 and beyond grow ever stronger, they also become less accessible for specific use
[4]: Passage ID 4: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
[5]: Passage ID 5: an issue in current state-of-the-art LLMs, if they are used correctly. Correct usage, in our knowledge-intensive task of mining insights from text, consists of giving the model a context to work with (as opposed to giving the LLM just a question in the task of knowledge extraction). Given this context, we found OpenAI’s LLMs to be perfectly capable of answering questions on this context, without hallucinations.Further, “prompt engineering” can be used to simply instruct the LLM to state if an answer is not known (instead of hallucinating an answer).However, a limiting factor of autoregressive language models is that they cannot come up with novel ideas or solutions. They are well capable of finding answers to a given question by drawing on their pretrained knowledge. But the result is limited by the extent of the training data.As for the application of LLMs in the context of deep qualitative analysis, a qualitative researcher may argue that depth requires time and investment in