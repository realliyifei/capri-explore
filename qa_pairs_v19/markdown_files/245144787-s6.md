# Measure and Improve Robustness in NLP Models: A Survey

## Question

How do adversarial attacks in computer vision and natural language processing differ in terms of perceptibility and the alteration of sentence meaning?

## URLs

1. https://ar5iv.org/html/2203.06414
2. https://ar5iv.org/html/2208.10251
3. https://ar5iv.org/html/2311.11861
4. https://ar5iv.org/html/1901.06796
5. https://ar5iv.org/html/2409.16491
6. https://ar5iv.org/html/2110.13980
7. https://ar5iv.org/html/2307.15043
8. https://ar5iv.org/html/1903.06620
9. https://ar5iv.org/html/2303.01234
10. https://ar5iv.org/html/2407.21073

## Answer

## I don't know?: False

Adversarial attacks in computer vision (CV) and natural language processing (NLP) exhibit significant differences in terms of perceptibility and the alteration of sentence meaning due to the inherent characteristics of the data types involved. 

In computer vision, adversarial attacks often involve making small, imperceptible perturbations to images that can successfully deceive models while remaining unnoticed by human observers. This property is well established, as illustrated by Szegedy et al. (2013), who demonstrated that minimal changes to an image can lead to misclassifications without altering the human perception of the image itself [1]. Consequently, the CV community has actively studied these attacks due to their effectiveness and stealth, leading to further developments in adaptive attacks and defenses [2].

Conversely, in the domain of NLP, adversarial attacks do not follow the same paradigm. Because language is discrete, even minor alterations to text can be readily perceptible to human readers. This discrepancy means that NLP adversarial attacks typically do not aim for imperceptibility but rather focus on altering the text in a way that the overall meaning of the sentences remains intact [1]. The aim is often to distort the input minimally so that the essence of the sentence is preserved while still misleading the NLP model.

For instance, the attacks in NLP are designed to manipulate the text at various levels—character-level, word-level, and sentence-level—while ensuring that the modified text is still semantically equivalent to the original. This is opposed to the CV attacks where the images can be modified almost imperceptibly. Techniques like character-level attacks, which involve the insertion, deletion, or swapping of characters, are effective but can be easily detected and do not maintain the same level of stealth seen in image-based attacks [3]. Moreover, even though NLP attacks consider perceptibility, they often emphasize semantic consistency; hence, the changes made to the text are bound by the limitation of human understanding [1][4].

The primary challenge in NLP is that the discrete nature of text complicates the transfer of techniques from the CV domain. While CV can leverage small changes for effective deception, in NLP, one must contend with the readability and interpretability of text, as meaningful alterations can compromise the sentence comprehensibility. Thus, unlike CV, where stealthiness often governs attack efficacy, NLP adversarial attacks must strike a balance between exploiting model vulnerabilities while preserving semantic integrity and human interpretability [2][4][5].

In summary, adversarial attacks in CV are characterized by imperceptibility to humans and do not alter the underlying information conveyed by the input images, whereas NLP adversarial attacks must remain perceptible yet preserve the meaning of the text, making their execution inherently more complex [1][2].

[1]: https://ar5iv.org/html/2112.08313, No Title
[2]: https://ar5iv.org/html/2208.10251, [2208.10251] Rethinking Textual Adversarial Defense for Pre-trained Language Models
[3]: https://ar5iv.org/html/2203.06414, [2203.06414] A Survey of Adversarial Defences and Robustness in NLP
[4]: https://ar5iv.org/html/2311.11861, [2311.11861] Generating Valid and Natural Adversarial Examples with Large Language Models
[5]: https://ar5iv.org/html/2203.06414, [2203.06414] A Survey of Adversarial Defences and Robustness in NLP

[1]: Passage ID 1: and defense regime when the study in vision is transferred to NLP Lei et al. (2019); Zhang et al. (2020c),in the sense that simple gradient-based adversarial attacks will not directly translate to meaningful attacks in the discrete text space,and multiple novel attack methods are proposed to fill the gap, as we will discuss in later sections.Perceptible to Human vs. NotOn a related topic, one of the most impressive property of adversarial attack in vision is thatsmall perturbation of the image data imperceptible to human are sufficient to deceive the model Szegedy et al. (2013),while this can hardly be true for NLP attacks.Instead of being imperceptible, the adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences are not altered (despite being perceptible).On the other hand,there are ways to generate samples where the changes,although being perceptible,are often ignored by human brain due to some psychological prior on how a human
[2]: Passage ID 2: vision (CV) community and received considerable attention in the last five years. The properties of image adversarial examples have been well explored and analyzed. The fact that such attacks can be detected [11] and be invalidated by randomization [12, 13] has triggered many valuable following studies [14], including designs of stronger adaptive attacks [15] and better understanding of properties of adversarial examples [16]. However, compared to their counterparts in computer vision, adversarial examples in the natural language processing (NLP) domain are very different due to the discrete nature of languages, and properties of textual adversarial examples have not been well studied. Thus, in this work, comprehensive experiments are conducted to explore whether textual adversarial examples possess similar properties compared to their counterparts in computer vision. For the targeted tasks, we focus on text classification and natural language inference, which are the most important
[3]: Passage ID 3: designed for NLP tasks. It is important to note that adversarial examples in computer vision cannot be directly applied to text as they are fundamentally different. Therefore, several attack methods that modify the text data while maintaining imperceptibility to humans have been proposed in literature. Typically, these methods alter the text data at the word, character, or sentence levels. The following section presents some of these attack methods in NLP.Character level adversarial attacksCharacter-level attacks perturb the input sequences at a character level. These operations include insertion, deletion, and swapping of characters in a given input sequence. Despite the fact, these attacks are quite effective, they can easily be detected with a spell-checker mechanism. One of the techniques used in character-level attacks is adding natural and synthetic noise to the inputs (Belinkov and Bisk, 2018). For natural noise authors collected natural spelling mistakes and used them to
[4]: Passage ID 4: Related WorkAdversarial attacks have been extensively studied over the past years with success in computer vision and speech domains, considering that deep neural networks (DNNs) are vulnerable to adversarial attacks. However, due to the discrete nature of natural languages, it is still far from ideal to perform successful adversarial attacks on NLP models [6].Previous research on textual adversarial attacks can be divided onto four levels: character-level, word-level, sentence-level, and multi-level adversarial attacks. Some off-the-shelf toolkits have also been developed to generate adversarial examples against real-world models [20, 21]. Different from the character-level adversarial attack that manipulates the characters [22, 23] and the sentence-level adversarial attack that creates new sentences with generative adversarial network (GAN) [24], semantically equivalent rules [25], and paraphrase networks [26], word-level adversarial attack has been the most popular technique
[5]: Passage ID 5: named entity recognition, and natural language inference. Some of these methods not only defend neural networks against adversarial attacks but also act as a regularization mechanism during training, saving the model from overfitting. This survey aims to review the various methods proposed for adversarial defenses in NLP over the past few years by introducing a novel taxonomy. The survey also highlights the fragility of advanced deep neural networks in NLP and the challenges involved in defending them.Adversarial attacks, Adversarial defenses, Perturbations, NLP††ccs: Computer systems organization Embedded systems††ccs: Computer systems organization Redundancy††ccs: Computer systems organization Robotics††ccs: Networks Network reliability1. IntroductionRecently, there have been significant advancements in the field of Natural Lanaguage Processing (NLP) using deep learning algorithms. In fact, the proposed solutions for NLP have already surpassed human accuracy in some cases