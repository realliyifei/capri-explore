# Measure and Improve Robustness in NLP Models: A Survey

## Question

How do adversarial attacks in computer vision and natural language processing differ in terms of perceptibility and the alteration of sentence meaning?

## URLs

1. https://ar5iv.org/html/2203.06414. [2203.06414] A Survey of Adversarial Defences and Robustness in NLP
2. https://ar5iv.org/html/2208.10251. [2208.10251] Rethinking Textual Adversarial Defense for Pre-trained Language Models
3. https://ar5iv.org/html/2311.11861. [2311.11861] Generating Valid and Natural Adversarial Examples with Large Language Models
4. https://ar5iv.org/html/1901.06796. [1901.06796] Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey
5. https://ar5iv.org/html/2409.16491. [2409.16491] Proactive Schemes: A Survey of Adversarial Attacks for Social Good
6. https://ar5iv.org/html/2110.13980. [2110.13980] Adversarial Attacks and Defenses for Social Network Text Processing Applications: Techniques, Challenges and Future Research Directions
7. https://ar5iv.org/html/2307.15043. [2307.15043] Universal and Transferable Adversarial Attacks on Aligned Language Models
8. https://ar5iv.org/html/1903.06620. [1903.06620] On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models
9. https://ar5iv.org/html/2303.01234. [2303.01234] Fraud’s Bargain Attack: Generating Adversarial Text Samples via Word Manipulation Process
10. https://ar5iv.org/html/2407.21073. [2407.21073] Enhancing Adversarial Text Attacks on BERT Models with Projected Gradient Descent

## Answer (I don't know?: ✗)

Adversarial attacks in computer vision (CV) and natural language processing (NLP) differ significantly in terms of perceptibility and the alteration of meaning. In CV, adversarial attacks often utilize small perturbations to images that are imperceptible to humans, meaning that these modifications do not change the visual interpretation or perception of the image, yet they can effectively deceive the model [1] [2]. The ability to create imperceptible changes in images allows for the potential implementation of such attacks without the risk of being noticed, making them highly effective.

In contrast, in the NLP domain, adversarial attacks are fundamentally limited by the requirement that the meaning of the sentences must remain unchanged. This means that while modifications made to a text can be perceptible (i.e., noticeable to a human reader), they must not alter the underlying semantics of the sentences [1]. As stated, "adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences are not altered (despite being perceptible)" [1]. Thus, NLP adversarial attacks do not have the same leeway as those in CV to make imperceptible changes; any adjustments that could trick the model would likely be noticed by a human reader.

Furthermore, while various methods in NLP aim to modify text data at different levels (character, word, or sentence), these methods are often more susceptible to detection due to their visible nature [3]. For instance, character-level attacks, which involve actions such as insertion, deletion, or swapping of characters, may be effective but can be easily recognized through spell-check mechanisms [3]. The discrete nature of language makes it difficult to replicate the effectiveness found in computer vision, where the visual changes can be subtle and innocuous [4].

The exploration of adversarial examples in NLP is still in its infancy compared to CV. While the CV community has extensively analyzed various properties and addressed vulnerabilities, the understanding of textual adversarial examples remains less developed [2]. This inherent complexity gives rise to the notion that the strategies employed in CV cannot be directly treated as applicable in NLP due to these fundamental differences in perceptibility and semantic meaning [2] [4].

In summary, the key differentiation lies in the nature of the attacks: CV allows for imperceptible manipulations, while NLP attacks typically involve perceptible changes that do not alter meanings, reflecting the distinct challenges posed by the characteristics of each domain.

1. [1]:  https://ar5iv.org/html/2112.08313, No Title
2. [2]:  https://ar5iv.org/html/2208.10251, [2208.10251] Rethinking Textual Adversarial Defense for Pre-trained Language Models
3. [3]:  https://ar5iv.org/html/2203.06414, [2203.06414] A Survey of Adversarial Defences and Robustness in NLP
4. [4]:  https://ar5iv.org/html/2311.11861, [2311.11861] Generating Valid and Natural Adversarial Examples with Large Language Models
5. [5]:  https://ar5iv.org/html/2203.06414, [2203.06414] A Survey of Adversarial Defences and Robustness in NLP
---
1. [1]:  Passage ID 1: and defense regime when the study in vision is transferred to NLP Lei et al. (2019); Zhang et al. (2020c),in the sense that simple gradient-based adversarial attacks will not directly translate to meaningful attacks in the discrete text space,and multiple novel attack methods are proposed to fill the gap, as we will discuss in later sections.Perceptible to Human vs. NotOn a related topic, one of the most impressive property of adversarial attack in vision is thatsmall perturbation of the image data imperceptible to human are sufficient to deceive the model Szegedy et al. (2013),while this can hardly be true for NLP attacks.Instead of being imperceptible, the adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences are not altered (despite being perceptible).On the other hand,there are ways to generate samples where the changes,although being perceptible,are often ignored by human brain due to some psychological prior on how a human
2. [2]:  Passage ID 2: vision (CV) community and received considerable attention in the last five years. The properties of image adversarial examples have been well explored and analyzed. The fact that such attacks can be detected [11] and be invalidated by randomization [12, 13] has triggered many valuable following studies [14], including designs of stronger adaptive attacks [15] and better understanding of properties of adversarial examples [16]. However, compared to their counterparts in computer vision, adversarial examples in the natural language processing (NLP) domain are very different due to the discrete nature of languages, and properties of textual adversarial examples have not been well studied. Thus, in this work, comprehensive experiments are conducted to explore whether textual adversarial examples possess similar properties compared to their counterparts in computer vision. For the targeted tasks, we focus on text classification and natural language inference, which are the most important
3. [3]:  Passage ID 3: designed for NLP tasks. It is important to note that adversarial examples in computer vision cannot be directly applied to text as they are fundamentally different. Therefore, several attack methods that modify the text data while maintaining imperceptibility to humans have been proposed in literature. Typically, these methods alter the text data at the word, character, or sentence levels. The following section presents some of these attack methods in NLP.Character level adversarial attacksCharacter-level attacks perturb the input sequences at a character level. These operations include insertion, deletion, and swapping of characters in a given input sequence. Despite the fact, these attacks are quite effective, they can easily be detected with a spell-checker mechanism. One of the techniques used in character-level attacks is adding natural and synthetic noise to the inputs (Belinkov and Bisk, 2018). For natural noise authors collected natural spelling mistakes and used them to
4. [4]:  Passage ID 4: Related WorkAdversarial attacks have been extensively studied over the past years with success in computer vision and speech domains, considering that deep neural networks (DNNs) are vulnerable to adversarial attacks. However, due to the discrete nature of natural languages, it is still far from ideal to perform successful adversarial attacks on NLP models [6].Previous research on textual adversarial attacks can be divided onto four levels: character-level, word-level, sentence-level, and multi-level adversarial attacks. Some off-the-shelf toolkits have also been developed to generate adversarial examples against real-world models [20, 21]. Different from the character-level adversarial attack that manipulates the characters [22, 23] and the sentence-level adversarial attack that creates new sentences with generative adversarial network (GAN) [24], semantically equivalent rules [25], and paraphrase networks [26], word-level adversarial attack has been the most popular technique
5. [5]:  Passage ID 5: named entity recognition, and natural language inference. Some of these methods not only defend neural networks against adversarial attacks but also act as a regularization mechanism during training, saving the model from overfitting. This survey aims to review the various methods proposed for adversarial defenses in NLP over the past few years by introducing a novel taxonomy. The survey also highlights the fragility of advanced deep neural networks in NLP and the challenges involved in defending them.Adversarial attacks, Adversarial defenses, Perturbations, NLP††ccs: Computer systems organization Embedded systems††ccs: Computer systems organization Redundancy††ccs: Computer systems organization Robotics††ccs: Networks Network reliability1. IntroductionRecently, there have been significant advancements in the field of Natural Lanaguage Processing (NLP) using deep learning algorithms. In fact, the proposed solutions for NLP have already surpassed human accuracy in some cases