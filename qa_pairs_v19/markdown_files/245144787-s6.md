# Measure and Improve Robustness in NLP Models: A Survey

## Question

How do adversarial attacks in computer vision and natural language processing differ in terms of perceptibility and the alteration of sentence meaning?

## URLs

1. https://ar5iv.org/html/2203.06414. [2203.06414] A Survey of Adversarial Defences and Robustness in NLP
2. https://ar5iv.org/html/2208.10251. [2208.10251] Rethinking Textual Adversarial Defense for Pre-trained Language Models
3. https://ar5iv.org/html/2311.11861. [2311.11861] Generating Valid and Natural Adversarial Examples with Large Language Models
4. https://ar5iv.org/html/1901.06796. [1901.06796] Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey
5. https://ar5iv.org/html/2409.16491. [2409.16491] Proactive Schemes: A Survey of Adversarial Attacks for Social Good
6. https://ar5iv.org/html/2110.13980. [2110.13980] Adversarial Attacks and Defenses for Social Network Text Processing Applications: Techniques, Challenges and Future Research Directions
7. https://ar5iv.org/html/2307.15043. [2307.15043] Universal and Transferable Adversarial Attacks on Aligned Language Models
8. https://ar5iv.org/html/1903.06620. [1903.06620] On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models
9. https://ar5iv.org/html/2303.01234. [2303.01234] Fraud’s Bargain Attack: Generating Adversarial Text Samples via Word Manipulation Process
10. https://ar5iv.org/html/2407.21073. [2407.21073] Enhancing Adversarial Text Attacks on BERT Models with Projected Gradient Descent

## Answer (I don't know?: ✗)

Adversarial attacks in computer vision and natural language processing (NLP) differ significantly in terms of perceptibility and the alteration of sentence meaning due to the inherent nature of the data they handle.

Firstly, the perceptibility of adversarial examples differs between the two domains. In computer vision, adversarial attacks often involve making minor pixel-level perturbations to images that are designed to be imperceptible to human observers. These modifications can include subtle changes like adding noise or altering pixel values, which typically go unnoticed [1] [5]. However, in the case of NLP, creating similar imperceptible adversarial examples is much more challenging. Any alteration—such as changing a character, word, or sentence—can easily become perceptible, potentially affecting the naturalness and readability of the input text. For instance, character-level modifications like insertion or deletion can trigger responses from spell-checkers, making them detectable by automated systems [2] [3]. The alteration of textual data tends to be more overt; thus, maintaining the subtlety required for effective adversarial manipulation is significantly harder in NLP compared to computer vision.

Secondly, the impact on sentence meaning is another critical distinction. In computer vision, adversarial examples may slightly change an image without noticeably altering the object recognition outcome. An image can be altered in ways that remain contextually meaningful, leading to the same classification despite the presence of adversarial noise [1] [5]. Conversely, in NLP, even minor modifications can dramatically change the meaning of sentences, or render them nonsensical. For instance, swapping words or altering sentence structure can produce a completely different interpretation, thereby trailing off the intended meaning of the text. This sensitivity to meaning is exacerbated by the discrete nature of language, which necessitates that each word or phrase contribute cohesively to the overall message [3] [4]. 

Moreover, while multiple levels of attacks exist in NLP—character-level, word-level, and sentence-level—the most common techniques often manipulate language in ways that may lead to severe deviations from the intended meaning, thus making it harder to design efficient attacks without losing semantic integrity [3] [4]. 

In summary, adversarial attacks in computer vision can effectively create imperceptible perturbations while maintaining semantic coherence, whereas NLP adversarial attacks struggle with perceptibility and often lead to significant changes in meaning due to the discrete and sensitive nature of language [2] [5]. This highlights a fundamental challenge in developing effective adversarial methods within the NLP field, necessitating more innovative approaches that address these issues.

1. [1]:  https://ar5iv.org/html/2208.10251, [2208.10251] Rethinking Textual Adversarial Defense for Pre-trained Language Models
2. [2]:  https://ar5iv.org/html/2203.06414, [2203.06414] A Survey of Adversarial Defences and Robustness in NLP
3. [3]:  https://ar5iv.org/html/2311.11861, [2311.11861] Generating Valid and Natural Adversarial Examples with Large Language Models
4. [4]:  https://ar5iv.org/html/2203.06414, [2203.06414] A Survey of Adversarial Defences and Robustness in NLP
5. [5]:  https://ar5iv.org/html/2203.06414, [2203.06414] A Survey of Adversarial Defences and Robustness in NLP
---
1. [1]:  Passage ID 1: vision (CV) community and received considerable attention in the last five years. The properties of image adversarial examples have been well explored and analyzed. The fact that such attacks can be detected [11] and be invalidated by randomization [12, 13] has triggered many valuable following studies [14], including designs of stronger adaptive attacks [15] and better understanding of properties of adversarial examples [16]. However, compared to their counterparts in computer vision, adversarial examples in the natural language processing (NLP) domain are very different due to the discrete nature of languages, and properties of textual adversarial examples have not been well studied. Thus, in this work, comprehensive experiments are conducted to explore whether textual adversarial examples possess similar properties compared to their counterparts in computer vision. For the targeted tasks, we focus on text classification and natural language inference, which are the most important
2. [2]:  Passage ID 2: designed for NLP tasks. It is important to note that adversarial examples in computer vision cannot be directly applied to text as they are fundamentally different. Therefore, several attack methods that modify the text data while maintaining imperceptibility to humans have been proposed in literature. Typically, these methods alter the text data at the word, character, or sentence levels. The following section presents some of these attack methods in NLP.Character level adversarial attacksCharacter-level attacks perturb the input sequences at a character level. These operations include insertion, deletion, and swapping of characters in a given input sequence. Despite the fact, these attacks are quite effective, they can easily be detected with a spell-checker mechanism. One of the techniques used in character-level attacks is adding natural and synthetic noise to the inputs (Belinkov and Bisk, 2018). For natural noise authors collected natural spelling mistakes and used them to
3. [3]:  Passage ID 3: Related WorkAdversarial attacks have been extensively studied over the past years with success in computer vision and speech domains, considering that deep neural networks (DNNs) are vulnerable to adversarial attacks. However, due to the discrete nature of natural languages, it is still far from ideal to perform successful adversarial attacks on NLP models [6].Previous research on textual adversarial attacks can be divided onto four levels: character-level, word-level, sentence-level, and multi-level adversarial attacks. Some off-the-shelf toolkits have also been developed to generate adversarial examples against real-world models [20, 21]. Different from the character-level adversarial attack that manipulates the characters [22, 23] and the sentence-level adversarial attack that creates new sentences with generative adversarial network (GAN) [24], semantically equivalent rules [25], and paraphrase networks [26], word-level adversarial attack has been the most popular technique
4. [4]:  Passage ID 4: named entity recognition, and natural language inference. Some of these methods not only defend neural networks against adversarial attacks but also act as a regularization mechanism during training, saving the model from overfitting. This survey aims to review the various methods proposed for adversarial defenses in NLP over the past few years by introducing a novel taxonomy. The survey also highlights the fragility of advanced deep neural networks in NLP and the challenges involved in defending them.Adversarial attacks, Adversarial defenses, Perturbations, NLP††ccs: Computer systems organization Embedded systems††ccs: Computer systems organization Redundancy††ccs: Computer systems organization Robotics††ccs: Networks Network reliability1. IntroductionRecently, there have been significant advancements in the field of Natural Lanaguage Processing (NLP) using deep learning algorithms. In fact, the proposed solutions for NLP have already surpassed human accuracy in some cases
5. [5]:  Passage ID 5: the use of patches, the addition of watermarks, and so on, which can go unnoticed by humans. In contrast, adversarial attacks in NLP involve multiple proposed perturbations at the character, word, or sentence level through deletion, insertion, swapping, flipping, use of synonyms, concatenation with characters or words, insertion of numeric or alphanumeric characters, etc. However, it is more challenging to generate adversarial perturbations for text data than image data because altering a character or word in a sentence is more perceptible to humans. Moreover, creating imperceptible adversarial attacks is difficult in NLP since perturbations in textual data could result in less natural input data (Liet al., 2023). Adversarial attacks are classified into two categories based on motivation: targeted attacks and non-targeted attacks. Targeted attacks aim to misclassify inputs to a specific class, while non-targeted attacks aim to push the classifier boundary to cause the model to