# Diachronic word embeddings and semantic shifts: a survey

## Question

How do diachronic word embeddings address and solve temporal analogies across different time periods?

## URLs

1. https://ar5iv.org/html/2408.16209
2. https://ar5iv.org/html/1906.02376
3. https://ar5iv.org/html/2409.10649
4. https://ar5iv.org/html/1806.03537
5. https://ar5iv.org/html/1912.01072
6. https://ar5iv.org/html/2111.03320
7. https://ar5iv.org/html/1909.09907

## Answer

## I don't know?: False

Diachronic word embeddings effectively address and solve temporal analogies by utilizing models trained on linguistic data from different time periods, which allows them to align word representations over time. This process is framed within the temporal word analogy problem, extending traditional word analogies into a temporal context. Specifically, the goal is to identify relationships of the form 'word \(w_1\) at time period \(t_\alpha\) is like word \(w_2\) at time period \(t_\beta\)' [1].

To implement this, researchers establish a methodology where embedding models are trained on distinct time intervals. These models are then aligned through linear transformations, which provide a mathematical means to compare word vectors across different temporal contexts. Once aligned, the problem of temporal analogy reduces to finding which word vector in the designated time \(t_\beta\) is closest to the representation of \(w_1\) from time \(t_\alpha\) [1]. This alignment enables the identification of shifts in word meanings and relationships that occur over time, allowing researchers to explore how the relatedness of concepts evolves.

Moreover, diachronic word embeddings allow for the investigation of semantic changes and lexical replacements by tracing how particular words have been represented from 1800 to 2000, for example. This methodology offers insights beyond traditional methods (e.g., semasiological approaches), providing a clearer picture of societal changes reflected through language [3]. The effectiveness of this technique can be demonstrated through specific findings about word relationships across different epochs, such as discovering that the term "cart" in the 1800s holds a similar embedding to "truck" in contemporary contexts [3].

The emergent field is notably heterogenous, with intersections among various disciplines‚Äîsuch as natural language processing, information retrieval, and political science‚Äîleading to diverse terminologies, including 'temporal embeddings,' 'diachronic embeddings,' and 'dynamic embeddings' [2]. Despite this diversity, the commonality lies in the fundamental goal of modeling semantic shifts and enhancing the efficiency of user queries related to temporal word relationships.

Overall, the alignment of diachronic word embeddings not only aids in identifying historical semantic trends but also facilitates a deeper analysis of language evolution, thereby enriching our understanding of linguistics and social history [2] [3].

1. [1]:  https://ar5iv.org/html/1806.03537, [1806.03537] Diachronic word embeddings and semantic shifts: a survey
2. [2]:  https://ar5iv.org/html/1806.03537, [1806.03537] Diachronic word embeddings and semantic shifts: a survey
3. [3]:  https://ar5iv.org/html/2408.16209, [2408.16209] From cart to truck: meaning shift through words in English in the last two centuries
4. [4]:  https://ar5iv.org/html/1906.02376, [1906.02376] Training Temporal Word Embeddings with a Compass
5. [5]:  https://ar5iv.org/html/1906.02376, [1906.02376] Training Temporal Word Embeddings with a Compass
---
1. [1]:  Passage ID 1: retrieval from document collections with significant time spans. ?) frames this as the temporal word analogy problem, extending the word analogies concept into the temporal dimension. This work shows that diachronic word embeddings can successfully model relations like ‚Äòword w1subscriptùë§1w_{1} at time period tŒ±subscriptùë°ùõºt_{\alpha} is like word w2subscriptùë§2w_{2} at time period tŒ≤subscriptùë°ùõΩt_{\beta}‚Äô. To this end, embedding models trained on different time periods are aligned using linear transformations. Then, the temporal analogies are solved by simply finding out which word vector in the time period tŒ≤subscriptùë°ùõΩt_{\beta} is the closest to the vector of w1subscriptùë§1w_{1} in the time period tŒ±subscriptùë°ùõºt_{\alpha}.A variation of this task was studied in ?), where the authors learn the relatedness of words over time,answering queries like ‚Äòin which time period were the words Obama and president maximally related‚Äô. This technique can be used for a more efficient user query
2. [2]:  Passage ID 2: 3 and further below). However, this emerging field is highly heterogenous. There are at least three different research communities interested in it: natural language processing (and computational linguistics), information retrieval (and computer science in general), and political science. This is reflected in the terminology, which is far from being standardized. One can find mentions of ‚Äòtemporal embeddings,‚Äô ‚Äòdiachronic embeddings,‚Äô ‚Äòdynamic embeddings,‚Äô etc., depending on the background of a particular research group.The present survey paper attempts to describe this diversity, introduce some axes of comparison and outline main challenges which the practitioners face. Figure¬†1 shows the timeline of events that influenced the research in this area: in the following sections we cover them in detail.This survey is restricted in scope to research which traces semantic shifts using distributional word embedding models (that is, representing lexical meaning with dense vectors produced
3. [3]:  Passage ID 3: across time.In this study, we will use diachronic word embeddings trained with English from 1800 to 2000 to find interesting shifts in the representation of the same concept through time. This approach can complement the more common semasiological approaches, and give a better image of society from our current knowledge. For example, we found that in the 1800s, cart had the most similar embedding to the modern embedding of truck.The article structure is as follows: In the ‚ÄúDefinitions and Previous Work‚Äù section, we explain key terms and touch on previous research. Following this, we outline our methodology. Next, our findings will be presented, emphasizing specific words within the results section. Subsequently, we provide a summarized account of our findings and challenges in the ‚ÄúConclusion‚Äù section. Afterward, we address the limitations of our work and engage in an ethics discussion related to this type of study. Additionally, a summary of result tables is available in
4. [4]:  Passage ID 4: 2017]Szymanski, T.2017.Temporal word analogies: Identifying lexical replacement withdiachronic word embeddings.In ACL.Association for Computational Linguistics.[Taddy 2015]Taddy, M.2015.Document classification by inversion of distributed languagerepresentations.In IJCNLP, volume¬†2, 45‚Äì49.[Tredici, Nissim, andZaninello 2016]Tredici, M.¬†D.; Nissim, M.; and Zaninello, A.2016.Tracing metaphors in time through self-distance in vector spaces.In CLiC-it.[Yamada et al. 2017]Yamada, I.; Shindo, H.; Takeda, H.; and Takefuji, Y.2017.Learning distributed representations of texts and entities fromknowledge base.Transactions of the Association for Computational Linguistics5:397‚Äì411.[Yao et al. 2018]Yao, Z.; Sun, Y.; Ding, W.; Rao, N.; and Xiong, H.2018.Dynamic word embeddings for evolving semantic discovery.In Proceedings of the Eleventh ACM International Conference onWeb Search and Data Mining,
5. [5]:  Passage ID 5: 2017]Szymanski, T.2017.Temporal word analogies: Identifying lexical replacement withdiachronic word embeddings.In ACL.Association for Computational Linguistics.[Taddy 2015]Taddy, M.2015.Document classification by inversion of distributed languagerepresentations.In IJCNLP, volume¬†2, 45‚Äì49.[Tredici, Nissim, andZaninello 2016]Tredici, M.¬†D.; Nissim, M.; and Zaninello, A.2016.Tracing metaphors in time through self-distance in vector spaces.In CLiC-it.[Yamada et al. 2017]Yamada, I.; Shindo, H.; Takeda, H.; and Takefuji, Y.2017.Learning distributed representations of texts and entities fromknowledge base.Transactions of the Association for Computational Linguistics5:397‚Äì411.[Yao et al. 2018]Yao, Z.; Sun, Y.; Ding, W.; Rao, N.; and Xiong, H.2018.Dynamic word embeddings for evolving semantic discovery.In Proceedings of the Eleventh ACM International Conference onWeb Search and Data Mining,