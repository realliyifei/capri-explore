# A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios

## Question

What are the primary strategies for managing noise in distantly supervised NLP tasks, and how do they operate?

## URLs

1. https://ar5iv.org/html/2311.08010. [2311.08010] Distantly-Supervised Named Entity Recognition with Uncertainty-aware Teacher Learning and Student-student Collaborative Learning
2. https://ar5iv.org/html/2405.20978. [2405.20978] Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training
3. https://ar5iv.org/html/2106.11566. [2106.11566] SENT: Sentence-level Distant Relation Extraction via Negative Training
4. https://ar5iv.org/html/2004.13786. [2004.13786] A Practical Framework for Relation Extraction with Noisy Labels Based on Doubly Transitional Loss
5. https://ar5iv.org/html/2402.14948. [2402.14948] Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach
6. https://ar5iv.org/html/2305.04076. [2305.04076] SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition
7. https://ar5iv.org/html/1705.03995. [1705.03995] Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix
8. https://ar5iv.org/html/2209.06596. [2209.06596] Few Clean Instances Help Denoising Distant Supervision
9. https://ar5iv.org/html/2402.16159. [2402.16159] DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem
10. https://ar5iv.org/html/2105.08213. [2105.08213] Distantly Supervised Relation Extraction via Recursive Hierarchy-Interactive Attention and Entity-Order Perception

## Answer (I don't know?: ‚úó)

In distantly supervised natural language processing (NLP) tasks, managing noise is crucial due to the nature of the data being utilized, where large datasets are often annotated through automated methods relying on existing knowledge bases (KB). The primary strategies for managing noise can be categorized into several approaches.

1. **Dynamic Transition Matrix**: One proposed solution is the design of a dynamic transition matrix structure specifically to characterize the noise introduced by distant supervision (DS). This matrix helps to adaptively adjust the model's understanding of how noise affects the learning process, potentially mitigating its impact on the model's performance [3].

2. **Curriculum Learning**: Developing a curriculum learning framework is another effective strategy. This approach aims to guide the training process adaptively, helping the model learn from noisy data at a manageable pace. By structuring training examples in a way that gradually increases in complexity, the model can better handle the noise present in the data [3].

3. **Instance-Level Selective Attention**: Lin et al. introduced an instance-level selective attention model specifically to improve relation extraction in this noisy context. The model emphasizes the most relevant parts of the input despite the presence of noise, enhancing the model's focus on critical features for accurate extraction, thereby reducing the potential distractions caused by incorrect labels [1].

4. **Use of Focal Loss and k-Nearest Neighbors (KNN)**: The SANTA framework proposes tailored strategies to separately handle two types of noise: inaccurate annotation and incomplete annotation. This includes employing Memory-smoothed Focal Loss to address entity ambiguity and an entity-aware KNN approach to strengthen the handling of overlapping entities during relation extraction, ultimately reducing misclassifications caused by label noise [5].

5. **Boundary Mixup**: Another proposed strategy within SANTA is Boundary Mixup, which seeks to alleviate the decision boundary shifting problems caused by incomplete annotations. This technique helps to improve robustness against noise by generating new training examples that blend information from multiple sources, helping to define clearer boundaries in the classification task [5].

6. **Noise Models**: Various noise models have been applied across different domains, including NLP and computer vision. In NLP, noise models facilitate the understanding of how noise affects learning and help inform model architecture to better cope with these issues. For example, Fang and Cohn (2016) and Luo et al. (2017) explored different noise management techniques that may include probabilistic frameworks to estimate and correct for noise in labels [1].

7. **Addressing Gold Label Dependence**: While distant supervision can enhance Named Entity Recognition (NER) performance in specialized domains, it relies heavily on the quality of the underlying knowledge base. Limitations arise if the knowledge base is flawed, affecting the model's quality of supervision. To mitigate this, strategies must be in place to ensure that knowledge bases used for annotation are accurate and comprehensive [2].

In summary, effective management of noise in distantly supervised NLP tasks involves a combination of advanced model architectures, adaptive learning techniques, and tailored loss functions designed to address specific types of errors in training data. Each of these strategies works together to enhance the robustness of the models employed in NLP tasks.

1. [1]:  https://ar5iv.org/html/2004.13786, [2004.13786] A Practical Framework for Relation Extraction with Noisy Labels Based on Doubly Transitional Loss
2. [2]:  https://ar5iv.org/html/2402.16159, [2402.16159] DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem
3. [3]:  https://ar5iv.org/html/1705.03995, [1705.03995] Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix
4. [4]:  https://ar5iv.org/html/2402.14948, [2402.14948] Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach
5. [5]:  https://ar5iv.org/html/2305.04076, [2305.04076] SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition
---
1. [1]:  Passage ID 1: of the noisy data during the training process,such as (Riedel et¬†al., 2010; Hoffmann et¬†al., 2011; Surdeanu et¬†al., 2012; Zeng et¬†al., 2015).Lin et¬†al. propose an instance-level selective attention model for distantly supervised relation extraction.Wu et¬†al. build a model with neural noise converter and conditional optimal selector to PCNN (a variant of convolutional neural network) for distantly supervised relation extraction.Some other NLP related works with noise model include (Fang and Cohn, 2016) and (Luo et¬†al., 2017).In the domain of computer vision, researchers also propose many robust strategies including bootstrapping mechanism (Reed et¬†al., 2014), linear noise layer (Sukhbaatar et¬†al., 2015) and amortized transition matrix (Misra et¬†al., 2016).In general, dealing with noisy labels is important in machine learningespecially when the training data is large. The proposed methods to relievethe noisy label problems can roughly be classified into three categories (Wu
2. [2]:  Passage ID 2: we showed how distant supervision improves the performance of overall NER models in specialized domains like open source softwares where gold labels are scarce. As a follow up step, we also performed the closely-linked task of relation extraction and showed that the NER pipeline improves the extraction performance. In future, we plan to extend this setup for other open source software ecosystems as well as similar specialised domains.10 LimitationWhile the distant supervision improves NER performance it relies on an existing knowledge base to automatically annotate data. If this knowledge base contains errors or is incomplete, this can directly affect the quality of the supervision and thus the performance of the model. Further distantly supervised models may struggle with multi-token entities (e.g., software names with versions like ‚ÄúUbuntu 20.04 LTS‚Äù). Boundary detection for these entities can be problematic, leading to incomplete or incorrect entity recognition. Finally,
3. [3]:  Passage ID 3: contributions are to (1) design a dynamic transition matrix structure to characterize the noise introduced by DS, and (2) design a curriculum learning based framework to adaptively guide the training procedure to learn with noise.2 Problem DefinitionThe task of distantly supervised relation extraction is to extract knowledge triples, <<subj, rel, obj>>, from free text with the training data constructed by aligning existing KB triples with a large corpus.Specifically, given a triple in KB, DS works by first retrieving all the sentences containing both subj and obj of the triple, and then constructing the training data by considering these sentences as support to the existence of the triple.This task can be conducted in both the sentence and the bag levels.The former takes a sentence sùë†s containing both s‚Äãu‚Äãb‚Äãjùë†ùë¢ùëèùëósubj and o‚Äãb‚Äãjùëúùëèùëóobj as input, and outputs the relation expressedby the sentence between s‚Äãu‚Äãb‚Äãjùë†ùë¢ùëèùëósubj and o‚Äãb‚Äãjùëúùëèùëóobj.The latter settingalleviates the noisy
4. [4]:  Passage ID 4: Proceedings of the workshop on noisy user-generated text, pages 146‚Äì153.Hedderich and Klakow (2018)Michael¬†A Hedderich and Dietrich Klakow. 2018.Training a neural network in a low-resource setting on automatically annotated noisy data.In Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP, pages 12‚Äì18.Huang and Du (2019)Yuyun Huang and Jinhua Du. 2019.Self-attention enhanced CNNs and collaborative curriculum learning for distantly supervised relation extraction.In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 389‚Äì398, Hong Kong, China. Association for Computational Linguistics.Jafarpour et¬†al. (2021)Borna Jafarpour, Dawn Sepehr, and Nick Pogrebnyakov. 2021.Active curriculum learning.In Proceedings of the First Workshop on Interactive Learning for Natural Language Processing, pages 40‚Äì45,
5. [5]:  Passage ID 5: the burden of time-consuming and expensive annotation in the supervised setting.But the context-free matching process and the limited coverage of knowledge bases introduce inaccurate and incomplete annotation noise respectively.Previous studies either considered only incomplete annotation noise or indiscriminately handle two types of noise with the same strategy.In this paper, we argue that the different causes of two types of noise bring up the requirement of different strategies in model architecture.Therefore, we propose the SANTA to handle these two types of noise separately with(1) Memory-smoothed Focal Loss and Entity-aware KNN to relieve the entity ambiguity problem caused by inaccurate annotation, and (2) Boundary Mixup to alleviate decision boundary shifting problem caused by incomplete annotation and a noise-tolerant loss to improve the robustness.Benefiting from our separate tailored strategies, we confirm in the experiment that the two types of noise are well