# Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

How is BLEU utilized in the evaluation of Text Style Transfer (TST), and what are its limitations?

## URLs

1. https://ar5iv.org/html/2308.13577. [2308.13577] Text Style Transfer Evaluation Using Large Language Models
2. https://ar5iv.org/html/2407.14822. [2407.14822] Text Style Transfer: An Introductory Overview
3. https://ar5iv.org/html/2403.08943. [2403.08943] LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots
4. https://ar5iv.org/html/2010.12742. [2010.12742] Text Style Transfer: A Review and Experimental Evaluation
5. https://ar5iv.org/html/2407.16737. [2407.16737] A Survey of Text Style Transfer: Applications and Ethical Implications
6. https://ar5iv.org/html/2306.00539. [2306.00539] A Call for Standardization and Validation of Text Style Transfer Evaluation
7. https://ar5iv.org/html/2403.01106. [2403.01106] Distilling Text Style Transfer With Self-Explanation From LLMs
8. https://ar5iv.org/html/2311.08389. [2311.08389] TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer

## Answer (I don't know?: ✗)

BLEU (Bilingual Evaluation Understudy) is utilized in the evaluation of Text Style Transfer (TST) to measure the amount of original content preserved after the style transfer operation. It adapts the original purpose of BLEU, which was designed for machine translation quality assessment, by comparing the transformed sentences with a set of parallel reference sentences [1][2]. Specifically, BLEU is computed when parallel TST datasets or human reference outputs are available, allowing researchers to assess content preservation quantitatively [2].

However, there are notable limitations associated with the use of BLEU in TST evaluations. One primary concern is that many TST tasks operate in non-parallel settings where matching reference outputs are not available, which complicates the use of conventional BLEU scores [2]. Additionally, the BLEU score tends to favor shorter texts and might not adequately measure the richness or diversity of content [1]. Furthermore, the reliance on a style classifier to evaluate transfer accuracy can inhibit the overall effectiveness of the evaluation process, as it may not accurately distinguish between styles when evaluating transformed outputs [3]. This suggests that while BLEU provides some insights into content preservation, its effectiveness is often constrained by contextual and methodological challenges inherent in TST tasks.

Moreover, specific shortcomings of BLEU include the fact that it can yield unreliable scores if not enough reference sentences are available, as the metric requires a proper set of "good quality" translations for accurate scoring [5]. In practical applications, the need to compute BLEU scores against multiple human references can complicate evaluations, especially if these references are not universally accessible, as seen in many datasets used for TST [5]. Therefore, while BLEU is an important tool in assessing content preservation in TST, the nuanced challenges and constraints of TST contexts necessitate the exploration of additional or complementary evaluation metrics to provide a more comprehensive assessment of style transfer outcomes [3][5].

1. [1]:  https://ar5iv.org/html/2311.08389, [2311.08389] TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer
2. [2]:  https://ar5iv.org/html/2010.12742, [2010.12742] Text Style Transfer: A Review and Experimental Evaluation
3. [3]:  https://ar5iv.org/html/2010.12742, [2010.12742] Text Style Transfer: A Review and Experimental Evaluation
4. [4]:  https://ar5iv.org/html/2311.08389, [2311.08389] TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer
5. [5]:  https://ar5iv.org/html/2306.00539, [2306.00539] A Call for Standardization and Validation of Text Style Transfer Evaluation
---
1. [1]:  Passage ID 1: on TST have adopted automated evaluation metrics commonly used in various natural language generation tasks to quantitatively assess content preservation during the style transfer process. These metrics include BLEU score (Papineni et al., 2002), ROUGE score (Lin and Och, 2004), BERTScore (Zhang et al., 2019) and others. Furthermore, the strength of the model-based style transfer is a commonly used measure to assess the ability of a TST model. Typically, a binary style classifier is first separately pretrained to predict the style label of input sentences. This classifier is then used to estimate the style transfer accuracy of the transformed style sentences. However, there are several shortcomings with the above metrics when evaluating passage-level and speech-style text. For example, the PPL tends to favor shorter texts, and a single classifier may not accurately differentiate between formal text and speech-style. To address these limitations, we introduce special evaluation metrics
2. [2]:  Passage ID 2: the amount of original content preserved after the style transfer operation, TST studies have borrowed three automated evaluation metrics that are commonly used in other natural language generation tasks:•BLEU: The BLEU score [95] was originally designed to evaluate the quality of a machine-translated text. The BLEU score was one of the first metrics to claim a high correlation with human judgment on the quality of the translated text. To calculate the BLEU score, machine-translated text is compared with a set of good quality reference translations. Similarly, in TST, the BLEU is computed when parallel TST datasets or human references are available. Specifically, we compute the BLEU score between the transferred sentences and the parallel references available to evaluate content preservation.•source-BLEU (sBLEU): Nevertheless, most of the TST tasks assume a non-parallel setting, and matching references of style-transferred sentences are not always available. Therefore, TST
3. [3]:  Passage ID 3: Style TransferOur experimental evaluation in Section 6 has illustrated the challenges of evaluating the effectiveness of TST models. The existing evaluation methods have some limitations. First, the evaluation of text style transfer based on transfer accuracy is limited by the performance of the style classifier. Second, similar to previous studies [27, 94], we note that the transfer strength is inversely proportional to content preservation, suggesting that these metrics may be complementary and challenging to optimize simultaneously. The limitations of existing evaluation metrics motivate the exploration of novel automatic evaluation metrics to evaluate TST models.10 Discussion and ConclusionAlthough TST is a relatively new branch of the field of natural language processing, considerable research on TST has recently been conducted. The explosive growth of TST research has generated many novel and interesting TST models. This survey aims to organize these novel TST models
4. [4]:  Passage ID 4: have demonstrated outstanding performance in traditional NLP tasks, thanks to their ability to leverage massive amounts of data.However, LLMs still have significant shortcomings in imitating human cognition.Text Style Transfer (TST) is a task that can reflect this ability of LLMs to some extent.The style of language is intricately related to human cognition, reflecting various aspects of the speaker’s characteristics, habits, logical thinking, and the specific communicative context, which exhibits a higher level of abstraction (Jin et al., 2022).In linguistics, language style is closely intertwined with human subjective cognition, encompassing the habits and logical thinking of the speaker, the content being described, and specific contextual factors. As a result, “style” transcends mere text and approaches the realm of cognition, reflecting its attributes.The objective of the TST task is to transform text-based language into another style of language with a more abstract
5. [5]:  Passage ID 5: BLEU sufficiently (including above mentioned details). Ref-BLEU is the second most popular method to measure content preservation. In addition to the general problems with BLEU, we see two more shortcomings. On the one hand, the needed reference sentences are not available for all datasets. On the other hand, calculating BLEU scores between the output and multiple human references can improve its reliability and, thankfully, for the arguably most popular TST dataset—Yelp—Jin et al. (2019) and Luo et al. (2019) introduced additional reference sentences. These, however, are only used by 6/30 papers applying Ref-BLEU on Yelp.Style Transfer AccuracyFor automated evaluation of style transfer accuracy, we find 23 evaluation metrics, of which 14 have been validated. Overall, 77 papers use automated style transfer accuracy metrics. Table 6 provides an overview of the most common automated text style transfer strength metrics. The top two are TextCNN Kim (2014) and fastText Joulin et al.