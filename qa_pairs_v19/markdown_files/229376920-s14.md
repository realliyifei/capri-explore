# Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

How is BLEU utilized in the evaluation of Text Style Transfer (TST), and what are its limitations?

## URLs

1. https://ar5iv.org/html/2308.13577. [2308.13577] Text Style Transfer Evaluation Using Large Language Models
2. https://ar5iv.org/html/2407.14822. [2407.14822] Text Style Transfer: An Introductory Overview
3. https://ar5iv.org/html/2403.08943. [2403.08943] LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots
4. https://ar5iv.org/html/2010.12742. [2010.12742] Text Style Transfer: A Review and Experimental Evaluation
5. https://ar5iv.org/html/2407.16737. [2407.16737] A Survey of Text Style Transfer: Applications and Ethical Implications
6. https://ar5iv.org/html/2306.00539. [2306.00539] A Call for Standardization and Validation of Text Style Transfer Evaluation
7. https://ar5iv.org/html/2403.01106. [2403.01106] Distilling Text Style Transfer With Self-Explanation From LLMs
8. https://ar5iv.org/html/2311.08389. [2311.08389] TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer

## Answer (I don't know?: ✗)

BLEU (Bilingual Evaluation Understudy) is a critical evaluation metric adapted for Text Style Transfer (TST) to assess the amount of original content preserved after the style transfer operation. Originally intended for machine translation evaluation, BLEU compares the style-transferred text with reference translations, measuring n-gram overlaps to compute similarity. In TST applications, BLEU scores are computed when parallel datasets or human references are available, allowing for a quantitative measure of content preservation of the transformed sentences [1][2].

However, there are notable limitations associated with the use of BLEU in TST evaluation. One significant drawback is its dependency on the availability of reference sentences, which is not feasible in many TST tasks that operate in a non-parallel setting. In such scenarios, suitable matching references for comparing style-transferred sentences are often lacking [2][5]. Moreover, the classic BLEU metric tends to favor shorter texts, which can skew results when evaluating longer passages more typical of TST outputs [1]. Lastly, the nature of BLEU's calculation can inadvertently limit its effectiveness in capturing the nuances of style. For instance, while BLEU provides a measure of content preservation, it is suggested that there is an inverse relationship between transfer strength and content preservation in TST contexts, indicating that optimizing for one may detriment the other [3].

To alleviate some of these concerns, researchers have also explored variations like Ref-BLEU that compare outputs against multiple human references to enhance reliability. Nonetheless, the inherent shortcomings of BLEU, particularly the need for reference data and its tendency to favor shorter texts, underscore the complexities and challenges faced in establishing effective evaluation methods for TST [5]. This highlights the need for novel automatic evaluation metrics that can more accurately assess TST models and ensure a comprehensive evaluation of style transfer performance [3].

1. [1]:  https://ar5iv.org/html/2311.08389, [2311.08389] TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer
2. [2]:  https://ar5iv.org/html/2010.12742, [2010.12742] Text Style Transfer: A Review and Experimental Evaluation
3. [3]:  https://ar5iv.org/html/2010.12742, [2010.12742] Text Style Transfer: A Review and Experimental Evaluation
4. [4]:  https://ar5iv.org/html/2311.08389, [2311.08389] TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer
5. [5]:  https://ar5iv.org/html/2306.00539, [2306.00539] A Call for Standardization and Validation of Text Style Transfer Evaluation
---
1. [1]:  Passage ID 1: on TST have adopted automated evaluation metrics commonly used in various natural language generation tasks to quantitatively assess content preservation during the style transfer process. These metrics include BLEU score (Papineni et al., 2002), ROUGE score (Lin and Och, 2004), BERTScore (Zhang et al., 2019) and others. Furthermore, the strength of the model-based style transfer is a commonly used measure to assess the ability of a TST model. Typically, a binary style classifier is first separately pretrained to predict the style label of input sentences. This classifier is then used to estimate the style transfer accuracy of the transformed style sentences. However, there are several shortcomings with the above metrics when evaluating passage-level and speech-style text. For example, the PPL tends to favor shorter texts, and a single classifier may not accurately differentiate between formal text and speech-style. To address these limitations, we introduce special evaluation metrics
2. [2]:  Passage ID 2: the amount of original content preserved after the style transfer operation, TST studies have borrowed three automated evaluation metrics that are commonly used in other natural language generation tasks:•BLEU: The BLEU score [95] was originally designed to evaluate the quality of a machine-translated text. The BLEU score was one of the first metrics to claim a high correlation with human judgment on the quality of the translated text. To calculate the BLEU score, machine-translated text is compared with a set of good quality reference translations. Similarly, in TST, the BLEU is computed when parallel TST datasets or human references are available. Specifically, we compute the BLEU score between the transferred sentences and the parallel references available to evaluate content preservation.•source-BLEU (sBLEU): Nevertheless, most of the TST tasks assume a non-parallel setting, and matching references of style-transferred sentences are not always available. Therefore, TST
3. [3]:  Passage ID 3: Style TransferOur experimental evaluation in Section 6 has illustrated the challenges of evaluating the effectiveness of TST models. The existing evaluation methods have some limitations. First, the evaluation of text style transfer based on transfer accuracy is limited by the performance of the style classifier. Second, similar to previous studies [27, 94], we note that the transfer strength is inversely proportional to content preservation, suggesting that these metrics may be complementary and challenging to optimize simultaneously. The limitations of existing evaluation metrics motivate the exploration of novel automatic evaluation metrics to evaluate TST models.10 Discussion and ConclusionAlthough TST is a relatively new branch of the field of natural language processing, considerable research on TST has recently been conducted. The explosive growth of TST research has generated many novel and interesting TST models. This survey aims to organize these novel TST models
4. [4]:  Passage ID 4: have demonstrated outstanding performance in traditional NLP tasks, thanks to their ability to leverage massive amounts of data.However, LLMs still have significant shortcomings in imitating human cognition.Text Style Transfer (TST) is a task that can reflect this ability of LLMs to some extent.The style of language is intricately related to human cognition, reflecting various aspects of the speaker’s characteristics, habits, logical thinking, and the specific communicative context, which exhibits a higher level of abstraction (Jin et al., 2022).In linguistics, language style is closely intertwined with human subjective cognition, encompassing the habits and logical thinking of the speaker, the content being described, and specific contextual factors. As a result, “style” transcends mere text and approaches the realm of cognition, reflecting its attributes.The objective of the TST task is to transform text-based language into another style of language with a more abstract
5. [5]:  Passage ID 5: BLEU sufficiently (including above mentioned details). Ref-BLEU is the second most popular method to measure content preservation. In addition to the general problems with BLEU, we see two more shortcomings. On the one hand, the needed reference sentences are not available for all datasets. On the other hand, calculating BLEU scores between the output and multiple human references can improve its reliability and, thankfully, for the arguably most popular TST dataset—Yelp—Jin et al. (2019) and Luo et al. (2019) introduced additional reference sentences. These, however, are only used by 6/30 papers applying Ref-BLEU on Yelp.Style Transfer AccuracyFor automated evaluation of style transfer accuracy, we find 23 evaluation metrics, of which 14 have been validated. Overall, 77 papers use automated style transfer accuracy metrics. Table 6 provides an overview of the most common automated text style transfer strength metrics. The top two are TextCNN Kim (2014) and fastText Joulin et al.