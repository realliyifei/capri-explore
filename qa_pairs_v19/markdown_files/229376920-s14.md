# Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

How is BLEU utilized in the evaluation of Text Style Transfer (TST), and what are its limitations?

## URLs

1. https://ar5iv.org/html/2308.13577
2. https://ar5iv.org/html/2407.14822
3. https://ar5iv.org/html/2403.08943
4. https://ar5iv.org/html/2010.12742
5. https://ar5iv.org/html/2407.16737
6. https://ar5iv.org/html/2306.00539
7. https://ar5iv.org/html/2403.01106
8. https://ar5iv.org/html/2311.08389

## Answer

BLEU (Bilingual Evaluation Understudy) is utilized in the evaluation of Text Style Transfer (TST) primarily as a metric for assessing the content preservation of transformed text during the style transfer process. It was originally designed to evaluate machine-translated text, establishing a high correlation with human judgment regarding translation quality. For TST, the BLEU score is computed by comparing style-transferred sentences to a set of high-quality reference translations within parallel datasets, when available [1] [2]. This metric quantifies the degree to which the original content is retained after the style transfer, thus serving as a crucial element in evaluating the effectiveness of TST models.

However, there are noteworthy limitations to the BLEU metric when applied to TST evaluations. One major issue is that BLEU tends to favor shorter texts, which can skew the results and fail to accurately reflect the preservation of content in longer formats [1]. Furthermore, in many TST applications, matching reference sentences for style-transferred outputs are not always available, thereby complicating the evaluation process in non-parallel settings. This absence of reference data can lead to difficulties in computing the BLEU score, impacting its utility and reliability [2][5].

Another significant limitation involves the relationship between content preservation and style transfer accuracy. Research indicates that the transfer strength of the style—essentially how well the model alters the text's style—can be inversely proportional to content preservation. This means that as a model becomes more adept at transferring style, it may inadvertently compromise the content of the original text, which poses a challenge for metrics like BLEU that aim to optimize both aspects simultaneously [3]. The evaluation of performance is often constrained by the effectiveness of the style classifier used to determine transfer accuracy, limiting the overall assessment of TST models further [3].

In conclusion, while BLEU serves a vital role in evaluating content preservation in TST, its limitations such as a preference for shorter texts, dependency on the availability of reference sentences, and the inverse relationship between style transfer strength and content preservation highlight the need for more robust evaluation strategies. The context indicates that TST research is still evolving and may benefit from the exploration of novel automatic evaluation metrics that could address these shortcomings more effectively [3].

[1]: https://ar5iv.org/html/2311.08389, [2311.08389] TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer
[2]: https://ar5iv.org/html/2010.12742, [2010.12742] Text Style Transfer: A Review and Experimental Evaluation
[3]: https://ar5iv.org/html/2010.12742, [2010.12742] Text Style Transfer: A Review and Experimental Evaluation
[4]: https://ar5iv.org/html/2311.08389, [2311.08389] TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer
[5]: https://ar5iv.org/html/2306.00539, [2306.00539] A Call for Standardization and Validation of Text Style Transfer Evaluation

[1]: Passage ID 1: on TST have adopted automated evaluation metrics commonly used in various natural language generation tasks to quantitatively assess content preservation during the style transfer process. These metrics include BLEU score (Papineni et al., 2002), ROUGE score (Lin and Och, 2004), BERTScore (Zhang et al., 2019) and others. Furthermore, the strength of the model-based style transfer is a commonly used measure to assess the ability of a TST model. Typically, a binary style classifier is first separately pretrained to predict the style label of input sentences. This classifier is then used to estimate the style transfer accuracy of the transformed style sentences. However, there are several shortcomings with the above metrics when evaluating passage-level and speech-style text. For example, the PPL tends to favor shorter texts, and a single classifier may not accurately differentiate between formal text and speech-style. To address these limitations, we introduce special evaluation metrics
[2]: Passage ID 2: the amount of original content preserved after the style transfer operation, TST studies have borrowed three automated evaluation metrics that are commonly used in other natural language generation tasks:•BLEU: The BLEU score [95] was originally designed to evaluate the quality of a machine-translated text. The BLEU score was one of the first metrics to claim a high correlation with human judgment on the quality of the translated text. To calculate the BLEU score, machine-translated text is compared with a set of good quality reference translations. Similarly, in TST, the BLEU is computed when parallel TST datasets or human references are available. Specifically, we compute the BLEU score between the transferred sentences and the parallel references available to evaluate content preservation.•source-BLEU (sBLEU): Nevertheless, most of the TST tasks assume a non-parallel setting, and matching references of style-transferred sentences are not always available. Therefore, TST
[3]: Passage ID 3: Style TransferOur experimental evaluation in Section 6 has illustrated the challenges of evaluating the effectiveness of TST models. The existing evaluation methods have some limitations. First, the evaluation of text style transfer based on transfer accuracy is limited by the performance of the style classifier. Second, similar to previous studies [27, 94], we note that the transfer strength is inversely proportional to content preservation, suggesting that these metrics may be complementary and challenging to optimize simultaneously. The limitations of existing evaluation metrics motivate the exploration of novel automatic evaluation metrics to evaluate TST models.10 Discussion and ConclusionAlthough TST is a relatively new branch of the field of natural language processing, considerable research on TST has recently been conducted. The explosive growth of TST research has generated many novel and interesting TST models. This survey aims to organize these novel TST models
[4]: Passage ID 4: have demonstrated outstanding performance in traditional NLP tasks, thanks to their ability to leverage massive amounts of data.However, LLMs still have significant shortcomings in imitating human cognition.Text Style Transfer (TST) is a task that can reflect this ability of LLMs to some extent.The style of language is intricately related to human cognition, reflecting various aspects of the speaker’s characteristics, habits, logical thinking, and the specific communicative context, which exhibits a higher level of abstraction (Jin et al., 2022).In linguistics, language style is closely intertwined with human subjective cognition, encompassing the habits and logical thinking of the speaker, the content being described, and specific contextual factors. As a result, “style” transcends mere text and approaches the realm of cognition, reflecting its attributes.The objective of the TST task is to transform text-based language into another style of language with a more abstract
[5]: Passage ID 5: BLEU sufficiently (including above mentioned details). Ref-BLEU is the second most popular method to measure content preservation. In addition to the general problems with BLEU, we see two more shortcomings. On the one hand, the needed reference sentences are not available for all datasets. On the other hand, calculating BLEU scores between the output and multiple human references can improve its reliability and, thankfully, for the arguably most popular TST dataset—Yelp—Jin et al. (2019) and Luo et al. (2019) introduced additional reference sentences. These, however, are only used by 6/30 papers applying Ref-BLEU on Yelp.Style Transfer AccuracyFor automated evaluation of style transfer accuracy, we find 23 evaluation metrics, of which 14 have been validated. Overall, 77 papers use automated style transfer accuracy metrics. Table 6 provides an overview of the most common automated text style transfer strength metrics. The top two are TextCNN Kim (2014) and fastText Joulin et al.