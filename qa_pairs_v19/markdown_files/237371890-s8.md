# Survey of Low-Resource Machine Translation

## Question

How have external language models been integrated into neural machine translation (NMT) models to enhance performance in low-resource settings?

## URLs

1. https://ar5iv.org/html/2404.02392
2. https://ar5iv.org/html/2412.00966
3. https://ar5iv.org/html/2411.11295
4. https://ar5iv.org/html/2305.01181
5. https://ar5iv.org/html/2312.04807
6. https://ar5iv.org/html/2412.05184
7. https://ar5iv.org/html/2311.02851

## Answer

## I don't know?: True

I don't know, as the context provided does not contain sufficient information regarding the integration of external language models into neural machine translation (NMT) models specifically for enhancing performance in low-resource settings. The passages primarily discuss the advantages and limitations of NMT, the issues related to low-resource languages, and general advancements in the field but do not mention any specific mechanisms or methods for integrating external language models into NMT. Therefore, I cannot provide a comprehensive answer to the question based on the provided context.

[1]: https://ar5iv.org/html/2107.04239, [2107.04239] A Survey on Low-Resource Neural Machine Translation
[2]: https://ar5iv.org/html/2107.04239, [2107.04239] A Survey on Low-Resource Neural Machine Translation
[3]: https://ar5iv.org/html/2404.02392, [2404.02392] Low-resource neural machine translation with morphological modeling
[4]: https://ar5iv.org/html/2301.00688, No Title
[5]: https://ar5iv.org/html/2107.04239, [2107.04239] A Survey on Low-Resource Neural Machine Translation

[1]: Passage ID 1: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
[2]: Passage ID 2: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
[3]: Passage ID 3: information and the proposed model and data augmentations in low-resource NMT.Low-resource neural machine translation with morphological modelingAntoine NzeyimanaUniversity of Massachusetts Amherstanthonzeyi@gmail.com1 IntroductionNeural Machine Translation (NMT) has become a predominant approach in developing machine translation systems. Two important innovations in recent state-of-the-art NMT systems are the use of the Transformer architecture Vaswani et al. (2017) and sub-word tokenization methods such as byte-pair encoding (BPE) Sennrich et al. (2016). However, for morphologically-rich languages(MRLs), BPE-based tokenization is only limited to the surface forms of the words and less grounded on exact lexical units (i.e. morphemes), especially in the presence morphographemic alternations Bundy and Wallen (1984) and non-concatenative morphology Kastner et al. (2019). In this work, we tackle the challenge of modeling complex morphology in low-resource NMT and
[4]: Passage ID 4: learning problems for variable length source and target sentences and long-term dependency problems. The NMT system improves translation prediction and has excellent context-analyzing properties.The superiority of NMT over phrase-based SMT is undeniable, and neural networks are used in most online machine translation engines. However, in spite of the growth achieved in the domain of machine translation, the idea of NMT system development being data-hungry continues to be a vital issue in expanding the work for any low-resource languages. In order to train a high-quality translation model, NMT requires a large bilingual corpus. However, creating parallel corpora for most low-resource language pairs is costly and requires human effort. Nevertheless, the language and geographical coverage of NMT have yet to hit new heights due to resource accessibility concerns and a preference for well-established assessment benchmarks. This compels additional research into the present state of NMT
[5]: Passage ID 5: NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the contextual information in the entire sentence, and thus conduct high quality and fluent translations.One limitation of NMT is that it needs large scale of parallel data for model training. While there are thousands of languages in the world111https://en.wikipedia.org/wiki/Language, major popular commercial translators (e.g., Google translator, Microsoft translator, Amazon translator) only support tens or a hundred languages because of the lack of large-scale parallel training data for most languages. To handle those languages with limited parallel data, many algorithms have been designed for low-resource NMT in recent years. Therefore, a review on low-resource NMT is very helpful for fresh researchers entering this area and industry practitioners. Although there already exists surveys on