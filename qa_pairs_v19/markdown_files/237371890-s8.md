# Survey of Low-Resource Machine Translation

## Question

How have external language models been integrated into neural machine translation (NMT) models to enhance performance in low-resource settings?

## URLs

1. https://ar5iv.org/html/2404.02392. [2404.02392] Low-resource neural machine translation with morphological modeling
2. https://ar5iv.org/html/2412.00966. [2412.00966] From Priest to Doctor: Domain Adaptaion for Low-Resource Neural Machine Translation
3. https://ar5iv.org/html/2411.11295. [2411.11295] Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation
4. https://ar5iv.org/html/2305.01181. [2305.01181] New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT
5. https://ar5iv.org/html/2312.04807. [2312.04807] Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting
6. https://ar5iv.org/html/2412.05184. [2412.05184] QueEn: A Large Language Model for Quechua-English Translation
7. https://ar5iv.org/html/2311.02851. [2311.02851] Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding

## Answer (I don't know?: ✓)

I don't know. 

The context provided does not contain specific information regarding the integration of external language models into neural machine translation (NMT) models, particularly in low-resource settings. While it discusses the general advantages and limitations of NMT, the challenges faced in resource availability, and mentions innovations such as the Transformer architecture and byte-pair encoding, it lacks any detailed reference to how external language models are used to improve performance in NMT for low-resource languages. This omission makes it impossible to provide a comprehensive answer to the question posed.

1. [1]:  https://ar5iv.org/html/2107.04239, No Title
2. [2]:  https://ar5iv.org/html/2404.02392, [2404.02392] Low-resource neural machine translation with morphological modeling
3. [3]:  https://ar5iv.org/html/2301.00688, No Title
4. [4]:  https://ar5iv.org/html/2107.04239, No Title
5. [5]:  https://ar5iv.org/html/2410.22335, No Title
---
1. [1]:  Passage ID 1: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
2. [2]:  Passage ID 2: information and the proposed model and data augmentations in low-resource NMT.Low-resource neural machine translation with morphological modelingAntoine NzeyimanaUniversity of Massachusetts Amherstanthonzeyi@gmail.com1 IntroductionNeural Machine Translation (NMT) has become a predominant approach in developing machine translation systems. Two important innovations in recent state-of-the-art NMT systems are the use of the Transformer architecture Vaswani et al. (2017) and sub-word tokenization methods such as byte-pair encoding (BPE) Sennrich et al. (2016). However, for morphologically-rich languages(MRLs), BPE-based tokenization is only limited to the surface forms of the words and less grounded on exact lexical units (i.e. morphemes), especially in the presence morphographemic alternations Bundy and Wallen (1984) and non-concatenative morphology Kastner et al. (2019). In this work, we tackle the challenge of modeling complex morphology in low-resource NMT and
3. [3]:  Passage ID 3: learning problems for variable length source and target sentences and long-term dependency problems. The NMT system improves translation prediction and has excellent context-analyzing properties.The superiority of NMT over phrase-based SMT is undeniable, and neural networks are used in most online machine translation engines. However, in spite of the growth achieved in the domain of machine translation, the idea of NMT system development being data-hungry continues to be a vital issue in expanding the work for any low-resource languages. In order to train a high-quality translation model, NMT requires a large bilingual corpus. However, creating parallel corpora for most low-resource language pairs is costly and requires human effort. Nevertheless, the language and geographical coverage of NMT have yet to hit new heights due to resource accessibility concerns and a preference for well-established assessment benchmarks. This compels additional research into the present state of NMT
4. [4]:  Passage ID 4: NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the contextual information in the entire sentence, and thus conduct high quality and fluent translations.One limitation of NMT is that it needs large scale of parallel data for model training. While there are thousands of languages in the world111https://en.wikipedia.org/wiki/Language, major popular commercial translators (e.g., Google translator, Microsoft translator, Amazon translator) only support tens or a hundred languages because of the lack of large-scale parallel training data for most languages. To handle those languages with limited parallel data, many algorithms have been designed for low-resource NMT in recent years. Therefore, a review on low-resource NMT is very helpful for fresh researchers entering this area and industry practitioners. Although there already exists surveys on
5. [5]:  Passage ID 5: information. Furthermore, we explored key factors contributing to the model’s performance improvement, including but not limited to the design of the network architecture, optimization of training strategies, and adjustment of hyperparameters. These analyses not only deepen our understanding of the model’s internal working mechanisms but also provide valuable insights and guidance for future research, especially in terms of further enhancing the performance and application scope of machine translation systems.2 PreliminaryMachine translation, as an important branch of the field of Natural Language Processing (NLP), aims to achieve automatic conversion from one language to another. Early machine translation methods were primarily based on rules and dictionaries. Since the mid-20th century, machine translation has undergone a transition from rule-based translation to statistical methods Brown et al. (1993); Lopez (2008), and to the current Neural Machine Translation (NMT)