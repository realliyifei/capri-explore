# Survey of Low-Resource Machine Translation

## Question

How have external language models been integrated into neural machine translation (NMT) models to enhance performance in low-resource settings?

## URLs

1. https://ar5iv.org/html/2404.02392. [2404.02392] Low-resource neural machine translation with morphological modeling
2. https://ar5iv.org/html/2412.00966. [2412.00966] From Priest to Doctor: Domain Adaptaion for Low-Resource Neural Machine Translation
3. https://ar5iv.org/html/2411.11295. [2411.11295] Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation
4. https://ar5iv.org/html/2305.01181. [2305.01181] New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT
5. https://ar5iv.org/html/2312.04807. [2312.04807] Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting
6. https://ar5iv.org/html/2412.05184. [2412.05184] QueEn: A Large Language Model for Quechua-English Translation
7. https://ar5iv.org/html/2311.02851. [2311.02851] Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding

## Answer (I don't know?: ✗)

External language models have been integrated into neural machine translation (NMT) systems in several ways to enhance performance in low-resource settings. One significant method is the incorporation of pre-trained language models, which serve as a foundation for low-resource language tasks. This integration leverages the rich linguistic knowledge embedded in these models, allowing NMT systems to benefit from larger datasets associated with high-resource languages.

The approach of using transfer learning is pivotal in this context. Models that have been pretrained on high-resource languages are fine-tuned on low-resource languages, providing these under-resourced systems with vital linguistic insights and representations that they might otherwise lack due to limited training data [5]. For instance, the multilingual NMT approach trains models to handle multiple languages simultaneously, thereby allowing low-resource languages to share representations with their high-resource counterparts. This sharing of data effectively enhances the performance of translations in low-resource languages by enabling the NMT systems to draw from the broader information encoded in the multilingual architecture [5].

Moreover, data augmentation techniques play a crucial role in improving NMT performance for low-resource languages. Techniques like back-translation, where existing sentences in the target language are translated back into the source language to generate synthetic training examples, help mitigate the challenges posed by parallel data scarcity [5]. These methods enable NMT models to learn more robust features from available data, thereby improving translation quality.

Another aspect involves morphological modeling specifically designed for morphologically-rich languages (MRLs). The standard byte-pair encoding (BPE) tokenization is often insufficient as it does not capture morphological structures effectively. The integration of external language models can enhance morphology-aware translation systems by providing a deeper understanding of morphemes and their distribution, leading to better handling of complex morphemes that are common in low-resource languages [1][4]. 

Furthermore, external models can facilitate keyword-based retrieval strategies that optimize how translations are generated, further boosting translation quality in low-resource scenarios [3]. These strategies help in addressing the issues of misaligned vocabularies between source and target languages, particularly for models that must copy terms without translation, such as proper names [4].

In summary, integrating external language models into NMT systems enhances their performance in low-resource settings through the use of transfer learning from high-resource languages, leveraging multilingual architectures, applying effective data augmentation techniques, and incorporating morphological modeling strategies. These collective efforts aim at bridging the resource gap and ensuring that NMT systems deliver more accurate and coherent translations in less commonly spoken languages.

1. [1]:  https://ar5iv.org/html/2404.02392, [2404.02392] Low-resource neural machine translation with morphological modeling
2. [2]:  https://ar5iv.org/html/2411.11295, [2411.11295] Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation
3. [3]:  https://ar5iv.org/html/2411.11295, [2411.11295] Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation
4. [4]:  https://ar5iv.org/html/2404.02392, [2404.02392] Low-resource neural machine translation with morphological modeling
5. [5]:  https://ar5iv.org/html/2411.11295, [2411.11295] Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation
---
1. [1]:  Passage ID 1: information and the proposed model and data augmentations in low-resource NMT.Low-resource neural machine translation with morphological modelingAntoine NzeyimanaUniversity of Massachusetts Amherstanthonzeyi@gmail.com1 IntroductionNeural Machine Translation (NMT) has become a predominant approach in developing machine translation systems. Two important innovations in recent state-of-the-art NMT systems are the use of the Transformer architecture Vaswani et al. (2017) and sub-word tokenization methods such as byte-pair encoding (BPE) Sennrich et al. (2016). However, for morphologically-rich languages(MRLs), BPE-based tokenization is only limited to the surface forms of the words and less grounded on exact lexical units (i.e. morphemes), especially in the presence morphographemic alternations Bundy and Wallen (1984) and non-concatenative morphology Kastner et al. (2019). In this work, we tackle the challenge of modeling complex morphology in low-resource NMT and
2. [2]:  Passage ID 2: understanding and reasoning abilities pave the way toward Artificial General Intelligence (AGI) and can facilitate societal development across a wide range of domains [83, 44, 84, 82, 35, 32].2.2 Machine Translation on Low-Resource LanguageMachine Translation for low-resource languages has been a long-standing challenge in the field of NLP. While machine translation for high-resource languages, such as English, Chinese, or Spanish, has seen considerable improvements, particularly with the advent of NMT techniques, low-resource languages have lagged due to the scarcity of large parallel corpora and linguistic resources.Early efforts in machine translation, particularly for low-resource languages, were based on rule-based and SMT approaches. Rule-based systems relied on linguistic rules crafted by experts and extensive lexicons, but they were often labor-intensive and brittle when applied to complex languages. SMT, introduced in the 1990s, offered a data-driven approach, where
3. [3]:  Passage ID 3: the translations. Our goal is to assess how well these models generalize to languages with limited training data and to identify strategies, such as keyword-based retrieval, that can further improve translation quality for low-resource languages.In the broader context, improving LLM translation for low-resource languages can have far-reaching implications. It can not only help bridge the communication gap in critical domains like healthcare but also contribute to the preservation and revitalization of endangered languages. By making low-resource languages more accessible and usable in digital environments, we can help uplift communities and ensure that these languages continue to thrive in the modern world.2 Related Work2.1 Large Language ModelsLLMs have revolutionized NLP, with applications across diverse fields such as education, healthcare, robotics, etc [32, 63, 30, 73, 40, 31, 34, 85]. The foundation of LLMs lies in the transformer architecture introduced by Vaswani
4. [4]:  Passage ID 4: the transformer model. Beside augmentation from pre-trained language model integration, we also devise an augmentation based solely on the word order relationship between source and target languages. These model augmentations bring substantial improvement in translation performance when parallel text is scarce.One of the main challenges facing machine translation for low-resource languages obviously is parallel data scarcity. When the training data has limited lexical coverage, the NMT model may tend to hallucinate Raunak et al. (2021); Xu et al. (2023). Additionally, for a morphology-aware translation model, there is a problem of misaligned vocabularies between source and target languages. This makes it harder for the model to learn to copy unknown words and other tokens that need to be copied without translation such as proper names.To address these challenges, we take a data-centric approach by developing tools to extract more parallel data from public-domain documents and
5. [5]:  Passage ID 5: leveraging deep learning to create richer representations of source and target sentences. However, NMT’s effectiveness heavily depends on the availability of large datasets, making it less applicable to low-resource settings. Researchers began exploring techniques to alleviate the data scarcity issue. Transfer learning became a popular method, where models pretrained on high-resource languages are fine-tuned on low-resource languages. This technique allowed low-resource languages to benefit from knowledge gained from related high-resource languages. Multilingual NMT further extended this idea, training models to translate between multiple languages simultaneously. This method improved performance for low-resource languages by allowing them to share representations with high-resource languages, leveraging multilingual data in a shared model architecture.Data augmentation techniques, such as back-translation, have become widely used for low-resource Machine Translation. In