# Relational World Knowledge Representation in Contextual Language Models: A Review

## Question

How are statement scoring strategies categorized, and what are the key characteristics of each approach?

## URLs

1. https://ar5iv.org/html/2411.12570. [2411.12570] A data driven approach to classify descriptors based on their efficiency in translating noisy trajectories into physically-relevant information
2. https://ar5iv.org/html/1611.00252. No content available
3. https://ar5iv.org/html/2308.10220. [2308.10220] Designing and Evaluating Presentation Strategies for Fact-Checked Content
4. https://ar5iv.org/html/2407.05733. [2407.05733] Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative Judgment Approach Based on Rater Cognition
5. https://ar5iv.org/html/1703.09662. [1703.09662] Categorizing User Sessions at Pinterest
6. https://ar5iv.org/html/2402.19195. [2402.19195] Negative Sampling in Knowledge Graph Representation Learning: A Review
7. https://ar5iv.org/html/2305.11231. [2305.11231] Recent Trends in Unsupervised Summarization
8. https://ar5iv.org/html/2311.12990. [2311.12990] NERIF: GPT-4V for Automatic Scoring of Drawn Models
9. https://ar5iv.org/html/2410.20199. [2410.20199] Rethinking the Uncertainty: A Critical Review and Analysis in the Era of Large Language Models
10. https://ar5iv.org/html/2403.14988. [2403.14988] Risk and Response in Large Language Models: Evaluating Key Threat Categories

## Answer (I don't know?: ✗)

Statement scoring strategies can be categorized primarily into two main types: rubric-based scoring and CJ-based scoring, both of which have distinct characteristics and methodologies.

1. **Rubric-Based Scoring**: This strategy involves using predefined criteria to evaluate responses. It typically relies on a scoring rubric that outlines specific descriptors for different levels of performance. The study referenced in the context investigates whether the GPT-4 model can better imitate human-rater scores with a rubric-based scoring strategy compared to the previous version, GPT-3.5 [4]. Additionally, the effectiveness of an elaborated scoring rubric with well-defined descriptors is also evaluated to see if it enhances the imitation of human rater scores [4]. This method emphasizes structuring the evaluation criteria, which helps in reducing inter-rater variability and biases, as consensus among multiple human experts is critical for achieving reliable outcomes [3].

2. **CJ-Based (Cognitive Judgment-Based) Scoring**: This scoring strategy appears to focus on capturing more nuanced aspects of responses that could be more aligned with human cognitive judgments. The effectiveness of the GPT model in imitating human rater scores using this approach is also investigated [4]. CJ-based scoring may allow for finer distinctions in scoring by integrating human cognitive processes into the judgment criteria, although specific characteristics of this method are less detailed in the context provided.

Both strategies utilize human-scored data to train machine learning models, but they differ in their frameworks. The rubric-based approach prioritizes systematic standardization through clear criteria, while the CJ-based method may embrace more subjective and cognitive aspects of human evaluation.

In terms of performance, GPT-4V was shown to have varying degrees of accuracy across scoring categories, with an average scoring accuracy lower for more proficient classes [5]. These findings suggest that while rubric-based strategies can enhance scoring consistency, the inherent challenges in scoring more complex responses persist, indicating room for improvement in both methods [5].

In conclusion, statement scoring strategies in NLP involve structured approaches like rubric-based scoring, focused on predefined criteria, and more subjective methods such as CJ-based scoring, which may integrate cognitive processes. Further research is necessary to refine these strategies and improve the overall accuracy and reliability of automated scoring systems in educational contexts [1][2][3].

1. [1]:  https://ar5iv.org/html/2407.05733, [2407.05733] Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative Judgment Approach Based on Rater Cognition
2. [2]:  https://ar5iv.org/html/2410.20199, [2410.20199] Rethinking the Uncertainty: A Critical Review and Analysis in the Era of Large Language Models
3. [3]:  https://ar5iv.org/html/2311.12990, [2311.12990] NERIF: GPT-4V for Automatic Scoring of Drawn Models
4. [4]:  https://ar5iv.org/html/2407.05733, [2407.05733] Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative Judgment Approach Based on Rater Cognition
5. [5]:  https://ar5iv.org/html/2311.12990, [2311.12990] NERIF: GPT-4V for Automatic Scoring of Drawn Models
---
1. [1]:  Passage ID 1: domains that entail generating multi-trait scoring data with a hierarchy. The insight gained from this study can guide the development of automated scoring systems in various fields, emphasizing the significance of taking into account elements such as scoring criteria, scoring methods, and the specific language model used. This work highlights the significance of interdisciplinary collaboration among specialists in the areas of natural language processing, educational assessment, and cognitive psychology to further enhance the progress and implementation of LLMs in intricate educational problems.ReferencesBejar (2012)Issac I. Bejar.Rater cognition: Implications for validity.Educational Measurement: Issues and Practice, 31(3):2–9, 2012.doi: 10.1111/j.1745-3992.2012.00238.x.Bradley and Terry (1952)Ralph Allan Bradley and Milton E. Terry.Rank analysis of incomplete block designs: I. the method of paired comparisons.Biometrika, 39(3/4):324–345, 1952.doi:
2. [2]:  Passage ID 2: provide a detailed introduction to key related concepts and examine the limitations of current methods in mission-critical and safety-sensitive applications. The paper concludes with a perspective on future directions aimed at enhancing the reliability and practical adoption of these methods in real-world scenarios.1 IntroductionLarge Language Models (LLMs) have recently demonstrated remarkable capabilities in various complex reasoning and question-answering tasks (Zhao et al., 2023; Wang et al., 2024c; Liang et al., 2022). However, despite their potential, LLMsstill face significant challenges in generating erroneous answers (Ji et al., 2023a; Li et al., 2023a; Huang et al., 2023), which can have serious consequences, particularly in domains where high levels of accuracy and reliability are critical. A key issue undermining trust in LLM outputsis the models’ lack of transparency and expressiveness in their decision-making processes (Zhou et al., 2023; Lin et al., 2023; Yin
3. [3]:  Passage ID 3: scoring rubrics. Usually, more than two human experts are needed, and their consensus and interrater reliability are considered essential to reduce rater errors and potential bias. [10] suggest additional considerations such as scoring one item at a time, using a scoring rubric or ideal answer, and reading samples before scoring others. The human-scored data would be used to train machines to develop scoring models. Though researchers reported satisfied scoring accuracy, the entire algorithmic model development is usually costly and time-consuming. It is thus urgent to develop approaches to reduce the efforts of developing scoring models.With the recent development of GPT-4V, [22] provides opportunities to employ prompt engineering to reduce machine training time and costs. GPT-4V takes an image and a natural-language prompt about the image as input and can produce a natural-language answer as the output [1]. The capability of providing explainable answers in natural language to
4. [4]:  Passage ID 4: essay between two given essays without any additional training, using only zero-shot prompting.The study aims to address the following research questions:RQ1. When using a rubric-based scoring strategy, will the GPT-4 model be able to better imitate human-rater scores compared to the GPT-3.5 model?RQ2. When using a rubric-based scoring strategy, will GPT models be able to better imitate human rater’s scores if an elaborated scoring rubric with descriptors is used?RQ3. When using a CJ-based scoring strategy, will the GPT model be able to better imitate human rater scores compared to the rubric-based scoring strategy?RQ4. When using a CJ-based scoring strategy and utilizing fine-grained scores, will GPT models be able to better imitate human rater scores?4 Methods4.1 DatasetWe utilized essay sets 7 and 8 from the ASAP dataset111https://www.kaggle.com/c/asap-aes, which include multiple raters’ scores and analytical scoring based on 4 and 6 traits, respectively.
5. [5]:  Passage ID 5: Results show that GPT-4V’s average scoring accuracy was mean =.51, SD = .037, with varying accuracy across scoring categories. Specifically, average scoring accuracy was .64 for the ’Beginning’ class, .62 for the ’Developing’ class, and .26 for the ’Proficient’ class, indicating that more proficient models are more challenging to score. Further qualitative study reveals how GPT-4V retrieves information from image input, including problem context, example evaluations provided by human coders, and students’ drawing models. We also uncovered how GPT-4V catches the characteristics of student-drawn models and narrates them in natural language. At last, we demonstrated how GPT-4V assigns scores to student-drawn models according to the given scoring rubric and instructional notes. Our findings suggest that the NERIF method is an effective approach for employing GPT-4V to score drawn models. Even though there is space for GPT-4V to improve scoring accuracy, some mis-assigned scores seemed