# TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation

## Question

How is inter-annotator agreement assessed in various annotation tasks, and what factors can affect these scores?

## URLs

1. https://ar5iv.org/html/2410.14515. [2410.14515] Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media
2. https://ar5iv.org/html/2301.10684. [2301.10684] Consistency is Key: Disentangling Label Variation in Natural Language Processing with Intra-Annotator Agreement
3. https://ar5iv.org/html/2110.05699. [2110.05699] On Releasing Annotator-Level Labels and Information in Datasets
4. https://ar5iv.org/html/2410.13313. [2410.13313] Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language
5. https://ar5iv.org/html/2410.18889. [2410.18889] Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance
6. https://ar5iv.org/html/2404.15720. [2404.15720] Annotator-Centric Active Learning for Subjective NLP Tasks
7. https://ar5iv.org/html/2408.00880. [2408.00880] Annotator in the Loop: A Case Study of In-Depth Rater Engagement to Create a Bridging Benchmark Dataset
8. https://ar5iv.org/html/2410.08820. [2410.08820] Which Demographics do LLMs Default to During Annotation?
9. https://ar5iv.org/html/2404.09696. [2404.09696] Are Large Language Models Reliable Argument Quality Annotators?Supported by the German Research Foundation (Project Nr. 455911521).

## Answer (I don't know?: âœ—)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable