# TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation

## Question

How is inter-annotator agreement assessed in various annotation tasks, and what factors can affect these scores?

## URLs

1. https://ar5iv.org/html/2410.14515. [2410.14515] Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media
2. https://ar5iv.org/html/2301.10684. [2301.10684] Consistency is Key: Disentangling Label Variation in Natural Language Processing with Intra-Annotator Agreement
3. https://ar5iv.org/html/2110.05699. [2110.05699] On Releasing Annotator-Level Labels and Information in Datasets
4. https://ar5iv.org/html/2410.13313. [2410.13313] Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language
5. https://ar5iv.org/html/2410.18889. [2410.18889] Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance
6. https://ar5iv.org/html/2404.15720. [2404.15720] Annotator-Centric Active Learning for Subjective NLP Tasks
7. https://ar5iv.org/html/2408.00880. [2408.00880] Annotator in the Loop: A Case Study of In-Depth Rater Engagement to Create a Bridging Benchmark Dataset
8. https://ar5iv.org/html/2410.08820. [2410.08820] Which Demographics do LLMs Default to During Annotation?
9. https://ar5iv.org/html/2404.09696. [2404.09696] Are Large Language Models Reliable Argument Quality Annotators?Supported by the German Research Foundation (Project Nr. 455911521).

## Answer (I don't know?: ✗)

Inter-annotator agreement (IAA) is a critical metric for evaluating the consistency among annotators in various annotation tasks, particularly in natural language processing (NLP) and image annotation. IAA is typically assessed using methods such as majority voting or calculating Inter-Annotator Agreement scores, which quantify how similarly multiple annotators label the same data points [4][5]. 

In the realm of NLP, the level of subjectivity inherent in different tasks significantly impacts IAA scores. Tasks such as sentiment analysis, emotion detection, and hate speech detection are recognized for their subjective nature, making it difficult to achieve a single 'ground truth' [2]. Since different annotators may interpret subjective language divergingly, the consensus among them—reflected through IAA metrics—can vary widely based on personal biases, perspectives, and interpretations.

Additionally, the design of the annotation process can also influence IAA outcomes. For instance, the practice of involving multiple annotators to label each data point is often recommended, particularly for topics that are opinionated, like politics [4]. This multiplicity helps in mitigating biases that individual annotators may bring, thereby enhancing the reliability of the annotations. The aggregation of differing opinions through majority voting or IAA scores can lead to more robust outcomes, balancing out individual biases [4][5].

Interestingly, research suggests that involving non-expert annotators can sometimes yield effective outcomes; diverse annotations from a larger pool of non-experts may compensate for a lack of consistency, ultimately leading to better system performance when machine learning models are trained on these diverse datasets [3]. For instance, one study indicated that it might take annotations from four non-experts to achieve a quality level comparable to that of one expert [5]. This highlights that while expert annotators might provide higher quality labels, the diversity and volume of non-expert annotations can help capture a broader range of interpretations, which is particularly valuable in subjective tasks [3].

Moreover, the context in which the annotation occurs can also be a determining factor for IAA scores. In cases where sensitive information is involved, processes can be designed to ensure that abstracted or anonymized labels are used to prevent bias [4]. The overall goal across these strategies is to establish evaluations that are as objective as possible despite the subjective nature of many annotation tasks [4][2].

In summary, inter-annotator agreement is assessed through various means like majority voting and IAA scores. Factors influencing these scores include the inherent subjectivity of the task, the nature and expertise of the annotators, and the designs of the annotation process to mitigate biases. Combining insights from both expert and non-expert annotators might provide the best balance, ensuring rich diversity in views while also maintaining a higher quality of annotations.

1. [1]:  https://ar5iv.org/html/2408.09235, No Title
2. [2]:  https://ar5iv.org/html/2110.05699, [2110.05699] On Releasing Annotator-Level Labels and Information in Datasets
3. [3]:  https://ar5iv.org/html/1907.10450, No Title
4. [4]:  https://ar5iv.org/html/2401.01262, No Title
5. [5]:  https://ar5iv.org/html/1907.10450, No Title
---
1. [1]:  Passage ID 1: We recruit three graduate students from our academic network, all specialized in natural language processing, to serve as annotators. We provide the input given to the candidates, reference answers, and candidate responses. This format, while similar, is distinct from the judge models’ prompts which additionally require formatted decisions. The human annotators focus solely on the accuracy and relevance of the responses. To ensure impartial evaluations, we anonymize the origin of responses. Annotators do not know which candidate model generated such responses, reducing potential bias linked to model familiarity or reputation. We asked the annotators to score the candidate LLMs outputs on a binary scale: ‘1’ for ‘True’ and ‘0’ for ‘False’ based on alignment with the reference answer and contextual relevance.To ensure a rigorous evaluation, each of the three annotators independently assesses the entire set of outputs generated by each candidate model across all datasets. Specifically,
2. [2]:  Passage ID 2: annotators in training tasks.2 BackgroundNLP has a long history of developing techniques to interpret subjective language Wiebe et al. (2004); Alm (2011). While all human judgments embed some degree of subjectivity, some tasks such as sentiment analysis Liu et al. (2010), affect modeling Alm (2008); Liu et al. (2003), emotion detection Hirschberg et al. (2003), and hate speech detection Warner and Hirschberg (2012) are agreed upon as relatively more subjective in nature.As Alm (2011) points out, achieving a single real ‘ground truth’ is not possible, nor essential in case of such subjective tasks. Instead, we should investigate how to model the subjective interpretations of the annotators, and how to account for them in application scenarios.However, the current practice in the NLP community continues to be applying different aggregation strategies to arrive at a single score or label that makes it amenable to train and evaluate supervised machine learning models.
3. [3]:  Passage ID 3: the annotation quality of one expert annotator in selected tasks. Furthermore, they have trained machine learning classifiers on expert as well as non-expert annotations and reported better system performance for non-experts due to high annotation diversity reducing the annotator bias. However, these observations were exclusively made for natural language tasks.Nowak and Rueger [7] presented a study on inter-coder agreement for image annotation from both crowdsourcing and experts. Human annotators had to label 99 images of the ImageCLEF test data (http://www.imageclef.org/) with respect to 22 concept categories. Some of the categories were mutually exclusive (season, time of day, indoor/outdoor/none). The images were assessed by experts as well as by Mechanical Turk workers. They measure higher agreement for experts, but argue that majority voting filters out noise in non-expert annotations closing the gap to expert annotations of higher quality. A more recent study [2] deals with the
4. [4]:  Passage ID 4: facilitating a more objective decision. One may also require the process to be designed so that sensitive information stays hidden and cannot bias the annotators’ decisions (I7). An aspect frequently mentioned is involving multiple annotators in labeling a data point (I4, I5, I7, I10, I12, I13). Interviewee four suggests that it should be case-dependent to either require multiple annotators or not (I4). Particularly for topics that are very opinionated and perceived differently by every individual, like politics, multiple annotators should be involved to reduce biases (I4). The aggregation of multiple annotators’ opinions on the correct label can either be achieved by a majority vote between annotators (I5, I12) or by an Inter-Annotator Agreement (I4, I7, I10, I13). An Inter-Annotator Agreement score quantifies how similarly multiple annotators annotate the same pieces of text (I4, I10). Particularly referring to topics that tend to be opinionated, a very high score represents a high
5. [5]:  Passage ID 5: results of a comparison between expert vs. non-expert inter-coder agreements. Furthermore, a performance prediction for the task of concept classification is presented based on inter-coder agreement and training data size as input. In Section 5, the impact of expert vs. non-expert image annotations on person identification performance is investigated. Section 6 concludes the paper and outlines areas for future work.2 Related WorkIn this section, we briefly survey related work for inter-annotator studies conducted for natural language as well as image annotation tasks. Snow et al. [8] have evaluated non-expert annotations by means of Mechanical Turk workers for natural language tasks. Among other experiments, they found that annotations of four non-experts are needed to rival the annotation quality of one expert annotator in selected tasks. Furthermore, they have trained machine learning classifiers on expert as well as non-expert annotations and reported better system