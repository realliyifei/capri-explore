# TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation

## Question

How is inter-annotator agreement assessed in various annotation tasks, and what factors can affect these scores?

## URLs

1. https://ar5iv.org/html/2410.14515. [2410.14515] Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media
2. https://ar5iv.org/html/2301.10684. [2301.10684] Consistency is Key: Disentangling Label Variation in Natural Language Processing with Intra-Annotator Agreement
3. https://ar5iv.org/html/2110.05699. [2110.05699] On Releasing Annotator-Level Labels and Information in Datasets
4. https://ar5iv.org/html/2410.13313. [2410.13313] Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language
5. https://ar5iv.org/html/2410.18889. [2410.18889] Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance
6. https://ar5iv.org/html/2404.15720. [2404.15720] Annotator-Centric Active Learning for Subjective NLP Tasks
7. https://ar5iv.org/html/2408.00880. [2408.00880] Annotator in the Loop: A Case Study of In-Depth Rater Engagement to Create a Bridging Benchmark Dataset
8. https://ar5iv.org/html/2410.08820. [2410.08820] Which Demographics do LLMs Default to During Annotation?
9. https://ar5iv.org/html/2404.09696. [2404.09696] Are Large Language Models Reliable Argument Quality Annotators?Supported by the German Research Foundation (Project Nr. 455911521).

## Answer (I don't know?: ✗)

Inter-annotator agreement (IAA) is a crucial metric in evaluating the reliability and validity of annotations in various tasks, particularly in Natural Language Processing (NLP) and image annotation. The assessment of IAA typically involves measuring consistency among different annotators regarding how they categorize or evaluate a set of data.

In the context of NLP, one standard approach involves recruiting multiple annotators to independently assess responses based on predefined criteria. For instance, in a study detailed in the referenced material, three graduate students specializing in NLP were recruited to evaluate candidate outputs on a binary scale of 'True' or 'False' based on their relevance and alignment with reference answers. This method allows for direct comparison of responses and calculation of agreement scores among the annotators using various statistical measures, such as Cohen's Kappa or Fleiss' Kappa [2].

Factors influencing inter-annotator agreement scores can be manifold. One significant factor is the subjectivity inherent in the tasks. As noted in the context, NLP tasks like sentiment analysis and hate speech detection are particularly subjective, with varied interpretations of the same input leading to different annotations by different annotators [1]. The difficulty in establishing a single 'ground truth' further complicates this. Instead of converging on a uniform score, annotators might bring their individual biases and perspectives into their evaluations, affecting the overall agreement [1].

Additionally, the experience level and background of the annotators can significantly impact IAA. Studies have indicated that non-expert annotators can sometimes yield a diversity in annotations that, when aggregated properly, may not only reach but rival the quality of expert assessments [3] [4]. For example, while expert annotations are typically seen as more reliable due to their domain-specific knowledge, non-expert annotations can reflect broader viewpoints and interpretations, which can be beneficial in tasks such as sentiment analysis where the subjectivity varies widely [5].

Moreover, the use of aggregation strategies also plays a crucial role. Different methods for combining individual scores into a final assessment can lead to variations in reported IAA. In some cases, majority voting might elevate the agreement score by mitigating the noise associated with individual annotations, particularly in crowdsourced efforts [3]. This approach, nevertheless, requires careful consideration to prevent the dilution of valuable diversity in the annotations.

Regarding the nature of the annotation tasks themselves, tasks that demand deep cognitive engagement or are reliant on complex guidelines may see a larger discrepancy in annotations among annotators due to the varying levels of understanding and interpretation [5]. Thus, maintaining high inter-annotator agreement becomes a balancing act between ensuring sufficient clarity in the annotation guidelines and recognizing the subjective nature of the data being annotated.

In conclusion, assessing inter-annotator agreement in annotation tasks is a multifaceted process influenced by the subjectivity of the tasks, the expertise of the annotators, the methodologies used for aggregation, and the complexity of the guidelines. These factors combine to shape the overall quality and reliability of the annotation outcomes.

1. [1]:  https://ar5iv.org/html/2110.05699, [2110.05699] On Releasing Annotator-Level Labels and Information in Datasets
2. [2]:  https://ar5iv.org/html/2408.09235, No Title
3. [3]:  https://ar5iv.org/html/1907.10450, No Title
4. [4]:  https://ar5iv.org/html/1907.10450, No Title
5. [5]:  https://ar5iv.org/html/2410.18889, [2410.18889] Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance
---
1. [1]:  Passage ID 1: annotators in training tasks.2 BackgroundNLP has a long history of developing techniques to interpret subjective language Wiebe et al. (2004); Alm (2011). While all human judgments embed some degree of subjectivity, some tasks such as sentiment analysis Liu et al. (2010), affect modeling Alm (2008); Liu et al. (2003), emotion detection Hirschberg et al. (2003), and hate speech detection Warner and Hirschberg (2012) are agreed upon as relatively more subjective in nature.As Alm (2011) points out, achieving a single real ‘ground truth’ is not possible, nor essential in case of such subjective tasks. Instead, we should investigate how to model the subjective interpretations of the annotators, and how to account for them in application scenarios.However, the current practice in the NLP community continues to be applying different aggregation strategies to arrive at a single score or label that makes it amenable to train and evaluate supervised machine learning models.
2. [2]:  Passage ID 2: We recruit three graduate students from our academic network, all specialized in natural language processing, to serve as annotators. We provide the input given to the candidates, reference answers, and candidate responses. This format, while similar, is distinct from the judge models’ prompts which additionally require formatted decisions. The human annotators focus solely on the accuracy and relevance of the responses. To ensure impartial evaluations, we anonymize the origin of responses. Annotators do not know which candidate model generated such responses, reducing potential bias linked to model familiarity or reputation. We asked the annotators to score the candidate LLMs outputs on a binary scale: ‘1’ for ‘True’ and ‘0’ for ‘False’ based on alignment with the reference answer and contextual relevance.To ensure a rigorous evaluation, each of the three annotators independently assesses the entire set of outputs generated by each candidate model across all datasets. Specifically,
3. [3]:  Passage ID 3: the annotation quality of one expert annotator in selected tasks. Furthermore, they have trained machine learning classifiers on expert as well as non-expert annotations and reported better system performance for non-experts due to high annotation diversity reducing the annotator bias. However, these observations were exclusively made for natural language tasks.Nowak and Rueger [7] presented a study on inter-coder agreement for image annotation from both crowdsourcing and experts. Human annotators had to label 99 images of the ImageCLEF test data (http://www.imageclef.org/) with respect to 22 concept categories. Some of the categories were mutually exclusive (season, time of day, indoor/outdoor/none). The images were assessed by experts as well as by Mechanical Turk workers. They measure higher agreement for experts, but argue that majority voting filters out noise in non-expert annotations closing the gap to expert annotations of higher quality. A more recent study [2] deals with the
4. [4]:  Passage ID 4: results of a comparison between expert vs. non-expert inter-coder agreements. Furthermore, a performance prediction for the task of concept classification is presented based on inter-coder agreement and training data size as input. In Section 5, the impact of expert vs. non-expert image annotations on person identification performance is investigated. Section 6 concludes the paper and outlines areas for future work.2 Related WorkIn this section, we briefly survey related work for inter-annotator studies conducted for natural language as well as image annotation tasks. Snow et al. [8] have evaluated non-expert annotations by means of Mechanical Turk workers for natural language tasks. Among other experiments, they found that annotations of four non-experts are needed to rival the annotation quality of one expert annotator in selected tasks. Furthermore, they have trained machine learning classifiers on expert as well as non-expert annotations and reported better system
5. [5]:  Passage ID 5: advantage is fading, as models now approach near-human performance on such tasks (Chiang & Lee, 2023; Chen & Ding, 2023), and crowd workers increasingly rely on models for assistance, diminishing the human element in the process (Veselovsky et al., 2023b; a).Human Experts. Expert annotation is a reliable approach for NLP tasks that require domain-specific expertise (e.g., medical or legal domains) and for tasks that demand deep cognitive engagement, such as those requiring training to understand complex guidelines or intelligent, attentive annotators. However, this approach is slow and expensive compared to crowd-sourcing (Snow et al., 2008; Chau et al., 2020), limiting its scalability for the large datasets needed to train modern LLMs.Managing the trade-off between cost-effectiveness and annotation quality is critical, especially in tasks that require domain-specific accuracy (Chau et al., 2020). Maintaining inter-annotator agreement among experts presents an additional