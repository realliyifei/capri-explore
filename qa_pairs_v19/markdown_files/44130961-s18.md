# TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation

## Question

How is inter-annotator agreement assessed in various annotation tasks, and what factors can affect these scores?

## URLs

1. https://ar5iv.org/html/2410.14515. [2410.14515] Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media
2. https://ar5iv.org/html/2301.10684. [2301.10684] Consistency is Key: Disentangling Label Variation in Natural Language Processing with Intra-Annotator Agreement
3. https://ar5iv.org/html/2110.05699. [2110.05699] On Releasing Annotator-Level Labels and Information in Datasets
4. https://ar5iv.org/html/2410.13313. [2410.13313] Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language
5. https://ar5iv.org/html/2410.18889. [2410.18889] Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance
6. https://ar5iv.org/html/2404.15720. [2404.15720] Annotator-Centric Active Learning for Subjective NLP Tasks
7. https://ar5iv.org/html/2408.00880. [2408.00880] Annotator in the Loop: A Case Study of In-Depth Rater Engagement to Create a Bridging Benchmark Dataset
8. https://ar5iv.org/html/2410.08820. [2410.08820] Which Demographics do LLMs Default to During Annotation?
9. https://ar5iv.org/html/2404.09696. [2404.09696] Are Large Language Models Reliable Argument Quality Annotators?Supported by the German Research Foundation (Project Nr. 455911521).

## Answer (I don't know?: ✗)

Inter-annotator agreement (IAA) is a crucial metric in natural language processing (NLP) that assesses the consistency among different annotators when labeling data. It is particularly important because many linguistic tasks involve subjective interpretations, which can lead to variability in annotations. IAA can be evaluated through different methodologies, and several factors can influence the scores obtained.

Firstly, IAA is commonly assessed through metrics that quantify the level of agreement between annotators, such as Cohen's Kappa, Fleiss' Kappa, or Krippendorff's Alpha. These statistics help in understanding the degree of concordance among multiple annotators on a predefined set of labels or scores. In the context of NLP, tasks such as sentiment analysis, emotion detection, and hate speech detection are acknowledged as particularly subjective, making IAA measurement vital to ensure that the labels reflect a consensus among annotators [1] [2].

Research has shown that the reporting of IAA in NLP publications is very minimal, with only about 0.07% of papers documenting these measurements, indicating that it is often overlooked despite its importance [2]. This suggests a need for more consistent practices around IAA reporting, as the lack of IAA data can hinder the evaluation of annotation quality across the field.

Furthermore, the quality of IAA can be affected by various factors, including the expertise of the annotators, the clarity and complexity of the annotation guidelines, and the inherent subjectivity of the task itself. For example, expert annotators who have domain-specific knowledge (like medical or legal terminology) usually provide more reliable annotations compared to non-expert crowd workers, though expert annotation is slower and more costly [3]. The trade-off between achieving a high quality of annotations and the scalability of the data collection process is critical [3]. Additionally, maintaining inter-annotator agreement can be challenging, especially when complex guidelines or subjective criteria are involved, which further complicates the annotation process.

Moreover, some studies have also examined the use of automated tools or models to assist in the annotation process, which might alter the dynamics of human agreement. Evidence suggests that models like GPT-3 can yield varying degrees of correlation with human judgments across different quality attributes, thus indicating that reliance on automated systems may impact the perceived consistency of annotations [4].

In summary, assessing inter-annotator agreement in NLP is complex and depends on a variety of factors including the annotator's expertise, task subjectivity, clarity of guidelines, and the methodologies employed to measure agreement. More comprehensive reporting and understanding of IAA is essential for improving annotation practices and ensuring reliable training data for machine learning models in NLP.

1. [1]:  https://ar5iv.org/html/2110.05699, [2110.05699] On Releasing Annotator-Level Labels and Information in Datasets
2. [2]:  https://ar5iv.org/html/2301.10684, [2301.10684] Consistency is Key: Disentangling Label Variation in Natural Language Processing with Intra-Annotator Agreement
3. [3]:  https://ar5iv.org/html/2410.18889, [2410.18889] Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance
4. [4]:  https://ar5iv.org/html/2404.09696, [2404.09696] Are Large Language Models Reliable Argument Quality Annotators?Supported by the German Research Foundation (Project Nr. 455911521).
5. [5]:  https://ar5iv.org/html/2110.05699, [2110.05699] On Releasing Annotator-Level Labels and Information in Datasets
---
1. [1]:  Passage ID 1: annotators in training tasks.2 BackgroundNLP has a long history of developing techniques to interpret subjective language Wiebe et al. (2004); Alm (2011). While all human judgments embed some degree of subjectivity, some tasks such as sentiment analysis Liu et al. (2010), affect modeling Alm (2008); Liu et al. (2003), emotion detection Hirschberg et al. (2003), and hate speech detection Warner and Hirschberg (2012) are agreed upon as relatively more subjective in nature.As Alm (2011) points out, achieving a single real ‘ground truth’ is not possible, nor essential in case of such subjective tasks. Instead, we should investigate how to model the subjective interpretations of the annotators, and how to account for them in application scenarios.However, the current practice in the NLP community continues to be applying different aggregation strategies to arrive at a single score or label that makes it amenable to train and evaluate supervised machine learning models.
2. [2]:  Passage ID 2: Anthology of the Association for Computational Linguistics (ACL).222https://aclanthology.org/Here, we wish to discover for which tasks and what purposes NLP researchers collect and report on repeat annotations and evidence for how and when repeat items should be presented to annotators.Full details of the review methodology are available in Appendix A.To what extent and why is intra-annotator agreement reported in NLP?When we conducted our study, the search and filtering process returned only 56 relevant publications out of more than 80,000 papers listed in the Anthology.In other words, a tiny fraction (around 0.07%) of computational linguistics and NLP publications in the repository report measurement of intra-annotator agreement.333We acknowledge that intra-annotator agreement is irrelevant to a large proportion of papers, but highlight that the number of publications which report it is nevertheless extremely low.Publication dates of included papers range from 2005 to 2022,
3. [3]:  Passage ID 3: advantage is fading, as models now approach near-human performance on such tasks (Chiang & Lee, 2023; Chen & Ding, 2023), and crowd workers increasingly rely on models for assistance, diminishing the human element in the process (Veselovsky et al., 2023b; a).Human Experts. Expert annotation is a reliable approach for NLP tasks that require domain-specific expertise (e.g., medical or legal domains) and for tasks that demand deep cognitive engagement, such as those requiring training to understand complex guidelines or intelligent, attentive annotators. However, this approach is slow and expensive compared to crowd-sourcing (Snow et al., 2008; Chau et al., 2020), limiting its scalability for the large datasets needed to train modern LLMs.Managing the trade-off between cost-effectiveness and annotation quality is critical, especially in tasks that require domain-specific accuracy (Chau et al., 2020). Maintaining inter-annotator agreement among experts presents an additional
4. [4]:  Passage ID 4: of the task, their results showed a reasonable correlation between highly-trained human assessors and fully automated judgements.Closest to our work is that by Chiang et al. [5], who compared the judgments of GPT-3 on text quality to expert human judgments on a 5-point Likert scale for four quality attributes: grammaticality, cohesiveness, likability, and relevance. Their findings revealed varying degrees of positive correlations between GPT-3 and human judgments, ranging from weak to strong.When compared to existing research, our work pioneers the study of argument quality annotations generated by LLMs. In order to provide a thorough evaluation, we use an inter-annotator agreement metric to assess the consistency of annotations from these models, human experts and novices. This comparison allows us to understand the alignment between LLMs and human annotators, and to determine the potential of using LLMs as argument quality annotators.3 Experimental DesignTo investigate
5. [5]:  Passage ID 5: for Computational Linguistics:EMNLP 2020, pages 3296–3315, Online. Association for ComputationalLinguistics.Nowak and Rüger (2010)Stefanie Nowak and Stefan Rüger. 2010.Howreliable are annotations via crowdsourcing: a study about inter-annotatoragreement for multi-label image annotation.In Proceedings of the international conference on Multimediainformation retrieval, pages 557–566.Patton et al. (2019)Desmond Patton, Philipp Blandfort, William Frey, Michael Gaskell, and SveborKaraman. 2019.Annotating social media data from vulnerable populations: Evaluatingdisagreement between domain experts and graduate student annotators.In Proceedings of the 52nd Hawaii International Conference onSystem Sciences.Plank et al. (2014)Barbara Plank, Dirk Hovy, and Anders Søgaard. 2014.Learning part-of-speech taggers with inter-annotator agreement loss.In Proceedings of the 14th Conference of the European Chapterof the Association for Computational