# Survey of Low-Resource Machine Translation

## Question

How have morphological analyzers been used in subword segmentation for low-resource languages, and what have been the results?

## URLs

1. https://ar5iv.org/html/2403.20157. [2403.20157] Subword Segmentation for Multilingual Translation: Analysing Cross-Lingual Synergy, Interference, and Transfer
2. https://ar5iv.org/html/2404.02392. [2404.02392] Low-resource neural machine translation with morphological modeling
3. https://ar5iv.org/html/2410.11627. [2410.11627] Tokenization and Morphology in Multilingual Language Models: A Comparative Analysis of mT5 and ByT5
4. https://ar5iv.org/html/2410.23656. [2410.23656] Morphological Typology in BPE Subword Productivity and Language Modeling
5. https://ar5iv.org/html/2406.05424. [2406.05424] Recent advancements in computational morphology : A comprehensive survey
6. https://ar5iv.org/html/2411.14198. [2411.14198] Why do language models perform worse for morphologically complex languages?
7. https://ar5iv.org/html/2406.13560. [2406.13560] Lexically Grounded Subword Segmentation
8. https://ar5iv.org/html/2410.17094. [2410.17094] Team Ryu’s Submission to SIGMORPHON 2024 Shared Task on Subword Tokenization
9. https://ar5iv.org/html/2012.15079. [2012.15079] A Subword Guided Neural Word Segmentation Model for Sindhi
10. https://ar5iv.org/html/2307.07262. [2307.07262] MorphPiece : A Linguistic Tokenizer for Large Language Models

## Answer (I don't know?: ✗)

Morphological analyzers have been employed in subword segmentation for low-resource languages primarily as a means to improve tokenization by leveraging the morphological richness of these languages. Their use often occurs in conjunction with other tokenization methods to create more effective and data-efficient language models, particularly in settings where data scarcity is a significant challenge.

One approach discussed in the literature involves the use of unsupervised morphological analyzers like Morfessor during the pre-tokenization phase. This initial step assists in segmenting words into their morphologically grounded components, which can provide a better foundation for subsequent tokenization efforts [5]. However, it has been observed that relying solely on these linguistically motivated tools may lead to inferior performance compared to well-established statistical methods [5]. Particularly, in low-resource scenarios, while morphological analysis can enrich the tokenization process, it may still struggle with the challenges posed by rare tokens and overall vocabulary size [5]. Therefore, a hybrid strategy is often suggested, where morphological analyzers are utilized in the earlier stages, followed by traditional statistical methods for vocabulary construction to maintain efficiency [5].

In the context of specific studies, Mager et al. (2022) evaluated various morphological segmentation methods and found that both supervised and unsupervised approaches generally outperform traditional methods like Byte-Pair Encoding (BPE) in low-resource settings for tasks including machine translation and morphological segmentation [2]. This suggests that integrating morphological analysis can significantly enhance performance in certain contexts, leading to more robust models for languages with complex morphology.

Additionally, the research emphasizes the limitations of BPE and similar methods when faced with morphologically rich languages (MRLs). These methods often fail to capture the nuances found in morphologically derived forms, which can exclude important linguistic units such as morphemes and limit the model's understanding of the language [3]. As a result, leveraging morphological analyzers in conjunction with other tokenization strategies becomes crucial for achieving better performance in NMT (Neural Machine Translation) systems [3].

In conclusion, while morphological analyzers serve a valuable role in enhancing subword segmentation for low-resource languages, they are most effective when combined with other methodologies. They aid in preserving lexical meaning and reducing the number of rare tokens, yet challenges remain regarding their standalone efficacy and the complexities of vocabulary management [5]. These findings indicate a need for continued exploration of integrated approaches to optimize tokenization processes in low-resource scenarios.

1. [1]:  https://ar5iv.org/html/2410.17094, [2410.17094] Team Ryu’s Submission to SIGMORPHON 2024 Shared Task on Subword Tokenization
2. [2]:  https://ar5iv.org/html/2410.17094, [2410.17094] Team Ryu’s Submission to SIGMORPHON 2024 Shared Task on Subword Tokenization
3. [3]:  https://ar5iv.org/html/2404.02392, [2404.02392] Low-resource neural machine translation with morphological modeling
4. [4]:  https://ar5iv.org/html/2406.05424, [2406.05424] Recent advancements in computational morphology : A comprehensive survey
5. [5]:  https://ar5iv.org/html/2406.13560, [2406.13560] Lexically Grounded Subword Segmentation
---
1. [1]:  Passage ID 1: language learners (Gilkerson et al., 2017). Here people come across the question, whether we can build a more data efficient language model, so that it can achieve relatively good performance with a smaller data consumption. With a data efficient LM, it is easier for people to explore the influence of subword tokenization, as a small size of training data is more sensitive to the method of tokenization.This paper describes my submission to the SIGMORPHON 2024 Subword Tokenization shared task. Participants are asked to develop a subword tokenization system and use it to pretrain a language model on the 100M word tokens dataset from the BabyLM challenge (Warstadt et al., 2023). The performance of pretrained model is evaluated by model’s predictions after fine-tuning it on three different subtasks: Word and Definition, Word and Morphology and Word and Word. In this paper, I introduce two subword tokenization systems with their variants: one based on a statistics-based morphological
2. [2]:  Passage ID 2: subtasks: Word and Definition, Word and Morphology and Word and Word. In this paper, I introduce two subword tokenization systems with their variants: one based on a statistics-based morphological segmentation method and the other based on a neural seq2seq model. This paper basically describes my systems and seeks to verify conclusions of other studies based on my discoveries.2 Related work2.1 Subword TokenizationAlthough currently frequently used subword tokenization methods such as Byte-Pair Encoding (BPE), WordPiece, Unigram and SentencePiece have proven effective through the success of various LLMs, the source of their effectiveness is still unclear. Mager et al. (2022) compared the performance of different morphological segmentation methods with the BPE in four low-resource languages on machine translation and morphological segmentation tasks. They tested both unsupervised methods and supervised methods. They found that both of them outperform the BPE in the task of
3. [3]:  Passage ID 3: information and the proposed model and data augmentations in low-resource NMT.Low-resource neural machine translation with morphological modelingAntoine NzeyimanaUniversity of Massachusetts Amherstanthonzeyi@gmail.com1 IntroductionNeural Machine Translation (NMT) has become a predominant approach in developing machine translation systems. Two important innovations in recent state-of-the-art NMT systems are the use of the Transformer architecture Vaswani et al. (2017) and sub-word tokenization methods such as byte-pair encoding (BPE) Sennrich et al. (2016). However, for morphologically-rich languages(MRLs), BPE-based tokenization is only limited to the surface forms of the words and less grounded on exact lexical units (i.e. morphemes), especially in the presence morphographemic alternations Bundy and Wallen (1984) and non-concatenative morphology Kastner et al. (2019). In this work, we tackle the challenge of modeling complex morphology in low-resource NMT and
4. [4]:  Passage ID 4: the performance especially in the low resource settings. We conclude by highlighting some open research issues in the field. This survey article can be a good starting point for overall understanding of the computational morphology field and different approaches applied so far for solving various sub problems.References\bibcommentheadAbeera et al [2010]Abeera V, Aparna S, Rekha R, et al (2010) Morphological analyzer for malayalam using machine learning. In: International Conference on Data Engineering and Management, Springer, pp 252–254Ak and Yildiz [2012]Ak K, Yildiz OT (2012) Unsupervised Morphological Analysis Using Tries. Computer and Information Sciences II pp 69–75. 10.1007/978-1-4471-2155-8Akyürek et al [2019]Akyürek E, Dayanık E, Yuret D (2019) Morphological analysis using a sequence decoder. Transactions of the Association for Computational Linguistics 7:567–579. 10.1162/tacl_a_00286, URL https://aclanthology.org/Q19-1036Aronoff and Fudeman
5. [5]:  Passage ID 5: approach for lexically grounded word segmentation is touse unsupervised morphological analyzers, such as Morfessor. However, direct use of these linguistically motivated tools leads to worse results (machacek2018morphological) and is only beneficial in low-resource scenarios (soulos-etal-2021-structural; gaser-etal-2023-exploring). Furthermore, morphological analysis does not fully address the problems of rare tokens and vocabulary size.To address these issues, we propose to only use morphological analyzers in during pre-tokenization (Step 1 in Figure 1). After pre-tokenization, we apply the well-established statistical methods for vocabulary construction. This combination ensures that there will be a low number of rare tokens and efficient control of vocabulary size while still preserving the lexical meaning of the subwords.3 Segmentation with Subword EmbeddingsIn this section, we describe a novel lexically-grounded segmentation method (Step 2 in Figure 1).When considering