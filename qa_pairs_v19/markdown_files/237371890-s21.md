# Survey of Low-Resource Machine Translation

## Question

How have morphological analyzers been used in subword segmentation for low-resource languages, and what have been the results?

## URLs

1. https://ar5iv.org/html/2403.20157
2. https://ar5iv.org/html/2404.02392
3. https://ar5iv.org/html/2410.11627
4. https://ar5iv.org/html/2410.23656
5. https://ar5iv.org/html/2406.05424
6. https://ar5iv.org/html/2411.14198
7. https://ar5iv.org/html/2406.13560
8. https://ar5iv.org/html/2410.17094
9. https://ar5iv.org/html/2012.15079
10. https://ar5iv.org/html/2307.07262

## Answer

## I don't know?: False

Morphological analyzers have been utilized in the context of subword segmentation for low-resource languages primarily through their incorporation into various tokenization systems and methodologies. These systems aim to improve the handling of morphologically rich languages by segmenting words into meaningful morphemes (the smallest units of meaning) rather than relying purely on surface forms. This approach is particularly important for low-resource settings where traditional tokenization methods, such as Byte-Pair Encoding (BPE), may fall short.

Studies indicate that morphological segmentation methods can provide significant advantages over traditional BPE. For example, Mager et al. (2022) found that both supervised and unsupervised morphological segmentation methods outperformed BPE in specific tasks, suggesting that capturing the morphological structure of words leads to more effective machine translation and other natural language processing tasks [2][3]. This outcome emphasizes the importance of understanding and leveraging the underlying morphological characteristics of languages, particularly in low-resource scenarios.

In particular, the effectiveness of morphological analyzers as part of the segmentation process has highlighted a few key benefits. Firstly, by using high-quality morphological segmentation methods, models can potentially achieve higher machine translation scores for morphologically rich languages, which often possess complex inflectional and derivational processes [3]. This is vital, as low-resource languages often exhibit diverse morphological phenomena that require specialized processing techniques to handle effectively.

Furthermore, the research indicates a correlation between the quality of morphological segmentation and the performance of machine translation systems. The study presented segmentation experiments on polysynthetic languages like Nahuatl and Raramuri, emphasizing the need for quality morphological segmentation to improve translation accuracy [3]. Through this correlation, it becomes evident that more sophisticated morphological modeling approaches can enhance the capabilities of neural machine translation systems in low-resource contexts.

Moreover, subword information derived from morphological analyzers plays a crucial role in advancing cross-lingual word embeddings, especially for low-resource languages. Subword-level models can learn shared representations for related words across different languages, effectively addressing the challenges posed by limited training data [4]. For instance, word embeddings that capture morphological similarities enable models to leverage the inflectional features typical of certain languages, which aids in better generalization and performance during language tasks.

In summary, morphological analyzers in subword segmentation for low-resource languages have been shown to yield superior results by mitigating the limitations of surface-form-based approaches like BPE. The benefits include enhanced translation performance, improved modeling of complex morphology, and the facilitation of cross-lingual word embeddings. These findings underscore the importance of employing robust morphological analyses to advance natural language processing efforts within low-resource linguistic contexts [2][4].

[1]: https://ar5iv.org/html/2410.17094, [2410.17094] Team Ryu’s Submission to SIGMORPHON 2024 Shared Task on Subword Tokenization
[2]: https://ar5iv.org/html/2410.17094, [2410.17094] Team Ryu’s Submission to SIGMORPHON 2024 Shared Task on Subword Tokenization
[3]: https://ar5iv.org/html/2203.08954, [2203.08954] BPE vs. Morphological Segmentation: A Case Study on Machine Translation of Four Polysynthetic Languages
[4]: https://ar5iv.org/html/2411.05036, No Title
[5]: https://ar5iv.org/html/2404.02392, [2404.02392] Low-resource neural machine translation with morphological modeling

[1]: Passage ID 1: language learners (Gilkerson et al., 2017). Here people come across the question, whether we can build a more data efficient language model, so that it can achieve relatively good performance with a smaller data consumption. With a data efficient LM, it is easier for people to explore the influence of subword tokenization, as a small size of training data is more sensitive to the method of tokenization.This paper describes my submission to the SIGMORPHON 2024 Subword Tokenization shared task. Participants are asked to develop a subword tokenization system and use it to pretrain a language model on the 100M word tokens dataset from the BabyLM challenge (Warstadt et al., 2023). The performance of pretrained model is evaluated by model’s predictions after fine-tuning it on three different subtasks: Word and Definition, Word and Morphology and Word and Word. In this paper, I introduce two subword tokenization systems with their variants: one based on a statistics-based morphological
[2]: Passage ID 2: subtasks: Word and Definition, Word and Morphology and Word and Word. In this paper, I introduce two subword tokenization systems with their variants: one based on a statistics-based morphological segmentation method and the other based on a neural seq2seq model. This paper basically describes my systems and seeks to verify conclusions of other studies based on my discoveries.2 Related work2.1 Subword TokenizationAlthough currently frequently used subword tokenization methods such as Byte-Pair Encoding (BPE), WordPiece, Unigram and SentencePiece have proven effective through the success of various LLMs, the source of their effectiveness is still unclear. Mager et al. (2022) compared the performance of different morphological segmentation methods with the BPE in four low-resource languages on machine translation and morphological segmentation tasks. They tested both unsupervised methods and supervised methods. They found that both of them outperform the BPE in the task of
[3]: Passage ID 3: Their results show that BPEs outperform Morfessor and the morphological analyzer in all MT cases (but with better Language Modeling capabilities of morphological models over BPEs).However, most of these studies only rely on the usage of a limited set of segmentation methods and do not consider the quality of the used morphological segmentation methods.This study aims to answer the following research questions: i) is morphological segmentation beneficial for MT where one language is polysynthetic and ELR?; and ii) is higher morphological segmentation quality correlated with higher MT scores?To answer these questions, we perform segmentation experiments on four polysynthetic languages:111We choose the languages for this study based on the availability of a morphological segmentation dataset. Nahuatl (nah), Raramuri (tar), Shipibo-Konibo (shp) and Wixarika (hch) and apply those segmentations to MT paired with Spanish (spa). First, we revisit a wide set of supervised and
[4]: Passage ID 4: models, including subword-level approaches, and their applications across various NLP tasks.II-D2 Cross-Lingual Word Embeddings and Low-Resource LanguagesSubword information is instrumental in advancing cross-lingual word embeddings, particularly for low-resource languages with limited training data. By capturing morphological similarities across languages, subword-level models can learn shared representations for morphologically related words, even when they exhibit different surface forms. [27] explored universal and language-specific properties within word embeddings, revealing that word form features are particularly beneficial for inflectional languages. Similarly, [28] examined cross-lingual word embeddings derived from bilingual lexicons to enhance language models for low-resource languages. Applying this approach to Yongning Na, they highlighted challenges and potential solutions for low-resource settings.Furthermore, [29] developed a language-agnostic BERT model,
[5]: Passage ID 5: information and the proposed model and data augmentations in low-resource NMT.Low-resource neural machine translation with morphological modelingAntoine NzeyimanaUniversity of Massachusetts Amherstanthonzeyi@gmail.com1 IntroductionNeural Machine Translation (NMT) has become a predominant approach in developing machine translation systems. Two important innovations in recent state-of-the-art NMT systems are the use of the Transformer architecture Vaswani et al. (2017) and sub-word tokenization methods such as byte-pair encoding (BPE) Sennrich et al. (2016). However, for morphologically-rich languages(MRLs), BPE-based tokenization is only limited to the surface forms of the words and less grounded on exact lexical units (i.e. morphemes), especially in the presence morphographemic alternations Bundy and Wallen (1984) and non-concatenative morphology Kastner et al. (2019). In this work, we tackle the challenge of modeling complex morphology in low-resource NMT and