# Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

How do statistical association measures determine the strength of word associations in the discovery of multiword expressions (MWEs)?

## URLs

1. https://ar5iv.org/html/2011.02541. [2011.02541] MTLB-STRUCT @PARSEME 2020: Capturing Unseen Multiword Expressions Using Multi-task Learning and Pre-trained Masked Language Models
2. https://ar5iv.org/html/1703.08748. [1703.08748] LEPOR: An Augmented Machine Translation Evaluation Metric
3. https://ar5iv.org/html/1611.09799. [1611.09799] Geometry of Compositionality
4. https://ar5iv.org/html/2403.13106. [2403.13106] Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data

## Answer (I don't know?: ‚úì)

I don't know. The context provided does not include specific information regarding how statistical association measures determine the strength of word associations in the discovery of multiword expressions (MWEs). While it describes certain aspects of MWEs and their interactions, such as the effectiveness of models in capturing MWEs and the limitations of earlier systems [1] [4], it does not elaborate on statistical association measures or their application in discovering MWEs. Therefore, it lacks the necessary information to adequately answer the question.

1. [1]:  https://ar5iv.org/html/2403.13106, [2403.13106] Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data
2. [2]:  https://ar5iv.org/html/2403.13106, [2403.13106] Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data
3. [3]:  https://ar5iv.org/html/1611.09799, [1611.09799] Geometry of Compositionality
4. [4]:  https://ar5iv.org/html/2011.02541, [2011.02541] MTLB-STRUCT @PARSEME 2020: Capturing Unseen Multiword Expressions Using Multi-task Learning and Pre-trained Masked Language Models
5. [5]:  https://ar5iv.org/html/2403.13106, [2403.13106] Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data
---
1. [1]:  Passage ID 1: language modeling objective itself.3.3 Multiword Expressions(a) ALM experiments(b) MLM experimentsFigure 3: Each plot represents the trend in STII for a given prediction distance dpsubscriptùëëùëùd_{p}, responding to the x-axis of interacting pair distance disubscriptùëëùëñd_{i}. Trend is given for the average over: all pairs, strong MWE relations, and weak MWE relations.Classical treatments of semantics are compositional, implying that the meaning of a sentence is derived by composing the meanings of each individual word. However, there are groups of words whose meaning can only be derived when looking at the entire group rather than the individual words. These word groups, known as multiword expressions (MWEs), include idioms like break a leg, where the isolated meaning of each of the component words break, a, and leg fail to compose the meaning of the entire expression. Higher interaction values for the tokens in the idiom would indicate a less compositional
2. [2]:  Passage ID 2: the ALM (Figure¬†3(a)), token pairs in an MWE are associated with stronger interaction than average pairs when the interacting pair and prediction distances are higher, suggesting that at close distances, other factors dominate MWE membership to determine the level of non-linear interaction between the pair.For the MLM (Figure¬†3(b)), STII is higher when the interacting pair is in a MWE. However, unlike the ALM trend, this effect is consistent across positional distances and more pronounced when predicting nearby tokens. We conjecture that this result is connected to the hierarchical structure learned by MLMs, as validated by our syntax experiments.Appendix¬†A adds nuance by contrasting tokens that share a MWE with those sharing a word. Although the MLM increases interaction between MWE tokens, the model decreases interaction between tokens within the same word. This effect is present in multiple languages‚Äîeven in synthetic languages which compose words from long sequences of
3. [3]:  Passage ID 3: state of the art methods on many metrics, while being competitive on the others.We use three standard datasets spanning two MWE construction types (noun compounds and verb particle constructions) in two languages (English and German) in addition to a dataset in Chinese (heretofore unexplored language), and standard datasets for detection of metaphor and sarcasm in addition to a new dataset for sarcasm detection from Twitter. We summarize our contributions:Compositional Geometry: We show that a word (or MWE) and its context are geometrically related as jointly lying in a linear subspace, when it appears in a compositional sense, but not otherwise.Compositionality decision: The only input to the algorithm is a set of trained word vectors after the preprocessing step of removing function words on which, the algorithm performs a simple principle component analysis (PCA) operation.Multi-lingual applicability: The algorithm is very general, relies on no external resources and is agnostic
4. [4]:  Passage ID 4: CRF network [Rush (2020].Our experiments confirm that this joint learning architecture is effective for capturing MWEs in most languages represented in the shared task.¬†222The code for the system and configuration files for different languages are available at https://github.com/shivaat/MTLB-STRUCT/2 Related WorkIn earlier systems, MWEs were extracted using pre-defined patterns or statistical measures that either indicated associations among MWE components or (non-)compositionality of the expressions with regard to the components [Ramisch et al. (2010]. For example, ?) employed such a system for identifying MWEs. While these models can be effective for some frequent MWEs, their main disadvantage is that they capture MWE types (as opposed to tokens) and they are unable to take context into account in running texts.The use of supervised machine learning was facilitated by the availability of resources tagged for MWEs [Schneider et al. (2014, Savary et al. (2017, Ramisch et al.
5. [5]:  Passage ID 5: MWE tokens, the model decreases interaction between tokens within the same word. This effect is present in multiple languages‚Äîeven in synthetic languages which compose words from long sequences of morphemes‚Äîand our investigation suggests that same-word interaction scores are lower because subword tokens are predictable in context. In contrast, MWEs are often sequences of common words which change meaning within the context of an idiom.4 Automated Speech RecognitionWhen interpreting interactions in speech models‚Äîin this case, predictive models trained on speech‚Äîwe consider the structure imposed by the phonology that mediates the conversion of intended words to acoustic signal. The study of phonology is based on distinctions between different phonemes, such as the sounds represented by s or ee in English text. These phonemes are the intended sound, but any actual sound produced in context is called a phone, and its acoustic features are dictated by the shape of the vocal tract