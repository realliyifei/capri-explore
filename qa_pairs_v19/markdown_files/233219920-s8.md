# Relational World Knowledge Representation in Contextual Language Models: A Review

## Question

How are external entity embeddings integrated into BERT models for entity linking, and what are the strengths and drawbacks of this approach?

## URLs

1. https://ar5iv.org/html/2305.09329. No content available
2. https://ar5iv.org/html/2006.00575. [2006.00575] Neural Entity Linking: A Survey of Models Based on Deep Learning
3. https://ar5iv.org/html/1909.04164. [1909.04164] Knowledge Enhanced Contextual Word Representations
4. https://ar5iv.org/html/2109.12520. [2109.12520] Entity Linking Meets Deep Learning: Techniques and Solutions
5. https://ar5iv.org/html/2404.11968. [2404.11968] P-NAL: an Effective and Interpretable Entity Alignment Method
6. https://ar5iv.org/html/2402.15048. [2402.15048] Unlocking the Power of Large Language Models for Entity Alignment
7. https://ar5iv.org/html/2004.07202. [2004.07202] Entities as Experts: Sparse Memory Access with Entity Supervision
8. https://ar5iv.org/html/2407.00936. [2407.00936] Large Language Model Enhanced Knowledge Representation Learning: A Survey
9. https://ar5iv.org/html/2404.11061. [2404.11061] Unified Examination of Entity Linking in Absence of Candidate Sets
10. https://ar5iv.org/html/2309.14084. [2309.14084] Comprehensive Overview of Named Entity Recognition: Models, Domain-Specific Applications and Challenges

## Answer (I don't know?: ✗)

External entity embeddings are integrated into BERT models for entity linking through various methodologies that enhance the language model's ability to connect unstructured text with structured knowledge representations. One prominent approach involves injecting entity representations directly into the text being analyzed. For instance, Poerner et al. (2020) present an E-BERT model that integrates entity representations by mixing them with word embeddings while employing a linear transformation to align pre-trained entity representations with the word piece tokens of BERT. This adjustment allows E-BERT to outperform baselines in tasks such as unsupervised question answering and supervised relation classification, demonstrating the model's effectiveness in knowledge-oriented tasks [4].

Another method involves the use of knowledge-enhanced representations, such as in the KnowBERT model. This model combines contextual word representations with retrieved entity embeddings from knowledge bases (KBs) like WordNet and subsets of Wikipedia. The integration is achieved through word-to-entity attention that updates the word representations while training jointly with an entity linker in a multitask setting. This approach has been shown to improve performance on entity typing, word sense disambiguation, and overall ability to recall facts from the integrated knowledge [2][5].

The strengths of integrating external entity embeddings into BERT models include enhanced contextual understanding, improved accuracy in entity recognition and linking, and greater flexibility in handling various NLP tasks. Models like the E-BERT and KnowBERT leverage the structured knowledge embedded in KBs, leading to superior performance in tasks that rely heavily on factual knowledge [4][5]. The joint optimization of entity linking with language modeling also allows these models to incorporate both external knowledge and linguistic context harmoniously, which is beneficial for tasks such as question answering and relation extraction [4].

However, there are also drawbacks to this approach. One concern is the computational expense linked with training models that utilize external embeddings, as the integration of additional knowledge bases can increase both the training time and resource requirements [1]. Furthermore, reliance on structured KBs may limit the model's performance in cases where relevant knowledge is not represented or if the KB is incomplete. The need to ensure alignment between the entities in a knowledge base and the words in the text introduces challenges in generalization, particularly in zero-shot scenarios where the system encounters novel entities [3].

Additionally, while integrating entity embeddings can enhance model performance, it also adds complexity to the architecture, which may result in difficulties during deployment and scalability. As these models can become large and intricate due to the multiple layers of knowledge integration, practical considerations such as latency and memory footprint in real-world applications also need to be accounted for [4]. 

Overall, while external entity embeddings significantly enrich BERT models, a careful balance must be maintained between their strengths in improving comprehension and the complexities they introduce.

1. [1]:  https://ar5iv.org/html/2109.12520, [2109.12520] Entity Linking Meets Deep Learning: Techniques and Solutions
2. [2]:  https://ar5iv.org/html/1909.04164, [1909.04164] Knowledge Enhanced Contextual Word Representations
3. [3]:  https://ar5iv.org/html/2006.00575, [2006.00575] Neural Entity Linking: A Survey of Models Based on Deep Learning
4. [4]:  https://ar5iv.org/html/2006.00575, [2006.00575] Neural Entity Linking: A Survey of Models Based on Deep Learning
5. [5]:  https://ar5iv.org/html/2401.11972, No Title
---
1. [1]:  Passage ID 1: that jointly discover and link entities in the news documents. Li et al. [69] designed an end-to-end EL system used for downstream question answering systems. In summary, we consider it is worth exploring effective approaches for jointly performing NER and EL for real applications in the future.More advanced language models.Neural language models have revolutionized the field of NLP due to their superior expressive power. As introduced earlier, many effective language models have been applied in the field of EL and achieved great success. Recently, there are many more advanced language models being developed and available. For instance, BERT [83], widely leveraged by existing EL works, has been exceeded by several variants and other transformer-based models, which made major changes to loss functions, model architecture, and pre-training objectives. Specifically, RoBERTa [123] is more robust than BERT which is trained using much more training data and leveraging dynamic masking
2. [2]:  Passage ID 2: multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge.For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention.In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text.After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation.KnowBert’s runtime is comparable to BERT’s and it scales to large KBs.1 IntroductionLarge pretrained models such as ELMo Peters et al.
3. [3]:  Passage ID 3: et al. (2021) based on BART. Among published local models for disambiguation, the best results are reported by Shahbazi et al. (2019) and Wu et al. (2020b). The former solution leverages entity-aware ELMo (E-ELMo) trained to additionally predict entities along with words as in language-modelling task. The latter solution is based on a BERT bi-/cross-encoder and can be used in the zero-shot setting. Yamada et al. (2021) report results that are consistently better in comparison to all other solutions. Their high scores are attributed to the masked entity prediction mechanism for entity embedding and the usage of the pre-trained model based on BERT with a multi-step global scoring function.7 Future DirectionsWe identify five promising directions of future work in entity linking listed below:1.More end-to-end models without an explicit candidate generation step:The candidate generation step relies on pre-constructed external resources or heuristics, as discussed in Section
4. [4]:  Passage ID 4: knowledge than a comparably-sized BERT model.Poerner et al. (2020) present an E-BERT language model that also takes advantage of entity representations. This model is close to Zhang et al. (2019) as it also injects entities directly into the text and mixes entity representations with word embeddings in a similar way. However, instead of updating the weights of the whole pre-trained language model, they train only a linear transformation for aligning pre-trained entity representations with representations of word piece tokens of BERT. Such a small modification helps this model to outperform baselines on unsupervised question answering, supervised relation classification, and end-to-end entity linking.The considered works demonstrate that the integration of structured KGs and LMs usually helps to solve knowledge-oriented tasks: question answering (including open-domain QA), entity typing, relation extraction, and others. A high-precision supervision signal from KGs either leads to
5. [5]:  Passage ID 5: and obtained their embedding representation from entity descriptions. The resulting entity embedding was injected into the language model to generate the knowledge-enhanced representation of the text, and both entity linking and language models were jointly optimized. The authors observed an increase in the ability to recall facts in the resulting model called KnowBERT. Instead of linking entities,Ji et al. (2020) extracted the relevant subgraph from a knowledge graph using the Multi-hop technique and input their embedding representation by aggregating the node embedding obtained via a graph neural network. This concept representation was combined with the output of the encoder for predicting the next word.Liu et al. (2021) extended this idea by modifying the encoder-decoder architecture with a dedicated encoder and decoder augmented with an embedding representation obtained from the knowledge graph. Further, a dedicated convolutional neural network was used to generate the vector