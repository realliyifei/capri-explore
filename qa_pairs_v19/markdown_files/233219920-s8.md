# Relational World Knowledge Representation in Contextual Language Models: A Review

## Question

How are external entity embeddings integrated into BERT models for entity linking, and what are the strengths and drawbacks of this approach?

## URLs

1. https://ar5iv.org/html/2305.09329. No content available
2. https://ar5iv.org/html/2006.00575. [2006.00575] Neural Entity Linking: A Survey of Models Based on Deep Learning
3. https://ar5iv.org/html/1909.04164. [1909.04164] Knowledge Enhanced Contextual Word Representations
4. https://ar5iv.org/html/2109.12520. [2109.12520] Entity Linking Meets Deep Learning: Techniques and Solutions
5. https://ar5iv.org/html/2404.11968. [2404.11968] P-NAL: an Effective and Interpretable Entity Alignment Method
6. https://ar5iv.org/html/2402.15048. [2402.15048] Unlocking the Power of Large Language Models for Entity Alignment
7. https://ar5iv.org/html/2004.07202. [2004.07202] Entities as Experts: Sparse Memory Access with Entity Supervision
8. https://ar5iv.org/html/2407.00936. [2407.00936] Large Language Model Enhanced Knowledge Representation Learning: A Survey
9. https://ar5iv.org/html/2404.11061. [2404.11061] Unified Examination of Entity Linking in Absence of Candidate Sets
10. https://ar5iv.org/html/2309.14084. [2309.14084] Comprehensive Overview of Named Entity Recognition: Models, Domain-Specific Applications and Challenges

## Answer (I don't know?: ✗)

External entity embeddings are integrated into BERT models for entity linking through a process that merges entity representations with word embeddings. Specifically, one method involves using models like KnowBERT, which enhances BERT by incorporating external knowledge bases (KBs) such as WordNet and Wikipedia. This integration is achieved by retrieving relevant entity embeddings using an integrated entity linker and then updating the contextual word representations through a mechanism termed word-to-entity attention [3]. The incorporation of structured, human-curated knowledge allows for improved performance in various NLP tasks, including relationship extraction, entity typing, and word sense disambiguation [3].

One key strength of these approaches lies in their ability to improve the model's performance on knowledge-oriented tasks. For instance, the KnowBERT model demonstrates enhanced perplexity and better recall of facts in probing tasks when compared to standard BERT, suggesting that the integration of external knowledge can lead to significant improvements in understanding and processing language [3] [5]. Furthermore, these models retain a runtime comparable to that of the original BERT model, signifying that the added complexity does not drastically impact efficiency [3].

However, there are also drawbacks to this integration method. While there is a noted improvement in performance, integrating external embeddings may introduce complexity that can make the models harder to train and optimize. The process depends on the quality and relevance of the external knowledge bases used; if the KBs contain inaccuracies or irrelevant information, this could negatively impact the outcomes in entity linking tasks. Additionally, relying on external resources may limit the model's applicability to scenarios where such resources are not available or are poor in quality [4]. 

Another consideration is the trade-off between fine-tuning and maintaining the integrity of the pre-trained model. For example, the E-BERT approach updates only a linear transformation to align the representations rather than modifying the entire pre-trained model [5]. While this specialization can yield better performance in specific tasks like unsupervised question answering and supervised relation classification, it raises questions about the generalizability of the resultant model to a broader range of tasks [5].

In summary, the integration of external entity embeddings into BERT models for entity linking enhances performance through enriched contextual representations and attention mechanisms but may introduce complexities and dependencies on the quality of external knowledge sources, which needs to be considered in practical applications [3] [5].

1. [1]:  https://ar5iv.org/html/2109.12520, [2109.12520] Entity Linking Meets Deep Learning: Techniques and Solutions
2. [2]:  https://ar5iv.org/html/2006.00575, [2006.00575] Neural Entity Linking: A Survey of Models Based on Deep Learning
3. [3]:  https://ar5iv.org/html/1909.04164, [1909.04164] Knowledge Enhanced Contextual Word Representations
4. [4]:  https://ar5iv.org/html/2006.00575, [2006.00575] Neural Entity Linking: A Survey of Models Based on Deep Learning
5. [5]:  https://ar5iv.org/html/2006.00575, [2006.00575] Neural Entity Linking: A Survey of Models Based on Deep Learning
---
1. [1]:  Passage ID 1: that jointly discover and link entities in the news documents. Li et al. [69] designed an end-to-end EL system used for downstream question answering systems. In summary, we consider it is worth exploring effective approaches for jointly performing NER and EL for real applications in the future.More advanced language models.Neural language models have revolutionized the field of NLP due to their superior expressive power. As introduced earlier, many effective language models have been applied in the field of EL and achieved great success. Recently, there are many more advanced language models being developed and available. For instance, BERT [83], widely leveraged by existing EL works, has been exceeded by several variants and other transformer-based models, which made major changes to loss functions, model architecture, and pre-training objectives. Specifically, RoBERTa [123] is more robust than BERT which is trained using much more training data and leveraging dynamic masking
2. [2]:  Passage ID 2: text processing, or semantic parsing and question answering (see Section 5). This wide range of direct applications is the reason why entity linking is enjoying great interest from both academy and industry for more than two decades.1.1 Goal and Scope of this SurveyRecently, a new generation of approaches for entity linking based on neural models and deep learning emerged, pushing the state-of-the-art performance in this task to a new level. The goal of our survey is to provide an overview of this latest wave of models, emerging from 2015.Models based on neural networks have managed to excel in EL as in many other natural language processing tasks due to their ability to learn useful distributed semantic representations of linguistic data [Collobert et al. (2011), Young et al. (2018), Bengio et al. (2003)]. These current state-of-the-art neural entity linking models have shown significant improvements over “classical”333On classical ML vs deep learning:
3. [3]:  Passage ID 3: multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge.For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention.In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text.After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation.KnowBert’s runtime is comparable to BERT’s and it scales to large KBs.1 IntroductionLarge pretrained models such as ELMo Peters et al.
4. [4]:  Passage ID 4: et al. (2021) based on BART. Among published local models for disambiguation, the best results are reported by Shahbazi et al. (2019) and Wu et al. (2020b). The former solution leverages entity-aware ELMo (E-ELMo) trained to additionally predict entities along with words as in language-modelling task. The latter solution is based on a BERT bi-/cross-encoder and can be used in the zero-shot setting. Yamada et al. (2021) report results that are consistently better in comparison to all other solutions. Their high scores are attributed to the masked entity prediction mechanism for entity embedding and the usage of the pre-trained model based on BERT with a multi-step global scoring function.7 Future DirectionsWe identify five promising directions of future work in entity linking listed below:1.More end-to-end models without an explicit candidate generation step:The candidate generation step relies on pre-constructed external resources or heuristics, as discussed in Section
5. [5]:  Passage ID 5: knowledge than a comparably-sized BERT model.Poerner et al. (2020) present an E-BERT language model that also takes advantage of entity representations. This model is close to Zhang et al. (2019) as it also injects entities directly into the text and mixes entity representations with word embeddings in a similar way. However, instead of updating the weights of the whole pre-trained language model, they train only a linear transformation for aligning pre-trained entity representations with representations of word piece tokens of BERT. Such a small modification helps this model to outperform baselines on unsupervised question answering, supervised relation classification, and end-to-end entity linking.The considered works demonstrate that the integration of structured KGs and LMs usually helps to solve knowledge-oriented tasks: question answering (including open-domain QA), entity typing, relation extraction, and others. A high-precision supervision signal from KGs either leads to