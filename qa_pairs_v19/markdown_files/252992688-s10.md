# A Survey of Active Learning for Natural Language Processing

## Question

What are the common strategies for selecting a diverse batch of instances in active learning?

## URLs

1. https://ar5iv.org/html/2403.03728. [2403.03728] Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training
2. https://ar5iv.org/html/2312.15944. [2312.15944] BAL: Balancing Diversity and Novelty for Active Learning
3. https://ar5iv.org/html/2309.16139. [2309.16139] Two-Step Active Learning for Instance Segmentation with Uncertainty and Diversity Sampling
4. https://ar5iv.org/html/2207.12302. [2207.12302] Exploiting Diversity of Unlabeled Data for Label-Efficient Semi-Supervised Active Learning
5. https://ar5iv.org/html/2411.06353. [2411.06353] Deep Active Learning in the Open World
6. https://ar5iv.org/html/2107.01622. [2107.01622] Multiple-criteria Based Active Learning with Fixed-size Determinantal Point Processes
7. https://ar5iv.org/html/2405.10808. [2405.10808] ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios
8. https://ar5iv.org/html/2412.02310. [2412.02310] Active Learning via Classifier Impact and Greedy Selection for Interactive Image Retrieval
9. https://ar5iv.org/html/2411.17444. [2411.17444] Maximally Separated Active Learning

## Answer (I don't know?: ‚úó)

In the context of active learning (AL) for natural language processing (NLP), several strategies have been identified for selecting a diverse batch of instances. These strategies typically focus on instances that not only maximize informativeness but also ensure diversity and representativeness among the selected samples.

1. **Variance in Selection**: One common property is that selected instances should be dissimilar to other instances within the unlabeled data pool. This criterion promotes diversity as it ensures that the learning algorithm is exposed to a wider range of data points [3]. Ebert et al. introduced a formulation that emphasizes a time-varying trade-off between exploration (diversifying the data) and exploitation (focusing on the most informative data) [3]. Thus, the selected instances need to balance between being informative and different from one another.

2. **Informativeness and Clustering**: Shen et al. proposed two combination strategies that incorporate the principles of informativeness, representativeness, and diversity. Their first strategy involves pre-selecting the most informative samples using unsupervised methods, followed by clustering these samples into multiple classes. The centroids of these clusters are then selected as representative samples for the batch [3]. This method ensures that the instances chosen not only reflect the overall data distribution but also include those that are significant for decision-making.

3. **Uncertainty Sampling**: A foundational approach in active learning is uncertainty sampling, where the algorithm selects instances based on the unreliability of its current predictions. Instances for which the model has low confidence are prioritized for selection [4] [5]. This method can indirectly promote diversity because cases that are least certain are often those that do not conform to existing patterns in the labeled data, potentially leading to a more varied selection.

4. **Balanced Representation**: It is crucial that selected instances are not solely focused on informativeness but also maintain a diverse representation of the underlying population of data. By selecting across various characteristics (such as dissimilarity and diversity) within the batch, the model can gain a more comprehensive understanding of the data landscape [2] [3].

5. **Performance-Driven Strategies**: It has also been noted that selecting an appropriate AL strategy is vital as inappropriate choices may result in inferior performance relative to random selections [2]. This emphasizes the need for tailored selection strategies that consider the specific characteristics of the dataset and the objectives of the learning task.

In summary, common strategies for selecting a diverse batch of instances in active learning include focusing on instance dissimilarity, utilizing clustering to ensure representativeness, employing uncertainty sampling for variance, and ensuring that selections are tailored to enhance overall model performance. As the field continues to evolve, further empirical evaluations and refinements of these strategies will aid researchers and practitioners in effective decision-making in active learning contexts [2][4].

1. [1]:  https://ar5iv.org/html/2407.03895, No Title
2. [2]:  https://ar5iv.org/html/2407.03895, No Title
3. [3]:  https://ar5iv.org/html/2107.01622, [2107.01622] Multiple-criteria Based Active Learning with Fixed-size Determinantal Point Processes
4. [4]:  https://ar5iv.org/html/2401.10825, No Title
5. [5]:  https://ar5iv.org/html/2401.10825, No Title
---
1. [1]:  Passage ID 1: used 57 different datasets to evaluate their respective strategies. Most datasets contained newspaper articles or biomedical/medical data. Our analysis revealed that 26 out of 57 datasets are publicly accessible.Conclusion:Numerous active learning strategies have been identified, along with significant open questions that still need to be addressed. Researchers and practitioners face difficulties when making data-driven decisions about which active learning strategy to adopt. Conducting comprehensive empirical comparisons using the evaluation environment proposed in this study could help establish best practices in the domain.Keywords: Scoping Review Active Learning Selective Sampling Entity Recognition Span Labeling Annotation Effort Annotation Costs NLP.1 IntroductionRecent years showed significant advancements [98, 7, 19] in natural language processing (NLP): Large language models (LLMs) emerged [8], facilitating new methodologies by describing tasks in natural language
2. [2]:  Passage ID 2: However, the selection of an appropriate AL strategy is crucial. Selecting an inappropriate strategy can lead to lower performance compared to random data selection [10, 38].Over the past two decades, researchers have developed many active learning strategies in the field of NLP for various scenarios. However, it is still challenging for researchers and practitioners to select a promising strategy for a given use case. While existing AL surveys provide taxonomies [72, 105, 106], there is still a lack of comprehensive performance analyses. Towards closing this gap, an overview of the domain can support researchers in conducting such analyses.Therefore, we executed a scoping review focusing on active learning strategies and their evaluation environments limited to the entity recognition task in NLP. We concentrated our review on model-agnostic strategies so researchers can use our results for a broad range of models. Our review answers the following review questions:1.Which
3. [3]:  Passage ID 3: the largest impact of current models, other works also select instances with the following properties: 1) the selected instance should be dissimilar with other instances in the unlabeled data pool; and 2) the selected instance should be dissimilar with other instances in one batch. Properties 1 and 2 are considered as the representativeness and diversity criteria in the active learning task. Ebert et al. introduced a reinforced active learning formulation, which enables a time varying trade-off between exploration and exploitation [20].Shen et al. incorporated informativeness, representativeness and diversity with two different combination strategies 4. [4]: Strategy 1 first pre-selects the most informative data samples by US, then clusters them into kùëòk classes, whose centroids are the selected data samples in one batch. However, informative instances usually do not exploit the structure information of unlabeled data. Moreover, when the number of samples for clustering is small, the
4. [4]:  Passage ID 4: (the learning algorithm) is able to choose the data that it wishes to learn from, it can perform better than it otherwise would using traditional learning schemes. One of the main challenges of active learning is to determine what constitutes the most informative data, and how the learner can recognize this data. The most common approach today is uncertainty sampling (Settles, 2009), in which the model selects the examples for which its current prediction is the least reliable.Where active learning is successfully applied to NLP, either performance may be improved using the same amount of data, or a similar performance may be maintained while reducing the amount of data and annotation required to develop an effective model. In the field of deep learning research, pioneering adoptions of active learning have produced promising results (Siddhant and Lipton, 2018). As regards NER, approaches based on deep learning have been proposed in several works (Shen et¬†al., 2017; Yan et¬†al.,
5. [5]:  Passage ID 5: (the learning algorithm) is able to choose the data that it wishes to learn from, it can perform better than it otherwise would using traditional learning schemes. One of the main challenges of active learning is to determine what constitutes the most informative data, and how the learner can recognize this data. The most common approach today is uncertainty sampling (Settles, 2009), in which the model selects the examples for which its current prediction is the least reliable.Where active learning is successfully applied to NLP, either performance may be improved using the same amount of data, or a similar performance may be maintained while reducing the amount of data and annotation required to develop an effective model. In the field of deep learning research, pioneering adoptions of active learning have produced promising results (Siddhant and Lipton, 2018). As regards NER, approaches based on deep learning have been proposed in several works (Shen et¬†al., 2017; Yan et¬†al.,