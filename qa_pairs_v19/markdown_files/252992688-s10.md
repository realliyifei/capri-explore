# A Survey of Active Learning for Natural Language Processing

## Question

What are the common strategies for selecting a diverse batch of instances in active learning?

## URLs

1. https://ar5iv.org/html/2403.03728. [2403.03728] Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training
2. https://ar5iv.org/html/2312.15944. [2312.15944] BAL: Balancing Diversity and Novelty for Active Learning
3. https://ar5iv.org/html/2309.16139. [2309.16139] Two-Step Active Learning for Instance Segmentation with Uncertainty and Diversity Sampling
4. https://ar5iv.org/html/2207.12302. [2207.12302] Exploiting Diversity of Unlabeled Data for Label-Efficient Semi-Supervised Active Learning
5. https://ar5iv.org/html/2411.06353. [2411.06353] Deep Active Learning in the Open World
6. https://ar5iv.org/html/2107.01622. [2107.01622] Multiple-criteria Based Active Learning with Fixed-size Determinantal Point Processes
7. https://ar5iv.org/html/2405.10808. [2405.10808] ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios
8. https://ar5iv.org/html/2412.02310. [2412.02310] Active Learning via Classifier Impact and Greedy Selection for Interactive Image Retrieval
9. https://ar5iv.org/html/2411.17444. [2411.17444] Maximally Separated Active Learning

## Answer (I don't know?: ‚úó)

In active learning, selecting a diverse batch of instances is crucial for maximizing the model's performance while minimizing the data annotation effort. Several strategies have emerged in the field, particularly for natural language processing (NLP) tasks. 

One prominent strategy is uncertainty sampling, where the model selects instances for which its predictions are the least reliable, effectively prioritizing cases that are most likely to improve learning [4][5]. While this approach is foundational, it primarily focuses on informativeness without explicitly addressing diversity.

To enhance diversity, two key criteria are often employed: representativeness and diversity of selected instances. Representativeness ensures that the selected samples are dissimilar to one another, promoting a broad coverage of the input data space, while also ensuring dissimilarity within the batch itself. This means that instances selected from the unlabeled data pool should capture different characteristics and trends present in the data [3][4]. For instance, the work of Shen et al. incorporates these properties into their strategies by first pre-selecting informative data samples and then clustering them, ensuring that the centroids of these clusters represent diverse instances for selection [3].

Another approach mentioned is based on reinforced active learning formulations that enable a variable trade-off between exploration (selecting diverse instances) and exploitation (choosing the most informative instances) [3]. This allows the algorithm to adaptively balance its need for accumulating information from diverse data points while still focusing on those that offer significant learning potential.

Furthermore, the analysis of datasets has highlighted that many proposed active learning strategies can yield differing results based on the dataset context. Therefore, while there are established strategies such as uncertainty sampling and those that consider representativeness and diversity directly, the effectiveness of these methods can be dependent on the particulars of the data and task at hand [1][2].

In conclusion, common strategies for selecting a diverse batch of instances in active learning not only include uncertainty sampling but also more advanced techniques that factor in representativeness and diversity criteria. These are critical for enhancing the model's learning efficiency and effectiveness in various NLP applications.

1. [1]:  https://ar5iv.org/html/2407.03895, No Title
2. [2]:  https://ar5iv.org/html/2407.03895, No Title
3. [3]:  https://ar5iv.org/html/2107.01622, [2107.01622] Multiple-criteria Based Active Learning with Fixed-size Determinantal Point Processes
4. [4]:  https://ar5iv.org/html/2401.10825, No Title
5. [5]:  https://ar5iv.org/html/2401.10825, No Title
---
1. [1]:  Passage ID 1: used 57 different datasets to evaluate their respective strategies. Most datasets contained newspaper articles or biomedical/medical data. Our analysis revealed that 26 out of 57 datasets are publicly accessible.Conclusion:Numerous active learning strategies have been identified, along with significant open questions that still need to be addressed. Researchers and practitioners face difficulties when making data-driven decisions about which active learning strategy to adopt. Conducting comprehensive empirical comparisons using the evaluation environment proposed in this study could help establish best practices in the domain.Keywords: Scoping Review Active Learning Selective Sampling Entity Recognition Span Labeling Annotation Effort Annotation Costs NLP.1 IntroductionRecent years showed significant advancements [98, 7, 19] in natural language processing (NLP): Large language models (LLMs) emerged [8], facilitating new methodologies by describing tasks in natural language
2. [2]:  Passage ID 2: However, the selection of an appropriate AL strategy is crucial. Selecting an inappropriate strategy can lead to lower performance compared to random data selection [10, 38].Over the past two decades, researchers have developed many active learning strategies in the field of NLP for various scenarios. However, it is still challenging for researchers and practitioners to select a promising strategy for a given use case. While existing AL surveys provide taxonomies [72, 105, 106], there is still a lack of comprehensive performance analyses. Towards closing this gap, an overview of the domain can support researchers in conducting such analyses.Therefore, we executed a scoping review focusing on active learning strategies and their evaluation environments limited to the entity recognition task in NLP. We concentrated our review on model-agnostic strategies so researchers can use our results for a broad range of models. Our review answers the following review questions:1.Which
3. [3]:  Passage ID 3: the largest impact of current models, other works also select instances with the following properties: 1) the selected instance should be dissimilar with other instances in the unlabeled data pool; and 2) the selected instance should be dissimilar with other instances in one batch. Properties 1 and 2 are considered as the representativeness and diversity criteria in the active learning task. Ebert et al. introduced a reinforced active learning formulation, which enables a time varying trade-off between exploration and exploitation [20].Shen et al. incorporated informativeness, representativeness and diversity with two different combination strategies 4. [4]: Strategy 1 first pre-selects the most informative data samples by US, then clusters them into kùëòk classes, whose centroids are the selected data samples in one batch. However, informative instances usually do not exploit the structure information of unlabeled data. Moreover, when the number of samples for clustering is small, the
4. [4]:  Passage ID 4: (the learning algorithm) is able to choose the data that it wishes to learn from, it can perform better than it otherwise would using traditional learning schemes. One of the main challenges of active learning is to determine what constitutes the most informative data, and how the learner can recognize this data. The most common approach today is uncertainty sampling (Settles, 2009), in which the model selects the examples for which its current prediction is the least reliable.Where active learning is successfully applied to NLP, either performance may be improved using the same amount of data, or a similar performance may be maintained while reducing the amount of data and annotation required to develop an effective model. In the field of deep learning research, pioneering adoptions of active learning have produced promising results (Siddhant and Lipton, 2018). As regards NER, approaches based on deep learning have been proposed in several works (Shen et¬†al., 2017; Yan et¬†al.,
5. [5]:  Passage ID 5: (the learning algorithm) is able to choose the data that it wishes to learn from, it can perform better than it otherwise would using traditional learning schemes. One of the main challenges of active learning is to determine what constitutes the most informative data, and how the learner can recognize this data. The most common approach today is uncertainty sampling (Settles, 2009), in which the model selects the examples for which its current prediction is the least reliable.Where active learning is successfully applied to NLP, either performance may be improved using the same amount of data, or a similar performance may be maintained while reducing the amount of data and annotation required to develop an effective model. In the field of deep learning research, pioneering adoptions of active learning have produced promising results (Siddhant and Lipton, 2018). As regards NER, approaches based on deep learning have been proposed in several works (Shen et¬†al., 2017; Yan et¬†al.,