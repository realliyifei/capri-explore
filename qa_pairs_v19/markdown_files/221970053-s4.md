# A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English

## Question

How were advanced machine learning models utilized for coreference resolution tasks prior to the advent of deep learning?

## URLs

1. https://ar5iv.org/html/2009.12721. [2009.12721] A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution
2. https://ar5iv.org/html/2007.04301. [2007.04301] Segmentation Approach for Coreference Resolution Task
3. https://ar5iv.org/html/1910.09329. [1910.09329] A Neural Entity Coreference Resolution review
4. https://ar5iv.org/html/2211.04428. [2211.04428] Review of coreference resolution in English and Persian
5. https://ar5iv.org/html/2408.16893. [2408.16893] Exploring Multiple Strategies to Improve Multilingual Coreference Resolution in CorefUD
6. https://ar5iv.org/html/2405.10714. [2405.10714] Persian Pronoun Resolution: Leveraging Neural Networks and Language Models
7. https://ar5iv.org/html/2412.03782. [2412.03782] The broader spectrum of in-context learning
8. https://ar5iv.org/html/2209.07841. [2209.07841] Findings of the Shared Task on Multilingual Coreference Resolution
9. https://ar5iv.org/html/2011.00286. [2011.00286] Neural Coreference Resolution for Arabic
10. https://ar5iv.org/html/2404.19505. [2404.19505] Context-Aware Machine Translation with Source Coreference Explanation

## Answer (I don't know?: ✗)

Prior to the advent of deep learning, advanced machine learning models for coreference resolution tasks predominantly relied on human-designed rules, knowledge, and features. These approaches were crucial for correctly resolving pronouns, as certain linguistic rules guide their references; for example, "he" refers to males, "she" refers to females, "it" refers to singular objects, and "them" refers to plural objects [1]. The effectiveness of these rule- and feature-based methods depended significantly on the coverage and quality of the manually defined rules and features.

In this period, a variety of conventional methods were deployed. Firstly, traditional machine learning models were built upon these designed features to perform coreference resolution [1]. This included techniques where rules explicitly defined how coreference relationships could be established based on the input, emphasizing the reliance on linguistically grounded knowledge. However, such models often suffered from limitations in flexibility and scalability since manually crafting rules to encompass all possible cases in natural language was a daunting task [2].

Another significant advancement involved the introduction of more sophisticated machine learning models that shifted the focus from pair-wise coreference identification to an entity-centric approach. This method allowed for the incremental learning of effective policies to build coreference chains, which is more aligned with the natural flow of discourse [2]. These advanced models, while built upon traditional rules, started incorporating a degree of automation in the learning process, albeit still maintaining the foundational structure reliant on predefined rules and features.

The landscape began to change as researchers sought to incorporate heuristic rules based on linguistic knowledge into machine learning frameworks, allowing for improved coreference predictions without being strictly limited to human-generated conditions [2]. However, it is important to note that while these methods were relatively innovative at the time, they could not reach the efficiency and adaptability levels achieved by subsequent deep learning models that functioned without the limitations of manual rule design.

In summary, before deep learning, coreference resolution in NLP relied heavily on human-crafted rules and features, with advanced machine learning models seeking to enhance these systems through techniques that allowed for better learning from the defined features and linguistic insights [1] [2]. As these earlier machine learning methodologies paved the way for deeper methodologies, they highlighted the challenges of manual rule-based systems and set the stage for the later adoption of end-to-end deep learning models, which aimed to circumvent these limitations entirely.

1. [1]:  https://ar5iv.org/html/2009.12721, [2009.12721] A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution
2. [2]:  https://ar5iv.org/html/2009.12721, [2009.12721] A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution
3. [3]:  https://ar5iv.org/html/2408.16893, [2408.16893] Exploring Multiple Strategies to Improve Multilingual Coreference Resolution in CorefUD
4. [4]:  https://ar5iv.org/html/2011.00286, [2011.00286] Neural Coreference Resolution for Arabic
5. [5]:  https://ar5iv.org/html/2009.12721, [2009.12721] A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution
---
1. [1]:  Passage ID 1: tasks. After that, we briefly introduce a few recent improvements over the end-to-end model.2.2.1 Rule and Feature Based MethodsBefore the deep learning era, human-designed rules [2, 19], knowledge [20, 21], or features [3, 22] dominated the general coreference resolution and PCR tasks. Some rules and features are crucial for correctly resolving pronouns [23]. For example, ‘he’ can only refer to males and ‘she’ can only refer to females; ‘it’ can only refer to singular objects and ‘them’ can only refer to plural objects.The performances of these methods heavily rely on the coverage and quality of the manually defined rules and features.Based on these designed features [24], a few more advanced machine learning models were applied to the coreference resolution task. For example, instead of identifying coreference relation pair-wisely, [25] proposes an entity-centric coreference system that can learn an effective policy for building coreference chains incrementally. Besides that,
2. [2]:  Passage ID 2: of identifying coreference relation pair-wisely, [25] proposes an entity-centric coreference system that can learn an effective policy for building coreference chains incrementally. Besides that, a novel model was also proposed to predict coreference relations with a deep reinforcement learning framework [26].Moreover, heuristic rules based on linguistic knowledge can also be incorporated into constraints for machine learning models [27].2.2.2 End-to-end ModelLeveraging human-designed rules or features can help accurately resolve some pronouns, but it is hard to manually design rules to cover all cases.To solve this problem, an end-to-end deep model [9] was proposed.Different from other machine learning-based methods, it does not use any human-defined rules, yet achieves surprisingly good performance.Specifically, the end-to-end model first leverages the combination of Bi-directional LSTM and inner-attention modules to encode local context and generate representations for
3. [3]:  Passage ID 3: resolution, they are expensive to train and deploy, and traditional (discriminative) approaches remain competitive. Expressing this task in natural language is challenging, and to the best of our knowledge, there have been no successful attempts to utilize large chatbots (like ChatGPT-4) to achieve superior results.Coreference resolution becomes particularly challenging in low-resource languages. One strategy to address this challenge is to train a multilingual model on datasets from multiple languages, thereby transferring knowledge from resource-rich languages to those with fewer resources. However, a significant challenge with this approach lies in the differences in annotations across available corpora. The CorefUD initiative [3] tries to harmonize the datasets and create one annotation scheme for coreference in multiple languages similarly to Universal Dependencies [4] for syntactic annotations.This paper describes our approach to multilingual coreference resolution. The
4. [4]:  Passage ID 4: research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English and used for the 2011 and 2012 conll shared tasks [Pradhan et al., 2012].Since then,there has been substantial research on English coreference,most recently using neural coreference approaches [Lee et al., 2017, Lee et al., 2018, Kantor and Globerson, 2019a, Joshi et al., 2019b, Joshi et al., 2019a, Yu et al., 2020b, Wu et al., 2020], leading to a significant increase inthe performance of coreference resolvers for English.By contrast, there has been almost no research on Arabic coreference;the performance for Arabic coreference resolution has not improved much since the conll 2012 shared task,and in particular no neural architectures have been proposed–the current state-of-the-art system remainsthe model proposed in [Björkelund and Kuhn, 2014].In this paper we close this very
5. [5]:  Passage ID 5: of CoNLL shared tasks. Different from CoNLL, where most of the corpus is from the newswire, WikiCoref directly annotates Wikipedia pages, which provides a new way to evaluate models’ performances in the out-of-domain setting.5.Crowd-sourced Coref: [18] leveraged a crowd-sourced game to collect 2.2 million annotations about 108,000 coreference relations, which makes it one of the largest coreference dataset. Moreover, their annotations also include ambiguous coreference relations.2.2 MethodsIn this subsection, we introduce representative models for the ordinary PCR task. We first briefly introduce conventional approaches that rely on human-designed rules or features and then introduce the end-to-end model, which is a groundbreaking model for solving coreference resolution tasks. After that, we briefly introduce a few recent improvements over the end-to-end model.2.2.1 Rule and Feature Based MethodsBefore the deep learning era, human-designed rules [2, 19],