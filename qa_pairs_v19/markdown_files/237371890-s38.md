# Survey of Low-Resource Machine Translation

## Question

How are transliteration and alphabet mapping utilized in machine translation for languages with different scripts?

## URLs

1. https://ar5iv.org/html/2205.09578. [2205.09578] A machine transliteration tool between Uzbek alphabets
2. https://ar5iv.org/html/2402.16065. [2402.16065] Training a Bilingual Language Model by Mapping Tokens onto a Shared Character Space
3. https://ar5iv.org/html/2306.12693. [2306.12693] Multilingual Neural Machine Translation System for Indic to Indic Languages
4. https://ar5iv.org/html/2110.07804. [2110.07804] Alternative Input Signals Ease Transfer in Multilingual Machine Translation
5. https://ar5iv.org/html/2101.05162. [2101.05162] Uzbek Cyrillic-Latin-Cyrillic Machine Transliteration
6. https://ar5iv.org/html/2410.08974. [2410.08974] UniGlyph: A Seven-Segment Script for Universal Language Representation
7. https://ar5iv.org/html/2412.03877. [2412.03877] AyutthayaAlpha: A Thai-Latin Script Transliteration Transformer
8. https://ar5iv.org/html/2202.00794. [2202.00794] Learning to pronounce as measuring cross-lingual joint orthography-phonology complexity
9. https://ar5iv.org/html/2109.00486. [2109.00486] Survey of Low-Resource Machine Translation
10. https://ar5iv.org/html/2008.01391. [2008.01391] A Survey of Orthographic Information in Machine Translation

## Answer (I don't know?: ✓)

I don't know. The context provided does not contain any information regarding transliteration or alphabet mapping in the field of machine translation, particularly for languages with different scripts. The passages focus on various aspects of machine translation (MT), adaptive MT, and the challenges faced due to insufficient in-domain data, without specifically addressing transliteration techniques or how different scripts are handled in translation processes. Therefore, the information required to answer your question is not available in the provided context.

1. [1]:  https://ar5iv.org/html/2401.14559, No Title
2. [2]:  https://ar5iv.org/html/2107.04239, No Title
3. [3]:  https://ar5iv.org/html/2107.04239, No Title
4. [4]:  https://ar5iv.org/html/2107.04239, No Title
5. [5]:  https://ar5iv.org/html/2401.14559, No Title
---
1. [1]:  Passage ID 1: I examined how to leverage language modelling techniques in general and LLMs in particular to improve translation features that involve human interaction and continuous feedback, such as adaptive MT, terminology-constrained MT, domain-aware automatic post-editing, auto-suggestion and auto-completion. In the second research question, I addressed a common scenario in the translation industry, namely receiving highly specialised projects, where there is hardly any parallel in-domain data. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. One way to address this question is through domain-specific text generation with LLMs and then fine-tuning an MT model using the generated bilingual in-domain synthetic data. Furthermore, this question overlaps with the first question, since real-time adaptive MT with LLMs can serve as an alternative approach to address in-domain data
2. [2]:  Passage ID 2: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
3. [3]:  Passage ID 3: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
4. [4]:  Passage ID 4: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
5. [5]:  Passage ID 5: common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations (Axelrod et al.,, 2011; Haddow and Koehn,, 2012).Recent advances in language modelling techniques in general and large-scale language models (LLMs) in particular have shown significant potential in improving a wide range of NLP tasks. Inspired by this idea, this research aims to answer two major Research Questions (RQ):RQ1In scenarios involving human interaction and continuous feedback, can we employ language models to improve the quality of adaptive MT at inference time? In the subsequent sections, I will be referring to this question as “Adaptive and Interactive MT”.RQ2In the absence of sufficient in-domain data, can we use pre-trained LLMs to improve the process of NMT domain adaptation? In the following sections, I will be referring to this question as “Domain-specific Text Generation for MT”.Figure 1.1: