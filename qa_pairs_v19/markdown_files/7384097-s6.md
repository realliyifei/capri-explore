# A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena

## Question

How is reordering managed in n-gram based Statistical Machine Translation (SMT), and what are the limitations of this approach?

## URLs

1. https://ar5iv.org/html/1502.04938. [1502.04938] A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena
2. https://ar5iv.org/html/1809.01272. [1809.01272] Unsupervised Statistical Machine Translation
3. https://ar5iv.org/html/2109.15119. [2109.15119] Improved Statistical Machine Translation Using Monolingual Paraphrases
4. https://ar5iv.org/html/1605.04515. [1605.04515] Machine Translation Evaluation Resources and Methods: A Survey
5. https://ar5iv.org/html/2212.00138. [2212.00138] Word Alignment in the Era of Deep Learning: A Tutorial
6. https://ar5iv.org/html/1902.01313. [1902.01313] An Effective Approach to Unsupervised Machine Translation
7. https://ar5iv.org/html/1709.07809. [1709.07809] Statistical Machine Translation Draft of Chapter 13: Neural Machine Translation .
8. https://ar5iv.org/html/2011.02821. [2011.02821] Data Augmentation and Terminology Integration for Domain-Specific Sinhala-English-Tamil Statistical Machine Translation

## Answer (I don't know?: ✓)

I don't know. The context provided does not contain any information specifically addressing how reordering is managed in n-gram based Statistical Machine Translation (SMT) or the limitations of that specific approach. The passages mostly discuss the comparison and transition from Example-Based Machine Translation (EBMT) to SMT and finally to Neural Machine Translation (NMT), but do not delve into the technical mechanisms of n-gram SMT or its particular limitations beyond general issues with SMT, such as context handling and dependency challenges. Therefore, the context is insufficient to answer the question.

1. [1]:  https://ar5iv.org/html/2301.00688, No Title
2. [2]:  https://ar5iv.org/html/2301.00688, No Title
3. [3]:  https://ar5iv.org/html/2404.08661, No Title
4. [4]:  https://ar5iv.org/html/2404.08661, No Title
5. [5]:  https://ar5iv.org/html/2411.11295, No Title
---
1. [1]:  Passage ID 1: into Example-Based Machine Translation (EBMT), Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). The scope of EBMT is quite limited because, it requires a large corpus, and not everything can be covered by example, the spoken languages are too vivid, diverse and ambiguous. Hence, SMT came into existence which relies upon bayesian inference. SMT predicts translation probabilities of phrase pairs in corresponding source-target languages. By increasing the size of the dataset, the probability of a certain pair of phrases can be enhanced. However, the inability to achieve context information, differently trained components and system complexity are the weak points of SMT, which led to the development of the NMT system [S, 2017]. The NMT system can handles sequence-to-sequence learning problems for variable length source and target sentences and long-term dependency problems. The NMT system improves translation prediction and has excellent context-analyzing
2. [2]:  Passage ID 2: into Example-Based Machine Translation (EBMT), Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). The scope of EBMT is quite limited because, it requires a large corpus, and not everything can be covered by example, the spoken languages are too vivid, diverse and ambiguous. Hence, SMT came into existence which relies upon bayesian inference. SMT predicts translation probabilities of phrase pairs in corresponding source-target languages. By increasing the size of the dataset, the probability of a certain pair of phrases can be enhanced. However, the inability to achieve context information, differently trained components and system complexity are the weak points of SMT, which led to the development of the NMT system [S, 2017]. The NMT system can handles sequence-to-sequence learning problems for variable length source and target sentences and long-term dependency problems. The NMT system improves translation prediction and has excellent context-analyzing
3. [3]:  Passage ID 3: highly automation and efficiency are its two main problems. (2) A corpus-based system with statistical machine translation (SMT) or phrase-based machine translation (PB- SMT) as the primary method uses machine learning algorithms by using a large amount of parallel corpus as input, overcoming the problem of using labor work to boost efficiency automatically (Lopez, 2008); however, it has issues such as with translation for idiomatic expressions, compound words that have to be translated by more than one word, long dependencies, and ambiguous words with different meanings depending on contexts (Nießen, 2000). (3) The currently predominant neural machine translation (NMT) system has gained the most extensive popularity in machine translation domain. In contrast to more established system SMT, NMT makes use of its architecture and capacity to capture complex sentence dependencies, which suggests that it has a great deal of potential to become a new trend in machine translation.
4. [4]:  Passage ID 4: highly automation and efficiency are its two main problems. (2) A corpus-based system with statistical machine translation (SMT) or phrase-based machine translation (PB- SMT) as the primary method uses machine learning algorithms by using a large amount of parallel corpus as input, overcoming the problem of using labor work to boost efficiency automatically (Lopez, 2008); however, it has issues such as with translation for idiomatic expressions, compound words that have to be translated by more than one word, long dependencies, and ambiguous words with different meanings depending on contexts (Nießen, 2000). (3) The currently predominant neural machine translation (NMT) system has gained the most extensive popularity in machine translation domain. In contrast to more established system SMT, NMT makes use of its architecture and capacity to capture complex sentence dependencies, which suggests that it has a great deal of potential to become a new trend in machine translation.
5. [5]:  Passage ID 5: rules crafted by experts and extensive lexicons, but they were often labor-intensive and brittle when applied to complex languages. SMT, introduced in the 1990s, offered a data-driven approach, where models learned translation probabilities from bilingual text corpora. However, SMT required large volumes of parallel data, which was a significant limitation for low-resource languages, and its reliance on phrase-based techniques often failed to capture complex linguistic phenomena like morphology and syntax in underrepresented languages.With the advent of Neural Machine Translation (NMT), particularly sequence-to-sequence (Seq2Seq) models with attention mechanisms, the field of Machine Translation entered a new era. NMT demonstrated superior performance over SMT in many language pairs by leveraging deep learning to create richer representations of source and target sentences. However, NMT’s effectiveness heavily depends on the availability of large datasets, making it less applicable