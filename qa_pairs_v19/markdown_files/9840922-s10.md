# Word Meaning Inducing via Character Ontology: A Survey on the Semantic Prediction of Chinese Two-Character Words

## Question

How is the weight value for characters in Chinese two-character words determined and calculated?

## URLs

1. https://ar5iv.org/html/2310.12751. [2310.12751] Character-level Chinese Backpack Language Models
2. https://ar5iv.org/html/2207.05928. [2207.05928] Exploiting Word Semantics to Enrich Character Representations of Chinese Pre-trained Models
3. https://ar5iv.org/html/2403.18542. [2403.18542] Attention-aware semantic relevance predicting Chinese sentence reading
4. https://ar5iv.org/html/2109.07877. [2109.07877] MFE-NER: Multi-feature Fusion Embedding for Chinese Named Entity Recognition
5. https://ar5iv.org/html/2405.07006. [2405.07006] Word-specific tonal realizations in Mandarin
6. https://ar5iv.org/html/2010.07711. [2010.07711] Does Chinese BERT Encode Word Structure?
7. https://ar5iv.org/html/2102.00621. [2102.00621] Polyphone Disambiguition in Mandarin Chinese with Semi-Supervised Learning
8. https://ar5iv.org/html/2201.12155. [2201.12155] Reducing Multilingual Context Confusion for End-to-end Code-switching Automatic Speech Recognition
9. https://ar5iv.org/html/2411.11770. [2411.11770] CNMBert: A Model For Hanyu Pinyin Abbreviation to Character Conversion Task

## Answer (I don't know?: ✗)

The weight values for characters in Chinese two-character words are determined through a series of steps aimed at capturing the importance of each character in relation to the overall word representation. Initially, the significance of each character is calculated based on the cosine similarity between its representation and that of the paired word. This technique allows for quantifying how closely a character's individual semantic representation aligns with the meaning of the word as a whole, effectively assessing the character's contribution to the word's meaning [2].

Once the importance of each character is established, these values are then integrated into the representation of each internal character according to their calculated importance weights. This integration process ensures that characters deemed more important have a greater influence on the overall representation [2]. Furthermore, a mixing mechanism is employed, enabling characters within a word to exchange information with one another, which enhances the quality of each character's representation through collaboration [2].

In addition to this, advanced attention mechanisms are introduced. A multi-head attention mechanism is used, along with a masked multi-head attention where unimportant characters are temporarily disregarded during the representation process. These mechanisms help to refine the final character representations by focusing attention on the most relevant characters in the context of the word [2].

The final character representation obtained after this series of calculations and mechanisms is subsequently utilized in various downstream natural language processing tasks. This approach collectively minimizes issues such as segmentation errors, as a multi-tokenizer voting mechanism is implemented to ensure the final segmentation results are more accurate [2] [4].

In summation, the calculation of weights for characters in Chinese two-character words involves an assessment of cosine similarity for determining importance, integration of character and word representations, character interaction through mixing mechanisms, and refinement through attentive processes. This layered methodology enhances the character-level meanings and ensures that the final representation is well-suited for various applications in NLP.

1. [1]:  https://ar5iv.org/html/2310.12751, [2310.12751] Character-level Chinese Backpack Language Models
2. [2]:  https://ar5iv.org/html/2207.05928, [2207.05928] Exploiting Word Semantics to Enrich Character Representations of Chinese Pre-trained Models
3. [3]:  https://ar5iv.org/html/2310.12751, [2310.12751] Character-level Chinese Backpack Language Models
4. [4]:  https://ar5iv.org/html/2310.12751, [2310.12751] Character-level Chinese Backpack Language Models
5. [5]:  https://ar5iv.org/html/2010.07711, [2010.07711] Does Chinese BERT Encode Word Structure?
---
1. [1]:  Passage ID 1: is composed of the characters "词" (word) and "典" (book, in ancient Chinese), and when generating text from input containing this word, the model could focus on either the "word" or "book" property. By adjusting the weights of the sense vectors of the constituent characters, we were able to amplify implicit meaning of a constituent character and bias the model toward generating text related to a specific property. Specifically, we conducted experiments to amplify the contribution of the first or second character four times each while keeping the total contribution of the word unchanged in the output. We found that the model tended to generate sentences that relate to the amplified character with greater probability, as shown in Appendix 11. We assessed the efficacy of the proposed method by computing the ratio of expectations for the controlled model relative to an uncontrolled model in the context of predicting semantically related characters from an open-topic prompt as
2. [2]:  Passage ID 2: of Chinese pre-trained models in the fine-tuning stage. Firstly, the importance of each character in a word is calculated based on the cosine similarity between the character’s representation and its paired word’s representation. Then we integrate the word embedding into the representation of each internal character according to the importance weight. After that, we apply a mixing mechanism to enable characters within a word exchange information with each other to further enhance the character representation. In addition, we apply a multi-head attention and a masked multi-head attention which masks the unimportant characters, and the final representation is obtained by fusing the representations given by the two attention mechanisms, which is then applied in downstream tasks. Moreover, in order to minimize the impact of word segmentation errors, we adopt a multi-tokenizer voting mechanism to obtain the final word segmentation result.We conduct extensive experiments on several NLP
3. [3]:  Passage ID 3: words are often composed of many characters.We find that our (134M parameter) Chinese Backpack language model performs comparably to a (104M parameter) Transformer, and learns rich character-level meanings that log-additively compose to form word meanings.In SimLex-style lexical semantic evaluations, simple averages of Backpack character senses outperform input embeddings from a Transformer.We find that complex multi-character meanings are often formed by using the same per-character sense weights consistently across context.Exploring interpretability-through control, we show that we can localize a source of gender bias in our Backpacks to specific character senses and intervene to reduce the bias.1 IntroductionFigure 1: The general structure of the character-level Chinese Backpack Language Model. The next character is predicted by the weight sum of the senses of characters in the previous context. The sense vector of "显" (show) provides information for word composition,
4. [4]:  Passage ID 4: Language Model. The next character is predicted by the weight sum of the senses of characters in the previous context. The sense vector of "显" (show) provides information for word composition, while the senses of "电" and "脑" (computer) provide semantic information through linear combination. Language modeling is a crucial task in natural language processing, where the goal is to compute the probability of the next word in a sequence given the preceding words. Recently, large language models based on the Transformer architecture Vaswani et al. (2017) have achieved remarkable success in various NLP applications, including text generation Radford et al. (2018b); Brown et al. (2020); Wang and Komatsuzaki (2021), machine translation Bawden et al. (2019); Lewis et al. (2019), and question-answering Miller et al. (2017); Karpukhin et al. (2020); Ram et al. (2021). However, Transformers are notoriously hard to interpret and control. Their non-linear contextualization functions imply that
5. [5]:  Passage ID 5: in the word “钱其琛(Qichen Qian)”, and the character “题(question)” puts most attention to characters in the word “台湾(Taiwan)”. This indicates that the head 6-5 takes most of the information from the previous words to generate contextualized character representation.4 Probing TaskWe probe the contextualized representation of each character for Chinese Word Segmentation (CWS) [2003]. In particular, CWS can be treated as a character-level sequence labeling task, where the label set includes B, M, E, and S (which stand for the beginning, middle, ending of word and single character word, respectively). We directly use the fixed hidden representations in each layer as the input, on which a trainable linear classifier is built, as Figure 4 shows. We use local classifier rather than the conditional random fields [2001], in order to focus on information extracted from hidden representations directly. The intuition is that if a simple linear classifier can predict the segmentation labels,