# Word Meaning Inducing via Character Ontology: A Survey on the Semantic Prediction of Chinese Two-Character Words

## Question

How is the weight value for characters in Chinese two-character words determined and calculated?

## URLs

1. https://ar5iv.org/html/2310.12751
2. https://ar5iv.org/html/2207.05928
3. https://ar5iv.org/html/2403.18542
4. https://ar5iv.org/html/2109.07877
5. https://ar5iv.org/html/2405.07006
6. https://ar5iv.org/html/2010.07711
7. https://ar5iv.org/html/2102.00621
8. https://ar5iv.org/html/2201.12155
9. https://ar5iv.org/html/2411.11770

## Answer

The weight value for characters in Chinese two-character words is determined through a combination of cosine similarity calculations and a mixing mechanism that facilitates the exchange of information between characters. Initially, the importance of each character is calculated based on the cosine similarity between the character’s representation and the representation of its corresponding word. This similarity helps ascertain how closely related each character is to the overall meaning of the word, allowing for an accurate assessment of its significance [2].

Once the importance is measured, this information is integrated into the representation of each character. The integration is performed according to the calculated importance weight, which means that characters deemed more important will have a greater influence on the resulting representation [2]. Subsequently, a mixing mechanism is employed that allows for the exchange of information among the characters within the word. This step enhances the character representation by ensuring that the contextually relevant information from other characters is factored into each character's overall representation [2].

Furthermore, to refine this process, a multi-head attention mechanism is used, including a masked approach that prioritizes important characters while limiting the impact of less relevant ones. This approach mitigates the dilution of character representations due to less significant characters, thereby emphasizing those that contribute meaningfully to the word's overall significance [2]. The culmination of this process produces a final character representation, which is instrumental for downstream natural language processing tasks [2].

In summary, the weight values for characters in Chinese two-character words are systematically determined through a series of steps focusing on cosine similarity evaluations, integration of these values into character representations, and the implementation of attention mechanisms that prioritize important characters while allowing for contextual information exchange [2].

[1]: https://ar5iv.org/html/2310.12751, [2310.12751] Character-level Chinese Backpack Language Models
[2]: https://ar5iv.org/html/2207.05928, [2207.05928] Exploiting Word Semantics to Enrich Character Representations of Chinese Pre-trained Models
[3]: https://ar5iv.org/html/2310.12751, [2310.12751] Character-level Chinese Backpack Language Models
[4]: https://ar5iv.org/html/2310.12751, [2310.12751] Character-level Chinese Backpack Language Models
[5]: https://ar5iv.org/html/2010.07711, [2010.07711] Does Chinese BERT Encode Word Structure?

[1]: Passage ID 1: is composed of the characters "词" (word) and "典" (book, in ancient Chinese), and when generating text from input containing this word, the model could focus on either the "word" or "book" property. By adjusting the weights of the sense vectors of the constituent characters, we were able to amplify implicit meaning of a constituent character and bias the model toward generating text related to a specific property. Specifically, we conducted experiments to amplify the contribution of the first or second character four times each while keeping the total contribution of the word unchanged in the output. We found that the model tended to generate sentences that relate to the amplified character with greater probability, as shown in Appendix 11. We assessed the efficacy of the proposed method by computing the ratio of expectations for the controlled model relative to an uncontrolled model in the context of predicting semantically related characters from an open-topic prompt as
[2]: Passage ID 2: of Chinese pre-trained models in the fine-tuning stage. Firstly, the importance of each character in a word is calculated based on the cosine similarity between the character’s representation and its paired word’s representation. Then we integrate the word embedding into the representation of each internal character according to the importance weight. After that, we apply a mixing mechanism to enable characters within a word exchange information with each other to further enhance the character representation. In addition, we apply a multi-head attention and a masked multi-head attention which masks the unimportant characters, and the final representation is obtained by fusing the representations given by the two attention mechanisms, which is then applied in downstream tasks. Moreover, in order to minimize the impact of word segmentation errors, we adopt a multi-tokenizer voting mechanism to obtain the final word segmentation result.We conduct extensive experiments on several NLP
[3]: Passage ID 3: words are often composed of many characters.We find that our (134M parameter) Chinese Backpack language model performs comparably to a (104M parameter) Transformer, and learns rich character-level meanings that log-additively compose to form word meanings.In SimLex-style lexical semantic evaluations, simple averages of Backpack character senses outperform input embeddings from a Transformer.We find that complex multi-character meanings are often formed by using the same per-character sense weights consistently across context.Exploring interpretability-through control, we show that we can localize a source of gender bias in our Backpacks to specific character senses and intervene to reduce the bias.1 IntroductionFigure 1: The general structure of the character-level Chinese Backpack Language Model. The next character is predicted by the weight sum of the senses of characters in the previous context. The sense vector of "显" (show) provides information for word composition,
[4]: Passage ID 4: Language Model. The next character is predicted by the weight sum of the senses of characters in the previous context. The sense vector of "显" (show) provides information for word composition, while the senses of "电" and "脑" (computer) provide semantic information through linear combination. Language modeling is a crucial task in natural language processing, where the goal is to compute the probability of the next word in a sequence given the preceding words. Recently, large language models based on the Transformer architecture Vaswani et al. (2017) have achieved remarkable success in various NLP applications, including text generation Radford et al. (2018b); Brown et al. (2020); Wang and Komatsuzaki (2021), machine translation Bawden et al. (2019); Lewis et al. (2019), and question-answering Miller et al. (2017); Karpukhin et al. (2020); Ram et al. (2021). However, Transformers are notoriously hard to interpret and control. Their non-linear contextualization functions imply that
[5]: Passage ID 5: in the word “钱其琛(Qichen Qian)”, and the character “题(question)” puts most attention to characters in the word “台湾(Taiwan)”. This indicates that the head 6-5 takes most of the information from the previous words to generate contextualized character representation.4 Probing TaskWe probe the contextualized representation of each character for Chinese Word Segmentation (CWS) [2003]. In particular, CWS can be treated as a character-level sequence labeling task, where the label set includes B, M, E, and S (which stand for the beginning, middle, ending of word and single character word, respectively). We directly use the fixed hidden representations in each layer as the input, on which a trainable linear classifier is built, as Figure 4 shows. We use local classifier rather than the conditional random fields [2001], in order to focus on information extracted from hidden representations directly. The intuition is that if a simple linear classifier can predict the segmentation labels,