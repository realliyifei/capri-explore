# Word Meaning Inducing via Character Ontology: A Survey on the Semantic Prediction of Chinese Two-Character Words

## Question

How is the weight value for characters in Chinese two-character words determined and calculated?

## URLs

1. https://ar5iv.org/html/2310.12751. [2310.12751] Character-level Chinese Backpack Language Models
2. https://ar5iv.org/html/2207.05928. [2207.05928] Exploiting Word Semantics to Enrich Character Representations of Chinese Pre-trained Models
3. https://ar5iv.org/html/2403.18542. [2403.18542] Attention-aware semantic relevance predicting Chinese sentence reading
4. https://ar5iv.org/html/2109.07877. [2109.07877] MFE-NER: Multi-feature Fusion Embedding for Chinese Named Entity Recognition
5. https://ar5iv.org/html/2405.07006. [2405.07006] Word-specific tonal realizations in Mandarin
6. https://ar5iv.org/html/2010.07711. [2010.07711] Does Chinese BERT Encode Word Structure?
7. https://ar5iv.org/html/2102.00621. [2102.00621] Polyphone Disambiguition in Mandarin Chinese with Semi-Supervised Learning
8. https://ar5iv.org/html/2201.12155. [2201.12155] Reducing Multilingual Context Confusion for End-to-end Code-switching Automatic Speech Recognition
9. https://ar5iv.org/html/2411.11770. [2411.11770] CNMBert: A Model For Hanyu Pinyin Abbreviation to Character Conversion Task

## Answer (I don't know?: ✗)

The weight value for characters in Chinese two-character words is determined and calculated based on several key methodologies outlined in the provided context. 

Firstly, the importance of each character within a word is calculated using the cosine similarity between the representation of the character and the representation of the paired word. This metric helps in quantifying how closely aligned the meaning of a character is with its corresponding word representation, allowing for a more nuanced understanding of each character's contribution [2].

Once the importance weights are established, the word embedding is integrated into the internal representation of each character based on these weights. This integration process is critical because it ensures that the character representations carry essential semantic information from the word context, enhancing the quality of the representations used in downstream tasks [2].

To further refine the character representations, a mixing mechanism is employed, which enables characters within a word to exchange information with one another. This collaborative representation allows for a richer understanding of how characters interact in the context of a word, ultimately leading to enhanced semantic representation [2].

Additionally, multi-head attention and masked multi-head attention mechanisms are implemented. The masked version helps to focus on the more important characters by reducing the influence of those deemed less significant, thus ensuring that the final representation reflects the relevant semantic structures of the characters more effectively [2].

Moreover, to mitigate errors stemming from word segmentation, a multi-tokenizer voting mechanism is utilized to derive the final segmentation results. This approach aids in minimizing the impact of any inaccuracies in how words are formed from characters, providing a more accurate basis for determining the weights assigned to each character [2].

In summary, the determination of weight values for characters in Chinese two-character words involves calculating importance through cosine similarity, integrating word embeddings, facilitating character interaction through mixing mechanisms and attention strategies, and addressing segmentation issues. These methodologies work together to enhance the character representations used in natural language processing tasks.

1. [1]:  https://ar5iv.org/html/2310.12751, [2310.12751] Character-level Chinese Backpack Language Models
2. [2]:  https://ar5iv.org/html/2207.05928, [2207.05928] Exploiting Word Semantics to Enrich Character Representations of Chinese Pre-trained Models
3. [3]:  https://ar5iv.org/html/2207.05928, [2207.05928] Exploiting Word Semantics to Enrich Character Representations of Chinese Pre-trained Models
4. [4]:  https://ar5iv.org/html/2010.07711, [2010.07711] Does Chinese BERT Encode Word Structure?
5. [5]:  https://ar5iv.org/html/2310.12751, [2310.12751] Character-level Chinese Backpack Language Models
---
1. [1]:  Passage ID 1: is composed of the characters "词" (word) and "典" (book, in ancient Chinese), and when generating text from input containing this word, the model could focus on either the "word" or "book" property. By adjusting the weights of the sense vectors of the constituent characters, we were able to amplify implicit meaning of a constituent character and bias the model toward generating text related to a specific property. Specifically, we conducted experiments to amplify the contribution of the first or second character four times each while keeping the total contribution of the word unchanged in the output. We found that the model tended to generate sentences that relate to the amplified character with greater probability, as shown in Appendix 11. We assessed the efficacy of the proposed method by computing the ratio of expectations for the controlled model relative to an uncontrolled model in the context of predicting semantically related characters from an open-topic prompt as
2. [2]:  Passage ID 2: of Chinese pre-trained models in the fine-tuning stage. Firstly, the importance of each character in a word is calculated based on the cosine similarity between the character’s representation and its paired word’s representation. Then we integrate the word embedding into the representation of each internal character according to the importance weight. After that, we apply a mixing mechanism to enable characters within a word exchange information with each other to further enhance the character representation. In addition, we apply a multi-head attention and a masked multi-head attention which masks the unimportant characters, and the final representation is obtained by fusing the representations given by the two attention mechanisms, which is then applied in downstream tasks. Moreover, in order to minimize the impact of word segmentation errors, we adopt a multi-tokenizer voting mechanism to obtain the final word segmentation result.We conduct extensive experiments on several NLP
3. [3]:  Passage ID 3: "dark"); } else { localStorage.setItem("ar5iv_theme", "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }11institutetext: MOE Key Laboratory of Computational Linguistics, Peking University 22institutetext: School of Software & Microelectronics, Peking University, Beijing, China 33institutetext: School of Computer Science, Peking University, Beijing, China33email: {2001210322,sunrui0720}@stu.pku.edu.cn, wuyf@pku.edu.cnExploiting Word Semantics to Enrich Character Representations of Chinese Pre-trained ModelsWenbiao Li1122  Rui Sun1122  Yunfang Wu1133AbstractMost of the Chinese pre-trained models adopt characters as basic units for downstream tasks. However, these models ignore the information carried by words and thus lead to the loss of some important semantics.In this paper, we propose a new method to exploit word structure and integrate lexical semantics into character representations of pre-trained models.
4. [4]:  Passage ID 4: "light"); } } else { localStorage.setItem("ar5iv_theme", "dark"); } detectColorScheme(); }Does Chinese BERT Encode Word Structure?Yile Wang1,2,3, Leyang Cui1,2,3 and Yue Zhang2,31Zhejiang University 2School of Engineering, Westlake University 3Institute of Advanced Technology, Westlake Institute for Advanced Study {wangyile,cuileyang}@westlake.edu.cnyue.zhang@wias.org.cnAbstractContextualized representations give significantly improved results for a wide range of NLP tasks. Much work has been dedicated to analyzing the features captured by representative models such as BERT. Existing work finds that syntactic, semantic and word sense knowledge are encoded in BERT. However, little work has investigated word features for character-based languages such as Chinese. We investigate Chinese BERT using both attention weight distribution statistics and probing tasks, finding that (1) word information is captured by BERT; (2) word-level features are mostly in the middle
5. [5]:  Passage ID 5: words are often composed of many characters.We find that our (134M parameter) Chinese Backpack language model performs comparably to a (104M parameter) Transformer, and learns rich character-level meanings that log-additively compose to form word meanings.In SimLex-style lexical semantic evaluations, simple averages of Backpack character senses outperform input embeddings from a Transformer.We find that complex multi-character meanings are often formed by using the same per-character sense weights consistently across context.Exploring interpretability-through control, we show that we can localize a source of gender bias in our Backpacks to specific character senses and intervene to reduce the bias.1 IntroductionFigure 1: The general structure of the character-level Chinese Backpack Language Model. The next character is predicted by the weight sum of the senses of characters in the previous context. The sense vector of "显" (show) provides information for word composition,