# Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

What are the challenges and limitations associated with pre-identifying multiword expressions (MWEs) before parsing in natural language processing (NLP) systems?

## URLs

1. https://ar5iv.org/html/2306.10419. [2306.10419] Untitled Document
2. https://ar5iv.org/html/2208.07832. [2208.07832] BERT(s) to Detect Multiword Expressions
3. https://ar5iv.org/html/1810.09947. [1810.09947] Object-oriented lexical encoding of multiword expressions: Short and sweet
4. https://ar5iv.org/html/2304.14745. [2304.14745] Made of Steel? Learning Plausible Materials for Components in the Vehicle Repair Domain
5. https://ar5iv.org/html/2005.10583. [2005.10583] MultiMWE: Building a Multi-lingual Multi-Word Expression (MWE) Parallel Corpora
6. https://ar5iv.org/html/2110.10064. [2110.10064] Idiomatic Expression Identification using Semantic Compatibility
7. https://ar5iv.org/html/2411.02610. [2411.02610] Investigating Idiomaticity in Word Representations
8. https://ar5iv.org/html/1911.08829. [1911.08829] Casting a Wide Net: Robust Extraction of Potentially Idiomatic Expressions
9. https://ar5iv.org/html/2106.00473. [2106.00473] SemEval-2021 Task 1: Lexical Complexity Prediction

## Answer (I don't know?: ‚úó)

Pre-identifying multiword expressions (MWEs) before parsing in natural language processing (NLP) systems poses several challenges and limitations that significantly complicate the processing of natural language. One of the primary issues is the non-compositionality of MWEs, meaning that the individual components of an MWE do not contribute to a straightforward, combined meaning. This characteristic is particularly problematic because MWEs can obscure the intended meaning when their parts are interpreted individually, which is a common scenario in various NLP applications including parsing and machine translation (MT) [1] [2].

Another significant challenge is the non-substitutability of MWEs. This feature implies that the components of MWEs cannot be replaced with synonyms, which limits flexibility in linguistic processing. For instance, phrases like "by and large" cannot simply have "by" or "large" replaced with synonyms without losing their intended meaning [2]. Additionally, both MWEs and non-MWEs can be ambiguous, presenting further complications. An example of this is the phrase "by and large," which can have different interpretations depending on context, leading to potential pitfalls in understanding during parsing tasks [2].

Moreover, the difficulties in correctly integrating MWEs into parsing processes create fundamental problems for various NLP tasks. Accurate parsing is crucial for generating meaningful and contextually relevant translations or interpretations of text. The complications arise because traditional parsing methods often rely on clear distinctions between word tokens and phrases, which becomes convoluted in the presence of MWEs [1] [2]. Consequently, NLP systems must develop specialized strategies for handling MWEs to maintain accuracy in parsing and downstream applications.

Furthermore, there are limitations associated with pre-trained word embeddings traditionally used in NLP models. These embeddings often treat polysemous words (words with multiple meanings) the same, which can lead to inaccuracies when processing MWEs that would benefit from context-specific representations. This limitation exacerbates the challenges posed by MWEs, as the ambiguous nature of these phrases can impede the performance of models that rely on traditional embeddings [1] [5].

Neural network architectures have shown promise in addressing some of these challenges, particularly models that incorporate context, such as Long Short Term Memory (LSTM) and Convolutional Neural Networks (CNNs). These architectures have achieved state-of-the-art performance in MWE identification tasks [1] [5]. However, research into different transformer models specifically for MWE identification remains limited [5]. As a result, there is a clear need for further exploration and adaptation of models that can effectively handle the complexity of MWEs to improve overall NLP performance.

In conclusion, the challenges and limitations associated with pre-identifying MWEs in NLP systems stem from their non-compositional nature, non-substitutability, ambiguity, and complications arising from traditional word embedding approaches. Addressing these issues is crucial for advancing the effectiveness of parsing and related applications [1] [2] [5].

1. [1]:  https://ar5iv.org/html/2208.07832, [2208.07832] BERT(s) to Detect Multiword Expressions
2. [2]:  https://ar5iv.org/html/2208.07832, [2208.07832] BERT(s) to Detect Multiword Expressions
3. [3]:  https://ar5iv.org/html/2106.00473, [2106.00473] SemEval-2021 Task 1: Lexical Complexity Prediction
4. [4]:  https://ar5iv.org/html/2005.10583, [2005.10583] MultiMWE: Building a Multi-lingual Multi-Word Expression (MWE) Parallel Corpora
5. [5]:  https://ar5iv.org/html/2208.07832, [2208.07832] BERT(s) to Detect Multiword Expressions
---
1. [1]:  Passage ID 1: (MT) [17, 16], which depends on a clear distinction between word tokens and phrases, has to be re-thought to accommodate MWEs [8, 29]. The usual approach in these applications is to identify MWEs first, and then treat them accordingly. Therefore, detecting MWEs is a key research area in NLP.In recent years, the identification of MWEs has been modelled as a supervised machine learning task where the machine learning models are trained on an annotated dataset. As we explain in Section 2, several datasets have been released to train these machine learning models. Furthermore shared tasks such as SemEval-2016 Task 10 [28] and PARSEME [27] have contributed to develop datasets. In recent years, neural network-based models, and in particular architectures incorporating RecurrentNeural Networks (RNNs) such as Long Short Term Memory (LSTM) and Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in MWE identification tasks [27]. Usually, these models utilise
2. [2]:  Passage ID 2: categories such as lexicalised phrases and institutionalised phrases; however the basic definition remains same in all the categories. MWEs appear in almost all languages and is a common method of expressing ideas.Apart from the difficulty of deriving meaning from individual components, which is known as non-compositionality in phraseology, MWEs have several challenges when processing them computationally [8]. 1. MWEs are non-substitutable, which means that the components of MWE cannot be replaced by synonyms (e.g., by and big). 2. MWEs and non-MWEs can be ambiguous (e.g., by and large, we agree vs he walked by and large tractors passed him) . These unique challenges in MWEs raise several fundamental problems with many NLP applications. For example, parsing and machine translation (MT) [17, 16], which depends on a clear distinction between word tokens and phrases, has to be re-thought to accommodate MWEs [8, 29]. The usual approach in these applications is to identify MWEs
3. [3]:  Passage ID 3: to the input embeddings, or may be concatenated at the output prior to further training. Whilst this strategy appears to be the best of both worlds, uniting linguistic knowledge with the power of pre-trained language models, the hybrid systems do not tend to perform as well as either feature based or deep learning systems.6.3 MWEsFor Sub-task 2 we asked participants to submit both predictions for single words and multi-words from our corpus. We hoped this would encourage participants to consider models that adapted single word lexical complexity to multi-word lexical complexity. We observed a number of strategies that participants employed to create the annotations for this secondary portion of our data.For systems that employed a deep learning approach, it was relatively simple to incorporate MWEs as part of their training procedure. These systems encoded the input using a query and context, separated by a ‚ü®S‚ÄãE‚ÄãP‚ü©delimited-‚ü®‚ü©ùëÜùê∏ùëÉ\langle SEP\rangle token. The number of tokens
4. [4]:  Passage ID 4: of this paper.2.1. Machine Translation and Multiword ExpressionsMT methods seek to translate one human language into another one. MT belongs to a branch of computational linguistics (CL) and artificial intelligence (AI), in which researchers try to use computational modeling to address linguistic text translation problems. It is a very challenging task for MT to achieve both accuracy of translated information and fluency at the level of a human expert‚Äôs performance or what linguists expect as output. There are many reasons for this, one of which is that the use of MWEs presents a significant obstacle for a machine to learn and generate human languages in a natural form. We use three examples to illustrate the importance of correct use of MWEs in MT.We use ZH/Zh to represent Chinese, and EN/En as English. We use pinyin (pƒ´nyƒ´n) to annotate the B√°ihu√† Chinese for its pronunciation and tones (phoneticism). The MT outputs in the examples were from Google Translator engine [Vaswani
5. [5]:  Passage ID 5: (RNNs) such as Long Short Term Memory (LSTM) and Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in MWE identification tasks [27]. Usually, these models utilise pre-trained word embedding models such as word2vec [15] and glove [22]. We describe these models in Section 2. However, these traditional word embeddings provide the same embedding for polysemous words [21] [20]. Therefore, non-substitutability and the ambiguous nature of the MWEs can cause complications with traditional word embeddings.A possible solution is to utilise neural architectures such as transformers that incorporate context more into the learning process. However, as far as we know, there has not been any research done to compare the performance of different transformer models in the MWE identification task. In this research, we empirically evaluate several transformer models in detecting MWEs to fill this gap. The findings of this research can be beneficial for many NLP