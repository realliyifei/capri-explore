# Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey

## Question

What are the primary limitations of employing the top-k approximation in neural ranking models?

## URLs

1. https://ar5iv.org/html/2402.09360
2. https://ar5iv.org/html/2409.00822
3. https://ar5iv.org/html/2407.07912
4. https://ar5iv.org/html/2310.10462
5. https://ar5iv.org/html/2312.16828
6. https://ar5iv.org/html/1706.03847
7. https://ar5iv.org/html/2405.14589
8. https://ar5iv.org/html/1903.06902

## Answer

I don't know. The context provided does not contain information specifically addressing the primary limitations of employing the top-k approximation in neural ranking models. While it mentions reasoning strategies and discusses influences of various data sources for training, it does not elaborate on the limitations associated with top-k approximations or neural ranking models specifically. The focus appears to be more on general issues in NLP, the effectiveness of language models, and data scarcity in under-analyzed languages without detailing the mechanics of ranking models or approximations. Therefore, the context is insufficient to answer the question thoroughly.

[1]: https://ar5iv.org/html/2407.06564, No Title
[2]: https://ar5iv.org/html/2411.12580, No Title
[3]: https://ar5iv.org/html/2305.12544, No Title
[4]: https://ar5iv.org/html/1807.10854, No Title
[5]: https://ar5iv.org/html/2411.12580, No Title

[1]: Passage ID 1: recent years, Natural Language Processing (NLP) has played a significant role in various Artificial Intelligence (AI) applications such as chatbots, text generation, and language translation. The emergence of large language models (LLMs) has greatly improved the performance of these applications, showing astonishing results in language understanding and generation. However, they still show some disadvantages, such as hallucinations and lack of domain-specific knowledge, that affect their performance in real-world tasks.These issues can be effectively mitigated by incorporating knowledge graphs (KGs), which organise information in structured formats that capture relationships between entities in a versatile and interpretable fashion. Likewise, the construction and validation of KGs present challenges that LLMs can help resolve. The complementary relationship between LLMs and KGs has led to a trend that combines these technologies to achieve trustworthy results. This work collected 28
[2]: Passage ID 2: and compare the count to the pretraining distribution. We present the details in Appendix A.8.4, but mention here that code data is highly influential for reasoning. StackExchange as a source has ten times more influential data in the top portions of the rankings than expected if the influential data was randomly sampled from the pretraining distribution. Other code sources are twice as influential as expected when drawing randomly from the pretraining distribution for k=50ùëò50k=50 up to k=50000ùëò50000k=50000. Similar patterns hold for the bottom portions of the rankings.6 Discussion, Limitations, and Future WorkIn this work, we investigate what kind of generalisation strategy two LLMs (7B and 35B respectively) employ when reasoning, and contrast it to the strategy used for a task that requires retrieving factual parametric knowledge. By creating rankings for 200 such questions over 5 million pretraining documents based on their influence on the likelihood of the completions, we
[3]: Passage ID 3: as work to date has primarily focused on English or other high-resource languages Mondal et¬†al. (2022) but devoted less efforts towards minority languages. Additionally, the lack of human evaluation of NLP-based health systems has made it challenging to measure their effectiveness in the real world. Current automatic evaluation metrics do not necessarily speak to patient outcomes. Hence, human-centric studies must be conducted in evaluating the efficacy of NLP-powered tools in healthcare.Research Directions.1.Healthcare benchmark construction. Although the documentation of recent LLMs reports very high performance for various medical question answering benchmarks, or medical licensing texts, there are many other tasks in healthcare that lack the data required to achieve similarly good performance. Access to medical datasets is often limited because of privacy issues, and therefore other approaches may be required to compile such benchmarks. Synthetic datasets are one such
[4]: Passage ID 4: NLP software. Furthermore, there are thousands of languages spoken throughout the world, with at least eighty spoken by more than 10 million people, meaning that current research excludes an immense segment of humankind. Collection and validation of data in under-analyzed languages, as well as testing NLP models using such data, will be a tremendous contribution to not only the field of natural language processing, but to human society as a whole.Due to the small amounts of data available in many languages, the authors do not foresee the complete usurpation of traditional NLP models by deep learning any time in the near future. Deep learning models (and even shallow ANNs) are extremely data-hungry. Contrastingly, many traditional models require only relatively small amounts of training data. However, looking further forward, it can be anticipated that deep learning models will become the norm in computational linguistics, with pre-training and transfer learning playing highly
[5]: Passage ID 5: law for factual questions than for reasoning (meaning more of the total positive influence contained at the top parts of the ranking), but our results show evidence for the opposite. Perhaps a model needs to generalise from a broader set of documents for factual retrieval than for reasoning because it needs to see the same information more often to memorise it. This is supported by the finding that for factual questions the answer often shows up multiple times in the top 0.01% most influential data.There are important limitations to our approach, most notably that we do not calculate influence on the entire training set, which is intractable. An alternative explanation of our results is then the opposite conclusion: the model is relying on data for reasoning that are so infrequent that a random sample of 2.5B tokens does not surface relatively highly influential samples for any of the 60 reasoning queries. This would result in the conclusion that LLMs rely on sparse and infrequent