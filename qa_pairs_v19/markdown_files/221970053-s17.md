# A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English

## Question

What are the two approaches for predicting probability in language model-based methods, and which approach yields better performance?

## URLs

1. https://ar5iv.org/html/2404.08382. [2404.08382] Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think
2. https://ar5iv.org/html/2406.04446. [2406.04446] Can Language Models Use Forecasting Strategies?
3. https://ar5iv.org/html/2402.13887. [2402.13887] Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models
4. https://ar5iv.org/html/2402.02047. [2402.02047] Calibration and Correctness of Language Models for Code
5. https://ar5iv.org/html/2401.04757. [2401.04757] How predictable is language model benchmark performance?
6. https://ar5iv.org/html/2403.14668. [2403.14668] Predicting Learning Performance with Large Language Models: A Study in Adult Literacy
7. https://ar5iv.org/html/2212.11281. [2212.11281] Language models are better than humans at next-token prediction
8. https://ar5iv.org/html/2410.02725. [2410.02725] Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation
9. https://ar5iv.org/html/2410.01035. [2410.01035] Untitled Document
10. https://ar5iv.org/html/2402.13213. [2402.13213] Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&A

## Answer (I don't know?: ‚úó)

The two approaches for predicting probability in language model-based methods, as discussed in the context, are probability-based methods and generation-based predictions. Probability-based methods involve selecting the option with the highest probability assigned by the model to predict the next token, which is relatively straightforward but may not fully capture the richness of the model's output [4]. Generation-based methods, on the other hand, focus on the overall quality of the generated text or the context provided rather than simply the token probabilities.

The context indicates that there is a significant disconnect between these two approaches, particularly highlighting that even when probability-based predictions are correct, the alignment with generation-based predictions is notably low [5]. This suggests that while probability-based methods may offer a quick assessment of model performance, they might not necessarily reflect the true capabilities of the language models in terms of generating coherent or relevant outputs.

In terms of performance, the findings from the context indicate that probability-based methods do not consistently yield better outcomes compared to generation-based methods. Specifically, it mentions instances where the performance rankings from multiple-choice NLP benchmark rankings do not align with human preferences for free-text generation output, implying that generation-based methods may be more indicative of the models' true abilities in practical applications [5].

Ultimately, while both approaches have their merits, the generation-based approach appears to be more aligned with measuring actual performance in natural language tasks as it accounts for broader context and the quality of text generation, beyond just the mere probability of token prediction [5].

1. [1]:  https://ar5iv.org/html/2212.11281, [2212.11281] Language models are better than humans at next-token prediction
2. [2]:  https://ar5iv.org/html/2212.11281, [2212.11281] Language models are better than humans at next-token prediction
3. [3]:  https://ar5iv.org/html/2406.04446, [2406.04446] Can Language Models Use Forecasting Strategies?
4. [4]:  https://ar5iv.org/html/2402.13887, [2402.13887] Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models
5. [5]:  https://ar5iv.org/html/2402.13887, [2402.13887] Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models
---
1. [1]:  Passage ID 1: clear whether language models are better or worse than humans at next token prediction. To try to answer this question, we performed two distinct experiments to directly compare humans and language models on this front: one measuring top-1 accuracy and the other measuring perplexity. In both experiments, we find humans to be consistently worse than even relatively small language models like GPT3-Ada at next-token prediction.1 IntroductionRecent language models (LMs) have demonstrated impressive capabilities in natural language tasks, like writing convincing human-like text, coding, or answering general knowledge questions. However, LMs are not considered to have yet surpassed human performance at these tasks. But performance at such tasks is not a fair way of comparing LMs and humans. LMs are not explicitly trained to perform well at natural language tasks. Their loss function is simply next token prediction: accurately predicting the next token given previous tokens in tokenized
2. [2]:  Passage ID 2: many predictions, that the predictor assigns the highest probability to the correct next token. This is relatively easy to measure, but does not capture information about the rest of the probability distribution that the predictor assigns over possible next tokens. A more all-encompassing but harder to measure variable is perplexity: defined by 2Lsuperscript2ùêø2^{L} where the loss LùêøL is the cross-entropy of the predictors distribution and the true distribution over possible next tokens.Contrary to some previous claims, we found humans to be consistently worse at next token prediction than even small models like GPT3-Ada (with 350 million parameters), in terms of both top-1 accuracy and perplexity. That is, even small LMs are superhuman at next token prediction.We structure this paper as follows. We first review claims made to date about the comparison between human and language model next token prediction in Section 2. In sections 3 and 4 we detail two small experiments we ran to
3. [3]:  Passage ID 3: A number of events were collected by the tournament organizers. Forecasters made predictions on the likelihood of these events taking place from 0 to 1 and then were scored by the accuracy of their predictions. Mellers et al.[11] and later Tetlock et al.[17] found that indeed some participants consistently outperformed their peers in accuracy and certain strategies seemed correlated with an ability to correctly predict if events would occur.In this paper, we study whether Large Language Models (LLMs) can meet or even exceed human performance on forecasting tasks. We first describe a novel dataset of prediction events and associated human forecasts, and discuss the tradeoffs of various approaches for implementing and measuring LLM forecasting abilities. We then present results from instructing an LLM to use several "superforecasting" [17] strategies known to improve human forecasters‚Äô performance, ultimately finding that our LLM superforecasting approaches do not consistently
4. [4]:  Passage ID 4: the scale of model parameters of language models expands from the million to billion or even trillion levels, a proficient LLM is expected to exhibit a broad mastery across various tasks. Recent works aim to assess LLMs comprehensively by aggregating a substantial array of NLP benchmarks (Srivastava et¬†al., 2022; Sanh et¬†al., 2022; Liang et¬†al., 2022; Longpre et¬†al., 2023). Additionally, there exists a line of research that curates human exam questions to challenge LLMs (Hendrycks et¬†al., 2021; Huang et¬†al., 2023; Li et¬†al., 2023b; Koto et¬†al., 2023). The collected questions and NLP benchmarks are adapted into prompts via standardized templates.Figure 1: An illustration of label-based, sequence-based and generation-based predictions for evaluating LLMs on NLP benchmarks.Due to computational constraints, recent evaluation frameworks commonly adopt the approach of selecting the option with the highest probability as the prediction of LLMs, as illustrated in Figure¬†1. These
5. [5]:  Passage ID 5: to accurately assess the capabilities of LLMs?In this position study, we argue that the current LLM evaluation and leaderboard misalign the actual LLM capabilities. We examine three prediction methodologies: generation-based, label-based, and sequence-based predictions. We conducted extensive experiments across LLMs with varying model sizes on three prominent benchmarks: MMLU (Hendrycks et¬†al., 2021), TruthfulQA (Lin et¬†al., 2022), and Belebele (Bandarkar et¬†al., 2023). Our findings reveal a significant disconnect between probability-based methods and generation-based predictions. Even when predictions are correct, the consistency between probability-based methods and generation-based predictions remains notably low. We additionally find that many of these multiple-choice NLP benchmark rankings do not agree with human preference for free-text generation output.Consequently, these results raise serious doubts about the reliability of evaluation outcomes derived from popular