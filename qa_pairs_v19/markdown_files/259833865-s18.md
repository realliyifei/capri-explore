# A Survey of Evaluation Methods of Generated Medical Textual Reports

## Question

What is the significance of involving domain experts in the human evaluation of generated medical textual reports?

## URLs

1. https://ar5iv.org/html/2410.17357. [2410.17357] Image-aware Evaluation of Generated Medical Reports
2. https://ar5iv.org/html/2401.06775. [2401.06775] Large Language Models in Healthcare and Medical Domain: A Review
3. https://ar5iv.org/html/2402.15422. [2402.15422] A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models
4. https://ar5iv.org/html/2310.05694. [2310.05694] A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics
5. https://ar5iv.org/html/2406.15045. [2406.15045] Harnessing Knowledge Retrieval with Large Language Models for Clinical Report Error Correction
6. https://ar5iv.org/html/2410.23851. [2410.23851] Leveraging Large Language Models for Medical Information Extraction and Query Generation
7. https://ar5iv.org/html/2309.07430. [2309.07430] Adapted Large Language Models Can Outperform Medical Experts in Clinical Text Summarization
8. https://ar5iv.org/html/2406.15019. [2406.15019] MedOdyssey: A Medical Domain Benchmark for Long Context Evaluation Up to 200K Tokens
9. https://ar5iv.org/html/2410.00163. [2410.00163] Adapting LLMs for the Medical Domain in Portuguese: A Study on Fine-Tuning and Model Evaluation This study was funded by the São Paulo Research Foundation (FAPESP) grants 2013/07375-0, 2019/07665-4, 2023/14427-8, 2024/00789-8, and 2024/01336-7, and the National Council for Scientific and Technological Development (CNPq) grants 308529/2021-9 and 400756/2024-2. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.

## Answer (I don't know?: ✗)

Involving domain experts in the human evaluation of generated medical textual reports is crucial for several reasons. First, the complex nature of medical tasks necessitates a thorough and nuanced understanding that general NLP metrics alone cannot provide. Current NLP systems, including advanced language models (LLMs), often face limitations that can lead to inaccuracies, particularly in specialized fields like healthcare. This highlights the need for human experts to evaluate the clinical readiness of these models [1][2].

One significant point is that language models do not effectively address the specific requirements of medical communication, such as coherence, factual consistency, evidence of statements, and professionalism. These criteria are essential for ensuring the quality and reliability of medical reports and responses. Human evaluations reveal that the average scores for AI-generated question answering are significantly lower than those for question generation, indicating a gap in performance that could affect real-world patient care [5]. Thus, expert insights can help identify deficiencies in model outputs that automated metrics overlook, and contribute to improving the overall quality of AI-generated medical text.

Furthermore, there are specific healthcare tasks where current benchmarks and datasets are inadequate to measure the effectiveness of NLP outputs properly. The lack of human evaluation in assessing NLP-based health systems’ real-world effectiveness makes it challenging to gauge their impact on patient outcomes [3]. Consequently, involving medical domain experts is not just beneficial but necessary to bridge this gap and enhance the evaluation of LLMs.

The critical nature of healthcare information necessitates that evaluations adapt beyond standard NLP metrics to include more relevant clinical measures and expert assessments [2]. As medical language increasingly relies on accurate communication, the integration of expert evaluations enables a more reliable assessment of how well LLMs can assist healthcare professionals [2][3].

Additionally, considering that privacy issues limit access to medical datasets, employing mechanisms where experts evaluate the outputs from AI-generated systems can help guide future improvements and adaptations of LLMs for better performance in real-world medical contexts [4]. Experts can also address the challenges posed by the inherent limitations in data sharing and the operational uses of LLMs in clinical settings, leading to innovations in how these technologies can be employed [4]. 

In summary, the involvement of domain experts in evaluating generated medical textual reports is significant because it ensures that AI outputs meet the critical quality standards required in healthcare, supports the development of better benchmarks, and enhances the efficacy of NLP solutions in a highly specialized and sensitive field. This multifaceted evaluation approach ultimately contributes to more reliable patient care and trust in AI-driven healthcare technologies [3][5].

1. [1]:  https://ar5iv.org/html/2310.05694, [2310.05694] A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics
2. [2]:  https://ar5iv.org/html/2309.07430, [2309.07430] Adapted Large Language Models Can Outperform Medical Experts in Clinical Text Summarization
3. [3]:  https://ar5iv.org/html/2305.12544, No Title
4. [4]:  https://ar5iv.org/html/2410.23769, No Title
5. [5]:  https://ar5iv.org/html/2410.23769, No Title
---
1. [1]:  Passage ID 1: it is important to note that their present capabilities do not categorize them as comprehensive AI systems. They still face performance limitations, particularly when compared to expert models, across multiple domains that necessitate domain-specific knowledge. On the other hand, the state-of-the-art LLMs demonstrate commendable performance in grasping general scientific knowledge and are capable of generating open-ended responses to science-related inquiries. Nevertheless, they are susceptible to errors, particularly when tackling questions that necessitate intricate multi-step reasoning. The exceptional proficiency in language presents a hurdle for users to accurately evaluate the factual correctness of information, thereby giving rise to a spectrum of ethical considerations.In this section, we will begin by introducing studies on the evaluation of general NLP tasks.Subsequently, we will review studies focusing on Healthcare evaluation, discussing aspects such as robustness,
2. [2]:  Passage ID 2: a concise “problem list” of medical diagnoses [42]. Lastly, there has been significant work on summarizing extended conversations between a doctor and patient into patient visit summaries [43, 44, 28].While the aforementioned contributions incorporate methods to adapt language models, they often include only a small subset of potential approaches and models, and/or they predominantly rely on evaluation via standard NLP metrics. Given the critical nature of medical tasks, demonstrating clinical readiness requires including human experts in the evaluation process. To address this, there have been recent releases of expert evaluations for instruction following [3] and radiology report generation [45]. Other work employs human experts to evaluate synthesized Cochrane review abstracts, demonstrating that NLP metrics are not sufficient to measure summary quality [46]. With this in mind, weextend our comprehensive evaluation of methods and LLMs beyond NLP metrics to incorporate a clinical
3. [3]:  Passage ID 3: as work to date has primarily focused on English or other high-resource languages Mondal et al. (2022) but devoted less efforts towards minority languages. Additionally, the lack of human evaluation of NLP-based health systems has made it challenging to measure their effectiveness in the real world. Current automatic evaluation metrics do not necessarily speak to patient outcomes. Hence, human-centric studies must be conducted in evaluating the efficacy of NLP-powered tools in healthcare.Research Directions.1.Healthcare benchmark construction. Although the documentation of recent LLMs reports very high performance for various medical question answering benchmarks, or medical licensing texts, there are many other tasks in healthcare that lack the data required to achieve similarly good performance. Access to medical datasets is often limited because of privacy issues, and therefore other approaches may be required to compile such benchmarks. Synthetic datasets are one such
4. [4]:  Passage ID 4: answers for medical qualification exams aimed at medical students, interns and residents can be a significant focus of future research.1 IntroductionIn the interdisciplinary field of artificial intelligence and medicine, text generation is a challenging yet significant task. The rise of deep learning has brought great opportunities to the medical text generation, especially the booming development of Transformer-based autoregressive language models, which has considerably enhanced the model’s ability to process long contextual semantics, thereby enabling the generation of coherent and comprehensive text. Further, the automated generation of patient reports, doctor-patient question answering, and diagnostic reasoning, all rely on extensive and high-quality medical corpora. Nevertheless, due to hospital privacy protection requirements and the challenges in data sharing, LLMs show limited generalization to real-world medical usage.Researchers have employed fine-tuning, prompting
5. [5]:  Passage ID 5: in determining whether the questions are professionally appropriate, the LLMs still incur a loss of critical information during the process of extracting and abstracting information from the input reports.Furthermore, human medical experts evaluated the AI-generated answers based on four criteria: coherence, factual consistency, evidence of statement, and professionalism. Figure 2 depicts the average scores of different LLMs’ answers toward a same sampled set of AI-generated questions. It can be observed that the average score for question answering is lower than that for question generation, with LLMs’ ratings hovering around 3.5 across all evaluation metrics. This indicates a significant gap between the performance of LLMs and the critical requirements of human experts for medical open-ended question answering. Moreover, with the task of question answering in the specific domain of elderly chronic diseases under prompting of limited references, identifying and improving strategies