# Which *BERT? A Survey Organizing Contextualized Encoders

## Question

What strategies are recent research efforts employing to reduce the wall-clock training time for BERT models?

## URLs

1. https://ar5iv.org/html/2404.11015. [2404.11015] FedFa: A Fully Asynchronous Training Paradigm for Federated Learning
2. https://ar5iv.org/html/2407.15819. [2407.15819] Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight
3. https://ar5iv.org/html/2410.18779. [2410.18779] A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs
4. https://ar5iv.org/html/2406.00061. [2406.00061] STAT: Shrinking Transformers After Training
5. https://ar5iv.org/html/2306.09305. [2306.09305] Fast Training of Diffusion Models with Masked Transformers
6. https://ar5iv.org/html/2310.14423. [2310.14423] A Quadratic Synchronization Rule for Distributed Deep Learning
7. https://ar5iv.org/html/2406.06962. [2406.06962] Evolving Subnetwork Training for Large Language Models
8. https://ar5iv.org/html/2201.12423. [2201.12423] Benchmarking Resource Usage for Efficient Distributed Deep Learning

## Answer (I don't know?: ✓)

I don't know. The context provided does not contain specific information regarding recent research strategies employed to reduce the wall-clock training time for BERT models. While it mentions various aspects of BERT and other models in terms of their architectures, training challenges, and research opportunities, it lacks direct references to methods or strategies that specifically target reducing training time for BERT. Therefore, without that specific information, I cannot provide a comprehensive answer.

1. [1]:  https://ar5iv.org/html/2305.12544, No Title
2. [2]:  https://ar5iv.org/html/2010.15036, No Title
3. [3]:  https://ar5iv.org/html/2305.12544, No Title
4. [4]:  https://ar5iv.org/html/1908.10063, No Title
5. [5]:  https://ar5iv.org/html/1807.10854, No Title
---
1. [1]:  Passage ID 1: (Kurdi et al., 2020)) and tools for education researchers (e.g., systems to build representations of classroom interactions (Alic et al., 2022)). Researchers have been testing the application of models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) in these areas since their release, and are now beginning to incorporate larger models.Gaps.Many of the deployed NLP applications in the education space have been developed prior to wide spread use of LLMs, and we are likely to see large-scale deployment of task-specific models based on LLMs soon. While much of the prior work includes standalone applications, developing models that can easily be incorporated into existing education pipelines, e.g., by integrating what students have learned thus far, is an area open for further exploration. Importantly, a long-standing goal in education is to personalize materials and assessments to the needs of individual students, and NLP has the potential to contribute towards that
2. [2]:  Passage ID 2: that is long-term or pragmatic.BERT is trained in the dataset of Books Corpus (Zhu et al., 2015) and English Wikipedia text passages. There are two BERT pre-trained model available: BERT-Base and BERT-Large. BERT can be used on un-annotated data or fine-tuned on one’s task-specific data straight from the pre-trained model. The publicly accessible pre-trained model and fine-tuning code are available online 333https://github.com/google-research/bert.•BERT Variants:Recent research also explores and strengthens the goal and architecture ofBERT. Some of them are briefly discussed below:•GPT2: The OpenAI team released a scaled-up variant of GPT in 2019 with GPT2 (Radford et al., 2018). It incorporates some slight improvements compared to the previous concerning the position of layer normalisation and residual relations. Overall, there are four distinct GPT2 variants with the smallest being identical to GPT, the medium one being similar in size to BERT-LARGE and the xlarge
3. [3]:  Passage ID 3: 2023b; Zhang et al., 2023).Indeed, it is widely acknowledged that scaling up is an essential approach for achieving state-of-the-art performance on NLP tasks, especially those skills emerged with the scaling law (Wei et al., 2022; Bowman, 2023).However, developing LLMs requires substantial energy and financial resources for training and inference, which raises concerns about the AI carbon footprint and the economic burden on NLP product development (Strubell et al., 2019). In light of these concerns, prior research has underscored the critical need for effectively reducting CO2 equivalent emissions (CO2e) and Megawatt hours (MWh), and increase of Power Usage Effectiveness (Patterson et al., 2022; Thompson et al., 2020).Gaps.There is significant scope for improving the efficiency of NLP across various dimensions, including data curation, model design, and training paradigms, presenting numerous research opportunities. Addressing data efficiency involves tackling challenges like
4. [4]:  Passage ID 4: especially with a small labeled dataset. The complete dataset included more than 3000 examples, but FinBERT was able to surpass the previous state-of-the art even with a training set as small as 500 examples. This is an important result, since deep learning techniques for NLP have been traditionally labeled as too ”data-hungry”, which is apparently no longer the case.We conducted extensive experiments with BERT, investigating the effects of further pre-training and several training strategies. We couldn’t conclude that further pre-training on a domain-specific corpus was significantly better than not doing so for our case. Our theory is that BERT already performs good enough with our dataset that there is not much room for improvement that further pre-training can provide. We also found that learning rate regimes that fine-tune the higher layers more aggressively than the lower ones perform better and are more effective in preventing catastrophic forgetting. Another conclusion from
5. [5]:  Passage ID 5: A. Deoras, D. Povey, L. Burget, and J. Černockỳ,“Strategies for training large scale neural network language models,” inIEEE Workshop on Automatic Speech Recognition and Understanding, 2011.[32]J. Schmidhuber, “Learning complex, extended sequences using the principle ofhistory compression,” Neural Computation, vol. 4, no. 2, pp.234–242, 1992.[33]S. El Hihi and Y. Bengio, “Hierarchical recurrent neural networks forlong-term dependencies,” in NIPS, 1996, pp. 493–499.[34]S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neuralcomputation, vol. 9, no. 8, pp. 1735–1780, 1997.[35]K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink, andJ. Schmidhuber, “Lstm: A search space odyssey,” IEEE Transactions onNeural Networks and Learning Systems, vol. 28, no. 10, 2017.[36]K. Cho, B. Van Merriënboer, D. Bahdanau, and Y. Bengio, “On the propertiesof neural machine translation: Encoder-decoder approaches,” arXivpreprint arXiv:1409.1259,