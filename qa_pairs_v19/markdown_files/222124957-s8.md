# Which *BERT? A Survey Organizing Contextualized Encoders

## Question

What strategies are recent research efforts employing to reduce the wall-clock training time for BERT models?

## URLs

1. https://ar5iv.org/html/2404.11015. [2404.11015] FedFa: A Fully Asynchronous Training Paradigm for Federated Learning
2. https://ar5iv.org/html/2407.15819. [2407.15819] Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight
3. https://ar5iv.org/html/2410.18779. [2410.18779] A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs
4. https://ar5iv.org/html/2406.00061. [2406.00061] STAT: Shrinking Transformers After Training
5. https://ar5iv.org/html/2306.09305. [2306.09305] Fast Training of Diffusion Models with Masked Transformers
6. https://ar5iv.org/html/2310.14423. [2310.14423] A Quadratic Synchronization Rule for Distributed Deep Learning
7. https://ar5iv.org/html/2406.06962. [2406.06962] Evolving Subnetwork Training for Large Language Models
8. https://ar5iv.org/html/2201.12423. [2201.12423] Benchmarking Resource Usage for Efficient Distributed Deep Learning

## Answer (I don't know?: ✗)

Recent research efforts to reduce the wall-clock training time for BERT models are focusing on various strategies aimed at improving training efficiency while maintaining performance. Some of these strategies include employing techniques such as factorized embedding parameterization and cross-layer parameter sharing, which help reduce the model size and enhance training efficiency [5]. These approaches are particularly important given the computational challenges faced with traditional BERT implementations.

Additionally, researchers are investigating the fine-tuning processes of BERT models. It has been noted that refining the higher layers of the model more aggressively than the lower layers during training yields better results and is more effective in preventing catastrophic forgetting. This implies a prioritization of computational resources towards the upper layers of the model, which are likely to influence outputs more significantly [3].

Moreover, the research surrounding pre-training and further training on domain-specific corpora is revealing interesting insights. One particular study found that further pre-training on a specialized dataset was not significantly better than regular training methods in their specific context, suggesting that BERT already performs adequately with the given datasets [3]. This indicates a potential for optimizing training time by minimizing unnecessary additional training stages.

Finally, leveraging the distinct embedding creation capabilities of BERT to handle polysemy—as well as using the masking language modeling (MLM) objective—also plays a role in enhancing efficiency by capturing meanings of words from surrounding contexts without necessitating overly complex computations [5]. By utilizing these strategies effectively, recent research aims to streamline the training process for BERT models, thereby reducing wall-clock time while sustaining or improving model performance.

1. [1]:  https://ar5iv.org/html/2305.12544, No Title
2. [2]:  https://ar5iv.org/html/2410.00427, No Title
3. [3]:  https://ar5iv.org/html/1908.10063, No Title
4. [4]:  https://ar5iv.org/html/1908.10063, No Title
5. [5]:  https://ar5iv.org/html/2411.05036, No Title
---
1. [1]:  Passage ID 1: (Kurdi et al., 2020)) and tools for education researchers (e.g., systems to build representations of classroom interactions (Alic et al., 2022)). Researchers have been testing the application of models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) in these areas since their release, and are now beginning to incorporate larger models.Gaps.Many of the deployed NLP applications in the education space have been developed prior to wide spread use of LLMs, and we are likely to see large-scale deployment of task-specific models based on LLMs soon. While much of the prior work includes standalone applications, developing models that can easily be incorporated into existing education pipelines, e.g., by integrating what students have learned thus far, is an area open for further exploration. Importantly, a long-standing goal in education is to personalize materials and assessments to the needs of individual students, and NLP has the potential to contribute towards that
2. [2]:  Passage ID 2: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman’s terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman’s language. For example, the question “How are computers able to respond when we ask them questions?” was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as “Who discovered the
3. [3]:  Passage ID 3: especially with a small labeled dataset. The complete dataset included more than 3000 examples, but FinBERT was able to surpass the previous state-of-the art even with a training set as small as 500 examples. This is an important result, since deep learning techniques for NLP have been traditionally labeled as too ”data-hungry”, which is apparently no longer the case.We conducted extensive experiments with BERT, investigating the effects of further pre-training and several training strategies. We couldn’t conclude that further pre-training on a domain-specific corpus was significantly better than not doing so for our case. Our theory is that BERT already performs good enough with our dataset that there is not much room for improvement that further pre-training can provide. We also found that learning rate regimes that fine-tune the higher layers more aggressively than the lower ones perform better and are more effective in preventing catastrophic forgetting. Another conclusion from
4. [4]:  Passage ID 4: language used in financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain-specific corpora. We introduce FinBERT, a language model based on BERT, to tackle NLP tasks in financial domain. Our results show improvement in every measured metric on current state-of-the-art results for two financial sentiment analysis datasets. We find that even with a smaller training set and fine-tuning only a part of the model, FinBERT outperforms state-of-the-art machine learning methods.1. IntroductionPrices in an open market reflects all of the available information regarding assets exchanged in an economy (Malkiel, 2003). When new information becomes available, all actors in the economy update their positions and prices adjust accordingly, which makes beating the markets consistently impossible. However, the definition of ”new information” might change as new information
5. [5]:  Passage ID 5: techniques, such as factorized embedding parameterization and cross-layer parameter sharing, to reduce the model size and improve training efficiency while maintaining performance. These variants address some of the computational challenges associated with BERT and offer improved performance on various downstream tasks. BERT and its variants handle polysemy by generating distinct embeddings for a word based on its context. The MLM objective helps capture the meaning of rare words by leveraging the surrounding context. However, long sequences can still pose challenges due to the quadratic complexity of the self-attention mechanism in the Transformer architecture. The tying of word vectors and word classifiers as a loss framework for language modeling [46] has also been explored as a way to improve language modeling performance.II-C3 Other Contextualized Embeddings: GPT, XLNet, and XLMGPT [20] utilizes a Transformer decoder architecture and is trained using a language modeling