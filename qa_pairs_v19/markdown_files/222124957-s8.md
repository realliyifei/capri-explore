# Which *BERT? A Survey Organizing Contextualized Encoders

## Question

What strategies are recent research efforts employing to reduce the wall-clock training time for BERT models?

## URLs

1. https://ar5iv.org/html/2404.11015. [2404.11015] FedFa: A Fully Asynchronous Training Paradigm for Federated Learning
2. https://ar5iv.org/html/2407.15819. [2407.15819] Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight
3. https://ar5iv.org/html/2410.18779. [2410.18779] A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs
4. https://ar5iv.org/html/2406.00061. [2406.00061] STAT: Shrinking Transformers After Training
5. https://ar5iv.org/html/2306.09305. [2306.09305] Fast Training of Diffusion Models with Masked Transformers
6. https://ar5iv.org/html/2310.14423. [2310.14423] A Quadratic Synchronization Rule for Distributed Deep Learning
7. https://ar5iv.org/html/2406.06962. [2406.06962] Evolving Subnetwork Training for Large Language Models
8. https://ar5iv.org/html/2201.12423. [2201.12423] Benchmarking Resource Usage for Efficient Distributed Deep Learning

## Answer (I don't know?: ✗)

Recent research efforts have identified several strategies to reduce the wall-clock training time for BERT models and other large language models (LLMs). These approaches primarily address the computational bottlenecks associated with both the architecture of the models and the data involved in the training process.

1. **Reduction of Visual Tokens**: One straightforward method to speed up training is by reducing the number of visual tokens in image-text pairs during pre-training. This reduction allows for an increase in batch size and a concurrent decrease in step time, thus leading to improved training efficiency. Specifically, models often handle varying numbers of visual tokens, with traditional models managing around 144 to 256 tokens or more [2]. By minimizing the number of these tokens, researchers can optimize the training duration without sacrificing performance.

2. **Optimizing Attention Mechanisms**: Various architectures have been proposed to address the memory access issues of the attention module, which is frequently identified as a critical bottleneck in Transformer-based models. For instance, FlashAttention optimizes the attention computation process, effectively reducing the overall training cost [4]. Other methods like Reformer and Performer offer approximations for attention computation that lessen the computational burden, thus contributing to shorter training times [4]. 

3. **Sparse Training Techniques**: Incorporating sparse training methods such as Mixture of Experts (MoE) allows models to maintain high capacity while reducing training costs. MoE applies conditional computation based on the inputs, ensuring that only relevant parts of the model are trained at any given time [4]. This can significantly decrease the wall-clock time for training as it circumvents the need to compute over the entire model for every training iteration.

4. **Algorithmic Efficiency**: As highlighted in recent literature, an emphasis on optimizing the algorithms used for training large language models is crucial. This includes addressing the rampant over-parameterization of models, as demonstrated by the redundancy of parameters in models like GPT-3 [5]. By focusing on training subnetworks rather than the full model initially, researchers can enhance training speeds while efficiently managing resources [5].

5. **Energy Usage and Carbon Footprint Considerations**: The practical aspects of training, such as energy consumption and carbon emissions, also drive research into more efficient training strategies. The extensive amount of GPU time required for training large models—exemplified by the 60,000 GPU hours needed for a 7 billion parameter model—underscores the need for efficient methodologies. Innovations that minimize resource usage while maintaining model performance are paramount [2] [5].

In summary, recent strategies to reduce wall-clock training time for BERT models include reducing visual token counts, optimizing attention mechanisms, leveraging sparse training techniques, improving algorithmic efficiency, and considering the overall energy impact of training large models. These diverse approaches highlight the evolving nature of NLP research as it seeks to balance performance with efficiency.

1. [1]:  https://ar5iv.org/html/2406.00061, [2406.00061] STAT: Shrinking Transformers After Training
2. [2]:  https://ar5iv.org/html/2407.15819, [2407.15819] Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight
3. [3]:  https://ar5iv.org/html/2201.12423, [2201.12423] Benchmarking Resource Usage for Efficient Distributed Deep Learning
4. [4]:  https://ar5iv.org/html/2406.06962, [2406.06962] Evolving Subnetwork Training for Large Language Models
5. [5]:  https://ar5iv.org/html/2406.06962, [2406.06962] Evolving Subnetwork Training for Large Language Models
---
1. [1]:  Passage ID 1: Izsak, and Moshe Wasserblat.Q8bert: Quantized 8bit BERT.In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS). IEEE, dec 2019.doi: 10.1109/emc2-nips53020.2019.00016.URL https://doi.org/10.1109%2Femc2-nips53020.2019.00016.Zhang et al. [2022]Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.Opt: Open pre-trained transformer language models, 2022.URL https://arxiv.org/abs/2205.01068.Appendix A Appendix / supplemental materialA.1 Wall Clock Time Analysis on Consumer HardwareWe measure the wall clock time to evaluate the performance of a BERT network on the QQP test dataset at various levels of compression. All networks are run on the MPS cores of a 2023 MacBook M2 Pro chip
2. [2]:  Passage ID 2: so does the wall-clock training time, which has become a major obstackle in further explorations.According to [62], 60,000 GPU hours are needed for training a 7B model on just 96 million image-text pairs.This intensive computational demand is not only prohibitive to many researchers, but also leads to a significant carbon footprint.One of the key reasons for the prolonged training time is the extensive length of visual tokensTypically, the image-text pairs in the pre-training phase involve around 23 text tokens (see Table 1).In contrast, most MLLMs handle substantially more visual tokens during pre-training, e.g., 144 [9, 10], 256 [4, 49, 96], or even higher [62, 15, 54, 55, 63, 47].Reducing the number of visual tokens presents a straightforward way to speed up training, as it allows for an increase in batch size and a concurrent decrease in step time.Meanwhile, the reduced memory consumption allows for better optimization stages [80], further reducing time
3. [3]:  Passage ID 3: sufficient utilization data for analysis on those settings. Both DimeNet and SchNet were trained using a learning rate of 10−3superscript10310^{-3} with Adam  [21].2.2 Natural Language Processing (NLP)Language modeling is a set of approaches for obtaining distributions over sequences of words and is an important first step towards many common NLP tasks. Models are typically “pre-trained" using self-supervised learning methods for predicting word sequences before applying them to supervised tasks such as entailment or question answering.While much attention is given to the most accurate language models, they can require considerably long training times and significant energy usage  [39, 5].For our language model representative, we trained Google’s Bidirectional Encoder Representations from Transformers (BERT) language model [10] with masked language modeling, which involves predicting randomly chosen word tokens that are obscured from a large training set of text. We use a
4. [4]:  Passage ID 4: training large language models, ranging from addressing low-level hardware computations and memory bottlenecks to designing high-level training strategies. There are numerous approaches to overcome the computation bottleneck of Transformer-based models. FlashAttention (Dao et al., 2022b) identifies that the attention module is bottlenecked by memory access, and optimizes the process of attention computation, effectively reducing the training cost. Reformer (Kitaev et al., 2020) approximates attention computation based on locality-sensitive hashing and Performer (Choromanski et al., 2021) simplifies attention computation with low-rank approximation. Sparse training methods also benefit optimization efficiency. The main component of sparse training methods is the Mixture of Experts (MoE). MoE methods (Fedus et al., 2022; Du et al., 2022) apply conditional computation according to different inputs in order to scale up models without significantly increasing training costs. The drawback
5. [5]:  Passage ID 5: has become a bottleneck, hindering further development in research and applications. Additionally, the escalating hardware demands and increasing carbon footprints associated with training large language models are also crucial issues (Schwartz et al., 2020). This highlights the importance of researching efficient algorithms for training large language models.The enormous training cost of large language models stems from their massive number of parameters. For instance, the GPT3 (Brown et al., 2020) model has 175 billion parameters, requiring 355 GPU-years and incurring a training cost of $4.6M. However, numerous studies have highlighted the redundancy in the parameters of large language models, manifested in the over-parameterization (Li et al., 2020) and conditional sparsity (Li et al., 2023b) of these models. This inspires us to optimize the training process by exploring the possibility of not training the complete model at certain stages but focusing on training subnetworks,