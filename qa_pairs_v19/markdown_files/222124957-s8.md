# Which *BERT? A Survey Organizing Contextualized Encoders

## Question

What strategies are recent research efforts employing to reduce the wall-clock training time for BERT models?

## URLs

1. https://ar5iv.org/html/2404.11015. [2404.11015] FedFa: A Fully Asynchronous Training Paradigm for Federated Learning
2. https://ar5iv.org/html/2407.15819. [2407.15819] Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight
3. https://ar5iv.org/html/2410.18779. [2410.18779] A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs
4. https://ar5iv.org/html/2406.00061. [2406.00061] STAT: Shrinking Transformers After Training
5. https://ar5iv.org/html/2306.09305. [2306.09305] Fast Training of Diffusion Models with Masked Transformers
6. https://ar5iv.org/html/2310.14423. [2310.14423] A Quadratic Synchronization Rule for Distributed Deep Learning
7. https://ar5iv.org/html/2406.06962. [2406.06962] Evolving Subnetwork Training for Large Language Models
8. https://ar5iv.org/html/2201.12423. [2201.12423] Benchmarking Resource Usage for Efficient Distributed Deep Learning

## Answer (I don't know?: âœ—)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable