# Survey of Low-Resource Machine Translation

## Question

What are the methods and benefits of implementing domain tagging in low-resource machine translation tasks?

## URLs

1. https://ar5iv.org/html/2409.15924. [2409.15924] Multilingual Transfer and Domain Adaptation for Low-Resource Languages of Spain
2. https://ar5iv.org/html/2210.11628. [2210.11628] Can Domains Be Transferred Across Languages in Multi-Domain Multilingual Neural Machine Translation?
3. https://ar5iv.org/html/2104.06951. [2104.06951] Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey
4. https://ar5iv.org/html/2402.05140. [2402.05140] Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains
5. https://ar5iv.org/html/2401.14559. [2401.14559] Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–
6. https://ar5iv.org/html/2206.01137. [2206.01137] Finding the Right Recipe for Low Resource Domain Adaptation in Neural Machine Translation
7. https://ar5iv.org/html/2210.11912. [2210.11912] 𝒎^𝟒⁢𝑨⁢𝒅⁢𝒂⁢𝒑⁢𝒕⁢𝒆⁢𝒓: Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter
8. https://ar5iv.org/html/2409.15879. [2409.15879] Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning
9. https://ar5iv.org/html/1906.07978. [1906.07978] Multilingual Multi-Domain Adaptation Approaches for Neural Machine Translation

## Answer (I don't know?: ✗)

Implementing domain tagging in low-resource machine translation (MT) tasks can enhance translation quality and relevance, particularly in the absence of extensive linguistic resources. Domain tagging involves assigning a specific domain or context to the translation task, which enables more tailored outputs that align with the needs of the target audience.

### Methods of Implementing Domain Tagging

1. **Domain Adaptation Techniques**: One effective method is domain adaptation, which involves modifying a generic MT model to perform better in specific areas. For example, Haque et al. (2020) applied domain adaptation to an English-to-Hindi MT system by using terminology to select relevant sentences and employing mixed fine-tuning to improve quality in both generic and targeted domains [1]. This approach allows the model to understand the nuances of different domains, thus improving translation fidelity.

2. **Data Augmentation**: Data augmentation plays a critical role, especially in the context of low-resource languages. Öktem et al. (2022) used diverse data augmentation strategies to build NMT systems for very low-resource languages, demonstrating how increased data variety can enhance model performance [1]. This method can involve back-translation and syntactic adjustments tailored to the specific domain, ensuring the translated output is contextually appropriate.

3. **Fine-tuning on Domain-Specific Corpora**: Fine-tuning pre-existing models on specialized domain datasets is another method supported by research indicating that tailored training data results in better performance. For instance, studies have shown that models like BERT perform better when trained on curated, domain-specific data rather than generic datasets [3]. By aligning training datasets with domain tasks (e.g., legal, technical), models can become adept at recognizing and generating domain-relevant vocabulary and syntax.

4. **Incorporating External Knowledge Sources**: Leveraging external knowledge sources, such as glossaries or domain-specific ontologies, can enhance domain tagging in low-resource MT. By integrating these resources into the training process, MT models can access specialized terminology and context that might not be present in the limited datasets typically available for low-resource languages.

### Benefits of Domain Tagging

1. **Improved Translation Quality**: Accurate domain tagging allows models to produce translations that are contextually and semantically relevant, thereby decreasing the likelihood of errors caused by misunderstood domain-specific language. Domain-specific tweaks enhance the linguistic accuracy of translations, which can be critical in fields like legal translation, healthcare, or technical communications [5].

2. **Enhanced User Experience**: Translating content that aligns with user expectations leads to higher satisfaction rates. When translations reflect the specialized language and idioms of a domain, users are more likely to find the translations usable and trustworthy [4].

3. **Resource Efficiency**: In low-resource settings, where data is limited, domain tagging enables more efficient use of available resources. By focusing on relevant areas, researchers can produce better outcomes without needing extensive parallel corpora. This allows for advances in domains that typically suffer from a lack of linguistic resources [2][5].

4. **Facilitating Domain Expansion**: Successful domain tagging practices can serve as models for expanding into other domains, thus providing frameworks that other researchers can adopt. This can lead to broader advancements in MT for various low-resource languages, ultimately fostering improvements across the board [4][5].

In summary, domain tagging in low-resource machine translation not only enhances the relevance and accuracy of translations but also optimizes the use of scarce resources, thereby paving the way for more effective language processing in diverse domains.

1. [1]:  https://ar5iv.org/html/2401.14559, [2401.14559] Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–
2. [2]:  https://ar5iv.org/html/2304.07869, No Title
3. [3]:  https://ar5iv.org/html/2411.05503, No Title
4. [4]:  https://ar5iv.org/html/2104.06951, [2104.06951] Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey
5. [5]:  https://ar5iv.org/html/2411.11295, No Title
---
1. [1]:  Passage ID 1: for multilingual and low-resource settingsI aim to investigate the aforementioned approaches in multilingual and/or low-resource settings. During my PhD, I worked with more than 20 language pairs. In respect of my research questions, I already experimented with some low-resource languages, such as Czech and Kinyarwanda. Moreover, I co-authored a couple of works on MT for low-resource languages. For example, in our work (Haque et al., 2020a, ), we applied domain adaptation to an English-to-Hindi MT system. To this end, we used terminology to select relevant target sentences for the AI domain, generated the sources with back-translation, and finally applied mixed fine-tuning of the generic model, which improved the translation quality for the AI domain while retaining the translation quality of generic texts. Similarly, in our work (Öktem et al.,, 2022), we built NMT systems for a very low-resource language, Ladino, utilising diverse data augmentation approaches, employing rule-based
2. [2]:  Passage ID 2: task due to the inherent complex nature and the fluidity that natural languages bring. Nonetheless, in recent years, it has achieved state-of-the-art performance in several language pairs. Although, a lot of traction can be seen in the areas of multilingual neural machine translation (MNMT) in the recent years, there are no comprehensive survey done to identify what approaches work well. The goal of this paper is to investigate the realm of low resource languages and build a Neural Machine Translation model to achieve state-of-the-art results. The paper looks to build upon the mBART.CC25 [1] language model and explore strategies to augment it with various NLP and Deep Learning techniques like back translation and transfer learning. This implementation tries to unpack the architecture of the NMT application and determine the different components which offers us opportunities to amend the said application within the purview of the low resource languages problem space.1
3. [3]:  Passage ID 3: general NLP models in domain-specific tasks like named entity recognition, relation extraction, and others [56, 76]. This demonstrated the importance of specialized data and triggered a trend in fine-tuning models on annotated, domain-specific datasets.A more recent study [77] showed that training BERT [16] on the British National Corpus [12] (i.e., a carefully curated yet much smaller text collection than that used to train the original model) achieved even better performance than the original BERT model.Thus, even in the age of powerful neural models, the quality and specificity of data remain critical factors in achieving high-performance NLP systems. Moreover, without datasets for training and validation, the field of Kyrgyz NLP simply cannot advance.2.2 Processing Methods for Less-Resourced LanguagesAddressing the challenges faced by LRLs requires innovative approaches that compensate for the lack of resources. Several common methods have been employed to process LRLs
4. [4]:  Passage ID 4: but need to infer domain at inference time. However, they are simpler to consider in isolation, and solutions addressing each individually can also often by combined.In section 9 we provide case studies on separate lines of machine translation research: low-resource language translation, gender handling in translation, and document-level translation. These are well-established NMT research topics in their own right. Elements of at least the latter two may be viewed as relevant to genre or topic in terms of gender- and document-consistent grammar and lexical choice. However, they are not always treated as relevant to domain adaptation. We will demonstrate that these lines of translation research can also be treated as domain adaptation problems.3 A Brief Overview of Neural Machine TranslationMachine Translation (MT) aims to translate written text from a source natural language to a target language. Originally accomplished with statistical MT (SMT) using phrase-based frequency
5. [5]:  Passage ID 5: understanding and reasoning abilities pave the way toward Artificial General Intelligence (AGI) and can facilitate societal development across a wide range of domains [83, 44, 84, 82, 35, 32].2.2 Machine Translation on Low-Resource LanguageMachine Translation for low-resource languages has been a long-standing challenge in the field of NLP. While machine translation for high-resource languages, such as English, Chinese, or Spanish, has seen considerable improvements, particularly with the advent of NMT techniques, low-resource languages have lagged due to the scarcity of large parallel corpora and linguistic resources.Early efforts in machine translation, particularly for low-resource languages, were based on rule-based and SMT approaches. Rule-based systems relied on linguistic rules crafted by experts and extensive lexicons, but they were often labor-intensive and brittle when applied to complex languages. SMT, introduced in the 1990s, offered a data-driven approach, where