# Survey of Low-Resource Machine Translation

## Question

What are the methods and benefits of implementing domain tagging in low-resource machine translation tasks?

## URLs

1. https://ar5iv.org/html/2409.15924. [2409.15924] Multilingual Transfer and Domain Adaptation for Low-Resource Languages of Spain
2. https://ar5iv.org/html/2210.11628. [2210.11628] Can Domains Be Transferred Across Languages in Multi-Domain Multilingual Neural Machine Translation?
3. https://ar5iv.org/html/2104.06951. [2104.06951] Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey
4. https://ar5iv.org/html/2402.05140. [2402.05140] Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains
5. https://ar5iv.org/html/2401.14559. [2401.14559] Language Modelling Approaches to Adaptive Machine Translation â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ PhD Thesis â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“
6. https://ar5iv.org/html/2206.01137. [2206.01137] Finding the Right Recipe for Low Resource Domain Adaptation in Neural Machine Translation
7. https://ar5iv.org/html/2210.11912. [2210.11912] ğ’^ğŸ’â¢ğ‘¨â¢ğ’…â¢ğ’‚â¢ğ’‘â¢ğ’•â¢ğ’†â¢ğ’“: Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter
8. https://ar5iv.org/html/2409.15879. [2409.15879] Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning
9. https://ar5iv.org/html/1906.07978. [1906.07978] Multilingual Multi-Domain Adaptation Approaches for Neural Machine Translation

## Answer (I don't know?: âœ—)

Implementing domain tagging in low-resource machine translation (MT) tasks involves the use of specific labels or tags that denote the domain of the text being translated. This method can enhance the translation quality, especially in contexts where resources are scarce. Here are some methods and benefits associated with domain tagging in such settings:

### Methods of Implementing Domain Tagging

1. **Terminology Selection**: Domain tagging can facilitate the selection of relevant terminology for specific domains. For instance, in research referenced, terminology is used to curate target sentences that align closely with the AI domain when adapting an English-to-Hindi MT system [2]. This ensures that the translated content reflects domain-specific language usage, which is critical in specialized fields.

2. **Mixed Fine-tuning with Domain Tags**: Fine-tuning MT models by combining in-domain data with domain tags during training allows for improved adaptability. By over-sampling in-domain data alongside generic content tagged by domain labels, the model can learn more effectively how to handle specialized terminology within a broader context [4].

3. **Tag-assisted Data Augmentation**: Data augmentation strategies can include using domain tags in conjunction with back-translation to generate additional training sentences. Applying domain information in this way helps create a more robust training dataset, enabling the model to generalize better across domains [2].

### Benefits of Domain Tagging

1. **Improved Translation Quality**: One of the primary advantages of using domain tagging is the enhancement of translation quality, particularly in low-resource contexts where the available training data may not cover the necessary specialized vocabulary [3]. Domain tags guide the MT system to prioritize contextually appropriate terms and structures.

2. **Knowledge Sharing Across Domains**: Domain tagging can facilitate knowledge sharing across related domains, allowing an MT model trained on one domain to assist in translations from another, even when direct training data is limited. This is exemplified in the work exploring unified models that draw from multiple languages and domains [5]. The shared knowledge can fill gaps for low-resource languages and improve overall translation performance.

3. **Handling of Diverse Languages**: Applying domain tagging allows researchers to examine translation quality across various languages, including high-resource, medium-resource, and low-resource languages. This adaptability is crucial for ensuring that less common languages benefit from state-of-the-art practices developed for more widely spoken languages [4]. By adopting tailored approaches for each domain, MT systems can learn to perform better even with minimal in-domain training data.

4. **Enhanced Ability for Adaptive MT**: In scenarios where continuous interaction and feedback are involved, domain tagging significantly boosts an MT systemâ€™s capability to adapt its outputs dynamically based on user inputs or changing contexts [1]. This leads to personalized translations that are more relevant to the user's needs.

Implementing domain tagging in low-resource machine translation, therefore, not only aids in enhancing specific translation tasks but also contributes to the broader goal of creating more flexible and effective multilingual MT systems.

1. [1]:  https://ar5iv.org/html/2401.14559, [2401.14559] Language Modelling Approaches to Adaptive Machine Translation â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ PhD Thesis â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“
2. [2]:  https://ar5iv.org/html/2401.14559, [2401.14559] Language Modelling Approaches to Adaptive Machine Translation â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ PhD Thesis â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“
3. [3]:  https://ar5iv.org/html/2401.14559, [2401.14559] Language Modelling Approaches to Adaptive Machine Translation â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ PhD Thesis â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“
4. [4]:  https://ar5iv.org/html/2401.14559, [2401.14559] Language Modelling Approaches to Adaptive Machine Translation â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ PhD Thesis â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“
5. [5]:  https://ar5iv.org/html/2210.11628, [2210.11628] Can Domains Be Transferred Across Languages in Multi-Domain Multilingual Neural Machine Translation?
---
1. [1]:  Passage ID 1: I examined how to leverage language modelling techniques in general and LLMs in particular to improve translation features that involve human interaction and continuous feedback, such as adaptive MT, terminology-constrained MT, domain-aware automatic post-editing, auto-suggestion and auto-completion. In the second research question, I addressed a common scenario in the translation industry, namely receiving highly specialised projects, where there is hardly any parallel in-domain data. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. One way to address this question is through domain-specific text generation with LLMs and then fine-tuning an MT model using the generated bilingual in-domain synthetic data. Furthermore, this question overlaps with the first question, since real-time adaptive MT with LLMs can serve as an alternative approach to address in-domain data
2. [2]:  Passage ID 2: for multilingual and low-resource settingsI aim to investigate the aforementioned approaches in multilingual and/or low-resource settings. During my PhD, I worked with more than 20 language pairs. In respect of my research questions, I already experimented with some low-resource languages, such as Czech and Kinyarwanda. Moreover, I co-authored a couple of works on MT for low-resource languages. For example, in our work (Haque etÂ al., 2020a, ), we applied domain adaptation to an English-to-Hindi MT system. To this end, we used terminology to select relevant target sentences for the AI domain, generated the sources with back-translation, and finally applied mixed fine-tuning of the generic model, which improved the translation quality for the AI domain while retaining the translation quality of generic texts. Similarly, in our work (Ã–ktem etÂ al.,, 2022), we built NMT systems for a very low-resource language, Ladino, utilising diverse data augmentation approaches, employing rule-based
3. [3]:  Passage ID 3: common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations (Axelrod etÂ al.,, 2011; Haddow and Koehn,, 2012).Recent advances in language modelling techniques in general and large-scale language models (LLMs) in particular have shown significant potential in improving a wide range of NLP tasks. Inspired by this idea, this research aims to answer two major Research Questions (RQ):RQ1In scenarios involving human interaction and continuous feedback, can we employ language models to improve the quality of adaptive MT at inference time? In the subsequent sections, I will be referring to this question as â€œAdaptive and Interactive MTâ€.RQ2In the absence of sufficient in-domain data, can we use pre-trained LLMs to improve the process of NMT domain adaptation? In the following sections, I will be referring to this question as â€œDomain-specific Text Generation for MTâ€.Figure 1.1:
4. [4]:  Passage ID 4: I am interested in conducting an in-depth investigation into fine-tuning LLMs to enhance their domain-specific in-context learning capability, and improve their ability to work better in adaptive MT scenarios. To this end, I would like to conduct a range of experiments, including: instruction fine-tuning on the in-domain data only; instruction mixed fine-tuning on the in-domain data and a randomly sampled portion of the original generic data, over-sampling the in-domain data; and investigating diverse scenarios of boosting in-context learning for adaptive MT, ranging from zero-shot and few-shot translation to integration of terminology and in-domain monolingual data.The aforementioned experiments should employ efficient fine-tuning approaches (Wan etÂ al.,, 2023) such as QLoRA (Hu etÂ al.,, 2021; Dettmers etÂ al.,, 2023). I would like to apply the experiments to a range of diverse languages, including high-resource, medium-resource, and low-resource languages, as well as different
5. [5]:  Passage ID 5: and in-domain bitext in other translation pairs?We explore the benefits of having a single MDML model trained on all available training data from multiple languages and domains over the multi-domain bilingual (MDBL) and the single domain multilingual (SDML) models learned on a subset of training data from a single language pair or domain. We carefully design controlled experiments to build incomplete data conditions and study the translation quality of the unified MDML-NMT model on both seen and unseen (zero-shot) translation directions.We hypothesise that the translation involving the out-of-domain languages can be beneficial from the in-domain languages thanks to the knowledge sharing across domain and languages.â€¢RQ2: What is effective method to handle composition of domains and languages?We investigate strategies for effective integration of existing multi-domain and multilingual NMT methods, including the use of language and domain tags and auxiliary task