# Survey of Low-Resource Machine Translation

## Question

What are the methods and benefits of implementing domain tagging in low-resource machine translation tasks?

## URLs

1. https://ar5iv.org/html/2409.15924. [2409.15924] Multilingual Transfer and Domain Adaptation for Low-Resource Languages of Spain
2. https://ar5iv.org/html/2210.11628. [2210.11628] Can Domains Be Transferred Across Languages in Multi-Domain Multilingual Neural Machine Translation?
3. https://ar5iv.org/html/2104.06951. [2104.06951] Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey
4. https://ar5iv.org/html/2402.05140. [2402.05140] Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains
5. https://ar5iv.org/html/2401.14559. [2401.14559] Language Modelling Approaches to Adaptive Machine Translation â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ PhD Thesis â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“
6. https://ar5iv.org/html/2206.01137. [2206.01137] Finding the Right Recipe for Low Resource Domain Adaptation in Neural Machine Translation
7. https://ar5iv.org/html/2210.11912. [2210.11912] ğ’^ğŸ’â¢ğ‘¨â¢ğ’…â¢ğ’‚â¢ğ’‘â¢ğ’•â¢ğ’†â¢ğ’“: Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter
8. https://ar5iv.org/html/2409.15879. [2409.15879] Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning
9. https://ar5iv.org/html/1906.07978. [1906.07978] Multilingual Multi-Domain Adaptation Approaches for Neural Machine Translation

## Answer (I don't know?: âœ—)

The implementation of domain tagging in low-resource machine translation (MT) tasks involves several methods and offers notable benefits, particularly in enhancing translation quality and adaptability across various domains and languages.

### Methods of Domain Tagging

1. **Domain-Specific Text Generation**: One effective method is to leverage large-scale language models (LLMs) for domain-specific text generation. This can involve generating bilingual synthetic data that reflects the specific terminology and context of the target domain, particularly when there is a lack of in-domain data [1] [3]. By doing so, the MT models can be fine-tuned on this synthetic data to better understand domain-specific nuances.

2. **Domain Adaptation Techniques**: Techniques such as back-translation and terminology usage to select relevant sentences are commonly employed. For instance, in experiments related to English-to-Hindi MT, terminology was used to enhance translation quality within the AI domain by selecting contextually appropriate sentences for training [2]. This approach ensures that the model learns the correct usage of specialized terms, improving its performance in low-resource settings.

3. **Instruction Fine-Tuning**: Instruction fine-tuning of LLMs specifically on in-domain data, as well as mixed fine-tuning with a combination of in-domain and generic data, is suggested to boost in-context learning abilities of the models [4]. This method allows the model to become proficient in the specificities of the domain while retaining versatility for general translations.

4. **Efficient Fine-Tuning Approaches**: Applying advanced fine-tuning techniques, such as QLoRA, promotes effective learning from limited data [4]. This ensures that even with sparse in-domain examples, the model can adaptively improve its translation outputs.

### Benefits of Domain Tagging

1. **Improved Translation Quality**: By implementing domain tagging, translators can achieve better quality outputs, as the models become adept at using domain-specific terminology correctly [1] [3]. This is particularly vital in low-resource scenarios where domain expertise is scarce.

2. **Knowledge Sharing Across Domains**: Research suggests that utilizing a unified multilingual domain model can enhance translation quality for both seen and unseen translation directions by facilitating knowledge sharing across different domains and languages [5]. This approach can be particularly advantageous in low-resource settings, where inter-domain information can help bridge gaps in translation accuracy.

3. **Tailored Adaptability**: Domain tagging allows MT systems to be more adaptable to specific user needs and industry contexts, making them suitable for various applications within specialized fields [2]. The ability to rapidly improve models through domain-specific data also fosters a more responsive translation solution, especially relevant in industries that deal with highly technical or specialized content.

4. **Increased Efficiency in Data Usage**: By effectively utilizing available in-domain data and optimizing the training process for low-resource languages, domain tagging enhances overall efficiency in resource-constrained environments [4]. This results in improved language models without the need for extensive datasets.

In conclusion, domain tagging in low-resource MT tasks employs methods like domain-specific text generation, back-translation, and advanced fine-tuning, offering benefits that include improved translation quality, adaptability, and efficient usage of limited resources. These approaches collectively contribute to more effective machine translation solutions in specialized contexts.

1. [1]:  https://ar5iv.org/html/2401.14559, [2401.14559] Language Modelling Approaches to Adaptive Machine Translation â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ PhD Thesis â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“
2. [2]:  https://ar5iv.org/html/2401.14559, [2401.14559] Language Modelling Approaches to Adaptive Machine Translation â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ PhD Thesis â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“
3. [3]:  https://ar5iv.org/html/2401.14559, [2401.14559] Language Modelling Approaches to Adaptive Machine Translation â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ PhD Thesis â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“
4. [4]:  https://ar5iv.org/html/2401.14559, [2401.14559] Language Modelling Approaches to Adaptive Machine Translation â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ PhD Thesis â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“
5. [5]:  https://ar5iv.org/html/2210.11628, [2210.11628] Can Domains Be Transferred Across Languages in Multi-Domain Multilingual Neural Machine Translation?
---
1. [1]:  Passage ID 1: I examined how to leverage language modelling techniques in general and LLMs in particular to improve translation features that involve human interaction and continuous feedback, such as adaptive MT, terminology-constrained MT, domain-aware automatic post-editing, auto-suggestion and auto-completion. In the second research question, I addressed a common scenario in the translation industry, namely receiving highly specialised projects, where there is hardly any parallel in-domain data. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. One way to address this question is through domain-specific text generation with LLMs and then fine-tuning an MT model using the generated bilingual in-domain synthetic data. Furthermore, this question overlaps with the first question, since real-time adaptive MT with LLMs can serve as an alternative approach to address in-domain data
2. [2]:  Passage ID 2: for multilingual and low-resource settingsI aim to investigate the aforementioned approaches in multilingual and/or low-resource settings. During my PhD, I worked with more than 20 language pairs. In respect of my research questions, I already experimented with some low-resource languages, such as Czech and Kinyarwanda. Moreover, I co-authored a couple of works on MT for low-resource languages. For example, in our work (Haque etÂ al., 2020a, ), we applied domain adaptation to an English-to-Hindi MT system. To this end, we used terminology to select relevant target sentences for the AI domain, generated the sources with back-translation, and finally applied mixed fine-tuning of the generic model, which improved the translation quality for the AI domain while retaining the translation quality of generic texts. Similarly, in our work (Ã–ktem etÂ al.,, 2022), we built NMT systems for a very low-resource language, Ladino, utilising diverse data augmentation approaches, employing rule-based
3. [3]:  Passage ID 3: common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations (Axelrod etÂ al.,, 2011; Haddow and Koehn,, 2012).Recent advances in language modelling techniques in general and large-scale language models (LLMs) in particular have shown significant potential in improving a wide range of NLP tasks. Inspired by this idea, this research aims to answer two major Research Questions (RQ):RQ1In scenarios involving human interaction and continuous feedback, can we employ language models to improve the quality of adaptive MT at inference time? In the subsequent sections, I will be referring to this question as â€œAdaptive and Interactive MTâ€.RQ2In the absence of sufficient in-domain data, can we use pre-trained LLMs to improve the process of NMT domain adaptation? In the following sections, I will be referring to this question as â€œDomain-specific Text Generation for MTâ€.Figure 1.1:
4. [4]:  Passage ID 4: I am interested in conducting an in-depth investigation into fine-tuning LLMs to enhance their domain-specific in-context learning capability, and improve their ability to work better in adaptive MT scenarios. To this end, I would like to conduct a range of experiments, including: instruction fine-tuning on the in-domain data only; instruction mixed fine-tuning on the in-domain data and a randomly sampled portion of the original generic data, over-sampling the in-domain data; and investigating diverse scenarios of boosting in-context learning for adaptive MT, ranging from zero-shot and few-shot translation to integration of terminology and in-domain monolingual data.The aforementioned experiments should employ efficient fine-tuning approaches (Wan etÂ al.,, 2023) such as QLoRA (Hu etÂ al.,, 2021; Dettmers etÂ al.,, 2023). I would like to apply the experiments to a range of diverse languages, including high-resource, medium-resource, and low-resource languages, as well as different
5. [5]:  Passage ID 5: and in-domain bitext in other translation pairs?We explore the benefits of having a single MDML model trained on all available training data from multiple languages and domains over the multi-domain bilingual (MDBL) and the single domain multilingual (SDML) models learned on a subset of training data from a single language pair or domain. We carefully design controlled experiments to build incomplete data conditions and study the translation quality of the unified MDML-NMT model on both seen and unseen (zero-shot) translation directions.We hypothesise that the translation involving the out-of-domain languages can be beneficial from the in-domain languages thanks to the knowledge sharing across domain and languages.â€¢RQ2: What is effective method to handle composition of domains and languages?We investigate strategies for effective integration of existing multi-domain and multilingual NMT methods, including the use of language and domain tags and auxiliary task