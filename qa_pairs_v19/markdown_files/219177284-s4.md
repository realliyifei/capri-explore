# Conversational Machine Comprehension: a Literature Review

## Question

How is the structure and evaluation of conversations and dialogs organized in the CoQA dataset?

## URLs

1. https://ar5iv.org/html/1808.07042
2. https://ar5iv.org/html/2401.10225
3. https://ar5iv.org/html/2102.02864
4. https://ar5iv.org/html/2205.11685
5. https://ar5iv.org/html/2408.11219
6. https://ar5iv.org/html/2005.01328
7. https://ar5iv.org/html/2312.16511
8. https://ar5iv.org/html/2209.12617
9. https://ar5iv.org/html/2405.13003
10. https://ar5iv.org/html/2405.01121

## Answer

## I don't know?: False

The CoQA (Conversational Question Answering) dataset is organized to facilitate the evaluation of conversational question answering systems through its carefully structured dialogue format and diverse evaluation metrics. Here’s a detailed overview of its structure and evaluation methodology based on the context provided.

### Structure of Conversations in CoQA

1. **Conversation Turns**: CoQA consists of 127,000 conversation turns that are derived from 8,000 complete conversations. Each conversation typically features an average length of 15 turns, where each turn is composed of a question and its corresponding answer [3][5]. This turn-based structure mimics natural conversational flow, allowing for dynamic interactions between the questioner and answerer.

2. **Free-Form Answers and Rationale**: Each answer in CoQA is free-form, meaning that answerers can express responses in their own words rather than being restricted to exact phrases from the text. Additionally, each answer includes a span-based rationale highlighted in the relevant passage, providing context and supporting evidence for the answer given [3][5]. This design helps capture the complexities of human-like responses and reasoning in conversations.

3. **Anaphoric References and Pragmatic Reasoning**: Almost half of the questions in the dataset involve referencing earlier parts of the conversation through anaphors, which necessitates a deeper understanding of context. Many questions also require pragmatic reasoning skills, moving beyond simple lexical associations to grasp the intended meaning and relevance of the conversation [3]. This complexity challenges conventional models that rely solely on surface features.

### Evaluation of Conversational Models

1. **Benchmarking**: The CoQA dataset is used to benchmark several deep neural network models that build upon state-of-the-art conversational and reading comprehension models. The current best-performing system on the CoQA dataset achieved an F1 score of 65.4% [3]. F1 scores are crucial as they provide a comprehensive measure of a model's accuracy while balancing precision and recall.

2. **State-of-the-Art Improvements**: Since the launch of CoQA in August 2018, there has been rapid progress in model development utilizing this dataset. For instance, Huang et al. (2019) proposed models that effectively manage conversational context without merely stacking questions and answers. Moreover, researchers like Zhu et al. (2018) have integrated advanced pre-trained models like BERT, leading to substantial gains in performance on various NLP tasks including CoQA [2]. As of January 2019, the state-of-the-art F1 score on the test set reached 82.8%, highlighting the competitive nature of model evaluation in this area [2].

3. **Domains and Out-of-Domain Challenges**: CoQA's evaluation spans a range of domains—five for in-domain evaluation and two for out-of-domain evaluation—with questions covering diverse topics, from children's stories to scientific narratives. This broad domain inclusion allows for robust testing of a model's generalizability and performance when encountering unfamiliar contexts [3][5].

In summary, the CoQA dataset is meticulously structured to reflect real conversational dynamics while presenting unique challenges that require sophisticated reasoning and contextual understanding. Its evaluation framework emphasizes the effectiveness of various models in accurately interpreting and responding to conversational questions using diverse approaches.

1. [1]:  https://ar5iv.org/html/1808.07042, [1808.07042] CoQA: A Conversational Question Answering Challenge
2. [2]:  https://ar5iv.org/html/1808.07042, [1808.07042] CoQA: A Conversational Question Answering Challenge
3. [3]:  https://ar5iv.org/html/1808.07042, [1808.07042] CoQA: A Conversational Question Answering Challenge
4. [4]:  https://ar5iv.org/html/2307.07255, [2307.07255] Dialogue Agents 101: A Beginner’s Guide to Critical Ingredients for Designing Effective Conversational Systems
5. [5]:  https://ar5iv.org/html/2005.01328, [2005.01328] DoQA - Accessing Domain-Specific FAQs via Conversational QA
---
1. [1]:  Passage ID 1: this paper, we introduced CoQA, a large scale dataset for building conversational question answering systems.Unlike existing reading comprehension datasets, CoQA contains conversational questions, free-form answers along with text spans as rationales, and text passages from seven diverse domains.We hope this work will stir more research in conversational modeling, a key ingredient for enabling natural human-machine communication.AcknowledgementsWe would like to thank MTurk workers, especially the Master Chatters and the MTC forum members, for contributing to the creation of CoQA, for giving feedback on various pilot interfaces, and for promoting our hits enthusiastically on various forums.CoQA has been made possible with financial support from the Facebook ParlAI and the Amazon Research awards, and gift funding from Toyota Research Institute. Danqi is supported by a Facebook PhD fellowship. We also would like to thank the members of the Stanford NLP group for critical
2. [2]:  Passage ID 2: on CoQASince we first released the dataset in August 2018, the progress of developing better models on CoQA has been rapid.Instead of simply prepending the current question with its previous questions and answers, Huang et al. (2019) proposed a more sophisticated solution to effectively stack single-turn models along the conversational flow.Others (e.g., Zhu et al., 2018) attempted to incorporate the most recent pretrained language representation model BERT  Devlin et al. (2018)121212Pretrained BERT models were released in November 2018, which have demonstrated large improvements across a wide variety of NLP tasks. into CoQA and demonstrated superior results.As of the time we finalized the paper (Jan 8, 2019), the state-of-art F1 score on the test set was 82.8.8 ConclusionsIn this paper, we introduced CoQA, a large scale dataset for building conversational question answering systems.Unlike existing reading comprehension datasets, CoQA contains conversational questions,
3. [3]:  Passage ID 3: Reddit and science.The last two are used for out-of-domain evaluation.To summarize, CoQA has the following key characteristics:•It consists of 127k conversation turns collected from 8k conversations over text passages. The average conversation length is 15 turns, and each turn consists of a question and an answer.•It contains free-form answers and each answer has a span-based rationale highlighted in the passage.•Its text passages are collected from seven diverse domains: five are used for in-domain evaluation and two are used for out-of-domain evaluation.Almost half of CoQA questions refer back to conversational history using anaphors, and a large portion require pragmatic reasoning making it challenging for models that rely on lexical cues alone.We benchmark several deep neural network models, building on top of state-of-the-art conversational and reading comprehension models (Section 5).The best-performing system achieves an F1 score of 65.4%. In
4. [4]:  Passage ID 4: dialogue dataset.Proceedings of the AAAI Conference on Artificial Intelligence,34(05):8689–8696.Reddy et al. (2019)Siva Reddy, Danqi Chen, and Christopher D. Manning. 2019.CoQA: Aconversational question answering challenge.Transactions of the Association for Computational Linguistics,7:249–266.Reddy et al. (2014)Siva Reddy, Mirella Lapata, and Mark Steedman. 2014.Large-scale semanticparsing without question-answer pairs.Transactions of the Association for Computational Linguistics,2:377–392.Roller et al. (2021)Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu,Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. 2021.Recipes forbuilding an open-domain chatbot.In Proceedings of the 16th Conference of the European Chapterof the Association for Computational Linguistics: Main Volume, pages300–325, Online. Association for Computational Linguistics.Ruusuvuori (2012)Johanna Ruusuvuori.
5. [5]:  Passage ID 5: et al. (2018) are two conversational QA datasetscomprising QA dialogues that fulfill the information need of auser by answering questions about different topics. Similarly to our, bothdatasets are built by crowdsourcing, where one person (the questioner) ispresented with a topic and has to pose free-form questions about it. Anotherperson (the answerer) has to select an answer to the question by choosing anexcerpt from the relevant passage describing the topic. Some of thequestions in both datasets are unanswerable, and access to previous questions and answers areneeded in order to answer some of the questions.CoQA contains 127k questions with answers, obtained from 8k conversations about passages from broad domains, ranging from children stories to science. The answers are also excerpts from the relevant passage, but answerers have the choice of reformulating them. The authors report that 78%percent7878\% of the answers had at least one edit. Although reformulating answers