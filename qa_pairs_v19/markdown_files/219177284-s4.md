# Conversational Machine Comprehension: a Literature Review

## Question

How is the structure and evaluation of conversations and dialogs organized in the CoQA dataset?

## URLs

1. https://ar5iv.org/html/1808.07042. [1808.07042] CoQA: A Conversational Question Answering Challenge
2. https://ar5iv.org/html/2401.10225. [2401.10225] ChatQA: Building GPT-4 Level Conversational QA Models
3. https://ar5iv.org/html/2102.02864. [2102.02864] ChainCQG: Flow-Aware Conversational Question Generation
4. https://ar5iv.org/html/2205.11685. [2205.11685] A Dataset for Sentence Retrieval for Open-Ended Dialogues
5. https://ar5iv.org/html/2408.11219. [2408.11219] CoDi: Conversational Distillation for Grounded Question Answering
6. https://ar5iv.org/html/2005.01328. [2005.01328] DoQA - Accessing Domain-Specific FAQs via Conversational QA
7. https://ar5iv.org/html/2312.16511. [2312.16511] S2M: Converting Single-Turn to Multi-Turn Datasets for Conversational Question Answering
8. https://ar5iv.org/html/2209.12617. [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
9. https://ar5iv.org/html/2405.13003. [2405.13003] A Survey on Recent Advances in Conversational Data Generation
10. https://ar5iv.org/html/2405.01121. [2405.01121] Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts

## Answer (I don't know?: ✓)

I don't know. The context provided does not contain specific information about the structure or evaluation of conversations and dialogs in the CoQA (Conversational Question Answering) dataset. References to other datasets and works related to question answering and dialogues are included, but there is no mention of the CoQA dataset itself, its structure, or its evaluation methodology. Thus, I cannot provide an answer based on the available context.

1. [1]:  https://ar5iv.org/html/2407.09413, No Title
2. [2]:  https://ar5iv.org/html/2310.18345, No Title
3. [3]:  https://ar5iv.org/html/2407.18418, No Title
4. [4]:  https://ar5iv.org/html/2405.13003, [2405.13003] A Survey on Recent Advances in Conversational Data Generation
5. [5]:  https://ar5iv.org/html/2405.13003, [2405.13003] A Survey on Recent Advances in Conversational Data Generation
---
1. [1]:  Passage ID 1: and relations and matching them with structure knowledge resources. The questions in such datasets follow a pre-defined format; hence, they are unsuitable for real-world usage where the reader asks detailed open-ended questions [31]. To overcome these issues, PubMedQA [20], BioAsq [29] and QASPER [14] construct corpora of 1K, 3.2K, and 5K human-written questions, respectively. However, the annotators in these datasets only read the abstracts when writing the questions, and hence, most questions are simple and can be answerable in yes/no format or with short extractive spans. ArgSciChat [63] proposes a dataset of argumentive dialogues between scientists on 20 NLP papers. Closer to our work, QASA [34] generates 1798 free-form advanced questions on AI/ML papers where the annotators read the whole paper. However, QASA questions are answerable just from the text paragraphs, neglecting the structured information present in terms of figures and tables.Datasets for QA on Scientific
2. [2]:  Passage ID 2: of the Association for Computational Linguistics, pp. 7871–7880.Li et al. (2008a)Li, B., Liu, Y., Agichtein, E., 2008a.CoCQA: Co-training over questions and answers with an application to predicting question subjectivity orientation, in: Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pp. 937–946.Li et al. (2008b)Li, B., Liu, Y., Ram, A., Garcia, E.V., Agichtein, E., 2008b.Exploring question subjectivity prediction in community qa, in: Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 735–736.Li et al. (2010)Li, C., Donizelli, M., Rodriguez, N., Dharuri, H., Endler, L., Chelliah, V., Li, L., He, E., Henry, A., Stefan, M.I., et al., 2010.BioModels database: An enhanced, curated and annotated resource for published quantitative kinetic models.BMC Systems Biology 4, 1–14.Li et al. (2019)Li, C., Xu, X., Zhou, G., He, K., Qi, T., Zhang,
3. [3]:  Passage ID 3: of the Association for Computational Linguistics, 9:962–977.Jin et al. (2019)Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019.PubMedQA: A dataset for biomedical research question answering.In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2567–2577, Hong Kong, China. Association for Computational Linguistics.Kadavath et al. (2022)Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022.Language models (mostly) know what they know.ArXiv preprint, abs/2207.05221.Kamath et al. (2020)Amita Kamath, Robin Jia, and Percy Liang. 2020.Selective question answering under domain shift.In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages
4. [4]:  Passage ID 4: Dialogues Dataset.CoRR abs/1912.04639(2019).arXiv:1912.04639http://arxiv.org/abs/1912.04639Qi et al. (2023)Peng Qi, Nina Du,Christopher Manning, and Jing Huang.2023.PragmatiCQA: A Dataset for Pragmatic QuestionAnswering in Conversations. In Findings of theAssociation for Computational Linguistics: ACL 2023,Anna Rogers, JordanBoyd-Graber, and Naoaki Okazaki (Eds.).Association for Computational Linguistics,Toronto, Canada, 6175–6191.https://doi.org/10.18653/v1/2023.findings-acl.385Qu et al. (2018a)C. Qu, L. Yang,W. B. Croft, J. Trippas,Y. Zhang, and M. Qiu.2018a.Analyzing and Characterizing User Intent inInformation-seeking Conversations.. In SIGIR’18.Qu et al. (2018b)Chen Qu, Liu Yang,W. Bruce Croft, Johanne R. Trippas,Yongfeng Zhang, and Minghui Qiu.2018b.Analyzing and Characterizing User Intent inInformation-seeking Conversations. In The 41stInternational ACM SIGIR Conference on Research & Development inInformation
5. [5]:  Passage ID 5: Dialogues Dataset.CoRR abs/1912.04639(2019).arXiv:1912.04639http://arxiv.org/abs/1912.04639Qi et al. (2023)Peng Qi, Nina Du,Christopher Manning, and Jing Huang.2023.PragmatiCQA: A Dataset for Pragmatic QuestionAnswering in Conversations. In Findings of theAssociation for Computational Linguistics: ACL 2023,Anna Rogers, JordanBoyd-Graber, and Naoaki Okazaki (Eds.).Association for Computational Linguistics,Toronto, Canada, 6175–6191.https://doi.org/10.18653/v1/2023.findings-acl.385Qu et al. (2018a)C. Qu, L. Yang,W. B. Croft, J. Trippas,Y. Zhang, and M. Qiu.2018a.Analyzing and Characterizing User Intent inInformation-seeking Conversations.. In SIGIR’18.Qu et al. (2018b)Chen Qu, Liu Yang,W. Bruce Croft, Johanne R. Trippas,Yongfeng Zhang, and Minghui Qiu.2018b.Analyzing and Characterizing User Intent inInformation-seeking Conversations. In The 41stInternational ACM SIGIR Conference on Research & Development inInformation