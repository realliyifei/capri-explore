# Conversational Machine Comprehension: a Literature Review

## Question

How is the structure and evaluation of conversations and dialogs organized in the CoQA dataset?

## URLs

1. https://ar5iv.org/html/1808.07042. [1808.07042] CoQA: A Conversational Question Answering Challenge
2. https://ar5iv.org/html/2401.10225. [2401.10225] ChatQA: Building GPT-4 Level Conversational QA Models
3. https://ar5iv.org/html/2102.02864. [2102.02864] ChainCQG: Flow-Aware Conversational Question Generation
4. https://ar5iv.org/html/2205.11685. [2205.11685] A Dataset for Sentence Retrieval for Open-Ended Dialogues
5. https://ar5iv.org/html/2408.11219. [2408.11219] CoDi: Conversational Distillation for Grounded Question Answering
6. https://ar5iv.org/html/2005.01328. [2005.01328] DoQA - Accessing Domain-Specific FAQs via Conversational QA
7. https://ar5iv.org/html/2312.16511. [2312.16511] S2M: Converting Single-Turn to Multi-Turn Datasets for Conversational Question Answering
8. https://ar5iv.org/html/2209.12617. [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
9. https://ar5iv.org/html/2405.13003. [2405.13003] A Survey on Recent Advances in Conversational Data Generation
10. https://ar5iv.org/html/2405.01121. [2405.01121] Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts

## Answer (I don't know?: ✗)

The CoQA dataset, which stands for Conversational Question Answering, is structured to enhance the development and effectiveness of conversational QA systems. It comprises a large collection of conversation turns that are integral to understanding how dialogs are formed and evaluated within the domain of NLP.

First, the CoQA dataset includes a total of 127,000 conversation turns, which are generated from 8,000 conversations. Each conversation has an average length of 15 turns, where a turn consists of a question and an answer. This structure aims to simulate the natural flow of human conversations, making it crucial for conversational modeling [1] [3].

The questions within CoQA are designed to not only be standalone but often refer back to previous turns in the dialog, which necessitates the use of anaphoric references. Almost half of the questions leverage the conversational history, adding a layer of complexity that challenges models aimed solely at lexical understanding [1] [3]. This highlights the conversational context's importance, encouraging models to incorporate memory mechanisms for better contextual understanding.

In terms of answer types, the dataset features free-form responses, with each answer providing a span-based rationale that is highlighted in the relevant passage. This indicates that while answers may take the form of excerpts from text passages, participants in the conversation possess the liberty to reformulate these answers. Notably, around 78% of the responses included some form of editing, showcasing the dataset's allowance for flexibility in expression while ensuring that the underpinning information is accurate [1] [4].

Regarding evaluation metrics, the CoQA dataset's performance benchmarks employ the F1 score, which reflects the accuracy of model responses against the ground truth. As of January 2019, the highest recorded F1 score achieved by a model on the CoQA test set was 82.8, indicating a robust standard for evaluating conversational understanding performance. The dataset serves as a pivotal resource for enhancing current methodologies in conversational QA by providing varied domain passages and dialogue examples that push the boundaries of comprehension in multi-turn interactions [2] [3].

CoQA utilizes several deep neural network architectures that build on established models for reading comprehension and conversational AI. The necessity for advanced modeling techniques stems from the dataset's complexities, such as its incorporation of pragmatic reasoning, where models must go beyond simple keyword matching to grasp nuanced conversational cues [3] [4].

In summary, the structure and evaluation of conversations within the CoQA dataset emphasize a dynamic dialogue system that incorporates historical context, allows for reformulated answers, and is rigorously evaluated using F1 scores to measure its effectiveness in conversational AI applications.

1. [1]:  https://ar5iv.org/html/1808.07042, [1808.07042] CoQA: A Conversational Question Answering Challenge
2. [2]:  https://ar5iv.org/html/1808.07042, [1808.07042] CoQA: A Conversational Question Answering Challenge
3. [3]:  https://ar5iv.org/html/1808.07042, [1808.07042] CoQA: A Conversational Question Answering Challenge
4. [4]:  https://ar5iv.org/html/2005.01328, [2005.01328] DoQA - Accessing Domain-Specific FAQs via Conversational QA
5. [5]:  https://ar5iv.org/html/2005.01328, [2005.01328] DoQA - Accessing Domain-Specific FAQs via Conversational QA
---
1. [1]:  Passage ID 1: this paper, we introduced CoQA, a large scale dataset for building conversational question answering systems.Unlike existing reading comprehension datasets, CoQA contains conversational questions, free-form answers along with text spans as rationales, and text passages from seven diverse domains.We hope this work will stir more research in conversational modeling, a key ingredient for enabling natural human-machine communication.AcknowledgementsWe would like to thank MTurk workers, especially the Master Chatters and the MTC forum members, for contributing to the creation of CoQA, for giving feedback on various pilot interfaces, and for promoting our hits enthusiastically on various forums.CoQA has been made possible with financial support from the Facebook ParlAI and the Amazon Research awards, and gift funding from Toyota Research Institute. Danqi is supported by a Facebook PhD fellowship. We also would like to thank the members of the Stanford NLP group for critical
2. [2]:  Passage ID 2: on CoQASince we first released the dataset in August 2018, the progress of developing better models on CoQA has been rapid.Instead of simply prepending the current question with its previous questions and answers, Huang et al. (2019) proposed a more sophisticated solution to effectively stack single-turn models along the conversational flow.Others (e.g., Zhu et al., 2018) attempted to incorporate the most recent pretrained language representation model BERT  Devlin et al. (2018)121212Pretrained BERT models were released in November 2018, which have demonstrated large improvements across a wide variety of NLP tasks. into CoQA and demonstrated superior results.As of the time we finalized the paper (Jan 8, 2019), the state-of-art F1 score on the test set was 82.8.8 ConclusionsIn this paper, we introduced CoQA, a large scale dataset for building conversational question answering systems.Unlike existing reading comprehension datasets, CoQA contains conversational questions,
3. [3]:  Passage ID 3: Reddit and science.The last two are used for out-of-domain evaluation.To summarize, CoQA has the following key characteristics:•It consists of 127k conversation turns collected from 8k conversations over text passages. The average conversation length is 15 turns, and each turn consists of a question and an answer.•It contains free-form answers and each answer has a span-based rationale highlighted in the passage.•Its text passages are collected from seven diverse domains: five are used for in-domain evaluation and two are used for out-of-domain evaluation.Almost half of CoQA questions refer back to conversational history using anaphors, and a large portion require pragmatic reasoning making it challenging for models that rely on lexical cues alone.We benchmark several deep neural network models, building on top of state-of-the-art conversational and reading comprehension models (Section 5).The best-performing system achieves an F1 score of 65.4%. In
4. [4]:  Passage ID 4: et al. (2018) are two conversational QA datasetscomprising QA dialogues that fulfill the information need of auser by answering questions about different topics. Similarly to our, bothdatasets are built by crowdsourcing, where one person (the questioner) ispresented with a topic and has to pose free-form questions about it. Anotherperson (the answerer) has to select an answer to the question by choosing anexcerpt from the relevant passage describing the topic. Some of thequestions in both datasets are unanswerable, and access to previous questions and answers areneeded in order to answer some of the questions.CoQA contains 127k questions with answers, obtained from 8k conversations about passages from broad domains, ranging from children stories to science. The answers are also excerpts from the relevant passage, but answerers have the choice of reformulating them. The authors report that 78%percent7878\% of the answers had at least one edit. Although reformulating answers
5. [5]:  Passage ID 5: On top, the original post, comprising a topic and an excerpt of the answer passage. In italics, dialogue acts (cf. Section 3).The overarching objective of our work is to access the large body of domain-specific information available in Frequently Asked Question sites (FAQ for short) via conversational Question Answering (QA) systems. In particular, we want to know whether current techniques are able to work with limited training data, and without needing to gather data for each target FAQ domain. In this paper we present DoQA, a task and associated dataset for accessing domain-specific FAQs via conversational QA111The DoQA dataset is available here: http://ixa.eus/node/12931. The dataset contains 2,437 information-seeking question/answer dialogues on three different domains (10,917 questions in total). These dialogues are created using the Wizard of Oz technique by crowdworkers that play the following two roles: the user asks questions about a given topic posted in Stack