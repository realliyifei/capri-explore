# Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics

## Question

What are the key assumptions of Centering Theory, and how does it model coherence and salience in discourse?

## URLs

1. https://ar5iv.org/html/1805.04993
2. https://ar5iv.org/html/2004.14626
3. https://ar5iv.org/html/1507.08234

## Answer

I don't know. The context provided does not mention Centering Theory, coherence, or salience in discourse. It focuses primarily on natural language processing (NLP) subfields like natural language understanding (NLU) and natural language generation (NLG), as well as various evaluation benchmarks in NLP, without addressing any specific discourse theories or models related to coherence and salience. Thus, there is insufficient information to answer the question.

[1]: https://ar5iv.org/html/2103.00020, No Title
[2]: https://ar5iv.org/html/2209.12617, No Title
[3]: https://ar5iv.org/html/2209.12617, No Title
[4]: https://ar5iv.org/html/2209.12617, No Title
[5]: https://ar5iv.org/html/2310.19736, No Title

[1]: Passage ID 1: admittedly extremely broad area and covers most work in the field of distributional semantics including topic models (Blei et al., 2003), word, sentence, and paragraph vectors (Mikolov et al., 2013; Kiros et al., 2015; Le & Mikolov, 2014), and language models (Bengio et al., 2003). It also includes much of the broader field of NLP that deals with predicting or modeling sequences of natural language in some way. Work in NLP intentionally leveraging natural language supervision in the form of explanations, feedback, instructions, and advice for tasks such as classification (as opposed to the commonly used representation of supervision as a set of arbitrarily encoded discrete category labels) has been explored in many creative and advanced ways. Dialog based learning (Weston, 2016; Li et al., 2016; Hancock et al., 2019) develops techniques to learn from interactive natural language feedback in dialog. Several papers have leveraged semantic parsing to convert natural language explanations
[2]: Passage ID 2: Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence (AI) concerned with text understanding and text generation. The former subject is studied in the sub-branch natural language understanding (NLU) 1; 2; 3 and the latter in natural language generation (NLG) 4; 5. Over the years, both fields, i.e., NLU and NLG developed enormously with an extensive literature which requires nowadays a dedicated discussion of specialized subtasks when presenting approaches or methods thereof despite the fact that a systems understanding of NLP can only be achieved holistically.In this paper, we focus on subtasks of NLP centered around question answering (QA). The task of a QA system is to find an answer (output) in the form of a natural language for a given question (input) usually presented in form of a sentence. While our focus is on QA systems and their
[3]: Passage ID 3: Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence (AI) concerned with text understanding and text generation. The former subject is studied in the sub-branch natural language understanding (NLU) 1; 2; 3 and the latter in natural language generation (NLG) 4; 5. Over the years, both fields, i.e., NLU and NLG developed enormously with an extensive literature which requires nowadays a dedicated discussion of specialized subtasks when presenting approaches or methods thereof despite the fact that a systems understanding of NLP can only be achieved holistically.In this paper, we focus on subtasks of NLP centered around question answering (QA). The task of a QA system is to find an answer (output) in the form of a natural language for a given question (input) usually presented in form of a sentence. While our focus is on QA systems and their
[4]: Passage ID 4: Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence (AI) concerned with text understanding and text generation. The former subject is studied in the sub-branch natural language understanding (NLU) 1; 2; 3 and the latter in natural language generation (NLG) 4; 5. Over the years, both fields, i.e., NLU and NLG developed enormously with an extensive literature which requires nowadays a dedicated discussion of specialized subtasks when presenting approaches or methods thereof despite the fact that a systems understanding of NLP can only be achieved holistically.In this paper, we focus on subtasks of NLP centered around question answering (QA). The task of a QA system is to find an answer (output) in the form of a natural language for a given question (input) usually presented in form of a sentence. While our focus is on QA systems and their
[5]: Passage ID 5: of intelligence has become a crucial issue.In the nascent stages of NLP, researchers have commonly employed a set of straightforward benchmark tests to evaluate their language models. These initial evaluations primarily concentrate on aspects such as grammar and vocabulary, encompassing tasks like syntactic parsing, word sense disambiguation, and so on.In the early 1990s, the advent of the MUC evaluation (Grishman & Sundheim, 1996) has marked a significant milestone in the NLP community.The MUC evaluation primarily centers on information extraction tasks, challenging participants to extract specific information from text. This evaluation framework plays a pivotal role in propelling the field of information extraction forward. Subsequently, with the emergence of deep learning in the 2010s, the NLP community embraces more expansive benchmarks like SNLI (Bowman et al., 2015) and SQuAD (Rajpurkar et al., 2016). These benchmarks not only evaluate system performance but also provide