# To what extent is Immediate Constituency Analysis dependency-based? A survey of foundational texts

## Question

How do dependency trees and phrase structure trees (PSTs) represent syntagmatic relations and connections differently?

## URLs

1. https://ar5iv.org/html/1604.08120. [1604.08120] Untitled Document
2. https://ar5iv.org/html/2105.12530. [2105.12530] Deception detection in text and its relation to the cultural dimension of individualism/collectivism

## Answer (I don't know?: ✓)

I don't know. The provided context does not include information on dependency trees or phrase structure trees (PSTs) and their representation of syntagmatic relations and connections. The excerpts focus primarily on deception detection methods involving linguistic features, psycholinguistics, and some theory applications such as Rhetorical Structure Theory and Vector Space Modeling, but they do not discuss the specifics of dependency trees or PSTs. Therefore, the context is insufficient to answer the question.

1. [1]:  https://ar5iv.org/html/2105.12530, [2105.12530] Deception detection in text and its relation to the cultural dimension of individualism/collectivism
2. [2]:  https://ar5iv.org/html/2105.12530, [2105.12530] Deception detection in text and its relation to the cultural dimension of individualism/collectivism
3. [3]:  https://ar5iv.org/html/2105.12530, [2105.12530] Deception detection in text and its relation to the cultural dimension of individualism/collectivism
4. [4]:  https://ar5iv.org/html/2105.12530, [2105.12530] Deception detection in text and its relation to the cultural dimension of individualism/collectivism
5. [5]:  https://ar5iv.org/html/2105.12530, [2105.12530] Deception detection in text and its relation to the cultural dimension of individualism/collectivism
---
1. [1]:  Passage ID 1: have been used as a gold standard in many works.Kleinberg et al. (2018) examined the hypothesis that the number ofnamed entities is higher in truthful than in deceptive statements,by comparing the discriminative ability of named entitieswith a lexicon word count approach (LIWC) and a measure of sentence specificity. The results suggest that named entities may be a useful addition to existing approaches.Feng et al. (2012) investigated howsyntactic stylometry can help in text deception detection.The features were obtained from Context FreeGrammar (CFG) parse trees and were tested over four different datasets, spanning fromproduct reviews to essays. The results showed improved performance compared to several baselines that were based on shallower lexico-syntactic features.Discourse and pragmatics have also been used for the task of deceptiondetection. Rhetorical Structure Theory (RST) and Vector Space Modeling (VSM) arethe twotheoretical components that have been applied
2. [2]:  Passage ID 2: and pragmatics have also been used for the task of deceptiondetection. Rhetorical Structure Theory (RST) and Vector Space Modeling (VSM) arethe twotheoretical components that have been applied by Rubin and Vashchilko (2012) in order toset apart deceptive and truthful stories. Theauthors proposed a two-step approach: in the first step, they analyzedrhetoricalstructures, discourse constituent parts and their coherence relations, whereas inthe second, they applied a vector space model to cluster the stories bydiscourse feature similarity. Pisarevskaya and Galitsky (2019) alsoexplored thehypothesis that deception in text should be visible from itsdiscourse structure. They formulated the task of deception detectionas a classification task using discourse trees, based on RST. For evaluation reasons,they created a dataset containing 2,746 truthful and deceptive complaints about banks in English, where the proposed solution achieved a classification accuracy of 70 per cent.The
3. [3]:  Passage ID 3: Forthe extraction of the psycholinguistic features, the MPQA subjectivitylexicon222http://mpqa.cs.pitt.edu/lexicons/subj_lexicon was used,as well as manually created lists. Various LIWCpsycholinguistic, morphological and n-gram features for tackling the problem of the automatic detectionof deceptive opinion spam333Any fictitious opinion that has been deliberatelywritten to sound authentic. are examined by Ott et al. (2011, 2013). Thesefeature setswere tested in a linear Support Vector Machine (SVM) (Cortes and Vapnik, 1995). In these two works Ott et al. (2011, 2013) provide two datasets with deceptive and truthfulopinions, one with positive sentiment reviews (Ott et al., 2011) and one with negative sentiment (Ott et al., 2013). These datasets, either in isolation or combined, have been used as a gold standard in many works.Kleinberg et al. (2018) examined the hypothesis that the number ofnamed entities is higher in truthful than in deceptive statements,by comparing the
4. [4]:  Passage ID 4: 2012).Similarly, in the work of Tsunomori et al. (2015) a dialoguecorpus for theJapaneselanguage is presented and subsequently abinary classification based on decision trees over this corpus is performedusing acoustic/prosodic,lexical andsubject-dependent features.The comparison with asimilar English corpus has shown interesting results. More specifically, whilein theprosodic/acoustic features there were no differences between the two languages,inlexical features the results were greatly different. In English, noise, thirdperson pronoun, and features indicating the presence of “Yes” or “No” wereeffective. In Japanese the lexical features used in this research were largely ineffective; and only one lexical feature, the one that indicated the presence of a verb base form,proved effective.For theChinese language, one of the first studies is that of Zhou and Sung (2008) whoexamined thecomputer-mediated communication of Chinese players engaged in the Werewolfgame.
5. [5]:  Passage ID 5: lower performance. However, POS-gram seem to perform quite better than the syntactic n-grams.The difference in accuracy decreases in cross domain experiments in which semantic information is more diverse, and as already discussed, linguistic indications of deception change from one domain to another. Lastly, stemming, stopwords removal and lowercase conversion are generally beneficiary, so it is a preprocessing step that must be examined.The experimental results show that the discriminative power of linguistic markers of deception is overly better than random baseline and the expected human performance (according to literature slightly better than chance, see Section 2) especially in one domain scenarios (see Tables 13 to 17). More specifically, linguistic markers of deception are struggling in cross domain settings (see Section 6.1.2). This confirms that linguistic markers of deception vary considerably and are extremely sensitive even within the same culture, letalone across