# Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation

## Question

What strategies can be employed to improve low inter-rater reliability in annotator judgments for natural language generation tasks?

## URLs

1. https://ar5iv.org/html/2410.14515. [2410.14515] Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media
2. https://ar5iv.org/html/2412.04363. [2412.04363] Challenges in Trustworthy Human Evaluation of Chatbots
3. https://ar5iv.org/html/2411.15634. [2411.15634] "All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations
4. https://ar5iv.org/html/2410.13313. [2410.13313] Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language
5. https://ar5iv.org/html/2301.10684. [2301.10684] Consistency is Key: Disentangling Label Variation in Natural Language Processing with Intra-Annotator Agreement
6. https://ar5iv.org/html/2404.09696. [2404.09696] Are Large Language Models Reliable Argument Quality Annotators?Supported by the German Research Foundation (Project Nr. 455911521).
7. https://ar5iv.org/html/2410.03608. [2410.03608] TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation
8. https://ar5iv.org/html/2410.14632. [2410.14632] Diverging Preferences: When do Annotators Disagree and do Models Know?
9. https://ar5iv.org/html/2405.00291. [2405.00291] How Can I Improve? Using GPT to Highlight the Desired and Undesired Parts of Open-ended Responses

## Answer (I don't know?: ✗)

To improve low inter-rater reliability in annotator judgments for natural language generation (NLG) tasks, several strategies can be employed based on the insights from the provided context.

1. **Standardization of Guidelines**: One effective strategy is to develop and implement standardized guidelines for annotators that clearly define the criteria for evaluation. This can help ensure that all annotators have a consistent understanding of what needs to be assessed. The context mentions guidelines collected from high-quality NLP conferences, indicating that drawing from established practices can enhance consistency and clarity in judgment [1].

2. **Training for Annotators**: Providing comprehensive training sessions for annotators can also improve inter-rater reliability. Training can focus on familiarizing annotators with the evaluation criteria, including practical examples and clarifications. The process of engaging annotators with clear expectations and detailed frameworks can align their judgments more effectively, as consistent training reduces subjective interpretation [2].

3. **Regular Calibration Sessions**: Incorporating regular calibration sessions where annotators discuss and align their interpretations of the guidelines can be beneficial. Such discussions help identify discrepancies and foster a shared understanding among annotators. This method ensures that refinements in judgment criteria are well-communicated and understood, which can further enhance reliability [2][3].

4. **Quantitative Metrics and Error Analysis**: Introducing quantitative metrics to evaluate annotators' decisions can facilitate the identification of patterns in their judgments. Utilizing error scores based on approaches like n-gram matching and contextual embeddings can provide objective measures to assess the reliability of annotators’ judgments [2][3]. By quantifying the evaluation outcomes, researchers can spotlight specific areas where annotators tend to disagree, allowing for targeted improvements in guidelines or training.

5. **Vulgarity Examination and Feedback Mechanisms**: The contextual data hints at a relationship between the number of identified vulnerabilities and the inter-annotator agreements. Analyzing these vulnerabilities or points of contention can help researchers recognize the sources of variability in judgments [1]. Establishing feedback mechanisms where annotators can report difficulties or ambiguities in the guidelines could lead to iterative improvements in the evaluation process.

6. **Use of Multiple Annotators per Task**: Involving multiple annotators for each judgment can inherently increase reliability as it allows for averaging out individual biases. When disagreements do occur, conducting discussions to resolve differences can contribute to a deeper understanding of the subtleties in judgments. This approach not only raises average reliability but also allows for identifying nuanced aspects of NLG tasks that may require further clarification in guidelines [1][3].

In conclusion, improving inter-rater reliability in annotator judgments for NLG tasks can be achieved through the establishment of standardized guidelines, thorough training, calibration sessions, quantitative assessments, and collaborative feedback mechanisms. These strategies, when implemented cohesively, can significantly enhance the consistency and quality of evaluation in NLG projects.

1. [1]:  https://ar5iv.org/html/2406.07935, No Title
2. [2]:  https://ar5iv.org/html/2209.12617, No Title
3. [3]:  https://ar5iv.org/html/2209.12617, No Title
4. [4]:  https://ar5iv.org/html/2209.12617, No Title
5. [5]:  https://ar5iv.org/html/2209.12617, No Title
---
1. [1]:  Passage ID 1: nlg evaluation: Evaluation practices, assumptions, and their implications.arXiv preprint arXiv:2205.06828.Appendix A Authentic Guidelines DetailsFor the collected data, we focused on work related to human evaluation in NLG tasks. In descending order of frequency, specific tasks include summarization (42), dialogue generation(36), question answering (34), machine translation (26), story generation (20), image captioning (9), etc. These guidelines are collected from high-quality NLP conferences ACL, EMNLP and NAACL over the past three years (2020-2022). Apart from machine translation that cover a range of language pairs like English-French, English-Japanese, Chinese -English, English-German, English-Spanish, and English-Russian, most of the tasks primarily focus on the English language. Additionally, we have gathered information on the reported inter-annotator agreement, revealing a general inverse relationship between the number of identified vulnerabilities and the
2. [2]:  Passage ID 2: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence
3. [3]:  Passage ID 3: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence
4. [4]:  Passage ID 4: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence
5. [5]:  Passage ID 5: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence