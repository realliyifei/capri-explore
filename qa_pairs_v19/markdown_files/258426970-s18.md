# Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation

## Question

What strategies can be employed to improve low inter-rater reliability in annotator judgments for natural language generation tasks?

## URLs

1. https://ar5iv.org/html/2410.14515. [2410.14515] Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media
2. https://ar5iv.org/html/2412.04363. [2412.04363] Challenges in Trustworthy Human Evaluation of Chatbots
3. https://ar5iv.org/html/2411.15634. [2411.15634] "All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations
4. https://ar5iv.org/html/2410.13313. [2410.13313] Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language
5. https://ar5iv.org/html/2301.10684. [2301.10684] Consistency is Key: Disentangling Label Variation in Natural Language Processing with Intra-Annotator Agreement
6. https://ar5iv.org/html/2404.09696. [2404.09696] Are Large Language Models Reliable Argument Quality Annotators?Supported by the German Research Foundation (Project Nr. 455911521).
7. https://ar5iv.org/html/2410.03608. [2410.03608] TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation
8. https://ar5iv.org/html/2410.14632. [2410.14632] Diverging Preferences: When do Annotators Disagree and do Models Know?
9. https://ar5iv.org/html/2405.00291. [2405.00291] How Can I Improve? Using GPT to Highlight the Desired and Undesired Parts of Open-ended Responses

## Answer (I don't know?: âœ—)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable