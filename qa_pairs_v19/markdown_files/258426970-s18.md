# Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation

## Question

What strategies can be employed to improve low inter-rater reliability in annotator judgments for natural language generation tasks?

## URLs

1. https://ar5iv.org/html/2410.14515. [2410.14515] Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media
2. https://ar5iv.org/html/2412.04363. [2412.04363] Challenges in Trustworthy Human Evaluation of Chatbots
3. https://ar5iv.org/html/2411.15634. [2411.15634] "All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations
4. https://ar5iv.org/html/2410.13313. [2410.13313] Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language
5. https://ar5iv.org/html/2301.10684. [2301.10684] Consistency is Key: Disentangling Label Variation in Natural Language Processing with Intra-Annotator Agreement
6. https://ar5iv.org/html/2404.09696. [2404.09696] Are Large Language Models Reliable Argument Quality Annotators?Supported by the German Research Foundation (Project Nr. 455911521).
7. https://ar5iv.org/html/2410.03608. [2410.03608] TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation
8. https://ar5iv.org/html/2410.14632. [2410.14632] Diverging Preferences: When do Annotators Disagree and do Models Know?
9. https://ar5iv.org/html/2405.00291. [2405.00291] How Can I Improve? Using GPT to Highlight the Desired and Undesired Parts of Open-ended Responses

## Answer (I don't know?: ✗)

To improve low inter-rater reliability in annotator judgments for natural language generation tasks, various strategies can be employed based on the context provided.

1. **Utilizing Multiple Evaluators:** Engaging multiple annotators for evaluating the models can help in mitigating individual biases and discrepancies in judgments. For instance, in the evaluation process described, the study employs three graduate students specialized in natural language processing as annotators who independently assess candidate models' outputs [2]. This allows for a broader perspective and potentially increases reliability by averaging out individual differences in judgement.

2. **Anonymizing Responses:** Impartial evaluations can be enhanced by anonymizing the responses to prevent biases linked to the familiarity with the candidate models. The annotators were intentionally not informed about which model generated specific responses, thereby promoting fairness in their assessments [2]. This practice helps in ensuring that the evaluations are based solely on the quality of responses and not influenced by any preconceived notions about the models.

3. **Binary Assessment Framework:** Implementing a clear and straightforward scoring system can help reduce the complexity and ambiguity in evaluations. The annotators in the study used a binary scale to score outputs, marking ‘1’ for ‘True’ and ‘0’ for ‘False,’ which allows for straightforward interpretations of their judgments [2]. This simplifies the decision-making process and helps in aligning the evaluators' responses more consistently.

4. **Rigorous Training and Guidelines for Annotators:** Providing comprehensive training and clear guidelines for the annotators can greatly improve their ability to make consistent judgments. This might involve training on what constitutes a "True" response versus a "False" one, focusing their attention on the accuracy and relevance of the responses [2]. When evaluators have a solid understanding of evaluation criteria, it can significantly enhance inter-rater reliability. 

5. **Consensus Building Among LLM Raters:** The description of the evaluation process utilizing LLMs (GPT-4 and Llama3-70b) as judges also highlights a method for achieving consensus in ratings. If these two judges reach an agreement, their judgment is taken as final, which can streamline the evaluation process [1]. However, if there's disagreement, it prompts a manual review, thus ensuring that any inconsistencies are systematically addressed.

6. **Focusing on Contextual Relevance:** The annotators’ emphasis on both accuracy and contextual relevance when assessing answers enhances reliability, as it supports evaluations that consider the nuances of language and intended meaning, rather than just superficial correctness [2].

7. **Handling Complex Annotations with Expert Judgments:** Recognizing the challenges posed by tasks requiring complex expert judgment suggests that developing tools and methods to interpret low reliability annotations is essential. This can involve crafting expressive evaluation methods that are responsive to the inherent complexities in NLP tasks, particularly those related to education, as highlighted [3][4]. Addressing the known vulnerabilities of NLP models due to inexpert annotations emphasizes the need for more robust methods in evaluating model outputs.

By combining these strategies, researchers and practitioners in NLP can enhance inter-rater reliability, leading to better evaluation of natural language generation tasks.

1. [1]:  https://ar5iv.org/html/2406.17675, No Title
2. [2]:  https://ar5iv.org/html/2408.09235, No Title
3. [3]:  https://ar5iv.org/html/2411.15634, [2411.15634] "All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations
4. [4]:  https://ar5iv.org/html/2411.15634, [2411.15634] "All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations
5. [5]:  https://ar5iv.org/html/2411.15634, [2411.15634] "All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations
---
1. [1]:  Passage ID 1: and human evaluators.This approach is inspired by the CoAnnotating method [136]. The evaluation process is as follows: We first employ GPT-4 and Llama3-70b as two judges, instructing them to determine whether the answers to the question demonstrate their confidence. If both LLM raters reach a consensus, their judgment stands as the result. If they do not agree, our research team manually reviews the responses to determine the outcome. The following prompt is used for LLMs:You are a helpful evaluator. You will be provided with a question-answer pair, and the answer is given by an AI model. Your task is to help me identify whether the answer reflects the statement you give you. If the answer reflects the statement, please return ‘‘Yes’’, if not, please return ‘‘No’’. The statement is {statement}.Here is the question: {question}.Here is the answer: {answer}.Answer rule:- You should only reply with ‘‘Yes’’ or ‘‘No’’.The final confidence score for the specific category of
2. [2]:  Passage ID 2: We recruit three graduate students from our academic network, all specialized in natural language processing, to serve as annotators. We provide the input given to the candidates, reference answers, and candidate responses. This format, while similar, is distinct from the judge models’ prompts which additionally require formatted decisions. The human annotators focus solely on the accuracy and relevance of the responses. To ensure impartial evaluations, we anonymize the origin of responses. Annotators do not know which candidate model generated such responses, reducing potential bias linked to model familiarity or reputation. We asked the annotators to score the candidate LLMs outputs on a binary scale: ‘1’ for ‘True’ and ‘0’ for ‘False’ based on alignment with the reference answer and contextual relevance.To ensure a rigorous evaluation, each of the three annotators independently assesses the entire set of outputs generated by each candidate model across all datasets. Specifically,
3. [3]:  Passage ID 3: from not using tools expressive enough to interpret labels in low reliability. To that end, this work demonstrates methods for working with low/unknown reliability annotations, often found in tasks requiring complex expert judgment.The field of education has many complex tasks that often yield low reliabilities in labels (Jurenka et al., 2024; Kane and Staiger, 2012) which make edtech NLP models and research particularly vulnerable to the effects of inexpert annotations Belz et al. (2020); van der Lee et al. (2019); Zhou et al. (2023). The case study used to illustrate more expressive methods for working with unreliable labels will be from K12 education. Specifically, this study examines a use case where expert annotations are highly unreliable and yet used in high-stakes decisions: automated rating of the quality of classroom teaching. Methods used in this paper answer the call from others to evaluate the psychometric properties of models that perform this task (Casabianca et al.,
4. [4]:  Passage ID 4: from not using tools expressive enough to interpret labels in low reliability. To that end, this work demonstrates methods for working with low/unknown reliability annotations, often found in tasks requiring complex expert judgment.The field of education has many complex tasks that often yield low reliabilities in labels (Jurenka et al., 2024; Kane and Staiger, 2012) which make edtech NLP models and research particularly vulnerable to the effects of inexpert annotations Belz et al. (2020); van der Lee et al. (2019); Zhou et al. (2023). The case study used to illustrate more expressive methods for working with unreliable labels will be from K12 education. Specifically, this study examines a use case where expert annotations are highly unreliable and yet used in high-stakes decisions: automated rating of the quality of classroom teaching. Methods used in this paper answer the call from others to evaluate the psychometric properties of models that perform this task (Casabianca et al.,
5. [5]:  Passage ID 5: from not using tools expressive enough to interpret labels in low reliability. To that end, this work demonstrates methods for working with low/unknown reliability annotations, often found in tasks requiring complex expert judgment.The field of education has many complex tasks that often yield low reliabilities in labels (Jurenka et al., 2024; Kane and Staiger, 2012) which make edtech NLP models and research particularly vulnerable to the effects of inexpert annotations Belz et al. (2020); van der Lee et al. (2019); Zhou et al. (2023). The case study used to illustrate more expressive methods for working with unreliable labels will be from K12 education. Specifically, this study examines a use case where expert annotations are highly unreliable and yet used in high-stakes decisions: automated rating of the quality of classroom teaching. Methods used in this paper answer the call from others to evaluate the psychometric properties of models that perform this task (Casabianca et al.,