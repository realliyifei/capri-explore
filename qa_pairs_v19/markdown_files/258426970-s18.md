# Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation

## Question

What strategies can be employed to improve low inter-rater reliability in annotator judgments for natural language generation tasks?

## URLs

1. https://ar5iv.org/html/2410.14515. [2410.14515] Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media
2. https://ar5iv.org/html/2412.04363. [2412.04363] Challenges in Trustworthy Human Evaluation of Chatbots
3. https://ar5iv.org/html/2411.15634. [2411.15634] "All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations
4. https://ar5iv.org/html/2410.13313. [2410.13313] Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language
5. https://ar5iv.org/html/2301.10684. [2301.10684] Consistency is Key: Disentangling Label Variation in Natural Language Processing with Intra-Annotator Agreement
6. https://ar5iv.org/html/2404.09696. [2404.09696] Are Large Language Models Reliable Argument Quality Annotators?Supported by the German Research Foundation (Project Nr. 455911521).
7. https://ar5iv.org/html/2410.03608. [2410.03608] TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation
8. https://ar5iv.org/html/2410.14632. [2410.14632] Diverging Preferences: When do Annotators Disagree and do Models Know?
9. https://ar5iv.org/html/2405.00291. [2405.00291] How Can I Improve? Using GPT to Highlight the Desired and Undesired Parts of Open-ended Responses

## Answer (I don't know?: ✗)

Improving low inter-rater reliability in annotator judgments for natural language generation (NLG) tasks can be approached through several strategies rooted in the evaluation of annotator reliability and the incorporation of advanced methodologies. Here are some key strategies drawn from the provided context:

1. **Utilization of Advanced Annotation Frameworks**: Implementing frameworks such as the EffiARA annotation framework can significantly enhance understanding of annotator reliability. This framework emphasizes inter- and intra-annotator agreement, which allows for a more comprehensive evaluation of each annotator's reliability [4]. By actively modeling the variability in annotations, researchers can identify and mitigate instances of low reliability.

2. **Incorporating Large Language Models (LLMs)**: A significant strategy involves the use of LLMs, such as those in the GPT family, to assist in the annotation process. Research indicates that LLMs can be prompted to generate annotations that align more closely with expert judgments, thereby potentially elevating overall annotation quality [3] [5]. The comparative analysis between LLM-generated annotations and those from human raters can provide insights into the consistency and reliability of various strategies.

3. **Structured Annotation Tasks and Guidelines**: Developing a prescriptive annotation framework that clearly outlines the criteria for what constitutes high-quality annotations is crucial. This prescriptive approach includes developing training materials and guidelines tailored to the specific NLG tasks being evaluated, which could reduce confusion among annotators and enhance reliability [5].

4. **Inter-Annotator Agreement Metrics**: Employing inter-annotator agreement metrics rigorously can provide a quantifiable measure of consistency among raters. By applying these metrics, as seen in the study of argument quality annotations, researchers can identify specific areas where agreement is low and adjust training or guidelines to address these inconsistencies [2] [3].

5. **Sample Weighting and Soft-Label Training**: In assessment scenarios, implementing sample weighting based on annotator reliability can optimize model training. This approach leverages inter-annotator reliability to adjust the influence of each annotator's judgment on the training of models, ultimately leading to improved performance outcomes [4]. 

6. **Training and Calibration of Annotators**: Providing targeted training for annotators, particularly in complex areas where expertise is often low, can improve judgment reliability. Providing experience and calibration sessions before extensive annotation tasks helps standardize judgments among annotators by establishing a common understanding of quality expectations [1].

7. **Continuous Evaluation and Feedback Mechanisms**: Incorporating a feedback loop where annotators receive continuous insights into their performance against a benchmark can foster improvement. By identifying patterns in errors or discrepancies, annotators can recalibrate their approach to align more closely with desired outcomes.

In conclusion, enhancing inter-rater reliability in NLG tasks relies on combining advanced frameworks, the integration of LLMs, clear guidelines, robust evaluative metrics, targeted training initiatives, and continuous feedback mechanisms. These strategies collectively aim to mitigate the impact of unreliable annotations, thereby improving the overall quality of NLG systems [1] [3] [4] [5].

1. [1]:  https://ar5iv.org/html/2411.15634, [2411.15634] "All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations
2. [2]:  https://ar5iv.org/html/2404.09696, [2404.09696] Are Large Language Models Reliable Argument Quality Annotators?Supported by the German Research Foundation (Project Nr. 455911521).
3. [3]:  https://ar5iv.org/html/2411.15634, [2411.15634] "All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations
4. [4]:  https://ar5iv.org/html/2410.14515, [2410.14515] Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media
5. [5]:  https://ar5iv.org/html/2410.13313, [2410.13313] Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language
---
1. [1]:  Passage ID 1: from not using tools expressive enough to interpret labels in low reliability. To that end, this work demonstrates methods for working with low/unknown reliability annotations, often found in tasks requiring complex expert judgment.The field of education has many complex tasks that often yield low reliabilities in labels (Jurenka et al., 2024; Kane and Staiger, 2012) which make edtech NLP models and research particularly vulnerable to the effects of inexpert annotations Belz et al. (2020); van der Lee et al. (2019); Zhou et al. (2023). The case study used to illustrate more expressive methods for working with unreliable labels will be from K12 education. Specifically, this study examines a use case where expert annotations are highly unreliable and yet used in high-stakes decisions: automated rating of the quality of classroom teaching. Methods used in this paper answer the call from others to evaluate the psychometric properties of models that perform this task (Casabianca et al.,
2. [2]:  Passage ID 2: of the task, their results showed a reasonable correlation between highly-trained human assessors and fully automated judgements.Closest to our work is that by Chiang et al. [5], who compared the judgments of GPT-3 on text quality to expert human judgments on a 5-point Likert scale for four quality attributes: grammaticality, cohesiveness, likability, and relevance. Their findings revealed varying degrees of positive correlations between GPT-3 and human judgments, ranging from weak to strong.When compared to existing research, our work pioneers the study of argument quality annotations generated by LLMs. In order to provide a thorough evaluation, we use an inter-annotator agreement metric to assess the consistency of annotations from these models, human experts and novices. This comparison allows us to understand the alignment between LLMs and human annotators, and to determine the potential of using LLMs as argument quality annotators.3 Experimental DesignTo investigate
3. [3]:  Passage ID 3: Ho and Kane?Answering whether automated ratings can similarly improve human annotations is understanding the extent to which models’ added contributions would result in similar benefits as expected from humans. Thus, this study illustrates methods for working with unreliable labels in NLP tasks by investigating and disentangling the variation found in human and model raters from the variation found within the observations and the instrument used for the annotation task. The model raters are comprised of two families: the "GPT" family of autoregressive in-context learners from Wang and Demszky (2023) (using ChatGPT) with three models whose siblings differ by prompt engineering strategies and an "Encoder" family built for this study whose five siblings differ in embeddings and a few adjustments to training hyperparameters. Quality of ratings will be examined between and within families and individual raters.2 Related Work2.1 Annotation Quality and BiasBetter understanding
4. [4]:  Passage ID 4: a mitigation strategy, such as X’s community notes, which is currently a manual process. This study takes a knowledge-based approach to misinformation detection, modelling the problem similarly to one of natural language inference. The EffiARA annotation framework is introduced, aiming to utilise inter- and intra-annotator agreement to understand the reliability of each annotator and influence the training of large language models for classification based on annotator reliability. In assessing the EffiARA annotation framework, the Russo-Ukrainian Conflict Knowledge-Based Misinformation Classification Dataset (RUC-MCD) was developed and made publicly available. This study finds that sample weighting using annotator reliability performs the best, utilising both inter- and intra-annotator agreement and soft-label training. The highest classification performance achieved using Llama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.Efficient Annotator Reliability
5. [5]:  Passage ID 5: language use, making them more accommodating of English variety. Finally, we observed that skilled and well-trained human annotators are not always readily available. Instead of relying on untrained annotators who lack expertise in language or social studies, we investigate whether prompted large language models (LLMs) can serve as a viable alternative.As shown in Figure 1, our research addresses three key components: (1) proposing criteria for a prescriptive annotation framework that will be introduced in methodology, (2) conducting a small-scale statistical analysis to compare the framework with the descriptive paradigm and evaluate the performance of prescriptively-prompted LLMs, and (3) testing the framework under limited conditions, using smaller datasets with complex language features without human annotators.To assess annotation quality, we compared inter-rater reliability across three sets: 400 pieces from the Davidson et al., 2017 dataset following general definitions,