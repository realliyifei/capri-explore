# Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey

## Question

How do filtering methods enhance the quality of questions generated by neural ranking models in open-domain question answering systems?

## URLs

1. https://ar5iv.org/html/2410.19572
2. https://ar5iv.org/html/2308.02926
3. https://ar5iv.org/html/2112.05452
4. https://ar5iv.org/html/2406.05794
5. https://ar5iv.org/html/2406.05733
6. https://ar5iv.org/html/2402.17497
7. https://ar5iv.org/html/2405.17822
8. https://ar5iv.org/html/2404.08695
9. https://ar5iv.org/html/2310.13682
10. https://ar5iv.org/html/2406.01549

## Answer

## I don't know?: False

The context provided does not contain sufficient information about how filtering methods specifically enhance the quality of questions generated by neural ranking models in open-domain question answering (QA) systems. Although there are discussions surrounding answer validation, answer filtering, and the quality improvements in QA systems, the content primarily revolves around the methods used to filter answers rather than how these methods relate to the question generation process.

For instance, the passages highlight that filtering methods can lead to a significantly improved QA quality by removing incorrect answers, thus increasing the overall accuracy when evaluating answer candidates [2]. However, the focus is on answer validation rather than the generation of questions or the mechanisms of neural ranking models [1]. 

Moreover, while there is mention of generating questions using personas and layman‚Äôs language [3] [5], there is no direct connection made to how filtering impacts the quality of those questions generated by neural ranking models. The passages do not provide a clear outline or mechanism explaining the relationship between filtering methods and the enhancement of question quality in the context of neural ranking models.

Therefore, an expert in NLP seeking a thorough understanding of the enhancement of question quality through filtering methods would find the context lacking as it does not address the mechanisms or processes by which filtering could improve question generation specifically. Hence, I cannot provide a precise answer to the question asked.

[1]: https://ar5iv.org/html/2112.05452, [2112.05452] Improving the Question Answering Quality using Answer Candidate Filtering based on Natural-Language Features
[2]: https://ar5iv.org/html/2112.05452, [2112.05452] Improving the Question Answering Quality using Answer Candidate Filtering based on Natural-Language Features
[3]: https://ar5iv.org/html/2410.00427, No Title
[4]: https://ar5iv.org/html/2112.05452, [2112.05452] Improving the Question Answering Quality using Answer Candidate Filtering based on Natural-Language Features
[5]: https://ar5iv.org/html/2410.00427, No Title

[1]: Passage ID 1: computed using Natural Language Generation (NLG) considering the contained facts, and(A3subscriptùê¥3A_{3}) computed using a bag-of-labels approach of available entities.In this paper, we follow our long-term research agenda of improving the overall quality of KGQA systems following a domain-agnostic approach that is not limited to just a single class of KGQA systems.Therefore, while having limited access to internal data structures of a KGQA system, the NL form of the questions and answers gains importance.To show the significance of our approach, we not only consider AV module quality (i.e.,¬†F1 Score) but also its impact on the end-to-end QA quality (i.e.,¬†Precision@k, NDCG@k).In this paper, we address the following research questions considering the task of filtering NL answer candidates:RQ1Is it possible to improve the QA quality while filtering answers just by their NL representation?RQ2What QA quality is achievable while filtering the well-formed NL
[2]: Passage ID 2: answers from a list of answer candidates is leading to a highly improved QA quality.In particular, our approach has shown its potential while removing in many cases the majority of incorrect answers, which increases the QA quality significantly in comparison to the non-filtered output of a system.Index Terms: question answering, answer validation, answer filtering, answer ranking, improving question answering quality, natural language processing, English languageI IntroductionFigure 1: Overview of the general research ideaTABLE I: Answer Validation publications of the last decade [1]Ref.YearLanguagesDatasetsMethodsEvaluation score[2]2010English, SpanishResPubliQA [3]EAT, NER, Acronym Checking 65% Accuracy (English)57% Accuracy (Spanish)[4]2010SpanishCLEF 06 [5]RTE53% Accuracy[6]2011GermanCLEF 11 [7]Rule-set44% Accuracy[8]2011FrenchWebDecision Tree Combination53% MRR[9]2012GermanCLEF-QA [10]LogAnswer
[3]: Passage ID 3: familiar with all existing fields of study and struggle to phrase their queries using the correct terminology. Due to the absence of datasets that map search goals expressed in layman‚Äôs terms to NLP topics, we created a synthetic multi-class dataset using GPT-3.5-Turbo (version: 0613) OpenAI (2022). We prompted the LLM to generate questions on the 12 main topics in our taxonomy using three distinct personas: a computer science student with only peripheral NLP knowledge, a businessperson with practical experience of NLP tools but minimal technical expertise, and a non-technical, non-academic individual whose technology use is limited to basic tasks. Persona-specific prompting yielded diverse inquiries in layman‚Äôs language. For example, the question ‚ÄúHow are computers able to respond when we ask them questions?‚Äù was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as ‚ÄúWho discovered the
[4]: Passage ID 4: training negatively affects mostly the recall which is the most important metric for the intended goal.Hence, assuming NL answers of very high quality are provided by a QA system, then our approach should be capable of identifying (and therefore, filtering) incorrect answers.VI-B Experiment 2: NLG of limited qualityIn this setting, we are evaluating artificially generated NL answers.There are computed automatically from SPARQL queries and the corresponding results.To our best knowledge, there is no KGQA system available that is providing an API to produce full-fledged NL answers.Consequently, it is required to generate answer verbalization from the available information (i.e.,¬†SPARQL query candidates).Generating artificial answers in a three-step process includes (1) providing a question in textual form to the KGQA system, (2) sending the computed SPARQL query answer candidates to Wikidata, and (3) generating NL representation from the obtained list of the query
[5]: Passage ID 5: ask them questions?‚Äù was generated for the topic Natural Language Interfaces. To account for questions unrelated to NLP, we also generated a set of out-of-scope questions such as ‚ÄúWho discovered the laws of thermodynamics?‚Äù Following a quality inspection of the synthetically produced questions, we assembled a training dataset of 1601 examples, consisting of 120 questions for each of the 12 topics and 161 general questions. We also derived a test dataset containing 364 examples with a balanced class distribution similar to the training dataset.In our experiments, we evaluated three classification approaches: vector similarity search, prompting a LLM (GPT-3.5-Turbo), and few-shot fine-tuning of a transformer model with the SetFit framework Tunstall et¬†al. (2022). Concerning the vector search approach with the SPECTER2 model, we measured the cosine similarity to compare vectors of embedded user queries with paper embeddings in our vector database to retrieve the 100 most similar