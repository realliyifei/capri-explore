# Narrative Why-Question Answering: A Review of Challenges and Datasets

## Question

How is the performance of free-form QA models assessed in narrative contexts using both automatic and human evaluation methods?

## URLs

1. https://ar5iv.org/html/2312.07592. [2312.07592] Evaluating ChatGPT as a Question Answering System: A Comprehensive Analysis and Comparison with Existing Models
2. https://ar5iv.org/html/2401.13170. [2401.13170] CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering
3. https://ar5iv.org/html/2402.11161. [2402.11161] \abrPANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering
4. https://ar5iv.org/html/2209.12617. [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
5. https://ar5iv.org/html/2402.06204. [2402.06204] The Generative AI Paradox in Evaluation: "What It Can Solve, It May Not Evaluate"
6. https://ar5iv.org/html/2410.00526. [2410.00526] Benchmarking Large Language Models for Conversational Question Answering in Multi-instructional Documents
7. https://ar5iv.org/html/2407.09413. [2407.09413] SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers
8. https://ar5iv.org/html/2406.11811. [2406.11811] RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content
9. https://ar5iv.org/html/2406.07935. [2406.07935] Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable NLG Evaluation

## Answer (I don't know?: ✗)

The assessment of free-form question answering (QA) models, particularly in narrative contexts, involves a combination of automatic and human evaluation methods aimed at accurately measuring the effectiveness of the generated responses.

Firstly, automatic evaluation methods typically deploy error scores based on various linguistic benchmarks, including n-gram matching and word embeddings. These techniques facilitate the quantitative measurement of a QA system's performance. For example, the use of contextual embeddings aids in comparing generated answers to reference answers systematically, allowing for an assessment grounded in mathematical frameworks ([1] [2] [3]). However, the reliance on these metrics has limitations, as they often do not fully capture the nuances of human judgment, especially in contextually rich narrative environments.

On the other hand, human evaluation methods are essential because they can incorporate qualitative aspects that automated methods might overlook. Human annotators assess how well the QA models respond to questions, particularly focusing on comprehension and contextual relevance. This approach mimics how end-users interact with information and thus offers a more accurate reflection of a model's effectiveness in real-world scenarios ([4]).

Additionally, the narrative context presents unique challenges; it often requires models to perform complex reasoning and extraction of targeted information from extensive text sources. To evaluate this aspect, hybrid approaches can be utilized. For instance, frameworks like ProxyQA are specifically designed for long-form content generation, which employs human-curated meta-questions alongside automated proxy-questions. This dual methodology allows evaluators to assess the generated narratives' quality through their accuracy in addressing the proxy-questions provided, thereby integrating human insights into the evaluation process ([5]).

In summary, effectively assessing the performance of free-form QA models in narrative contexts hinges on a balanced integration of automatic evaluation metrics that quantify performance and human evaluation techniques that interpret qualitative aspects of the generated content. The complexity and subjectivity inherent in narrative-based QA highlight the necessity of combining these methodologies to obtain a comprehensive understanding of model performance.

1. [1]:  https://ar5iv.org/html/2209.12617, [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
2. [2]:  https://ar5iv.org/html/2209.12617, [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
3. [3]:  https://ar5iv.org/html/2209.12617, [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
4. [4]:  https://ar5iv.org/html/2211.14880, No Title
5. [5]:  https://ar5iv.org/html/2401.15042, No Title
---
1. [1]:  Passage ID 1: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence
2. [2]:  Passage ID 2: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence
3. [3]:  Passage ID 3: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence
4. [4]:  Passage ID 4: experts.Our findings show that our novel approach, where humans are incorporated as early as possible in the process, boosts performance in the low-resource, domain-specific setting, allowing for low-labeling-effort question answering systems in new, specialized domains.They further demonstrate how human annotation affects the performance of QA depending on the stage it is performed.1 IntroductionMachine Reading Question Answering (MRQA) is a challenging and important problem.Facilitating targeted information extraction from documents, it allows users to get fast, easy access to a vast amount of documents available. MRQA models generally need plenty of annotations, therefore several methods have been devised for augmenting data by generating new annotated samples, with the ultimate goal of improving quality of predictions.Some of these approaches show a real benefit in the downstream MRQA task; however, there is no work focusing on employing Language Models (LM) fine-tuned
5. [5]:  Passage ID 5: their capability for generating long-form contents, such as reports and articles, has been relatively unexplored and inadequately assessed by existing benchmarks. The prevalent evaluation methods, which predominantly rely on crowdsourcing, are recognized for their labor-intensive nature and lack of efficiency, whereas automated metrics, such as the ROUGE score, demonstrate discordance with human judgment criteria. In this paper, we propose ProxyQA, an innovative framework dedicated to the assessment of long-text generation. ProxyQA comprises in-depth human-curated meta-questions spanning various domains, each accompanied by specific proxy-questions with pre-annotated answers. LLMs are tasked to generate extensive content in response to these meta-questions. By engaging an evaluator and incorporating the generated texts as contextual background, ProxyQA assesses the generated content’s quality through the evaluator’s accuracy on addressing the proxy-questions.We examine multiple LLMs,