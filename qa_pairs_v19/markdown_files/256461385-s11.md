# Narrative Why-Question Answering: A Review of Challenges and Datasets

## Question

How is the performance of free-form QA models assessed in narrative contexts using both automatic and human evaluation methods?

## URLs

1. https://ar5iv.org/html/2312.07592. [2312.07592] Evaluating ChatGPT as a Question Answering System: A Comprehensive Analysis and Comparison with Existing Models
2. https://ar5iv.org/html/2401.13170. [2401.13170] CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering
3. https://ar5iv.org/html/2402.11161. [2402.11161] \abrPANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering
4. https://ar5iv.org/html/2209.12617. [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
5. https://ar5iv.org/html/2402.06204. [2402.06204] The Generative AI Paradox in Evaluation: "What It Can Solve, It May Not Evaluate"
6. https://ar5iv.org/html/2410.00526. [2410.00526] Benchmarking Large Language Models for Conversational Question Answering in Multi-instructional Documents
7. https://ar5iv.org/html/2407.09413. [2407.09413] SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers
8. https://ar5iv.org/html/2406.11811. [2406.11811] RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content
9. https://ar5iv.org/html/2406.07935. [2406.07935] Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable NLG Evaluation

## Answer (I don't know?: âœ—)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable