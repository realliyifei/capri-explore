# Narrative Why-Question Answering: A Review of Challenges and Datasets

## Question

How is the performance of free-form QA models assessed in narrative contexts using both automatic and human evaluation methods?

## URLs

1. https://ar5iv.org/html/2312.07592
2. https://ar5iv.org/html/2401.13170
3. https://ar5iv.org/html/2402.11161
4. https://ar5iv.org/html/2209.12617
5. https://ar5iv.org/html/2402.06204
6. https://ar5iv.org/html/2410.00526
7. https://ar5iv.org/html/2407.09413
8. https://ar5iv.org/html/2406.11811
9. https://ar5iv.org/html/2406.07935

## Answer

## I don't know?: False

The performance of free-form Question Answering (QA) models in narrative contexts can be assessed using both automatic evaluation methods and human evaluation approaches.

**Automatic Evaluation Methods**: These methods typically leverage various metrics to quantitatively assess the performance of QA models. Common metrics include n-gram matching and embedding-based comparisons (e.g., word and contextual embeddings) that measure how well a model‚Äôs generated answers align with reference answers. Specifically, evaluation scores, such as exact match (EM) and F1, are utilized in the automated evaluation of models to provide a quantitative basis for comparison [1] [2]. The use of benchmarks and datasets collected from real-world sources, such as Ask Ubuntu, Super User, and Stack Overflow, facilitates a robust evaluation process for quantifying model performance across different datasets [3] [5]. 

For instance, in one study, the authors reported that their proposed model outperformed several state-of-the-art baselines in automatic evaluations, showing a significant margin in results derived from comprehensive tests across various datasets [3] [5]. These automatic metrics are crucial because they allow for consistent and repeatable evaluations of different QA systems, even though they may not fully capture the nuances of human judgment [1].

**Human Evaluation Methods**: While automatic metrics are valuable, they often fail to encapsulate the quality of interaction between the question asker and potential helpers. Human evaluation methods involve domain experts or users who provide qualitative feedback on the relevance and effectiveness of the answers generated by QA models. For example, a study involved soliciting feedback from five domain experts to evaluate the generated clarifying questions and answers of a proposed model. Their insights were integral in determining the usefulness of the interaction quality within the QA system [3]. 

Human evaluations are typically used to complement and validate the findings from automatic assessments. This dual approach ensures that the contextual relevance and interaction dynamics, often overlooked in quantitative measures, are adequately addressed. For example, human studies may assess whether the model effectively addresses unanswered or unresolved questions by recommending relevant solutions based on historical data [5]. 

In conclusion, assessing the performance of free-form QA models in narrative contexts requires a multi-faceted approach that combines robust automatic evaluation techniques with qualitative human judgment to ensure a comprehensive understanding of a model's effectiveness in real-world applications. This mixed-method strategy provides a holistic view of QA system performance, capturing both the precision of the output and its alignment with user needs.

1. [1]:  https://ar5iv.org/html/2209.12617, [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
2. [2]:  https://ar5iv.org/html/2209.12617, [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
3. [3]:  https://ar5iv.org/html/2210.15846, No Title
4. [4]:  https://ar5iv.org/html/2401.13170, [2401.13170] CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering
5. [5]:  https://ar5iv.org/html/2210.15846, No Title
---
1. [1]:  Passage ID 1: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence
2. [2]:  Passage ID 2: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence
3. [3]:  Passage ID 3: Post on AskubuntuTo evaluate the performance of our proposed approach, we conducted comprehensive experiments with four datasets, collected from the technical Q&A sites Ask Ubuntu, Super User and Stack Overflow respectively.The large-scale automatic evaluation results suggest that our model outperforms a collection of state-of-the-art baselines by a large margin. For human evaluation, we asked 5 domain experts for their feedback on our generated clarifying questions and answers. Our user study results further demonstrate the effectiveness and superiority of our approach in solving unanswered/unresolved questions. In summary, this paper makes the following contributions:‚Ä¢Previous studies neglect the value of interactions between the question asker and the potential helper. We argue that a clarifying question between the question and answers is an important aspect of judging the relevance and usefulness of the QA pair.Therefore, we train a sequence-to-sequence model to
4. [4]:  Passage ID 4: machine comprehension and question-answering (QA) capabilities of artificial intelligence systems¬†Yang et¬†al. (2018), where each question is encompassed with a long context.4.2 Evaluation Method EvaluationCandidate Answer Generationsqa models have quite diverse performance with different model sizes. We select Flan-T5-xl (3B)¬†Chung et¬†al. (2022), LLaMA 2 (7B)¬†Touvron et¬†al. (2023), gpt-3.5-turbo (black-box model) as our qa models and generate answers for the three benchmark datasets. We concatenate the contexts with the questions if available. 666https://openai.comAnswer Set ExpansionWe use pywikibot¬†Wikidata Contributors (2019) to crawl entity aliases for the reference and candidate answers for all four datasets if they are available. We save the original test sets and the expanded test datasets that contain a list of reference and candidate answers with aliases.Automated Evaluation Metric ResultsWe use em, F1subscriptùêπ1F_{1} matching, bert, improved bert
5. [5]:  Passage ID 5: evaluate the performance of our proposed model, we conducted a large scale evaluation on four datasets, collected from the real world technical Q&A sites (i.e., Ask Ubuntu, Super User, Stack Overflow Python andStack Overflow Java). Our experimental results show that our approach significantly outperforms several state-of-the-art baselines in automatic evaluation. We also conducted a user study with 50 solved/unanswered/unresolved questions. The user study results demonstrate that our approach is effective in solving the answer hungry problem by recommending the most relevant answers from historical archives.CQA, Question Boosting, Question Answering, Sequence-to-sequence, Deep Neural Network, Weakly Supervised Learning‚Ä†‚Ä†journal: TOSEM‚Ä†‚Ä†journalvolume: 9‚Ä†‚Ä†journalnumber: 4‚Ä†‚Ä†article: 39‚Ä†‚Ä†journalyear: 2019‚Ä†‚Ä†publicationmonth: 3‚Ä†‚Ä†articleseq: 9‚Ä†‚Ä†copyright: acmlicensed‚Ä†‚Ä†doi: 0000001.0000001‚Ä†‚Ä†ccs: Software and its engineering¬†Software evolution‚Ä†‚Ä†ccs: Software and its