# Narrative Why-Question Answering: A Review of Challenges and Datasets

## Question

How is the performance of free-form QA models assessed in narrative contexts using both automatic and human evaluation methods?

## URLs

1. https://ar5iv.org/html/2312.07592. [2312.07592] Evaluating ChatGPT as a Question Answering System: A Comprehensive Analysis and Comparison with Existing Models
2. https://ar5iv.org/html/2401.13170. [2401.13170] CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering
3. https://ar5iv.org/html/2402.11161. [2402.11161] \abrPANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering
4. https://ar5iv.org/html/2209.12617. [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
5. https://ar5iv.org/html/2402.06204. [2402.06204] The Generative AI Paradox in Evaluation: "What It Can Solve, It May Not Evaluate"
6. https://ar5iv.org/html/2410.00526. [2410.00526] Benchmarking Large Language Models for Conversational Question Answering in Multi-instructional Documents
7. https://ar5iv.org/html/2407.09413. [2407.09413] SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers
8. https://ar5iv.org/html/2406.11811. [2406.11811] RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content
9. https://ar5iv.org/html/2406.07935. [2406.07935] Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable NLG Evaluation

## Answer (I don't know?: ‚úó)

The performance of free-form Question Answering (QA) models in narrative contexts is assessed through a combination of automatic evaluation methods and human evaluation techniques. 

Automatic evaluation methods for QA systems typically employ metrics such as Exact Match (EM), F1 score, and contextual embeddings metrics like BERT scores. These metrics allow for a quantitative assessment of the model's output against reference answers. In particular, the research highlights that several QA models, such as Flan-T5-xl and LLaMA 2, are evaluated based on their performance across benchmark datasets, which are crucial for determining the reliability and accuracy of the answers generated by these systems [2].

However, automatic methods have limitations, primarily due to their rigid nature, which might not fully capture the nuances of human judgment. This limitation leads to the exploration of more human-like automated evaluation methods. These new methodologies aim to align the assessment of QA systems with human judgment, thereby allowing for more nuanced and personalized answer generation. The transition to these advanced evaluation frameworks seeks to improve how QA models are trained and selected, focusing on not just delivering specific answers but also more diverse and contextually relevant responses to questions [5].

Human evaluation complements the automatic evaluation process and focuses on judging the correctness of produced answers based on more subjective criteria. Human evaluators are often brought in to assess the quality of the answers from a usability perspective, addressing factors such as coherence, relevance, and overall satisfaction with the responses. This evaluation is particularly important in narrative contexts, where the complexity of questions and required answers can vary significantly, and where nuanced understanding plays a crucial role [4].

Overall, a robust assessment of free-form QA models in narrative contexts involves a dual framework combining automated metrics for quantitative analysis and human evaluations for qualitative insights, aiming to enhance both the accuracy of the models and the human experience in interpreting answer quality [5]. Thus, to effectively evaluate these models, researchers advocate for a synergy between traditional automated evaluation scores and innovative human-like assessment frameworks that aim to provide a more comprehensive evaluation of QA systems' performance in varied contexts [3] [4].

1. [1]:  https://ar5iv.org/html/2209.12617, [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
2. [2]:  https://ar5iv.org/html/2401.13170, [2401.13170] CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering
3. [3]:  https://ar5iv.org/html/2209.12617, [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
4. [4]:  https://ar5iv.org/html/2209.12617, [2209.12617] Evaluation of Question Answering Systems: Complexity of judging a natural language
5. [5]:  https://ar5iv.org/html/2401.13170, [2401.13170] CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering
---
1. [1]:  Passage ID 1: or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.keywords: Natural Language Processing, Question Answering, Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence
2. [2]:  Passage ID 2: machine comprehension and question-answering (QA) capabilities of artificial intelligence systems¬†Yang et¬†al. (2018), where each question is encompassed with a long context.4.2 Evaluation Method EvaluationCandidate Answer Generationsqa models have quite diverse performance with different model sizes. We select Flan-T5-xl (3B)¬†Chung et¬†al. (2022), LLaMA 2 (7B)¬†Touvron et¬†al. (2023), gpt-3.5-turbo (black-box model) as our qa models and generate answers for the three benchmark datasets. We concatenate the contexts with the questions if available. 666https://openai.comAnswer Set ExpansionWe use pywikibot¬†Wikidata Contributors (2019) to crawl entity aliases for the reference and candidate answers for all four datasets if they are available. We save the original test sets and the expanded test datasets that contain a list of reference and candidate answers with aliases.Automated Evaluation Metric ResultsWe use em, F1subscriptùêπ1F_{1} matching, bert, improved bert
3. [3]:  Passage ID 3: Knowledge Sources, Deep learning, Machine Learning, Neural-based Techniques, Evaluation Scores1 IntroductionNatural language processing (NLP) is an important branch of artificial intelligence (AI) concerned with text understanding and text generation. The former subject is studied in the sub-branch natural language understanding (NLU) 1; 2; 3 and the latter in natural language generation (NLG) 4; 5. Over the years, both fields, i.e., NLU and NLG developed enormously with an extensive literature which requires nowadays a dedicated discussion of specialized subtasks when presenting approaches or methods thereof despite the fact that a systems understanding of NLP can only be achieved holistically.In this paper, we focus on subtasks of NLP centered around question answering (QA). The task of a QA system is to find an answer (output) in the form of a natural language for a given question (input) usually presented in form of a sentence. While our focus is on QA systems and their
4. [4]:  Passage ID 4: The task of a QA system is to find an answer (output) in the form of a natural language for a given question (input) usually presented in form of a sentence. While our focus is on QA systems and their quantitative evaluations, we discuss also evaluation scores introduced in related fields. The reason for this is that machine translation (MT), text summarization (TS) and dialogue systems (DS) (also called conversational agents) also have the problem of evaluating various forms of input-output text pairs. Hence, despite the fact that there are considerable differences between question answering, machine translation, text summarization, and dialogue systems several evaluation scores are typically used across those tasks.For QA many methods and techniques have been introduced varying from simple rule-based systems to advanced and complex machine learning techniques 8; 9; 10; 11. In recent years, deep neural network-based approaches to realizing different forms of data-driven
5. [5]:  Passage ID 5: guidelines) still hold us from solving qa and buildingmore human preferable qa models that are not just trained toprovide very specific answers to a question, but also more diverse andpersonalized answers. The way to build and select a more intelligentand creative qa model is by improving qa evaluationmethods, where machines are judged not merely by rigid automatedevaluation methods, but with more human-like automated evaluationmethods. More human-like automated evaluation methods can scale uptraining, while not requiring humans to individually look at all theproduced answers. By writing up a basic framework derived fromqa human experts and adopt it to machine qa paradigm, weare able to enhance and generalize current automated qaevaluation methods that are more aligned with human judgments. Inaddition, our ae framework also improve human annotators‚Äôexperience in judging the correctness of answers. Future work canexplore automated qa evaluations that can also handle and