# Measure and Improve Robustness in NLP Models: A Survey

## Question

What distinguishes robustness under distribution shift from domain adaptation and transfer learning in NLP?

## URLs

1. https://ar5iv.org/html/2112.08313. [2112.08313] Measure and Improve Robustness in NLP Models: A Survey
2. https://ar5iv.org/html/2404.04452. [2404.04452] Vision Transformers in Domain Adaptation and Generalization: A Study of Robustness
3. https://ar5iv.org/html/1901.05335. [1901.05335] A review of domain adaptation without target labels
4. https://ar5iv.org/html/2404.00897. [2404.00897] Chapter 0 Machine Learning Robustness: A Primer
5. https://ar5iv.org/html/2405.12217. [2405.12217] Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning
6. https://ar5iv.org/html/2210.12517. [2210.12517] Exploring The Landscape of Distributional Robustness for Question Answering Models
7. https://ar5iv.org/html/2103.03097. [2103.03097] Generalizing to Unseen Domains: A Survey on Domain Generalization
8. https://ar5iv.org/html/1903.04687. [1903.04687] Transfer Adaptation Learning: A Decade Survey
9. https://ar5iv.org/html/2403.00935. [2403.00935] Transfer Learning for Security: Challenges and Future Directions

## Answer (I don't know?: âœ—)

In the field of Natural Language Processing (NLP), robustness under distribution shift, domain adaptation, and transfer learning are related concepts that address how models perform when faced with varying data distributions. However, they differ in focus and methodology.

1. **Robustness Under Distribution Shift**: This refers to the ability of an NLP model to maintain its performance when the statistical properties of the input data change between training and testing. Specifically, robustness is concerned with how well a model can generalize to data that it has not seen before, particularly when there is a shift in distribution that can occur naturally (e.g., different contexts or styles of text) or artificially (e.g., adversarial attacks). Research indicates that NLP models often struggle with this because they may rely on spurious correlations between input features and labelsâ€”correlations that do not hold in novel contexts [1] [3]. The evaluation of robustness often requires robust benchmarks that measure performance across various distribution shifts rather than solely relying on in-distribution accuracy [1][4].

2. **Domain Adaptation**: This is a specific subfield concerned with adapting a model trained on one domain (source domain) to perform effectively in another, potentially different domain (target domain). It typically involves techniques that help bridge the gap between the source and target domains, allowing models to transfer knowledge effectively. Domain adaptation focuses explicitly on the distributional differences and aims to minimize the performance drop when the model is applied to a different domain by fine-tuning on data from the target domain [2]. 

3. **Transfer Learning**: Transfer learning is a broader concept where knowledge gained while solving one problem is applied to a different but related problem. In NLP, pre-trained models (like BERT or GPT) are fine-tuned on specific tasks such as sentiment analysis or question answering. Transfer learning often requires less data compared to training a model from scratch because the model leverages prior knowledge acquired during initial training [2]. However, it may not specifically address the performance drop that can occur due to distribution shifts unless the model is also fine-tuned appropriately on the target domain data [5].

In summary, while robustness under distribution shift focuses on maintaining model performance across variations in data distribution, domain adaptation specifically addresses the challenges posed by applying a model from one domain to another. In contrast, transfer learning utilizes previously acquired knowledge to adapt to new tasks, but it may not inherently tackle the issues of robustness to distribution shifts unless combined with domain adaptation techniques. Understanding these distinctions is vital for developing effective NLP models that can perform reliably across diverse and changing contexts [3][5].

1. [1]:  https://ar5iv.org/html/2210.12517, [2210.12517] Exploring The Landscape of Distributional Robustness for Question Answering Models
2. [2]:  https://ar5iv.org/html/2112.08313, [2112.08313] Measure and Improve Robustness in NLP Models: A Survey
3. [3]:  https://ar5iv.org/html/2112.08313, [2112.08313] Measure and Improve Robustness in NLP Models: A Survey
4. [4]:  https://ar5iv.org/html/2112.08313, [2112.08313] Measure and Improve Robustness in NLP Models: A Survey
5. [5]:  https://ar5iv.org/html/2210.12517, [2210.12517] Exploring The Landscape of Distributional Robustness for Question Answering Models
---
1. [1]:  Passage ID 1: 2020; Hendrycks etÂ al., 2020; Gardner etÂ al., 2020; Arora etÂ al., 2021; Veitch etÂ al., 2021; Goel etÂ al., 2021; Miller etÂ al., 2020, inter alia), the community has not yet adopted a common set of best practices for evaluating robustness.As a result, new methods often do not evaluate on comparable or even any robustness test sets, which makes it challenging to understand which methods generalize more reliably and whether NLP is making progress on robustness to distribution shift.To address this challenge and shed light on the robustness landscape in NLP, we conduct a large empirical evaluation of distributional robustness in question answering (QA).Building on recent research on robustness in computer vision Taori etÂ al. (2020); Miller etÂ al. (2021a), we focus on distribution shifts that arise between two related but different test sets.These distribution shifts are sometimes called dataset shift to distinguish them from other kinds of distribution shift.An example of dataset
2. [2]:  Passage ID 2: (2018a); Rudinger etÂ al. (2017), occupation classification De-Arteaga etÂ al. (2019), and neural machine translation Prates etÂ al. (2019); Font andCosta-jussÃ  (2019).2.3 Connections and A Common ThemeThe above two categories of robustness can be unified under the same framework, i.e., whether ğ’Ÿâ€²superscriptğ’Ÿâ€²\mathcal{D}^{\prime} represents a synthetic distribution shift (via adversarial attacks) or a natural distribution shift.Existing work has shown a modelâ€™s performance might degrade substantially in both cases, but the transferability of the two categories is relatively under-explored.In the vision domain, Taori etÂ al. (2020) investigate modelsâ€™ robustness to natural distribution shift, and show that robustness to synthetic distribution shift might offer little to no robustness improvement under natural distribution shift.Some studies show NLP models might not generalize to unseen adversarial patterns Huang etÂ al. (2020); Jha etÂ al. (2020); Joshi and He (2021), but more
3. [3]:  Passage ID 3: improvement under natural distribution shift.Some studies show NLP models might not generalize to unseen adversarial patterns Huang etÂ al. (2020); Jha etÂ al. (2020); Joshi and He (2021), but more work is needed to systematically bridge the gap between NLP modelsâ€™ robustness under natural and synthetic distribution shifts.To better understand why models exhibit a lack of robustness, some existing work attributed this to the fact that models sometimes utilize spurious correlations between input features and labels, rather than the genuine ones, where spurious features are commonly defined as features that do not causally affect a taskâ€™s label Srivastava etÂ al. (2020); Wang and Culotta (2020b): they correlate with task labels but fail to transfer to more challenging test conditions or out-of-distribution data Geirhos etÂ al. (2020).Some other work defined it as â€œprediction rules that work for the majority examplesbut do not hold in generalâ€ Tu etÂ al. (2020).Such spurious
4. [4]:  Passage ID 4: by giving lower uncertainty estimates over out-of-distribution data.This is another example where models can be less robust under distributional shifts, and again emphasizes the need of building more unified benchmarks to measure a modelâ€™s performance (e.g., robust accuracy, calibration, stability) under distribution shifts, in addition to in-distribution accuracy.3 Robustness in Vision vs. in NLPDespite the widely study of robustness in vision,the study of robustness in NLP cannot always directly borrow the ideas.We categorize the main differences with the three followingpoints:Continuous vs. Discrete in Search SpaceThe most obvious characteristic is probably the discrete nature of the space of text.This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP Lei etÂ al. (2019); Zhang etÂ al. (2020c),in the sense that simple gradient-based adversarial attacks will not directly translate to
5. [5]:  Passage ID 5: affect robustness and in-distribution performance alone determines out-of-distribution performance.Moreover, our findings indicate thati) zero-shot and in-context learning methods are more robust to distribution shifts than fully fine-tuned models;ii) few-shot prompt fine-tuned models exhibit better robustness than few-shot fine-tuned span prediction models;iii) parameter-efficient and robustness enhancing training methods provide no significant robustness improvements.In addition, we publicly release all evaluations to encourage researchers to further analyze robustness trends for question answering models.Exploring The Landscape of Distributional Robustness for Question Answering ModelsAnas Awadalla1Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Mitchell Wortsman1Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Gabriel Ilharco1Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Sewon Min1Ian Magnusson2Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Hannaneh Hajishirzi1,2Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Ludwig Schmidt1,200footnotetext: