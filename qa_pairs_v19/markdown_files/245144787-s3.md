# Measure and Improve Robustness in NLP Models: A Survey

## Question

What distinguishes robustness under distribution shift from domain adaptation and transfer learning in NLP?

## URLs

1. https://ar5iv.org/html/2112.08313
2. https://ar5iv.org/html/2404.04452
3. https://ar5iv.org/html/1901.05335
4. https://ar5iv.org/html/2404.00897
5. https://ar5iv.org/html/2405.12217
6. https://ar5iv.org/html/2210.12517
7. https://ar5iv.org/html/2103.03097
8. https://ar5iv.org/html/1903.04687
9. https://ar5iv.org/html/2403.00935

## Answer

## I don't know?: False

Robustness under distribution shift in Natural Language Processing (NLP) refers to the model's ability to maintain performance when faced with data that has a different distribution from what it was trained on. This phenomenon is characterized by how model performance degrades in response to changes in data distribution, which can occur through both synthetic (e.g., adversarial attacks) and natural shifts [2][3]. Specifically, robustness can be evaluated by examining a model's performance accuracy in out-of-distribution scenarios, where unexpected data patterns might lead to failures in inference [1][2].

In contrast, domain adaptation and transfer learning are techniques aimed at enhancing the model's capability to generalize to new tasks or domains by leveraging knowledge from source domains. Domain adaptation focuses on adjusting models trained on a source domain so they can perform well on a target domain, often with similar but not identical distributions [2]. Transfer learning, while encompassing domain adaptation, generally refers to using knowledge acquired from one task (the source) to improve performance on a different but related task (the target) [4].

The distinction lies in the intention and methodology: robustness under distribution shift emphasizes performance stability in various data shifts without necessarily adapting the model, while domain adaptation and transfer learning involve proactively modifying the model to handle new domains or tasks. Robustness is concerned with model reliability under unforeseen variations in input data, highlighting the need for comprehensive benchmarks that assess performance consistency [4][5]. 

Furthermore, existing studies suggest that models might not generalize well when exposed to unseen adversarial patterns, which complicates both robustness and the processes of adaptation and transfer [3][4]. This indicates a crucial need for ongoing research to better understand these dynamics, as well as the spurious correlations within the data that may lead to performance drops during unseen conditions [3]. 

Ultimately, the emphasis on robust performance amidst data distribution changes underscores a foundational challenge in NLP, necessitating further exploration of unified frameworks that could effectively integrate concepts from robustness and adaptation frameworks [5].

[1]: https://ar5iv.org/html/2303.00164, No Title
[2]: https://ar5iv.org/html/2112.08313, [2112.08313] Measure and Improve Robustness in NLP Models: A Survey
[3]: https://ar5iv.org/html/2112.08313, [2112.08313] Measure and Improve Robustness in NLP Models: A Survey
[4]: https://ar5iv.org/html/2112.08313, [2112.08313] Measure and Improve Robustness in NLP Models: A Survey
[5]: https://ar5iv.org/html/2112.08313, [2112.08313] Measure and Improve Robustness in NLP Models: A Survey

[1]: Passage ID 1: such as models for tasks in natural language inference¬†(Naik et¬†al., 2018), question answering¬†(Guptaet¬†al., 2021; Milleret¬†al., 2020), and speech recognition¬†(Leeet¬†al., 2018).NLP robustness can be defined as understanding how model performance changes when testing on a new dataset, which has a different distribution from the dataset the model is trained on¬†(Wanget¬†al., 2021).In practice, users‚Äô real world interactions with voice assistants could differ from data used in development, which mimics the data distribution shift in NLP robustness research.Such data distribution shifts are shown to lead to model failures.In the case of question answering, state-of-art models perform nearly at human-level for reading comprehension on standard benchmarks collected from Wikipedia¬†(Rajpurkar et¬†al., 2016).However,Milleret¬†al. (2020) found that model performance drops when the question answering model is evaluated on different topic domains, such as New York Times articles, Reddit
[2]: Passage ID 2: (2018a); Rudinger et¬†al. (2017), occupation classification De-Arteaga et¬†al. (2019), and neural machine translation Prates et¬†al. (2019); Font andCosta-juss√† (2019).2.3 Connections and A Common ThemeThe above two categories of robustness can be unified under the same framework, i.e., whether ùíü‚Ä≤superscriptùíü‚Ä≤\mathcal{D}^{\prime} represents a synthetic distribution shift (via adversarial attacks) or a natural distribution shift.Existing work has shown a model‚Äôs performance might degrade substantially in both cases, but the transferability of the two categories is relatively under-explored.In the vision domain, Taori et¬†al. (2020) investigate models‚Äô robustness to natural distribution shift, and show that robustness to synthetic distribution shift might offer little to no robustness improvement under natural distribution shift.Some studies show NLP models might not generalize to unseen adversarial patterns Huang et¬†al. (2020); Jha et¬†al. (2020); Joshi and He (2021), but more
[3]: Passage ID 3: improvement under natural distribution shift.Some studies show NLP models might not generalize to unseen adversarial patterns Huang et¬†al. (2020); Jha et¬†al. (2020); Joshi and He (2021), but more work is needed to systematically bridge the gap between NLP models‚Äô robustness under natural and synthetic distribution shifts.To better understand why models exhibit a lack of robustness, some existing work attributed this to the fact that models sometimes utilize spurious correlations between input features and labels, rather than the genuine ones, where spurious features are commonly defined as features that do not causally affect a task‚Äôs label Srivastava et¬†al. (2020); Wang and Culotta (2020b): they correlate with task labels but fail to transfer to more challenging test conditions or out-of-distribution data Geirhos et¬†al. (2020).Some other work defined it as ‚Äúprediction rules that work for the majority examplesbut do not hold in general‚Äù Tu et¬†al. (2020).Such spurious
[4]: Passage ID 4: by giving lower uncertainty estimates over out-of-distribution data.This is another example where models can be less robust under distributional shifts, and again emphasizes the need of building more unified benchmarks to measure a model‚Äôs performance (e.g., robust accuracy, calibration, stability) under distribution shifts, in addition to in-distribution accuracy.3 Robustness in Vision vs. in NLPDespite the widely study of robustness in vision,the study of robustness in NLP cannot always directly borrow the ideas.We categorize the main differences with the three followingpoints:Continuous vs. Discrete in Search SpaceThe most obvious characteristic is probably the discrete nature of the space of text.This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP Lei et¬†al. (2019); Zhang et¬†al. (2020c),in the sense that simple gradient-based adversarial attacks will not directly translate to
[5]: Passage ID 5: Linguistic Generalization and NLP GeneralizationLinzen (2020) argue NLP models should behave more like humans to achieve better generalization consistently.It is interesting to note that how humans process information in NLP tasks exactly is still under exploration, and to what extent models should leverage human-knowledge is still a debatable topic.111http://www.incompleteideas.net/IncIdeas/BitterLesson.html Nonetheless, if we can better understand and utilize the robustness properties in human perception, we can potentially advance models‚Äô robustness in a more meaningful way.7 ConclusionIn this paper, we provided a unifying overview over robustness definitions, evaluations and mitigation strategies in the NLP domain.We also highlighted open challenges in this area to motivate future research, encouraging people to think deeply about more comprehensive benchmarks, transferability and validity of adversarial examples, unified framework to evaluate and improve robustness,