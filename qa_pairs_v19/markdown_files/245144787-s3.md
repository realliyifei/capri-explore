# Measure and Improve Robustness in NLP Models: A Survey

## Question

What distinguishes robustness under distribution shift from domain adaptation and transfer learning in NLP?

## URLs

1. https://ar5iv.org/html/2112.08313. [2112.08313] Measure and Improve Robustness in NLP Models: A Survey
2. https://ar5iv.org/html/2404.04452. [2404.04452] Vision Transformers in Domain Adaptation and Generalization: A Study of Robustness
3. https://ar5iv.org/html/1901.05335. [1901.05335] A review of domain adaptation without target labels
4. https://ar5iv.org/html/2404.00897. [2404.00897] Chapter 0 Machine Learning Robustness: A Primer
5. https://ar5iv.org/html/2405.12217. [2405.12217] Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning
6. https://ar5iv.org/html/2210.12517. [2210.12517] Exploring The Landscape of Distributional Robustness for Question Answering Models
7. https://ar5iv.org/html/2103.03097. [2103.03097] Generalizing to Unseen Domains: A Survey on Domain Generalization
8. https://ar5iv.org/html/1903.04687. [1903.04687] Transfer Adaptation Learning: A Decade Survey
9. https://ar5iv.org/html/2403.00935. [2403.00935] Transfer Learning for Security: Challenges and Future Directions

## Answer (I don't know?: âœ—)

Robustness under distribution shift, domain adaptation, and transfer learning are interconnected concepts in Natural Language Processing (NLP), but they serve distinct purposes and are governed by different principles.

Robustness under distribution shift primarily concerns the ability of NLP models to maintain performance when faced with changes in the data distribution between training and testing sets. This encompasses scenarios where models encounter variations in input that differ from their training data, reflecting real-world shifts that may occur across datasets [1]. For instance, models may struggle to generalize when spurious correlations within the training data do not hold true in out-of-distribution data, limiting their robustness [3]. The focus on robustness requires ongoing evaluation against various distributional shifts, which are often categorized as either synthetic (e.g., adversarial attacks) or natural [2][5].

In contrast, domain adaptation aims to improve a model's performance on a target domain that may differ from the source domain where the model was trained. The primary goal is to minimize the gap in performance between these domains, typically through specialized strategies that enhance the model's generalizability to new contexts [2]. Domain adaptation often leverages labeled data from the target domain to adjust the model's parameters or employs techniques like fine-tuning to align the model more closely with the characteristics of the target data.

Transfer learning, involving the application of knowledge gained from one task to improve learning in another related task, also plays a vital role in this spectrum. In NLP, pre-trained models (e.g., BERT, GPT) are often fine-tuned on specific tasks, allowing them to leverage learned representations that can lead to improved performance across various domains [2]. While transfer learning can bolster robustness, its focus is less on maintaining performance against distributional shifts and more on effectively utilizing prior knowledge to handle related tasks.

The essential distinction lies in the focus of each approach: robustness under distribution shift is primarily about maintaining performance in the face of unseen variations, domain adaptation resolves performance discrepancies across different but related datasets, and transfer learning utilizes prior knowledge to benefit new tasks. Research in NLP emphasizes the need for unified benchmarks to evaluate robustness adequately and investigate how best practices can be adopted across these different paradigms to enhance model performance [4][5].

In summary, while robustness, domain adaptation, and transfer learning interrelate in enabling models to effectively respond to varying data conditions, their core objectives and methodologies differ, requiring tailored approaches and evaluation frameworks in NLP research [1][5].

1. [1]:  https://ar5iv.org/html/2210.12517, [2210.12517] Exploring The Landscape of Distributional Robustness for Question Answering Models
2. [2]:  https://ar5iv.org/html/2112.08313, [2112.08313] Measure and Improve Robustness in NLP Models: A Survey
3. [3]:  https://ar5iv.org/html/2112.08313, [2112.08313] Measure and Improve Robustness in NLP Models: A Survey
4. [4]:  https://ar5iv.org/html/2112.08313, [2112.08313] Measure and Improve Robustness in NLP Models: A Survey
5. [5]:  https://ar5iv.org/html/2210.12517, [2210.12517] Exploring The Landscape of Distributional Robustness for Question Answering Models
---
1. [1]:  Passage ID 1: 2020; Hendrycks etÂ al., 2020; Gardner etÂ al., 2020; Arora etÂ al., 2021; Veitch etÂ al., 2021; Goel etÂ al., 2021; Miller etÂ al., 2020, inter alia), the community has not yet adopted a common set of best practices for evaluating robustness.As a result, new methods often do not evaluate on comparable or even any robustness test sets, which makes it challenging to understand which methods generalize more reliably and whether NLP is making progress on robustness to distribution shift.To address this challenge and shed light on the robustness landscape in NLP, we conduct a large empirical evaluation of distributional robustness in question answering (QA).Building on recent research on robustness in computer vision Taori etÂ al. (2020); Miller etÂ al. (2021a), we focus on distribution shifts that arise between two related but different test sets.These distribution shifts are sometimes called dataset shift to distinguish them from other kinds of distribution shift.An example of dataset
2. [2]:  Passage ID 2: (2018a); Rudinger etÂ al. (2017), occupation classification De-Arteaga etÂ al. (2019), and neural machine translation Prates etÂ al. (2019); Font andCosta-jussÃ  (2019).2.3 Connections and A Common ThemeThe above two categories of robustness can be unified under the same framework, i.e., whether ğ’Ÿâ€²superscriptğ’Ÿâ€²\mathcal{D}^{\prime} represents a synthetic distribution shift (via adversarial attacks) or a natural distribution shift.Existing work has shown a modelâ€™s performance might degrade substantially in both cases, but the transferability of the two categories is relatively under-explored.In the vision domain, Taori etÂ al. (2020) investigate modelsâ€™ robustness to natural distribution shift, and show that robustness to synthetic distribution shift might offer little to no robustness improvement under natural distribution shift.Some studies show NLP models might not generalize to unseen adversarial patterns Huang etÂ al. (2020); Jha etÂ al. (2020); Joshi and He (2021), but more
3. [3]:  Passage ID 3: improvement under natural distribution shift.Some studies show NLP models might not generalize to unseen adversarial patterns Huang etÂ al. (2020); Jha etÂ al. (2020); Joshi and He (2021), but more work is needed to systematically bridge the gap between NLP modelsâ€™ robustness under natural and synthetic distribution shifts.To better understand why models exhibit a lack of robustness, some existing work attributed this to the fact that models sometimes utilize spurious correlations between input features and labels, rather than the genuine ones, where spurious features are commonly defined as features that do not causally affect a taskâ€™s label Srivastava etÂ al. (2020); Wang and Culotta (2020b): they correlate with task labels but fail to transfer to more challenging test conditions or out-of-distribution data Geirhos etÂ al. (2020).Some other work defined it as â€œprediction rules that work for the majority examplesbut do not hold in generalâ€ Tu etÂ al. (2020).Such spurious
4. [4]:  Passage ID 4: by giving lower uncertainty estimates over out-of-distribution data.This is another example where models can be less robust under distributional shifts, and again emphasizes the need of building more unified benchmarks to measure a modelâ€™s performance (e.g., robust accuracy, calibration, stability) under distribution shifts, in addition to in-distribution accuracy.3 Robustness in Vision vs. in NLPDespite the widely study of robustness in vision,the study of robustness in NLP cannot always directly borrow the ideas.We categorize the main differences with the three followingpoints:Continuous vs. Discrete in Search SpaceThe most obvious characteristic is probably the discrete nature of the space of text.This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP Lei etÂ al. (2019); Zhang etÂ al. (2020c),in the sense that simple gradient-based adversarial attacks will not directly translate to
5. [5]:  Passage ID 5: affect robustness and in-distribution performance alone determines out-of-distribution performance.Moreover, our findings indicate thati) zero-shot and in-context learning methods are more robust to distribution shifts than fully fine-tuned models;ii) few-shot prompt fine-tuned models exhibit better robustness than few-shot fine-tuned span prediction models;iii) parameter-efficient and robustness enhancing training methods provide no significant robustness improvements.In addition, we publicly release all evaluations to encourage researchers to further analyze robustness trends for question answering models.Exploring The Landscape of Distributional Robustness for Question Answering ModelsAnas Awadalla1Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Mitchell Wortsman1Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Gabriel Ilharco1Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Sewon Min1Ian Magnusson2Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Hannaneh Hajishirzi1,2Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Ludwig Schmidt1,200footnotetext: