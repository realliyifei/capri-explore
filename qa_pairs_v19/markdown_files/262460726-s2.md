# A Practical Survey on Zero-shot Prompt Design for In-context Learning

## Question

What methods have been proposed for designing discrete prompts in BERT models, and how do these methods work?

## URLs

1. https://ar5iv.org/html/2411.00985. [2411.00985] FedDTPT: Federated Discrete and Transferable Prompt Tuning for Black-Box Large Language Models
2. https://ar5iv.org/html/2404.01077. [2404.01077] Efficient Prompting Methods for Large Language Models: A Survey
3. https://ar5iv.org/html/2309.13205. [2309.13205] A Practical Survey on Zero-shot Prompt Design for In-context Learning
4. https://ar5iv.org/html/2308.08758. [2308.08758] Discrete Prompt Compression with Reinforcement Learning
5. https://ar5iv.org/html/2310.01691. [2310.01691] Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models
6. https://ar5iv.org/html/2409.16674. [2409.16674] A Prompting-Based Representation Learning Method for Recommendation with Large Language Models
7. https://ar5iv.org/html/2412.01644. [2412.01644] Concept Based Continuous Prompts for Interpretable Text Classification
8. https://ar5iv.org/html/2312.03740. [2312.03740] Prompting in Autoregressive Large Language Models
9. https://ar5iv.org/html/2211.02483. [2211.02483] Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing

## Answer (I don't know?: ‚úó)

Several methods have been proposed for designing discrete prompts in BERT models, focusing primarily on optimizing prompt effectiveness to enhance model performance. 

One of the earliest approaches is heuristic-based prompt design, as introduced by Jiang et al. (2020), which employs dependency parsing techniques to identify useful prompts drawn from a corpus, such as Wikipedia. This method systematically analyzes the structure of sentences and uses linguistic dependencies to create prompts that align with the language model's understanding [1]. 

Building on this foundational work, Gao et al. (2021) utilized T5 as a prompt generator, employing a beam search method to develop a diversified array of prompts. They subsequently selected the most effective prompt from the generated set using a development set denoted as D_dev, aiming for high performance [1]. This illustrates a method of diversification followed by evaluation to refine prompt selection.

Additionally, Shin et al. (2020) proposed a gradient-based prompt search technique. In this approach, each prompt token is optimized directly on a downstream task, allowing for a gradient-based optimization process that adjusts the tokens to maximize performance [1][2]. This method emphasizes iteratively improving prompt components based on task-specific outcomes.

The challenges associated with discrete prompt design are notable and lead to a reliance on manual methods that can be time-consuming and subjective. As stated in the context, initial hard prompts are often derived through trial and error, reflecting significant empirical knowledge and human intuition. However, this process can yield inconsistencies and requires careful consideration of the model‚Äôs performance sensitivity to different natural language prompt formats [4].

There are also more recent contributions aimed at refining how discrete prompts are structured. For example, Prasad et al. (2023) introduced an edit-based search method applicable in situations where gradient information may not be available, allowing for exploration in non-traditional configurations of prompts [2]. This highlights the versatility of discrete prompt optimization strategies beyond conventional gradients.

While substantial progress has been made, the integration of various components, such as exemplars and verbalizers, remains crucial to enhancing the prompts' effectiveness. A study by Zhang et al. (2022) contributed to this by persuading several components of a prompt to work cohesively through reinforcement learning techniques, thereby optimizing their collective performance in various contexts [2]. 

In summary, the methods for designing discrete prompts in BERT models encompass heuristic parsing, generator utilization, gradient-based optimization, and component integration, all aimed at leveraging the model's capabilities for improved downstream task performance. However, the inherent challenges and the manual nature of initial prompt design highlight the ongoing need for more effective methodologies and theoretical foundations in this field [3][4].

1. [1]:  https://ar5iv.org/html/2309.13205, [2309.13205] A Practical Survey on Zero-shot Prompt Design for In-context Learning
2. [2]:  https://ar5iv.org/html/2308.08758, [2308.08758] Discrete Prompt Compression with Reinforcement Learning
3. [3]:  https://ar5iv.org/html/2309.13205, [2309.13205] A Practical Survey on Zero-shot Prompt Design for In-context Learning
4. [4]:  https://ar5iv.org/html/2404.01077, [2404.01077] Efficient Prompting Methods for Large Language Models: A Survey
5. [5]:  https://ar5iv.org/html/2309.13205, [2309.13205] A Practical Survey on Zero-shot Prompt Design for In-context Learning
---
1. [1]:  Passage ID 1: As depicted in Figure 2, prompts in BERT are usually combined with input to form a cloze-style structure, while for transformer decoder-based models, prompts are more flexible.Numerous studies have investigated prompt design in BERT. In the work by Jiang et¬†al. (2020), the authors proposed heuristic-based approaches for designing discrete prompts. Dependency parsing is employed to identify useful prompts from Wikipedia. In Gao et¬†al. (2021), the authors utilized T5 as a prompt generator with a beam search to create a set of diversified prompts. They then used Dd‚Äãe‚Äãvsubscriptùê∑ùëëùëíùë£D_{dev} to select a single prompt with the best performance. In Shin et¬†al. (2020), a gradient-based prompt search approach was proposed, wherein each prompt token is learned by directly optimizing LMs on the downstream task.In addition to prompt designing strategies, other research work focuses on enriching the prompt candidates and ensembling the output from multiple prompts for the same input. To enrich
2. [2]:  Passage ID 2: 2022). Consequently, prompt optimization in LMs has emerged as a significant area of study. For example, prompt tuning optimizes continuous embeddings using gradient descent (Lester, Al-Rfou, and Constant 2021; Liu et¬†al. 2021). In contrast, discrete prompt optimization searches for tokens or exemplars to construct more effective prompts. A study by Shin et¬†al. (2020), utilizes gradient information to search for the best-performing prompt. Prasad et¬†al. (2023) proposed an edit-based search method applicable in gradient-free scenarios. Zhou et¬†al. (2022) leveraged LMs to generate and evaluate prompts. Deng et¬†al. (2022) introduced an RL-based framework that generated optimal prompts to improve the LM‚Äôs performance. Another noteworthy study (Zhang et¬†al. 2022), integrated various components of a prompt, including exemplars and verbalizer, and optimized them using RL. Although these studies have made remarkable progress, they have focused on enhancing performance, largely neglecting the
3. [3]:  Passage ID 3: in HELM for existing models, it is worth noting that the design of our prompt will directly impact the models‚Äô performance.5 ConclusionThe rapid development of large language models (LLMs) has significantly influenced various NLP tasks. Among the techniques to harness their capabilities, in-context learning with different types of prompts‚Äîdiscrete, continuous, few-shot, and zero-shot‚Äîhas shown remarkable promise.Discrete prompt engineering emphasizes human-readable prompts that can enhance model performance, while continuous prompt optimization involves soft prompts that can be learned and optimized directly in the same language model. Few-shot learning leverages a small number of examples to guide the model in the right direction, whereas zero-shot discrete prompts only require task instructions, offering a more straightforward design process.Manual design of prompts can be guided by principles based on model behavior, and optimization algorithms can be used to find
4. [4]:  Passage ID 4: LLMs.Difficult Prompt Design Due to the discrete nature of natural language, early usable hard prompts are usually manual-designed and then obtained through repeated trial and error. Hand-crafted prompt templates rely heavily on empirical knowledge and involve obvious human subjectivity. But there is a difference between human problem-solving approaches and those of neural networks, in other words, the interpretability of LLMs is still a topic of continuous exploration, with no recognized theoretical guidance at present. As a result, there are many challenges in prompt design for LLMs, including the high sensitivity of LLMs to natural language prompt formats, the large performance gap of semantically similar prompts, the correlation between prompt complexity and task difficulty, and the model- and task-specific attributes of the prompts. So, it is time-consuming and laborious to manually design prompts of high quality in the face of different models and different tasks.In
5. [5]:  Passage ID 5: in GPT-style models. In Jiang et¬†al. (2020), the authors proposed rank-based ensemble and optimized ensemble methods to aggregate answers generated from different prompts.Among the studies designing prompts for BERT models, the majority focus on discrete prompts (i.e., hard prompts). To the best of our knowledge, we did not find any work attempting to generate continuous prompts. In general, optimizing prompts in BERT brings only marginal improvements to the original model. Given the size and structure of BERT, it is more favorable to fine-tune on downstream tasks.Figure 2: Prompt categorization by model types3.2 Prompts for Decoder-only Transformer (GPT)3.2.1 Continuous PromptAnother line of research has focused on optimizing soft prompts, which eliminate the constraint that prompts have to be natural language. Soft prompts can be learned and optimized directly within the same language model. The key difference between soft prompt tuning and fine-tuning is that prompt