# A Practical Survey on Zero-shot Prompt Design for In-context Learning

## Question

What methods have been proposed for designing discrete prompts in BERT models, and how do these methods work?

## URLs

1. https://ar5iv.org/html/2411.00985. [2411.00985] FedDTPT: Federated Discrete and Transferable Prompt Tuning for Black-Box Large Language Models
2. https://ar5iv.org/html/2404.01077. [2404.01077] Efficient Prompting Methods for Large Language Models: A Survey
3. https://ar5iv.org/html/2309.13205. [2309.13205] A Practical Survey on Zero-shot Prompt Design for In-context Learning
4. https://ar5iv.org/html/2308.08758. [2308.08758] Discrete Prompt Compression with Reinforcement Learning
5. https://ar5iv.org/html/2310.01691. [2310.01691] Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models
6. https://ar5iv.org/html/2409.16674. [2409.16674] A Prompting-Based Representation Learning Method for Recommendation with Large Language Models
7. https://ar5iv.org/html/2412.01644. [2412.01644] Concept Based Continuous Prompts for Interpretable Text Classification
8. https://ar5iv.org/html/2312.03740. [2312.03740] Prompting in Autoregressive Large Language Models
9. https://ar5iv.org/html/2211.02483. [2211.02483] Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing

## Answer (I don't know?: ‚úó)

Several methods have been proposed for designing discrete prompts in BERT models. These methods focus primarily on heuristic-based approaches, dependency parsing, and ensemble techniques, aimed at optimizing prompt performance through a blend of manual design and algorithm-driven optimizations.

1. **Heuristic-based Approaches:** One significant method was introduced by Jiang et al. (2020), which employed heuristic techniques to design discrete prompts. This research involved utilizing dependency parsing to extract useful prompts from Wikipedia, providing a systematic way to identify and select prompts that are likely to improve model output [1].

2. **Diversified Prompt Generation:** In the work by Gao et al. (2021), the authors used the T5 model as a prompt generator. This approach adopted a beam search strategy to create a variety of prompts [1]. The intent was to develop diverse prompts, which could then be sifted through for those that yield the best performance on a given task.

3. **Gradient-based Prompt Search:** Another innovative approach was suggested by Shin et al. (2020), involving a gradient-based prompt search mechanism. In this method, every token in the prompt is optimized by directly training the language model on the relevant downstream task, aiming to discover tokens that enhance the model's performance by analyzing the gradients [1][2].

4. **Ensemble Methods:** Jiang et al. (2020) also outlined the use of rank-based ensemble and optimized ensemble techniques, which aggregate answers generated from multiple prompts. This ensemble strategy allows for taking advantage of various prompt designs to improve the overall response accuracy of the model [5].

These methods illustrate a combination of empirical techniques and advanced algorithms aimed at refining discrete prompts in BERT models, suggesting that while the manual design of prompts can be resource-intensive and heavily subjective, innovative strategies arise to automate and optimize this process [4][5].

It's also noteworthy that while these prompt engineering techniques have shown promise, they still frequently yield only marginal improvements to BERT's original capabilities. Given the architecture and size of BERT, fine-tuning on specific downstream tasks often remains a more effective means of improving model performance compared to prompt optimization alone [5]. Thus, the interplay between manual prompt design, empirical insights, and machine learning techniques continues to be a vibrant area of research within NLP.

1. [1]:  https://ar5iv.org/html/2309.13205, [2309.13205] A Practical Survey on Zero-shot Prompt Design for In-context Learning
2. [2]:  https://ar5iv.org/html/2308.08758, [2308.08758] Discrete Prompt Compression with Reinforcement Learning
3. [3]:  https://ar5iv.org/html/2309.13205, [2309.13205] A Practical Survey on Zero-shot Prompt Design for In-context Learning
4. [4]:  https://ar5iv.org/html/2404.01077, [2404.01077] Efficient Prompting Methods for Large Language Models: A Survey
5. [5]:  https://ar5iv.org/html/2309.13205, [2309.13205] A Practical Survey on Zero-shot Prompt Design for In-context Learning
---
1. [1]:  Passage ID 1: As depicted in Figure 2, prompts in BERT are usually combined with input to form a cloze-style structure, while for transformer decoder-based models, prompts are more flexible.Numerous studies have investigated prompt design in BERT. In the work by Jiang et¬†al. (2020), the authors proposed heuristic-based approaches for designing discrete prompts. Dependency parsing is employed to identify useful prompts from Wikipedia. In Gao et¬†al. (2021), the authors utilized T5 as a prompt generator with a beam search to create a set of diversified prompts. They then used Dd‚Äãe‚Äãvsubscriptùê∑ùëëùëíùë£D_{dev} to select a single prompt with the best performance. In Shin et¬†al. (2020), a gradient-based prompt search approach was proposed, wherein each prompt token is learned by directly optimizing LMs on the downstream task.In addition to prompt designing strategies, other research work focuses on enriching the prompt candidates and ensembling the output from multiple prompts for the same input. To enrich
2. [2]:  Passage ID 2: 2022). Consequently, prompt optimization in LMs has emerged as a significant area of study. For example, prompt tuning optimizes continuous embeddings using gradient descent (Lester, Al-Rfou, and Constant 2021; Liu et¬†al. 2021). In contrast, discrete prompt optimization searches for tokens or exemplars to construct more effective prompts. A study by Shin et¬†al. (2020), utilizes gradient information to search for the best-performing prompt. Prasad et¬†al. (2023) proposed an edit-based search method applicable in gradient-free scenarios. Zhou et¬†al. (2022) leveraged LMs to generate and evaluate prompts. Deng et¬†al. (2022) introduced an RL-based framework that generated optimal prompts to improve the LM‚Äôs performance. Another noteworthy study (Zhang et¬†al. 2022), integrated various components of a prompt, including exemplars and verbalizer, and optimized them using RL. Although these studies have made remarkable progress, they have focused on enhancing performance, largely neglecting the
3. [3]:  Passage ID 3: in HELM for existing models, it is worth noting that the design of our prompt will directly impact the models‚Äô performance.5 ConclusionThe rapid development of large language models (LLMs) has significantly influenced various NLP tasks. Among the techniques to harness their capabilities, in-context learning with different types of prompts‚Äîdiscrete, continuous, few-shot, and zero-shot‚Äîhas shown remarkable promise.Discrete prompt engineering emphasizes human-readable prompts that can enhance model performance, while continuous prompt optimization involves soft prompts that can be learned and optimized directly in the same language model. Few-shot learning leverages a small number of examples to guide the model in the right direction, whereas zero-shot discrete prompts only require task instructions, offering a more straightforward design process.Manual design of prompts can be guided by principles based on model behavior, and optimization algorithms can be used to find
4. [4]:  Passage ID 4: LLMs.Difficult Prompt Design Due to the discrete nature of natural language, early usable hard prompts are usually manual-designed and then obtained through repeated trial and error. Hand-crafted prompt templates rely heavily on empirical knowledge and involve obvious human subjectivity. But there is a difference between human problem-solving approaches and those of neural networks, in other words, the interpretability of LLMs is still a topic of continuous exploration, with no recognized theoretical guidance at present. As a result, there are many challenges in prompt design for LLMs, including the high sensitivity of LLMs to natural language prompt formats, the large performance gap of semantically similar prompts, the correlation between prompt complexity and task difficulty, and the model- and task-specific attributes of the prompts. So, it is time-consuming and laborious to manually design prompts of high quality in the face of different models and different tasks.In
5. [5]:  Passage ID 5: in GPT-style models. In Jiang et¬†al. (2020), the authors proposed rank-based ensemble and optimized ensemble methods to aggregate answers generated from different prompts.Among the studies designing prompts for BERT models, the majority focus on discrete prompts (i.e., hard prompts). To the best of our knowledge, we did not find any work attempting to generate continuous prompts. In general, optimizing prompts in BERT brings only marginal improvements to the original model. Given the size and structure of BERT, it is more favorable to fine-tune on downstream tasks.Figure 2: Prompt categorization by model types3.2 Prompts for Decoder-only Transformer (GPT)3.2.1 Continuous PromptAnother line of research has focused on optimizing soft prompts, which eliminate the constraint that prompts have to be natural language. Soft prompts can be learned and optimized directly within the same language model. The key difference between soft prompt tuning and fine-tuning is that prompt