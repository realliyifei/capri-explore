# A Practical Survey on Zero-shot Prompt Design for In-context Learning

## Question

What methods have been proposed for designing discrete prompts in BERT models, and how do these methods work?

## URLs

1. https://ar5iv.org/html/2411.00985. [2411.00985] FedDTPT: Federated Discrete and Transferable Prompt Tuning for Black-Box Large Language Models
2. https://ar5iv.org/html/2404.01077. [2404.01077] Efficient Prompting Methods for Large Language Models: A Survey
3. https://ar5iv.org/html/2309.13205. [2309.13205] A Practical Survey on Zero-shot Prompt Design for In-context Learning
4. https://ar5iv.org/html/2308.08758. [2308.08758] Discrete Prompt Compression with Reinforcement Learning
5. https://ar5iv.org/html/2310.01691. [2310.01691] Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models
6. https://ar5iv.org/html/2409.16674. [2409.16674] A Prompting-Based Representation Learning Method for Recommendation with Large Language Models
7. https://ar5iv.org/html/2412.01644. [2412.01644] Concept Based Continuous Prompts for Interpretable Text Classification
8. https://ar5iv.org/html/2312.03740. [2312.03740] Prompting in Autoregressive Large Language Models
9. https://ar5iv.org/html/2211.02483. [2211.02483] Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing

## Answer (I don't know?: âœ—)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable