# Transformers as Recognizers of Formal Languages: A Survey on Expressivity

## Question

What are the challenges and considerations when limiting numeric precision in transformer models?

## URLs

1. https://ar5iv.org/html/2402.08164. [2402.08164] On Limitations of the Transformer Architecture
2. https://ar5iv.org/html/2410.13857. [2410.13857] How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs
3. https://ar5iv.org/html/2407.11722. [2407.11722] Exploring Quantization for Efficient Pre-Training of Transformer Language Models
4. https://ar5iv.org/html/2402.05785. [2402.05785] Limits of Transformer Language Models on Learning Algorithmic Compositions
5. https://ar5iv.org/html/2406.04089. [2406.04089] On Limitation of Transformer for Learning HMMs
6. https://ar5iv.org/html/2411.03766. [2411.03766] Number Cookbook: Number Understanding of Language Models and How to Improve It
7. https://ar5iv.org/html/2103.14636. [2103.14636] A Practical Survey on Faster and Lighter Transformers
8. https://ar5iv.org/html/2310.18313. [2310.18313] FP8-LM: Training FP8 Large Language Models
9. https://ar5iv.org/html/2311.00208. [2311.00208] Transformers as Recognizers of Formal Languages: A Survey on Expressivity

## Answer (I don't know?: âœ—)

Error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable