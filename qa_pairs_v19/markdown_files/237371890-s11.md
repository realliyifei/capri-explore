# Survey of Low-Resource Machine Translation

## Question

In what ways have language models been employed to create synthetic parallel data for enhancing low-resource machine translation?

## URLs

1. https://ar5iv.org/html/2409.02712. [2409.02712] A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations
2. https://ar5iv.org/html/2411.11295. [2411.11295] Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation
3. https://ar5iv.org/html/2408.12079. [2408.12079] High-Quality Data Augmentation for Low-Resource NMT: Combining a Translation Memory, a GAN Generator, and Filtering
4. https://ar5iv.org/html/2409.00071. [2409.00071] Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation
5. https://ar5iv.org/html/2412.04497. [2412.04497] Opportunities and Challenges of Large Language Models for Low-Resource Languages in Humanities Research
6. https://ar5iv.org/html/2404.02392. [2404.02392] Low-resource neural machine translation with morphological modeling
7. https://ar5iv.org/html/2408.12780. [2408.12780] Quality or Quantity? On Data Scale and Diversity in Adapting Large Language Models for Low-Resource Translation
8. https://ar5iv.org/html/2307.05779. [2307.05779] Neural Machine Translation Data Generation and Augmentation using ChatGPT
9. https://ar5iv.org/html/2412.05184. [2412.05184] QueEn: A Large Language Model for Quechua-English Translation
10. https://ar5iv.org/html/2411.08348. [2411.08348] Refining Translations with LLMs: A Constraint-Aware Iterative Prompting Approach

## Answer (I don't know?: ✗)

Language models have been employed in several innovative ways to create synthetic parallel data that enhances low-resource machine translation (MT). These approaches aim to leverage available high-resource language data to improve the translation capabilities of models trained on low-resource languages.

One prominent method involves using high-resource languages as a pivot for unsupervised machine translation (UMT). For instance, Xia et al. (2019) proposed creating synthetic sentence pairs by translating from a high-resource language to a low-resource language. This strategy can enrich the training dataset for low-resource languages where parallel data is scarce, thereby improving translation performance through augmentation with synthetic data [2].

Another approach highlighted in the context is the manipulation of perplexity, where lower perplexity data from high-resource languages is selected for inclusion in low-resource models. This method, proposed by Lakew et al. (2019), emphasizes the importance of data quality in boosting the performance of translation models for low-resource languages [2].

Various studies have also indicated that leveraging similar languages can enhance translation capabilities. For instance, Hassan et al. (2017) demonstrated how synthetic parallel data could be generated from existing corpora of related dialects or languages to improve the translation model's performance. This approach reflects an understanding of linguistic proximity to create data that can effectively enhance learning in models focusing on low-resource languages [2].

In addition, Siddhant et al. (2020) highlighted the potential of training multilingual translation and denoising tasks concurrently. Their research showed that models could translate between languages without parallel data by achieving performance levels close to supervised multilingual neural machine translation (NMT) approaches, suggesting a promising way of generating useful translation data for low-resource languages [2].

Despite their great promise, the literature indicates that many traditional methods used for high-resource language translation may not yield the same results for low-resource languages. Challenges remain, including a lack of effective strategies for translating in the opposite direction— from low-resource languages to high-resource languages— as evidenced by subpar performance [2][3]. 

Finally, as discussed in various passages, new and innovative techniques are necessary. Current research efforts focus on enhancing language models and leveraging multi-modal data to overcome these limitations. Integration of insights from multilingual lexicons and linguistic tools appears to be moving towards more extensible solutions, helping models generalize better across low-resource languages [3][4][5].

Overall, the creation of synthetic parallel data for low-resource MT is evolving through several methodologies, each aiming to leverage existing data and linguistic relationships. Enhancements in model architecture and interdisciplinary techniques may further advance this field.

1. [1]:  https://ar5iv.org/html/2307.05779, [2307.05779] Neural Machine Translation Data Generation and Augmentation using ChatGPT
2. [2]:  https://ar5iv.org/html/2105.15071, No Title
3. [3]:  https://ar5iv.org/html/2408.12780, [2408.12780] Quality or Quantity? On Data Scale and Diversity in Adapting Large Language Models for Low-Resource Translation
4. [4]:  https://ar5iv.org/html/2107.04239, No Title
5. [5]:  https://ar5iv.org/html/2107.04239, No Title
---
1. [1]:  Passage ID 1: well in naturally low resource settings, where it is not possible for ChatGPT to generate synthetic parallel data.8 ConclusionsOur experiments demonstrate a possible issue with using large language models to generate synthetic data: the lack of diversity in generated data. It is becoming evident that many of the capabilities of these models are limited only by the specific prompts used to communicate with them. More research is necessary to determine whether prompt engineering can improve the diversity issue. For example, using more sentences during the ’Assistant’ few shot phase in section 3.1.1. As new and varied prompt-based models are released, the problem may also naturally be solved by better language models and more thorough linguistic understanding on the part of the models.Our experiments do show, however, that augmenting natural data with entirely synthetic data shows promise for training machine translators. Despite a domain mismatch, translation quality improved,
2. [2]:  Passage ID 2: low-resource languages with high-resource language data. Lakew et al. (2019) proposed selecting high-resource language data with lower perplexity in the low-resource language model. Xia et al. (2019) created synthetic sentence pairs by unsupervised machine translation, using the high-resource language as a pivot. However these previous approaches emphasize translating from the low-resource language to English, while the opposite direction is either unconsidered or shows poor translation performance. Siddhant et al. (2020) trained multilingual translation and denoising simultaneously, and showed that the model could translate languages without parallel data into English near the performance of supervised multilingual NMT.Similar language translationSimilar to our work, there have been methods proposed that leverage similar languages to improve translation. Hassan et al. (2017) generated synthetic English-dialect parallel data from English-main language corpus. However, this method
3. [3]:  Passage ID 3: Neural Machine Translation (NMT) models, trained mainly on parallel data, in the latest WMT shared task (Kocmi et al., 2024).However, LLM translation for low-resource languages (LRLs) still lags significantly behind NMT models (Robinson et al., 2023; Zhu et al., 2024b). While the strong performance of LLMs on high-resource languages can be attributed to the skewed language distribution during pre-training and the unintentional consumption of parallel data at scale (Briakou et al., 2023), no such relief exists for LRLs.This leads to the main question motivating this paper: What would it take to adapt LLMs for low-resource MT?Recent work on LRL translation with LLMs has explored using resources like multilingual lexicons (Lu et al., 2023), word alignments (Mao and Yu, 2024) or linguistic tools (Zhang et al., 2024b). While effective, reliance on such tools hinders ease of extensibility across languages. Instead, in this work, we take inspiration from research done for high-resource
4. [4]:  Passage ID 4: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the
5. [5]:  Passage ID 5: target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.1 IntroductionMachine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT) Bahdanau et al. (2015); Vaswani et al. (2017) has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the