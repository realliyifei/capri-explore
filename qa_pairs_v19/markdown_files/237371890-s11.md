# Survey of Low-Resource Machine Translation

## Question

In what ways have language models been employed to create synthetic parallel data for enhancing low-resource machine translation?

## URLs

1. https://ar5iv.org/html/2409.02712. [2409.02712] A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations
2. https://ar5iv.org/html/2411.11295. [2411.11295] Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation
3. https://ar5iv.org/html/2408.12079. [2408.12079] High-Quality Data Augmentation for Low-Resource NMT: Combining a Translation Memory, a GAN Generator, and Filtering
4. https://ar5iv.org/html/2409.00071. [2409.00071] Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation
5. https://ar5iv.org/html/2412.04497. [2412.04497] Opportunities and Challenges of Large Language Models for Low-Resource Languages in Humanities Research
6. https://ar5iv.org/html/2404.02392. [2404.02392] Low-resource neural machine translation with morphological modeling
7. https://ar5iv.org/html/2408.12780. [2408.12780] Quality or Quantity? On Data Scale and Diversity in Adapting Large Language Models for Low-Resource Translation
8. https://ar5iv.org/html/2307.05779. [2307.05779] Neural Machine Translation Data Generation and Augmentation using ChatGPT
9. https://ar5iv.org/html/2412.05184. [2412.05184] QueEn: A Large Language Model for Quechua-English Translation
10. https://ar5iv.org/html/2411.08348. [2411.08348] Refining Translations with LLMs: A Constraint-Aware Iterative Prompting Approach

## Answer (I don't know?: ✗)

Language models, particularly large language models (LLMs), have been utilized to address the challenges of low-resource machine translation (MT) through the creation of synthetic parallel data. The generation of synthetic data is critical in low-resource settings where the availability of high-quality parallel corpora is limited, which is often the case for languages with smaller speaker populations and less linguistic resources [2] [4]. Here are several ways in which language models have been employed in this context:

1. **Data Augmentation Strategies**: Recent research indicates that augmenting existing natural data with entirely synthetic data shows promise in improving translation quality. This augmentation is particularly beneficial in scenarios where parallel data is scarce [1]. The integration of pre-trained language models enhances this process, allowing for more effective data generation by leveraging learned language patterns [4].

2. **Transformers and Attention Mechanisms**: The implementation of transformer models, which utilize attention mechanisms, allows for a more nuanced focus on different parts of input sentences. This flexibility improves translation accuracy and becomes vital when dealing with limited data [2]. The transformer architecture facilitates the creation of diverse synthetic data, aiding in the training of MT systems even in low-resource situations.

3. **Leveraging Multilingual Resources**: Explorations in LLMs for low-resource languages have included using multilingual lexicons, word alignments, and other linguistic tools to enhance the performance of these models in generating synthetic parallel data. Although reliance on such external tools can complicate extensibility across languages, they are instrumental in generating necessary training examples for underrepresented languages [3].

4. **Pre-training on Synthetic Data**: There are efforts focused on pre-training LLMs with synthetic, code-switched parallel data. This pre-training has been shown to significantly impact low-resource performance, with experiments conducted on parallel sentences indicating notable improvements in translation outcomes. Specifically, the discovery that bitext (bilingual text) plays a crucial role in influencing gains in less-resourced languages highlights the importance of synthetic data in this domain [5].

5. **Word Order Relationships**: Some methodologies have explored augmenting models by analyzing the word order relationships between source and target languages. This strategy has yielded considerable improvements in translation performance, particularly when intersecting with the challenges of parallel data scarcity. By focusing on syntactic structures, these models can enrich the synthetic data generated and better accommodate languages with varying grammar [4].

In summary, language models contribute to enhancing low-resource machine translation through strategic data augmentation, leveraging transformers, integrating multilingual resources, pre-training with synthetic data, and analyzing linguistic aspects such as word order. These efforts collectively address the scarcity of parallel data and aim to boost the translation capabilities for languages that traditionally suffer from such limitations in available resources.

1. [1]:  https://ar5iv.org/html/2307.05779, [2307.05779] Neural Machine Translation Data Generation and Augmentation using ChatGPT
2. [2]:  https://ar5iv.org/html/2408.12079, [2408.12079] High-Quality Data Augmentation for Low-Resource NMT: Combining a Translation Memory, a GAN Generator, and Filtering
3. [3]:  https://ar5iv.org/html/2408.12780, [2408.12780] Quality or Quantity? On Data Scale and Diversity in Adapting Large Language Models for Low-Resource Translation
4. [4]:  https://ar5iv.org/html/2404.02392, [2404.02392] Low-resource neural machine translation with morphological modeling
5. [5]:  https://ar5iv.org/html/2408.12780, [2408.12780] Quality or Quantity? On Data Scale and Diversity in Adapting Large Language Models for Low-Resource Translation
---
1. [1]:  Passage ID 1: well in naturally low resource settings, where it is not possible for ChatGPT to generate synthetic parallel data.8 ConclusionsOur experiments demonstrate a possible issue with using large language models to generate synthetic data: the lack of diversity in generated data. It is becoming evident that many of the capabilities of these models are limited only by the specific prompts used to communicate with them. More research is necessary to determine whether prompt engineering can improve the diversity issue. For example, using more sentences during the ’Assistant’ few shot phase in section 3.1.1. As new and varied prompt-based models are released, the problem may also naturally be solved by better language models and more thorough linguistic understanding on the part of the models.Our experiments do show, however, that augmenting natural data with entirely synthetic data shows promise for training machine translators. Despite a domain mismatch, translation quality improved,
2. [2]:  Passage ID 2: Language Processing (NLP) [16].However, today, most of the researchers design their models based on transformer [19].The transformer model, based on attention mechanisms, allows more flexible focus on different parts of the input sentence, further improving translation accuracy.It also set new benchmarks for most other tasks in the field of NLP.Low-resource translation [9] refers to the situation where machine translation encounters a shortage of parallel corpora for training models.This commonly occurs for languages with limited linguistic resources or smaller speaker populations, leading to difficulties in achieving effective translation performance.A Translation Memory (TM) is a repository that archives pairs of source sentences along with their corresponding translations.Recent studies have validated the beneficial impact of TM on enhancing NMT models.This enhancement has been demonstrated through various approaches, including concatenating both the source and target
3. [3]:  Passage ID 3: Neural Machine Translation (NMT) models, trained mainly on parallel data, in the latest WMT shared task (Kocmi et al., 2024).However, LLM translation for low-resource languages (LRLs) still lags significantly behind NMT models (Robinson et al., 2023; Zhu et al., 2024b). While the strong performance of LLMs on high-resource languages can be attributed to the skewed language distribution during pre-training and the unintentional consumption of parallel data at scale (Briakou et al., 2023), no such relief exists for LRLs.This leads to the main question motivating this paper: What would it take to adapt LLMs for low-resource MT?Recent work on LRL translation with LLMs has explored using resources like multilingual lexicons (Lu et al., 2023), word alignments (Mao and Yu, 2024) or linguistic tools (Zhang et al., 2024b). While effective, reliance on such tools hinders ease of extensibility across languages. Instead, in this work, we take inspiration from research done for high-resource
4. [4]:  Passage ID 4: the transformer model. Beside augmentation from pre-trained language model integration, we also devise an augmentation based solely on the word order relationship between source and target languages. These model augmentations bring substantial improvement in translation performance when parallel text is scarce.One of the main challenges facing machine translation for low-resource languages obviously is parallel data scarcity. When the training data has limited lexical coverage, the NMT model may tend to hallucinate Raunak et al. (2021); Xu et al. (2023). Additionally, for a morphology-aware translation model, there is a problem of misaligned vocabularies between source and target languages. This makes it harder for the model to learn to copy unknown words and other tokens that need to be copied without translation such as proper names.To address these challenges, we take a data-centric approach by developing tools to extract more parallel data from public-domain documents and
5. [5]:  Passage ID 5: et al., 2023; Wei et al., 2023; Fujii et al., 2024; Alves et al., 2024). Concurrent work has explored pre-training on synthetic, code-switched parallel data for 101 languages (Lu et al., 2024) with a total of 400M sentences This work explores the impact of parallel data exclusively for low-resource performance and experiments with 1M–13M parallel sentences (50M–750M tokens) during pre-training, two orders of magnitude smaller than prior work. Moreover, we discover that bitext is particularly important for the least-resourced languages in our setup, which enjoys the highest and most consistent gains.Low-Resource Explorations in LLM-MTEvaluation in massively multilingual settings have shown that LLMs are poor at low-resource translation (Robinson et al., 2023; Zhu et al., 2024b). In response, some works have advocated leveraging external resources in the MT pipeline, including multilingual lexicons (Lu et al., 2023), rule-based linguistic tools (Zhang et al., 2024b), word