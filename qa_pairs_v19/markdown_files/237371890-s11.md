# Survey of Low-Resource Machine Translation

## Question

In what ways have language models been employed to create synthetic parallel data for enhancing low-resource machine translation?

## URLs

1. https://ar5iv.org/html/2409.02712
2. https://ar5iv.org/html/2411.11295
3. https://ar5iv.org/html/2408.12079
4. https://ar5iv.org/html/2409.00071
5. https://ar5iv.org/html/2412.04497
6. https://ar5iv.org/html/2404.02392
7. https://ar5iv.org/html/2408.12780
8. https://ar5iv.org/html/2307.05779
9. https://ar5iv.org/html/2412.05184
10. https://ar5iv.org/html/2411.08348

## Answer

## I don't know?: False

Language models (LLMs) have been effectively employed in various ways to create synthetic parallel data aimed at enhancing low-resource machine translation (MT). The key approaches highlighted in the provided context include leveraging LLMs for generating synthetic data, utilizing multilingual resources, and employing knowledge distillation methods.

Firstly, the generation of synthetic data using advanced LLMs has shown promise. For instance, Claude, an LLM, was used to produce synthetic data that significantly improved translations, specifically in the case of Yoruba-English translation. This approach allowed researchers to compress advancements in LLM translation into traditional neural machine translation (NMT) models, demonstrating superior translation performance that met or even surpassed established benchmarks like NLLB-54B and Google Translate [5]. This indicates that LLMs not only create the necessary language pairs but also enhance the existing capabilities of NMT systems.

Secondly, combining external multilingual resources for enhancing parallel data has been explored. In the ongoing efforts to adapt LLMs for low-resource languages, resources such as multilingual lexicons, word alignments, and various linguistic tools have been utilized. While the reliance on such resources can hinder extensibility across languages, they provide essential frameworks that assist in generating synthetic translations for low-resource language pairs [3] [4]. This is particularly significant given that LLMs have lagged behind NMT models in low-resource contexts [3].

Furthermore, the context highlights that the exploration of parallel data, especially for low-resource languages, is crucial, as bitext (bilingual text) is found to be particularly beneficial [4]. The experiments conducted with a limited number of parallel sentences (1M–13M parallel sentences) during pre-training have shown notable improvements in low-resource translation performance, confirming the importance of strategically curated synthetic parallel data [4].

In summary, LLMs contribute to creating synthetic parallel data by generating translations through models like Claude, utilizing existing multilingual resources to support translation pipelines, and focusing on bitext in pre-training processes. These combined strategies aim to improve translation accuracy and performance in under-resourced languages, addressing the gap that has traditionally existed between high-resource and low-resource machine translation capabilities [3] [5].

[1]: https://ar5iv.org/html/2408.12079, [2408.12079] High-Quality Data Augmentation for Low-Resource NMT: Combining a Translation Memory, a GAN Generator, and Filtering
[2]: https://ar5iv.org/html/2408.12079, [2408.12079] High-Quality Data Augmentation for Low-Resource NMT: Combining a Translation Memory, a GAN Generator, and Filtering
[3]: https://ar5iv.org/html/2408.12780, [2408.12780] Quality or Quantity? On Data Scale and Diversity in Adapting Large Language Models for Low-Resource Translation
[4]: https://ar5iv.org/html/2408.12780, [2408.12780] Quality or Quantity? On Data Scale and Diversity in Adapting Large Language Models for Low-Resource Translation
[5]: https://ar5iv.org/html/2404.13813, No Title

[1]: Passage ID 1: Language Processing (NLP) [16].However, today, most of the researchers design their models based on transformer [19].The transformer model, based on attention mechanisms, allows more flexible focus on different parts of the input sentence, further improving translation accuracy.It also set new benchmarks for most other tasks in the field of NLP.Low-resource translation [9] refers to the situation where machine translation encounters a shortage of parallel corpora for training models.This commonly occurs for languages with limited linguistic resources or smaller speaker populations, leading to difficulties in achieving effective translation performance.A Translation Memory (TM) is a repository that archives pairs of source sentences along with their corresponding translations.Recent studies have validated the beneficial impact of TM on enhancing NMT models.This enhancement has been demonstrated through various approaches, including concatenating both the source and target
[2]: Passage ID 2: Language Processing (NLP) [16].However, today, most of the researchers design their models based on transformer [19].The transformer model, based on attention mechanisms, allows more flexible focus on different parts of the input sentence, further improving translation accuracy.It also set new benchmarks for most other tasks in the field of NLP.Low-resource translation [9] refers to the situation where machine translation encounters a shortage of parallel corpora for training models.This commonly occurs for languages with limited linguistic resources or smaller speaker populations, leading to difficulties in achieving effective translation performance.A Translation Memory (TM) is a repository that archives pairs of source sentences along with their corresponding translations.Recent studies have validated the beneficial impact of TM on enhancing NMT models.This enhancement has been demonstrated through various approaches, including concatenating both the source and target
[3]: Passage ID 3: Neural Machine Translation (NMT) models, trained mainly on parallel data, in the latest WMT shared task (Kocmi et al., 2024).However, LLM translation for low-resource languages (LRLs) still lags significantly behind NMT models (Robinson et al., 2023; Zhu et al., 2024b). While the strong performance of LLMs on high-resource languages can be attributed to the skewed language distribution during pre-training and the unintentional consumption of parallel data at scale (Briakou et al., 2023), no such relief exists for LRLs.This leads to the main question motivating this paper: What would it take to adapt LLMs for low-resource MT?Recent work on LRL translation with LLMs has explored using resources like multilingual lexicons (Lu et al., 2023), word alignments (Mao and Yu, 2024) or linguistic tools (Zhang et al., 2024b). While effective, reliance on such tools hinders ease of extensibility across languages. Instead, in this work, we take inspiration from research done for high-resource
[4]: Passage ID 4: et al., 2023; Wei et al., 2023; Fujii et al., 2024; Alves et al., 2024). Concurrent work has explored pre-training on synthetic, code-switched parallel data for 101 languages (Lu et al., 2024) with a total of 400M sentences This work explores the impact of parallel data exclusively for low-resource performance and experiments with 1M–13M parallel sentences (50M–750M tokens) during pre-training, two orders of magnitude smaller than prior work. Moreover, we discover that bitext is particularly important for the least-resourced languages in our setup, which enjoys the highest and most consistent gains.Low-Resource Explorations in LLM-MTEvaluation in massively multilingual settings have shown that LLMs are poor at low-resource translation (Robinson et al., 2023; Zhu et al., 2024b). In response, some works have advocated leveraging external resources in the MT pipeline, including multilingual lexicons (Lu et al., 2023), rule-based linguistic tools (Zhang et al., 2024b), word
[5]: Passage ID 5: level. Finally, we show that advancements in LLM translation can be compressed into traditional neural machine translation (NMT) models. Using Claude to generate synthetic data, we demonstrate that knowledge distillation advances the state-of-the-art in Yoruba-English translation, meeting or surpassing strong baselines like NLLB-54B and Google Translate.1 IntroductionLarge language models (LLMs) Kaplan et al. (2020); Hoffmann et al. (2022) have emerged as a breakthrough technology for natural language processing. LLMs have demonstrated a remarkable ability to perform many downstream tasks Brown et al. (2020), including machine translation. In fact, Zhu et al. (2023); Jiao et al. (2023); Robinson et al. (2023) have shown that the translation performance of GPT-4 Achiam et al. (2023) is competitive with state-of-the-art neural machine translation (NMT) systems on several high-resource language pairs. However on low-resource language pairs, they showed that existing LLMs lag behind